{
    "id": "zr6rfsxqbk26hehs65yqm5m2fa5c6qd6",
    "title": "Introduction to kernel methods",
    "info": {
        "author": [
            "Bernhard Sch\u00f6lkopf, Max Planck Institute"
        ],
        "published": "Aug. 20, 2007",
        "recorded": "August 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/mlss07_scholkopf_intkmet/",
    "segmentation": [
        [
            "OK, so this would be maybe slightly less elementary, some of it, but but still fairly elementary and many of you will still know it, but nevertheless I hope some of you don't know it yet, and maybe they will.",
            "Can't hear OK, so I'll try to put this in closer.",
            "How is that is the same?",
            "So I have to speak up.",
            "OK so I will talk about learning with kernels.",
            "It's a particular area of machine learning that started around 10 years ago or a little bit more and it's now part of the standard toolkit of machine learning.",
            "So if you are working in this field you should really know about these kind of things because you can use them in various contexts."
        ],
        [
            "Let me see how do I go forward here?",
            "So overall I said this morning we are sharing this first course.",
            "We have six hours of lectures.",
            "The first one was just given by Alex.",
            "I will now give 3 hours.",
            "Actually, I think I'll use the pointer on the screen because this may be better for the people next door.",
            "So still allowed on him.",
            "Can we turn up the volume somehow?",
            "OK.",
            "I could just speak like this maybe.",
            "OK so I will give three lectures now.",
            "The first one is about the concept of similarity and the formalization of this concept in terms of kernels.",
            "And also I will talk a bit about feature spaces associated with kernels.",
            "But then we'll have a short coffee break.",
            "Afterwards.",
            "I'll talk about more mathematically about the notion of positive definite kernels.",
            "The associated reproducing kernel Hilbert spaces.",
            "I'll show you how to, knowing it.",
            "It doesn't cost much time.",
            "And finally, I will talk you about one application of these feature spaces, which is already now.",
            "This is touching some fairly recent research connected to the notion of kernel means, so maybe not everybody knows about this one yet.",
            "Tomorrow Alex will continue with support vector machines and structured estim."
        ],
        [
            "Nation.",
            "OK, let's start informally.",
            "So suppose we have inputs and outputs taken from some sets X&Y.",
            "And then we're given a training set of pairs XIYI that are taken from the product of these two sets, and we assume there's some regularity.",
            "Ex tells us something about why.",
            "If we talk about learning, then there's this notion of generalization.",
            "So we want to find rules from our observations which generalize well in the sense that if we're given a new point X that we haven't seen before.",
            "We somehow should be able to find a suitable wire that fits together with this X well in a certain sense.",
            "And in which sensory fit together well, one sense would be to say somehow this new pair XY should be similar to the ones that we have seen before.",
            "But then, of course, immediately the question comes up.",
            "How do we measure similarity and four outputs?",
            "This is usually fairly straightforward, although this is also a topic of research, and it can be arbitrarily complex, but for simplicity, let's say for outputs this problem is trivial and we just use a loss function.",
            "So suppose if we're interested in binary classification, outputs are just plus minus one, in which case we could use the so called 01 loss function is just a loss function that checks.",
            "Are the two Y values the same or are they different?",
            "And it assigns a loss of, let's say, plus one whenever the values are different and assigns a loss of zero whenever they are the same.",
            "But for the inputs, it's less trivial problem to come up with the similarity function."
        ],
        [
            "And this leads to the notion of kernels.",
            "So let's say we use a symmetric function that takes 3 inputs and assigns a real number.",
            "As the similarity value.",
            "So for instance, if our input space where the Euclidean vector space out to the end, we could use just the Canonical dot product between these inputs.",
            "Over this notation eyes denote the entry of the vector X.",
            "And that could be used as a similarity measure.",
            "In a way, it's the simplest similarity measure that we're considering machine learning.",
            "But of course, if our SpaceX or if our set X at the beginning only call this exit set if it's not a dot product space, we cannot use the DOT product, and in this case, what can we do?",
            "Well, we could, for instance, assume that our similarity function K as at least a representation as a dot product in some space that has a dot product.",
            "So by this I mean we could assume there exists a map.",
            "That takes our inputs, Maps them into a dot product space.",
            "And then I will similarity measure is equal to the dot product in that other space.",
            "So in that case we can think of our inputs, so sometimes I will use the term patterns for the inputs.",
            "In this case we could think of them.",
            "As the points represented in that vector space.",
            "And then we can carry out whatever symmetric algorithms we want to carry out in that vector space, and this vector space is sometimes called the feature space."
        ],
        [
            "Let's look at an example of such and such an algorithm that we could carry out in this representation.",
            "And this is maybe the simplest possible classification algorithm, and in fact, so when we first described this, it was just for the introduction of the book that we wrote about kernel machines, just as a tutorial introduction.",
            "But recently we notice this is actually.",
            "Much more useful and I'll come back to this in my third lecture in the context of kernel means.",
            "So this classification algorithm, but it does is just given two sets of points.",
            "The pluses here and the circles here we try to find a classifier that separates one from the other and that given the new point X which is down here for instance, given the new point assigns it to one of the two classes, how do we assign a new test point X to one of the classes where we could just compute the means of the positive class, here denoted C plus the mean of the negative Class C minor.",
            "So I've also written them up here.",
            "M Plus is the number of positive points.",
            "These are all the positive points or something overall positive points.",
            "By positive points I mean once with labels that are plus one, so this is the mean.",
            "Here we have the same for the negative plus and given the test point X, we will just check whether we're closer to the negative or the positive mean.",
            "So it's a.",
            "It's a trivial way of classifying these data points and.",
            "One way to do this, the different ways you could just compute the distances directly.",
            "But another way to do this would be.",
            "I mean, we're working in the DOT product space, so we can compute distances another way in which we can use the DOT products directly rather than the distances would be to compute this Point C which is halfway in between the positive and the negative class means.",
            "Then compute the vector connecting C to our test point.",
            "So this is this vector X -- C and now we just have to check whether this vector what kind of angle does this vector in close with this other vector W which is now the vector connecting the class means?",
            "So if this angle is smaller than 90 degrees, then this point, the test point X is on this side of that hyperplane that have been noted here.",
            "And if the angle is larger than degrees, it would be on this side.",
            "And obviously the set of all points that are closer to see minus than to see plus forms 1/2 space over here, which is separate from the other half spaces by these hyperplanes in the vector W connecting the class means is the normal vector that happened.",
            "OK, so that would be one way to do it.",
            "And if we write it down like this so we have to check whether this dot product is this angle is smaller or larger than 90 degrees and to check this we just have to look at the dot product because you remember the dot product is the cosine of the angle, so we just have to check whether this dot product is positive or negative.",
            "So we can do that.",
            "The product we write down all these things."
        ],
        [
            "OK, so if we substitute all of this."
        ],
        [
            "So we have all these quantities up here, right?",
            "We have CC minus.",
            "We know how to compute this thing.",
            "We know how to compute X -- C."
        ],
        [
            "Leave.",
            "Then we just substitute everything we get this quantity here.",
            "So this is our dot product.",
            "Which is the cosine of the angle, so we have to look at it.",
            "Arithmetic sign.",
            "Is it positive or is it negative and now we use the fact that the DOT product between 2:00?",
            "Representing that space.",
            "Can be computed.",
            "Maybe I'll go a bit further in this direction, can be computed by this similarity function, because we said before, we are assuming our similarity function has a representation as the dot product in this space.",
            "So now we substitute the similarity function in here and in here back here.",
            "I have a term plus V. We're not going to worry about this.",
            "This is a term.",
            "That you that just contains all the quantities that don't depend on the test point X.",
            "So we consider this a constant.",
            "So anyway, so this is our classification function and let's think about it and look at it for a minute.",
            "What does this function do?",
            "It just takes a test point X.",
            "It computes the similarity between X and all positive points.",
            "So this is that the sum of the positive points.",
            "Takes the average such similarity.",
            "It does the same for all negative points.",
            "Again, the average and here we have some constant.",
            "And this can actually be also interpreted in statistical terms.",
            "If you want in statistics, there's that.",
            "Method for density estimation, which is called the powers and Windows methods.",
            "What is the passing Windows method do?",
            "Given some kernel function, which is a similarity function like this with special properties with the property that it's non negative and that it integrates to one.",
            "So think of it as a normalized Gaussian.",
            "So given such density function or window function we place one such function on each training point XI.",
            "And then this thing would be a positive Windows estimate of the density that has generated the positive class.",
            "Likewise, this would be a passing Windows estimate of the density that has generated the negative plus, and roughly speaking, the classifier would just as well it's.",
            "It's more likely that the point comes from the negative or from the positive class.",
            "But but in this case we arrive."
        ],
        [
            "That is, terminal purely geometric POV."
        ],
        [
            "And we will use this geometric point of view all the time now.",
            "So since we're running a little bit late, normally I take a break at this point and people workout themselves how to."
        ],
        [
            "Derive this thing, but I recommend that if you haven't seen this kind of classifier before, try to do it tonight or do it in the break."
        ],
        [
            "And do it in a slightly different way, because there are different ways of doing it.",
            "As I mentioned before.",
            "I mean, we've now done it using this angle between this vector on this vector, but you might as well just as well directly compute the distance between this point in this point in the distance between this point.",
            "Sorry, between this point at this point and just check which of the two distances up here, so you just write down the one distance, subtract the other distance, and then check whether what you get is positive or negative."
        ],
        [
            "And it turns out, if you if you do that.",
            "And if you get it right, you should exactly arrive at the same same decision function.",
            "OK, so."
        ],
        [
            "It's a little exercise to try at some point.",
            "I could actually.",
            "Try to give you a little bit more of this.",
            "By the way, I should have mentioned this morning when I mentioned the sponsors of this meeting.",
            "I should have mentioned also the math works because it turned out they sponsored the practical sessions by giving us MATLAB licenses for free for this.",
            "So thank you, Mathworks.",
            "So.",
            "Let's take a look.",
            "OK, so let me start.",
            "I have some parameters here.",
            "I just I think only the top one is active, so I'm using a Gaussian similarity function.",
            "I'll talk about that more later, but think of it as a Gaussian function.",
            "It's a Gaussian kernel, and here this is the width of the kernel of the standard deviation, so I have now a fairly wide.",
            "Hello.",
            "And.",
            "I have two sets of points.",
            "And I train this classifier.",
            "It turns out if the width of this kernel is very wide and effectively I'm working directly in the input space.",
            "So in this space that you're seeing here is a 2 dimensional space.",
            "In this case, you see the decision boundary is pretty much a straight line.",
            "And the decision boundary just sort of if you think of the two class means, it's sort of more or less orthogonal to the vector connecting the two class means.",
            "And if I make the problem in a little bit more complicated.",
            "Then this classifier model might start to get problems.",
            "Let's let's make it more difficult.",
            "So now this is a problem that cannot be solved with a straight line anymore and you can see indeed this decision function that I've shown you before doesn't correctly classify all these points.",
            "But what happens if we do it, but I make it a little bit more nonlinear.",
            "By making the width of these Gaussians smaller.",
            "This is not quite enough yet, but let's make it even smaller.",
            "And I get this kind of solution now.",
            "It's important to remember in all these cases, the solution is a hyperplane in some feature space.",
            "But in the input space it's a nonlinear function as you can certainly see here so.",
            "And it."
        ],
        [
            "Look at it again, so it's in this feature space.",
            "It's just a hyperplane.",
            "That's how we constructed it."
        ],
        [
            "But in the input space as a function of X rather than as a function of Pi of X as a function of X, it's a nonlinear function.",
            "Just because this similarity measure K is nonlinear, Gaussian is depends nonlinearly on its inputs.",
            "OK."
        ],
        [
            "Cheap.",
            "So Alex already mentioned a little bit about statistical learning.",
            "I won't tell you much about it because next week we have a course going into details and statistical learning theory just to confirm 90 intuition.",
            "Is the statistical learning theories may be the most well developed theory of machine learning.",
            "It was started by Vatican shoveling keys to Russian statisticians in the 60s.",
            "While they were doing their PhD thesis and the model is that we observed data generated by some unknown stochastic regularity and learning, roughly speaking, the extraction of this regularity from the data and the interesting thing of Optic Germany's theory is that the analysis of this learning problem.",
            "At least two notions of what they call capacity.",
            "Of the function class, there's a learning machine can implement, so if you have a learning machine which is able just by the design to come up with very complex nonlinear functions, then in the sense of African English this learning machine will be set to have a higher capacity.",
            "You know it has a high capacity to explain data.",
            "And it turns out, if you use a learning machine with very high capacity, then you will also need more learning data in order to be able to generalize well.",
            "Because if you have high capacity and you can explain many possible datasets.",
            "Then whenever you are given a concrete data set and you can explain that set, maybe it doesn't tell you much because capacity is so high you could have explained anything that I had given to you, even if it were complete random garbage.",
            "So that's a little bit the intuition behind, but you will hear this probably over and over again.",
            "So now support vector machines, which is what I'm trying to prepare in my lectures.",
            "Alex will talk about them.",
            "They use a particular type of function class.",
            "And we've already seen this function class in this season.",
            "Simple parsing windows based classifier.",
            "What we've seen a similar function class.",
            "These are functions that correspond to hyperplane separations or simple linear functions in a feature space, which corresponds to a Colonel in the support vector machine case.",
            "It will be a little bit more sophisticated because we will not use decision functions that are or.",
            "We will not use hyper planes that are orthogonal to vector connecting the two class means, but we will use different kinds of hyperplanes that lead to so-called large margins of separation between the two."
        ],
        [
            "Anyway, I won't talk more about this now because Alex will come back today.",
            "So OK, let's revisit this idea of feature spaces and kernels a little bit.",
            "So we said what we will do is given some data we pre process them with some mapping fire.",
            "Take them into a dot product space and then solve our learning problem based on 5X.",
            "So for instance, if our learning problem consists of mapping of learning the mapping from X to a variable Y.",
            "Now we just learned the problem.",
            "The mapping from \u03c0 of X to Y.",
            "Now usually it turns out for the kind of terms that we will consider the dimensionality of this space.",
            "H is very high.",
            "And compared to the dimensionality of X, if X is a vector space at all, this dimensionality can be a lot higher.",
            "And in classical statistics there's something called the curse of dimensionality and I remember at first conferences where we were talking about the public machines, classical statisticians were really worried about this and they thought this doesn't make sense at all.",
            "But in the case of or in the sense of statistical learning theory.",
            "Dimensionality is not really the crucial thing.",
            "This capacity so just to wet your appetite a bit more."
        ],
        [
            "Learning theory, but I won't talk about it.",
            "So here's an example of a kernel and a mapping into a future space for a simple type problem.",
            "So suppose our type problem.",
            "It is this kind of problem.",
            "So we have two classes of data.",
            "The blue crosses to the red circles.",
            "And we are only given the data, not the decision boundary.",
            "So suppose the true decision boundary is this ellipse here, so we don't know it.",
            "We only know the data.",
            "And we could try to take some sufficient sufficiently nonlinear class of functions and somehow learn this ellipse.",
            "From these data.",
            "Or, in the case of kernel methods, what we would do is we would first blow up this problem into a higher dimensional space and then try to solve linear problem in that space.",
            "And here's an example of what mapping we would choose.",
            "In this case, we could choose the mapping that compute given a point XX1X2.",
            "It computes all possible products of order two in these points.",
            "So what's the product of order two?",
            "It means I take two of those and pick that product.",
            "So there are three such products X1 squared X, ONE X2 and X 2 ^2.",
            "And don't worry about this scaling factor now, so there are three such products we will just apply this mapping from R2 to R3 to all these points.",
            "And it turns out, if you remember what an ellipse equation looks like specially for this simple axis aligned ellipse.",
            "So once you write this equation in terms of X1 squared and X2 squared, it's actually an affine equation.",
            "So.",
            "Therefore, in this 3 dimensional space.",
            "Our decision boundary in this becomes a hyperplane, so it's a linear separation, and in this case it's even a hyperplane that's independent of the two instances question.",
            "Yeah, right?",
            "Yes.",
            "Anne.",
            "Well, I mean up, actually I'm only I mean these points will also have zed two coordinates, but I've not shown them becausw the interesting dimensions for the separation.",
            "I only said one ends at three, but so so the crucial part means that one is it free for this separation?",
            "So that's why I'm also grading out this set to."
        ],
        [
            "Taxes a little bit.",
            "So more generally, of course, we're not really interested in 2 dimensional data.",
            "We're interested in high dimensional or maybe even structured data, but let's first talk about high dimensional data.",
            "So suppose we have N dimensional data points and we would like to compute products."
        ],
        [
            "So we call these these axes of this space the features.",
            "So this is the feature space.",
            "These are.",
            "These are the feet."
        ],
        [
            "Suppose we are interested in features of higher order, not just the border 2, but of all the deep.",
            "So in this case the high dimensional space the feature space is dimensionality that grows like into the power of the software.",
            "In is the input dimensionality, the is the order of the features that we want to compute, so already if we had a simple images of 16 by 16 that are really low dimensional by today's standards, and we if we want to compute products of order five, we would end up in the space of dimensionality 10 to the power of 10.",
            "So."
        ],
        [
            "That's a problem, but there's a trick how to deal with it.",
            "And that's the main content of this first lecture and people call it the kernel trick.",
            "And let's first look at it in the case where the dimensionality and the order of the features is 2."
        ],
        [
            "So remember this mapping that I had before or the ellipse.",
            "This mapping here into a 3 dimensional space.",
            "I'm going to take two points now and map into that space, so let's take this point in this point, for instance.",
            "Map them both into that space and it turns out that I can then compute."
        ],
        [
            "The DOT product between these two points quite elegantly.",
            "So here we have two points.",
            "X&X prime just write out the mapping and then compute the Canonical dot product between these two points.",
            "If we do that, that's also a little too line exercise if you want.",
            "If you do that, you notice you get a complete binomial formula, which is the square of the dot product in the original.",
            "Original variables.",
            "So just just work it out sometime yourself.",
            "Take the dot product between the two original variables, take the square and you will notice you get the same as if you had first transformed these points non linearly.",
            "And then taking the dot product so we can do the nonlinear transformation either."
        ],
        [
            "On the points.",
            "So here we do that under transformation on the points or."
        ],
        [
            "Or if we're interested only in DOT products.",
            "Between these points, we might as well first take the product and then apply a certain nonlinear transformation to the value of the dot product.",
            "In this case, it's only raising it to the power of two, so that's a very simple one, and it turns out the same trick works more generally.",
            "Also, for in dimensional inputs and product already, and that's quite nice.",
            "So in this case, let's actually go backwards.",
            "So we'll start from the DOT product in the input space race to the power of the.",
            "So this is the dot product in the."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this would be maybe slightly less elementary, some of it, but but still fairly elementary and many of you will still know it, but nevertheless I hope some of you don't know it yet, and maybe they will.",
                    "label": 0
                },
                {
                    "sent": "Can't hear OK, so I'll try to put this in closer.",
                    "label": 0
                },
                {
                    "sent": "How is that is the same?",
                    "label": 0
                },
                {
                    "sent": "So I have to speak up.",
                    "label": 0
                },
                {
                    "sent": "OK so I will talk about learning with kernels.",
                    "label": 1
                },
                {
                    "sent": "It's a particular area of machine learning that started around 10 years ago or a little bit more and it's now part of the standard toolkit of machine learning.",
                    "label": 0
                },
                {
                    "sent": "So if you are working in this field you should really know about these kind of things because you can use them in various contexts.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me see how do I go forward here?",
                    "label": 0
                },
                {
                    "sent": "So overall I said this morning we are sharing this first course.",
                    "label": 0
                },
                {
                    "sent": "We have six hours of lectures.",
                    "label": 0
                },
                {
                    "sent": "The first one was just given by Alex.",
                    "label": 0
                },
                {
                    "sent": "I will now give 3 hours.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think I'll use the pointer on the screen because this may be better for the people next door.",
                    "label": 0
                },
                {
                    "sent": "So still allowed on him.",
                    "label": 0
                },
                {
                    "sent": "Can we turn up the volume somehow?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I could just speak like this maybe.",
                    "label": 0
                },
                {
                    "sent": "OK so I will give three lectures now.",
                    "label": 0
                },
                {
                    "sent": "The first one is about the concept of similarity and the formalization of this concept in terms of kernels.",
                    "label": 0
                },
                {
                    "sent": "And also I will talk a bit about feature spaces associated with kernels.",
                    "label": 1
                },
                {
                    "sent": "But then we'll have a short coffee break.",
                    "label": 0
                },
                {
                    "sent": "Afterwards.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about more mathematically about the notion of positive definite kernels.",
                    "label": 1
                },
                {
                    "sent": "The associated reproducing kernel Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "I'll show you how to, knowing it.",
                    "label": 0
                },
                {
                    "sent": "It doesn't cost much time.",
                    "label": 0
                },
                {
                    "sent": "And finally, I will talk you about one application of these feature spaces, which is already now.",
                    "label": 0
                },
                {
                    "sent": "This is touching some fairly recent research connected to the notion of kernel means, so maybe not everybody knows about this one yet.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow Alex will continue with support vector machines and structured estim.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "OK, let's start informally.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have inputs and outputs taken from some sets X&Y.",
                    "label": 0
                },
                {
                    "sent": "And then we're given a training set of pairs XIYI that are taken from the product of these two sets, and we assume there's some regularity.",
                    "label": 1
                },
                {
                    "sent": "Ex tells us something about why.",
                    "label": 0
                },
                {
                    "sent": "If we talk about learning, then there's this notion of generalization.",
                    "label": 0
                },
                {
                    "sent": "So we want to find rules from our observations which generalize well in the sense that if we're given a new point X that we haven't seen before.",
                    "label": 0
                },
                {
                    "sent": "We somehow should be able to find a suitable wire that fits together with this X well in a certain sense.",
                    "label": 0
                },
                {
                    "sent": "And in which sensory fit together well, one sense would be to say somehow this new pair XY should be similar to the ones that we have seen before.",
                    "label": 1
                },
                {
                    "sent": "But then, of course, immediately the question comes up.",
                    "label": 0
                },
                {
                    "sent": "How do we measure similarity and four outputs?",
                    "label": 0
                },
                {
                    "sent": "This is usually fairly straightforward, although this is also a topic of research, and it can be arbitrarily complex, but for simplicity, let's say for outputs this problem is trivial and we just use a loss function.",
                    "label": 0
                },
                {
                    "sent": "So suppose if we're interested in binary classification, outputs are just plus minus one, in which case we could use the so called 01 loss function is just a loss function that checks.",
                    "label": 0
                },
                {
                    "sent": "Are the two Y values the same or are they different?",
                    "label": 0
                },
                {
                    "sent": "And it assigns a loss of, let's say, plus one whenever the values are different and assigns a loss of zero whenever they are the same.",
                    "label": 0
                },
                {
                    "sent": "But for the inputs, it's less trivial problem to come up with the similarity function.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this leads to the notion of kernels.",
                    "label": 0
                },
                {
                    "sent": "So let's say we use a symmetric function that takes 3 inputs and assigns a real number.",
                    "label": 0
                },
                {
                    "sent": "As the similarity value.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if our input space where the Euclidean vector space out to the end, we could use just the Canonical dot product between these inputs.",
                    "label": 0
                },
                {
                    "sent": "Over this notation eyes denote the entry of the vector X.",
                    "label": 0
                },
                {
                    "sent": "And that could be used as a similarity measure.",
                    "label": 0
                },
                {
                    "sent": "In a way, it's the simplest similarity measure that we're considering machine learning.",
                    "label": 0
                },
                {
                    "sent": "But of course, if our SpaceX or if our set X at the beginning only call this exit set if it's not a dot product space, we cannot use the DOT product, and in this case, what can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, we could, for instance, assume that our similarity function K as at least a representation as a dot product in some space that has a dot product.",
                    "label": 1
                },
                {
                    "sent": "So by this I mean we could assume there exists a map.",
                    "label": 0
                },
                {
                    "sent": "That takes our inputs, Maps them into a dot product space.",
                    "label": 0
                },
                {
                    "sent": "And then I will similarity measure is equal to the dot product in that other space.",
                    "label": 1
                },
                {
                    "sent": "So in that case we can think of our inputs, so sometimes I will use the term patterns for the inputs.",
                    "label": 0
                },
                {
                    "sent": "In this case we could think of them.",
                    "label": 0
                },
                {
                    "sent": "As the points represented in that vector space.",
                    "label": 0
                },
                {
                    "sent": "And then we can carry out whatever symmetric algorithms we want to carry out in that vector space, and this vector space is sometimes called the feature space.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at an example of such and such an algorithm that we could carry out in this representation.",
                    "label": 0
                },
                {
                    "sent": "And this is maybe the simplest possible classification algorithm, and in fact, so when we first described this, it was just for the introduction of the book that we wrote about kernel machines, just as a tutorial introduction.",
                    "label": 0
                },
                {
                    "sent": "But recently we notice this is actually.",
                    "label": 0
                },
                {
                    "sent": "Much more useful and I'll come back to this in my third lecture in the context of kernel means.",
                    "label": 0
                },
                {
                    "sent": "So this classification algorithm, but it does is just given two sets of points.",
                    "label": 0
                },
                {
                    "sent": "The pluses here and the circles here we try to find a classifier that separates one from the other and that given the new point X which is down here for instance, given the new point assigns it to one of the two classes, how do we assign a new test point X to one of the classes where we could just compute the means of the positive class, here denoted C plus the mean of the negative Class C minor.",
                    "label": 0
                },
                {
                    "sent": "So I've also written them up here.",
                    "label": 0
                },
                {
                    "sent": "M Plus is the number of positive points.",
                    "label": 0
                },
                {
                    "sent": "These are all the positive points or something overall positive points.",
                    "label": 0
                },
                {
                    "sent": "By positive points I mean once with labels that are plus one, so this is the mean.",
                    "label": 0
                },
                {
                    "sent": "Here we have the same for the negative plus and given the test point X, we will just check whether we're closer to the negative or the positive mean.",
                    "label": 0
                },
                {
                    "sent": "So it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a trivial way of classifying these data points and.",
                    "label": 0
                },
                {
                    "sent": "One way to do this, the different ways you could just compute the distances directly.",
                    "label": 0
                },
                {
                    "sent": "But another way to do this would be.",
                    "label": 0
                },
                {
                    "sent": "I mean, we're working in the DOT product space, so we can compute distances another way in which we can use the DOT products directly rather than the distances would be to compute this Point C which is halfway in between the positive and the negative class means.",
                    "label": 0
                },
                {
                    "sent": "Then compute the vector connecting C to our test point.",
                    "label": 0
                },
                {
                    "sent": "So this is this vector X -- C and now we just have to check whether this vector what kind of angle does this vector in close with this other vector W which is now the vector connecting the class means?",
                    "label": 1
                },
                {
                    "sent": "So if this angle is smaller than 90 degrees, then this point, the test point X is on this side of that hyperplane that have been noted here.",
                    "label": 0
                },
                {
                    "sent": "And if the angle is larger than degrees, it would be on this side.",
                    "label": 0
                },
                {
                    "sent": "And obviously the set of all points that are closer to see minus than to see plus forms 1/2 space over here, which is separate from the other half spaces by these hyperplanes in the vector W connecting the class means is the normal vector that happened.",
                    "label": 0
                },
                {
                    "sent": "OK, so that would be one way to do it.",
                    "label": 0
                },
                {
                    "sent": "And if we write it down like this so we have to check whether this dot product is this angle is smaller or larger than 90 degrees and to check this we just have to look at the dot product because you remember the dot product is the cosine of the angle, so we just have to check whether this dot product is positive or negative.",
                    "label": 0
                },
                {
                    "sent": "So we can do that.",
                    "label": 0
                },
                {
                    "sent": "The product we write down all these things.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if we substitute all of this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have all these quantities up here, right?",
                    "label": 0
                },
                {
                    "sent": "We have CC minus.",
                    "label": 0
                },
                {
                    "sent": "We know how to compute this thing.",
                    "label": 0
                },
                {
                    "sent": "We know how to compute X -- C.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Leave.",
                    "label": 0
                },
                {
                    "sent": "Then we just substitute everything we get this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So this is our dot product.",
                    "label": 0
                },
                {
                    "sent": "Which is the cosine of the angle, so we have to look at it.",
                    "label": 0
                },
                {
                    "sent": "Arithmetic sign.",
                    "label": 0
                },
                {
                    "sent": "Is it positive or is it negative and now we use the fact that the DOT product between 2:00?",
                    "label": 0
                },
                {
                    "sent": "Representing that space.",
                    "label": 0
                },
                {
                    "sent": "Can be computed.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll go a bit further in this direction, can be computed by this similarity function, because we said before, we are assuming our similarity function has a representation as the dot product in this space.",
                    "label": 0
                },
                {
                    "sent": "So now we substitute the similarity function in here and in here back here.",
                    "label": 0
                },
                {
                    "sent": "I have a term plus V. We're not going to worry about this.",
                    "label": 0
                },
                {
                    "sent": "This is a term.",
                    "label": 0
                },
                {
                    "sent": "That you that just contains all the quantities that don't depend on the test point X.",
                    "label": 0
                },
                {
                    "sent": "So we consider this a constant.",
                    "label": 0
                },
                {
                    "sent": "So anyway, so this is our classification function and let's think about it and look at it for a minute.",
                    "label": 0
                },
                {
                    "sent": "What does this function do?",
                    "label": 0
                },
                {
                    "sent": "It just takes a test point X.",
                    "label": 0
                },
                {
                    "sent": "It computes the similarity between X and all positive points.",
                    "label": 0
                },
                {
                    "sent": "So this is that the sum of the positive points.",
                    "label": 0
                },
                {
                    "sent": "Takes the average such similarity.",
                    "label": 0
                },
                {
                    "sent": "It does the same for all negative points.",
                    "label": 0
                },
                {
                    "sent": "Again, the average and here we have some constant.",
                    "label": 0
                },
                {
                    "sent": "And this can actually be also interpreted in statistical terms.",
                    "label": 0
                },
                {
                    "sent": "If you want in statistics, there's that.",
                    "label": 0
                },
                {
                    "sent": "Method for density estimation, which is called the powers and Windows methods.",
                    "label": 0
                },
                {
                    "sent": "What is the passing Windows method do?",
                    "label": 0
                },
                {
                    "sent": "Given some kernel function, which is a similarity function like this with special properties with the property that it's non negative and that it integrates to one.",
                    "label": 0
                },
                {
                    "sent": "So think of it as a normalized Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So given such density function or window function we place one such function on each training point XI.",
                    "label": 0
                },
                {
                    "sent": "And then this thing would be a positive Windows estimate of the density that has generated the positive class.",
                    "label": 0
                },
                {
                    "sent": "Likewise, this would be a passing Windows estimate of the density that has generated the negative plus, and roughly speaking, the classifier would just as well it's.",
                    "label": 0
                },
                {
                    "sent": "It's more likely that the point comes from the negative or from the positive class.",
                    "label": 0
                },
                {
                    "sent": "But but in this case we arrive.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is, terminal purely geometric POV.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we will use this geometric point of view all the time now.",
                    "label": 0
                },
                {
                    "sent": "So since we're running a little bit late, normally I take a break at this point and people workout themselves how to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Derive this thing, but I recommend that if you haven't seen this kind of classifier before, try to do it tonight or do it in the break.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And do it in a slightly different way, because there are different ways of doing it.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "I mean, we've now done it using this angle between this vector on this vector, but you might as well just as well directly compute the distance between this point in this point in the distance between this point.",
                    "label": 0
                },
                {
                    "sent": "Sorry, between this point at this point and just check which of the two distances up here, so you just write down the one distance, subtract the other distance, and then check whether what you get is positive or negative.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turns out, if you if you do that.",
                    "label": 0
                },
                {
                    "sent": "And if you get it right, you should exactly arrive at the same same decision function.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a little exercise to try at some point.",
                    "label": 0
                },
                {
                    "sent": "I could actually.",
                    "label": 0
                },
                {
                    "sent": "Try to give you a little bit more of this.",
                    "label": 0
                },
                {
                    "sent": "By the way, I should have mentioned this morning when I mentioned the sponsors of this meeting.",
                    "label": 0
                },
                {
                    "sent": "I should have mentioned also the math works because it turned out they sponsored the practical sessions by giving us MATLAB licenses for free for this.",
                    "label": 0
                },
                {
                    "sent": "So thank you, Mathworks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's take a look.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me start.",
                    "label": 0
                },
                {
                    "sent": "I have some parameters here.",
                    "label": 0
                },
                {
                    "sent": "I just I think only the top one is active, so I'm using a Gaussian similarity function.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about that more later, but think of it as a Gaussian function.",
                    "label": 0
                },
                {
                    "sent": "It's a Gaussian kernel, and here this is the width of the kernel of the standard deviation, so I have now a fairly wide.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I have two sets of points.",
                    "label": 0
                },
                {
                    "sent": "And I train this classifier.",
                    "label": 0
                },
                {
                    "sent": "It turns out if the width of this kernel is very wide and effectively I'm working directly in the input space.",
                    "label": 0
                },
                {
                    "sent": "So in this space that you're seeing here is a 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "In this case, you see the decision boundary is pretty much a straight line.",
                    "label": 0
                },
                {
                    "sent": "And the decision boundary just sort of if you think of the two class means, it's sort of more or less orthogonal to the vector connecting the two class means.",
                    "label": 0
                },
                {
                    "sent": "And if I make the problem in a little bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "Then this classifier model might start to get problems.",
                    "label": 0
                },
                {
                    "sent": "Let's let's make it more difficult.",
                    "label": 0
                },
                {
                    "sent": "So now this is a problem that cannot be solved with a straight line anymore and you can see indeed this decision function that I've shown you before doesn't correctly classify all these points.",
                    "label": 0
                },
                {
                    "sent": "But what happens if we do it, but I make it a little bit more nonlinear.",
                    "label": 0
                },
                {
                    "sent": "By making the width of these Gaussians smaller.",
                    "label": 0
                },
                {
                    "sent": "This is not quite enough yet, but let's make it even smaller.",
                    "label": 0
                },
                {
                    "sent": "And I get this kind of solution now.",
                    "label": 0
                },
                {
                    "sent": "It's important to remember in all these cases, the solution is a hyperplane in some feature space.",
                    "label": 0
                },
                {
                    "sent": "But in the input space it's a nonlinear function as you can certainly see here so.",
                    "label": 0
                },
                {
                    "sent": "And it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at it again, so it's in this feature space.",
                    "label": 0
                },
                {
                    "sent": "It's just a hyperplane.",
                    "label": 0
                },
                {
                    "sent": "That's how we constructed it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in the input space as a function of X rather than as a function of Pi of X as a function of X, it's a nonlinear function.",
                    "label": 0
                },
                {
                    "sent": "Just because this similarity measure K is nonlinear, Gaussian is depends nonlinearly on its inputs.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cheap.",
                    "label": 0
                },
                {
                    "sent": "So Alex already mentioned a little bit about statistical learning.",
                    "label": 0
                },
                {
                    "sent": "I won't tell you much about it because next week we have a course going into details and statistical learning theory just to confirm 90 intuition.",
                    "label": 0
                },
                {
                    "sent": "Is the statistical learning theories may be the most well developed theory of machine learning.",
                    "label": 0
                },
                {
                    "sent": "It was started by Vatican shoveling keys to Russian statisticians in the 60s.",
                    "label": 0
                },
                {
                    "sent": "While they were doing their PhD thesis and the model is that we observed data generated by some unknown stochastic regularity and learning, roughly speaking, the extraction of this regularity from the data and the interesting thing of Optic Germany's theory is that the analysis of this learning problem.",
                    "label": 1
                },
                {
                    "sent": "At least two notions of what they call capacity.",
                    "label": 0
                },
                {
                    "sent": "Of the function class, there's a learning machine can implement, so if you have a learning machine which is able just by the design to come up with very complex nonlinear functions, then in the sense of African English this learning machine will be set to have a higher capacity.",
                    "label": 0
                },
                {
                    "sent": "You know it has a high capacity to explain data.",
                    "label": 0
                },
                {
                    "sent": "And it turns out, if you use a learning machine with very high capacity, then you will also need more learning data in order to be able to generalize well.",
                    "label": 0
                },
                {
                    "sent": "Because if you have high capacity and you can explain many possible datasets.",
                    "label": 0
                },
                {
                    "sent": "Then whenever you are given a concrete data set and you can explain that set, maybe it doesn't tell you much because capacity is so high you could have explained anything that I had given to you, even if it were complete random garbage.",
                    "label": 0
                },
                {
                    "sent": "So that's a little bit the intuition behind, but you will hear this probably over and over again.",
                    "label": 0
                },
                {
                    "sent": "So now support vector machines, which is what I'm trying to prepare in my lectures.",
                    "label": 0
                },
                {
                    "sent": "Alex will talk about them.",
                    "label": 1
                },
                {
                    "sent": "They use a particular type of function class.",
                    "label": 0
                },
                {
                    "sent": "And we've already seen this function class in this season.",
                    "label": 0
                },
                {
                    "sent": "Simple parsing windows based classifier.",
                    "label": 1
                },
                {
                    "sent": "What we've seen a similar function class.",
                    "label": 0
                },
                {
                    "sent": "These are functions that correspond to hyperplane separations or simple linear functions in a feature space, which corresponds to a Colonel in the support vector machine case.",
                    "label": 0
                },
                {
                    "sent": "It will be a little bit more sophisticated because we will not use decision functions that are or.",
                    "label": 0
                },
                {
                    "sent": "We will not use hyper planes that are orthogonal to vector connecting the two class means, but we will use different kinds of hyperplanes that lead to so-called large margins of separation between the two.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyway, I won't talk more about this now because Alex will come back today.",
                    "label": 0
                },
                {
                    "sent": "So OK, let's revisit this idea of feature spaces and kernels a little bit.",
                    "label": 0
                },
                {
                    "sent": "So we said what we will do is given some data we pre process them with some mapping fire.",
                    "label": 0
                },
                {
                    "sent": "Take them into a dot product space and then solve our learning problem based on 5X.",
                    "label": 1
                },
                {
                    "sent": "So for instance, if our learning problem consists of mapping of learning the mapping from X to a variable Y.",
                    "label": 0
                },
                {
                    "sent": "Now we just learned the problem.",
                    "label": 1
                },
                {
                    "sent": "The mapping from \u03c0 of X to Y.",
                    "label": 0
                },
                {
                    "sent": "Now usually it turns out for the kind of terms that we will consider the dimensionality of this space.",
                    "label": 0
                },
                {
                    "sent": "H is very high.",
                    "label": 0
                },
                {
                    "sent": "And compared to the dimensionality of X, if X is a vector space at all, this dimensionality can be a lot higher.",
                    "label": 0
                },
                {
                    "sent": "And in classical statistics there's something called the curse of dimensionality and I remember at first conferences where we were talking about the public machines, classical statisticians were really worried about this and they thought this doesn't make sense at all.",
                    "label": 0
                },
                {
                    "sent": "But in the case of or in the sense of statistical learning theory.",
                    "label": 0
                },
                {
                    "sent": "Dimensionality is not really the crucial thing.",
                    "label": 0
                },
                {
                    "sent": "This capacity so just to wet your appetite a bit more.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning theory, but I won't talk about it.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of a kernel and a mapping into a future space for a simple type problem.",
                    "label": 0
                },
                {
                    "sent": "So suppose our type problem.",
                    "label": 0
                },
                {
                    "sent": "It is this kind of problem.",
                    "label": 0
                },
                {
                    "sent": "So we have two classes of data.",
                    "label": 0
                },
                {
                    "sent": "The blue crosses to the red circles.",
                    "label": 0
                },
                {
                    "sent": "And we are only given the data, not the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "So suppose the true decision boundary is this ellipse here, so we don't know it.",
                    "label": 0
                },
                {
                    "sent": "We only know the data.",
                    "label": 0
                },
                {
                    "sent": "And we could try to take some sufficient sufficiently nonlinear class of functions and somehow learn this ellipse.",
                    "label": 0
                },
                {
                    "sent": "From these data.",
                    "label": 0
                },
                {
                    "sent": "Or, in the case of kernel methods, what we would do is we would first blow up this problem into a higher dimensional space and then try to solve linear problem in that space.",
                    "label": 0
                },
                {
                    "sent": "And here's an example of what mapping we would choose.",
                    "label": 0
                },
                {
                    "sent": "In this case, we could choose the mapping that compute given a point XX1X2.",
                    "label": 0
                },
                {
                    "sent": "It computes all possible products of order two in these points.",
                    "label": 0
                },
                {
                    "sent": "So what's the product of order two?",
                    "label": 0
                },
                {
                    "sent": "It means I take two of those and pick that product.",
                    "label": 0
                },
                {
                    "sent": "So there are three such products X1 squared X, ONE X2 and X 2 ^2.",
                    "label": 0
                },
                {
                    "sent": "And don't worry about this scaling factor now, so there are three such products we will just apply this mapping from R2 to R3 to all these points.",
                    "label": 0
                },
                {
                    "sent": "And it turns out, if you remember what an ellipse equation looks like specially for this simple axis aligned ellipse.",
                    "label": 0
                },
                {
                    "sent": "So once you write this equation in terms of X1 squared and X2 squared, it's actually an affine equation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Therefore, in this 3 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Our decision boundary in this becomes a hyperplane, so it's a linear separation, and in this case it's even a hyperplane that's independent of the two instances question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean up, actually I'm only I mean these points will also have zed two coordinates, but I've not shown them becausw the interesting dimensions for the separation.",
                    "label": 0
                },
                {
                    "sent": "I only said one ends at three, but so so the crucial part means that one is it free for this separation?",
                    "label": 0
                },
                {
                    "sent": "So that's why I'm also grading out this set to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Taxes a little bit.",
                    "label": 0
                },
                {
                    "sent": "So more generally, of course, we're not really interested in 2 dimensional data.",
                    "label": 0
                },
                {
                    "sent": "We're interested in high dimensional or maybe even structured data, but let's first talk about high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have N dimensional data points and we would like to compute products.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we call these these axes of this space the features.",
                    "label": 0
                },
                {
                    "sent": "So this is the feature space.",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 0
                },
                {
                    "sent": "These are the feet.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Suppose we are interested in features of higher order, not just the border 2, but of all the deep.",
                    "label": 1
                },
                {
                    "sent": "So in this case the high dimensional space the feature space is dimensionality that grows like into the power of the software.",
                    "label": 1
                },
                {
                    "sent": "In is the input dimensionality, the is the order of the features that we want to compute, so already if we had a simple images of 16 by 16 that are really low dimensional by today's standards, and we if we want to compute products of order five, we would end up in the space of dimensionality 10 to the power of 10.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's a problem, but there's a trick how to deal with it.",
                    "label": 0
                },
                {
                    "sent": "And that's the main content of this first lecture and people call it the kernel trick.",
                    "label": 1
                },
                {
                    "sent": "And let's first look at it in the case where the dimensionality and the order of the features is 2.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So remember this mapping that I had before or the ellipse.",
                    "label": 0
                },
                {
                    "sent": "This mapping here into a 3 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "I'm going to take two points now and map into that space, so let's take this point in this point, for instance.",
                    "label": 0
                },
                {
                    "sent": "Map them both into that space and it turns out that I can then compute.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The DOT product between these two points quite elegantly.",
                    "label": 0
                },
                {
                    "sent": "So here we have two points.",
                    "label": 0
                },
                {
                    "sent": "X&X prime just write out the mapping and then compute the Canonical dot product between these two points.",
                    "label": 0
                },
                {
                    "sent": "If we do that, that's also a little too line exercise if you want.",
                    "label": 0
                },
                {
                    "sent": "If you do that, you notice you get a complete binomial formula, which is the square of the dot product in the original.",
                    "label": 1
                },
                {
                    "sent": "Original variables.",
                    "label": 0
                },
                {
                    "sent": "So just just work it out sometime yourself.",
                    "label": 0
                },
                {
                    "sent": "Take the dot product between the two original variables, take the square and you will notice you get the same as if you had first transformed these points non linearly.",
                    "label": 0
                },
                {
                    "sent": "And then taking the dot product so we can do the nonlinear transformation either.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the points.",
                    "label": 0
                },
                {
                    "sent": "So here we do that under transformation on the points or.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or if we're interested only in DOT products.",
                    "label": 0
                },
                {
                    "sent": "Between these points, we might as well first take the product and then apply a certain nonlinear transformation to the value of the dot product.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's only raising it to the power of two, so that's a very simple one, and it turns out the same trick works more generally.",
                    "label": 0
                },
                {
                    "sent": "Also, for in dimensional inputs and product already, and that's quite nice.",
                    "label": 0
                },
                {
                    "sent": "So in this case, let's actually go backwards.",
                    "label": 0
                },
                {
                    "sent": "So we'll start from the DOT product in the input space race to the power of the.",
                    "label": 0
                },
                {
                    "sent": "So this is the dot product in the.",
                    "label": 1
                }
            ]
        }
    }
}