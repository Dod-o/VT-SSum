{
    "id": "edqqhozf7yscwraiwjejv72xhctqva7s",
    "title": "Random projection, margins, kernels, and feature-selection",
    "info": {
        "author": [
            "Avrim Blum, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "February 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/slsfs05_blum_rpmkf/",
    "segmentation": [
        [
            "Alright.",
            "So let me start by thanking the organizers I.",
            "For inviting me here, it's really been a lot of fun, so I've actually a common machine learning from sort of different.",
            "So direction and then then many of you here I've come from the algorithms and complexity theory side of things have come to machine learning from that, but it's been really.",
            "Great seeing all the all the talks and really a lot of fun.",
            "And let me just thank you all for waking up early.",
            "So I'm offering Blum and portions when I'm going to talk about our joint work with my student Nina Balcan and with Santosh from Paula at MIT.",
            "Portions have nothing to do with me at all.",
            "'cause part of this is a tutorial type of thing.",
            "And at the end I'll be talking more about some of our own work.",
            "OK, so so the title kind of looks like a list of keywords, so let me let me start random projection.",
            "So random projection is a simple technique that."
        ],
        [
            "From where I come from, it's been very useful in approximation algorithms.",
            "And what I want to do today is talk about how random projection can give some insight into various problems and topics and machine learning.",
            "So one of those is margins.",
            "So so why is having such a large margin a good thing?",
            "So that's something actually understand pretty well, but random projection can give a different perspective on this.",
            "Kernel functions, I think random projection gives some interesting point of view on kind of what kernels are doing for us in a classification setting and then.",
            "About how both of these are related to feature construction.",
            "OK, so I'll be talking about that, so that's to explain the title.",
            "So so."
        ],
        [
            "OK, So what do I mean by random projection?",
            "So the kind of thing where the mean is this.",
            "So given endpoints in some Euclidean space like RN project, down to a random K dimensional subspace just orthogonal projection to a random K dimensional subspace for some K much smaller than M. So what do I mean by random K dimensional subspace?",
            "So you can think of it as Picker say random unit vector like this V1.",
            "Pick a random vector V2 subject to being orthogonal to V1.",
            "Keep going up to K and then just orthogonally project them.",
            "That's the kind of thing I mean.",
            "OK, so something pretty.",
            "Simple and well, the kind of thing you get is that if this K is of medium size, sort of one over epsilon squared times log of the number of points, then this random projection will approximately preserve a lot of interesting quantities of the data, so you can actually get down the sums of medium dimensional space and preserve a lot of interesting quantities that you.",
            "Yeah, I mean and the number of points.",
            "The reason I say like RN is me given endpoints.",
            "They sort of define at most an end dimensional space between them among them.",
            "So yeah, ending the number of points.",
            "If K is small like one, so we just project to a random line.",
            "This can often still get something useful for us, surprisingly, and so we're going to see both aspects of both of these things here."
        ],
        [
            "Right, so let me just not that has anything to do with this talk will just say a little bit about how random projections been using approximation algorithms.",
            "So one way has been dimensionality reduction through the Johnson Lindenstrauss lemma.",
            "So there's this really nice kind of beautiful fact that I'll give us some intuition of the proof for later.",
            "So I called the Johnson Lindenstrauss lemma and this is kind of that first type.",
            "It says if you have end points in Euclidean space and project randomly to a space of dimension one over epsilon squared, log in with high probability, all relative distances between points are preserved up to one plus or minus epsilon.",
            "OK, so if you do a projection, everything shrinks OK, but they're all going to shrink it just about the same amount.",
            "And that's really nice.",
            "For instance, if you're interested in, say, doing approximate nearest neighbor, it means you can go down to this sort of medium dimensional space and you really don't have to worry if they were originally in a higher dimensional space from the nearest neighbor POV.",
            "You're kind of getting distances approximately right.",
            "Another place that's been used is so randomized rounding of semi definite program, so semi definite programming is a technique that's been often used for.",
            "Approximation algorithms things like like graph coloring.",
            "You have nodes and you want to give him collars semi definite.",
            "Programming won't give them colors, it will give them vectors instead of having every edge two endpoints, different color for every adds, the two vectors will have a high angle between them.",
            "That's not really what you wanted.",
            "You wanted colors, but you can do things like maybe pick a random hyperplane and use that to separate for Max Cut.",
            "That's something you do anyway.",
            "Right on time machine learning."
        ],
        [
            "So, so first of all, we talk about supervised learning instead of just a swap in standard supervised learning setting.",
            "We've got examples there points and some instant space.",
            "Yeah, and I'm going to be using.",
            "Like RN but actually when is going to be the number of points in this talk anyway?",
            "They're labeled, say positive or negative, so I'm going to be talking about binary labels.",
            "I'm going to assume they're drawn from some probability distribution, so depending on sort of where you went to Graduate School, you might prefer to think of that as a distribution over labeled examples or as a distribution over unlabeled examples and then labeled by some target function.",
            "See so that so therefore, in some picture like this, this target function and we've got some distribution or examples or positive here and they get up there and then the general setting we have given some labeled training data.",
            "We want our algorithm well and new data drawn from the same distribution.",
            "So the usual setting.",
            "Right?"
        ],
        [
            "So margins OK.",
            "So one fact that we know.",
            "That's that's been quite useful is that if data separable set by linear separator not just separable but separable by a large margin is a big gap between the positive and negative examples.",
            "Then that's a good thing, it's just it's a great thing if our data has that property, it means, among other things, that.",
            "No matter what the dimensionality, the space is that fundamentally we need a sample size depends only roughly one over the margin squared.",
            "OK, and technically, what do we mean by margin?",
            "I mean things example are separated by distance gamma where you scale the examples by there by their length.",
            "So I'm thinking of the separator is going through the origin.",
            "And so actually margin is kind of an angle sort of thing, but I think the distance or an angle kind of tells you how much wiggle room you have in your solution.",
            "If you took your solution and kind of wiggled it by roughly an angle gamma, it still would be an OK solution.",
            "So that's something that's been known.",
            "It's it's nice if our data has that problem.",
            "So how do we know it's nice actually?",
            "So a couple of ways.",
            "So one way to see one way to see why it's nice is that sort of a classic 4 year old ways that's been known that perceptron algorithm, for instance, does well in this case.",
            "The number of mistakes it makes.",
            "It makes only one over gamma squared.",
            "So.",
            "If your data has that property perceptron algorithm, you have a nice bound on how many mistakes it's going to make.",
            "That depends only on this margin, not the underlying dimensionality of space.",
            "So that's of a classic way of seeing why large margin is a good thing.",
            "Then there are fancier, sort of.",
            "Much harder to prove modern margin bounds, but say something a little bit different.",
            "They say in fact that it's not just that actually with high probability, every consistent large margin separator for your data will have a low true error under the distribution.",
            "OK, so that if you look at your data and it has a large margin, you find a large margin separator like support vector machines would do.",
            "You can talk about your confidence in its ability to generalize to new data based on the margin of your sample, so that's great.",
            "Just a little pain improve.",
            "And so I'm going to talk about two other ways to see this based on random projection."
        ],
        [
            "So so.",
            "First one, so first of all, I mentioned this Johnson Lindenstrauss lemma.",
            "If we project randomly given end points in Euclidean space, which I'm going to think it was being an RN project randomly to K dimensional space with medium dimensional space with high probability, all the pairwise distances are going to be preserved after we're all going to shrink by this.",
            "This amount.",
            "But there will be preserved and there's a couple of really nice clean proofs by 1 by indicon with one another by Dasgupta and Gupta, the 1st Paper Hat was a paper on approximate nearest neighbor.",
            "The 2nd paper has a more obvious title, something like a simple proof of the Johnson Lindenstrauss lemma like that.",
            "And I thought since I kind of sort of math person types of like proofs, I give you some intuition for how the proof codes."
        ],
        [
            "And so the intuition I like is the following.",
            "So why is kind of an amazing fact?",
            "Why should all the distances BBB preserved?",
            "So here here's some intuition for that.",
            "Think about.",
            "A random unit length vector in our end, so it's a point on the sphere in this high dimensional space, A random point on this sphere, so uniform random on the sphere.",
            "What does the X1 coordinate look like for a random point on the unit sphere?",
            "So what do we expect that to look like?",
            "Well, one easy thing to say is, well, you know the sum of the squares is equal to 1 'cause it's unit length vector.",
            "So the expected value is random.",
            "So expected value for the first coordinate square.",
            "There's gotta be 1 / M. But actually, it's pretty tightly concentrated OK if you the higher dimensional space you get the more tightly concentrated is and.",
            "And so in fact, X1 squared expected values one over and usually it will be at most some constant friends pretty tightly concentrated.",
            "And if they were independent, the different excise, then they're not.",
            "They're not independent effects.",
            "One is big.",
            "It means the others have to be small.",
            "'cause this whole thing is unit length.",
            "OK, so they're not independent, but if they were this intuition, right?",
            "'cause they're kind of going to be kind of independent.",
            "If they were.",
            "And Furthermore, if they were really bounded by C over and then you could apply your sort of favorite, like, say, Husting bound, and say, well, the sum of the squares of the first K entries.",
            "It's going to be pretty close to its expectation.",
            "The chance it's off by by some epsilon is going to most this very small amount.",
            "Alright, so just OK, that's fine and so one thing that means I haven't told you what this has to do with the lemma yet, but.",
            "Bear with me.",
            "So one thing that means just my random unit length vector.",
            "I just look at the first K coordinates units that look like the first K coordinates, random unit vector.",
            "What is the length squared of the first K coordinates?",
            "Let's go, it's going to be probably pretty close to what we expect the length squared to be, and actually you set K to be.",
            "One over epsilon squared log in.",
            "So this thing is getting small, so this whole thing is like you know one over an.",
            "Cube there whatever you like one over some liver polynomial.",
            "I feel like telling you in N so make this like 1 / N cube with very high probability 1 -- 1 / N ^3.",
            "But we have is that this length squared in the first K coordinates is pretty close.",
            "We expect it's actually the length itself is pretty close to what we expect with a little bit of fudge.",
            "Now, what does it have to do with this statement?",
            "I'm talking about random unit length vectors when it has to do with the statement is take our set of data points, look at any two points PIPJ look at the vector between them, Pi minus PJ taking that vector, projecting onto a random K dimensional space is the same thing as taking that vector, giving it a random rotation and just reading off the first K coordinates.",
            "I mean it's the same.",
            "Same thing, you just take this vector rotated randomly and project on the 1st K coordinates.",
            "So this via you can think of this.",
            "It's really this vector.",
            "Is just a random vector looking at the first K coordinates, and this statement implies then that that with high probability.",
            "It's.",
            "The length of this vector, like this vector, when we project to this random K dimensional space will be pretty close to what we expect it to be, and it's with such high probability that with high probability simultaneously this happens for every pair.",
            "For every pair PIPJ.",
            "So it's such a high probability that just by union bound with high, probably every by PJ, they all project too.",
            "P minus PJ projects to a vector of length what you expect times one plus or minus epsilon?",
            "So that's some intuition for for why this may expect this to be true."
        ],
        [
            "Technically, the proof actually turns out to be easier for a slightly different kind of projection, so in fact it's known this statement is known for lots of different sorts of projections.",
            "So when I say random projection, there's a lot of different mean versions of random projection where this holds an example of another one which actually turns be easier to prove is it, rather than picking a random K dimensional subspace and doing orthogonal projection, just pick K vectors IID from from.",
            "UN dimensional gaussian.",
            "Just a straight.",
            "You know, spherical, Gaussian independently and then just project with DOT product.",
            "So take a point P and just map it to p.1commapw2commallp.uk OK. And the reason that's actually nicer to prove anything about is that if you look at what happens to Pi minus PJ that just projects two will be nice.",
            "PJ 1, Pi minus PJ dot YouTube blah blah blah.",
            "And the nice thing about Gaussians is a Gaussian is Gaussian.",
            "Any direction you like, right?",
            "So in particular, in the Vijay direction you one looks like a 1 dimensional Gaussian and you 2 looks like an independent 1 dimensional Gaussian and so forth.",
            "So we really do get independence now, which is nice.",
            "So each component is IID from a 1 dimensional Gaussian scaled by the length of the IJ.",
            "Then you go dig in a book somewhere to find a bound for a sum of squares of iid Gaussian random variables, and then you're done.",
            "Anyway, so so we got it with high probability, all lengths or approximately preserved.",
            "In fact, it's hard to see with high probability all the angles.",
            "The angle between piplup that's gonna be approximately preserved.",
            "Alright, so so enough this so."
        ],
        [
            "What does this have to do with?",
            "Margins, so here's a natural connection.",
            "Suppose we have a set of points in RN, separable by a margin gamma.",
            "Um?",
            "Johnson Lindenstrauss he project to a random K dimensional space where K is this one over gamma, say or whatever game is squared log of a number of points with high probability.",
            "It will still be separable by margin GAM over 2 WHI is that will think of projecting the points plus projecting the target vector that you don't know.",
            "I mean, there's a target vector we don't know, but just imagine you're projecting it.",
            "All the angles are approximately preserved, so we assumed that they were separable by some margin gamma.",
            "There was this target vector where where the positives had, you know.",
            "Angle roughly at least plus gamma.",
            "From 90 degrees in the in the negatives were minus gamma, and so if all the angles are properly preserved, then you know if you change the angles by only gamma over too well this still be separable.",
            "In fact still be separable.",
            "You still got game over two more to go, so if all the angles between the points and the target vector I mean the normal to the hyperplane.",
            "So if all the angles change by just a little bit, is things still going to still be separable, we still have half the margin to go even if we want.",
            "So what does that say?",
            "That says, you know, I could have picked this project in before I even took the data.",
            "I mean just a random projection.",
            "I just look around and look at that OK dimensional space.",
            "I have this data.",
            "Someone tells me separable.",
            "I think a random K dimensional space.",
            "I project down.",
            "We need to look at the data to pick my space depicted before sampling the data.",
            "So really, you know this wasn't an end dimensional problem, it was really just a K dimensional problem.",
            "Alright, so you can say, well, that's one, so why is it good to have a large margin if you have a large margin?",
            "It's not really a high dimensional problem after all.",
            "It's really only a kind of one over gamma squared dimensional problem 'cause you can project to roughly that dimension space and still have separability.",
            "That's one way of saying why why large margins are good?",
            "'cause it really really.",
            "It's a lower dimensional problem than you thought.",
            "And so this this you can use to get those kind of those sample size bands.",
            "Anyway, so I like this.",
            "It's sort of I find a nice way of thinking about why a large margin is a good thing.",
            "It means it really.",
            "You thought your problem was hard to get high dimensional, but you just randomly project to this lower dimensional space and you still still have the properties you wanted."
        ],
        [
            "Here's another way.",
            "Random projection can help us think about why having a large margin is a good thing.",
            "Sorry if you don't like random projections, it's not the talk for you.",
            "OK. Let me propose a really stupid learning algorithm.",
            "It's a simple learning algorithm.",
            "Pick a random hyperplane.",
            "Right?",
            "OK, first of all, I'm assuming the data is separable by some large margin, and I'm trying to find that.",
            "How do I pick a random hyperplane, see how it does?",
            "So the algorithm has to have three steps, so OK, so pick a random hyperplane, see if it's any good.",
            "If it is, if it seems to do, have a reasonable error rate, a little better than guessing, I'll plug it into boosting.",
            "If not, Oh well, pick another random hyperplane.",
            "OK, here's an algorithm.",
            "Hyperplane is good.",
            "Plug into boosting, boosting what re adjust the weights of the points doesn't affect the margin.",
            "Let's keep going, well, blah blah.",
            "Claim is if the data was separable by a large margin, there's actually a reasonable chance a random hyperplane will be a weak learner.",
            "You updated the distribution of the points as well, or else, so you're just rewriting the points doesn't affect the margin.",
            "The effect is that effect.",
            "Yeah, yeah.",
            "Right?"
        ],
        [
            "So it should be.",
            "It should be OK.",
            "Right, I'll actually give you a proof because I like proofs.",
            "Pick a pick.",
            "An example.",
            "Just take an example in your data set.",
            "Let's say it's positive.",
            "For sake of argument called X, consider the two dimensional plane I like to dimensional planes, 'cause you can draw them defined by X and the target functions were imagining there's this separator like this.",
            "And here's the earth orthogonal vector to that separator.",
            "So 2 dimensional plane defined by the separator vector annex.",
            "So let's say this.",
            "This is the plane defined by them.",
            "Right now, now pick your random H. Pick a random H so so random by.",
            "So I pick a random vector H random random vector H random vector H. Projecting this 2 dimensional surface looks like a random vector in two dimensions.",
            "Now what is the probability that H gets this positive example wrong, namely HX less than or equal to 0, given that it has a positive dot product with W star?",
            "What's that?",
            "Well that we can think about pretty pretty nicely.",
            "This is a 2 dimensional space.",
            "Let me just draw the orthogonal's here.",
            "So the.",
            "Perpendicular lines, so the chance that a child X is less equal to 0 given HW stars.",
            "Wrinkles zero is just the chance.",
            "If I pick a random point in this, you know these of these possibilities that it lands in this region.",
            "Here this is the region of H is that would classify this point wrong.",
            "This is the region of H is that we classify's point right.",
            "The probability that H gets this guy wrong given as a positive W star is this divided by that.",
            "Oh, and we assume."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So let me start by thanking the organizers I.",
                    "label": 0
                },
                {
                    "sent": "For inviting me here, it's really been a lot of fun, so I've actually a common machine learning from sort of different.",
                    "label": 0
                },
                {
                    "sent": "So direction and then then many of you here I've come from the algorithms and complexity theory side of things have come to machine learning from that, but it's been really.",
                    "label": 0
                },
                {
                    "sent": "Great seeing all the all the talks and really a lot of fun.",
                    "label": 0
                },
                {
                    "sent": "And let me just thank you all for waking up early.",
                    "label": 0
                },
                {
                    "sent": "So I'm offering Blum and portions when I'm going to talk about our joint work with my student Nina Balcan and with Santosh from Paula at MIT.",
                    "label": 0
                },
                {
                    "sent": "Portions have nothing to do with me at all.",
                    "label": 1
                },
                {
                    "sent": "'cause part of this is a tutorial type of thing.",
                    "label": 0
                },
                {
                    "sent": "And at the end I'll be talking more about some of our own work.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the title kind of looks like a list of keywords, so let me let me start random projection.",
                    "label": 0
                },
                {
                    "sent": "So random projection is a simple technique that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From where I come from, it's been very useful in approximation algorithms.",
                    "label": 0
                },
                {
                    "sent": "And what I want to do today is talk about how random projection can give some insight into various problems and topics and machine learning.",
                    "label": 0
                },
                {
                    "sent": "So one of those is margins.",
                    "label": 0
                },
                {
                    "sent": "So so why is having such a large margin a good thing?",
                    "label": 0
                },
                {
                    "sent": "So that's something actually understand pretty well, but random projection can give a different perspective on this.",
                    "label": 0
                },
                {
                    "sent": "Kernel functions, I think random projection gives some interesting point of view on kind of what kernels are doing for us in a classification setting and then.",
                    "label": 0
                },
                {
                    "sent": "About how both of these are related to feature construction.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll be talking about that, so that's to explain the title.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what do I mean by random projection?",
                    "label": 0
                },
                {
                    "sent": "So the kind of thing where the mean is this.",
                    "label": 0
                },
                {
                    "sent": "So given endpoints in some Euclidean space like RN project, down to a random K dimensional subspace just orthogonal projection to a random K dimensional subspace for some K much smaller than M. So what do I mean by random K dimensional subspace?",
                    "label": 0
                },
                {
                    "sent": "So you can think of it as Picker say random unit vector like this V1.",
                    "label": 0
                },
                {
                    "sent": "Pick a random vector V2 subject to being orthogonal to V1.",
                    "label": 0
                },
                {
                    "sent": "Keep going up to K and then just orthogonally project them.",
                    "label": 0
                },
                {
                    "sent": "That's the kind of thing I mean.",
                    "label": 0
                },
                {
                    "sent": "OK, so something pretty.",
                    "label": 0
                },
                {
                    "sent": "Simple and well, the kind of thing you get is that if this K is of medium size, sort of one over epsilon squared times log of the number of points, then this random projection will approximately preserve a lot of interesting quantities of the data, so you can actually get down the sums of medium dimensional space and preserve a lot of interesting quantities that you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean and the number of points.",
                    "label": 0
                },
                {
                    "sent": "The reason I say like RN is me given endpoints.",
                    "label": 0
                },
                {
                    "sent": "They sort of define at most an end dimensional space between them among them.",
                    "label": 0
                },
                {
                    "sent": "So yeah, ending the number of points.",
                    "label": 0
                },
                {
                    "sent": "If K is small like one, so we just project to a random line.",
                    "label": 1
                },
                {
                    "sent": "This can often still get something useful for us, surprisingly, and so we're going to see both aspects of both of these things here.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so let me just not that has anything to do with this talk will just say a little bit about how random projections been using approximation algorithms.",
                    "label": 0
                },
                {
                    "sent": "So one way has been dimensionality reduction through the Johnson Lindenstrauss lemma.",
                    "label": 0
                },
                {
                    "sent": "So there's this really nice kind of beautiful fact that I'll give us some intuition of the proof for later.",
                    "label": 0
                },
                {
                    "sent": "So I called the Johnson Lindenstrauss lemma and this is kind of that first type.",
                    "label": 0
                },
                {
                    "sent": "It says if you have end points in Euclidean space and project randomly to a space of dimension one over epsilon squared, log in with high probability, all relative distances between points are preserved up to one plus or minus epsilon.",
                    "label": 1
                },
                {
                    "sent": "OK, so if you do a projection, everything shrinks OK, but they're all going to shrink it just about the same amount.",
                    "label": 0
                },
                {
                    "sent": "And that's really nice.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you're interested in, say, doing approximate nearest neighbor, it means you can go down to this sort of medium dimensional space and you really don't have to worry if they were originally in a higher dimensional space from the nearest neighbor POV.",
                    "label": 0
                },
                {
                    "sent": "You're kind of getting distances approximately right.",
                    "label": 0
                },
                {
                    "sent": "Another place that's been used is so randomized rounding of semi definite program, so semi definite programming is a technique that's been often used for.",
                    "label": 0
                },
                {
                    "sent": "Approximation algorithms things like like graph coloring.",
                    "label": 0
                },
                {
                    "sent": "You have nodes and you want to give him collars semi definite.",
                    "label": 0
                },
                {
                    "sent": "Programming won't give them colors, it will give them vectors instead of having every edge two endpoints, different color for every adds, the two vectors will have a high angle between them.",
                    "label": 0
                },
                {
                    "sent": "That's not really what you wanted.",
                    "label": 0
                },
                {
                    "sent": "You wanted colors, but you can do things like maybe pick a random hyperplane and use that to separate for Max Cut.",
                    "label": 0
                },
                {
                    "sent": "That's something you do anyway.",
                    "label": 0
                },
                {
                    "sent": "Right on time machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so first of all, we talk about supervised learning instead of just a swap in standard supervised learning setting.",
                    "label": 1
                },
                {
                    "sent": "We've got examples there points and some instant space.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and I'm going to be using.",
                    "label": 0
                },
                {
                    "sent": "Like RN but actually when is going to be the number of points in this talk anyway?",
                    "label": 0
                },
                {
                    "sent": "They're labeled, say positive or negative, so I'm going to be talking about binary labels.",
                    "label": 0
                },
                {
                    "sent": "I'm going to assume they're drawn from some probability distribution, so depending on sort of where you went to Graduate School, you might prefer to think of that as a distribution over labeled examples or as a distribution over unlabeled examples and then labeled by some target function.",
                    "label": 1
                },
                {
                    "sent": "See so that so therefore, in some picture like this, this target function and we've got some distribution or examples or positive here and they get up there and then the general setting we have given some labeled training data.",
                    "label": 0
                },
                {
                    "sent": "We want our algorithm well and new data drawn from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "So the usual setting.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So margins OK.",
                    "label": 0
                },
                {
                    "sent": "So one fact that we know.",
                    "label": 0
                },
                {
                    "sent": "That's that's been quite useful is that if data separable set by linear separator not just separable but separable by a large margin is a big gap between the positive and negative examples.",
                    "label": 1
                },
                {
                    "sent": "Then that's a good thing, it's just it's a great thing if our data has that property, it means, among other things, that.",
                    "label": 1
                },
                {
                    "sent": "No matter what the dimensionality, the space is that fundamentally we need a sample size depends only roughly one over the margin squared.",
                    "label": 0
                },
                {
                    "sent": "OK, and technically, what do we mean by margin?",
                    "label": 0
                },
                {
                    "sent": "I mean things example are separated by distance gamma where you scale the examples by there by their length.",
                    "label": 0
                },
                {
                    "sent": "So I'm thinking of the separator is going through the origin.",
                    "label": 0
                },
                {
                    "sent": "And so actually margin is kind of an angle sort of thing, but I think the distance or an angle kind of tells you how much wiggle room you have in your solution.",
                    "label": 0
                },
                {
                    "sent": "If you took your solution and kind of wiggled it by roughly an angle gamma, it still would be an OK solution.",
                    "label": 0
                },
                {
                    "sent": "So that's something that's been known.",
                    "label": 0
                },
                {
                    "sent": "It's it's nice if our data has that problem.",
                    "label": 0
                },
                {
                    "sent": "So how do we know it's nice actually?",
                    "label": 0
                },
                {
                    "sent": "So a couple of ways.",
                    "label": 0
                },
                {
                    "sent": "So one way to see one way to see why it's nice is that sort of a classic 4 year old ways that's been known that perceptron algorithm, for instance, does well in this case.",
                    "label": 0
                },
                {
                    "sent": "The number of mistakes it makes.",
                    "label": 1
                },
                {
                    "sent": "It makes only one over gamma squared.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If your data has that property perceptron algorithm, you have a nice bound on how many mistakes it's going to make.",
                    "label": 0
                },
                {
                    "sent": "That depends only on this margin, not the underlying dimensionality of space.",
                    "label": 0
                },
                {
                    "sent": "So that's of a classic way of seeing why large margin is a good thing.",
                    "label": 0
                },
                {
                    "sent": "Then there are fancier, sort of.",
                    "label": 1
                },
                {
                    "sent": "Much harder to prove modern margin bounds, but say something a little bit different.",
                    "label": 1
                },
                {
                    "sent": "They say in fact that it's not just that actually with high probability, every consistent large margin separator for your data will have a low true error under the distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so that if you look at your data and it has a large margin, you find a large margin separator like support vector machines would do.",
                    "label": 0
                },
                {
                    "sent": "You can talk about your confidence in its ability to generalize to new data based on the margin of your sample, so that's great.",
                    "label": 1
                },
                {
                    "sent": "Just a little pain improve.",
                    "label": 0
                },
                {
                    "sent": "And so I'm going to talk about two other ways to see this based on random projection.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "First one, so first of all, I mentioned this Johnson Lindenstrauss lemma.",
                    "label": 0
                },
                {
                    "sent": "If we project randomly given end points in Euclidean space, which I'm going to think it was being an RN project randomly to K dimensional space with medium dimensional space with high probability, all the pairwise distances are going to be preserved after we're all going to shrink by this.",
                    "label": 1
                },
                {
                    "sent": "This amount.",
                    "label": 0
                },
                {
                    "sent": "But there will be preserved and there's a couple of really nice clean proofs by 1 by indicon with one another by Dasgupta and Gupta, the 1st Paper Hat was a paper on approximate nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "The 2nd paper has a more obvious title, something like a simple proof of the Johnson Lindenstrauss lemma like that.",
                    "label": 0
                },
                {
                    "sent": "And I thought since I kind of sort of math person types of like proofs, I give you some intuition for how the proof codes.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the intuition I like is the following.",
                    "label": 0
                },
                {
                    "sent": "So why is kind of an amazing fact?",
                    "label": 0
                },
                {
                    "sent": "Why should all the distances BBB preserved?",
                    "label": 0
                },
                {
                    "sent": "So here here's some intuition for that.",
                    "label": 0
                },
                {
                    "sent": "Think about.",
                    "label": 0
                },
                {
                    "sent": "A random unit length vector in our end, so it's a point on the sphere in this high dimensional space, A random point on this sphere, so uniform random on the sphere.",
                    "label": 0
                },
                {
                    "sent": "What does the X1 coordinate look like for a random point on the unit sphere?",
                    "label": 1
                },
                {
                    "sent": "So what do we expect that to look like?",
                    "label": 0
                },
                {
                    "sent": "Well, one easy thing to say is, well, you know the sum of the squares is equal to 1 'cause it's unit length vector.",
                    "label": 0
                },
                {
                    "sent": "So the expected value is random.",
                    "label": 0
                },
                {
                    "sent": "So expected value for the first coordinate square.",
                    "label": 0
                },
                {
                    "sent": "There's gotta be 1 / M. But actually, it's pretty tightly concentrated OK if you the higher dimensional space you get the more tightly concentrated is and.",
                    "label": 0
                },
                {
                    "sent": "And so in fact, X1 squared expected values one over and usually it will be at most some constant friends pretty tightly concentrated.",
                    "label": 0
                },
                {
                    "sent": "And if they were independent, the different excise, then they're not.",
                    "label": 0
                },
                {
                    "sent": "They're not independent effects.",
                    "label": 0
                },
                {
                    "sent": "One is big.",
                    "label": 0
                },
                {
                    "sent": "It means the others have to be small.",
                    "label": 0
                },
                {
                    "sent": "'cause this whole thing is unit length.",
                    "label": 0
                },
                {
                    "sent": "OK, so they're not independent, but if they were this intuition, right?",
                    "label": 0
                },
                {
                    "sent": "'cause they're kind of going to be kind of independent.",
                    "label": 0
                },
                {
                    "sent": "If they were.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, if they were really bounded by C over and then you could apply your sort of favorite, like, say, Husting bound, and say, well, the sum of the squares of the first K entries.",
                    "label": 0
                },
                {
                    "sent": "It's going to be pretty close to its expectation.",
                    "label": 0
                },
                {
                    "sent": "The chance it's off by by some epsilon is going to most this very small amount.",
                    "label": 0
                },
                {
                    "sent": "Alright, so just OK, that's fine and so one thing that means I haven't told you what this has to do with the lemma yet, but.",
                    "label": 0
                },
                {
                    "sent": "Bear with me.",
                    "label": 0
                },
                {
                    "sent": "So one thing that means just my random unit length vector.",
                    "label": 0
                },
                {
                    "sent": "I just look at the first K coordinates units that look like the first K coordinates, random unit vector.",
                    "label": 0
                },
                {
                    "sent": "What is the length squared of the first K coordinates?",
                    "label": 0
                },
                {
                    "sent": "Let's go, it's going to be probably pretty close to what we expect the length squared to be, and actually you set K to be.",
                    "label": 0
                },
                {
                    "sent": "One over epsilon squared log in.",
                    "label": 0
                },
                {
                    "sent": "So this thing is getting small, so this whole thing is like you know one over an.",
                    "label": 0
                },
                {
                    "sent": "Cube there whatever you like one over some liver polynomial.",
                    "label": 0
                },
                {
                    "sent": "I feel like telling you in N so make this like 1 / N cube with very high probability 1 -- 1 / N ^3.",
                    "label": 0
                },
                {
                    "sent": "But we have is that this length squared in the first K coordinates is pretty close.",
                    "label": 0
                },
                {
                    "sent": "We expect it's actually the length itself is pretty close to what we expect with a little bit of fudge.",
                    "label": 0
                },
                {
                    "sent": "Now, what does it have to do with this statement?",
                    "label": 0
                },
                {
                    "sent": "I'm talking about random unit length vectors when it has to do with the statement is take our set of data points, look at any two points PIPJ look at the vector between them, Pi minus PJ taking that vector, projecting onto a random K dimensional space is the same thing as taking that vector, giving it a random rotation and just reading off the first K coordinates.",
                    "label": 0
                },
                {
                    "sent": "I mean it's the same.",
                    "label": 0
                },
                {
                    "sent": "Same thing, you just take this vector rotated randomly and project on the 1st K coordinates.",
                    "label": 0
                },
                {
                    "sent": "So this via you can think of this.",
                    "label": 0
                },
                {
                    "sent": "It's really this vector.",
                    "label": 0
                },
                {
                    "sent": "Is just a random vector looking at the first K coordinates, and this statement implies then that that with high probability.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "The length of this vector, like this vector, when we project to this random K dimensional space will be pretty close to what we expect it to be, and it's with such high probability that with high probability simultaneously this happens for every pair.",
                    "label": 0
                },
                {
                    "sent": "For every pair PIPJ.",
                    "label": 0
                },
                {
                    "sent": "So it's such a high probability that just by union bound with high, probably every by PJ, they all project too.",
                    "label": 0
                },
                {
                    "sent": "P minus PJ projects to a vector of length what you expect times one plus or minus epsilon?",
                    "label": 0
                },
                {
                    "sent": "So that's some intuition for for why this may expect this to be true.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Technically, the proof actually turns out to be easier for a slightly different kind of projection, so in fact it's known this statement is known for lots of different sorts of projections.",
                    "label": 0
                },
                {
                    "sent": "So when I say random projection, there's a lot of different mean versions of random projection where this holds an example of another one which actually turns be easier to prove is it, rather than picking a random K dimensional subspace and doing orthogonal projection, just pick K vectors IID from from.",
                    "label": 0
                },
                {
                    "sent": "UN dimensional gaussian.",
                    "label": 0
                },
                {
                    "sent": "Just a straight.",
                    "label": 0
                },
                {
                    "sent": "You know, spherical, Gaussian independently and then just project with DOT product.",
                    "label": 0
                },
                {
                    "sent": "So take a point P and just map it to p.1commapw2commallp.uk OK. And the reason that's actually nicer to prove anything about is that if you look at what happens to Pi minus PJ that just projects two will be nice.",
                    "label": 0
                },
                {
                    "sent": "PJ 1, Pi minus PJ dot YouTube blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about Gaussians is a Gaussian is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Any direction you like, right?",
                    "label": 0
                },
                {
                    "sent": "So in particular, in the Vijay direction you one looks like a 1 dimensional Gaussian and you 2 looks like an independent 1 dimensional Gaussian and so forth.",
                    "label": 0
                },
                {
                    "sent": "So we really do get independence now, which is nice.",
                    "label": 0
                },
                {
                    "sent": "So each component is IID from a 1 dimensional Gaussian scaled by the length of the IJ.",
                    "label": 1
                },
                {
                    "sent": "Then you go dig in a book somewhere to find a bound for a sum of squares of iid Gaussian random variables, and then you're done.",
                    "label": 1
                },
                {
                    "sent": "Anyway, so so we got it with high probability, all lengths or approximately preserved.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's hard to see with high probability all the angles.",
                    "label": 0
                },
                {
                    "sent": "The angle between piplup that's gonna be approximately preserved.",
                    "label": 0
                },
                {
                    "sent": "Alright, so so enough this so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What does this have to do with?",
                    "label": 0
                },
                {
                    "sent": "Margins, so here's a natural connection.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have a set of points in RN, separable by a margin gamma.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Johnson Lindenstrauss he project to a random K dimensional space where K is this one over gamma, say or whatever game is squared log of a number of points with high probability.",
                    "label": 0
                },
                {
                    "sent": "It will still be separable by margin GAM over 2 WHI is that will think of projecting the points plus projecting the target vector that you don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a target vector we don't know, but just imagine you're projecting it.",
                    "label": 1
                },
                {
                    "sent": "All the angles are approximately preserved, so we assumed that they were separable by some margin gamma.",
                    "label": 0
                },
                {
                    "sent": "There was this target vector where where the positives had, you know.",
                    "label": 0
                },
                {
                    "sent": "Angle roughly at least plus gamma.",
                    "label": 0
                },
                {
                    "sent": "From 90 degrees in the in the negatives were minus gamma, and so if all the angles are properly preserved, then you know if you change the angles by only gamma over too well this still be separable.",
                    "label": 0
                },
                {
                    "sent": "In fact still be separable.",
                    "label": 0
                },
                {
                    "sent": "You still got game over two more to go, so if all the angles between the points and the target vector I mean the normal to the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So if all the angles change by just a little bit, is things still going to still be separable, we still have half the margin to go even if we want.",
                    "label": 0
                },
                {
                    "sent": "So what does that say?",
                    "label": 0
                },
                {
                    "sent": "That says, you know, I could have picked this project in before I even took the data.",
                    "label": 0
                },
                {
                    "sent": "I mean just a random projection.",
                    "label": 1
                },
                {
                    "sent": "I just look around and look at that OK dimensional space.",
                    "label": 0
                },
                {
                    "sent": "I have this data.",
                    "label": 0
                },
                {
                    "sent": "Someone tells me separable.",
                    "label": 0
                },
                {
                    "sent": "I think a random K dimensional space.",
                    "label": 0
                },
                {
                    "sent": "I project down.",
                    "label": 0
                },
                {
                    "sent": "We need to look at the data to pick my space depicted before sampling the data.",
                    "label": 0
                },
                {
                    "sent": "So really, you know this wasn't an end dimensional problem, it was really just a K dimensional problem.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you can say, well, that's one, so why is it good to have a large margin if you have a large margin?",
                    "label": 0
                },
                {
                    "sent": "It's not really a high dimensional problem after all.",
                    "label": 0
                },
                {
                    "sent": "It's really only a kind of one over gamma squared dimensional problem 'cause you can project to roughly that dimension space and still have separability.",
                    "label": 0
                },
                {
                    "sent": "That's one way of saying why why large margins are good?",
                    "label": 0
                },
                {
                    "sent": "'cause it really really.",
                    "label": 0
                },
                {
                    "sent": "It's a lower dimensional problem than you thought.",
                    "label": 0
                },
                {
                    "sent": "And so this this you can use to get those kind of those sample size bands.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so I like this.",
                    "label": 0
                },
                {
                    "sent": "It's sort of I find a nice way of thinking about why a large margin is a good thing.",
                    "label": 0
                },
                {
                    "sent": "It means it really.",
                    "label": 0
                },
                {
                    "sent": "You thought your problem was hard to get high dimensional, but you just randomly project to this lower dimensional space and you still still have the properties you wanted.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's another way.",
                    "label": 0
                },
                {
                    "sent": "Random projection can help us think about why having a large margin is a good thing.",
                    "label": 0
                },
                {
                    "sent": "Sorry if you don't like random projections, it's not the talk for you.",
                    "label": 0
                },
                {
                    "sent": "OK. Let me propose a really stupid learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's a simple learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Pick a random hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK, first of all, I'm assuming the data is separable by some large margin, and I'm trying to find that.",
                    "label": 0
                },
                {
                    "sent": "How do I pick a random hyperplane, see how it does?",
                    "label": 0
                },
                {
                    "sent": "So the algorithm has to have three steps, so OK, so pick a random hyperplane, see if it's any good.",
                    "label": 0
                },
                {
                    "sent": "If it is, if it seems to do, have a reasonable error rate, a little better than guessing, I'll plug it into boosting.",
                    "label": 0
                },
                {
                    "sent": "If not, Oh well, pick another random hyperplane.",
                    "label": 0
                },
                {
                    "sent": "OK, here's an algorithm.",
                    "label": 0
                },
                {
                    "sent": "Hyperplane is good.",
                    "label": 0
                },
                {
                    "sent": "Plug into boosting, boosting what re adjust the weights of the points doesn't affect the margin.",
                    "label": 0
                },
                {
                    "sent": "Let's keep going, well, blah blah.",
                    "label": 0
                },
                {
                    "sent": "Claim is if the data was separable by a large margin, there's actually a reasonable chance a random hyperplane will be a weak learner.",
                    "label": 1
                },
                {
                    "sent": "You updated the distribution of the points as well, or else, so you're just rewriting the points doesn't affect the margin.",
                    "label": 0
                },
                {
                    "sent": "The effect is that effect.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it should be.",
                    "label": 0
                },
                {
                    "sent": "It should be OK.",
                    "label": 0
                },
                {
                    "sent": "Right, I'll actually give you a proof because I like proofs.",
                    "label": 0
                },
                {
                    "sent": "Pick a pick.",
                    "label": 0
                },
                {
                    "sent": "An example.",
                    "label": 0
                },
                {
                    "sent": "Just take an example in your data set.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's positive.",
                    "label": 0
                },
                {
                    "sent": "For sake of argument called X, consider the two dimensional plane I like to dimensional planes, 'cause you can draw them defined by X and the target functions were imagining there's this separator like this.",
                    "label": 1
                },
                {
                    "sent": "And here's the earth orthogonal vector to that separator.",
                    "label": 0
                },
                {
                    "sent": "So 2 dimensional plane defined by the separator vector annex.",
                    "label": 0
                },
                {
                    "sent": "So let's say this.",
                    "label": 0
                },
                {
                    "sent": "This is the plane defined by them.",
                    "label": 0
                },
                {
                    "sent": "Right now, now pick your random H. Pick a random H so so random by.",
                    "label": 0
                },
                {
                    "sent": "So I pick a random vector H random random vector H random vector H. Projecting this 2 dimensional surface looks like a random vector in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "Now what is the probability that H gets this positive example wrong, namely HX less than or equal to 0, given that it has a positive dot product with W star?",
                    "label": 0
                },
                {
                    "sent": "What's that?",
                    "label": 0
                },
                {
                    "sent": "Well that we can think about pretty pretty nicely.",
                    "label": 0
                },
                {
                    "sent": "This is a 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Let me just draw the orthogonal's here.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Perpendicular lines, so the chance that a child X is less equal to 0 given HW stars.",
                    "label": 0
                },
                {
                    "sent": "Wrinkles zero is just the chance.",
                    "label": 0
                },
                {
                    "sent": "If I pick a random point in this, you know these of these possibilities that it lands in this region.",
                    "label": 0
                },
                {
                    "sent": "Here this is the region of H is that would classify this point wrong.",
                    "label": 0
                },
                {
                    "sent": "This is the region of H is that we classify's point right.",
                    "label": 0
                },
                {
                    "sent": "The probability that H gets this guy wrong given as a positive W star is this divided by that.",
                    "label": 0
                },
                {
                    "sent": "Oh, and we assume.",
                    "label": 0
                }
            ]
        }
    }
}