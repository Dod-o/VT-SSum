{
    "id": "c4l3yxl3zs672l4hcgtvlz2xzauevy33",
    "title": "Metric Anomaly Detection Via Asymmetric Risk Minimization",
    "info": {
        "author": [
            "Eitan Menahem, Department of Information Systems Engineering, Ben-Gurion University of the Negev"
        ],
        "published": "Oct. 17, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/simbad2011_menahem_minimization/",
    "segmentation": [
        [
            "OK hello, my name is Jeff Dunham and I'm a PhD student.",
            "Under the supervision of Volovich and your focus, and today I'm going to present a walk on anomaly detection.",
            "We are symmetric risk minimization, which is collaborative work with the other Kantorovich and Mr. Daniel."
        ],
        [
            "Little.",
            "So let's begin.",
            "How will you know when you think an alien?",
            "Well, this is a kind of philosophical question because you cannot model an alien which you haven't seen.",
            "OK. Let's assume that aliens are very rare case, and if you're going to see an alien you don't want to miss it.",
            "Kind of anomaly, which has might have dire consequences, and we want to detect that."
        ],
        [
            "So what we really want to do?",
            "Is well to detect anomalies and we have during the learning we can see only positive example and by meaning positive is women.",
            "Example, humans only and there is an inherent symmetry between classification errors.",
            "Be cause false alarms are usually model.",
            "Mustang usually must have far less disastrous than the other kind of error, which is the missed anomalies.",
            "And another.",
            "Another thing about the problem is that we say that we pay a fixed cost for each false alarm, while we only pay once for Mr.",
            "Normally.",
            "OK, for example if we have.",
            "Warehouse and we say we call to the fire Department and we say that it's on fire, which is of course can be full of alarm.",
            "Then we pay one across which equals one.",
            "But once we missed a fire which is a missed anomaly, we pay for example C which is considerably larger than one, and we pay it only once, because if they were House brand once again and again and again.",
            "It doesn't matter, right?"
        ],
        [
            "OK.",
            "So.",
            "And how we defined formally this problem?",
            "This is not so easy.",
            "It as it might seems becausw, unlike Buck, it's not clear what a good or a bad classifier is.",
            "Becausw for example.",
            "What is to prevent the trivial classifier which always classify positive?",
            "Because it seems only positive example during training right?",
            "So this is not so easy and then you have also the probability of mistake.",
            "How can you define it when you don't have any distribution over the negative examples?"
        ],
        [
            "So Drawbridge this problem.",
            "We need to begin with making some assumption of our model and a common model.",
            "Assumption is to use the Canadian space.",
            "See that in many works and well there is there is.",
            "There are some advantages to it.",
            "For example you have an inner product.",
            "You have also flexible kernels, which you can.",
            "Incorporate your knowledge and you have also efficient algorithms such as SVM.",
            "Also, you have some good generalization bounds and.",
            "Have some more, but I didn't write it here anyway.",
            "It's not all good since the Canadian structure.",
            "Sometimes it's too strong structure and many Canadian.",
            "For example.",
            "Pictures or audio files so.",
            "So the Canadian space not always fit to the problem.",
            "And of course you have to choose the kernel, which is also a problem 'cause you might choose a kernel which is good for your problem and then it will solve your problem very easily.",
            "But On the contrary, you might choose the wrong kernel and the same problem becomes solvable."
        ],
        [
            "So.",
            "We choose to use a much relaxed assumption.",
            "We use the metric space, which of course you know we can represent many natural things such as strings, images, audio, web pages, you name it.",
            "But the problem is that you don't have any vector representation.",
            "So.",
            "This is a little bit of a problem.",
            "You don't have the notion of the dot product for example.",
            "So what can you do?",
            "Should you invent kernels?",
            "But that will make you kind of distortion.",
            "You don't want to bring into your modeling, so you can use for example the nearest neighbor search.",
            "Which is kind of convenient, but we also know that the nearest neighbor classifier can have an infinite visit dimension, which is of course that.",
            "So what does the nearest neighbor search doesn't guarantee we'll see about that a little later, but let me start.",
            "Sorry."
        ],
        [
            "That little about the background."
        ],
        [
            "So.",
            "Just to remind the audience what is a metric space.",
            "Symmetric SpaceX and YX is a set of points where these distance function which is nonnegative, symmetric and triangle inequality holds.",
            "And we know that inner product entails a norm.",
            "And if you have known we have also a metric.",
            "But it's not true.",
            "And the other side, you could have metric, but you don't have known you could have known, but without an inner product."
        ],
        [
            "And This is why the metric data is much more.",
            "A weaker assumption.",
            "So classifying in a metric data is not new.",
            "You have for example phone looks broken, Busquet presented a work in 2004.",
            "They present a powerful framework and they of course consider they choose to consider only naturally prothesis, which are smoothly a maximal smoothly Lipschitz functions.",
            "They said there also that a given classifier, if you want to evaluate it, the problem reduces to finding a ellipse function which is consistent with the classifier you want to evaluate.",
            "And we also know that the Lipschitz extension problem which you have to use is a classical problem.",
            "OK, we have many kinds of solution there.",
            "But the most interesting part of this work was that the nearest neighbor, and in fact the one nearest neighbor is a special case of the Lipschitz classifier.",
            "So if you have electric classifier you can reduce it to one nearest neighbor and.",
            "This gives you a strong theoretical motivation to use it."
        ],
        [
            "However, what about the efficiency of the nearest neighbor search?",
            "We know that.",
            "In arbitrary metric space.",
            "It requires you get data of and.",
            "Which is not so efficient and the question is can we do any better and last year Gotlib and his friend showed that the answer was is yes."
        ],
        [
            "And they use the notion of the doubling dimension to show that we can do nearest neighbor search with sublinear time and.",
            "I will explain a little bit of the doubling dimension.",
            "So Bing, I'll fix an hour is a ball which contains.",
            "All the examples.",
            "Of a distance of eggs.",
            "And then.",
            "We said that the doubling constant Lambda is the minimum value, such as that every ball can be covered by Lambda balls, that of the half of the radius.",
            "It's kind of.",
            "Complex, but a little example here will show you the trick.",
            "Here you have on the right a space with examples with dots and we have a sphere of radius R and we want to know what is the minimal number of fill half the size that can cover all the points.",
            "And this number here is 7 is the Lambda which is the doubling cost constant.",
            "And we say that the doubling dimension is the log of Lambda.",
            "For example, in the Canadian space doubling dimension of RN is O of N."
        ],
        [
            "OK, so what's about?",
            "What about giving bound to generalization?",
            "Which Lipschitz classified can make in the metric space?",
            "Well, you have all types of many types of bounds, but here we want we used the fact shattering theory.",
            "And we show that for any Delta is the probability is as follows, and you have the notion here of the.",
            "Which is bound of course by L, which is the Lipschitz constant.",
            "And the diameter or fix which is the diameter of the metric space and powers by the doubling dimension.",
            "Can also see that the chance for for mistake is actually goes.",
            "It goes like log N / N."
        ],
        [
            "So before we continue with our model, let's talk about our assumptions."
        ],
        [
            "Little bit more so we have.",
            "A sphere of positive example.",
            "This is our sample.",
            "Of course it contains only positives."
        ],
        [
            "And then we know that.",
            "Anomalies or negative points are out there, but we don't see them during the training.",
            "Also."
        ],
        [
            "We know that there are additional positive points which we just don't see during the training 'cause our sample is finite, OK and we don't see any."
        ],
        [
            "We think of it.",
            "And also we have gamma which is the separation distance which is nonnegative.",
            "That separates the anomaly region from the positive region."
        ],
        [
            "So.",
            "Let's talk now about our model."
        ],
        [
            "We divide our walk into three sections.",
            "The first one we assume that we know the grammar, which is the separation distance, yes?",
            "We don't talk about margin.",
            "We talk about separation distance, which is analog, but.",
            "Yes.",
            "So our first assumption that we know gamma and then we assume that we know only a prior on grammar and Lastly, and I think the most important contribution in our work work is that we don't assume anything about gamma.",
            "And yet we want to make a cost sensitive classification."
        ],
        [
            "So a little more notions we know that we're talking about risk and not talking about classification errors, because we just can't, and the risk we say that it has two components.",
            "Of course you have the false alarm and you have the mist anomalies.",
            "And the false alarm is when your classifier Rept anomaly, which is of course wrong.",
            "And then you have the Mr.",
            "Normally when you have anomaly but you missed it.",
            "And the risk is the weighted sum, and we know that Mr normally cost a much more much more than the false alarm.",
            "And finally we have the notion of separation distance, which is defined as follows.",
            "It's very simple."
        ],
        [
            "Definition so let's begin the first case.",
            "We know gamma, and then we have a very simple classifier.",
            "Our classifier, which is F and gamma, classify X as normal when the distance between X to our sample is just below a gamma and it classifieds anomaly.",
            "If the distance is."
        ],
        [
            "Greater development, so visually you can see here it's it's yellow.",
            "My computer is green anyway, what you see our classifier is classified positively.",
            "All the positive example in the middle and also classify as positive the yellow region beyond the real region we have.",
            "If we always classify as anomalies.",
            "And then some positive points such as those were marked with red circle, is misclassified.",
            "Of course you can see that in this setup we cannot miss anomalies cause for missing anomalies we should take gamma star to be much greater than gamma, but we would not do that because we know exactly why."
        ],
        [
            "Discover.",
            "So.",
            "We want to bound the false alarm, which is the only kind of fellow we can make and.",
            "And.",
            "You can see the formula that calculate the false alarm.",
            "This is the integral, the first one, and you can obviously see that this number cannot be calculated because we need to know about all all the sphere.",
            "Sorry all the space and we only see.",
            "Only a sample.",
            "So what we can do is to use the feature during theory to bound from above the false alarm and this is what we do.",
            "And again, you can see that the false alarm grows.",
            "There is a low N / N and we have also the D which is a.",
            "A function of Delta, which is the diameter of our metric space divided by the known gamma and power off."
        ],
        [
            "Doubling dimension.",
            "However, the risk.",
            "Yes.",
            "Should.",
            "Go faster, OK, so the risk is the expectation of the false alarm and we bounded from above by A and gamma and B."
        ],
        [
            "OK, so the second case is when you have a prior on gamma and now we can do miss anomalies and you can see here how we define the risk."
        ],
        [
            "OK, so there is here as we find earlier we need to compute the expectation of the false alone and we use the same notion as before with a gamma N + B and we defined it as there is scan of, and we need to choose the right a gamma zero and we also know that.",
            "The risk of, zero goes to zero when."
        ],
        [
            "And goes to Infinity.",
            "Now let's go to the focus when we don't have anything about gamma.",
            "Now I should go very fast.",
            "Here we use heuristic.",
            "We don't have anything of gamma, so we use the nearest neighbor search.",
            "And we use the notion of isolation distance.",
            "Which is defined as follow a row is.",
            "Isolation distance of which we cannot compute because we need also information about all this space.",
            "So we are.",
            "We compute row head which we can compute cause we have because it's computed from our sample.",
            "Now the relation between row head and roll is that rojatt always smaller or equal to row and row head goes tomorrow also."
        ],
        [
            "Surely when you have enough examples.",
            "So what we use here is the notion of the.",
            "An epsilon net, which is a net of points of spheres of radius epsilon around every point of our sample.",
            "We have a sphere.",
            "Which is to be epsilon and the unification of the Bulls.",
            "We call it the epsilon envelope and our classifier use this envelope as follows.",
            "Every point new point inside the envelope is classified as positive, otherwise it's classified as anomaly and then you have the UN which is the answer must, which is all the math which does not include inside our epsilon net."
        ],
        [
            "So our mistake, if we have a mistake, is within the answer must so expectation of this mass is.",
            "You can see at the bound of it here.",
            "It was calculated by bending control, which this year.",
            "So what we do here we take the rohat plus two times the chosen epsilon which is always greater and greater than the real role."
        ],
        [
            "We don't know about and.",
            "Now about the missed anomaly component, which is the hardest part of this part because we don't have any knowledge about it.",
            "So we need to use a heuristic about it.",
            "And the Ristic we used is that the probability of making sorry the probability of pro being greater than gamma grows linearly with the with the team with Delta team.",
            "It's very far fetched, but it's the best we think we could do and when we do that we can actually calculate the missed anomalies.",
            "And then we can combine the two parts, the missed anomalies and the false alarm, and we get an estimation for the risk.",
            "As you can see it here and what we want from that equation is the epsilon that minimize this risk.",
            "And we've shown that we can calculate it efficiently using the doubling dimension."
        ],
        [
            "I will skip this."
        ],
        [
            "And finally, we can we have the empirical."
        ],
        [
            "Experiment.",
            "Sorry.",
            "OK, we have the experimental.",
            "We did.",
            "We took three kind of classifier or based on the the nearest neighbor Search, AG stands for our anomaly detection classifier.",
            "GD is the global density estimator and PGA is the peer group analysis.",
            "And we show that.",
            "Excuse me, we have three kinds of data sets to our toys problem till the first one the two dimension and nine dimension.",
            "And finally we have a real data.",
            "The database we call it the PG AARP, which is a real attack on our inner network enabling going on University.",
            "We of course capture normal normal behavior for a long time and then we trained our classifiers on that.",
            "And we want to see how many errors we were going to make.",
            "And the most important thing about this table is the final row column, which is the incurred cost.",
            "OK, so we showed that our classifier make the list.",
            "The least cost.",
            "It cost us the list, which really showed that our direction is is good."
        ],
        [
            "OK, thank you very much.",
            "Uh, so maybe the fundamental question relating to your water really example of seeing an alien so I've never seen one, so I couldn't tell which features make it possible to distinguish from you home.",
            "Which on the other hand means I will be node would be a good distance measure, and so and all that is the question.",
            "How realistic is it to to assume that there is a positive separating constant count?",
            "OK, an OK.",
            "The beginning of the answer is that.",
            "We have a database.",
            "We assume that we have one which is.",
            "Very it's not philosophical anymore because you have a finite amount of features.",
            "Now you don't know for certain that the future of the alien would be really separated.",
            "You don't know that, but.",
            "You want to be able to do that.",
            "If that was true.",
            "If it was separatable, because if you saw the gamma is really so it is.",
            "We assume that it's separatable.",
            "Without features, unless it is, I don't have a good question answer to your question.",
            "OK, thank you once more."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK hello, my name is Jeff Dunham and I'm a PhD student.",
                    "label": 0
                },
                {
                    "sent": "Under the supervision of Volovich and your focus, and today I'm going to present a walk on anomaly detection.",
                    "label": 1
                },
                {
                    "sent": "We are symmetric risk minimization, which is collaborative work with the other Kantorovich and Mr. Daniel.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Little.",
                    "label": 0
                },
                {
                    "sent": "So let's begin.",
                    "label": 0
                },
                {
                    "sent": "How will you know when you think an alien?",
                    "label": 1
                },
                {
                    "sent": "Well, this is a kind of philosophical question because you cannot model an alien which you haven't seen.",
                    "label": 0
                },
                {
                    "sent": "OK. Let's assume that aliens are very rare case, and if you're going to see an alien you don't want to miss it.",
                    "label": 0
                },
                {
                    "sent": "Kind of anomaly, which has might have dire consequences, and we want to detect that.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we really want to do?",
                    "label": 1
                },
                {
                    "sent": "Is well to detect anomalies and we have during the learning we can see only positive example and by meaning positive is women.",
                    "label": 1
                },
                {
                    "sent": "Example, humans only and there is an inherent symmetry between classification errors.",
                    "label": 1
                },
                {
                    "sent": "Be cause false alarms are usually model.",
                    "label": 0
                },
                {
                    "sent": "Mustang usually must have far less disastrous than the other kind of error, which is the missed anomalies.",
                    "label": 0
                },
                {
                    "sent": "And another.",
                    "label": 0
                },
                {
                    "sent": "Another thing about the problem is that we say that we pay a fixed cost for each false alarm, while we only pay once for Mr.",
                    "label": 1
                },
                {
                    "sent": "Normally.",
                    "label": 0
                },
                {
                    "sent": "OK, for example if we have.",
                    "label": 0
                },
                {
                    "sent": "Warehouse and we say we call to the fire Department and we say that it's on fire, which is of course can be full of alarm.",
                    "label": 0
                },
                {
                    "sent": "Then we pay one across which equals one.",
                    "label": 0
                },
                {
                    "sent": "But once we missed a fire which is a missed anomaly, we pay for example C which is considerably larger than one, and we pay it only once, because if they were House brand once again and again and again.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter, right?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And how we defined formally this problem?",
                    "label": 1
                },
                {
                    "sent": "This is not so easy.",
                    "label": 0
                },
                {
                    "sent": "It as it might seems becausw, unlike Buck, it's not clear what a good or a bad classifier is.",
                    "label": 1
                },
                {
                    "sent": "Becausw for example.",
                    "label": 1
                },
                {
                    "sent": "What is to prevent the trivial classifier which always classify positive?",
                    "label": 0
                },
                {
                    "sent": "Because it seems only positive example during training right?",
                    "label": 0
                },
                {
                    "sent": "So this is not so easy and then you have also the probability of mistake.",
                    "label": 1
                },
                {
                    "sent": "How can you define it when you don't have any distribution over the negative examples?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Drawbridge this problem.",
                    "label": 0
                },
                {
                    "sent": "We need to begin with making some assumption of our model and a common model.",
                    "label": 0
                },
                {
                    "sent": "Assumption is to use the Canadian space.",
                    "label": 0
                },
                {
                    "sent": "See that in many works and well there is there is.",
                    "label": 0
                },
                {
                    "sent": "There are some advantages to it.",
                    "label": 0
                },
                {
                    "sent": "For example you have an inner product.",
                    "label": 1
                },
                {
                    "sent": "You have also flexible kernels, which you can.",
                    "label": 0
                },
                {
                    "sent": "Incorporate your knowledge and you have also efficient algorithms such as SVM.",
                    "label": 0
                },
                {
                    "sent": "Also, you have some good generalization bounds and.",
                    "label": 1
                },
                {
                    "sent": "Have some more, but I didn't write it here anyway.",
                    "label": 0
                },
                {
                    "sent": "It's not all good since the Canadian structure.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's too strong structure and many Canadian.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Pictures or audio files so.",
                    "label": 0
                },
                {
                    "sent": "So the Canadian space not always fit to the problem.",
                    "label": 0
                },
                {
                    "sent": "And of course you have to choose the kernel, which is also a problem 'cause you might choose a kernel which is good for your problem and then it will solve your problem very easily.",
                    "label": 0
                },
                {
                    "sent": "But On the contrary, you might choose the wrong kernel and the same problem becomes solvable.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We choose to use a much relaxed assumption.",
                    "label": 0
                },
                {
                    "sent": "We use the metric space, which of course you know we can represent many natural things such as strings, images, audio, web pages, you name it.",
                    "label": 1
                },
                {
                    "sent": "But the problem is that you don't have any vector representation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit of a problem.",
                    "label": 0
                },
                {
                    "sent": "You don't have the notion of the dot product for example.",
                    "label": 0
                },
                {
                    "sent": "So what can you do?",
                    "label": 0
                },
                {
                    "sent": "Should you invent kernels?",
                    "label": 0
                },
                {
                    "sent": "But that will make you kind of distortion.",
                    "label": 0
                },
                {
                    "sent": "You don't want to bring into your modeling, so you can use for example the nearest neighbor search.",
                    "label": 0
                },
                {
                    "sent": "Which is kind of convenient, but we also know that the nearest neighbor classifier can have an infinite visit dimension, which is of course that.",
                    "label": 0
                },
                {
                    "sent": "So what does the nearest neighbor search doesn't guarantee we'll see about that a little later, but let me start.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That little about the background.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just to remind the audience what is a metric space.",
                    "label": 0
                },
                {
                    "sent": "Symmetric SpaceX and YX is a set of points where these distance function which is nonnegative, symmetric and triangle inequality holds.",
                    "label": 0
                },
                {
                    "sent": "And we know that inner product entails a norm.",
                    "label": 0
                },
                {
                    "sent": "And if you have known we have also a metric.",
                    "label": 0
                },
                {
                    "sent": "But it's not true.",
                    "label": 0
                },
                {
                    "sent": "And the other side, you could have metric, but you don't have known you could have known, but without an inner product.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And This is why the metric data is much more.",
                    "label": 0
                },
                {
                    "sent": "A weaker assumption.",
                    "label": 0
                },
                {
                    "sent": "So classifying in a metric data is not new.",
                    "label": 1
                },
                {
                    "sent": "You have for example phone looks broken, Busquet presented a work in 2004.",
                    "label": 1
                },
                {
                    "sent": "They present a powerful framework and they of course consider they choose to consider only naturally prothesis, which are smoothly a maximal smoothly Lipschitz functions.",
                    "label": 0
                },
                {
                    "sent": "They said there also that a given classifier, if you want to evaluate it, the problem reduces to finding a ellipse function which is consistent with the classifier you want to evaluate.",
                    "label": 0
                },
                {
                    "sent": "And we also know that the Lipschitz extension problem which you have to use is a classical problem.",
                    "label": 0
                },
                {
                    "sent": "OK, we have many kinds of solution there.",
                    "label": 0
                },
                {
                    "sent": "But the most interesting part of this work was that the nearest neighbor, and in fact the one nearest neighbor is a special case of the Lipschitz classifier.",
                    "label": 0
                },
                {
                    "sent": "So if you have electric classifier you can reduce it to one nearest neighbor and.",
                    "label": 0
                },
                {
                    "sent": "This gives you a strong theoretical motivation to use it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, what about the efficiency of the nearest neighbor search?",
                    "label": 0
                },
                {
                    "sent": "We know that.",
                    "label": 0
                },
                {
                    "sent": "In arbitrary metric space.",
                    "label": 0
                },
                {
                    "sent": "It requires you get data of and.",
                    "label": 0
                },
                {
                    "sent": "Which is not so efficient and the question is can we do any better and last year Gotlib and his friend showed that the answer was is yes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And they use the notion of the doubling dimension to show that we can do nearest neighbor search with sublinear time and.",
                    "label": 1
                },
                {
                    "sent": "I will explain a little bit of the doubling dimension.",
                    "label": 0
                },
                {
                    "sent": "So Bing, I'll fix an hour is a ball which contains.",
                    "label": 0
                },
                {
                    "sent": "All the examples.",
                    "label": 0
                },
                {
                    "sent": "Of a distance of eggs.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "We said that the doubling constant Lambda is the minimum value, such as that every ball can be covered by Lambda balls, that of the half of the radius.",
                    "label": 0
                },
                {
                    "sent": "It's kind of.",
                    "label": 0
                },
                {
                    "sent": "Complex, but a little example here will show you the trick.",
                    "label": 0
                },
                {
                    "sent": "Here you have on the right a space with examples with dots and we have a sphere of radius R and we want to know what is the minimal number of fill half the size that can cover all the points.",
                    "label": 0
                },
                {
                    "sent": "And this number here is 7 is the Lambda which is the doubling cost constant.",
                    "label": 0
                },
                {
                    "sent": "And we say that the doubling dimension is the log of Lambda.",
                    "label": 0
                },
                {
                    "sent": "For example, in the Canadian space doubling dimension of RN is O of N.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so what's about?",
                    "label": 0
                },
                {
                    "sent": "What about giving bound to generalization?",
                    "label": 0
                },
                {
                    "sent": "Which Lipschitz classified can make in the metric space?",
                    "label": 0
                },
                {
                    "sent": "Well, you have all types of many types of bounds, but here we want we used the fact shattering theory.",
                    "label": 0
                },
                {
                    "sent": "And we show that for any Delta is the probability is as follows, and you have the notion here of the.",
                    "label": 0
                },
                {
                    "sent": "Which is bound of course by L, which is the Lipschitz constant.",
                    "label": 0
                },
                {
                    "sent": "And the diameter or fix which is the diameter of the metric space and powers by the doubling dimension.",
                    "label": 0
                },
                {
                    "sent": "Can also see that the chance for for mistake is actually goes.",
                    "label": 0
                },
                {
                    "sent": "It goes like log N / N.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before we continue with our model, let's talk about our assumptions.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit more so we have.",
                    "label": 0
                },
                {
                    "sent": "A sphere of positive example.",
                    "label": 0
                },
                {
                    "sent": "This is our sample.",
                    "label": 0
                },
                {
                    "sent": "Of course it contains only positives.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we know that.",
                    "label": 0
                },
                {
                    "sent": "Anomalies or negative points are out there, but we don't see them during the training.",
                    "label": 1
                },
                {
                    "sent": "Also.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We know that there are additional positive points which we just don't see during the training 'cause our sample is finite, OK and we don't see any.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We think of it.",
                    "label": 0
                },
                {
                    "sent": "And also we have gamma which is the separation distance which is nonnegative.",
                    "label": 1
                },
                {
                    "sent": "That separates the anomaly region from the positive region.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's talk now about our model.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We divide our walk into three sections.",
                    "label": 0
                },
                {
                    "sent": "The first one we assume that we know the grammar, which is the separation distance, yes?",
                    "label": 0
                },
                {
                    "sent": "We don't talk about margin.",
                    "label": 0
                },
                {
                    "sent": "We talk about separation distance, which is analog, but.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So our first assumption that we know gamma and then we assume that we know only a prior on grammar and Lastly, and I think the most important contribution in our work work is that we don't assume anything about gamma.",
                    "label": 0
                },
                {
                    "sent": "And yet we want to make a cost sensitive classification.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a little more notions we know that we're talking about risk and not talking about classification errors, because we just can't, and the risk we say that it has two components.",
                    "label": 0
                },
                {
                    "sent": "Of course you have the false alarm and you have the mist anomalies.",
                    "label": 0
                },
                {
                    "sent": "And the false alarm is when your classifier Rept anomaly, which is of course wrong.",
                    "label": 0
                },
                {
                    "sent": "And then you have the Mr.",
                    "label": 0
                },
                {
                    "sent": "Normally when you have anomaly but you missed it.",
                    "label": 0
                },
                {
                    "sent": "And the risk is the weighted sum, and we know that Mr normally cost a much more much more than the false alarm.",
                    "label": 0
                },
                {
                    "sent": "And finally we have the notion of separation distance, which is defined as follows.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Definition so let's begin the first case.",
                    "label": 0
                },
                {
                    "sent": "We know gamma, and then we have a very simple classifier.",
                    "label": 0
                },
                {
                    "sent": "Our classifier, which is F and gamma, classify X as normal when the distance between X to our sample is just below a gamma and it classifieds anomaly.",
                    "label": 0
                },
                {
                    "sent": "If the distance is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Greater development, so visually you can see here it's it's yellow.",
                    "label": 0
                },
                {
                    "sent": "My computer is green anyway, what you see our classifier is classified positively.",
                    "label": 0
                },
                {
                    "sent": "All the positive example in the middle and also classify as positive the yellow region beyond the real region we have.",
                    "label": 0
                },
                {
                    "sent": "If we always classify as anomalies.",
                    "label": 0
                },
                {
                    "sent": "And then some positive points such as those were marked with red circle, is misclassified.",
                    "label": 0
                },
                {
                    "sent": "Of course you can see that in this setup we cannot miss anomalies cause for missing anomalies we should take gamma star to be much greater than gamma, but we would not do that because we know exactly why.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discover.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We want to bound the false alarm, which is the only kind of fellow we can make and.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You can see the formula that calculate the false alarm.",
                    "label": 0
                },
                {
                    "sent": "This is the integral, the first one, and you can obviously see that this number cannot be calculated because we need to know about all all the sphere.",
                    "label": 0
                },
                {
                    "sent": "Sorry all the space and we only see.",
                    "label": 0
                },
                {
                    "sent": "Only a sample.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is to use the feature during theory to bound from above the false alarm and this is what we do.",
                    "label": 0
                },
                {
                    "sent": "And again, you can see that the false alarm grows.",
                    "label": 0
                },
                {
                    "sent": "There is a low N / N and we have also the D which is a.",
                    "label": 0
                },
                {
                    "sent": "A function of Delta, which is the diameter of our metric space divided by the known gamma and power off.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doubling dimension.",
                    "label": 0
                },
                {
                    "sent": "However, the risk.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Should.",
                    "label": 0
                },
                {
                    "sent": "Go faster, OK, so the risk is the expectation of the false alarm and we bounded from above by A and gamma and B.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the second case is when you have a prior on gamma and now we can do miss anomalies and you can see here how we define the risk.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so there is here as we find earlier we need to compute the expectation of the false alone and we use the same notion as before with a gamma N + B and we defined it as there is scan of, and we need to choose the right a gamma zero and we also know that.",
                    "label": 0
                },
                {
                    "sent": "The risk of, zero goes to zero when.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Now let's go to the focus when we don't have anything about gamma.",
                    "label": 0
                },
                {
                    "sent": "Now I should go very fast.",
                    "label": 0
                },
                {
                    "sent": "Here we use heuristic.",
                    "label": 0
                },
                {
                    "sent": "We don't have anything of gamma, so we use the nearest neighbor search.",
                    "label": 0
                },
                {
                    "sent": "And we use the notion of isolation distance.",
                    "label": 0
                },
                {
                    "sent": "Which is defined as follow a row is.",
                    "label": 0
                },
                {
                    "sent": "Isolation distance of which we cannot compute because we need also information about all this space.",
                    "label": 0
                },
                {
                    "sent": "So we are.",
                    "label": 0
                },
                {
                    "sent": "We compute row head which we can compute cause we have because it's computed from our sample.",
                    "label": 0
                },
                {
                    "sent": "Now the relation between row head and roll is that rojatt always smaller or equal to row and row head goes tomorrow also.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Surely when you have enough examples.",
                    "label": 0
                },
                {
                    "sent": "So what we use here is the notion of the.",
                    "label": 0
                },
                {
                    "sent": "An epsilon net, which is a net of points of spheres of radius epsilon around every point of our sample.",
                    "label": 0
                },
                {
                    "sent": "We have a sphere.",
                    "label": 0
                },
                {
                    "sent": "Which is to be epsilon and the unification of the Bulls.",
                    "label": 0
                },
                {
                    "sent": "We call it the epsilon envelope and our classifier use this envelope as follows.",
                    "label": 0
                },
                {
                    "sent": "Every point new point inside the envelope is classified as positive, otherwise it's classified as anomaly and then you have the UN which is the answer must, which is all the math which does not include inside our epsilon net.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our mistake, if we have a mistake, is within the answer must so expectation of this mass is.",
                    "label": 0
                },
                {
                    "sent": "You can see at the bound of it here.",
                    "label": 0
                },
                {
                    "sent": "It was calculated by bending control, which this year.",
                    "label": 0
                },
                {
                    "sent": "So what we do here we take the rohat plus two times the chosen epsilon which is always greater and greater than the real role.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We don't know about and.",
                    "label": 0
                },
                {
                    "sent": "Now about the missed anomaly component, which is the hardest part of this part because we don't have any knowledge about it.",
                    "label": 1
                },
                {
                    "sent": "So we need to use a heuristic about it.",
                    "label": 0
                },
                {
                    "sent": "And the Ristic we used is that the probability of making sorry the probability of pro being greater than gamma grows linearly with the with the team with Delta team.",
                    "label": 0
                },
                {
                    "sent": "It's very far fetched, but it's the best we think we could do and when we do that we can actually calculate the missed anomalies.",
                    "label": 0
                },
                {
                    "sent": "And then we can combine the two parts, the missed anomalies and the false alarm, and we get an estimation for the risk.",
                    "label": 0
                },
                {
                    "sent": "As you can see it here and what we want from that equation is the epsilon that minimize this risk.",
                    "label": 0
                },
                {
                    "sent": "And we've shown that we can calculate it efficiently using the doubling dimension.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will skip this.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, we can we have the empirical.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiment.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, we have the experimental.",
                    "label": 0
                },
                {
                    "sent": "We did.",
                    "label": 0
                },
                {
                    "sent": "We took three kind of classifier or based on the the nearest neighbor Search, AG stands for our anomaly detection classifier.",
                    "label": 0
                },
                {
                    "sent": "GD is the global density estimator and PGA is the peer group analysis.",
                    "label": 0
                },
                {
                    "sent": "And we show that.",
                    "label": 0
                },
                {
                    "sent": "Excuse me, we have three kinds of data sets to our toys problem till the first one the two dimension and nine dimension.",
                    "label": 0
                },
                {
                    "sent": "And finally we have a real data.",
                    "label": 0
                },
                {
                    "sent": "The database we call it the PG AARP, which is a real attack on our inner network enabling going on University.",
                    "label": 0
                },
                {
                    "sent": "We of course capture normal normal behavior for a long time and then we trained our classifiers on that.",
                    "label": 0
                },
                {
                    "sent": "And we want to see how many errors we were going to make.",
                    "label": 0
                },
                {
                    "sent": "And the most important thing about this table is the final row column, which is the incurred cost.",
                    "label": 1
                },
                {
                    "sent": "OK, so we showed that our classifier make the list.",
                    "label": 0
                },
                {
                    "sent": "The least cost.",
                    "label": 0
                },
                {
                    "sent": "It cost us the list, which really showed that our direction is is good.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Uh, so maybe the fundamental question relating to your water really example of seeing an alien so I've never seen one, so I couldn't tell which features make it possible to distinguish from you home.",
                    "label": 0
                },
                {
                    "sent": "Which on the other hand means I will be node would be a good distance measure, and so and all that is the question.",
                    "label": 0
                },
                {
                    "sent": "How realistic is it to to assume that there is a positive separating constant count?",
                    "label": 0
                },
                {
                    "sent": "OK, an OK.",
                    "label": 0
                },
                {
                    "sent": "The beginning of the answer is that.",
                    "label": 0
                },
                {
                    "sent": "We have a database.",
                    "label": 0
                },
                {
                    "sent": "We assume that we have one which is.",
                    "label": 0
                },
                {
                    "sent": "Very it's not philosophical anymore because you have a finite amount of features.",
                    "label": 0
                },
                {
                    "sent": "Now you don't know for certain that the future of the alien would be really separated.",
                    "label": 0
                },
                {
                    "sent": "You don't know that, but.",
                    "label": 0
                },
                {
                    "sent": "You want to be able to do that.",
                    "label": 0
                },
                {
                    "sent": "If that was true.",
                    "label": 0
                },
                {
                    "sent": "If it was separatable, because if you saw the gamma is really so it is.",
                    "label": 0
                },
                {
                    "sent": "We assume that it's separatable.",
                    "label": 0
                },
                {
                    "sent": "Without features, unless it is, I don't have a good question answer to your question.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you once more.",
                    "label": 0
                }
            ]
        }
    }
}