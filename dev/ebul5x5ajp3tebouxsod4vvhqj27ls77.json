{
    "id": "ebul5x5ajp3tebouxsod4vvhqj27ls77",
    "title": "The Limit of One-Class SVM",
    "info": {
        "author": [
            "Regis Vert, University of Paris-Sud 11"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "October 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/mcslw04_vert_locs/",
    "segmentation": [
        [
            "For arranging the meeting and for inviting me here.",
            "So I'm going to speak about one class SVM.",
            "And so the work I'm going to present is a joint work with my brother Rafeedie to.",
            "And for those who are interested in having further details on that work.",
            "There is a full and detailed version available on my web page, so this is.",
            "Work that we've been doing, and.",
            "So I will present a result, then the associated proof.",
            "It's a consistency result and the interest of the proof will be.",
            "The presence of an exotic term.",
            "So usually when you want to prove a result about the convergence of of an error term of the risk.",
            "There is a split that in which arise variance term and a bias term.",
            "And here in the split there will be a.",
            "The third term, which we call the regularization error term.",
            "And to bomb this term, we will have to investigate the link between the arcade just normal of the Gaussian kernel and the L2 norm.",
            "So this will be the funny part of the proof.",
            "That's why I'm presenting the proof there is something funny in it.",
            "So, so this is about one class SVM, so."
        ],
        [
            "The first thing I have to do is maybe to present the algorithm.",
            "So one class SVM was introduced by 5 researchers.",
            "Social curve smaller.",
            "Show Taylor Platten Williamson.",
            "In a paper called estimating the support of a high dimensional distribution and in this paper the problem that was considered.",
            "Is the problem of estimating quantiles that is, minimum subsets of given probability?",
            "And so to tackle the problem of contract estimation, they proposed kernel algorithm caused called one class SVM.",
            "Which works as follows.",
            "Suppose you're given data sample X1, XN are leaving in some input space.",
            "Dicks and you choose a so called regularization parameter or Lambda.",
            "And you also choose a reproducing kernel Hilbert space H, so that is a functional space associated with the kernel.",
            "The algorithm amounts to minimizing this empirical quantity, so I think many of you are familiar with this kind of objective function.",
            "It's regularized impairment called risk functional.",
            "So here is what we call the regularization term, and here is the.",
            "The empirical error count.",
            "So if we look at at both terms separately.",
            "We see that if we want to minimize OK, maybe I have to say a word about this notation.",
            "This is a positive part, so.",
            "So I say.",
            "So the the subscript plus denotes this function.",
            "OK, so if you want to minimize this term, we have to pick to pick up from the arcade S function that should be at least that should take values at least one of the training data so that this is.",
            "As small as possible, but in the same time we have to take a function that is small with respect to the arcade is known and so minimizing the sum.",
            "I feel both terms typically leads to decision function.",
            "That takes values around equal to 1 on many of the training points, but not all the points because in the same time it has to be.",
            "Reasonably smaller than regular and with smaller infinite number so.",
            "Maybe I can draw a picture here so you get the points.",
            "These are the training points in one dimension.",
            "So.",
            "And so.",
            "So here is the value one.",
            "And.",
            "So the function will try to take values at least one on many training points.",
            "So essentially where the points are.",
            "Are well sample, so here it's once and in the same time we want a small function.",
            "So typically on the tail here will have vanish in function like that.",
            "OK, so this is a.",
            "So this is typically how the resulting function F hat look like.",
            "OK, so this step, which consists of minimizing efforts can be called the training step of the algorithm.",
            "This is the step where the optimization takes place.",
            "And once the function has been computed, it is associated with the threshold.",
            "So in the original paper that proposed the algorithm, the threshold is 1 and while in the paper it was also suggested to take a threshold of the Form 1 minus Delta, where Delta is a small positive quantity.",
            "And the meaning of sort of thresholding the function is.",
            "For instance, if we do anomaly detection with one class SVM.",
            "When given a new test point.",
            "We want to check the normality of this new test point.",
            "So what we do is we compute the value of F hat.",
            "On this new test point, and we compare this value with the threshold.",
            "So if this value falls in four is above 1 minus Delta, then the point is considered as a normal point, otherwise it is considered as somehow suspect.",
            "Maybe to be rejected as an outlier.",
            "So this is basically how the algorithm works.",
            "So is it clear how it works or OK?",
            "So in the paper it was proposed as a tool for estimating quantiles.",
            "That is this acceptance region.",
            "Is supposed to estimate a quantile.",
            "So here I want to make sure that we all agree on the definition of quantile so."
        ],
        [
            "So I have a.",
            "A little slide about quantile and quantized estimation.",
            "So the setup is the following one.",
            "We have a distribution probability distribution capital P on say RG.",
            "So I just precise now our deal with will be the largest input space that we consider throughout the talk.",
            "And.",
            "So we have these distribution and we define a quantile as follows.",
            "So an Alpha quantile, where Alpha is quantile level between zero and one is simply a subset measurable subset of Rd.",
            "With probability mass at least Alpha.",
            "OK, this is the constraint here, which has which is of minimum lebeck measure.",
            "So this is a simple definition.",
            "And the statistical problem that we consider is given a sample ID so independently and identically drawn from the unknown distribution P estimate crew of Alpha.",
            "So.",
            "Why not?",
            "Maybe I could precise what estimates mean so?",
            "So suppose this is.",
            "Queue of Alpha for a certain Alpha.",
            "And you use a quantile estimator that outputs.",
            "That outputs a subset of the input space, so this is your estimate of your Alpha.",
            "Man.",
            "And so, for instance, one way for assessing the.",
            "Formance of the estimator is to consider the.",
            "The standard way to measure the performance is to take the low back measure of.",
            "The symmetric difference between Q of Alpha and Q hat.",
            "OK, so this is for instance criterion.",
            "That enables you to.",
            "To evaluate the performance of Q hat of Alpha.",
            "So since I'm talking about quantile estimation, I would like to also address the very closely related problem of."
        ],
        [
            "Density level set estimation so.",
            "Then still sat estimation is just about estimating from a data sample.",
            "A density level set, so here the setup is.",
            "We assume that the distribution has density with respect to the lab measure.",
            "And given a desired density level, mu.",
            "We want to estimate the.",
            "The density level set.",
            "So this is a very related problem to quantile estimation.",
            "And."
        ],
        [
            "More precisely, in standard situation, if the density is well behaved, both problems have the same set of solutions that is.",
            "Given a quantile level Alpha, there exists a unique density level new such that.",
            "The Alpha quantile equals the density level set at level mu.",
            "What qualifies is not defined in a unique way, so this equality holds up to null measured subsets.",
            "OK, and I would like to mention in passing while it's interesting to get interest in quantile estimation, so by construction quanties are subsets.",
            "Very dense in probability mass.",
            "For a fixed volume, they maximize the probability mass, while equivalently for a fixed probability mass, they minimize the volume.",
            "And so they are tools of special interest for investigating the structure of distribution, and that's why they are.",
            "They are used in many unsupervised learning applications such that anomaly detection, cluster analysis and probably many other applications."
        ],
        [
            "So now back to the one class SVM.",
            "So this is the algorithm that things changed and the question is.",
            "What's the link between?",
            "The acceptance region computed by."
        ],
        [
            "The algorithm and quantize, or equivalently, the density levels.",
            "So.",
            "In in the paper.",
            "From Shell Coffin coasters, it was suggested that the algorithm might be relevant for this problem.",
            "But so I think so far there has been no proof stating that this algorithm is effectively consistent for this task.",
            "So that's the main domain contribution of the work I'm going to present now.",
            "Small command in this context, so there's proof.",
            "Live Individal saying that the problem of estimating for dependency so hard.",
            "Yeah.",
            "So this is probably one of the main reasons why people didn't like trying too much afterwards.",
            "So."
        ],
        [
            "The main contribution here is to say that under certain assumptions, and in some context.",
            "One class SVM actually is actually consistent for estimating density level sets.",
            "So this context is.",
            "Characterized by the use of a fixed regularization parameter, Lambda, that is whatever the number of training data points, we will work with fixed value for the parameter Lambda, so this.",
            "This might be not in the spirit of standard regularization, since when we do regularization, the parameter Lambda is usually finishing term aseptically.",
            "But here we will work with a constant parameter, Lambda that is.",
            "One of the main particularity of the setup.",
            "In addition, we will consider the Goshen kernel.",
            "And with the with a well calibrated bandwidth Sigma so will consider bandwidth that decrease at the start at a certain speed aseptically.",
            "Right?",
            "So now."
        ],
        [
            "Now the very talk can begin.",
            "It was a sort of introduction, and so the talk is structured as follows.",
            "In the first part I will present the main result together with necessary notation for expressing the result and also I will present what I call a big picture of the whole machinery.",
            "So in order to give you the intuition behind the main result, while the main result actually holds.",
            "And afterwards, the next four parts will be dedicated to the very funny proof of that.",
            "So."
        ],
        [
            "Yeah, so the."
        ],
        [
            "So.",
            "And Mobiliti to give some notations so X1 XN is the data sample drawn from an unknown probability distribution P. The probability has compact support.",
            "Well supports included in a reference subset X, which is assumed to be compact.",
            "And so X is included in our D. And so really keep in mind that X is compact because it's strong assumption and.",
            "The result that I'm going to present are only valid for compact subsets.",
            "Then the Goshen kernel here note that we use the normalized Gaussian kernel, so this this is the standard one with this constant that depends on Sigma, and The thing is, if we take the integral on Rd of this function with respect to the first variable.",
            "The integral equals one.",
            "H Sigma is the associated Goshen Dark Ages and it is endowed with the norm denoted like that.",
            "We denote by F Sigma hat.",
            "The output of the one class SVM algorithm when used with the Goshen kernel with bandwidth Sigma OK. And finally we use this notation.",
            "Our Sigma of F to denote.",
            "This synthetic risk, so this is the theoretical version of this entotic.",
            "Sorry of this empirical SVM risk, so we just replace the empirical counts here by its true expectation.",
            "So.",
            "Or maybe our Sigma, so maybe I write it down, write it down.",
            "I've seen.",
            "OK. OK.",
            "So is it OK for the notation?",
            "So this is the big picture, so this is the big slide of the talk."
        ],
        [
            "Is.",
            "The most important one.",
            "So to understand what's behind the main result, it is good to focus on the pointwise convergence.",
            "So by pointwise convergence I mean.",
            "We fix Goshen RK chess with reference bandwidth that we call Sigma one, so it's a fixed RKHS and we take a fixed function in it and then we see what happens to the.",
            "What a simplistically happens to the SVM?",
            "Empirical risk of F?",
            "So the first thing I have to say is that for any Sigma that is smaller than Sigma one.",
            "F is F is an element of the Goshen kernel with Bandwidth Sigma.",
            "OK, so this is a property that we shall see in what in the Seeker.",
            "So the Dark Ages norm with respect to Sigma is well defined, and in addition, what's interesting is that there are cages.",
            "Normal F converges to itself, 2, known as Sigma tends to 0.",
            "So this is a.",
            "Very important property that makes things work.",
            "This is this is the big picture, but this is true.",
            "This is this.",
            "This pointwise convergence is true, but this is not really part of the proof because afterwards we have to make the.",
            "We have to prove the same sort of things, but uniformly of a set of function.",
            "So this is pointwise convergence and the proof consists in turning it into a uniform.",
            "Statement.",
            "So this is here you, I think everybody knows better than me that.",
            "This converge to this, it's the low of larger numbers.",
            "So there is the same when you want to study empirical risk minimization then you get you can get an intuition with pointwise convergence of the empirical risk through the true risk.",
            "But then if you want to make a real proof you consider.",
            "If it is bounded.",
            "But in this case, I think.",
            "For the convergence of the right looks like, yeah.",
            "Yeah.",
            "OK so this point wise.",
            "Convergence suggests.",
            "Suggest us to make the following guess."
        ],
        [
            "Assume that eccle one class SVM might amount to minimizing this syntactic risk.",
            "That is simply this risk, so we just take the limit of each term and.",
            "So we can make this case so I just have a little remark on the notation.",
            "Here we use the notation R 0 because it is consistent with what happens, point wisely because.",
            "Sure.",
            "For any F. In H Sigma one.",
            "We have just seen that are Sigma over.",
            "Tends to RO.",
            "Of F. Precisely when Sigma tends to zero OK.",
            "So the risk are.",
            "Well, the risk are Sigma.",
            "Tends to the risk are zero when Sigma tends to zero.",
            "OK, so this is a consistent notation.",
            "OK, and what's the use?",
            "What's the interest in?",
            "What's the interest of minimizing the risk are zero?",
            "Well, to see that the surface is to actually compute minimizer of R0, and it turns out that you can do it.",
            "You can do it.",
            "There is a closed form for.",
            "This minimizer."
        ],
        [
            "So.",
            "With some lines of computing we finally find.",
            "This expression for the minimizer of R0 we find that this is a function that is constant equals to one on the density level set of level 2 Lambda.",
            "And outside this level set this proportional to the true density.",
            "So.",
            "Maybe a?",
            "Little drilling so.",
            "So you suppose you have the true density here row.",
            "Here is a two Lambda.",
            "And so this.",
            "So here you have the density level set of two Lambda.",
            "And so the output of the SVM as so.",
            "Now the functionality rule is.",
            "Equals to Warner on this.",
            "Density will set and then proportional to the true density.",
            "So this is maybe we will prove actually that this is the aseptic decision function of one class SVM, so we see now that why it's interesting to minimize our zero.",
            "It gives a function that.",
            "Up to a multiplicative constant is equal to a truncated version of the true density.",
            "So this is for the moment just a guess made from the observation of the pointwise convergence.",
            "And now I will state the.",
            "The."
        ],
        [
            "In results that formalizes this intuition.",
            "So for that purpose we first need to make some assumption on the distribution.",
            "So we suppose that first it has a density with respect to the big measure.",
            "We still suppose that it it has a compact support.",
            "This is important.",
            "We assume that the density is uniformly bounded.",
            "On on ITS support.",
            "And eventually we make the following assumption on on the.",
            "So this is the smoothness assumption that we make on the density, so we consider its modulus of continuity with respect to the L1 node.",
            "And we suppose that this modulus is controlled in terms of of.",
            "So the modulus of rule at scale Delta is upper bounded by Delta to the beta.",
            "So here beta is the exponent.",
            "That qualifies the regularity of the density function.",
            "So it's typically equal to 1 with.",
            "Well behaving density functions.",
            "OK, and so with these."
        ],
        [
            "These assumptions and formal notation at hand, we have the following result.",
            "If we choose well calibrated Sigma that is Sigma equals this.",
            "Then, with high probability, the azira risk of the SVM estimator will converge to the smallest achievable zero risk on L2.",
            "With this speed.",
            "So I'd just like to mention that.",
            "I am obliged to add this ipsilon in the bounce, but just.",
            "Just forget about it.",
            "So the true rates of convergence, the moral rates of convergence is this one and just don't look at the epsilon because you can take it as small as possible.",
            "So we have the following bounds, and Furthermore we have.",
            "That the decision function of output by one class SVM.",
            "Itself converges to the target function, F0 in the L2 norm.",
            "And at the same speed as this one, these are the same rates.",
            "OK.",
            "So this is the main result that I wanted to present and maybe a few comments about this result first.",
            "Will take some absolute Masters, which one K yes K depends on everything except N and Sigma.",
            "OK, so K is constant with respect to N and Sigma but.",
            "It really truly depends on everything else.",
            "Play when you were telling us if we could take epsilon Smalls.",
            "We liked it because it's probably.",
            "Yeah yeah, that's why I'm obliged to put an epsilon.",
            "Otherwise I would.",
            "This would be Infinity so.",
            "Yeah, you said that case depending on Sigma or independent node is not so important.",
            "Because Sigma depends on beta, yeah?",
            "So I said so, so I maybe I have to say that in practice Sigma is not computable.",
            "This Sigma is not really computable.",
            "I mean this is.",
            "This is not what we can call an adaptive result in the sense that.",
            "In practice, we cannot calibrate Sigma because we don't know the regularity of the density.",
            "I feel that you first have Sigma and then you compare choosing no.",
            "I have I have bit I have N and so I calibrate Sigma.",
            "Yeah.",
            "Can you instead of the Gaussian RBF kernel, just use any other regularizer?",
            "So at least any other RBF type regularizer doesn't mean all you really need is that the reference form.",
            "Is upper bounded?",
            "And one of the quantities.",
            "Then they found the ticket for later.",
            "Maybe we'll see that later.",
            "Oh yeah, we use several properties of the Gaussian kernel here.",
            "And yeah, maybe we will see what's important that that process of convergence.",
            "I mean, you could just start with the regularizer being, say, the L2 norm, plus maybe the L2 norm of the first relative and all you do is you just turn that one down so you don't really need to.",
            "Yeah, maybe specifically with the Gaussian RBF kernel maybe?",
            "But the reason why I'm taking the Goshen kernel is that we use it in practice and.",
            "But for sure it may be true.",
            "It works with more general kernels.",
            "At the arcade, Jason, or is just the L2 term plus a whole bunch of other terms.",
            "Yeah, which banishes with which other terms you choose.",
            "I mean, that's pretty much OK. Yeah, it shows the current, that's it.",
            "But I mean there's nothing really sacred about the RBF about the car, yeah?",
            "So we're done with this slide, OK?",
            "And Oh no, no, no.",
            "So the comments is that yeah, I prefer to.",
            "I'll consider those.",
            "This result has no more.",
            "No more than the consistency result because I really don't know if this convergence rate is optimal.",
            "I have no lower bound for the moment and so.",
            "I can say that we have stated the consistency of the algorithm, but I can't say these are fast rates for one class SVM.",
            "A second remark is that from this L2 convergence from F Sigma hat to the target F0, it can be derived that the.",
            "That's the level set.",
            "Have Sigma hats.",
            "Greater than 1 -- 10.",
            "Converges to the level sets.",
            "Let's sing.",
            "So with respect to.",
            "For instance.",
            "The criterion that I have described before.",
            "So from then to convergence we can derive the following convergence between sets.",
            "And well, this set is precisely so by definition of 0.",
            "This is the density level set at level 2, Lambda times 1 minus Delta.",
            "So from this result we can derive the consistency of one class SVM for density level set estimation.",
            "120 minutes.",
            "OK, so.",
            "We can move through the proof.",
            "So the proof begins with the split."
        ],
        [
            "As usual, we split the excess of rich."
        ],
        [
            "Ask.",
            "In different terms that we separately bound.",
            "So I won't enter into details.",
            "Well I don't have time so.",
            "I'd just like to mention that we we introduce the excess of our Sigma risk because we know that we can say something on this.",
            "Because, well, by definition of F Sigma hat, we know that F Sigma hats minimizes an empirical version of our Sigma, so we will have things to say to control this term.",
            "And well, then we have to.",
            "Well, it's OK.",
            "So the."
        ],
        [
            "Good thing is that there are two terms that are negative."
        ],
        [
            "So we can drop them.",
            "And."
        ],
        [
            "Finally, there remain three turns to bound, so the first one is the one that we wanted to introduce.",
            "It's referred to as the."
        ],
        [
            "Summation error.",
            "The second one is the funny one, so this is the regularization error."
        ],
        [
            "And to control it we will have to control the difference between our cages normal function and its L2 norm.",
            "And finally, the approximation error, so this is."
        ],
        [
            "OK.",
            "So I will be fast."
        ],
        [
            "Yeah I will."
        ],
        [
            "Lastly, show how to bond the estimation error.",
            "This is just an application of theorem from Scotland Stein Mart in the article fast rates for SVM with Gaussian kernel.",
            "So here we just plug the.",
            "Our are entries in their theorems and nothing more.",
            "There is no there is no trick.",
            "So the the only work we had to do was determining the smallest functional subspace.",
            "Where we know for sure that the estimator F Sigma lies OK, so of course we know by definition of F Sigma hat that it belongs to the Dark Ages H Sigma.",
            "But what we can show is that it belongs more precisely to this blue subsets, which is up to this multiplicative constant.",
            "The unit ball of the span of.",
            "This subset.",
            "So to see that this is like Representer theorem that we use to show that the solution of the SVM can be expanded on this kind of function where X only belongs to the compact subset Big X.",
            "And it's very interesting and important for us to have a compact index here, since it enables to nicely control the covering numbers of this set.",
            "So here we took a result directly from Scotland and Stein Mart, so we bound recovering number of this unit bowl with respect to the Empire recall L2 norm.",
            "So here T is the training sample and at this scale that we are interested in.",
            "And so we get a bound on the covering number, so maybe I don't have time to say more in any way.",
            "That's not where I want to put the focus, so it turns out that.",
            "Still directly using."
        ],
        [
            "Children from school and steinbart.",
            "We have the following bound on the excess of our Sigma risk OK.",
            "So just note that this bound is parameterized by two values, P that we can play with.",
            "We can take it between zero and one and also.",
            "I forgot there is a number of Delta that we can take as we want it just have to be positive quantity.",
            "And of course, playing with P and Delta affects the constant K1.",
            "So here K1 is a constant.",
            "Well, it does not depend neither on neither on Sigma, but depends on all other quantities.",
            "So that's the way we bound the estimation error, so that's really there is no surprise here, just use a theorem.",
            "OK, so we've got two more."
        ],
        [
            "Use to bound.",
            "So yeah."
        ],
        [
            "So the regularization error so."
        ],
        [
            "To control this term.",
            "It's very convenient to work with the following characterization of the Goshen Ark ages, so that's what Alexander said here.",
            "We can say that the Goshen Arcade is the set of continuous function vanishing at Infinity.",
            "That are integrable on our D. And which have a fully transform that satisfies this integrability constraint.",
            "OK, so this constraints means that for you transform.",
            "Vanish is at Infinity.",
            "Exponentially fast.",
            "OK, because this has to be integrable so we don't allow the big frequencies.",
            "And so this is characterization of the Goshen Ark HSN, the RK just known is precisely this quantity.",
            "With this multiplicative constant.",
            "OK. OK, so from this characterization we can show 2 interesting properties."
        ],
        [
            "1st.",
            "When Sigma and two are ordered this way, then the corresponding are cages are ordered this way, but that means that the when Sigma gets smaller and smaller.",
            "The Associated Arc HH Sigma gets larger and larger.",
            "It forms a nested collections of functional spaces and at the end we almost reach L2 of RG.",
            "And the second property that is more quantitative relates the arcade is Norm in eight Sigma to the arcade is Norm in H2 for two different values Sigma into.",
            "So to pass from one to another, we have to.",
            "To pay this price, Sigma squared over 2 ^2.",
            "This is the web to pass from H Sigma to edge to.",
            "And.",
            "So from those properties.",
            "We can now.",
            "We cannot."
        ],
        [
            "See how to bound the regularization error term so this is the regularization error term.",
            "So this is the difference between the Arcadis Norman the L2 norm of this function.",
            "So we just use.",
            "Excuse me, I just go back.",
            "We just use this inequality.",
            "With tool equals.",
            "We take 2 = sqrt 2 Sigma one.",
            "OK, we take these values and this gives.",
            "The following inequality.",
            "And then using the expression of the of the dark Ages known of the functions.",
            "We just easily see that.",
            "This quantity is precisely equal to the L2 norm of F0.",
            "So this is very easy computing only.",
            "You just have to use the characterization of the norm of the dark Ages in terms of the fully transform.",
            "So eventually here is the way we control the regularization error.",
            "So it's essentially in Sigma squared over Sigma 1, ^2.",
            "So.",
            "Here we are."
        ],
        [
            "And."
        ],
        [
            "The last thing is to proximation."
        ],
        [
            "So this is more conventional.",
            "So this is what we have called the approximation error term.",
            "So in with one or two lines of computing, we related with L1 distance between FO and its convolution with the kernel.",
            "The Goshen kernel.",
            "And well then.",
            "This L1 distance can be bounded.",
            "By the modulus of continuity of F0 at Scale Sigma, one well to get that.",
            "You only use the very.",
            "Standard equality and inequality's.",
            "Fubini's theorem, holders inequality.",
            "The fact that the kernel Gaussian kernel integrates to one.",
            "And what else?",
            "Well, things like that it's it took us 12 lines of computing, but not so difficult computing.",
            "And then while we use the assumption that we made on the on the modulus of continuity.",
            "OK, so.",
            "Here we are.",
            "We're almost there."
        ],
        [
            "We have this global bound which holds with high probability it's not written here by lack of space, but this holds with probability at least one minus exponential minus X, so that's why there is an X here.",
            "And we now we have to optimize this bound with respect to the four parameters.",
            "That is with respect to P, 2D2 Sigma and two Sigma one.",
            "And if we do that, we find the result that was stated before.",
            "OK.",
            "So some."
        ],
        [
            "Concluding remarks.",
            "So here we I insist we have been working with the regularization parameter Lambda.",
            "That was held fixed.",
            "It's not.",
            "It was not vanishing at Infinity.",
            "It was a fixed parameter and the regularization intensity that we put.",
            "Was only tuned through the choice of Sigma.",
            "Just just playing with Sigma enables to control the amount of regularization that we want to put.",
            "So this is the first mark.",
            "Second remark is that well, considering the results that we have, we can say that one class SVM belongs to.",
            "Well in this case when we use it with the Gaussian kernel, it can be considered as belonging to kernel density estimator family such as parsing window.",
            "And maybe a practical remark.",
            "Yeah.",
            "The in practice, so this is a.",
            "So the SVM decision function is close to this purple curve and we see that it's really not good to threshold this function at level one.",
            "Because of this flat part.",
            "So this is really an argument to say that you really have to lower the threshold and to take 1 minus Delta.",
            "When you use the one class SVM for.",
            "Anomaly detection or whatever.",
            "So this.",
            "So this idea of lowering the thresholds was already suggested in the in the original paper.",
            "And while here it is justified in another way.",
            "Anne.",
            "And last remark, if you read the paper that we wrote, you will see that we didn't directly study the one class SVM, but actually we studied were not as we analyzed.",
            "Sorry, more general family of algorithm, namely.",
            "Algorithm that minimizes such.",
            "Empirical regularized functionals.",
            "HDMI squared.",
            "Hi why I?",
            "So actually we we propose an analysis of this kind of algorithm.",
            "That solves this optimization problem so.",
            "It encompasses the two class case because here the yiz are labels of the data and we don't only consider the hinge loss function, but general convex loss functions and this results remains valid.",
            "Simulation over eight Sigma.",
            "OK. And OK, so that's it.",
            "So actually, the results that we proposed is still valid for for convex loss function.",
            "Oh no no.",
            "No more evil said because when you change five you have to compute the associated target function F0.",
            "OK yeah, for each file you've got an associated target function F0, and that's true that you converge to this function.",
            "In the West to normal.",
            "So in the for instance that shows that two class SVM is consistent for classification.",
            "Even when we don't take a regularization parameter, that vanish is at Infinity.",
            "If you use two class SVM with a fixed regularization parameter and with a well calibrated bandwidth Sigma then.",
            "The classification risk of the estimator will converge to the smallest achievable.",
            "Convert went well then.",
            "Yeah, when yes when N goes to Infinity and Sigma goes to zero at the right speed.",
            "Yesterday."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For arranging the meeting and for inviting me here.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to speak about one class SVM.",
                    "label": 0
                },
                {
                    "sent": "And so the work I'm going to present is a joint work with my brother Rafeedie to.",
                    "label": 0
                },
                {
                    "sent": "And for those who are interested in having further details on that work.",
                    "label": 0
                },
                {
                    "sent": "There is a full and detailed version available on my web page, so this is.",
                    "label": 0
                },
                {
                    "sent": "Work that we've been doing, and.",
                    "label": 0
                },
                {
                    "sent": "So I will present a result, then the associated proof.",
                    "label": 0
                },
                {
                    "sent": "It's a consistency result and the interest of the proof will be.",
                    "label": 0
                },
                {
                    "sent": "The presence of an exotic term.",
                    "label": 0
                },
                {
                    "sent": "So usually when you want to prove a result about the convergence of of an error term of the risk.",
                    "label": 0
                },
                {
                    "sent": "There is a split that in which arise variance term and a bias term.",
                    "label": 0
                },
                {
                    "sent": "And here in the split there will be a.",
                    "label": 0
                },
                {
                    "sent": "The third term, which we call the regularization error term.",
                    "label": 0
                },
                {
                    "sent": "And to bomb this term, we will have to investigate the link between the arcade just normal of the Gaussian kernel and the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "So this will be the funny part of the proof.",
                    "label": 0
                },
                {
                    "sent": "That's why I'm presenting the proof there is something funny in it.",
                    "label": 0
                },
                {
                    "sent": "So, so this is about one class SVM, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first thing I have to do is maybe to present the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So one class SVM was introduced by 5 researchers.",
                    "label": 0
                },
                {
                    "sent": "Social curve smaller.",
                    "label": 0
                },
                {
                    "sent": "Show Taylor Platten Williamson.",
                    "label": 0
                },
                {
                    "sent": "In a paper called estimating the support of a high dimensional distribution and in this paper the problem that was considered.",
                    "label": 0
                },
                {
                    "sent": "Is the problem of estimating quantiles that is, minimum subsets of given probability?",
                    "label": 0
                },
                {
                    "sent": "And so to tackle the problem of contract estimation, they proposed kernel algorithm caused called one class SVM.",
                    "label": 0
                },
                {
                    "sent": "Which works as follows.",
                    "label": 0
                },
                {
                    "sent": "Suppose you're given data sample X1, XN are leaving in some input space.",
                    "label": 0
                },
                {
                    "sent": "Dicks and you choose a so called regularization parameter or Lambda.",
                    "label": 0
                },
                {
                    "sent": "And you also choose a reproducing kernel Hilbert space H, so that is a functional space associated with the kernel.",
                    "label": 0
                },
                {
                    "sent": "The algorithm amounts to minimizing this empirical quantity, so I think many of you are familiar with this kind of objective function.",
                    "label": 0
                },
                {
                    "sent": "It's regularized impairment called risk functional.",
                    "label": 0
                },
                {
                    "sent": "So here is what we call the regularization term, and here is the.",
                    "label": 0
                },
                {
                    "sent": "The empirical error count.",
                    "label": 0
                },
                {
                    "sent": "So if we look at at both terms separately.",
                    "label": 0
                },
                {
                    "sent": "We see that if we want to minimize OK, maybe I have to say a word about this notation.",
                    "label": 0
                },
                {
                    "sent": "This is a positive part, so.",
                    "label": 0
                },
                {
                    "sent": "So I say.",
                    "label": 0
                },
                {
                    "sent": "So the the subscript plus denotes this function.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you want to minimize this term, we have to pick to pick up from the arcade S function that should be at least that should take values at least one of the training data so that this is.",
                    "label": 0
                },
                {
                    "sent": "As small as possible, but in the same time we have to take a function that is small with respect to the arcade is known and so minimizing the sum.",
                    "label": 0
                },
                {
                    "sent": "I feel both terms typically leads to decision function.",
                    "label": 0
                },
                {
                    "sent": "That takes values around equal to 1 on many of the training points, but not all the points because in the same time it has to be.",
                    "label": 0
                },
                {
                    "sent": "Reasonably smaller than regular and with smaller infinite number so.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can draw a picture here so you get the points.",
                    "label": 0
                },
                {
                    "sent": "These are the training points in one dimension.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "So here is the value one.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the function will try to take values at least one on many training points.",
                    "label": 0
                },
                {
                    "sent": "So essentially where the points are.",
                    "label": 0
                },
                {
                    "sent": "Are well sample, so here it's once and in the same time we want a small function.",
                    "label": 0
                },
                {
                    "sent": "So typically on the tail here will have vanish in function like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a.",
                    "label": 0
                },
                {
                    "sent": "So this is typically how the resulting function F hat look like.",
                    "label": 0
                },
                {
                    "sent": "OK, so this step, which consists of minimizing efforts can be called the training step of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is the step where the optimization takes place.",
                    "label": 0
                },
                {
                    "sent": "And once the function has been computed, it is associated with the threshold.",
                    "label": 0
                },
                {
                    "sent": "So in the original paper that proposed the algorithm, the threshold is 1 and while in the paper it was also suggested to take a threshold of the Form 1 minus Delta, where Delta is a small positive quantity.",
                    "label": 0
                },
                {
                    "sent": "And the meaning of sort of thresholding the function is.",
                    "label": 0
                },
                {
                    "sent": "For instance, if we do anomaly detection with one class SVM.",
                    "label": 0
                },
                {
                    "sent": "When given a new test point.",
                    "label": 0
                },
                {
                    "sent": "We want to check the normality of this new test point.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we compute the value of F hat.",
                    "label": 0
                },
                {
                    "sent": "On this new test point, and we compare this value with the threshold.",
                    "label": 0
                },
                {
                    "sent": "So if this value falls in four is above 1 minus Delta, then the point is considered as a normal point, otherwise it is considered as somehow suspect.",
                    "label": 0
                },
                {
                    "sent": "Maybe to be rejected as an outlier.",
                    "label": 0
                },
                {
                    "sent": "So this is basically how the algorithm works.",
                    "label": 0
                },
                {
                    "sent": "So is it clear how it works or OK?",
                    "label": 0
                },
                {
                    "sent": "So in the paper it was proposed as a tool for estimating quantiles.",
                    "label": 0
                },
                {
                    "sent": "That is this acceptance region.",
                    "label": 0
                },
                {
                    "sent": "Is supposed to estimate a quantile.",
                    "label": 0
                },
                {
                    "sent": "So here I want to make sure that we all agree on the definition of quantile so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have a.",
                    "label": 0
                },
                {
                    "sent": "A little slide about quantile and quantized estimation.",
                    "label": 0
                },
                {
                    "sent": "So the setup is the following one.",
                    "label": 0
                },
                {
                    "sent": "We have a distribution probability distribution capital P on say RG.",
                    "label": 0
                },
                {
                    "sent": "So I just precise now our deal with will be the largest input space that we consider throughout the talk.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So we have these distribution and we define a quantile as follows.",
                    "label": 0
                },
                {
                    "sent": "So an Alpha quantile, where Alpha is quantile level between zero and one is simply a subset measurable subset of Rd.",
                    "label": 0
                },
                {
                    "sent": "With probability mass at least Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the constraint here, which has which is of minimum lebeck measure.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple definition.",
                    "label": 0
                },
                {
                    "sent": "And the statistical problem that we consider is given a sample ID so independently and identically drawn from the unknown distribution P estimate crew of Alpha.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Why not?",
                    "label": 0
                },
                {
                    "sent": "Maybe I could precise what estimates mean so?",
                    "label": 0
                },
                {
                    "sent": "So suppose this is.",
                    "label": 0
                },
                {
                    "sent": "Queue of Alpha for a certain Alpha.",
                    "label": 0
                },
                {
                    "sent": "And you use a quantile estimator that outputs.",
                    "label": 0
                },
                {
                    "sent": "That outputs a subset of the input space, so this is your estimate of your Alpha.",
                    "label": 0
                },
                {
                    "sent": "Man.",
                    "label": 0
                },
                {
                    "sent": "And so, for instance, one way for assessing the.",
                    "label": 0
                },
                {
                    "sent": "Formance of the estimator is to consider the.",
                    "label": 0
                },
                {
                    "sent": "The standard way to measure the performance is to take the low back measure of.",
                    "label": 0
                },
                {
                    "sent": "The symmetric difference between Q of Alpha and Q hat.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is for instance criterion.",
                    "label": 0
                },
                {
                    "sent": "That enables you to.",
                    "label": 0
                },
                {
                    "sent": "To evaluate the performance of Q hat of Alpha.",
                    "label": 0
                },
                {
                    "sent": "So since I'm talking about quantile estimation, I would like to also address the very closely related problem of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Density level set estimation so.",
                    "label": 1
                },
                {
                    "sent": "Then still sat estimation is just about estimating from a data sample.",
                    "label": 1
                },
                {
                    "sent": "A density level set, so here the setup is.",
                    "label": 1
                },
                {
                    "sent": "We assume that the distribution has density with respect to the lab measure.",
                    "label": 0
                },
                {
                    "sent": "And given a desired density level, mu.",
                    "label": 0
                },
                {
                    "sent": "We want to estimate the.",
                    "label": 0
                },
                {
                    "sent": "The density level set.",
                    "label": 0
                },
                {
                    "sent": "So this is a very related problem to quantile estimation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More precisely, in standard situation, if the density is well behaved, both problems have the same set of solutions that is.",
                    "label": 0
                },
                {
                    "sent": "Given a quantile level Alpha, there exists a unique density level new such that.",
                    "label": 0
                },
                {
                    "sent": "The Alpha quantile equals the density level set at level mu.",
                    "label": 0
                },
                {
                    "sent": "What qualifies is not defined in a unique way, so this equality holds up to null measured subsets.",
                    "label": 0
                },
                {
                    "sent": "OK, and I would like to mention in passing while it's interesting to get interest in quantile estimation, so by construction quanties are subsets.",
                    "label": 0
                },
                {
                    "sent": "Very dense in probability mass.",
                    "label": 0
                },
                {
                    "sent": "For a fixed volume, they maximize the probability mass, while equivalently for a fixed probability mass, they minimize the volume.",
                    "label": 0
                },
                {
                    "sent": "And so they are tools of special interest for investigating the structure of distribution, and that's why they are.",
                    "label": 0
                },
                {
                    "sent": "They are used in many unsupervised learning applications such that anomaly detection, cluster analysis and probably many other applications.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now back to the one class SVM.",
                    "label": 0
                },
                {
                    "sent": "So this is the algorithm that things changed and the question is.",
                    "label": 0
                },
                {
                    "sent": "What's the link between?",
                    "label": 0
                },
                {
                    "sent": "The acceptance region computed by.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithm and quantize, or equivalently, the density levels.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In in the paper.",
                    "label": 0
                },
                {
                    "sent": "From Shell Coffin coasters, it was suggested that the algorithm might be relevant for this problem.",
                    "label": 0
                },
                {
                    "sent": "But so I think so far there has been no proof stating that this algorithm is effectively consistent for this task.",
                    "label": 0
                },
                {
                    "sent": "So that's the main domain contribution of the work I'm going to present now.",
                    "label": 0
                },
                {
                    "sent": "Small command in this context, so there's proof.",
                    "label": 0
                },
                {
                    "sent": "Live Individal saying that the problem of estimating for dependency so hard.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is probably one of the main reasons why people didn't like trying too much afterwards.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main contribution here is to say that under certain assumptions, and in some context.",
                    "label": 1
                },
                {
                    "sent": "One class SVM actually is actually consistent for estimating density level sets.",
                    "label": 1
                },
                {
                    "sent": "So this context is.",
                    "label": 0
                },
                {
                    "sent": "Characterized by the use of a fixed regularization parameter, Lambda, that is whatever the number of training data points, we will work with fixed value for the parameter Lambda, so this.",
                    "label": 0
                },
                {
                    "sent": "This might be not in the spirit of standard regularization, since when we do regularization, the parameter Lambda is usually finishing term aseptically.",
                    "label": 0
                },
                {
                    "sent": "But here we will work with a constant parameter, Lambda that is.",
                    "label": 0
                },
                {
                    "sent": "One of the main particularity of the setup.",
                    "label": 1
                },
                {
                    "sent": "In addition, we will consider the Goshen kernel.",
                    "label": 0
                },
                {
                    "sent": "And with the with a well calibrated bandwidth Sigma so will consider bandwidth that decrease at the start at a certain speed aseptically.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the very talk can begin.",
                    "label": 0
                },
                {
                    "sent": "It was a sort of introduction, and so the talk is structured as follows.",
                    "label": 0
                },
                {
                    "sent": "In the first part I will present the main result together with necessary notation for expressing the result and also I will present what I call a big picture of the whole machinery.",
                    "label": 0
                },
                {
                    "sent": "So in order to give you the intuition behind the main result, while the main result actually holds.",
                    "label": 0
                },
                {
                    "sent": "And afterwards, the next four parts will be dedicated to the very funny proof of that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And Mobiliti to give some notations so X1 XN is the data sample drawn from an unknown probability distribution P. The probability has compact support.",
                    "label": 0
                },
                {
                    "sent": "Well supports included in a reference subset X, which is assumed to be compact.",
                    "label": 0
                },
                {
                    "sent": "And so X is included in our D. And so really keep in mind that X is compact because it's strong assumption and.",
                    "label": 0
                },
                {
                    "sent": "The result that I'm going to present are only valid for compact subsets.",
                    "label": 0
                },
                {
                    "sent": "Then the Goshen kernel here note that we use the normalized Gaussian kernel, so this this is the standard one with this constant that depends on Sigma, and The thing is, if we take the integral on Rd of this function with respect to the first variable.",
                    "label": 0
                },
                {
                    "sent": "The integral equals one.",
                    "label": 0
                },
                {
                    "sent": "H Sigma is the associated Goshen Dark Ages and it is endowed with the norm denoted like that.",
                    "label": 0
                },
                {
                    "sent": "We denote by F Sigma hat.",
                    "label": 0
                },
                {
                    "sent": "The output of the one class SVM algorithm when used with the Goshen kernel with bandwidth Sigma OK. And finally we use this notation.",
                    "label": 0
                },
                {
                    "sent": "Our Sigma of F to denote.",
                    "label": 0
                },
                {
                    "sent": "This synthetic risk, so this is the theoretical version of this entotic.",
                    "label": 0
                },
                {
                    "sent": "Sorry of this empirical SVM risk, so we just replace the empirical counts here by its true expectation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Or maybe our Sigma, so maybe I write it down, write it down.",
                    "label": 0
                },
                {
                    "sent": "I've seen.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                },
                {
                    "sent": "So is it OK for the notation?",
                    "label": 0
                },
                {
                    "sent": "So this is the big picture, so this is the big slide of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "The most important one.",
                    "label": 0
                },
                {
                    "sent": "So to understand what's behind the main result, it is good to focus on the pointwise convergence.",
                    "label": 0
                },
                {
                    "sent": "So by pointwise convergence I mean.",
                    "label": 1
                },
                {
                    "sent": "We fix Goshen RK chess with reference bandwidth that we call Sigma one, so it's a fixed RKHS and we take a fixed function in it and then we see what happens to the.",
                    "label": 0
                },
                {
                    "sent": "What a simplistically happens to the SVM?",
                    "label": 0
                },
                {
                    "sent": "Empirical risk of F?",
                    "label": 0
                },
                {
                    "sent": "So the first thing I have to say is that for any Sigma that is smaller than Sigma one.",
                    "label": 0
                },
                {
                    "sent": "F is F is an element of the Goshen kernel with Bandwidth Sigma.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a property that we shall see in what in the Seeker.",
                    "label": 0
                },
                {
                    "sent": "So the Dark Ages norm with respect to Sigma is well defined, and in addition, what's interesting is that there are cages.",
                    "label": 0
                },
                {
                    "sent": "Normal F converges to itself, 2, known as Sigma tends to 0.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "Very important property that makes things work.",
                    "label": 0
                },
                {
                    "sent": "This is this is the big picture, but this is true.",
                    "label": 1
                },
                {
                    "sent": "This is this.",
                    "label": 0
                },
                {
                    "sent": "This pointwise convergence is true, but this is not really part of the proof because afterwards we have to make the.",
                    "label": 0
                },
                {
                    "sent": "We have to prove the same sort of things, but uniformly of a set of function.",
                    "label": 0
                },
                {
                    "sent": "So this is pointwise convergence and the proof consists in turning it into a uniform.",
                    "label": 0
                },
                {
                    "sent": "Statement.",
                    "label": 0
                },
                {
                    "sent": "So this is here you, I think everybody knows better than me that.",
                    "label": 0
                },
                {
                    "sent": "This converge to this, it's the low of larger numbers.",
                    "label": 0
                },
                {
                    "sent": "So there is the same when you want to study empirical risk minimization then you get you can get an intuition with pointwise convergence of the empirical risk through the true risk.",
                    "label": 0
                },
                {
                    "sent": "But then if you want to make a real proof you consider.",
                    "label": 0
                },
                {
                    "sent": "If it is bounded.",
                    "label": 0
                },
                {
                    "sent": "But in this case, I think.",
                    "label": 0
                },
                {
                    "sent": "For the convergence of the right looks like, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK so this point wise.",
                    "label": 0
                },
                {
                    "sent": "Convergence suggests.",
                    "label": 0
                },
                {
                    "sent": "Suggest us to make the following guess.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Assume that eccle one class SVM might amount to minimizing this syntactic risk.",
                    "label": 0
                },
                {
                    "sent": "That is simply this risk, so we just take the limit of each term and.",
                    "label": 1
                },
                {
                    "sent": "So we can make this case so I just have a little remark on the notation.",
                    "label": 0
                },
                {
                    "sent": "Here we use the notation R 0 because it is consistent with what happens, point wisely because.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 1
                },
                {
                    "sent": "For any F. In H Sigma one.",
                    "label": 0
                },
                {
                    "sent": "We have just seen that are Sigma over.",
                    "label": 0
                },
                {
                    "sent": "Tends to RO.",
                    "label": 0
                },
                {
                    "sent": "Of F. Precisely when Sigma tends to zero OK.",
                    "label": 0
                },
                {
                    "sent": "So the risk are.",
                    "label": 0
                },
                {
                    "sent": "Well, the risk are Sigma.",
                    "label": 0
                },
                {
                    "sent": "Tends to the risk are zero when Sigma tends to zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a consistent notation.",
                    "label": 0
                },
                {
                    "sent": "OK, and what's the use?",
                    "label": 0
                },
                {
                    "sent": "What's the interest in?",
                    "label": 0
                },
                {
                    "sent": "What's the interest of minimizing the risk are zero?",
                    "label": 0
                },
                {
                    "sent": "Well, to see that the surface is to actually compute minimizer of R0, and it turns out that you can do it.",
                    "label": 0
                },
                {
                    "sent": "You can do it.",
                    "label": 0
                },
                {
                    "sent": "There is a closed form for.",
                    "label": 0
                },
                {
                    "sent": "This minimizer.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "With some lines of computing we finally find.",
                    "label": 0
                },
                {
                    "sent": "This expression for the minimizer of R0 we find that this is a function that is constant equals to one on the density level set of level 2 Lambda.",
                    "label": 1
                },
                {
                    "sent": "And outside this level set this proportional to the true density.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Maybe a?",
                    "label": 0
                },
                {
                    "sent": "Little drilling so.",
                    "label": 0
                },
                {
                    "sent": "So you suppose you have the true density here row.",
                    "label": 0
                },
                {
                    "sent": "Here is a two Lambda.",
                    "label": 0
                },
                {
                    "sent": "And so this.",
                    "label": 0
                },
                {
                    "sent": "So here you have the density level set of two Lambda.",
                    "label": 0
                },
                {
                    "sent": "And so the output of the SVM as so.",
                    "label": 0
                },
                {
                    "sent": "Now the functionality rule is.",
                    "label": 0
                },
                {
                    "sent": "Equals to Warner on this.",
                    "label": 0
                },
                {
                    "sent": "Density will set and then proportional to the true density.",
                    "label": 0
                },
                {
                    "sent": "So this is maybe we will prove actually that this is the aseptic decision function of one class SVM, so we see now that why it's interesting to minimize our zero.",
                    "label": 0
                },
                {
                    "sent": "It gives a function that.",
                    "label": 0
                },
                {
                    "sent": "Up to a multiplicative constant is equal to a truncated version of the true density.",
                    "label": 0
                },
                {
                    "sent": "So this is for the moment just a guess made from the observation of the pointwise convergence.",
                    "label": 0
                },
                {
                    "sent": "And now I will state the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In results that formalizes this intuition.",
                    "label": 0
                },
                {
                    "sent": "So for that purpose we first need to make some assumption on the distribution.",
                    "label": 0
                },
                {
                    "sent": "So we suppose that first it has a density with respect to the big measure.",
                    "label": 1
                },
                {
                    "sent": "We still suppose that it it has a compact support.",
                    "label": 0
                },
                {
                    "sent": "This is important.",
                    "label": 0
                },
                {
                    "sent": "We assume that the density is uniformly bounded.",
                    "label": 0
                },
                {
                    "sent": "On on ITS support.",
                    "label": 0
                },
                {
                    "sent": "And eventually we make the following assumption on on the.",
                    "label": 1
                },
                {
                    "sent": "So this is the smoothness assumption that we make on the density, so we consider its modulus of continuity with respect to the L1 node.",
                    "label": 0
                },
                {
                    "sent": "And we suppose that this modulus is controlled in terms of of.",
                    "label": 0
                },
                {
                    "sent": "So the modulus of rule at scale Delta is upper bounded by Delta to the beta.",
                    "label": 0
                },
                {
                    "sent": "So here beta is the exponent.",
                    "label": 0
                },
                {
                    "sent": "That qualifies the regularity of the density function.",
                    "label": 0
                },
                {
                    "sent": "So it's typically equal to 1 with.",
                    "label": 0
                },
                {
                    "sent": "Well behaving density functions.",
                    "label": 0
                },
                {
                    "sent": "OK, and so with these.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These assumptions and formal notation at hand, we have the following result.",
                    "label": 0
                },
                {
                    "sent": "If we choose well calibrated Sigma that is Sigma equals this.",
                    "label": 0
                },
                {
                    "sent": "Then, with high probability, the azira risk of the SVM estimator will converge to the smallest achievable zero risk on L2.",
                    "label": 0
                },
                {
                    "sent": "With this speed.",
                    "label": 0
                },
                {
                    "sent": "So I'd just like to mention that.",
                    "label": 0
                },
                {
                    "sent": "I am obliged to add this ipsilon in the bounce, but just.",
                    "label": 0
                },
                {
                    "sent": "Just forget about it.",
                    "label": 0
                },
                {
                    "sent": "So the true rates of convergence, the moral rates of convergence is this one and just don't look at the epsilon because you can take it as small as possible.",
                    "label": 0
                },
                {
                    "sent": "So we have the following bounds, and Furthermore we have.",
                    "label": 0
                },
                {
                    "sent": "That the decision function of output by one class SVM.",
                    "label": 0
                },
                {
                    "sent": "Itself converges to the target function, F0 in the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "And at the same speed as this one, these are the same rates.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the main result that I wanted to present and maybe a few comments about this result first.",
                    "label": 0
                },
                {
                    "sent": "Will take some absolute Masters, which one K yes K depends on everything except N and Sigma.",
                    "label": 0
                },
                {
                    "sent": "OK, so K is constant with respect to N and Sigma but.",
                    "label": 0
                },
                {
                    "sent": "It really truly depends on everything else.",
                    "label": 0
                },
                {
                    "sent": "Play when you were telling us if we could take epsilon Smalls.",
                    "label": 0
                },
                {
                    "sent": "We liked it because it's probably.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, that's why I'm obliged to put an epsilon.",
                    "label": 0
                },
                {
                    "sent": "Otherwise I would.",
                    "label": 0
                },
                {
                    "sent": "This would be Infinity so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you said that case depending on Sigma or independent node is not so important.",
                    "label": 0
                },
                {
                    "sent": "Because Sigma depends on beta, yeah?",
                    "label": 0
                },
                {
                    "sent": "So I said so, so I maybe I have to say that in practice Sigma is not computable.",
                    "label": 0
                },
                {
                    "sent": "This Sigma is not really computable.",
                    "label": 0
                },
                {
                    "sent": "I mean this is.",
                    "label": 0
                },
                {
                    "sent": "This is not what we can call an adaptive result in the sense that.",
                    "label": 0
                },
                {
                    "sent": "In practice, we cannot calibrate Sigma because we don't know the regularity of the density.",
                    "label": 0
                },
                {
                    "sent": "I feel that you first have Sigma and then you compare choosing no.",
                    "label": 0
                },
                {
                    "sent": "I have I have bit I have N and so I calibrate Sigma.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Can you instead of the Gaussian RBF kernel, just use any other regularizer?",
                    "label": 0
                },
                {
                    "sent": "So at least any other RBF type regularizer doesn't mean all you really need is that the reference form.",
                    "label": 0
                },
                {
                    "sent": "Is upper bounded?",
                    "label": 0
                },
                {
                    "sent": "And one of the quantities.",
                    "label": 0
                },
                {
                    "sent": "Then they found the ticket for later.",
                    "label": 0
                },
                {
                    "sent": "Maybe we'll see that later.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, we use several properties of the Gaussian kernel here.",
                    "label": 0
                },
                {
                    "sent": "And yeah, maybe we will see what's important that that process of convergence.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could just start with the regularizer being, say, the L2 norm, plus maybe the L2 norm of the first relative and all you do is you just turn that one down so you don't really need to.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe specifically with the Gaussian RBF kernel maybe?",
                    "label": 0
                },
                {
                    "sent": "But the reason why I'm taking the Goshen kernel is that we use it in practice and.",
                    "label": 0
                },
                {
                    "sent": "But for sure it may be true.",
                    "label": 0
                },
                {
                    "sent": "It works with more general kernels.",
                    "label": 0
                },
                {
                    "sent": "At the arcade, Jason, or is just the L2 term plus a whole bunch of other terms.",
                    "label": 0
                },
                {
                    "sent": "Yeah, which banishes with which other terms you choose.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's pretty much OK. Yeah, it shows the current, that's it.",
                    "label": 0
                },
                {
                    "sent": "But I mean there's nothing really sacred about the RBF about the car, yeah?",
                    "label": 0
                },
                {
                    "sent": "So we're done with this slide, OK?",
                    "label": 0
                },
                {
                    "sent": "And Oh no, no, no.",
                    "label": 0
                },
                {
                    "sent": "So the comments is that yeah, I prefer to.",
                    "label": 0
                },
                {
                    "sent": "I'll consider those.",
                    "label": 0
                },
                {
                    "sent": "This result has no more.",
                    "label": 0
                },
                {
                    "sent": "No more than the consistency result because I really don't know if this convergence rate is optimal.",
                    "label": 0
                },
                {
                    "sent": "I have no lower bound for the moment and so.",
                    "label": 0
                },
                {
                    "sent": "I can say that we have stated the consistency of the algorithm, but I can't say these are fast rates for one class SVM.",
                    "label": 0
                },
                {
                    "sent": "A second remark is that from this L2 convergence from F Sigma hat to the target F0, it can be derived that the.",
                    "label": 0
                },
                {
                    "sent": "That's the level set.",
                    "label": 0
                },
                {
                    "sent": "Have Sigma hats.",
                    "label": 0
                },
                {
                    "sent": "Greater than 1 -- 10.",
                    "label": 0
                },
                {
                    "sent": "Converges to the level sets.",
                    "label": 0
                },
                {
                    "sent": "Let's sing.",
                    "label": 0
                },
                {
                    "sent": "So with respect to.",
                    "label": 0
                },
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "The criterion that I have described before.",
                    "label": 0
                },
                {
                    "sent": "So from then to convergence we can derive the following convergence between sets.",
                    "label": 0
                },
                {
                    "sent": "And well, this set is precisely so by definition of 0.",
                    "label": 0
                },
                {
                    "sent": "This is the density level set at level 2, Lambda times 1 minus Delta.",
                    "label": 0
                },
                {
                    "sent": "So from this result we can derive the consistency of one class SVM for density level set estimation.",
                    "label": 0
                },
                {
                    "sent": "120 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We can move through the proof.",
                    "label": 0
                },
                {
                    "sent": "So the proof begins with the split.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As usual, we split the excess of rich.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ask.",
                    "label": 0
                },
                {
                    "sent": "In different terms that we separately bound.",
                    "label": 0
                },
                {
                    "sent": "So I won't enter into details.",
                    "label": 0
                },
                {
                    "sent": "Well I don't have time so.",
                    "label": 0
                },
                {
                    "sent": "I'd just like to mention that we we introduce the excess of our Sigma risk because we know that we can say something on this.",
                    "label": 0
                },
                {
                    "sent": "Because, well, by definition of F Sigma hat, we know that F Sigma hats minimizes an empirical version of our Sigma, so we will have things to say to control this term.",
                    "label": 0
                },
                {
                    "sent": "And well, then we have to.",
                    "label": 0
                },
                {
                    "sent": "Well, it's OK.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good thing is that there are two terms that are negative.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can drop them.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, there remain three turns to bound, so the first one is the one that we wanted to introduce.",
                    "label": 0
                },
                {
                    "sent": "It's referred to as the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Summation error.",
                    "label": 0
                },
                {
                    "sent": "The second one is the funny one, so this is the regularization error.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to control it we will have to control the difference between our cages normal function and its L2 norm.",
                    "label": 0
                },
                {
                    "sent": "And finally, the approximation error, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I will be fast.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah I will.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lastly, show how to bond the estimation error.",
                    "label": 0
                },
                {
                    "sent": "This is just an application of theorem from Scotland Stein Mart in the article fast rates for SVM with Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "So here we just plug the.",
                    "label": 0
                },
                {
                    "sent": "Our are entries in their theorems and nothing more.",
                    "label": 0
                },
                {
                    "sent": "There is no there is no trick.",
                    "label": 0
                },
                {
                    "sent": "So the the only work we had to do was determining the smallest functional subspace.",
                    "label": 0
                },
                {
                    "sent": "Where we know for sure that the estimator F Sigma lies OK, so of course we know by definition of F Sigma hat that it belongs to the Dark Ages H Sigma.",
                    "label": 0
                },
                {
                    "sent": "But what we can show is that it belongs more precisely to this blue subsets, which is up to this multiplicative constant.",
                    "label": 0
                },
                {
                    "sent": "The unit ball of the span of.",
                    "label": 0
                },
                {
                    "sent": "This subset.",
                    "label": 0
                },
                {
                    "sent": "So to see that this is like Representer theorem that we use to show that the solution of the SVM can be expanded on this kind of function where X only belongs to the compact subset Big X.",
                    "label": 0
                },
                {
                    "sent": "And it's very interesting and important for us to have a compact index here, since it enables to nicely control the covering numbers of this set.",
                    "label": 0
                },
                {
                    "sent": "So here we took a result directly from Scotland and Stein Mart, so we bound recovering number of this unit bowl with respect to the Empire recall L2 norm.",
                    "label": 0
                },
                {
                    "sent": "So here T is the training sample and at this scale that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "And so we get a bound on the covering number, so maybe I don't have time to say more in any way.",
                    "label": 0
                },
                {
                    "sent": "That's not where I want to put the focus, so it turns out that.",
                    "label": 0
                },
                {
                    "sent": "Still directly using.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Children from school and steinbart.",
                    "label": 0
                },
                {
                    "sent": "We have the following bound on the excess of our Sigma risk OK.",
                    "label": 0
                },
                {
                    "sent": "So just note that this bound is parameterized by two values, P that we can play with.",
                    "label": 0
                },
                {
                    "sent": "We can take it between zero and one and also.",
                    "label": 0
                },
                {
                    "sent": "I forgot there is a number of Delta that we can take as we want it just have to be positive quantity.",
                    "label": 0
                },
                {
                    "sent": "And of course, playing with P and Delta affects the constant K1.",
                    "label": 0
                },
                {
                    "sent": "So here K1 is a constant.",
                    "label": 0
                },
                {
                    "sent": "Well, it does not depend neither on neither on Sigma, but depends on all other quantities.",
                    "label": 0
                },
                {
                    "sent": "So that's the way we bound the estimation error, so that's really there is no surprise here, just use a theorem.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've got two more.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use to bound.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the regularization error so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To control this term.",
                    "label": 0
                },
                {
                    "sent": "It's very convenient to work with the following characterization of the Goshen Ark ages, so that's what Alexander said here.",
                    "label": 0
                },
                {
                    "sent": "We can say that the Goshen Arcade is the set of continuous function vanishing at Infinity.",
                    "label": 0
                },
                {
                    "sent": "That are integrable on our D. And which have a fully transform that satisfies this integrability constraint.",
                    "label": 0
                },
                {
                    "sent": "OK, so this constraints means that for you transform.",
                    "label": 0
                },
                {
                    "sent": "Vanish is at Infinity.",
                    "label": 0
                },
                {
                    "sent": "Exponentially fast.",
                    "label": 0
                },
                {
                    "sent": "OK, because this has to be integrable so we don't allow the big frequencies.",
                    "label": 0
                },
                {
                    "sent": "And so this is characterization of the Goshen Ark HSN, the RK just known is precisely this quantity.",
                    "label": 0
                },
                {
                    "sent": "With this multiplicative constant.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so from this characterization we can show 2 interesting properties.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "When Sigma and two are ordered this way, then the corresponding are cages are ordered this way, but that means that the when Sigma gets smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "The Associated Arc HH Sigma gets larger and larger.",
                    "label": 0
                },
                {
                    "sent": "It forms a nested collections of functional spaces and at the end we almost reach L2 of RG.",
                    "label": 0
                },
                {
                    "sent": "And the second property that is more quantitative relates the arcade is Norm in eight Sigma to the arcade is Norm in H2 for two different values Sigma into.",
                    "label": 0
                },
                {
                    "sent": "So to pass from one to another, we have to.",
                    "label": 0
                },
                {
                    "sent": "To pay this price, Sigma squared over 2 ^2.",
                    "label": 0
                },
                {
                    "sent": "This is the web to pass from H Sigma to edge to.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So from those properties.",
                    "label": 0
                },
                {
                    "sent": "We can now.",
                    "label": 0
                },
                {
                    "sent": "We cannot.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See how to bound the regularization error term so this is the regularization error term.",
                    "label": 0
                },
                {
                    "sent": "So this is the difference between the Arcadis Norman the L2 norm of this function.",
                    "label": 0
                },
                {
                    "sent": "So we just use.",
                    "label": 0
                },
                {
                    "sent": "Excuse me, I just go back.",
                    "label": 0
                },
                {
                    "sent": "We just use this inequality.",
                    "label": 0
                },
                {
                    "sent": "With tool equals.",
                    "label": 0
                },
                {
                    "sent": "We take 2 = sqrt 2 Sigma one.",
                    "label": 0
                },
                {
                    "sent": "OK, we take these values and this gives.",
                    "label": 0
                },
                {
                    "sent": "The following inequality.",
                    "label": 0
                },
                {
                    "sent": "And then using the expression of the of the dark Ages known of the functions.",
                    "label": 0
                },
                {
                    "sent": "We just easily see that.",
                    "label": 0
                },
                {
                    "sent": "This quantity is precisely equal to the L2 norm of F0.",
                    "label": 0
                },
                {
                    "sent": "So this is very easy computing only.",
                    "label": 0
                },
                {
                    "sent": "You just have to use the characterization of the norm of the dark Ages in terms of the fully transform.",
                    "label": 0
                },
                {
                    "sent": "So eventually here is the way we control the regularization error.",
                    "label": 0
                },
                {
                    "sent": "So it's essentially in Sigma squared over Sigma 1, ^2.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here we are.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last thing is to proximation.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is more conventional.",
                    "label": 0
                },
                {
                    "sent": "So this is what we have called the approximation error term.",
                    "label": 0
                },
                {
                    "sent": "So in with one or two lines of computing, we related with L1 distance between FO and its convolution with the kernel.",
                    "label": 0
                },
                {
                    "sent": "The Goshen kernel.",
                    "label": 0
                },
                {
                    "sent": "And well then.",
                    "label": 0
                },
                {
                    "sent": "This L1 distance can be bounded.",
                    "label": 0
                },
                {
                    "sent": "By the modulus of continuity of F0 at Scale Sigma, one well to get that.",
                    "label": 0
                },
                {
                    "sent": "You only use the very.",
                    "label": 0
                },
                {
                    "sent": "Standard equality and inequality's.",
                    "label": 0
                },
                {
                    "sent": "Fubini's theorem, holders inequality.",
                    "label": 0
                },
                {
                    "sent": "The fact that the kernel Gaussian kernel integrates to one.",
                    "label": 0
                },
                {
                    "sent": "And what else?",
                    "label": 0
                },
                {
                    "sent": "Well, things like that it's it took us 12 lines of computing, but not so difficult computing.",
                    "label": 0
                },
                {
                    "sent": "And then while we use the assumption that we made on the on the modulus of continuity.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here we are.",
                    "label": 0
                },
                {
                    "sent": "We're almost there.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have this global bound which holds with high probability it's not written here by lack of space, but this holds with probability at least one minus exponential minus X, so that's why there is an X here.",
                    "label": 0
                },
                {
                    "sent": "And we now we have to optimize this bound with respect to the four parameters.",
                    "label": 0
                },
                {
                    "sent": "That is with respect to P, 2D2 Sigma and two Sigma one.",
                    "label": 0
                },
                {
                    "sent": "And if we do that, we find the result that was stated before.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So some.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Concluding remarks.",
                    "label": 0
                },
                {
                    "sent": "So here we I insist we have been working with the regularization parameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "That was held fixed.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It was not vanishing at Infinity.",
                    "label": 0
                },
                {
                    "sent": "It was a fixed parameter and the regularization intensity that we put.",
                    "label": 0
                },
                {
                    "sent": "Was only tuned through the choice of Sigma.",
                    "label": 0
                },
                {
                    "sent": "Just just playing with Sigma enables to control the amount of regularization that we want to put.",
                    "label": 0
                },
                {
                    "sent": "So this is the first mark.",
                    "label": 0
                },
                {
                    "sent": "Second remark is that well, considering the results that we have, we can say that one class SVM belongs to.",
                    "label": 0
                },
                {
                    "sent": "Well in this case when we use it with the Gaussian kernel, it can be considered as belonging to kernel density estimator family such as parsing window.",
                    "label": 1
                },
                {
                    "sent": "And maybe a practical remark.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The in practice, so this is a.",
                    "label": 0
                },
                {
                    "sent": "So the SVM decision function is close to this purple curve and we see that it's really not good to threshold this function at level one.",
                    "label": 0
                },
                {
                    "sent": "Because of this flat part.",
                    "label": 0
                },
                {
                    "sent": "So this is really an argument to say that you really have to lower the threshold and to take 1 minus Delta.",
                    "label": 0
                },
                {
                    "sent": "When you use the one class SVM for.",
                    "label": 0
                },
                {
                    "sent": "Anomaly detection or whatever.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "So this idea of lowering the thresholds was already suggested in the in the original paper.",
                    "label": 0
                },
                {
                    "sent": "And while here it is justified in another way.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And last remark, if you read the paper that we wrote, you will see that we didn't directly study the one class SVM, but actually we studied were not as we analyzed.",
                    "label": 0
                },
                {
                    "sent": "Sorry, more general family of algorithm, namely.",
                    "label": 0
                },
                {
                    "sent": "Algorithm that minimizes such.",
                    "label": 0
                },
                {
                    "sent": "Empirical regularized functionals.",
                    "label": 0
                },
                {
                    "sent": "HDMI squared.",
                    "label": 0
                },
                {
                    "sent": "Hi why I?",
                    "label": 0
                },
                {
                    "sent": "So actually we we propose an analysis of this kind of algorithm.",
                    "label": 0
                },
                {
                    "sent": "That solves this optimization problem so.",
                    "label": 0
                },
                {
                    "sent": "It encompasses the two class case because here the yiz are labels of the data and we don't only consider the hinge loss function, but general convex loss functions and this results remains valid.",
                    "label": 0
                },
                {
                    "sent": "Simulation over eight Sigma.",
                    "label": 0
                },
                {
                    "sent": "OK. And OK, so that's it.",
                    "label": 0
                },
                {
                    "sent": "So actually, the results that we proposed is still valid for for convex loss function.",
                    "label": 0
                },
                {
                    "sent": "Oh no no.",
                    "label": 0
                },
                {
                    "sent": "No more evil said because when you change five you have to compute the associated target function F0.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, for each file you've got an associated target function F0, and that's true that you converge to this function.",
                    "label": 0
                },
                {
                    "sent": "In the West to normal.",
                    "label": 0
                },
                {
                    "sent": "So in the for instance that shows that two class SVM is consistent for classification.",
                    "label": 0
                },
                {
                    "sent": "Even when we don't take a regularization parameter, that vanish is at Infinity.",
                    "label": 0
                },
                {
                    "sent": "If you use two class SVM with a fixed regularization parameter and with a well calibrated bandwidth Sigma then.",
                    "label": 0
                },
                {
                    "sent": "The classification risk of the estimator will converge to the smallest achievable.",
                    "label": 0
                },
                {
                    "sent": "Convert went well then.",
                    "label": 0
                },
                {
                    "sent": "Yeah, when yes when N goes to Infinity and Sigma goes to zero at the right speed.",
                    "label": 0
                },
                {
                    "sent": "Yesterday.",
                    "label": 0
                }
            ]
        }
    }
}