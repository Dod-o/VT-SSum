{
    "id": "whmixqvom73jlsbmc35hj23pqbfywhm2",
    "title": "Inference for PCFGs and Adaptor Grammars",
    "info": {
        "author": [
            "Mark Johnson, Brown Laboratory for Linguistic Information Processing, Brown University"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Programming Languages"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_johnson_ipcfgag/",
    "segmentation": [
        [
            "Thanks very much everyone, sorry for that delay."
        ],
        [
            "OK, so actually I often have a slide here about, you know the joke about the drunk looking under the lamppost.",
            "I think you know part of the story of why I'm looking at this stuff is because the error other colleagues here at NIPS have invented these nonparametric Bayes methods.",
            "And so I'm looking to see this is like a new lamp post and I'm looking to see if there's anything interesting under that lamp post.",
            "So I'm going to start off talking a little bit about just straightforward PC FG rule production estimation, and then because that serves as the backbone for the nonparametric Bayesian method and will talk about generalizing PCF's making them nonparametric.",
            "And so one of the key ideas is that even though there's an unbounded number of potential rules, because any parse or any finite number of passes can only use a finite number of these rules in constructing their past trees, that means that we only need to explicitly represent in a sampling approach, we need to explicitly represent the rules that have actually been used, even those, even though there's an infinite number of potential rules.",
            "And then if there's interest in time, I'll actually go over some of the methods that we found that are very useful for actually making inference in these things practical."
        ],
        [
            "OK, so just to start off a very brief review of what a probabilistic context free grammar is.",
            "You've got a series of rules associated with probabilities and the probability of a tree is the product of the probabilities of the rules that are used to build it."
        ],
        [
            "You probably all know about maximum likelihood estimation.",
            "It's very simple.",
            "All you have to do is count the number of times that each rule is used.",
            "This is assuming that you're actually given the tree Bank of trees.",
            "You count the number of times that each rule is used.",
            "And then compute the relative frequency of those things.",
            "The family has a certain number of problems.",
            "Things like you know accidental zeros can lead to probability estimates of zero, which was often."
        ],
        [
            "Good, so a Bayesian approach.",
            "Instead of picking one single value, actually instead says well what you should really do is estimate a posterior distribution of the set of possible values.",
            "If you pick Dursley priors, then modulo some interesting questions.",
            "The posterior is also directly.",
            "So down here I'm just simply showing a few dish lies.",
            "There's the uniform dish.",
            "Lay down here.",
            "You know if we happen to see rules appearing two times in one time, that would give us a posterior of 3 two which would look like this very broad curve.",
            "If instead of seeing it two times in one times we saw it 20 times and 10 times, that would give us this much more peaked distribution here."
        ],
        [
            "OK, one fun thing you can do with their slaves is that if you set the dash light parameter Alpha towards zero, what you actually start together.",
            "These very very peaked distributions like this and you can actually use this in terms of grammar induction to say Gee, maybe I've got some super set of rules of which I believe only a very small number actually.",
            "Going to be used in the grammar that I want to try and learn, so I might put a dish like prior on like you know, with some very small value of Alpha and then we get a peak distribution which says in effect G. You know, I think the grammar includes some of these rules, but I don't know which I want the estimated."
        ],
        [
            "The wanderer.",
            "So.",
            "Things get more interesting when you actually start trying to estimate from strings alone, instead of estimating from a tree bank.",
            "And then, as I think many of you guys know, basically in general there's no close form solution anymore for the rule probabilities, but instead we've got these iterative procedures that at least find local maximum.",
            "And I must, most of you will probably know the EM algorithm.",
            "Turns out that there's also sampling algorithms for achieving the same sort of thing, that is estimating the rule probabilities.",
            "So here's a very simple."
        ],
        [
            "Gibbs sampler that basically says somehow guess some past trees may be randomly assigned past trees to strings and then forever repeat the following things.",
            "Treat your past trees as if they really were the true tree bank.",
            "So compute your real rule counts from them.",
            "Sample your rule probabilities according to this Dursley formula over here, and then for each string, Now go back and instead of actually assigning it, the most likely parse tree instead, just simply sample APA's tree.",
            "From the string X and from the rule probabilities that you just sampled up here before, and this is a Gibbs sampler which is alternating between sampling the rule probabilities in the past trees and so according to the MCMC theory, after a certain burning.",
            "What you'll actually start.",
            "Getting our samples of real probabilities in trees that are distributed according to the basic."
        ],
        [
            "Posterior.",
            "So that's great.",
            "In fact, actually one of the things that's.",
            "Very useful to do is to notice that you can actually integrate out the rule probabilities, which is actually had a paper rejected when I was describing this collapsed Gibbs sampler, and the reviewer said, you know, look, you said the goal is to estimate the rule probabilities.",
            "That's why you're doing it.",
            "But if you integrate out the rule probabilities where the rule probability is gone.",
            "So in fact actually.",
            "So let me sort of just say first of all, we can sample the parse trees and then if you actually do want the rule probability."
        ],
        [
            "You go back and you use this same formula here.",
            "Compute the rule counts and then sample the rule probabilities from those."
        ],
        [
            "But there's actually a technical reason why it's actually handy to to integrate out these real probabilities, and that's when we start to move to grammars where there's an infinite number of potential rules.",
            "This rule probability vector over here will be infinitely long, so we won't want to represent that explicitly.",
            "But if we can integrate that out, as I said before, that one of the key ideas is that the parse trees are finite for any finite corpus.",
            "So even though we may have an infinite number of potential rules, 'cause we've integrated out the rule probabilities, we won't actually have to represent them, and so the collapsed Gibbs sampler basically says like this, it says.",
            "Basically, for each sentence XI in the training data.",
            "Replace your current guests of the past tree with a sample from the conditional distribution of pastor is given the string XI and this team minus I means the set of past trees for all other strings except XI.",
            "And one problem here is that the dynamic programming algorithms that are used to sample pass trees actually don't work in this particular case, and that's becausw.",
            "This distribution actually isn't a PCF distribution.",
            "One way to sort of think about this is think about what the probability is of the noun phrase expanding to dogs down here.",
            "That depends on the number of times that noun phrases have expanded to other things in the rest of the corpus.",
            "In particular, is going to depend on whether or not you decide to analyze this occurrence of dogs over here as a noun phrase, or something else.",
            "So what that means is that this probability here is coupled to the analysis that you chose over here.",
            "But that breaks the context freeness property that our dynamic programming algorithms depend."
        ],
        [
            "Yeah gone.",
            "These coincidences.",
            "So you can alright so you can.",
            "So first let me give you the fix and then let me basically agree with you.",
            "They don't know except in a small corpus.",
            "So the fix is actually to use site.",
            "Well there are many fixes, but one fix is to use a metropolis Hastings acceptance rejection procedure, which is where.",
            "Effect you sample from a proposal distribution.",
            "And we actually use the PC FG that's computed from all the other pastors, ignoring what's happening inside of this past tree as the proposal distribution.",
            "And then apply a metropolis Hastings acceptance rejection procedure to, in effect, sort of transform the samples from the CFG distribution to the samples from the distribution that we really care about.",
            "And in fact, in practice, except when the corpus is very small, the acceptance probability is very, very high.",
            "So when I actually run these things on, say, a corpus of 2030 thousand sentences, you might reject, you know, three or four sentences per per iteration for the pass, so.",
            "So David's points basically correct.",
            "OK, so."
        ],
        [
            "So the.",
            "The basic algorithm then sort of looks like this.",
            "Initialized by assigning pass trees to string somehow and then repeat forever.",
            "Pick a sentence inside of your training data.",
            "Compute rule counts from all the other trees for all the other sentence is computer proposal grammar from these rule counts.",
            "Sample a tree.",
            "According to the conditional distribution, with this proposal, grammar and then replace the current tree for the string XI with the sample tree according to the Metropolis Hastings, except project proceeds."
        ],
        [
            "OK, this is like a little interesting question here, which I think I'll just skip."
        ],
        [
            "OK, so of course the really interesting thing is not so much learning rule probabilities as actually learning the rules themselves and.",
            "So what we'd like to have is is some procedure which takes just strings as input and actually produces rule probability is rules as well as real probabilities and the basic method that I think has been used.",
            "I mean, I want so successfully used because I think this is an incredibly hard problem and I don't think anybody's really got a successful method for.",
            "It's far as I know, maybe somebody here does.",
            "Sort of is what I sometimes call the Chinese Revolution approach to these things, which is you let 1000 Flowers let 1000 Flowers bloom, and then you as the central committee come in and kill off a whole lot of them, and then you then encourage another 1000 Flowers to bloom and like that, right?",
            "So guess an initial set of rules reuse.",
            "Use your EM or whatever it is procedure to estimate the rule probabilities from the strings.",
            "Prune the low probability rules and then down.",
            "Here is another step, which is where you need to have a procedure.",
            "This outside of the estimator.",
            "Which proposes additional potentially useful rules.",
            "And I actually think one of the one of one of the strengths of these nonparametric Bayesian methods is that they seem to be able to provide a more systematic approach.",
            "You know, in the all in the previous approaches, this step here was usually a heuristic procedure that wasn't connected to the objective function that was motivating the learning procedure.",
            "And so the nonparametric Bayesian methods actually permit us to sort of wrap all these things together and integrate these two steps."
        ],
        [
            "OK.",
            "So basically one standard way of coming up with nonparametric methods is to take parametric methods and then try to generalize the number of parameters to to Infinity.",
            "So with probabilistic context free grammar, there really are sort of two obvious ways of becoming non parametric.",
            "So one method is to let the number of nonterminals grow unboundedly.",
            "So someone gave you an original grammar with S goes to end PvP, but you say, well, you know really, actually, maybe there's not just one type of sentence, there's really.",
            "20 types of sentence is, and there's really 40 types of noun phrases, and you're going to try and learn how many different types of sentences and noun phrases there are, and so in fact, you're actually going to try and learn rules like the following sentence of type 12 goes to noun phrase of type 7.",
            "Right?",
            "And this actually leads to the Infinite PC FG and a bunch of extensions like that.",
            "What I'll be talking about here today is is an approach where I actually let the the set of rules themselves, right the set of productions grow unboundedly.",
            "And so the idea really is to use a grammar or a meta grammar to generate potential rules.",
            "Potential expansions of some non terminal, and then I'm going to try and learn which of those rules or are in fact actually useful.",
            "So in fact actually might the rules of my final object level grammar are going to be whole subtrees of the matter grammar.",
            "So what I'm really going to be learning is something like a tree substitution grammar where we learn the probabilities of entire fragments.",
            "We learn which fragments are useful and we learn their probabilities.",
            "I should point out, there's no theoretical reason why both things can't be done together, although I actually wonder whether if you did try to combine them, whether you'd actually get some competition between these two mechanisms might not be clear which Gen."
        ],
        [
            "Causations correct?",
            "OK, so all the examples I'm going to be talking about are actually going to be involving learning things like word segmentation and.",
            "And morphology.",
            "So you guys are probably saying, why isn't he learning syntax and semantics with this stuff, so I will admit so far I really haven't got.",
            "You know, we've tried learning dependency grammars.",
            "The results aren't bad, but they're not.",
            "Great, you know it's a bit like what's the joke.",
            "The dog walking on its hind legs, perhaps amazing that it can be done at all, but still not what you'd call the success, you know.",
            "Whereas I think actually with this low level stuff.",
            "That in fact actually."
        ],
        [
            "It is kind of cute and I think also there's a lot to actually learn by actually looking at learning in very simple situations.",
            "So let's actually sort of start off with a very simple case, so let's just imagine that I just wanted to learn how to parse words into their morphemes.",
            "So what I wanted to do is to learn I'm going to want to learn representations like this for the word talking.",
            "It says that essentially talking consists of a stamp followed by a suffix talk.",
            "Is the stem ING is the suffix, so the first thing to note is that if I just simply use this context free grammar I can represent.",
            "That segmentation perfectly fine, but I'm hoping that at least all the grammar geeks in here are sort of boggling at this stage because there's actually no hope that this grammar could actually learn how to segment talking into talk plus ING, and that's becausw.",
            "The rules.",
            "The units of generalization in this grammar rules and the rules are way too smaller.",
            "Unit, right?",
            "So this grammar here can learn what the probability is that a character is a T or it can learn what they can learn.",
            "Essentially a geometric distribution over the length of characters, but that's about it.",
            "Can't actually learn that talk is a stem because that generalization isn't aligned with anyone single rule."
        ],
        [
            "So one thing you might say is OK, well, let's actually change the grammar.",
            "Let's actually introduce a rule for every possible scam, and another rule for every possible suffix.",
            "And so then we've learned a tree that looks something like this.",
            "So this tree here now actually sort of this grammar looks like it's actually got a hope of doing learning this thing, so you could actually learn what the probability is that talk is in fact actually a stem.",
            "The only thing is that whenever anybody writes up a grammar that's got little bits of English in it, I advise you to keep your hand on your wallet 'cause they might be trying to pull another fast one on you as well, right?",
            "This isn't a PC FG anymore, right?",
            "There's now an infinite number of rules, but what's actually interesting here is that.",
            "There isn't actually really a practical problem here.",
            "If I'm allowed to look at the corpus ahead of time, my training data, I can actually identify all the possible prefixes and all the possible suffixes in it, right?",
            "So in fact, actually, given the data, I can actually identify which subset of these infinitely many different rules here could actually possibly be useful.",
            "So this keep that thought in mind.",
            "I mean, the idea is that the data itself actually supplies us with enough information to know which rules can actually possibly be useful.",
            "So and really this nonparametric Bayes, I think sort of gives us a way of formalizing that intuition and formalizing it in situations where it's not quite so straightforward to see which rules are the ones you should restrict."
        ],
        [
            "Attention to.",
            "OK, alright, well so let's just say we've done that.",
            "We look at the data we pick the set of rules that can possibly be useful in this data and then we throw this into our favorite maximum likelihood liner like EM or something like that.",
            "In fact, actually, I've done that.",
            "And what you wind up getting your analysis that look like this.",
            "Every word is analyzed as a stem on its own.",
            "And there's a mathematical reason for why that's the case.",
            "You can actually show that the maximum likelihood solution is this.",
            "This this grammar like this actually is going to fit the empirical distribution in your data exactly, and you can show that in fact actually that the you can see up there.",
            "It's in terms of KL divergent.",
            "You can actually prove that that is in the ML solution, so it's not.",
            "It's not a search problem here, it's actually a model problem."
        ],
        [
            "Well, you know we were just talking about how great Bayesian reasoning was and how great dursley's are for enforcing sparsity.",
            "What we might want to do is to put additional a prior that's actually going to prefer, instead of letting every word be its own separate morpheme.",
            "Maybe in fact, if we use the Dursley prior, it would actually prefer to have a small number of morphemes, and that's going to force it to generalize, so you can."
        ],
        [
            "Do that, here's the results for some number of different deuschle parameters.",
            "It's not exactly great, right?",
            "I mean you can see every so often, it's sort of managed to pull off and ING suffix or needy suffix, but it's it's really pretty bad, yeah?",
            "This is for a particular sample.",
            "This is in fact actually all the words from all the verbs from the Wall Street Journal.",
            "Yes, yes yes, that's right.",
            "This is at this stage.",
            "This is just this is just one sample.",
            "So yes, so So what UI is saying an in fact actually is.",
            "It's a good point because.",
            "Just in case I don't get to it.",
            "In fact, actually you can improve the accuracy of these things by averaging over multiple samples, and in fact actually that's now what I do with these adaptive grammars is run them for generally around 2000 iterations.",
            "I'll run eight runs.",
            "And then collect from each round the last 1000 samples.",
            "So I'll actually get 8000 samples and then average over those and that improves accuracy.",
            "So right now in doing a different task and doing a word segmentation task.",
            "So I'm getting accuracies of around 85% and that takes it up to about 87%, so that's it.",
            "Very sensible and very good idea."
        ],
        [
            "OK, the point of this plot here is actually just to sort of show that there's a huge difference in log likelihood.",
            "Cost of the posterior probability between the true suffixes, which are this green line.",
            "Here in this little problem actually know what the true segmentation ought to be.",
            "And this this line.",
            "Here is the line which the sampler itself is actually finding, so you can see.",
            "And this is like a gap of several, many, many millions of times, right?",
            "So you probably would have to wait like since the beginning of the universe or something like that to actually find a sample which actually agrees with the true sample, right so?"
        ],
        [
            "And in fact, actually there's a reason for that, which is that the grammar itself has got this top level rule up here.",
            "Word goes to stamman suffix, which essentially says that the distribution of stems should be independent of the distribution of suffixes.",
            "Here I'm actually sort of just picked 5 very high frequency verbs, so I don't not plotting error bars here, but if I did plot error bars they would be very, very tiny.",
            "Wouldn't even see them on this plot.",
            "And you can see this assumption up here is just sort of nowhere close to true, right?",
            "There's huge variation in the frequency of a suffix given the stamp.",
            "So the context freeness assumption inside of this model is sort of very heavily violated."
        ],
        [
            "So one approach that we've taken to deal with that essentially is to say, Gee, maybe what you should be doing is type based inference instead of token based inference.",
            "So here the idea is Gee if instead of actually sort of collecting all of the verb types, imagine we just collected sorry Oliver tokens.",
            "Imagine collected all of the types and we just tried to learn from types themselves.",
            "If there's, say, roughly 4 suffixes for every verb, then the probability of seeing anyone suffix like ING.",
            "Is going to be about .25 no matter what the verb is and no matter what the actual token frequency of that suffix was."
        ],
        [
            "And so in fact, actually, when you try to learn from types rather than tokens, what you also notice is that in fact, actually now over a wide range of different values.",
            "For these alphas we wind up getting pretty good segmentations and the segmentations that are sort of strictly speaking wrong like this guy up here, you know, really it's include plus S in fact, actually you can see the reason why it's done that is so it can actually capture this generalization down here.",
            "So it's because the model doesn't really have any notion of.",
            "Of morphophonology, but it's actually sort of forced to make a MIS analysis up here in order to capture the other correct analysis down there.",
            "And."
        ],
        [
            "In fact, actually when we do this other plot Now what you can actually see is that over fairly wide range up here we're actually saying that the true suffixes are pretty close to what the optimal suffixes the most most probable suffixes are from the from the sampler."
        ],
        [
            "So I think there's two different.",
            "Rules two different sort of rules for two different ideas.",
            "I want you to take away from this little example, so one is that P CFG rules are often too small to be effective units of generalization.",
            "So what we'd like to be able to do is to generalize over whole groups of rules, and we'd also like the units of generalization to be chosen based on the data.",
            "The second thing is that this type based inference can often mitigate the overdispersion that will wind up saying.",
            "And so this leads, I think, to a hierarchical Bayesian model where the context free grammar itself sort of generates the types.",
            "And then we use another process to replicate the types to actually generate the tokens.",
            "And and adapter grammars are sort of an attempt to try and do both things together, so they're going to try and learn the probability of entire subtrees, and then we're going to actually use the grammatical hierarchy to define a Bayesian hierarchy from which type based inference should emerge, and a big part of the motivation for this was that when Sharon Goldwater was at Brown, doing her thesis work, which inspires, you know a lot of this stuff.",
            "She she implemented a hierarchical dislike process for actually solving that morphology problem that we just talked about.",
            "She also implemented another one to solve a word segmentation problem, and I go to, you know, I was her advisor, so I said, Gee, you know, and you could combine these two together, right?",
            "So we can have a model that does both simultaneously as you guys all know, don't make me do that.",
            "These are these are two horrible C++ programs you know and the data structures are all completely inconsistent with each other.",
            "How I'm going?",
            "Yeah, there's gotta be a better way, right?"
        ],
        [
            "OK, so onto adapter grammars so this is this is my attempt to come up with a better way right?",
            "So the idea is that that reason adaptor grammar defined by context free grammar rules as as in a context free grammar.",
            "So you the grammar designer needs to actually sort of come up with a set of base rules that generate the structures that you think are going to be useful.",
            "You also need to declare a subset of the non terminals to be adapted.",
            "And you need to give each adapted nonterminal concentration parameter.",
            "If you're building a additional process adaptor grammar.",
            "So unadapted nonterminals.",
            "If you don't declare any non terminals to be adapted, then you what you wind up with is exactly a PC FG.",
            "Because the unadapted nonterminals expand.",
            "Just, you know, using a rule an ordinary rule using the using the current estimate of what the rules probabilities.",
            "So this is just like in an ordinary PC FG.",
            "But with an adapted non terminal something interesting happens.",
            "So in with every adapted Mount Terminal we remember how many times with its expanded completely to a full subtree before so.",
            "The adapted Montana was going to expand to a subtree with probability proportional to the number of times of that subtree is been generated previously.",
            "So that's why sometimes people describe these adaptive grammars as being like caching grammars that they remember absolutely everything that they've generated in the past.",
            "But there's also some probability using this dish like concentration parameter over here, have actually generating it completely fresh tree.",
            "So at the very beginning of inference, when essentially you haven't generated very much stuff before, you're almost always generating fresh trees.",
            "But as the number of trees that's been generated grows, you're actually going to be far more likely to reuse existing."
        ],
        [
            "So here's a simple little run.",
            "In this morphology example in here.",
            "This is in a Chinese restaurant process style sampler, so."
        ],
        [
            "These are funny Chinese restaurants in here, so the first customer walks in.",
            "And you should think about the rules that expand into.",
            "So there's one restaurant per adapted non terminal.",
            "And the rules that that expand the adapted nonterminals are kind of like recipes.",
            "So this recipe says, Gee, to build a word, give me a stamina suffix and combine them together.",
            "So we need to give this first customer some food on the table over here.",
            "So as I said, this is a funny Chinese restaurant.",
            "What it does is it sends out Waiters."
        ],
        [
            "To the stand in the suffix restaurants and they sit down at their tables and."
        ],
        [
            "Side, yeah I want to get a stamp and I want to get a suffix so we generate us and these guys here are just strings of phones so we generated string of phones to give a stamina suffix tree and then we then bring them back."
        ],
        [
            "And set them up in this Chinese restaurant table up here and so this guy up here gets his dishes.",
            "The is combined out of the stamina suffix that were produced down in this restaurant down here.",
            "The second customer walks in."
        ],
        [
            "The word restaurant maybe sits at a fresh table, so we need to come up with a fresh dish for that person.",
            "So again, we order out, go to the stamina."
        ],
        [
            "Fix restaurants so now what's happened is that we've decided to generate a fresh stem.",
            "But because this customer is sitting down at this same suffix table down here, we reuse the soft."
        ],
        [
            "So we generate a fresh start."
        ],
        [
            "So maybe dog.",
            "And this generates dogs up here.",
            "OK, the next custom."
        ],
        [
            "Walks in the probability of picking a table is proportional to the number of customers sitting at it, so you might pick this first table up here.",
            "In this case, there's already some food on it, so we just simply generate cats.",
            "Here's the sequence of words that's being generated down there at the bottom."
        ],
        [
            "And someone like that.",
            "OK, so again, so unadapted nonterminals expand by picking a rule and recursively expanding to their children.",
            "Adapted Nonterminals can expand in two ways.",
            "Again, just as by picking a rule and recursively expanding children, or else by picking a previously generated holtry with probability proportional to the number of times of the Triesman previously generated.",
            "So in terms of the implementation, each adapted non terminal has a Chinese restaurant process, or there's a generalization of that called the Pitman Yor process.",
            "The case is the previously generated subtrees.",
            "CFG rules in effect sort of determine the base distributions of these CRPS orbit manual processes.",
            "What's interesting is that the trees that are generated by an adaptor grammar aren't independent, so in particular, if an adapted subtree is been used frequently in the past, it's actually more more likely to be used again.",
            "So this actually means that the adaptor grammar learns from the trees that it's previously generated, but it does have."
        ],
        [
            "Property of exchangeability down here.",
            "OK. Becausw the probability of an adaptive non terminal expanding to some new subtree is proportional to the number of times it was seen before.",
            "This leads to a rich gets richer dynamics which leads to zipf distributions.",
            "Particularly in the manual process.",
            "And it also has another interesting property which PCF's don't have, which is that useful compound structures can actually be more probable than their parts.",
            "And then you actually learn the base PC after rule probabilities from the table labels.",
            "This is a technical point which actually means that you're actually learning the rule probabilities from types rather than from tokens in general.",
            "OK."
        ],
        [
            "So it's I think I've got enough time to.",
            "Keep on going right, so let me talk a little bit about another application of these.",
            "Probably the application we've looked at most of all, which is unsupervised word segmentation.",
            "So here the idea is that the.",
            "The inputs are finding sequences with sentence boundaries, and the task is to identify word boundaries and hence the words in this in this input here and there's two sorts of cues that are very useful.",
            "Sharon Goldwater pointed out the usefulness of trying to track into word dependencies, and Margaret Fleck pointed out the importance of funnel tactics.",
            "So here you can see there's a potential boundary between each phone, but you know only some of these.",
            "This corresponds to you want to see the book."
        ],
        [
            "So again, we can actually represent the segmentation of the book.",
            "Into words with a very simple PCF G. The structures from the very simple PC FG.",
            "But again you should have the same response.",
            "This grammar can't possibly learn that the and book our words because that's a property.",
            "The entire tree, rather than a property of any single rule in here."
        ],
        [
            "So again, we can apply the same sort of trick.",
            "We can say Gee, maybe in fact, the grammar really ought to actually look something like this.",
            "Word goes to all possible phone strings.",
            "And again, now there's an infinite number of P CFG rules, but in fact actually again, once we see the finite training data, we actually know which rules could possibly be useful."
        ],
        [
            "And so in fact, actually, if we write an adaptive grammar.",
            "That just simply says OK, look.",
            "Actually I want to adapt the entire word subtrees.",
            "What that means is we'll actually learn the probability, not just of the word goes to phonemes rule, but in fact they actually have this entire tree here, and this entire tree over here.",
            "So in fact actually in the plot seals in the trees you'll see further on.",
            "In general what I'll show is the adapted Nonterminals will be indicated in the grammars by underlining and also by highlighting as well.",
            "So adapting words essentially means that the grammar learns the probability of the entire subtree independently, and so this model here essentially is a unigram model of word segmentation.",
            "It says that the sentence consists of some sequence of words.",
            "There's no relationship between them.",
            "So we just generating them ascentia Lee at random and this was in fact actually a model that Michael Brent proposed for this task.",
            "And in fact, we get essentially the same score that you get if you run.",
            "And an adapter grammar unigram segment or around 50%.",
            "Fifty 6% of the tokens are in fact correct."
        ],
        [
            "Um?",
            "And in fact, the actual grammar that you learn winds up looking something like this, so the actual learned adaptor grammar contains about 1700 rules or subtrees.",
            "So here the original rules with their posterior counts and then down.",
            "Here are some of the other rules that we've learned, so you learn you an in and with the doggie in the in the House.",
            "So I'm like that, right?",
            "So, and this is in fact actually gives you some idea of what the internal data structure actually look."
        ],
        [
            "Like one of the problems with this model is that it tends to under segment.",
            "This is something again that people had notice earlier, and part of the reason for that is because there's very strong into word dependencies.",
            "So in this particular corpus, if you hear the word doggy, you're probably going to hear the word the in front of it as well, and so a unigram model really has two choices.",
            "It can either generate them as separate words, in which case it fails to capture that generalization, or else it says OG.",
            "Maybe I should just simply posit.",
            "A larger word posit the doggy is a single unit and what Sharon Goldwater."
        ],
        [
            "Did in order to capture that generalization was to actually learn a bigram model.",
            "Turns out for technical reasons, we can't learn bigram models over in adaptive grammars.",
            "We've only got a finite number of adapted nonterminals, but we can do something that's.",
            "That it seems to do just as well, and that is we can learn collocations.",
            "So we say that Gia sentence consists of a sequence of collocations, collocations consists of some sequence of one or more words in a word consists of one or more phonemes.",
            "So now what we wind up learning is something that looks like this.",
            "You want to down here so we're still under segmented one too.",
            "But we've learned that that's a you want to as a whole colocation, and the book is also learned as a colocation.",
            "So this gives the adaptor grammar, the ability to learn that there's a relationship between these two words, but still also posit these things as independent words, and in fact, actually wind up with you run that, you end up getting a segmentation accuracy of about 76% instead of.",
            "56% and this is about the same improvement that Sharon Goldwater found with her bigram model."
        ],
        [
            "OK, so as I mentioned before.",
            "Margaret Fleck, who's also been working on this problem, not with adapter grammars, but just sort of, you know, on our custom custom built models.",
            "One of the things that she notes is that.",
            "Trying to learn syllabic structure.",
            "Trying to, you know, noticing that words typically consists of sequences of syllables.",
            "Syllables begin with a sequence of consonants, followed by some vowels, followed by some other constants.",
            "If you can learn those things.",
            "Then in fact, actually they can be very, very powerful cues to where the word boundaries are.",
            "So this now is an adaptor grammar.",
            "You know, once you've got this tool, you can sort of go wild, right?",
            "I mean so this adaptor grammar now says, Gee, that a word consists.",
            "Sorry, collocation consists of a sequence of words.",
            "A word now consists of a sequence of syllables.",
            "So this says us and these ones here in fact.",
            "Actually it's maybe a little weird, even though I don't know where the word boundaries are, my grammar can actually try and distinguish between syllables that our initial syllables that are final and syllables that aren't word initial word final, because we know that in fact actually there are different onsets and coders associated with syllables that our initial and final, and in fact actually.",
            "If I build that in what I wind up learning, then is so I actually need to tell it.",
            "Now the difference between consonants and vowels.",
            "So that's all I tell it about.",
            "I just simply tell it that an onset consists of a sequence of consonants.",
            "And decoder also consists of a sequence of consonants.",
            "Putting that in actually improves the segmentation accuracy up from what was at 76% to about 87%.",
            "Yeah, 5 minutes OK good."
        ],
        [
            "OK, so this is now we're sort of building sort of really quite rich trees, and as I said this, as far as I know this is the best word segmentation accuracy that's up there."
        ],
        [
            "OK, so in the last five minutes let me just say a little bit about the estimation techniques that are used here.",
            "In early work that I've done, I in fact actually would, so there's a number of parameters.",
            "Hyperparameters like the concentration parameters for the dash, lights that have to be learned in early work.",
            "What I've done is I just simply clamp those values as the models get more and more complicated.",
            "You don't necessarily actually want to have the same value for for all the parameters.",
            "The concentration parameters which are associated, say with the word or the collocations should not necessarily the same as the optimal ones that are associated with syllables or something like that.",
            "And here what I'm basically saying is that in fact, actually we can go Bayesian about those, put, put, put prior Bayesian priors on these parameters, and in fact actually that produces a fairly substantial performance improvement.",
            "So basically with tide parameters.",
            "On the most complex models I was getting about 78% F score.",
            "If you if you work with a Chinese restaurant process, you and sample its values go to about 84% and then if you move to this more complex Pitman Yor process, you get 87%.",
            "One of the things I think is actually sort of very encouraging about this stuff is that as the models get more complicated, actually the performance seems to improve.",
            "I don't know whether you guys, I think probably most of you guys are probably played with something like EM where oftentimes if you make the models more complicated, all you're doing is just giving more rope to EM.",
            "And it's just hanging itself, you know?"
        ],
        [
            "Will find some weird way of doing things.",
            "OK so very very quickly.",
            "Let me just sort of mention here.",
            "There's a couple of other optimization techniques.",
            "This this modal word segmentation is exactly what you I was suggesting.",
            "That's that's important for about 2 or 3% performance improvement.",
            "Exactly how you initialize seems to really matter as well.",
            "And."
        ],
        [
            "The point here is that.",
            "So so so.",
            "One thing you can wind up doing is what I call it incremental initialization.",
            "So you start off with the first string sample at a tree at random for it, and then sample the tree for the second string conditioned on the tree that you sample for the first string.",
            "That actually produces better word segmentation results."
        ],
        [
            "But if you look at the log probabilities over here, this is of eight separate runs.",
            "What you actually notice is that starting off by guessing completely random trees for everything winds up coming up with solutions that have better posterior probabilities.",
            "So in fact actually, since I'm interested in the predictions of the model, rather than just simply building a device that's got better word accuracy, I actually go for this random initialization."
        ],
        [
            "Higher.",
            "Yeah I think so.",
            "Yeah, I'm in fact what I think actually is that all these models still there still are dependencies that I'm failing to capture.",
            "So I think one of the high level messages here is that what we're actually seeing is this Bayesian models are doing better.",
            "As as we start to.",
            "As we give it, the ability to learn more generalizations that can explain away longer range dependencies, but these collocations are still a very.",
            "Sort of trivial thing right there?",
            "Still not a terribly good model of all the dependencies that are present in real natural language.",
            "So in fact I actually think what's happening here is that the models tend to under segment because they don't have any way of explaining the real interword dependencies.",
            "And then incremental initialization because it greedily searches for common substrings.",
            "In general, I think there's likely this sort of the garden path into those things earlier.",
            "And then."
        ],
        [
            "Yeah, I think I'm running out of time, so I should probably skip you so anyway, so I think.",
            "I think there is something interesting under this nonparametric Bayes lamppost and I think there's also plenty of other things to try as well.",
            "If you come to my talk, I'm giving a talk on the topic modeling Workshop where I'll be talking about the relationship between these sorts of grammars and topic models, but I think there's plenty of other stuff to do with nonparametric Bayes as well.",
            "So thanks very much guys.",
            "Time maybe?",
            "Is it that of a power again?",
            "Skeptical about using this for syntax, sorts of modifications, do you think?",
            "Yeah yeah, so in fact, actually one of the things you can sort of notice.",
            "Just pick some tree here, right?",
            "What I'm actually doing is I'm learning the whole tree right down to the words themselves.",
            "Part of the reason why I'm doing that is because I think it's important.",
            "At least for search reasons to try and make these things very heavily dependent on the actual strings themselves, I think actually maybe it might be related to the intuitions that you've had in your stuff as well, that that in fact actually you know you want.",
            "You want the generalizations that your learner can make to be to to be grounded.",
            "In the data.",
            "Tim O'Donnell at Harvard is sort of said.",
            "Gee, there's no mathematical reason why these trees that were memorizing or caching need to all cash out in the terminal string, and in fact, he's got a generalization of adapter grammars called Fragmente grammars, which basically can learn arbitrary trees.",
            "They don't have to cash out into the into the terminals, so I think actually, that that framework actually has more hope of learning syntactic generalizations, I mean this.",
            "This framework here can if you apply it to syntactic trees, can learn that a given sequence of words is a noun phrase, but it can't learn a generalization about noun phrases.",
            "You know where is Tim O'donnell's grammar can now Tim is.",
            "Just as I feared is sort of running into problems actually getting his system to learn pretty much anything.",
            "Unreal culprit.",
            "So, so he's again.",
            "He's got success with with sort of small artificial.",
            "You know small, very, very small corpora.",
            "Very, very.",
            "You know, artificial ones.",
            "Yeah, I don't know.",
            "I mean, you know computational linguistics is actually sort of moved away from trying to work with toy examples.",
            "I'm actually not so sure that that's working with toy examples is actually so bad, but if you want to work with real data, at least Tim hasn't had success with that.",
            "Is that 'cause you know?",
            "Is that because my hunch that the search problem is just really way too hard?",
            "Is the case or is there something else going on there?",
            "Maybe 10 doesn't have quite the right model?",
            "I don't know, but so in principle one can apply these things to syntax.",
            "But in practice the one person that has tried that that I know of hasn't had great success."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks very much everyone, sorry for that delay.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so actually I often have a slide here about, you know the joke about the drunk looking under the lamppost.",
                    "label": 0
                },
                {
                    "sent": "I think you know part of the story of why I'm looking at this stuff is because the error other colleagues here at NIPS have invented these nonparametric Bayes methods.",
                    "label": 0
                },
                {
                    "sent": "And so I'm looking to see this is like a new lamp post and I'm looking to see if there's anything interesting under that lamp post.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to start off talking a little bit about just straightforward PC FG rule production estimation, and then because that serves as the backbone for the nonparametric Bayesian method and will talk about generalizing PCF's making them nonparametric.",
                    "label": 0
                },
                {
                    "sent": "And so one of the key ideas is that even though there's an unbounded number of potential rules, because any parse or any finite number of passes can only use a finite number of these rules in constructing their past trees, that means that we only need to explicitly represent in a sampling approach, we need to explicitly represent the rules that have actually been used, even those, even though there's an infinite number of potential rules.",
                    "label": 1
                },
                {
                    "sent": "And then if there's interest in time, I'll actually go over some of the methods that we found that are very useful for actually making inference in these things practical.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just to start off a very brief review of what a probabilistic context free grammar is.",
                    "label": 0
                },
                {
                    "sent": "You've got a series of rules associated with probabilities and the probability of a tree is the product of the probabilities of the rules that are used to build it.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You probably all know about maximum likelihood estimation.",
                    "label": 1
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "All you have to do is count the number of times that each rule is used.",
                    "label": 0
                },
                {
                    "sent": "This is assuming that you're actually given the tree Bank of trees.",
                    "label": 1
                },
                {
                    "sent": "You count the number of times that each rule is used.",
                    "label": 1
                },
                {
                    "sent": "And then compute the relative frequency of those things.",
                    "label": 0
                },
                {
                    "sent": "The family has a certain number of problems.",
                    "label": 0
                },
                {
                    "sent": "Things like you know accidental zeros can lead to probability estimates of zero, which was often.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, so a Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "Instead of picking one single value, actually instead says well what you should really do is estimate a posterior distribution of the set of possible values.",
                    "label": 0
                },
                {
                    "sent": "If you pick Dursley priors, then modulo some interesting questions.",
                    "label": 0
                },
                {
                    "sent": "The posterior is also directly.",
                    "label": 0
                },
                {
                    "sent": "So down here I'm just simply showing a few dish lies.",
                    "label": 0
                },
                {
                    "sent": "There's the uniform dish.",
                    "label": 0
                },
                {
                    "sent": "Lay down here.",
                    "label": 0
                },
                {
                    "sent": "You know if we happen to see rules appearing two times in one time, that would give us a posterior of 3 two which would look like this very broad curve.",
                    "label": 0
                },
                {
                    "sent": "If instead of seeing it two times in one times we saw it 20 times and 10 times, that would give us this much more peaked distribution here.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, one fun thing you can do with their slaves is that if you set the dash light parameter Alpha towards zero, what you actually start together.",
                    "label": 0
                },
                {
                    "sent": "These very very peaked distributions like this and you can actually use this in terms of grammar induction to say Gee, maybe I've got some super set of rules of which I believe only a very small number actually.",
                    "label": 0
                },
                {
                    "sent": "Going to be used in the grammar that I want to try and learn, so I might put a dish like prior on like you know, with some very small value of Alpha and then we get a peak distribution which says in effect G. You know, I think the grammar includes some of these rules, but I don't know which I want the estimated.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The wanderer.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Things get more interesting when you actually start trying to estimate from strings alone, instead of estimating from a tree bank.",
                    "label": 1
                },
                {
                    "sent": "And then, as I think many of you guys know, basically in general there's no close form solution anymore for the rule probabilities, but instead we've got these iterative procedures that at least find local maximum.",
                    "label": 0
                },
                {
                    "sent": "And I must, most of you will probably know the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "Turns out that there's also sampling algorithms for achieving the same sort of thing, that is estimating the rule probabilities.",
                    "label": 0
                },
                {
                    "sent": "So here's a very simple.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gibbs sampler that basically says somehow guess some past trees may be randomly assigned past trees to strings and then forever repeat the following things.",
                    "label": 1
                },
                {
                    "sent": "Treat your past trees as if they really were the true tree bank.",
                    "label": 0
                },
                {
                    "sent": "So compute your real rule counts from them.",
                    "label": 0
                },
                {
                    "sent": "Sample your rule probabilities according to this Dursley formula over here, and then for each string, Now go back and instead of actually assigning it, the most likely parse tree instead, just simply sample APA's tree.",
                    "label": 1
                },
                {
                    "sent": "From the string X and from the rule probabilities that you just sampled up here before, and this is a Gibbs sampler which is alternating between sampling the rule probabilities in the past trees and so according to the MCMC theory, after a certain burning.",
                    "label": 1
                },
                {
                    "sent": "What you'll actually start.",
                    "label": 0
                },
                {
                    "sent": "Getting our samples of real probabilities in trees that are distributed according to the basic.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Posterior.",
                    "label": 0
                },
                {
                    "sent": "So that's great.",
                    "label": 0
                },
                {
                    "sent": "In fact, actually one of the things that's.",
                    "label": 0
                },
                {
                    "sent": "Very useful to do is to notice that you can actually integrate out the rule probabilities, which is actually had a paper rejected when I was describing this collapsed Gibbs sampler, and the reviewer said, you know, look, you said the goal is to estimate the rule probabilities.",
                    "label": 1
                },
                {
                    "sent": "That's why you're doing it.",
                    "label": 0
                },
                {
                    "sent": "But if you integrate out the rule probabilities where the rule probability is gone.",
                    "label": 0
                },
                {
                    "sent": "So in fact actually.",
                    "label": 0
                },
                {
                    "sent": "So let me sort of just say first of all, we can sample the parse trees and then if you actually do want the rule probability.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You go back and you use this same formula here.",
                    "label": 0
                },
                {
                    "sent": "Compute the rule counts and then sample the rule probabilities from those.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But there's actually a technical reason why it's actually handy to to integrate out these real probabilities, and that's when we start to move to grammars where there's an infinite number of potential rules.",
                    "label": 0
                },
                {
                    "sent": "This rule probability vector over here will be infinitely long, so we won't want to represent that explicitly.",
                    "label": 0
                },
                {
                    "sent": "But if we can integrate that out, as I said before, that one of the key ideas is that the parse trees are finite for any finite corpus.",
                    "label": 0
                },
                {
                    "sent": "So even though we may have an infinite number of potential rules, 'cause we've integrated out the rule probabilities, we won't actually have to represent them, and so the collapsed Gibbs sampler basically says like this, it says.",
                    "label": 0
                },
                {
                    "sent": "Basically, for each sentence XI in the training data.",
                    "label": 1
                },
                {
                    "sent": "Replace your current guests of the past tree with a sample from the conditional distribution of pastor is given the string XI and this team minus I means the set of past trees for all other strings except XI.",
                    "label": 0
                },
                {
                    "sent": "And one problem here is that the dynamic programming algorithms that are used to sample pass trees actually don't work in this particular case, and that's becausw.",
                    "label": 0
                },
                {
                    "sent": "This distribution actually isn't a PCF distribution.",
                    "label": 0
                },
                {
                    "sent": "One way to sort of think about this is think about what the probability is of the noun phrase expanding to dogs down here.",
                    "label": 0
                },
                {
                    "sent": "That depends on the number of times that noun phrases have expanded to other things in the rest of the corpus.",
                    "label": 0
                },
                {
                    "sent": "In particular, is going to depend on whether or not you decide to analyze this occurrence of dogs over here as a noun phrase, or something else.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that this probability here is coupled to the analysis that you chose over here.",
                    "label": 0
                },
                {
                    "sent": "But that breaks the context freeness property that our dynamic programming algorithms depend.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah gone.",
                    "label": 0
                },
                {
                    "sent": "These coincidences.",
                    "label": 0
                },
                {
                    "sent": "So you can alright so you can.",
                    "label": 0
                },
                {
                    "sent": "So first let me give you the fix and then let me basically agree with you.",
                    "label": 0
                },
                {
                    "sent": "They don't know except in a small corpus.",
                    "label": 0
                },
                {
                    "sent": "So the fix is actually to use site.",
                    "label": 0
                },
                {
                    "sent": "Well there are many fixes, but one fix is to use a metropolis Hastings acceptance rejection procedure, which is where.",
                    "label": 0
                },
                {
                    "sent": "Effect you sample from a proposal distribution.",
                    "label": 1
                },
                {
                    "sent": "And we actually use the PC FG that's computed from all the other pastors, ignoring what's happening inside of this past tree as the proposal distribution.",
                    "label": 1
                },
                {
                    "sent": "And then apply a metropolis Hastings acceptance rejection procedure to, in effect, sort of transform the samples from the CFG distribution to the samples from the distribution that we really care about.",
                    "label": 0
                },
                {
                    "sent": "And in fact, in practice, except when the corpus is very small, the acceptance probability is very, very high.",
                    "label": 0
                },
                {
                    "sent": "So when I actually run these things on, say, a corpus of 2030 thousand sentences, you might reject, you know, three or four sentences per per iteration for the pass, so.",
                    "label": 0
                },
                {
                    "sent": "So David's points basically correct.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The basic algorithm then sort of looks like this.",
                    "label": 0
                },
                {
                    "sent": "Initialized by assigning pass trees to string somehow and then repeat forever.",
                    "label": 1
                },
                {
                    "sent": "Pick a sentence inside of your training data.",
                    "label": 1
                },
                {
                    "sent": "Compute rule counts from all the other trees for all the other sentence is computer proposal grammar from these rule counts.",
                    "label": 1
                },
                {
                    "sent": "Sample a tree.",
                    "label": 0
                },
                {
                    "sent": "According to the conditional distribution, with this proposal, grammar and then replace the current tree for the string XI with the sample tree according to the Metropolis Hastings, except project proceeds.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this is like a little interesting question here, which I think I'll just skip.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so of course the really interesting thing is not so much learning rule probabilities as actually learning the rules themselves and.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to have is is some procedure which takes just strings as input and actually produces rule probability is rules as well as real probabilities and the basic method that I think has been used.",
                    "label": 0
                },
                {
                    "sent": "I mean, I want so successfully used because I think this is an incredibly hard problem and I don't think anybody's really got a successful method for.",
                    "label": 0
                },
                {
                    "sent": "It's far as I know, maybe somebody here does.",
                    "label": 0
                },
                {
                    "sent": "Sort of is what I sometimes call the Chinese Revolution approach to these things, which is you let 1000 Flowers let 1000 Flowers bloom, and then you as the central committee come in and kill off a whole lot of them, and then you then encourage another 1000 Flowers to bloom and like that, right?",
                    "label": 0
                },
                {
                    "sent": "So guess an initial set of rules reuse.",
                    "label": 1
                },
                {
                    "sent": "Use your EM or whatever it is procedure to estimate the rule probabilities from the strings.",
                    "label": 1
                },
                {
                    "sent": "Prune the low probability rules and then down.",
                    "label": 0
                },
                {
                    "sent": "Here is another step, which is where you need to have a procedure.",
                    "label": 0
                },
                {
                    "sent": "This outside of the estimator.",
                    "label": 1
                },
                {
                    "sent": "Which proposes additional potentially useful rules.",
                    "label": 0
                },
                {
                    "sent": "And I actually think one of the one of one of the strengths of these nonparametric Bayesian methods is that they seem to be able to provide a more systematic approach.",
                    "label": 1
                },
                {
                    "sent": "You know, in the all in the previous approaches, this step here was usually a heuristic procedure that wasn't connected to the objective function that was motivating the learning procedure.",
                    "label": 0
                },
                {
                    "sent": "And so the nonparametric Bayesian methods actually permit us to sort of wrap all these things together and integrate these two steps.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically one standard way of coming up with nonparametric methods is to take parametric methods and then try to generalize the number of parameters to to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So with probabilistic context free grammar, there really are sort of two obvious ways of becoming non parametric.",
                    "label": 0
                },
                {
                    "sent": "So one method is to let the number of nonterminals grow unboundedly.",
                    "label": 1
                },
                {
                    "sent": "So someone gave you an original grammar with S goes to end PvP, but you say, well, you know really, actually, maybe there's not just one type of sentence, there's really.",
                    "label": 0
                },
                {
                    "sent": "20 types of sentence is, and there's really 40 types of noun phrases, and you're going to try and learn how many different types of sentences and noun phrases there are, and so in fact, you're actually going to try and learn rules like the following sentence of type 12 goes to noun phrase of type 7.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And this actually leads to the Infinite PC FG and a bunch of extensions like that.",
                    "label": 1
                },
                {
                    "sent": "What I'll be talking about here today is is an approach where I actually let the the set of rules themselves, right the set of productions grow unboundedly.",
                    "label": 1
                },
                {
                    "sent": "And so the idea really is to use a grammar or a meta grammar to generate potential rules.",
                    "label": 0
                },
                {
                    "sent": "Potential expansions of some non terminal, and then I'm going to try and learn which of those rules or are in fact actually useful.",
                    "label": 0
                },
                {
                    "sent": "So in fact actually might the rules of my final object level grammar are going to be whole subtrees of the matter grammar.",
                    "label": 1
                },
                {
                    "sent": "So what I'm really going to be learning is something like a tree substitution grammar where we learn the probabilities of entire fragments.",
                    "label": 0
                },
                {
                    "sent": "We learn which fragments are useful and we learn their probabilities.",
                    "label": 0
                },
                {
                    "sent": "I should point out, there's no theoretical reason why both things can't be done together, although I actually wonder whether if you did try to combine them, whether you'd actually get some competition between these two mechanisms might not be clear which Gen.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Causations correct?",
                    "label": 0
                },
                {
                    "sent": "OK, so all the examples I'm going to be talking about are actually going to be involving learning things like word segmentation and.",
                    "label": 0
                },
                {
                    "sent": "And morphology.",
                    "label": 0
                },
                {
                    "sent": "So you guys are probably saying, why isn't he learning syntax and semantics with this stuff, so I will admit so far I really haven't got.",
                    "label": 0
                },
                {
                    "sent": "You know, we've tried learning dependency grammars.",
                    "label": 0
                },
                {
                    "sent": "The results aren't bad, but they're not.",
                    "label": 0
                },
                {
                    "sent": "Great, you know it's a bit like what's the joke.",
                    "label": 0
                },
                {
                    "sent": "The dog walking on its hind legs, perhaps amazing that it can be done at all, but still not what you'd call the success, you know.",
                    "label": 0
                },
                {
                    "sent": "Whereas I think actually with this low level stuff.",
                    "label": 0
                },
                {
                    "sent": "That in fact actually.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is kind of cute and I think also there's a lot to actually learn by actually looking at learning in very simple situations.",
                    "label": 0
                },
                {
                    "sent": "So let's actually sort of start off with a very simple case, so let's just imagine that I just wanted to learn how to parse words into their morphemes.",
                    "label": 1
                },
                {
                    "sent": "So what I wanted to do is to learn I'm going to want to learn representations like this for the word talking.",
                    "label": 0
                },
                {
                    "sent": "It says that essentially talking consists of a stamp followed by a suffix talk.",
                    "label": 0
                },
                {
                    "sent": "Is the stem ING is the suffix, so the first thing to note is that if I just simply use this context free grammar I can represent.",
                    "label": 0
                },
                {
                    "sent": "That segmentation perfectly fine, but I'm hoping that at least all the grammar geeks in here are sort of boggling at this stage because there's actually no hope that this grammar could actually learn how to segment talking into talk plus ING, and that's becausw.",
                    "label": 0
                },
                {
                    "sent": "The rules.",
                    "label": 0
                },
                {
                    "sent": "The units of generalization in this grammar rules and the rules are way too smaller.",
                    "label": 1
                },
                {
                    "sent": "Unit, right?",
                    "label": 0
                },
                {
                    "sent": "So this grammar here can learn what the probability is that a character is a T or it can learn what they can learn.",
                    "label": 0
                },
                {
                    "sent": "Essentially a geometric distribution over the length of characters, but that's about it.",
                    "label": 0
                },
                {
                    "sent": "Can't actually learn that talk is a stem because that generalization isn't aligned with anyone single rule.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing you might say is OK, well, let's actually change the grammar.",
                    "label": 0
                },
                {
                    "sent": "Let's actually introduce a rule for every possible scam, and another rule for every possible suffix.",
                    "label": 1
                },
                {
                    "sent": "And so then we've learned a tree that looks something like this.",
                    "label": 0
                },
                {
                    "sent": "So this tree here now actually sort of this grammar looks like it's actually got a hope of doing learning this thing, so you could actually learn what the probability is that talk is in fact actually a stem.",
                    "label": 0
                },
                {
                    "sent": "The only thing is that whenever anybody writes up a grammar that's got little bits of English in it, I advise you to keep your hand on your wallet 'cause they might be trying to pull another fast one on you as well, right?",
                    "label": 0
                },
                {
                    "sent": "This isn't a PC FG anymore, right?",
                    "label": 0
                },
                {
                    "sent": "There's now an infinite number of rules, but what's actually interesting here is that.",
                    "label": 1
                },
                {
                    "sent": "There isn't actually really a practical problem here.",
                    "label": 1
                },
                {
                    "sent": "If I'm allowed to look at the corpus ahead of time, my training data, I can actually identify all the possible prefixes and all the possible suffixes in it, right?",
                    "label": 1
                },
                {
                    "sent": "So in fact, actually, given the data, I can actually identify which subset of these infinitely many different rules here could actually possibly be useful.",
                    "label": 0
                },
                {
                    "sent": "So this keep that thought in mind.",
                    "label": 0
                },
                {
                    "sent": "I mean, the idea is that the data itself actually supplies us with enough information to know which rules can actually possibly be useful.",
                    "label": 0
                },
                {
                    "sent": "So and really this nonparametric Bayes, I think sort of gives us a way of formalizing that intuition and formalizing it in situations where it's not quite so straightforward to see which rules are the ones you should restrict.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Attention to.",
                    "label": 0
                },
                {
                    "sent": "OK, alright, well so let's just say we've done that.",
                    "label": 0
                },
                {
                    "sent": "We look at the data we pick the set of rules that can possibly be useful in this data and then we throw this into our favorite maximum likelihood liner like EM or something like that.",
                    "label": 0
                },
                {
                    "sent": "In fact, actually, I've done that.",
                    "label": 0
                },
                {
                    "sent": "And what you wind up getting your analysis that look like this.",
                    "label": 0
                },
                {
                    "sent": "Every word is analyzed as a stem on its own.",
                    "label": 1
                },
                {
                    "sent": "And there's a mathematical reason for why that's the case.",
                    "label": 1
                },
                {
                    "sent": "You can actually show that the maximum likelihood solution is this.",
                    "label": 0
                },
                {
                    "sent": "This this grammar like this actually is going to fit the empirical distribution in your data exactly, and you can show that in fact actually that the you can see up there.",
                    "label": 0
                },
                {
                    "sent": "It's in terms of KL divergent.",
                    "label": 0
                },
                {
                    "sent": "You can actually prove that that is in the ML solution, so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not a search problem here, it's actually a model problem.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you know we were just talking about how great Bayesian reasoning was and how great dursley's are for enforcing sparsity.",
                    "label": 0
                },
                {
                    "sent": "What we might want to do is to put additional a prior that's actually going to prefer, instead of letting every word be its own separate morpheme.",
                    "label": 0
                },
                {
                    "sent": "Maybe in fact, if we use the Dursley prior, it would actually prefer to have a small number of morphemes, and that's going to force it to generalize, so you can.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do that, here's the results for some number of different deuschle parameters.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly great, right?",
                    "label": 0
                },
                {
                    "sent": "I mean you can see every so often, it's sort of managed to pull off and ING suffix or needy suffix, but it's it's really pretty bad, yeah?",
                    "label": 0
                },
                {
                    "sent": "This is for a particular sample.",
                    "label": 0
                },
                {
                    "sent": "This is in fact actually all the words from all the verbs from the Wall Street Journal.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes yes, that's right.",
                    "label": 0
                },
                {
                    "sent": "This is at this stage.",
                    "label": 0
                },
                {
                    "sent": "This is just this is just one sample.",
                    "label": 0
                },
                {
                    "sent": "So yes, so So what UI is saying an in fact actually is.",
                    "label": 0
                },
                {
                    "sent": "It's a good point because.",
                    "label": 0
                },
                {
                    "sent": "Just in case I don't get to it.",
                    "label": 0
                },
                {
                    "sent": "In fact, actually you can improve the accuracy of these things by averaging over multiple samples, and in fact actually that's now what I do with these adaptive grammars is run them for generally around 2000 iterations.",
                    "label": 0
                },
                {
                    "sent": "I'll run eight runs.",
                    "label": 0
                },
                {
                    "sent": "And then collect from each round the last 1000 samples.",
                    "label": 0
                },
                {
                    "sent": "So I'll actually get 8000 samples and then average over those and that improves accuracy.",
                    "label": 0
                },
                {
                    "sent": "So right now in doing a different task and doing a word segmentation task.",
                    "label": 0
                },
                {
                    "sent": "So I'm getting accuracies of around 85% and that takes it up to about 87%, so that's it.",
                    "label": 0
                },
                {
                    "sent": "Very sensible and very good idea.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the point of this plot here is actually just to sort of show that there's a huge difference in log likelihood.",
                    "label": 0
                },
                {
                    "sent": "Cost of the posterior probability between the true suffixes, which are this green line.",
                    "label": 0
                },
                {
                    "sent": "Here in this little problem actually know what the true segmentation ought to be.",
                    "label": 0
                },
                {
                    "sent": "And this this line.",
                    "label": 0
                },
                {
                    "sent": "Here is the line which the sampler itself is actually finding, so you can see.",
                    "label": 0
                },
                {
                    "sent": "And this is like a gap of several, many, many millions of times, right?",
                    "label": 0
                },
                {
                    "sent": "So you probably would have to wait like since the beginning of the universe or something like that to actually find a sample which actually agrees with the true sample, right so?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, actually there's a reason for that, which is that the grammar itself has got this top level rule up here.",
                    "label": 0
                },
                {
                    "sent": "Word goes to stamman suffix, which essentially says that the distribution of stems should be independent of the distribution of suffixes.",
                    "label": 0
                },
                {
                    "sent": "Here I'm actually sort of just picked 5 very high frequency verbs, so I don't not plotting error bars here, but if I did plot error bars they would be very, very tiny.",
                    "label": 0
                },
                {
                    "sent": "Wouldn't even see them on this plot.",
                    "label": 0
                },
                {
                    "sent": "And you can see this assumption up here is just sort of nowhere close to true, right?",
                    "label": 0
                },
                {
                    "sent": "There's huge variation in the frequency of a suffix given the stamp.",
                    "label": 0
                },
                {
                    "sent": "So the context freeness assumption inside of this model is sort of very heavily violated.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one approach that we've taken to deal with that essentially is to say, Gee, maybe what you should be doing is type based inference instead of token based inference.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is Gee if instead of actually sort of collecting all of the verb types, imagine we just collected sorry Oliver tokens.",
                    "label": 0
                },
                {
                    "sent": "Imagine collected all of the types and we just tried to learn from types themselves.",
                    "label": 0
                },
                {
                    "sent": "If there's, say, roughly 4 suffixes for every verb, then the probability of seeing anyone suffix like ING.",
                    "label": 0
                },
                {
                    "sent": "Is going to be about .25 no matter what the verb is and no matter what the actual token frequency of that suffix was.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so in fact, actually, when you try to learn from types rather than tokens, what you also notice is that in fact, actually now over a wide range of different values.",
                    "label": 0
                },
                {
                    "sent": "For these alphas we wind up getting pretty good segmentations and the segmentations that are sort of strictly speaking wrong like this guy up here, you know, really it's include plus S in fact, actually you can see the reason why it's done that is so it can actually capture this generalization down here.",
                    "label": 0
                },
                {
                    "sent": "So it's because the model doesn't really have any notion of.",
                    "label": 0
                },
                {
                    "sent": "Of morphophonology, but it's actually sort of forced to make a MIS analysis up here in order to capture the other correct analysis down there.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, actually when we do this other plot Now what you can actually see is that over fairly wide range up here we're actually saying that the true suffixes are pretty close to what the optimal suffixes the most most probable suffixes are from the from the sampler.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think there's two different.",
                    "label": 0
                },
                {
                    "sent": "Rules two different sort of rules for two different ideas.",
                    "label": 0
                },
                {
                    "sent": "I want you to take away from this little example, so one is that P CFG rules are often too small to be effective units of generalization.",
                    "label": 1
                },
                {
                    "sent": "So what we'd like to be able to do is to generalize over whole groups of rules, and we'd also like the units of generalization to be chosen based on the data.",
                    "label": 1
                },
                {
                    "sent": "The second thing is that this type based inference can often mitigate the overdispersion that will wind up saying.",
                    "label": 0
                },
                {
                    "sent": "And so this leads, I think, to a hierarchical Bayesian model where the context free grammar itself sort of generates the types.",
                    "label": 1
                },
                {
                    "sent": "And then we use another process to replicate the types to actually generate the tokens.",
                    "label": 0
                },
                {
                    "sent": "And and adapter grammars are sort of an attempt to try and do both things together, so they're going to try and learn the probability of entire subtrees, and then we're going to actually use the grammatical hierarchy to define a Bayesian hierarchy from which type based inference should emerge, and a big part of the motivation for this was that when Sharon Goldwater was at Brown, doing her thesis work, which inspires, you know a lot of this stuff.",
                    "label": 0
                },
                {
                    "sent": "She she implemented a hierarchical dislike process for actually solving that morphology problem that we just talked about.",
                    "label": 0
                },
                {
                    "sent": "She also implemented another one to solve a word segmentation problem, and I go to, you know, I was her advisor, so I said, Gee, you know, and you could combine these two together, right?",
                    "label": 0
                },
                {
                    "sent": "So we can have a model that does both simultaneously as you guys all know, don't make me do that.",
                    "label": 0
                },
                {
                    "sent": "These are these are two horrible C++ programs you know and the data structures are all completely inconsistent with each other.",
                    "label": 0
                },
                {
                    "sent": "How I'm going?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's gotta be a better way, right?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so onto adapter grammars so this is this is my attempt to come up with a better way right?",
                    "label": 0
                },
                {
                    "sent": "So the idea is that that reason adaptor grammar defined by context free grammar rules as as in a context free grammar.",
                    "label": 1
                },
                {
                    "sent": "So you the grammar designer needs to actually sort of come up with a set of base rules that generate the structures that you think are going to be useful.",
                    "label": 1
                },
                {
                    "sent": "You also need to declare a subset of the non terminals to be adapted.",
                    "label": 1
                },
                {
                    "sent": "And you need to give each adapted nonterminal concentration parameter.",
                    "label": 0
                },
                {
                    "sent": "If you're building a additional process adaptor grammar.",
                    "label": 0
                },
                {
                    "sent": "So unadapted nonterminals.",
                    "label": 0
                },
                {
                    "sent": "If you don't declare any non terminals to be adapted, then you what you wind up with is exactly a PC FG.",
                    "label": 0
                },
                {
                    "sent": "Because the unadapted nonterminals expand.",
                    "label": 0
                },
                {
                    "sent": "Just, you know, using a rule an ordinary rule using the using the current estimate of what the rules probabilities.",
                    "label": 0
                },
                {
                    "sent": "So this is just like in an ordinary PC FG.",
                    "label": 0
                },
                {
                    "sent": "But with an adapted non terminal something interesting happens.",
                    "label": 0
                },
                {
                    "sent": "So in with every adapted Mount Terminal we remember how many times with its expanded completely to a full subtree before so.",
                    "label": 0
                },
                {
                    "sent": "The adapted Montana was going to expand to a subtree with probability proportional to the number of times of that subtree is been generated previously.",
                    "label": 1
                },
                {
                    "sent": "So that's why sometimes people describe these adaptive grammars as being like caching grammars that they remember absolutely everything that they've generated in the past.",
                    "label": 0
                },
                {
                    "sent": "But there's also some probability using this dish like concentration parameter over here, have actually generating it completely fresh tree.",
                    "label": 0
                },
                {
                    "sent": "So at the very beginning of inference, when essentially you haven't generated very much stuff before, you're almost always generating fresh trees.",
                    "label": 0
                },
                {
                    "sent": "But as the number of trees that's been generated grows, you're actually going to be far more likely to reuse existing.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a simple little run.",
                    "label": 0
                },
                {
                    "sent": "In this morphology example in here.",
                    "label": 0
                },
                {
                    "sent": "This is in a Chinese restaurant process style sampler, so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are funny Chinese restaurants in here, so the first customer walks in.",
                    "label": 0
                },
                {
                    "sent": "And you should think about the rules that expand into.",
                    "label": 0
                },
                {
                    "sent": "So there's one restaurant per adapted non terminal.",
                    "label": 0
                },
                {
                    "sent": "And the rules that that expand the adapted nonterminals are kind of like recipes.",
                    "label": 0
                },
                {
                    "sent": "So this recipe says, Gee, to build a word, give me a stamina suffix and combine them together.",
                    "label": 0
                },
                {
                    "sent": "So we need to give this first customer some food on the table over here.",
                    "label": 0
                },
                {
                    "sent": "So as I said, this is a funny Chinese restaurant.",
                    "label": 0
                },
                {
                    "sent": "What it does is it sends out Waiters.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the stand in the suffix restaurants and they sit down at their tables and.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Side, yeah I want to get a stamp and I want to get a suffix so we generate us and these guys here are just strings of phones so we generated string of phones to give a stamina suffix tree and then we then bring them back.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And set them up in this Chinese restaurant table up here and so this guy up here gets his dishes.",
                    "label": 0
                },
                {
                    "sent": "The is combined out of the stamina suffix that were produced down in this restaurant down here.",
                    "label": 0
                },
                {
                    "sent": "The second customer walks in.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The word restaurant maybe sits at a fresh table, so we need to come up with a fresh dish for that person.",
                    "label": 0
                },
                {
                    "sent": "So again, we order out, go to the stamina.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fix restaurants so now what's happened is that we've decided to generate a fresh stem.",
                    "label": 0
                },
                {
                    "sent": "But because this customer is sitting down at this same suffix table down here, we reuse the soft.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we generate a fresh start.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So maybe dog.",
                    "label": 0
                },
                {
                    "sent": "And this generates dogs up here.",
                    "label": 0
                },
                {
                    "sent": "OK, the next custom.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Walks in the probability of picking a table is proportional to the number of customers sitting at it, so you might pick this first table up here.",
                    "label": 0
                },
                {
                    "sent": "In this case, there's already some food on it, so we just simply generate cats.",
                    "label": 0
                },
                {
                    "sent": "Here's the sequence of words that's being generated down there at the bottom.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And someone like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, so unadapted nonterminals expand by picking a rule and recursively expanding to their children.",
                    "label": 1
                },
                {
                    "sent": "Adapted Nonterminals can expand in two ways.",
                    "label": 1
                },
                {
                    "sent": "Again, just as by picking a rule and recursively expanding children, or else by picking a previously generated holtry with probability proportional to the number of times of the Triesman previously generated.",
                    "label": 1
                },
                {
                    "sent": "So in terms of the implementation, each adapted non terminal has a Chinese restaurant process, or there's a generalization of that called the Pitman Yor process.",
                    "label": 0
                },
                {
                    "sent": "The case is the previously generated subtrees.",
                    "label": 0
                },
                {
                    "sent": "CFG rules in effect sort of determine the base distributions of these CRPS orbit manual processes.",
                    "label": 1
                },
                {
                    "sent": "What's interesting is that the trees that are generated by an adaptor grammar aren't independent, so in particular, if an adapted subtree is been used frequently in the past, it's actually more more likely to be used again.",
                    "label": 0
                },
                {
                    "sent": "So this actually means that the adaptor grammar learns from the trees that it's previously generated, but it does have.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Property of exchangeability down here.",
                    "label": 0
                },
                {
                    "sent": "OK. Becausw the probability of an adaptive non terminal expanding to some new subtree is proportional to the number of times it was seen before.",
                    "label": 1
                },
                {
                    "sent": "This leads to a rich gets richer dynamics which leads to zipf distributions.",
                    "label": 0
                },
                {
                    "sent": "Particularly in the manual process.",
                    "label": 1
                },
                {
                    "sent": "And it also has another interesting property which PCF's don't have, which is that useful compound structures can actually be more probable than their parts.",
                    "label": 0
                },
                {
                    "sent": "And then you actually learn the base PC after rule probabilities from the table labels.",
                    "label": 0
                },
                {
                    "sent": "This is a technical point which actually means that you're actually learning the rule probabilities from types rather than from tokens in general.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's I think I've got enough time to.",
                    "label": 0
                },
                {
                    "sent": "Keep on going right, so let me talk a little bit about another application of these.",
                    "label": 0
                },
                {
                    "sent": "Probably the application we've looked at most of all, which is unsupervised word segmentation.",
                    "label": 1
                },
                {
                    "sent": "So here the idea is that the.",
                    "label": 0
                },
                {
                    "sent": "The inputs are finding sequences with sentence boundaries, and the task is to identify word boundaries and hence the words in this in this input here and there's two sorts of cues that are very useful.",
                    "label": 1
                },
                {
                    "sent": "Sharon Goldwater pointed out the usefulness of trying to track into word dependencies, and Margaret Fleck pointed out the importance of funnel tactics.",
                    "label": 0
                },
                {
                    "sent": "So here you can see there's a potential boundary between each phone, but you know only some of these.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to you want to see the book.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, we can actually represent the segmentation of the book.",
                    "label": 0
                },
                {
                    "sent": "Into words with a very simple PCF G. The structures from the very simple PC FG.",
                    "label": 0
                },
                {
                    "sent": "But again you should have the same response.",
                    "label": 0
                },
                {
                    "sent": "This grammar can't possibly learn that the and book our words because that's a property.",
                    "label": 0
                },
                {
                    "sent": "The entire tree, rather than a property of any single rule in here.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, we can apply the same sort of trick.",
                    "label": 0
                },
                {
                    "sent": "We can say Gee, maybe in fact, the grammar really ought to actually look something like this.",
                    "label": 0
                },
                {
                    "sent": "Word goes to all possible phone strings.",
                    "label": 1
                },
                {
                    "sent": "And again, now there's an infinite number of P CFG rules, but in fact actually again, once we see the finite training data, we actually know which rules could possibly be useful.",
                    "label": 1
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so in fact, actually, if we write an adaptive grammar.",
                    "label": 0
                },
                {
                    "sent": "That just simply says OK, look.",
                    "label": 0
                },
                {
                    "sent": "Actually I want to adapt the entire word subtrees.",
                    "label": 0
                },
                {
                    "sent": "What that means is we'll actually learn the probability, not just of the word goes to phonemes rule, but in fact they actually have this entire tree here, and this entire tree over here.",
                    "label": 0
                },
                {
                    "sent": "So in fact actually in the plot seals in the trees you'll see further on.",
                    "label": 0
                },
                {
                    "sent": "In general what I'll show is the adapted Nonterminals will be indicated in the grammars by underlining and also by highlighting as well.",
                    "label": 0
                },
                {
                    "sent": "So adapting words essentially means that the grammar learns the probability of the entire subtree independently, and so this model here essentially is a unigram model of word segmentation.",
                    "label": 1
                },
                {
                    "sent": "It says that the sentence consists of some sequence of words.",
                    "label": 0
                },
                {
                    "sent": "There's no relationship between them.",
                    "label": 0
                },
                {
                    "sent": "So we just generating them ascentia Lee at random and this was in fact actually a model that Michael Brent proposed for this task.",
                    "label": 0
                },
                {
                    "sent": "And in fact, we get essentially the same score that you get if you run.",
                    "label": 0
                },
                {
                    "sent": "And an adapter grammar unigram segment or around 50%.",
                    "label": 0
                },
                {
                    "sent": "Fifty 6% of the tokens are in fact correct.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And in fact, the actual grammar that you learn winds up looking something like this, so the actual learned adaptor grammar contains about 1700 rules or subtrees.",
                    "label": 0
                },
                {
                    "sent": "So here the original rules with their posterior counts and then down.",
                    "label": 0
                },
                {
                    "sent": "Here are some of the other rules that we've learned, so you learn you an in and with the doggie in the in the House.",
                    "label": 0
                },
                {
                    "sent": "So I'm like that, right?",
                    "label": 0
                },
                {
                    "sent": "So, and this is in fact actually gives you some idea of what the internal data structure actually look.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like one of the problems with this model is that it tends to under segment.",
                    "label": 0
                },
                {
                    "sent": "This is something again that people had notice earlier, and part of the reason for that is because there's very strong into word dependencies.",
                    "label": 0
                },
                {
                    "sent": "So in this particular corpus, if you hear the word doggy, you're probably going to hear the word the in front of it as well, and so a unigram model really has two choices.",
                    "label": 0
                },
                {
                    "sent": "It can either generate them as separate words, in which case it fails to capture that generalization, or else it says OG.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should just simply posit.",
                    "label": 0
                },
                {
                    "sent": "A larger word posit the doggy is a single unit and what Sharon Goldwater.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did in order to capture that generalization was to actually learn a bigram model.",
                    "label": 1
                },
                {
                    "sent": "Turns out for technical reasons, we can't learn bigram models over in adaptive grammars.",
                    "label": 0
                },
                {
                    "sent": "We've only got a finite number of adapted nonterminals, but we can do something that's.",
                    "label": 0
                },
                {
                    "sent": "That it seems to do just as well, and that is we can learn collocations.",
                    "label": 0
                },
                {
                    "sent": "So we say that Gia sentence consists of a sequence of collocations, collocations consists of some sequence of one or more words in a word consists of one or more phonemes.",
                    "label": 1
                },
                {
                    "sent": "So now what we wind up learning is something that looks like this.",
                    "label": 0
                },
                {
                    "sent": "You want to down here so we're still under segmented one too.",
                    "label": 0
                },
                {
                    "sent": "But we've learned that that's a you want to as a whole colocation, and the book is also learned as a colocation.",
                    "label": 0
                },
                {
                    "sent": "So this gives the adaptor grammar, the ability to learn that there's a relationship between these two words, but still also posit these things as independent words, and in fact, actually wind up with you run that, you end up getting a segmentation accuracy of about 76% instead of.",
                    "label": 0
                },
                {
                    "sent": "56% and this is about the same improvement that Sharon Goldwater found with her bigram model.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so as I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "Margaret Fleck, who's also been working on this problem, not with adapter grammars, but just sort of, you know, on our custom custom built models.",
                    "label": 0
                },
                {
                    "sent": "One of the things that she notes is that.",
                    "label": 0
                },
                {
                    "sent": "Trying to learn syllabic structure.",
                    "label": 0
                },
                {
                    "sent": "Trying to, you know, noticing that words typically consists of sequences of syllables.",
                    "label": 0
                },
                {
                    "sent": "Syllables begin with a sequence of consonants, followed by some vowels, followed by some other constants.",
                    "label": 0
                },
                {
                    "sent": "If you can learn those things.",
                    "label": 0
                },
                {
                    "sent": "Then in fact, actually they can be very, very powerful cues to where the word boundaries are.",
                    "label": 0
                },
                {
                    "sent": "So this now is an adaptor grammar.",
                    "label": 0
                },
                {
                    "sent": "You know, once you've got this tool, you can sort of go wild, right?",
                    "label": 0
                },
                {
                    "sent": "I mean so this adaptor grammar now says, Gee, that a word consists.",
                    "label": 0
                },
                {
                    "sent": "Sorry, collocation consists of a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "A word now consists of a sequence of syllables.",
                    "label": 0
                },
                {
                    "sent": "So this says us and these ones here in fact.",
                    "label": 0
                },
                {
                    "sent": "Actually it's maybe a little weird, even though I don't know where the word boundaries are, my grammar can actually try and distinguish between syllables that our initial syllables that are final and syllables that aren't word initial word final, because we know that in fact actually there are different onsets and coders associated with syllables that our initial and final, and in fact actually.",
                    "label": 0
                },
                {
                    "sent": "If I build that in what I wind up learning, then is so I actually need to tell it.",
                    "label": 0
                },
                {
                    "sent": "Now the difference between consonants and vowels.",
                    "label": 0
                },
                {
                    "sent": "So that's all I tell it about.",
                    "label": 0
                },
                {
                    "sent": "I just simply tell it that an onset consists of a sequence of consonants.",
                    "label": 0
                },
                {
                    "sent": "And decoder also consists of a sequence of consonants.",
                    "label": 0
                },
                {
                    "sent": "Putting that in actually improves the segmentation accuracy up from what was at 76% to about 87%.",
                    "label": 0
                },
                {
                    "sent": "Yeah, 5 minutes OK good.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is now we're sort of building sort of really quite rich trees, and as I said this, as far as I know this is the best word segmentation accuracy that's up there.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in the last five minutes let me just say a little bit about the estimation techniques that are used here.",
                    "label": 0
                },
                {
                    "sent": "In early work that I've done, I in fact actually would, so there's a number of parameters.",
                    "label": 0
                },
                {
                    "sent": "Hyperparameters like the concentration parameters for the dash, lights that have to be learned in early work.",
                    "label": 0
                },
                {
                    "sent": "What I've done is I just simply clamp those values as the models get more and more complicated.",
                    "label": 0
                },
                {
                    "sent": "You don't necessarily actually want to have the same value for for all the parameters.",
                    "label": 0
                },
                {
                    "sent": "The concentration parameters which are associated, say with the word or the collocations should not necessarily the same as the optimal ones that are associated with syllables or something like that.",
                    "label": 0
                },
                {
                    "sent": "And here what I'm basically saying is that in fact, actually we can go Bayesian about those, put, put, put prior Bayesian priors on these parameters, and in fact actually that produces a fairly substantial performance improvement.",
                    "label": 0
                },
                {
                    "sent": "So basically with tide parameters.",
                    "label": 0
                },
                {
                    "sent": "On the most complex models I was getting about 78% F score.",
                    "label": 0
                },
                {
                    "sent": "If you if you work with a Chinese restaurant process, you and sample its values go to about 84% and then if you move to this more complex Pitman Yor process, you get 87%.",
                    "label": 0
                },
                {
                    "sent": "One of the things I think is actually sort of very encouraging about this stuff is that as the models get more complicated, actually the performance seems to improve.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether you guys, I think probably most of you guys are probably played with something like EM where oftentimes if you make the models more complicated, all you're doing is just giving more rope to EM.",
                    "label": 0
                },
                {
                    "sent": "And it's just hanging itself, you know?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will find some weird way of doing things.",
                    "label": 0
                },
                {
                    "sent": "OK so very very quickly.",
                    "label": 0
                },
                {
                    "sent": "Let me just sort of mention here.",
                    "label": 0
                },
                {
                    "sent": "There's a couple of other optimization techniques.",
                    "label": 0
                },
                {
                    "sent": "This this modal word segmentation is exactly what you I was suggesting.",
                    "label": 1
                },
                {
                    "sent": "That's that's important for about 2 or 3% performance improvement.",
                    "label": 0
                },
                {
                    "sent": "Exactly how you initialize seems to really matter as well.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The point here is that.",
                    "label": 0
                },
                {
                    "sent": "So so so.",
                    "label": 0
                },
                {
                    "sent": "One thing you can wind up doing is what I call it incremental initialization.",
                    "label": 0
                },
                {
                    "sent": "So you start off with the first string sample at a tree at random for it, and then sample the tree for the second string conditioned on the tree that you sample for the first string.",
                    "label": 0
                },
                {
                    "sent": "That actually produces better word segmentation results.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if you look at the log probabilities over here, this is of eight separate runs.",
                    "label": 0
                },
                {
                    "sent": "What you actually notice is that starting off by guessing completely random trees for everything winds up coming up with solutions that have better posterior probabilities.",
                    "label": 0
                },
                {
                    "sent": "So in fact actually, since I'm interested in the predictions of the model, rather than just simply building a device that's got better word accuracy, I actually go for this random initialization.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Higher.",
                    "label": 0
                },
                {
                    "sent": "Yeah I think so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm in fact what I think actually is that all these models still there still are dependencies that I'm failing to capture.",
                    "label": 0
                },
                {
                    "sent": "So I think one of the high level messages here is that what we're actually seeing is this Bayesian models are doing better.",
                    "label": 0
                },
                {
                    "sent": "As as we start to.",
                    "label": 0
                },
                {
                    "sent": "As we give it, the ability to learn more generalizations that can explain away longer range dependencies, but these collocations are still a very.",
                    "label": 0
                },
                {
                    "sent": "Sort of trivial thing right there?",
                    "label": 0
                },
                {
                    "sent": "Still not a terribly good model of all the dependencies that are present in real natural language.",
                    "label": 0
                },
                {
                    "sent": "So in fact I actually think what's happening here is that the models tend to under segment because they don't have any way of explaining the real interword dependencies.",
                    "label": 1
                },
                {
                    "sent": "And then incremental initialization because it greedily searches for common substrings.",
                    "label": 1
                },
                {
                    "sent": "In general, I think there's likely this sort of the garden path into those things earlier.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, I think I'm running out of time, so I should probably skip you so anyway, so I think.",
                    "label": 0
                },
                {
                    "sent": "I think there is something interesting under this nonparametric Bayes lamppost and I think there's also plenty of other things to try as well.",
                    "label": 0
                },
                {
                    "sent": "If you come to my talk, I'm giving a talk on the topic modeling Workshop where I'll be talking about the relationship between these sorts of grammars and topic models, but I think there's plenty of other stuff to do with nonparametric Bayes as well.",
                    "label": 0
                },
                {
                    "sent": "So thanks very much guys.",
                    "label": 0
                },
                {
                    "sent": "Time maybe?",
                    "label": 0
                },
                {
                    "sent": "Is it that of a power again?",
                    "label": 0
                },
                {
                    "sent": "Skeptical about using this for syntax, sorts of modifications, do you think?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, so in fact, actually one of the things you can sort of notice.",
                    "label": 0
                },
                {
                    "sent": "Just pick some tree here, right?",
                    "label": 0
                },
                {
                    "sent": "What I'm actually doing is I'm learning the whole tree right down to the words themselves.",
                    "label": 0
                },
                {
                    "sent": "Part of the reason why I'm doing that is because I think it's important.",
                    "label": 0
                },
                {
                    "sent": "At least for search reasons to try and make these things very heavily dependent on the actual strings themselves, I think actually maybe it might be related to the intuitions that you've had in your stuff as well, that that in fact actually you know you want.",
                    "label": 0
                },
                {
                    "sent": "You want the generalizations that your learner can make to be to to be grounded.",
                    "label": 0
                },
                {
                    "sent": "In the data.",
                    "label": 0
                },
                {
                    "sent": "Tim O'Donnell at Harvard is sort of said.",
                    "label": 0
                },
                {
                    "sent": "Gee, there's no mathematical reason why these trees that were memorizing or caching need to all cash out in the terminal string, and in fact, he's got a generalization of adapter grammars called Fragmente grammars, which basically can learn arbitrary trees.",
                    "label": 0
                },
                {
                    "sent": "They don't have to cash out into the into the terminals, so I think actually, that that framework actually has more hope of learning syntactic generalizations, I mean this.",
                    "label": 0
                },
                {
                    "sent": "This framework here can if you apply it to syntactic trees, can learn that a given sequence of words is a noun phrase, but it can't learn a generalization about noun phrases.",
                    "label": 0
                },
                {
                    "sent": "You know where is Tim O'donnell's grammar can now Tim is.",
                    "label": 0
                },
                {
                    "sent": "Just as I feared is sort of running into problems actually getting his system to learn pretty much anything.",
                    "label": 0
                },
                {
                    "sent": "Unreal culprit.",
                    "label": 0
                },
                {
                    "sent": "So, so he's again.",
                    "label": 0
                },
                {
                    "sent": "He's got success with with sort of small artificial.",
                    "label": 0
                },
                {
                    "sent": "You know small, very, very small corpora.",
                    "label": 0
                },
                {
                    "sent": "Very, very.",
                    "label": 0
                },
                {
                    "sent": "You know, artificial ones.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know computational linguistics is actually sort of moved away from trying to work with toy examples.",
                    "label": 0
                },
                {
                    "sent": "I'm actually not so sure that that's working with toy examples is actually so bad, but if you want to work with real data, at least Tim hasn't had success with that.",
                    "label": 0
                },
                {
                    "sent": "Is that 'cause you know?",
                    "label": 0
                },
                {
                    "sent": "Is that because my hunch that the search problem is just really way too hard?",
                    "label": 0
                },
                {
                    "sent": "Is the case or is there something else going on there?",
                    "label": 0
                },
                {
                    "sent": "Maybe 10 doesn't have quite the right model?",
                    "label": 0
                },
                {
                    "sent": "I don't know, but so in principle one can apply these things to syntax.",
                    "label": 0
                },
                {
                    "sent": "But in practice the one person that has tried that that I know of hasn't had great success.",
                    "label": 0
                }
            ]
        }
    }
}