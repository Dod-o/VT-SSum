{
    "id": "5vh77ojpqc3eoeyrrge52vz4oj56nifh",
    "title": "Learning to Learn with Compound HD Models",
    "info": {
        "author": [
            "Ruslan Salakhutdinov, Department of Statistical Sciences, University of Toronto"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Neural Networks",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_salakhutdinov_hdmodels/",
    "segmentation": [
        [
            "OK, so this is this is the work, but basically we're trying to address the issue of one shot learning and learning from few examples and this is joint work with Josh and and Antonio.",
            "So just let me just remind you what one should learning.",
            "Problem is that we're trying to look at this paper and this is the idea that if I show you various images and I show you only one image of a Segway in this case, then human learns at least seems to be able to quickly get the concept and generalize to new instances of Segways."
        ],
        [
            "And in this work, we're looking at the compositional new compositional architectures that integrate deep networks with more structured hierarchical Bayesian models and just the motivation for what we're doing here is in recent years deep networks have been shown to give quite useful distributed feature representations for many large scale datasets and variety of application domains, and it's certainly the ability of deep networks to learn multiple layers allows these models to learn fairly sophisticated.",
            "App domain specific features from fairly large datasets as opposed to just relying on human crafted features, and so the name you can see there the name of was coined.",
            "It's called unsupervised feature learning.",
            "It's been quite successful, but in this paper we also argue that if you actually want to do one shot learning transfer, learning from few examples, then you also want to consider architectures or models that identify explicitly identify few degrees of freedom.",
            "Few parameters that are relevant to learning a novel concept.",
            "So in our work we've been looking at hierarchical Bayesian models that.",
            "Explicitly represent category hierarchies where it's easier, it's much easier to share knowledge about parameters of another class with the prior that's abstracted from related classes.",
            "So in this case, if you're trying to identify an image of a secret, for example, we would say, well, an image of a Segway.",
            "It looks like funny, kind of a vehicle, and we know we may know a lot about other kinds of vehicles.",
            "There is also notion of high level parts of class sensitive high level features.",
            "In this case, wheels and.",
            "And handles and such.",
            "And there is also notion of low level features or low level generic features.",
            "So let me give you a party."
        ],
        [
            "Your model we've been looking at.",
            "In this work we call it HDP DPM model, and the idea here is that we samplings at, which is a random variable from nested Chinese restaurant process prior, which is essentially a flexible prior over the tree structures.",
            "And so in our model tree structure is being learned conditional in particular tree structure.",
            "We sample the states of the latent variables, H3.",
            "In this case, these are the states of latent variables in the Boltzmann machine, high Level D Boston machine, and we sampling it based on her Apple juice let process private.",
            "Essentially apply that allows various high level features to be shared across various classes, and then conditional H3.",
            "We actually sample the states of observed variables, which could be pixels.",
            "The actual pixels in the data from a divorce machine.",
            "And one nice thing about that model is that the deep Boltzmann Shields machine itself allows us to enforce approximate global consistency via manual constraints, and Boltzmann machines are very good, and figuring out exactly where pixels should go and how different edges should be combined together.",
            "Whereas hierarchical Bayesian models are more.",
            "Appropriate for building putting structured representations into model, and we apply this model in variety of different datasets, such as motion capture data set, images of natural scene handling characters, so there's no particular bias to using one particular data modality for these models, so."
        ],
        [
            "Let me just show you 11 example of what the model is able to do.",
            "This is a handwritten characters data set.",
            "You have about 1500 characters coming from about 50 different alphabets and on the top half of the slide you're looking at three examples.",
            "If I show you 3 examples then these are the conditional samples generated by the model, so you can certainly see that the model is able to capture a lot of interesting structuring handwritten characters.",
            "And at the bottom of the slide you're looking at.",
            "If you ask the model to actually instantiate new characters, so you're looking at various different groupings different what we call pseudo alphabets that the model is able to learn, and you can certainly see that we can ask our model to generate new instances of these characters and the statistics of the generated connectors look a lot like the statistics of the real data.",
            "But the interesting thing here is a lot of these characters don't actually look like anything that we have in the training set, so these are genuinely a lot of these characters are genuinely new characters, and if you want to see more about the inference as well as specific details of our model, please come to our poster as well as.",
            "We have other domains such as images of natural scenes as well as motion capture data, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is this is the work, but basically we're trying to address the issue of one shot learning and learning from few examples and this is joint work with Josh and and Antonio.",
                    "label": 0
                },
                {
                    "sent": "So just let me just remind you what one should learning.",
                    "label": 0
                },
                {
                    "sent": "Problem is that we're trying to look at this paper and this is the idea that if I show you various images and I show you only one image of a Segway in this case, then human learns at least seems to be able to quickly get the concept and generalize to new instances of Segways.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this work, we're looking at the compositional new compositional architectures that integrate deep networks with more structured hierarchical Bayesian models and just the motivation for what we're doing here is in recent years deep networks have been shown to give quite useful distributed feature representations for many large scale datasets and variety of application domains, and it's certainly the ability of deep networks to learn multiple layers allows these models to learn fairly sophisticated.",
                    "label": 1
                },
                {
                    "sent": "App domain specific features from fairly large datasets as opposed to just relying on human crafted features, and so the name you can see there the name of was coined.",
                    "label": 1
                },
                {
                    "sent": "It's called unsupervised feature learning.",
                    "label": 1
                },
                {
                    "sent": "It's been quite successful, but in this paper we also argue that if you actually want to do one shot learning transfer, learning from few examples, then you also want to consider architectures or models that identify explicitly identify few degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "Few parameters that are relevant to learning a novel concept.",
                    "label": 1
                },
                {
                    "sent": "So in our work we've been looking at hierarchical Bayesian models that.",
                    "label": 0
                },
                {
                    "sent": "Explicitly represent category hierarchies where it's easier, it's much easier to share knowledge about parameters of another class with the prior that's abstracted from related classes.",
                    "label": 0
                },
                {
                    "sent": "So in this case, if you're trying to identify an image of a secret, for example, we would say, well, an image of a Segway.",
                    "label": 0
                },
                {
                    "sent": "It looks like funny, kind of a vehicle, and we know we may know a lot about other kinds of vehicles.",
                    "label": 0
                },
                {
                    "sent": "There is also notion of high level parts of class sensitive high level features.",
                    "label": 0
                },
                {
                    "sent": "In this case, wheels and.",
                    "label": 0
                },
                {
                    "sent": "And handles and such.",
                    "label": 0
                },
                {
                    "sent": "And there is also notion of low level features or low level generic features.",
                    "label": 0
                },
                {
                    "sent": "So let me give you a party.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Your model we've been looking at.",
                    "label": 0
                },
                {
                    "sent": "In this work we call it HDP DPM model, and the idea here is that we samplings at, which is a random variable from nested Chinese restaurant process prior, which is essentially a flexible prior over the tree structures.",
                    "label": 1
                },
                {
                    "sent": "And so in our model tree structure is being learned conditional in particular tree structure.",
                    "label": 0
                },
                {
                    "sent": "We sample the states of the latent variables, H3.",
                    "label": 0
                },
                {
                    "sent": "In this case, these are the states of latent variables in the Boltzmann machine, high Level D Boston machine, and we sampling it based on her Apple juice let process private.",
                    "label": 0
                },
                {
                    "sent": "Essentially apply that allows various high level features to be shared across various classes, and then conditional H3.",
                    "label": 0
                },
                {
                    "sent": "We actually sample the states of observed variables, which could be pixels.",
                    "label": 0
                },
                {
                    "sent": "The actual pixels in the data from a divorce machine.",
                    "label": 0
                },
                {
                    "sent": "And one nice thing about that model is that the deep Boltzmann Shields machine itself allows us to enforce approximate global consistency via manual constraints, and Boltzmann machines are very good, and figuring out exactly where pixels should go and how different edges should be combined together.",
                    "label": 0
                },
                {
                    "sent": "Whereas hierarchical Bayesian models are more.",
                    "label": 0
                },
                {
                    "sent": "Appropriate for building putting structured representations into model, and we apply this model in variety of different datasets, such as motion capture data set, images of natural scene handling characters, so there's no particular bias to using one particular data modality for these models, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just show you 11 example of what the model is able to do.",
                    "label": 0
                },
                {
                    "sent": "This is a handwritten characters data set.",
                    "label": 0
                },
                {
                    "sent": "You have about 1500 characters coming from about 50 different alphabets and on the top half of the slide you're looking at three examples.",
                    "label": 0
                },
                {
                    "sent": "If I show you 3 examples then these are the conditional samples generated by the model, so you can certainly see that the model is able to capture a lot of interesting structuring handwritten characters.",
                    "label": 0
                },
                {
                    "sent": "And at the bottom of the slide you're looking at.",
                    "label": 0
                },
                {
                    "sent": "If you ask the model to actually instantiate new characters, so you're looking at various different groupings different what we call pseudo alphabets that the model is able to learn, and you can certainly see that we can ask our model to generate new instances of these characters and the statistics of the generated connectors look a lot like the statistics of the real data.",
                    "label": 0
                },
                {
                    "sent": "But the interesting thing here is a lot of these characters don't actually look like anything that we have in the training set, so these are genuinely a lot of these characters are genuinely new characters, and if you want to see more about the inference as well as specific details of our model, please come to our poster as well as.",
                    "label": 0
                },
                {
                    "sent": "We have other domains such as images of natural scenes as well as motion capture data, thank you.",
                    "label": 0
                }
            ]
        }
    }
}