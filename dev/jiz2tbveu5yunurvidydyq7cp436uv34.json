{
    "id": "jiz2tbveu5yunurvidydyq7cp436uv34",
    "title": "On Surrogate Loss Functions, f-Divergences and Decentralized Detection",
    "info": {
        "author": [
            "Michael I. Jordan, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/mlss09us_jordan_slffddd/",
    "segmentation": [
        [
            "So with that, I'm very happy that we have Mike Jordan and he'll speak now.",
            "Do you wanna someone able to turn this thing on so I can move around?",
            "It's not, it's not working, testing, testing.",
            "So I can't see it.",
            "My eyes are not good at is on its way on.",
            "Is that thing?",
            "On off button you pleasure to be here.",
            "I want to thank the organizers for having me and let me thanks too for his flexibility.",
            "I have a flight from O'Hare later today and it was going to be possibly Mistified started later, so thanks just too.",
            "So this is a favorite topic of mine.",
            "This took several years to work out.",
            "There is a paper appearing in the Annals of Statistics or actually just appeared in statistics on this.",
            "So if you turn out to be interested in it, it's a 50 page paper with lots of convex analysis.",
            "And I thought I might only speak for a short time and then off the airport for 30 minutes.",
            "So I kind of prepared the short version talking.",
            "I don't know what I'm going to do in the longer version.",
            "I won't get into convex analysis, but it's there if you're curious.",
            "So this is a topic that unifies a number of areas.",
            "It has a strong information theory content.",
            "It has a strong convex analysis content, and then in a core statistics content it really goes back to work in the 50s by David Blackwell and turned out that David's work back in the 50s had a big influence on economic, so there's not going to be any economics percent in the talk.",
            "But that connection is latent under the toxicity of interests in economics might find some connections as well."
        ],
        [
            "Now we started in this problem in a very prosaic way.",
            "It was just we had a statistical problem on our hands and we aimed at trying to develop an algorithm to solve it.",
            "We want to prove something about that algorithm and we couldn't with all the standard tools.",
            "We couldn't prove anything.",
            "What we wanted to prove about it, and so we were going to theoretical talk showing how we can prove something that was the problem.",
            "Well, this is just a simple so called decentralized detection problem.",
            "So we had a sensor network that colleagues had set up at the Intel Labs in Berkeley, and so there were a number of modes on a grid.",
            "The modes are sitting up there on the right, and.",
            "There was a light source being held up above the motes and you are simply to detect where in three dimensional space the light source was, or really what is projection was onto the plane.",
            "So in particular draw green area on the ground.",
            "Then you would tell me whether the light source was above the green area or not.",
            "OK, and so this is just a classification problem and the covariates are all the voltages coming from the sensors.",
            "But it's a distributed or decentralized classification problem, because I can't transmit the real vector of measurements back from all those motes, they have really small, long, short life batteries.",
            "And if you transmit a lot, you're going to run out of battery real quick, so you can only transmit a little bit at every time slice.",
            "So it becomes a quantization problem.",
            "Each one of the modes has got to decide how to quantize it's real number, the the signal strength that it's measuring from the light light sensor.",
            "We want to transmit the quantized values back to a central computer who does a discrimination of some kind.",
            "OK, so how do you quantize to do good discrimination?",
            "OK, OK, that problem is not new IT is been around in the 60s electrical engineering.",
            "It was.",
            "It was studied some degree and there were a number of methods developed that were really heuristic.",
            "They're widely used.",
            "A radar is full of them, but they're fundamentally Ristic, so I'm going to show you what those methods were and how they relate to things which are less heuristic."
        ],
        [
            "And so let's start to try to formalize the problem a little bit.",
            "There are a lot of broader classes of problems, as I think you'll start to become aware, but let's look at the simplest one.",
            "Just a classification problem and do it in a decentralized way.",
            "OK, so binary problem, binary classification, so we have some hypothesis.",
            "World State is 1 or minus one by the light is above the green region or not.",
            "In conditional on that there are a number of real valued observations had sites, one through capital S. Find is a little bit loud.",
            "Adjust those are real numbers and then they pass through Quantizers Q1 through QS, which are local.",
            "There's one quantizer for every sensor.",
            "And outcome Z1 through ZS which are quantized values.",
            "The axes are probably going to be digitized in the beginning, so they would be in a set of cardinality M and there are quantized down to a set of cardinell.",
            "Rail is much much less than M and these quantizers for us we're going to often be stochastic, so Z is going to be a stochastic mapping from XA kernel and then based on the Z values were going to form a discriminant function gamma of Z1 through ZS, and then we'll make our decision based on the discriminant function."
        ],
        [
            "OK, so the general formal setup would be there.",
            "We had X XY pairs, ID for simplicity and Z of Q of X is the covariant vector and Q is a possibly random mapping an.",
            "In statistics we would call this an experimental design.",
            "OK, so design is a really broad notion.",
            "It's not kind of just, you know.",
            "You know I have a little grid of some kind.",
            "It's that for every entity, every object in some statistical problem, you will allocate it to some to some covariant.",
            "So in the case of analysis, variance would allocated to asellina, covering analysis, brain stable.",
            "You might do that randomly.",
            "That would be the simplest possible experimental design, but now you can do that in many other ways, and the general field is that of studying these mappings Q.",
            "So let's call that an experimental design, and now we also have a discriminant function which lies in some non parametric family capital gamma and so those could be support vector machines or whatever else you've been learning about this week.",
            "Anything you like, that's a good.",
            "Hopefully nonparametric, i.e.",
            "Comes from a large family discriminant function.",
            "And now the problem is to find the decision which is both the quantizer Q.",
            "Let's call it a quantizer or experimental design and discriminant function.",
            "Do that jointly, right?",
            "So we would know how to do those two things separately.",
            "Would know how to find discriminant functions if you already built the quantizer, that's just the SVM or the whatever.",
            "And as I'll talk about, we would know how to do the quantization if you've already fixed the discriminant function, but we want to jointly, and we're going to have a 01 loss, or our criterion is just the probability that Y is not equal to what the quantized and then discriminate function.",
            "Juices, and so there are lots of other possible problems with this."
        ],
        [
            "This just on classification.",
            "OK, so there are perspectives on this.",
            "In the signal processing literature going back to the 60s, there's been a lot of work on this and what they have assumed is that everything is known except for Q, so they assume that all the class conditional probabilities are known.",
            "So if you know the conditional probability, you can build an optimal discriminant classification classification of the Bayes classifier, and you assume that you can get that even though it might be hard, but you assume you can do that.",
            "So let's assume known and all that's left is just the quantizer.",
            "OK, and how do you find the quantizer?",
            "Well, conceptually what you're trying to do is that I should have had a picture.",
            "Maybe I can do it over here.",
            "There's a cloud of data for class One, and a cloud of data for Class 2 in the original.",
            "Backspace OK, here's some data X for class one, and here's some data for class minus one in the X space.",
            "Now I'm going to transform that into a Z space.",
            "And I can do that with adaptive quantizer.",
            "And so these things will hopefully spread out if I choose a good quantizer in the space.",
            "Spread out so that I can get a better discrimination.",
            "OK, and so I would like to develop a criterion for choosing Q that in some sense measures the spread among these distributions.",
            "How do you measure spread among distributions with lots of heuristic things?",
            "You can do?",
            "You can just measure pellinger distance or or kaldis divergents or whatever and try to maximize that so it's not it's not a minimization problem Now as it is for the discriminate.",
            "It's a maximization problem, so that was done.",
            "Chernov distance was widely used Helander distance.",
            "It's basically he ristic letter that was not approved about these things, and they were chosen just because it turned out that the optimization problems tractable for some choices.",
            "If they say Helander distance and not tractable for others.",
            "So they focused on the ones that were tractable.",
            "They had some kind of NP hard type proofs.",
            "Should I make tractable least rigorous?",
            "But they didn't have statistical story.",
            "OK, and the general objects that were installed after virgins is also known as Elvis, Sylvie Divergences, and they have lots of kind of information, theory properties, convexity, parsing.",
            "Prove people proved a lot of things with those properties, but again there was no statistical story about why should I care about those properties from the point of view of a performance of some system.",
            "OK, so that was one literature which is, you know, had a 20 year run Tom Kailath, one of the main people who worked in the literature.",
            "A lot of really strong E people.",
            "On the in the machine learning literature, of course, that's what a lot of you are here, because you're interested in really has been very little focus on the quantizer or the experiment design usually assume that's known.",
            "All the preprocessing you got your feature vector and now find me an optimal discriminate.",
            "So huge large amounts of literature on that.",
            "Right now there are some cases where you try to find features in some sense, but they're usually done in an unsupervised learning setting.",
            "They're not done for the purpose of discrimination, or in a distributed setting where there's really an experimental design problem.",
            "OK, so there is a little bit of novelty here and this literature on their hands more rigorous in that there really isn't.",
            "There's a full decision theoretic story there.",
            "Consistent rules, there's rates.",
            "There's kind of a satisfying statistical theory, and the main tool that's been used to develop that theory is this things these objects, called surrogate loss functions, where instead of trying to optimize the 01 loss, you optimize some relaxation of that and prove that it has desirable statistical."
        ],
        [
            "OK, So what are these?",
            "Let's talk about part one.",
            "Then what are these alley Sylvie distances or F divergences?",
            "Also chicharra studies.",
            "Lot of people studies alright, so they're just.",
            "They're just a measure of diversions between probability distributions, or in general measures.",
            "So let's look at measures mu and pie and take a convex function F of the ratio I, the likelihood ratio.",
            "So that would be a radon nikodym derivative in general.",
            "So convex function F of the ratio.",
            "And then we'll average that under under \u03a0. OK, so now that would be an integral, but let it be discrete for this for this talk.",
            "So there you go.",
            "So it looks kind of like a divergent, and in fact the killer versus a special case where F is chosen to be ulogd, ulogd.",
            "You if you just plug that in and then you get the killer vergence but you get lots of other familiar objects, and in particular you choose after be absolute value function, plug that in, you get the variational distance if you choose they have to be this square root, Y kind of thing.",
            "You plug that in, you get hell, injure distance and chair."
        ],
        [
            "Off distance and a bunch of other things.",
            "Let's see if I have no idea."
        ],
        [
            "Have any others alright, but it's a broad class of objects and again the use of these was mainly to get measures of discrimination among probability distributions and use them here, Stickley and of course some of these had some in statistics of other interpretations, in particular terms of hypothesis testing.",
            "You got error exponents being expressed in terms of these things.",
            "That's where Caleb originally came out, and so on.",
            "But there really wasn't a full kind of usage."
        ],
        [
            "These from mystical view alright, but there was a tantalizing result due to David Blackwell in 1951.",
            "Very famous paper.",
            "Which proved the following.",
            "It's kind of going to sound a little bit weak, but we're going to hopefully by the end of strength in this.",
            "So what Blackwell showed, he actually defined the after version.",
            "see I think is one of the first people actually write them down in generality, and so he said, if a procedure A has a smaller F divergences, that procedure B for some particular F one of those apps, or some other F some convex F. Alright, then there exists some set of prior probabilities such that procedure A has smaller probability of error than procedure be.",
            "OK, so the prior probabilities in this case are just the class probabilities.",
            "There's quite you're in class one or class minus one with some prior alright.",
            "And if you choose a particular F divergent particular function, say the log, then there are some set of prior probabilities.",
            "You don't know what they are, but where you're doing the right thing you have the best procedure.",
            "OK, if you knew those priors, you would know which you have to use because you don't know those priors, so this result was kind of an existence result.",
            "It wasn't really useful directly.",
            "Right nonetheless it did.",
            "It was an important paper.",
            "Did motivate people to start to study after Vergence is.",
            "They said, Oh well, let's choose some FI know it works at least in some problems.",
            "Let's just use it anyway.",
            "OK, maybe I'm going to look at it will be the right priors.",
            "If it's not, maybe it still works, so it became a kind of heuristic empirical literature.",
            "And so people started studying afterwards they said they turned out that some of them, you know, Helander distance became a popular one, also called Bhattacharyya distance.",
            "Because it had 01 loss was intractable, this one turned out to be tractable.",
            "You could get an algorithm that you can minimize.",
            "Maximize this guy and similar with sharing off distance.",
            "So there you go.",
            "And of course then there were some heuristic arguments from asymptotic supporting these things that these divergences arise as their exponents.",
            "But that was it."
        ],
        [
            "Alright, so now Part 2 of the talk are the story.",
            "Is the machine learning perspective.",
            "I don't need to.",
            "We go through this, but just for notation here.",
            "There's no Q, it's just find the discriminant function gamma and so its decision theoretic there's a loss function fee.",
            "It's a function of discriminate gamma and of the label Y, and particularly my mission 01 loss.",
            "Sophie would be that function, and in the binary case you can write that function in this kind of margin, like form, which will be using in the rest of the talk.",
            "OK, so the indicator of disagreement on the label is is it gives you a one.",
            "That's the loss you pay for disagreeing.",
            "OK, so the focus is on estimating gamma alright, so again, for the same reasons in the other literature, it's intractable to minimize 01 loss.",
            "So instead what people have done is said.",
            "Let's look at a surrogate loss function, which is a convex upper bound on the 01 loss.",
            "ANPR"
        ],
        [
            "System properties about that and so.",
            "Here are some of the surrogate loss functions that have been studied in machine learning, literature and statistics literature.",
            "So here's the 01 loss.",
            "It's obviously nonconvex, so intense it turns out to be intractable.",
            "And here's all these convex upper bound, so the hinge loss is the blue one that leads to the support vector machine.",
            "The Green one is the exponential, which leads to boosting and logistic function.",
            "Log logistic is the red one, and so on.",
            "There's lots of others that have been studied there, all convex, and they're all upper bounds on 01 loss."
        ],
        [
            "And you all know what to do.",
            "If you have a surrogate loss which you can do is then plug that in and form an empirical expectation of the surrogate loss informed.",
            "This would be an empirical risk and you would optimize that with respect to gamma.",
            "And you may or may not regularize that, and that would then lead to the."
        ],
        [
            "The theory that you've learned about it, so one paper that I wrote with calling Peter Bartlett and John McAuliffe gave necessary and sufficient conditions for surrogate loss functions to be to lead to consistent estimation, i.e.",
            "If you give me more and more data, I will actually converge the Bayes optimal classifier.",
            "So there's been a lot of sufficient kids around.",
            "We actually were able to find a completely general necessary condition as well, and it's this little condition here which I'm not going to spend time on, but it's really as a statistician what's known as Fisher consistency.",
            "I see Grace here and she's she's studying this as well and people have extended this to multi class classifiers.",
            "It's sort of a pretty natural condition.",
            "It sort of says that on the left side where you disagree with the right label, you pay more loss than on the right hand side where you agree with the right label.",
            "So we call that classification calibrated and it's just a condition on the loss function fee.",
            "It turns out then in this paper in Jazza that this leads to necessary and sufficient conditions for base consistency, and turns out that all those losses that I showed on the previous page are classification calibrated, so they all fit into this framework with lots of other losses due to an.",
            "Here's probably want to keep in mind that if these if you are actually working with convex functions fee, then its classification calibrate if and only if its differentiable at zero and the derivative is negative at zero you tilt up a little bit at the order that's all you need.",
            "For for Bayes consistency.",
            "So if you have Fisher consistent then you get based consistency.",
            "So that's kind of a fairly satisfying theory of surrogate loss functions for binary classification."
        ],
        [
            "Alright, and also it turns out that the meat of this talk is that there is turns out that if let's define a surrogate loss function has to be one that is calcification calibrated.",
            "That's not going to definition of rest talk.",
            "Turns out that there is a link between those objects of mathematical objects.",
            "An after vergence is it's it's a.",
            "It's a many to one correspondence and it's constructive.",
            "And then it's going to allow us to make relationships among F divergences and then translate them into relationships among loss functions.",
            "So we gotta talk about something called universal equivalence, where you get the same answer for all possible probably distributions under.",
            "So loss functions.",
            "There's no real reason to choose between one or the other, except for maybe computational reasons.",
            "So and then I'll show you how to do this.",
            "To go back to the problem."
        ],
        [
            "Talking about about jointly estimating Q and gamma.",
            "OK, so just a little bit of set up here.",
            "The problem then is to optimize the fee risk.",
            "Now that's not the risk.",
            "The risk was the 01 loss.",
            "Now this is expectation of the fee function, so it's expiration of a surrogate loss.",
            "Let's call that the fee risk, and we want to do joint estimation of Q, an gamma.",
            "That's the problem.",
            "It's a bivariate problem, alright?",
            "So let's define unnormalized measures.",
            "Gamma and mu.",
            "Just as these joint probabilities probably have one class and the covariance Z probably other classic over Etsy and you can just rewrite those using class conditional densities where P little P and look you are the prior probabilities of the two classes.",
            "Then the queue is the quantizer and then we're integrating against the class conditional densities in the two cases.",
            "Alright, so if you just use those two definitions and plug them into that expectation up there, you can rewrite.",
            "That thing is just the sum.",
            "Again, we're assuming discrete for simplicity.",
            "Watching this is in the space, we are definitely we are quantizing.",
            "So you just rewrite this as fee times gamma plus fee at minus gamma multiplied by \u03c0.",
            "So this is pretty standard little reduction.",
            "For those of you working classification.",
            "OK, so."
        ],
        [
            "Now we're going to profile.",
            "So what is a profiling?",
            "Profiling in statistics means you have a function of two arguments maximized over one of the arguments.",
            "Right and get a function of just the remaining argument.",
            "So in statistics there's going to flavors of statistics.",
            "There's frequentist and Bayesian, and you often have functions of multiple arguments, multiple parameters, and the Bayesians want to integrate over some of the arguments and reduce that to a function of this one of one of the arguments, so you can always integrate and frequency is used sometimes in great, but more often than not the you maximize, and that's called profiling, so we get a profile likelihood, for example by by maximizing over one of the arguments where is amazing.",
            "It's a marginal likelihood, so we're going to profile.",
            "We're going to frequencies today.",
            "In profile this fee risk, so we're going to particular profile over the gamma function for each for each Z.",
            "So you pin down Z.",
            "Then it's just a number.",
            "And so we optimize it called that are sub fee of Q.",
            "So it's the infime am overall choices of discriminant function.",
            "Of the fear risk.",
            "OK, for example, in the 01 loss, which is a little short calculation that you can do 01 loss can be written.",
            "In that way you can see easily see that.",
            "You want to, you know you minimize the one that would.",
            "You know these are the two class conditional losses and you can re write that in something looking like a variational distance.",
            "In fact it is just a variational distance so it can be written that way, so the profiled loss.",
            "That's what this thing is here actually has the looks like a NAFTA versions.",
            "It's after versions up to a constant.",
            "It's negative and after vergence OK, so that's kind of interesting.",
            "If you profile a few risk you get in after vergence and so we did this at some point and the 1st next question is is that more that more general?",
            "Is that that happen more often when you profile surrogate losses?",
            "Do you get after Vergence?"
        ],
        [
            "It turns out you do so.",
            "These are a bunch of fun little calculations you can do if you take the hinge loss and you profile it out, you get variational distance, not just the 01 loss but also the hinge loss gives you variational distance when you profile.",
            "If you profile the boosting loss then you turns out you get the exponential boosting loss, then turns out you get.",
            "That should be.",
            "It's a typo, it should be Hellinger distance, not variational distance.",
            "That fun little calculation to do an if you profile the logistic loss, you get this object where these are.",
            "Cal divergences this is kind of a symmetrized KL divergent which is called the capacity discrimination.",
            "OK, so this seems to be happening more generally.",
            "We take circuit losses we get after."
        ],
        [
            "Agences and it turns out that there's a theory that we uncovered, which is that this always happens that all surrogate loss functions actually induce by profiling and have to vergence and Moreover corresponding to every after vergence there is a class of surrogate loss functions.",
            "It's many to one that lead to that after version.",
            "So we already saw that with the hinge loss and the 01 loss.",
            "So I'm going."
        ],
        [
            "Talk a little bit more about how this link is established, and so the theory theory behind this really reposes on conjugate duality, which is my favorite topic in an analysis.",
            "If you pick up Rockefeller's book it sometime, you will be rewarded and you will learn about conjugate duality.",
            "Very important topic also knows Legendra transforms and lots of other versions of this.",
            "And just so I'm not going to get into a lot of the convex analysis, but I just want to say this is what drives the theory here, so just define what country duality is.",
            "So if you have a lower semi continuous convex function F. Then the convex duel of the conjugate dual of that is defined as solution to a variational problem.",
            "So you take a linear function of of you, minus the original function, and then you extreme eyes that an you get a function which we would call the conjugal F star of you, and these are dual in the sense that if you now take F star and find its conjugal, you're back to half again.",
            "If you start with a convex function you go to App Store, you go back again to F. But in any case F star is necessarily a convex function even if F was or not.",
            "OK, so that's conjugate duality widely used.",
            "I've worked in variational methods for graphical models and especially for long time, and this is the key tool there as well.",
            "So we're going to find this function would take the conjugate.",
            "Do we actually put a negative sign there and that will be an important object for us fee, but it's basically just the conjugate tool."
        ],
        [
            "Alright, here's the theorem we've proved, so let me sort of go a little bit slow on this so.",
            "For every margin based surrogate loss function Fi, there isn't after version such that the profiled losses the negative of an F. Diversions for some lower semi continuous convex function F. So that's One Direction of the theorem.",
            "OK, Moreover, if F is actually sorry, fee is continuous and then there's a regularity condition, there's a bunch of properties you can prove about this, expressed in terms of this conjugate dual function.",
            "There's a fixed point property, it's decreasing and convex, so it has for example properties.",
            "And then there's another fixed point property as well.",
            "So this is not.",
            "This is really a fixed point property here, and this is kind of an item potency kind of property, right?",
            "But the main then the other content of the theorem is the other direction, which is.",
            "Conversely, if F is lower semi continuous convex an it satisfies these conditions then there exists and go backwards.",
            "There existed the Crest decreasing convex surrogate loss that induces the corresponding after merchants.",
            "So it's kind of a full mathematical tie between surrogate loss functions and after versus both directions."
        ],
        [
            "OK, now the easy direction is from losses to.",
            "Afterwards my picture will be kind of losses in the left efforts in this direction is easy.",
            "Distraction is hard, so this direction.",
            "Here's the proof.",
            "Actually it's really simple, so let's recall that the after version.",
            "Sorry, the feed loss can be written in that form at the top of the page is just a function of fee and then the joint class conditional densities.",
            "And if you do the optimization, the profiling just take the infimum over the argument of fee.",
            "We're fixing Z.",
            "Now when we're inside of that sum, so you just pulled in FEMA inside an.",
            "Now you just do a little transformation.",
            "You pull Pi outside and then divide by \u03c0, and so I get fee plus 50 times the likelihood ratio.",
            "And now if you just let U equal that likelihood ratio and define F to be the thing inside of the northern FEMA with a negative sign, then we just define have to be this way.",
            "And now that function becomes exactly an after vergence.",
            "It's just a sum over.",
            "ZA paisy of a convex function F of a likelihood ratio.",
            "So that's kind of.",
            "That's the steps you go through in each of these individual examples.",
            "And that's just the general story.",
            "OK, and it's not hard to see that F is a convex function, it's just a.",
            "It's it's.",
            "It's a linear function.",
            "A set of linear functions and you're taking the negative of that, so you get a convex function.",
            "OK, so that's the easy direction."
        ],
        [
            "Then the hard direction.",
            "I'm not going to get into, but I'm going to note that it has a constructive consequence which is trying trying to go backwards when you work all about, it.",
            "Turns out you can actually characterize all you can go backwards constructively.",
            "It's not just an existence theorem, so here's the construction.",
            "This is probably last intensive kind of math slide, so any continuous loss function feed that induces afterwards must be of the following form.",
            "So this is all possible loss functions that are coming from and after vergence they have to be this form.",
            "They are the fief, the PSI function, that conjugate dual function.",
            "I defined two slides ago of some arbitrary function.",
            "G will not quite arbitrary has some additions, but it's this degree of freedom in the problem is just some function G for Alpha greater than zero.",
            "In office negative, it's just the function G all by itself.",
            "Now G is just happens to his needs to be increasing continuous and convex and it has to have this fixed point property.",
            "OK, so there's lots of functions that satisfy that.",
            "You can plug any of the men to that conjugate dual function.",
            "So if you start with an F over here find its conjugate dual and plug in any arbitrary function G that satisfy those properties and you will then induce a thief, you will now write down a fee function, which is a convex surrogate loss.",
            "So if you hadn't learned about convex sort of losses because Vapnik told you about hinge loss and Rob Shapiro told you about exponential loss and so on.",
            "This would have been another recipe to generate a whole family of convex circle losses, all of which lead to base consistency of estimators."
        ],
        [
            "That's not what happened historically, but it could have happened, right?",
            "So let's look at some examples of this.",
            "So, hell, injure distance.",
            "So that's not a loss function that's Afd vergence, so I'm over in the Vengeance world.",
            "I write down the hedger distance and I plug in a bunch of choices of the G function.",
            "So here's the ones that we chose there that exponential that you and that you squared.",
            "And those then are plugged into that that the previous slide.",
            "We get the fee out.",
            "And here it is.",
            "Those are bunch of loss functions, all of which are rising from the same after vergence.",
            "OK, so in particular, if you had chosen so you get hallager distance.",
            "That should have been G over here when geoview is equal to that, you get Helander distance between G is some of those other objects, you get, other other other things.",
            "OK, and here is actually the calculation for Hellinger distance of that conjugal.",
            "You can just do that calculation if you choose G equal to the this particular function, then you get the exponential loss.",
            "Phone.",
            "Actually, what am I saying or the top?",
            "But that was not a G over here that this is just the after versions were starting with.",
            "And we're going backward in recovering expensive loss by choosing this particular G function.",
            "I should have used you in that argument up there.",
            "OK, so that's a longer distance."
        ],
        [
            "So what about variational distance?",
            "Well, variational distances as we learned earlier, can be written as an after vergence, and this is the F function.",
            "Now you can do the conjugal calculation and turns out that size just the order that it's a little simple calculation.",
            "And now if you choose G to be just you, then you get out the hinge loss.",
            "OK, so you can kind of sort of see how this is."
        ],
        [
            "Recipe to generate things.",
            "What about KL divergent?",
            "Right, well there is no fee loss for the asymmetric functions.",
            "KL divergences are asymmetric, but if you symmetrize, then that is leads to an after vergence an there.",
            "In that case you can realize it.",
            "With with this particular choice of function OK, so enough on that those are examples."
        ],
        [
            "OK, so that was kind of the set up.",
            "Now we have a link between after Vergence is an FFI functions and now we can start to go off and try to prove Bayes consistency for choices jointly of Q, an Lambda or gamma before it's now been called Lambda.",
            "For some reason.",
            "OK, so how would we do this?",
            "So first of all, let's start with a 01 loss.",
            "Given the 01 loss, we can obtain the variational distance as.",
            "The corresponding after vergence where we use that function right now what I'm going to argue is that the way to solve this problem is to not consider just that specific F divergent which we got from this loss function.",
            "But to consider slightly broader class of after vergence over in that right hand space, we're going to not just take a point where it takes another after versus around that, and the way we do that is, we take the men of you, one.",
            "That's the one that we started with.",
            "Put a - in front of that, and then take this little affine combination of those after version.",
            "So now get a little little manifold of F divergences a little broader class, and that way, alright.",
            "And now let's range overall points, possible choices of these constants ABC and we will get our family.",
            "Of continuous convex and classification calibrated fee losses, all of which could be obtained from those after purchase and all of which have these nice properties.",
            "So we're getting.",
            "So we're starting on the left hand side.",
            "So here's our F divergences.",
            "Here's our corresponding feed losses.",
            "We have this one to one mapping.",
            "We're starting with the hinge loss over here or size 01 loss, and we're mapping that over to a particular F. Then we're broadening that to a broader class of F divergences without a laugh line combination.",
            "Then we're going backwards, and we're getting this little set here that all map into this guy, of which 01 loss is 1 particular.",
            "So we get a set of feed losses here.",
            "OK. Alright, so we're going to provide conditions under which all of these feed losses healed based consistency, and it turns out that that's a necessary condition as well, only these feed losses obtained in this way achieve based consistency jointly for Q and an and discriminate function, right?",
            "So there's going to be some losses that would work for classification, but won't work for the joint problem, 'cause this is going to be necessary and sufficient condition."
        ],
        [
            "Alright, so this part of talks kind of boring.",
            "It's just sort of standard consistency arguments and so let me not belabor it, but just sort of kind of just here it is, you know, we're going to kind of cevs, I guess.",
            "Do is with one of the originators of these ideas.",
            "We're talking about nonparametric classes.",
            "We look at classes that get larger and larger, and we control in some sense the covering numbers.",
            "Now it's total control.",
            "This disco fluctuations, so we have a sequence of function classes in which our discriminant functions are going to live here.",
            "Get bigger and bigger and bigger and.",
            "Let's assume that there exist an Oracle that is able to solve this particular risk minimization problem.",
            "So we take our empirical risk there.",
            "It's written out where we've now summed over the data points, but we're also averaging over our quantizer, 'cause we have this stochastic quantizer that Maps the axes in disease and that overall function is what we're trying to optimize.",
            "So let's suppose that we can do this over these compact subsets.",
            "There's some computational procedure that does that.",
            "Alright, man, let's call this one such solution to that optimization problem.",
            "OK, and now let's denote the best you could possibly do is the minimum Bayes risk where you optimize jointly over the quantizer.",
            "In over this covenant, where are Sobeys means 01 loss, so this is the risk function where you're using 01 loss.",
            "That's what we're trying to do in this problem is minimized 01 loss, so let's call that the Bayes solution.",
            "Best you can do so.",
            "The SXS Bayes risk would be the risk that you get from this optimization procedure minus the best you can do."
        ],
        [
            "And then you the standard thing to do at this point is to divide up the problem into approximation error an estimation error, so the approximation error is you're looking at the actual fear risk up there.",
            "There's no hat on top of that thing, and subtracting off the best you can do so that's an approximation error.",
            "There's no statistics up there, and then down here is the estimation error where we're taking an expectation of this.",
            "Estimation problem where we getting we're working with the estimated the empirical risk subtracting off the actual risk, so our focus will be not the approximation story.",
            "A lot of people have worked on that, but on the estimation."
        ],
        [
            "OK, and so we're able to prove a theorem that kind of pulls together various pieces that various people have worked on in this particular context, and so we're able to show that under the usual kind of conditions that drive approximation error to zero, an estimation error to 0 and under the additional condition on fee that it's one of these losses that I've I've expressed in terms of divergences.",
            "Alright, that excess risk can be written in terms of an estimation error.",
            "And then another term which is defined here that.",
            "Is for kind of all the you know, it's easy to to evaluate on particular problem for all the standard problems this thing can be shown to be finite, so you just need to show that this particular kind of maximized expression is finite and it is for for the usual source of set up.",
            "OK, so under the usual kind of conditions plus this condition on the fee risk, we get actual base consistency now for jointly choosing Q and gamma, right?",
            "And this is the important part of it, which is that you have to do it according to this recipe."
        ],
        [
            "OK, so that's kind of a nice theorem and kind of extends classification into this external design setting, but turned out there was a deeper idea here, and it really was when we started finally started reading David Blackwell's papers in the 50s that we work this out.",
            "So this guy, we're going to call this notion universal equivalence of loss functions.",
            "So everything in this set over here in the loss function cited, these guys are going to be called University equivalent on Earth under this new this definition.",
            "I'm not ready to to mention, alright, so let's consider two loss functions Phi one and Phi 2.",
            "And let's suppose they correspond to F divergences induced by F1 and F2.",
            "Alright, we're going to call them universally equivalent and denote it that way.",
            "Alright, if for any probability distribution so completely nonparametrically an any quantization rules, QA and QB, so you're looking at one condition rule QA, you're looking at QB.",
            "We're trying to evaluate which is best and we try to use nonparametric.",
            "Any possible choices?",
            "Probably distribution.",
            "Well, if it turns out that fee one ranks these two procedures in this way, that QA is better than QB if and only if that also happens for fee two.",
            "That if he wanted feature set to be universal equivalent, their ranking procedures in the same way.",
            "Under all possible distributions, so their their equipment.",
            "So that's a definition.",
            "Natural definition of equivalence."
        ],
        [
            "Right, and here's the last theorem.",
            "Then in the talk, which is that you have equivalence among loss functions if and only if the corresponding after virgins are related in that half fine way.",
            "OK, so that says that if you are interested in equivalence of loss functions, you're trying to say I have a whole bag of loss functions over here, and I'm trying to kind of characterize that space, you know.",
            "Is this loss function really that different from this one from this one?",
            "This one.",
            "Well, I might kind of like to find equivalence classes here and say everything in this class is actually the same.",
            "Give me the same answer.",
            "And this class is well away to do that, by the way.",
            "To do that is to go over this other space of F divergences, find the corresponding afterwards, and if they are related by this app line function, then your University equivalent back in the space.",
            "OK, so that was kind of surprising and would you know?",
            "I think it's nontrivial, right?",
            "This other class of mathematical objects over here is kind of the right way to set up the notion of equivalence back in loss function space for decision theory.",
            "OK, and again the proof is in this paper.",
            "The next, the necessary condition in this case is easy and the sufficiency is not easy.",
            "And in particular, now we can go back to zero and loss, which is what our original interest in this class of problems was, and so we can start with 01 loss here.",
            "And we can ask what's all classes of functions which are universally equivalent to 01 loss while we go over the course money after versions?",
            "That's the variational distance.",
            "Then we go back.",
            "Then we do the laugh line expansion.",
            "Over here we go back over.",
            "Here we get a bunch of other losses which are universally equivalent to 01 loss, in particular, hinge loss is in this set universal equivalent to 01 loss.",
            "Under this theory.",
            "OK, so all those V losses if you use them in your learning problem will give you the same answer as as the hinge loss.",
            "You might prefer one or the other and the 01 loss.",
            "For computational reasons you know etc etc.",
            "There's lots of other reasons could be there, right?",
            "But there are some other functions over here which are outside of this set.",
            "In particularly the boosting loss, the exponential losses outside this set.",
            "I mean it's not universal equivalent to 01 loss.",
            "Well, that's a problem, right?",
            "That means there exists some probably division P where you're not getting the right in the same answer 01 loss.",
            "Of course, the problem is the same as you were trying to do, right?",
            "So that loss function if you try to use it to optimize both the quantizer in this current function under some probabilities will give you the wrong answer.",
            "It's not consistent.",
            "OK, now that's not true.",
            "If you're just talking about learning discriminant function.",
            "Boosting has some consistency proofs.",
            "Alright, but it is true in this broader class of problems.",
            "OK."
        ],
        [
            "I think I'm gonna see if yeah."
        ],
        [
            "Let me just pause for a second there and see her questions.",
            "Yes, there was a question.",
            "Awesome station.",
            "Space.",
            "Statues sequentially for small numbers is that also in the category of quantization for jumping, but or is it King Roger Azatian?",
            "It's in the category mathematically, whether it would be the right kind of approach to that problem, whether it would be useful now whether it be useful, isn't it would be need to be looked at, and we definitely are applications of this have all been to these kind of distributed kind of problems, not just classification of others, and I would like that kind of problem that needs some more thought.",
            "Kind of, you know, really what you're asking is kind of is like for problems like choosing basis functions should be viewed as an experiment design problem.",
            "And certainly mathematically can be.",
            "And maybe it's really actually interesting had been looked at by me and I don't think by anybody else.",
            "Any other questions?",
            "I know this kind of looks like a tutorial flavor here, so people are getting lost or whatever it want to.",
            "Ask a dumb question or just make me pause for a minute.",
            "I'd be happy to do that, yes?",
            "Wondering is.",
            "Margin, hotel view or some other cost functions?",
            "And yes, can we see something which is equal something different on that because of the steep endpoint agent that you sent here?",
            "Well, I may not be interesting question entirely right where you're asking.",
            "What's the margin part of you?",
            "All of the classifiers were looking at her margin based classifiers.",
            "OK, so we're all aware that during that families, that's what surrogate loss functions kind of means, and So what we're saying is that some lost some margin based loss functions are perfectly fine for normal, old-fashioned classification, would just get the discriminant function, but are not fine when you actually had also the experiment design problem where we're separating those out, yeah?",
            "Class.",
            "Yeah, so that's the approximation estimation part of the story is you just have to be in a rich enough class that you drive those errors to 0, but there's nothing.",
            "We're not saying anything new about in there.",
            "Framework.",
            "What happens to this framework?",
            "Well, that's good.",
            "I mean we would not have a consistency store that we drive something to zero.",
            "We might have a great story that we drive something at some rate down to something jointly.",
            "Ann, and you know that would be kind of a mathematical exercise that would involve some approximation theory, some estimation theory and all this convexity theory all done together, 'cause you wouldn't be able to separate them out.",
            "We kind of separated that part of the problem by assuming we're rich enough that the non parametric stuff that everybody else is done is taken care of and we're now adding this ingredient of the experiment design.",
            "But you're right as part of the general picture here.",
            "Over time we would want a very general theory that had all those ingredients kind of competing against each other and we're just going to put in that other piece of the picture.",
            "And in one regime.",
            "But you're right, we should consider the other regimes as well.",
            "Yes.",
            "What does it mean exactly?",
            "Universal definition had nothing to do with these after version, since that's the theorem that related them.",
            "The definition is really pretty simple."
        ],
        [
            "It's it's this one and this is black.",
            "I should mention, this is Blackwell's idea actually, so Blackwell actually did this 401 loss.",
            "This was his definition and he showed that under this definition that put that theorem I showed about after versus under 1401 loss, and what we've really done.",
            "It just take Blackwell's idea and extend it to all the surrogate loss functions.",
            "And turned out, that's that's what's happening here.",
            "So here this this idea, and this is the one that actually gets used.",
            "A lot of economics and preference rankings and kind of the competitions using zeolite.",
            "It just says that if you have a risk function based on some feed loss, that's a hinge loss and it ranks of procedure QA as being better than a procedure QB under under under risk.",
            "Alright, so you're using his lesson.",
            "He over there is using exponential loss.",
            "Right, and he gets exactly the same ranking for those two procedures, but not only those two, but for all pairs of procedures and then under all possible choices, prohibition P. If that happens, then those two losses are really the same and we call them universally equivalent.",
            "So the universally really kind refers to this nonparametric story.",
            "This should be true in all possible probably distributions 'cause you don't know.",
            "The problem is not very OK, so it turns out that Ken's loss and 01 loss are equivalent.",
            "This way they rank procedures exactly the same way under all possible distributions, so they can give you the same answers under all choices distribution, whereas hinge and our side 01 an exponential laws don't.",
            "OK, so is that clear that definition?",
            "It really just kind of ranking that these two risk functions based on those losses rank things the same way no matter what day.",
            "Do you have?",
            "Yeah.",
            "Continuous feature extractors instead of quantizers.",
            "Yeah, this actually is results have nothing to do with the discrete part.",
            "This is just kind of presentation.",
            "To make it, there's a little bit of measure theory that you kind of start to worry about, but there's really nothing.",
            "It just convexity.",
            "Convex analysis is driving all this nothing without discrete continuous.",
            "Calico straight or constraint set notice.",
            "In some places, without getting to basically anything.",
            "Yeah, that's the same question individually asking is kind of we don't take rich function classes and make our estimation and approximation is not necessarily go to zero.",
            "You know, do we have rates?",
            "We start talking about how that trades off against our story, and again I didn't say answer.",
            "OK, good thanks for those questions.",
            "So I'm definitely not going to be late 'cause how kind he was to me.",
            "So let me kind of just kind of mention a little bit at the end of the talk here.",
            "Kind of dessert so.",
            "Let me see where was I."
        ],
        [
            "Right, OK, so this is kind of that I'm done with that part of the talk.",
            "That's one paper.",
            "There's another paper that you can find on our websites.",
            "I don't think I mentioned my colleagues.",
            "This is all done with you online again.",
            "Who was a student with me and Martin Wainwright at Berkeley?",
            "Who's now would be a faculty member at University of Michigan.",
            "Ann with Martin Wainwright, who's my colleague at Berkeley?",
            "So in another paper we have now we're kind of interesting divergences.",
            "You know, we're always interested.",
            "Loss functions.",
            "Now we got interesting divergences.",
            "And So what about divergences?",
            "For example?",
            "Can you estimate divergences?",
            "Give me some data, and you know two distributions.",
            "If I had the populations, there would be a well defined number which is the divergent between those populations.",
            "If you give me a finite sample from those distributions, can you estimate the vergence?",
            "Something guesstimate the entropy that Sanada vergence?",
            "Can you estimate the KL divergent's?",
            "So interesting problem.",
            "In fact, if you want to optimize, if you want to maximize the mutual information which lots of people want to do for various problems, you need to estimate the mutual information and that turns out to be.",
            "That's hard.",
            "OK, that's not a solved problem.",
            "Lots of papers in information theory talk about estimating the entropy, often in discrete settings.",
            "But really you want to do some continuous settings.",
            "So let's in particular look at the problem estimating the KL divergent's.",
            "So we have these two distributions Q&Q.",
            "With densities so we can define this this scale version.",
            "So here it is.",
            "Now let's talk about continuous things and we'd like to estimate that giving samples from P&Q and there is a lot of work on."
        ],
        [
            "Best Buy a lot of well known people, so good papers by Bequelin Rd Rita Resoft.",
            "Donahoe has an old paper on this bear.",
            "Jamos are long and various others.",
            "Ann, what's hard about this problem?",
            "It's that."
        ],
        [
            "Functional two densities, but the real hard part is that you have a density P an you have log.",
            "P&P is not unknown.",
            "You're getting data alone.",
            "You're getting data from P, so you could do a density estimate of P But you also have to do a density of log P. Those are on different.",
            "Those are different problems and you can't find good bandwidth for both of them simultaneously.",
            "That's kind of the core of the problem.",
            "You have P appearing twice, not just once, but once it does density estimation, but it's twice."
        ],
        [
            "OK."
        ],
        [
            "OK, and so it turns out the theory I presented actually speaks directly to this so.",
            "In that little variational formula, when we talk about conjugate duality, there was a way to lower bound these divergences.",
            "You can write them actually equality.",
            "It's in general inequality, but you get equality at a certain conditions.",
            "I guess here we've got the condition right here.",
            "There's some subgradient is intersect intersects the function class.",
            "But anyway this is the KL divergent can be written with equality as just a variational problem.",
            "It's the supremum or all functions of F average average under Q minus the conjugate dual function.",
            "Phi DP OK, and so even if you haven't seen the first part of the talk, you could just plug.",
            "You could just do this variational problem on the right hand side.",
            "Optimize over F and you will recover the KL divergent's so that you can check that this is correct.",
            "Alright, but anyway it comes out of the theory.",
            "Alright, and so that's pretty cool, because now this is a variational problem and you can now.",
            "Conceptualised, estimate those terms and then optimize overall so you can now turn this into an optimization problem, i.e.",
            "Do in estimation, empirical risk minimization, whatever.",
            "So you can now optimize over a function class F. Hopefully chosen rich enough that the optimum of that function actually gives you a good estimate of that.",
            "So that's a new approach to."
        ],
        [
            "Divergent estimation right?",
            "So particularly the vergence that variational problem just reduces to this where Phi is chosen to be that log of mu minus one.",
            "So you plug that in and this would be an even easier to exercise to verify.",
            "You can just optimize on the right hand side, you will get the elder vergence.",
            "And the optimas attained."
        ],
        [
            "OK, and so now we have a new procedure which is that, let's say get some data from P&Q and define the empirical measures P&QN and then plug in the empirical measures into that formula, take the supremum over your function class and you get a number and let's call it the estimate of KL divergent."
        ],
        [
            "OK, and so we've done that.",
            "We've worked out some theory for that.",
            "And, um.",
            "It turns out that you do that as an empirical risk measure, doesn't work very well, and you need to regularize and we worked out with the right regularizer is it's this expression here, and I'm going to cut this part of the talk short, but just let you know there is a regularization precede."
        ],
        [
            "For this, an without regularization procedure, we're able to get convergence rates for this.",
            "So if you chose the regularization thing in a certain way, then under Helander distance you can get a certain rate of convergence of this thing.",
            "And also the regularizer."
        ],
        [
            "Is a certain rate, so we can kind of do this and but what about empirically?",
            "Well, this turns out to be a successful and maybe even state of the art procedure.",
            "So kind of one of the ones that's mostly widely used is this thing called WKV, which I forget who the people there free people via their do, and I forget who the first 2 or if someone can remind me.",
            "This is some information through the kind of history.",
            "It's an adaptive histogram kind of procedure that they have, and so that's these curves.",
            "The these are estimation of the killer versus between two particular densities.",
            "This is number of data points and the right answer is that straight line there, and so the convergence rates of that state of the art procedure are given by those two curves in our new procedure is converging much much faster than in particular is conversion extremely fast, so these are just particular choices of our."
        ],
        [
            "Variance of our procedure.",
            "So I'm going to skip to the end.",
            "We just done a bunch of under those simulations and."
        ],
        [
            "There's a theoretical result.",
            "OK, so I'm done.",
            "Just to reiterate what I said now several times, the main meat of this theory was that we've formulated a link between F divergences, which you probably heard about with other names around them.",
            "They are very widely used, very interesting, and this object that we've been studying and learning theory for awhile now called serverless functions.",
            "It gives a decision theoretic perspective on average when I give a talk this talk to eat people.",
            "It's a very different talk and it's more you've been talking about for a lot of years.",
            "Actually, decision theoretic formulation of them.",
            "They're not just kind of heuristics that people come up with, but then back in the other direction they allow you to design equivalent classes of loss functions.",
            "It's worth going there, and there's lots of possible applications.",
            "This is kind of a first paper on this topic, so if you're interested in joining, there's tons of applications not just to decentralize problems with other kinds of experiment design problems, so thank you very much.",
            "Did I get all the questions out of the previous break?",
            "Yes.",
            "No, I didn't turn the multiclass case at all, and that would be a wonderful problem to tackle.",
            "Um?",
            "In fact, I mean it would be fun.",
            "I mean, it's just there's gonna be some convex analysis of the multiclass case, so they won't be after vergence anymore.",
            "He'll take multiclass losses, of which there are some for some theory.",
            "Now on again, Grace would be a person asked if you don't already know about this, but there's some theory on surrogate losses for multiclass that are also consistent.",
            "Some of the natural ones are not consistent.",
            "It's a harder problem, but there are now known some that are consistent and then you could ask if I profile them out, I'm going to class of mathematical objects will have some convexity properties.",
            "They won't have a name on them like Elvis Sylvia's.",
            "Then they may have been studied by some mathematician, but or you might build your own name on them, I don't know, but there will be some interesting class of functions.",
            "Yeah.",
            "Losses that map the same activities.",
            "Are there any interesting also found in the same beverages with zero one month after?",
            "That's why I was saying the hinge loss Maps to the same after versions of 01 loss.",
            "Exact same one.",
            "It's just a different G function when you're going backwards direction.",
            "This degree of freedom that may be uninteresting or agency for competition reasons but not statistically interesting.",
            "Or for rates maybe.",
            "Yeah.",
            "Functions like after this I mean you said cool, awesome but equal into the active agents have followed that rule that respected afterwards.",
            "Yeah, if they map to the same after versions and their equivalent kind of trivial consequence of our theorem, but it's broader class of every guys, half an expansion.",
            "All of those also equivalent.",
            "Say something like.",
            "So also if you order within a linear factor of each other or something, and so every every word, it's kinda says the odds between two Power Distribution.",
            "So we also linear factor of each other.",
            "That's kind of interesting thought.",
            "Try to think about that, not all after versions are kind of odds.",
            "I mean they're all functions of likelihood ratio.",
            "Their convex functions like the ratio.",
            "Better, I haven't actually gone for intuitions here, it's just that this is work that's been kind of drive convex analysis forward, and but definitely worth trying to think about that.",
            "I don't know.",
            "Yeah, it's definitely there.",
            "Functions likelihood ratio that's critical here that all this kind of a name in person is lurking under this.",
            "All this stuff too.",
            "Yes.",
            "Logistic regression.",
            "Functional.",
            "Gold is pronounced in character hey Elvis.",
            "So you're not interested in the Mail.",
            "This is almost that part that depends on when to implement, yes.",
            "Better than that, so that's a great question.",
            "And I'm not going to think on my feet fast enough to say that this was all done, really.",
            "It's a general theory and then all the examples have been done centered around 01 loss.",
            "But you know, there would be a recipe.",
            "I would know the first kind of steps to take to look at that and do you have a particular paper mine which has another, the kind of the best current way to do that?",
            "Accuracy over 10 years ago.",
            "OK, that might be a good another example for us to to look at.",
            "Yeah.",
            "Yeah.",
            "No, we have a short list of other problems.",
            "Look at with profiling.",
            "There's lots of kind of profile methods and stats that are that this could shed some light on, but that was a good example.",
            "Where the loss functions got is governed by it's a sub part of the problem and it's yeah.",
            "Yes.",
            "Any cost function?",
            "If two loss functions are equivalent, mean two after version or problem if the after equivalent, all the corresponding loss functions are equivalent.",
            "That's that's part of the theory, so you can talk about after versions being equivalent.",
            "That just means they're related by this fine thing.",
            "And what is surprising is that they go backwards and induced that all the course.",
            "My loss function requipment in this way.",
            "But the kind that was explained to the woman in the back, the definition of equivalence is really in terms of loss functions and their ranking properties and they kind of know that induce equivalency after verses that turns out to be this fine thing.",
            "But you would never defined it that way.",
            "Kind of some problem domain POV.",
            "It's just a mathematical definition.",
            "It turns out to be the right one.",
            "OK thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with that, I'm very happy that we have Mike Jordan and he'll speak now.",
                    "label": 0
                },
                {
                    "sent": "Do you wanna someone able to turn this thing on so I can move around?",
                    "label": 0
                },
                {
                    "sent": "It's not, it's not working, testing, testing.",
                    "label": 0
                },
                {
                    "sent": "So I can't see it.",
                    "label": 0
                },
                {
                    "sent": "My eyes are not good at is on its way on.",
                    "label": 0
                },
                {
                    "sent": "Is that thing?",
                    "label": 0
                },
                {
                    "sent": "On off button you pleasure to be here.",
                    "label": 0
                },
                {
                    "sent": "I want to thank the organizers for having me and let me thanks too for his flexibility.",
                    "label": 0
                },
                {
                    "sent": "I have a flight from O'Hare later today and it was going to be possibly Mistified started later, so thanks just too.",
                    "label": 0
                },
                {
                    "sent": "So this is a favorite topic of mine.",
                    "label": 0
                },
                {
                    "sent": "This took several years to work out.",
                    "label": 0
                },
                {
                    "sent": "There is a paper appearing in the Annals of Statistics or actually just appeared in statistics on this.",
                    "label": 0
                },
                {
                    "sent": "So if you turn out to be interested in it, it's a 50 page paper with lots of convex analysis.",
                    "label": 0
                },
                {
                    "sent": "And I thought I might only speak for a short time and then off the airport for 30 minutes.",
                    "label": 0
                },
                {
                    "sent": "So I kind of prepared the short version talking.",
                    "label": 0
                },
                {
                    "sent": "I don't know what I'm going to do in the longer version.",
                    "label": 0
                },
                {
                    "sent": "I won't get into convex analysis, but it's there if you're curious.",
                    "label": 0
                },
                {
                    "sent": "So this is a topic that unifies a number of areas.",
                    "label": 0
                },
                {
                    "sent": "It has a strong information theory content.",
                    "label": 0
                },
                {
                    "sent": "It has a strong convex analysis content, and then in a core statistics content it really goes back to work in the 50s by David Blackwell and turned out that David's work back in the 50s had a big influence on economic, so there's not going to be any economics percent in the talk.",
                    "label": 0
                },
                {
                    "sent": "But that connection is latent under the toxicity of interests in economics might find some connections as well.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we started in this problem in a very prosaic way.",
                    "label": 0
                },
                {
                    "sent": "It was just we had a statistical problem on our hands and we aimed at trying to develop an algorithm to solve it.",
                    "label": 0
                },
                {
                    "sent": "We want to prove something about that algorithm and we couldn't with all the standard tools.",
                    "label": 0
                },
                {
                    "sent": "We couldn't prove anything.",
                    "label": 0
                },
                {
                    "sent": "What we wanted to prove about it, and so we were going to theoretical talk showing how we can prove something that was the problem.",
                    "label": 0
                },
                {
                    "sent": "Well, this is just a simple so called decentralized detection problem.",
                    "label": 1
                },
                {
                    "sent": "So we had a sensor network that colleagues had set up at the Intel Labs in Berkeley, and so there were a number of modes on a grid.",
                    "label": 0
                },
                {
                    "sent": "The modes are sitting up there on the right, and.",
                    "label": 0
                },
                {
                    "sent": "There was a light source being held up above the motes and you are simply to detect where in three dimensional space the light source was, or really what is projection was onto the plane.",
                    "label": 0
                },
                {
                    "sent": "So in particular draw green area on the ground.",
                    "label": 0
                },
                {
                    "sent": "Then you would tell me whether the light source was above the green area or not.",
                    "label": 1
                },
                {
                    "sent": "OK, and so this is just a classification problem and the covariates are all the voltages coming from the sensors.",
                    "label": 0
                },
                {
                    "sent": "But it's a distributed or decentralized classification problem, because I can't transmit the real vector of measurements back from all those motes, they have really small, long, short life batteries.",
                    "label": 0
                },
                {
                    "sent": "And if you transmit a lot, you're going to run out of battery real quick, so you can only transmit a little bit at every time slice.",
                    "label": 0
                },
                {
                    "sent": "So it becomes a quantization problem.",
                    "label": 0
                },
                {
                    "sent": "Each one of the modes has got to decide how to quantize it's real number, the the signal strength that it's measuring from the light light sensor.",
                    "label": 0
                },
                {
                    "sent": "We want to transmit the quantized values back to a central computer who does a discrimination of some kind.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do you quantize to do good discrimination?",
                    "label": 0
                },
                {
                    "sent": "OK, OK, that problem is not new IT is been around in the 60s electrical engineering.",
                    "label": 0
                },
                {
                    "sent": "It was.",
                    "label": 0
                },
                {
                    "sent": "It was studied some degree and there were a number of methods developed that were really heuristic.",
                    "label": 0
                },
                {
                    "sent": "They're widely used.",
                    "label": 0
                },
                {
                    "sent": "A radar is full of them, but they're fundamentally Ristic, so I'm going to show you what those methods were and how they relate to things which are less heuristic.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so let's start to try to formalize the problem a little bit.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of broader classes of problems, as I think you'll start to become aware, but let's look at the simplest one.",
                    "label": 0
                },
                {
                    "sent": "Just a classification problem and do it in a decentralized way.",
                    "label": 0
                },
                {
                    "sent": "OK, so binary problem, binary classification, so we have some hypothesis.",
                    "label": 0
                },
                {
                    "sent": "World State is 1 or minus one by the light is above the green region or not.",
                    "label": 0
                },
                {
                    "sent": "In conditional on that there are a number of real valued observations had sites, one through capital S. Find is a little bit loud.",
                    "label": 0
                },
                {
                    "sent": "Adjust those are real numbers and then they pass through Quantizers Q1 through QS, which are local.",
                    "label": 0
                },
                {
                    "sent": "There's one quantizer for every sensor.",
                    "label": 0
                },
                {
                    "sent": "And outcome Z1 through ZS which are quantized values.",
                    "label": 0
                },
                {
                    "sent": "The axes are probably going to be digitized in the beginning, so they would be in a set of cardinality M and there are quantized down to a set of cardinell.",
                    "label": 0
                },
                {
                    "sent": "Rail is much much less than M and these quantizers for us we're going to often be stochastic, so Z is going to be a stochastic mapping from XA kernel and then based on the Z values were going to form a discriminant function gamma of Z1 through ZS, and then we'll make our decision based on the discriminant function.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the general formal setup would be there.",
                    "label": 0
                },
                {
                    "sent": "We had X XY pairs, ID for simplicity and Z of Q of X is the covariant vector and Q is a possibly random mapping an.",
                    "label": 1
                },
                {
                    "sent": "In statistics we would call this an experimental design.",
                    "label": 0
                },
                {
                    "sent": "OK, so design is a really broad notion.",
                    "label": 0
                },
                {
                    "sent": "It's not kind of just, you know.",
                    "label": 0
                },
                {
                    "sent": "You know I have a little grid of some kind.",
                    "label": 0
                },
                {
                    "sent": "It's that for every entity, every object in some statistical problem, you will allocate it to some to some covariant.",
                    "label": 0
                },
                {
                    "sent": "So in the case of analysis, variance would allocated to asellina, covering analysis, brain stable.",
                    "label": 0
                },
                {
                    "sent": "You might do that randomly.",
                    "label": 0
                },
                {
                    "sent": "That would be the simplest possible experimental design, but now you can do that in many other ways, and the general field is that of studying these mappings Q.",
                    "label": 0
                },
                {
                    "sent": "So let's call that an experimental design, and now we also have a discriminant function which lies in some non parametric family capital gamma and so those could be support vector machines or whatever else you've been learning about this week.",
                    "label": 1
                },
                {
                    "sent": "Anything you like, that's a good.",
                    "label": 0
                },
                {
                    "sent": "Hopefully nonparametric, i.e.",
                    "label": 1
                },
                {
                    "sent": "Comes from a large family discriminant function.",
                    "label": 0
                },
                {
                    "sent": "And now the problem is to find the decision which is both the quantizer Q.",
                    "label": 0
                },
                {
                    "sent": "Let's call it a quantizer or experimental design and discriminant function.",
                    "label": 0
                },
                {
                    "sent": "Do that jointly, right?",
                    "label": 0
                },
                {
                    "sent": "So we would know how to do those two things separately.",
                    "label": 0
                },
                {
                    "sent": "Would know how to find discriminant functions if you already built the quantizer, that's just the SVM or the whatever.",
                    "label": 0
                },
                {
                    "sent": "And as I'll talk about, we would know how to do the quantization if you've already fixed the discriminant function, but we want to jointly, and we're going to have a 01 loss, or our criterion is just the probability that Y is not equal to what the quantized and then discriminate function.",
                    "label": 0
                },
                {
                    "sent": "Juices, and so there are lots of other possible problems with this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This just on classification.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are perspectives on this.",
                    "label": 0
                },
                {
                    "sent": "In the signal processing literature going back to the 60s, there's been a lot of work on this and what they have assumed is that everything is known except for Q, so they assume that all the class conditional probabilities are known.",
                    "label": 1
                },
                {
                    "sent": "So if you know the conditional probability, you can build an optimal discriminant classification classification of the Bayes classifier, and you assume that you can get that even though it might be hard, but you assume you can do that.",
                    "label": 0
                },
                {
                    "sent": "So let's assume known and all that's left is just the quantizer.",
                    "label": 0
                },
                {
                    "sent": "OK, and how do you find the quantizer?",
                    "label": 0
                },
                {
                    "sent": "Well, conceptually what you're trying to do is that I should have had a picture.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can do it over here.",
                    "label": 0
                },
                {
                    "sent": "There's a cloud of data for class One, and a cloud of data for Class 2 in the original.",
                    "label": 0
                },
                {
                    "sent": "Backspace OK, here's some data X for class one, and here's some data for class minus one in the X space.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to transform that into a Z space.",
                    "label": 1
                },
                {
                    "sent": "And I can do that with adaptive quantizer.",
                    "label": 0
                },
                {
                    "sent": "And so these things will hopefully spread out if I choose a good quantizer in the space.",
                    "label": 0
                },
                {
                    "sent": "Spread out so that I can get a better discrimination.",
                    "label": 0
                },
                {
                    "sent": "OK, and so I would like to develop a criterion for choosing Q that in some sense measures the spread among these distributions.",
                    "label": 0
                },
                {
                    "sent": "How do you measure spread among distributions with lots of heuristic things?",
                    "label": 0
                },
                {
                    "sent": "You can do?",
                    "label": 0
                },
                {
                    "sent": "You can just measure pellinger distance or or kaldis divergents or whatever and try to maximize that so it's not it's not a minimization problem Now as it is for the discriminate.",
                    "label": 0
                },
                {
                    "sent": "It's a maximization problem, so that was done.",
                    "label": 0
                },
                {
                    "sent": "Chernov distance was widely used Helander distance.",
                    "label": 0
                },
                {
                    "sent": "It's basically he ristic letter that was not approved about these things, and they were chosen just because it turned out that the optimization problems tractable for some choices.",
                    "label": 0
                },
                {
                    "sent": "If they say Helander distance and not tractable for others.",
                    "label": 0
                },
                {
                    "sent": "So they focused on the ones that were tractable.",
                    "label": 0
                },
                {
                    "sent": "They had some kind of NP hard type proofs.",
                    "label": 0
                },
                {
                    "sent": "Should I make tractable least rigorous?",
                    "label": 0
                },
                {
                    "sent": "But they didn't have statistical story.",
                    "label": 0
                },
                {
                    "sent": "OK, and the general objects that were installed after virgins is also known as Elvis, Sylvie Divergences, and they have lots of kind of information, theory properties, convexity, parsing.",
                    "label": 0
                },
                {
                    "sent": "Prove people proved a lot of things with those properties, but again there was no statistical story about why should I care about those properties from the point of view of a performance of some system.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was one literature which is, you know, had a 20 year run Tom Kailath, one of the main people who worked in the literature.",
                    "label": 0
                },
                {
                    "sent": "A lot of really strong E people.",
                    "label": 0
                },
                {
                    "sent": "On the in the machine learning literature, of course, that's what a lot of you are here, because you're interested in really has been very little focus on the quantizer or the experiment design usually assume that's known.",
                    "label": 0
                },
                {
                    "sent": "All the preprocessing you got your feature vector and now find me an optimal discriminate.",
                    "label": 0
                },
                {
                    "sent": "So huge large amounts of literature on that.",
                    "label": 0
                },
                {
                    "sent": "Right now there are some cases where you try to find features in some sense, but they're usually done in an unsupervised learning setting.",
                    "label": 0
                },
                {
                    "sent": "They're not done for the purpose of discrimination, or in a distributed setting where there's really an experimental design problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a little bit of novelty here and this literature on their hands more rigorous in that there really isn't.",
                    "label": 0
                },
                {
                    "sent": "There's a full decision theoretic story there.",
                    "label": 0
                },
                {
                    "sent": "Consistent rules, there's rates.",
                    "label": 0
                },
                {
                    "sent": "There's kind of a satisfying statistical theory, and the main tool that's been used to develop that theory is this things these objects, called surrogate loss functions, where instead of trying to optimize the 01 loss, you optimize some relaxation of that and prove that it has desirable statistical.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what are these?",
                    "label": 0
                },
                {
                    "sent": "Let's talk about part one.",
                    "label": 0
                },
                {
                    "sent": "Then what are these alley Sylvie distances or F divergences?",
                    "label": 0
                },
                {
                    "sent": "Also chicharra studies.",
                    "label": 0
                },
                {
                    "sent": "Lot of people studies alright, so they're just.",
                    "label": 0
                },
                {
                    "sent": "They're just a measure of diversions between probability distributions, or in general measures.",
                    "label": 0
                },
                {
                    "sent": "So let's look at measures mu and pie and take a convex function F of the ratio I, the likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "So that would be a radon nikodym derivative in general.",
                    "label": 0
                },
                {
                    "sent": "So convex function F of the ratio.",
                    "label": 0
                },
                {
                    "sent": "And then we'll average that under under \u03a0. OK, so now that would be an integral, but let it be discrete for this for this talk.",
                    "label": 0
                },
                {
                    "sent": "So there you go.",
                    "label": 0
                },
                {
                    "sent": "So it looks kind of like a divergent, and in fact the killer versus a special case where F is chosen to be ulogd, ulogd.",
                    "label": 0
                },
                {
                    "sent": "You if you just plug that in and then you get the killer vergence but you get lots of other familiar objects, and in particular you choose after be absolute value function, plug that in, you get the variational distance if you choose they have to be this square root, Y kind of thing.",
                    "label": 0
                },
                {
                    "sent": "You plug that in, you get hell, injure distance and chair.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Off distance and a bunch of other things.",
                    "label": 0
                },
                {
                    "sent": "Let's see if I have no idea.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have any others alright, but it's a broad class of objects and again the use of these was mainly to get measures of discrimination among probability distributions and use them here, Stickley and of course some of these had some in statistics of other interpretations, in particular terms of hypothesis testing.",
                    "label": 0
                },
                {
                    "sent": "You got error exponents being expressed in terms of these things.",
                    "label": 0
                },
                {
                    "sent": "That's where Caleb originally came out, and so on.",
                    "label": 0
                },
                {
                    "sent": "But there really wasn't a full kind of usage.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These from mystical view alright, but there was a tantalizing result due to David Blackwell in 1951.",
                    "label": 0
                },
                {
                    "sent": "Very famous paper.",
                    "label": 0
                },
                {
                    "sent": "Which proved the following.",
                    "label": 0
                },
                {
                    "sent": "It's kind of going to sound a little bit weak, but we're going to hopefully by the end of strength in this.",
                    "label": 0
                },
                {
                    "sent": "So what Blackwell showed, he actually defined the after version.",
                    "label": 0
                },
                {
                    "sent": "see I think is one of the first people actually write them down in generality, and so he said, if a procedure A has a smaller F divergences, that procedure B for some particular F one of those apps, or some other F some convex F. Alright, then there exists some set of prior probabilities such that procedure A has smaller probability of error than procedure be.",
                    "label": 1
                },
                {
                    "sent": "OK, so the prior probabilities in this case are just the class probabilities.",
                    "label": 0
                },
                {
                    "sent": "There's quite you're in class one or class minus one with some prior alright.",
                    "label": 0
                },
                {
                    "sent": "And if you choose a particular F divergent particular function, say the log, then there are some set of prior probabilities.",
                    "label": 0
                },
                {
                    "sent": "You don't know what they are, but where you're doing the right thing you have the best procedure.",
                    "label": 0
                },
                {
                    "sent": "OK, if you knew those priors, you would know which you have to use because you don't know those priors, so this result was kind of an existence result.",
                    "label": 0
                },
                {
                    "sent": "It wasn't really useful directly.",
                    "label": 0
                },
                {
                    "sent": "Right nonetheless it did.",
                    "label": 0
                },
                {
                    "sent": "It was an important paper.",
                    "label": 0
                },
                {
                    "sent": "Did motivate people to start to study after Vergence is.",
                    "label": 0
                },
                {
                    "sent": "They said, Oh well, let's choose some FI know it works at least in some problems.",
                    "label": 0
                },
                {
                    "sent": "Let's just use it anyway.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe I'm going to look at it will be the right priors.",
                    "label": 0
                },
                {
                    "sent": "If it's not, maybe it still works, so it became a kind of heuristic empirical literature.",
                    "label": 0
                },
                {
                    "sent": "And so people started studying afterwards they said they turned out that some of them, you know, Helander distance became a popular one, also called Bhattacharyya distance.",
                    "label": 0
                },
                {
                    "sent": "Because it had 01 loss was intractable, this one turned out to be tractable.",
                    "label": 0
                },
                {
                    "sent": "You could get an algorithm that you can minimize.",
                    "label": 0
                },
                {
                    "sent": "Maximize this guy and similar with sharing off distance.",
                    "label": 0
                },
                {
                    "sent": "So there you go.",
                    "label": 0
                },
                {
                    "sent": "And of course then there were some heuristic arguments from asymptotic supporting these things that these divergences arise as their exponents.",
                    "label": 0
                },
                {
                    "sent": "But that was it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so now Part 2 of the talk are the story.",
                    "label": 0
                },
                {
                    "sent": "Is the machine learning perspective.",
                    "label": 1
                },
                {
                    "sent": "I don't need to.",
                    "label": 0
                },
                {
                    "sent": "We go through this, but just for notation here.",
                    "label": 0
                },
                {
                    "sent": "There's no Q, it's just find the discriminant function gamma and so its decision theoretic there's a loss function fee.",
                    "label": 0
                },
                {
                    "sent": "It's a function of discriminate gamma and of the label Y, and particularly my mission 01 loss.",
                    "label": 0
                },
                {
                    "sent": "Sophie would be that function, and in the binary case you can write that function in this kind of margin, like form, which will be using in the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "OK, so the indicator of disagreement on the label is is it gives you a one.",
                    "label": 0
                },
                {
                    "sent": "That's the loss you pay for disagreeing.",
                    "label": 0
                },
                {
                    "sent": "OK, so the focus is on estimating gamma alright, so again, for the same reasons in the other literature, it's intractable to minimize 01 loss.",
                    "label": 1
                },
                {
                    "sent": "So instead what people have done is said.",
                    "label": 0
                },
                {
                    "sent": "Let's look at a surrogate loss function, which is a convex upper bound on the 01 loss.",
                    "label": 1
                },
                {
                    "sent": "ANPR",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "System properties about that and so.",
                    "label": 0
                },
                {
                    "sent": "Here are some of the surrogate loss functions that have been studied in machine learning, literature and statistics literature.",
                    "label": 1
                },
                {
                    "sent": "So here's the 01 loss.",
                    "label": 0
                },
                {
                    "sent": "It's obviously nonconvex, so intense it turns out to be intractable.",
                    "label": 1
                },
                {
                    "sent": "And here's all these convex upper bound, so the hinge loss is the blue one that leads to the support vector machine.",
                    "label": 0
                },
                {
                    "sent": "The Green one is the exponential, which leads to boosting and logistic function.",
                    "label": 0
                },
                {
                    "sent": "Log logistic is the red one, and so on.",
                    "label": 0
                },
                {
                    "sent": "There's lots of others that have been studied there, all convex, and they're all upper bounds on 01 loss.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you all know what to do.",
                    "label": 0
                },
                {
                    "sent": "If you have a surrogate loss which you can do is then plug that in and form an empirical expectation of the surrogate loss informed.",
                    "label": 1
                },
                {
                    "sent": "This would be an empirical risk and you would optimize that with respect to gamma.",
                    "label": 0
                },
                {
                    "sent": "And you may or may not regularize that, and that would then lead to the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The theory that you've learned about it, so one paper that I wrote with calling Peter Bartlett and John McAuliffe gave necessary and sufficient conditions for surrogate loss functions to be to lead to consistent estimation, i.e.",
                    "label": 1
                },
                {
                    "sent": "If you give me more and more data, I will actually converge the Bayes optimal classifier.",
                    "label": 0
                },
                {
                    "sent": "So there's been a lot of sufficient kids around.",
                    "label": 0
                },
                {
                    "sent": "We actually were able to find a completely general necessary condition as well, and it's this little condition here which I'm not going to spend time on, but it's really as a statistician what's known as Fisher consistency.",
                    "label": 0
                },
                {
                    "sent": "I see Grace here and she's she's studying this as well and people have extended this to multi class classifiers.",
                    "label": 0
                },
                {
                    "sent": "It's sort of a pretty natural condition.",
                    "label": 0
                },
                {
                    "sent": "It sort of says that on the left side where you disagree with the right label, you pay more loss than on the right hand side where you agree with the right label.",
                    "label": 1
                },
                {
                    "sent": "So we call that classification calibrated and it's just a condition on the loss function fee.",
                    "label": 0
                },
                {
                    "sent": "It turns out then in this paper in Jazza that this leads to necessary and sufficient conditions for base consistency, and turns out that all those losses that I showed on the previous page are classification calibrated, so they all fit into this framework with lots of other losses due to an.",
                    "label": 0
                },
                {
                    "sent": "Here's probably want to keep in mind that if these if you are actually working with convex functions fee, then its classification calibrate if and only if its differentiable at zero and the derivative is negative at zero you tilt up a little bit at the order that's all you need.",
                    "label": 0
                },
                {
                    "sent": "For for Bayes consistency.",
                    "label": 0
                },
                {
                    "sent": "So if you have Fisher consistent then you get based consistency.",
                    "label": 1
                },
                {
                    "sent": "So that's kind of a fairly satisfying theory of surrogate loss functions for binary classification.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, and also it turns out that the meat of this talk is that there is turns out that if let's define a surrogate loss function has to be one that is calcification calibrated.",
                    "label": 0
                },
                {
                    "sent": "That's not going to definition of rest talk.",
                    "label": 0
                },
                {
                    "sent": "Turns out that there is a link between those objects of mathematical objects.",
                    "label": 1
                },
                {
                    "sent": "An after vergence is it's it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a many to one correspondence and it's constructive.",
                    "label": 1
                },
                {
                    "sent": "And then it's going to allow us to make relationships among F divergences and then translate them into relationships among loss functions.",
                    "label": 1
                },
                {
                    "sent": "So we gotta talk about something called universal equivalence, where you get the same answer for all possible probably distributions under.",
                    "label": 0
                },
                {
                    "sent": "So loss functions.",
                    "label": 0
                },
                {
                    "sent": "There's no real reason to choose between one or the other, except for maybe computational reasons.",
                    "label": 0
                },
                {
                    "sent": "So and then I'll show you how to do this.",
                    "label": 0
                },
                {
                    "sent": "To go back to the problem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talking about about jointly estimating Q and gamma.",
                    "label": 0
                },
                {
                    "sent": "OK, so just a little bit of set up here.",
                    "label": 0
                },
                {
                    "sent": "The problem then is to optimize the fee risk.",
                    "label": 0
                },
                {
                    "sent": "Now that's not the risk.",
                    "label": 0
                },
                {
                    "sent": "The risk was the 01 loss.",
                    "label": 0
                },
                {
                    "sent": "Now this is expectation of the fee function, so it's expiration of a surrogate loss.",
                    "label": 0
                },
                {
                    "sent": "Let's call that the fee risk, and we want to do joint estimation of Q, an gamma.",
                    "label": 1
                },
                {
                    "sent": "That's the problem.",
                    "label": 0
                },
                {
                    "sent": "It's a bivariate problem, alright?",
                    "label": 0
                },
                {
                    "sent": "So let's define unnormalized measures.",
                    "label": 0
                },
                {
                    "sent": "Gamma and mu.",
                    "label": 0
                },
                {
                    "sent": "Just as these joint probabilities probably have one class and the covariance Z probably other classic over Etsy and you can just rewrite those using class conditional densities where P little P and look you are the prior probabilities of the two classes.",
                    "label": 0
                },
                {
                    "sent": "Then the queue is the quantizer and then we're integrating against the class conditional densities in the two cases.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if you just use those two definitions and plug them into that expectation up there, you can rewrite.",
                    "label": 0
                },
                {
                    "sent": "That thing is just the sum.",
                    "label": 0
                },
                {
                    "sent": "Again, we're assuming discrete for simplicity.",
                    "label": 0
                },
                {
                    "sent": "Watching this is in the space, we are definitely we are quantizing.",
                    "label": 0
                },
                {
                    "sent": "So you just rewrite this as fee times gamma plus fee at minus gamma multiplied by \u03c0.",
                    "label": 0
                },
                {
                    "sent": "So this is pretty standard little reduction.",
                    "label": 0
                },
                {
                    "sent": "For those of you working classification.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we're going to profile.",
                    "label": 0
                },
                {
                    "sent": "So what is a profiling?",
                    "label": 0
                },
                {
                    "sent": "Profiling in statistics means you have a function of two arguments maximized over one of the arguments.",
                    "label": 0
                },
                {
                    "sent": "Right and get a function of just the remaining argument.",
                    "label": 0
                },
                {
                    "sent": "So in statistics there's going to flavors of statistics.",
                    "label": 0
                },
                {
                    "sent": "There's frequentist and Bayesian, and you often have functions of multiple arguments, multiple parameters, and the Bayesians want to integrate over some of the arguments and reduce that to a function of this one of one of the arguments, so you can always integrate and frequency is used sometimes in great, but more often than not the you maximize, and that's called profiling, so we get a profile likelihood, for example by by maximizing over one of the arguments where is amazing.",
                    "label": 0
                },
                {
                    "sent": "It's a marginal likelihood, so we're going to profile.",
                    "label": 0
                },
                {
                    "sent": "We're going to frequencies today.",
                    "label": 0
                },
                {
                    "sent": "In profile this fee risk, so we're going to particular profile over the gamma function for each for each Z.",
                    "label": 1
                },
                {
                    "sent": "So you pin down Z.",
                    "label": 0
                },
                {
                    "sent": "Then it's just a number.",
                    "label": 0
                },
                {
                    "sent": "And so we optimize it called that are sub fee of Q.",
                    "label": 0
                },
                {
                    "sent": "So it's the infime am overall choices of discriminant function.",
                    "label": 0
                },
                {
                    "sent": "Of the fear risk.",
                    "label": 1
                },
                {
                    "sent": "OK, for example, in the 01 loss, which is a little short calculation that you can do 01 loss can be written.",
                    "label": 0
                },
                {
                    "sent": "In that way you can see easily see that.",
                    "label": 0
                },
                {
                    "sent": "You want to, you know you minimize the one that would.",
                    "label": 1
                },
                {
                    "sent": "You know these are the two class conditional losses and you can re write that in something looking like a variational distance.",
                    "label": 0
                },
                {
                    "sent": "In fact it is just a variational distance so it can be written that way, so the profiled loss.",
                    "label": 0
                },
                {
                    "sent": "That's what this thing is here actually has the looks like a NAFTA versions.",
                    "label": 0
                },
                {
                    "sent": "It's after versions up to a constant.",
                    "label": 0
                },
                {
                    "sent": "It's negative and after vergence OK, so that's kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "If you profile a few risk you get in after vergence and so we did this at some point and the 1st next question is is that more that more general?",
                    "label": 0
                },
                {
                    "sent": "Is that that happen more often when you profile surrogate losses?",
                    "label": 0
                },
                {
                    "sent": "Do you get after Vergence?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It turns out you do so.",
                    "label": 0
                },
                {
                    "sent": "These are a bunch of fun little calculations you can do if you take the hinge loss and you profile it out, you get variational distance, not just the 01 loss but also the hinge loss gives you variational distance when you profile.",
                    "label": 1
                },
                {
                    "sent": "If you profile the boosting loss then you turns out you get the exponential boosting loss, then turns out you get.",
                    "label": 0
                },
                {
                    "sent": "That should be.",
                    "label": 0
                },
                {
                    "sent": "It's a typo, it should be Hellinger distance, not variational distance.",
                    "label": 1
                },
                {
                    "sent": "That fun little calculation to do an if you profile the logistic loss, you get this object where these are.",
                    "label": 0
                },
                {
                    "sent": "Cal divergences this is kind of a symmetrized KL divergent which is called the capacity discrimination.",
                    "label": 0
                },
                {
                    "sent": "OK, so this seems to be happening more generally.",
                    "label": 0
                },
                {
                    "sent": "We take circuit losses we get after.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Agences and it turns out that there's a theory that we uncovered, which is that this always happens that all surrogate loss functions actually induce by profiling and have to vergence and Moreover corresponding to every after vergence there is a class of surrogate loss functions.",
                    "label": 1
                },
                {
                    "sent": "It's many to one that lead to that after version.",
                    "label": 0
                },
                {
                    "sent": "So we already saw that with the hinge loss and the 01 loss.",
                    "label": 0
                },
                {
                    "sent": "So I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk a little bit more about how this link is established, and so the theory theory behind this really reposes on conjugate duality, which is my favorite topic in an analysis.",
                    "label": 0
                },
                {
                    "sent": "If you pick up Rockefeller's book it sometime, you will be rewarded and you will learn about conjugate duality.",
                    "label": 0
                },
                {
                    "sent": "Very important topic also knows Legendra transforms and lots of other versions of this.",
                    "label": 0
                },
                {
                    "sent": "And just so I'm not going to get into a lot of the convex analysis, but I just want to say this is what drives the theory here, so just define what country duality is.",
                    "label": 0
                },
                {
                    "sent": "So if you have a lower semi continuous convex function F. Then the convex duel of the conjugate dual of that is defined as solution to a variational problem.",
                    "label": 1
                },
                {
                    "sent": "So you take a linear function of of you, minus the original function, and then you extreme eyes that an you get a function which we would call the conjugal F star of you, and these are dual in the sense that if you now take F star and find its conjugal, you're back to half again.",
                    "label": 0
                },
                {
                    "sent": "If you start with a convex function you go to App Store, you go back again to F. But in any case F star is necessarily a convex function even if F was or not.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's conjugate duality widely used.",
                    "label": 0
                },
                {
                    "sent": "I've worked in variational methods for graphical models and especially for long time, and this is the key tool there as well.",
                    "label": 0
                },
                {
                    "sent": "So we're going to find this function would take the conjugate.",
                    "label": 0
                },
                {
                    "sent": "Do we actually put a negative sign there and that will be an important object for us fee, but it's basically just the conjugate tool.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, here's the theorem we've proved, so let me sort of go a little bit slow on this so.",
                    "label": 0
                },
                {
                    "sent": "For every margin based surrogate loss function Fi, there isn't after version such that the profiled losses the negative of an F. Diversions for some lower semi continuous convex function F. So that's One Direction of the theorem.",
                    "label": 1
                },
                {
                    "sent": "OK, Moreover, if F is actually sorry, fee is continuous and then there's a regularity condition, there's a bunch of properties you can prove about this, expressed in terms of this conjugate dual function.",
                    "label": 0
                },
                {
                    "sent": "There's a fixed point property, it's decreasing and convex, so it has for example properties.",
                    "label": 0
                },
                {
                    "sent": "And then there's another fixed point property as well.",
                    "label": 0
                },
                {
                    "sent": "So this is not.",
                    "label": 0
                },
                {
                    "sent": "This is really a fixed point property here, and this is kind of an item potency kind of property, right?",
                    "label": 0
                },
                {
                    "sent": "But the main then the other content of the theorem is the other direction, which is.",
                    "label": 1
                },
                {
                    "sent": "Conversely, if F is lower semi continuous convex an it satisfies these conditions then there exists and go backwards.",
                    "label": 0
                },
                {
                    "sent": "There existed the Crest decreasing convex surrogate loss that induces the corresponding after merchants.",
                    "label": 1
                },
                {
                    "sent": "So it's kind of a full mathematical tie between surrogate loss functions and after versus both directions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now the easy direction is from losses to.",
                    "label": 0
                },
                {
                    "sent": "Afterwards my picture will be kind of losses in the left efforts in this direction is easy.",
                    "label": 0
                },
                {
                    "sent": "Distraction is hard, so this direction.",
                    "label": 0
                },
                {
                    "sent": "Here's the proof.",
                    "label": 0
                },
                {
                    "sent": "Actually it's really simple, so let's recall that the after version.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the feed loss can be written in that form at the top of the page is just a function of fee and then the joint class conditional densities.",
                    "label": 0
                },
                {
                    "sent": "And if you do the optimization, the profiling just take the infimum over the argument of fee.",
                    "label": 0
                },
                {
                    "sent": "We're fixing Z.",
                    "label": 0
                },
                {
                    "sent": "Now when we're inside of that sum, so you just pulled in FEMA inside an.",
                    "label": 0
                },
                {
                    "sent": "Now you just do a little transformation.",
                    "label": 0
                },
                {
                    "sent": "You pull Pi outside and then divide by \u03c0, and so I get fee plus 50 times the likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "And now if you just let U equal that likelihood ratio and define F to be the thing inside of the northern FEMA with a negative sign, then we just define have to be this way.",
                    "label": 0
                },
                {
                    "sent": "And now that function becomes exactly an after vergence.",
                    "label": 0
                },
                {
                    "sent": "It's just a sum over.",
                    "label": 0
                },
                {
                    "sent": "ZA paisy of a convex function F of a likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of.",
                    "label": 0
                },
                {
                    "sent": "That's the steps you go through in each of these individual examples.",
                    "label": 0
                },
                {
                    "sent": "And that's just the general story.",
                    "label": 0
                },
                {
                    "sent": "OK, and it's not hard to see that F is a convex function, it's just a.",
                    "label": 1
                },
                {
                    "sent": "It's it's.",
                    "label": 0
                },
                {
                    "sent": "It's a linear function.",
                    "label": 0
                },
                {
                    "sent": "A set of linear functions and you're taking the negative of that, so you get a convex function.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's the easy direction.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the hard direction.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to get into, but I'm going to note that it has a constructive consequence which is trying trying to go backwards when you work all about, it.",
                    "label": 0
                },
                {
                    "sent": "Turns out you can actually characterize all you can go backwards constructively.",
                    "label": 0
                },
                {
                    "sent": "It's not just an existence theorem, so here's the construction.",
                    "label": 0
                },
                {
                    "sent": "This is probably last intensive kind of math slide, so any continuous loss function feed that induces afterwards must be of the following form.",
                    "label": 1
                },
                {
                    "sent": "So this is all possible loss functions that are coming from and after vergence they have to be this form.",
                    "label": 0
                },
                {
                    "sent": "They are the fief, the PSI function, that conjugate dual function.",
                    "label": 0
                },
                {
                    "sent": "I defined two slides ago of some arbitrary function.",
                    "label": 0
                },
                {
                    "sent": "G will not quite arbitrary has some additions, but it's this degree of freedom in the problem is just some function G for Alpha greater than zero.",
                    "label": 0
                },
                {
                    "sent": "In office negative, it's just the function G all by itself.",
                    "label": 1
                },
                {
                    "sent": "Now G is just happens to his needs to be increasing continuous and convex and it has to have this fixed point property.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's lots of functions that satisfy that.",
                    "label": 0
                },
                {
                    "sent": "You can plug any of the men to that conjugate dual function.",
                    "label": 0
                },
                {
                    "sent": "So if you start with an F over here find its conjugate dual and plug in any arbitrary function G that satisfy those properties and you will then induce a thief, you will now write down a fee function, which is a convex surrogate loss.",
                    "label": 0
                },
                {
                    "sent": "So if you hadn't learned about convex sort of losses because Vapnik told you about hinge loss and Rob Shapiro told you about exponential loss and so on.",
                    "label": 0
                },
                {
                    "sent": "This would have been another recipe to generate a whole family of convex circle losses, all of which lead to base consistency of estimators.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's not what happened historically, but it could have happened, right?",
                    "label": 0
                },
                {
                    "sent": "So let's look at some examples of this.",
                    "label": 0
                },
                {
                    "sent": "So, hell, injure distance.",
                    "label": 0
                },
                {
                    "sent": "So that's not a loss function that's Afd vergence, so I'm over in the Vengeance world.",
                    "label": 0
                },
                {
                    "sent": "I write down the hedger distance and I plug in a bunch of choices of the G function.",
                    "label": 0
                },
                {
                    "sent": "So here's the ones that we chose there that exponential that you and that you squared.",
                    "label": 0
                },
                {
                    "sent": "And those then are plugged into that that the previous slide.",
                    "label": 0
                },
                {
                    "sent": "We get the fee out.",
                    "label": 0
                },
                {
                    "sent": "And here it is.",
                    "label": 0
                },
                {
                    "sent": "Those are bunch of loss functions, all of which are rising from the same after vergence.",
                    "label": 0
                },
                {
                    "sent": "OK, so in particular, if you had chosen so you get hallager distance.",
                    "label": 0
                },
                {
                    "sent": "That should have been G over here when geoview is equal to that, you get Helander distance between G is some of those other objects, you get, other other other things.",
                    "label": 0
                },
                {
                    "sent": "OK, and here is actually the calculation for Hellinger distance of that conjugal.",
                    "label": 0
                },
                {
                    "sent": "You can just do that calculation if you choose G equal to the this particular function, then you get the exponential loss.",
                    "label": 0
                },
                {
                    "sent": "Phone.",
                    "label": 0
                },
                {
                    "sent": "Actually, what am I saying or the top?",
                    "label": 0
                },
                {
                    "sent": "But that was not a G over here that this is just the after versions were starting with.",
                    "label": 0
                },
                {
                    "sent": "And we're going backward in recovering expensive loss by choosing this particular G function.",
                    "label": 0
                },
                {
                    "sent": "I should have used you in that argument up there.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a longer distance.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what about variational distance?",
                    "label": 1
                },
                {
                    "sent": "Well, variational distances as we learned earlier, can be written as an after vergence, and this is the F function.",
                    "label": 0
                },
                {
                    "sent": "Now you can do the conjugal calculation and turns out that size just the order that it's a little simple calculation.",
                    "label": 1
                },
                {
                    "sent": "And now if you choose G to be just you, then you get out the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can kind of sort of see how this is.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recipe to generate things.",
                    "label": 0
                },
                {
                    "sent": "What about KL divergent?",
                    "label": 0
                },
                {
                    "sent": "Right, well there is no fee loss for the asymmetric functions.",
                    "label": 1
                },
                {
                    "sent": "KL divergences are asymmetric, but if you symmetrize, then that is leads to an after vergence an there.",
                    "label": 0
                },
                {
                    "sent": "In that case you can realize it.",
                    "label": 0
                },
                {
                    "sent": "With with this particular choice of function OK, so enough on that those are examples.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that was kind of the set up.",
                    "label": 0
                },
                {
                    "sent": "Now we have a link between after Vergence is an FFI functions and now we can start to go off and try to prove Bayes consistency for choices jointly of Q, an Lambda or gamma before it's now been called Lambda.",
                    "label": 1
                },
                {
                    "sent": "For some reason.",
                    "label": 0
                },
                {
                    "sent": "OK, so how would we do this?",
                    "label": 0
                },
                {
                    "sent": "So first of all, let's start with a 01 loss.",
                    "label": 0
                },
                {
                    "sent": "Given the 01 loss, we can obtain the variational distance as.",
                    "label": 0
                },
                {
                    "sent": "The corresponding after vergence where we use that function right now what I'm going to argue is that the way to solve this problem is to not consider just that specific F divergent which we got from this loss function.",
                    "label": 0
                },
                {
                    "sent": "But to consider slightly broader class of after vergence over in that right hand space, we're going to not just take a point where it takes another after versus around that, and the way we do that is, we take the men of you, one.",
                    "label": 0
                },
                {
                    "sent": "That's the one that we started with.",
                    "label": 0
                },
                {
                    "sent": "Put a - in front of that, and then take this little affine combination of those after version.",
                    "label": 0
                },
                {
                    "sent": "So now get a little little manifold of F divergences a little broader class, and that way, alright.",
                    "label": 0
                },
                {
                    "sent": "And now let's range overall points, possible choices of these constants ABC and we will get our family.",
                    "label": 0
                },
                {
                    "sent": "Of continuous convex and classification calibrated fee losses, all of which could be obtained from those after purchase and all of which have these nice properties.",
                    "label": 0
                },
                {
                    "sent": "So we're getting.",
                    "label": 0
                },
                {
                    "sent": "So we're starting on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "So here's our F divergences.",
                    "label": 0
                },
                {
                    "sent": "Here's our corresponding feed losses.",
                    "label": 0
                },
                {
                    "sent": "We have this one to one mapping.",
                    "label": 0
                },
                {
                    "sent": "We're starting with the hinge loss over here or size 01 loss, and we're mapping that over to a particular F. Then we're broadening that to a broader class of F divergences without a laugh line combination.",
                    "label": 0
                },
                {
                    "sent": "Then we're going backwards, and we're getting this little set here that all map into this guy, of which 01 loss is 1 particular.",
                    "label": 0
                },
                {
                    "sent": "So we get a set of feed losses here.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, so we're going to provide conditions under which all of these feed losses healed based consistency, and it turns out that that's a necessary condition as well, only these feed losses obtained in this way achieve based consistency jointly for Q and an and discriminate function, right?",
                    "label": 0
                },
                {
                    "sent": "So there's going to be some losses that would work for classification, but won't work for the joint problem, 'cause this is going to be necessary and sufficient condition.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so this part of talks kind of boring.",
                    "label": 0
                },
                {
                    "sent": "It's just sort of standard consistency arguments and so let me not belabor it, but just sort of kind of just here it is, you know, we're going to kind of cevs, I guess.",
                    "label": 0
                },
                {
                    "sent": "Do is with one of the originators of these ideas.",
                    "label": 0
                },
                {
                    "sent": "We're talking about nonparametric classes.",
                    "label": 0
                },
                {
                    "sent": "We look at classes that get larger and larger, and we control in some sense the covering numbers.",
                    "label": 0
                },
                {
                    "sent": "Now it's total control.",
                    "label": 0
                },
                {
                    "sent": "This disco fluctuations, so we have a sequence of function classes in which our discriminant functions are going to live here.",
                    "label": 0
                },
                {
                    "sent": "Get bigger and bigger and bigger and.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that there exist an Oracle that is able to solve this particular risk minimization problem.",
                    "label": 1
                },
                {
                    "sent": "So we take our empirical risk there.",
                    "label": 0
                },
                {
                    "sent": "It's written out where we've now summed over the data points, but we're also averaging over our quantizer, 'cause we have this stochastic quantizer that Maps the axes in disease and that overall function is what we're trying to optimize.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose that we can do this over these compact subsets.",
                    "label": 0
                },
                {
                    "sent": "There's some computational procedure that does that.",
                    "label": 0
                },
                {
                    "sent": "Alright, man, let's call this one such solution to that optimization problem.",
                    "label": 1
                },
                {
                    "sent": "OK, and now let's denote the best you could possibly do is the minimum Bayes risk where you optimize jointly over the quantizer.",
                    "label": 1
                },
                {
                    "sent": "In over this covenant, where are Sobeys means 01 loss, so this is the risk function where you're using 01 loss.",
                    "label": 0
                },
                {
                    "sent": "That's what we're trying to do in this problem is minimized 01 loss, so let's call that the Bayes solution.",
                    "label": 0
                },
                {
                    "sent": "Best you can do so.",
                    "label": 0
                },
                {
                    "sent": "The SXS Bayes risk would be the risk that you get from this optimization procedure minus the best you can do.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then you the standard thing to do at this point is to divide up the problem into approximation error an estimation error, so the approximation error is you're looking at the actual fear risk up there.",
                    "label": 1
                },
                {
                    "sent": "There's no hat on top of that thing, and subtracting off the best you can do so that's an approximation error.",
                    "label": 0
                },
                {
                    "sent": "There's no statistics up there, and then down here is the estimation error where we're taking an expectation of this.",
                    "label": 0
                },
                {
                    "sent": "Estimation problem where we getting we're working with the estimated the empirical risk subtracting off the actual risk, so our focus will be not the approximation story.",
                    "label": 0
                },
                {
                    "sent": "A lot of people have worked on that, but on the estimation.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so we're able to prove a theorem that kind of pulls together various pieces that various people have worked on in this particular context, and so we're able to show that under the usual kind of conditions that drive approximation error to zero, an estimation error to 0 and under the additional condition on fee that it's one of these losses that I've I've expressed in terms of divergences.",
                    "label": 1
                },
                {
                    "sent": "Alright, that excess risk can be written in terms of an estimation error.",
                    "label": 0
                },
                {
                    "sent": "And then another term which is defined here that.",
                    "label": 0
                },
                {
                    "sent": "Is for kind of all the you know, it's easy to to evaluate on particular problem for all the standard problems this thing can be shown to be finite, so you just need to show that this particular kind of maximized expression is finite and it is for for the usual source of set up.",
                    "label": 0
                },
                {
                    "sent": "OK, so under the usual kind of conditions plus this condition on the fee risk, we get actual base consistency now for jointly choosing Q and gamma, right?",
                    "label": 0
                },
                {
                    "sent": "And this is the important part of it, which is that you have to do it according to this recipe.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's kind of a nice theorem and kind of extends classification into this external design setting, but turned out there was a deeper idea here, and it really was when we started finally started reading David Blackwell's papers in the 50s that we work this out.",
                    "label": 0
                },
                {
                    "sent": "So this guy, we're going to call this notion universal equivalence of loss functions.",
                    "label": 1
                },
                {
                    "sent": "So everything in this set over here in the loss function cited, these guys are going to be called University equivalent on Earth under this new this definition.",
                    "label": 1
                },
                {
                    "sent": "I'm not ready to to mention, alright, so let's consider two loss functions Phi one and Phi 2.",
                    "label": 0
                },
                {
                    "sent": "And let's suppose they correspond to F divergences induced by F1 and F2.",
                    "label": 1
                },
                {
                    "sent": "Alright, we're going to call them universally equivalent and denote it that way.",
                    "label": 0
                },
                {
                    "sent": "Alright, if for any probability distribution so completely nonparametrically an any quantization rules, QA and QB, so you're looking at one condition rule QA, you're looking at QB.",
                    "label": 0
                },
                {
                    "sent": "We're trying to evaluate which is best and we try to use nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Any possible choices?",
                    "label": 0
                },
                {
                    "sent": "Probably distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, if it turns out that fee one ranks these two procedures in this way, that QA is better than QB if and only if that also happens for fee two.",
                    "label": 0
                },
                {
                    "sent": "That if he wanted feature set to be universal equivalent, their ranking procedures in the same way.",
                    "label": 0
                },
                {
                    "sent": "Under all possible distributions, so their their equipment.",
                    "label": 0
                },
                {
                    "sent": "So that's a definition.",
                    "label": 0
                },
                {
                    "sent": "Natural definition of equivalence.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, and here's the last theorem.",
                    "label": 0
                },
                {
                    "sent": "Then in the talk, which is that you have equivalence among loss functions if and only if the corresponding after virgins are related in that half fine way.",
                    "label": 1
                },
                {
                    "sent": "OK, so that says that if you are interested in equivalence of loss functions, you're trying to say I have a whole bag of loss functions over here, and I'm trying to kind of characterize that space, you know.",
                    "label": 1
                },
                {
                    "sent": "Is this loss function really that different from this one from this one?",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Well, I might kind of like to find equivalence classes here and say everything in this class is actually the same.",
                    "label": 0
                },
                {
                    "sent": "Give me the same answer.",
                    "label": 0
                },
                {
                    "sent": "And this class is well away to do that, by the way.",
                    "label": 0
                },
                {
                    "sent": "To do that is to go over this other space of F divergences, find the corresponding afterwards, and if they are related by this app line function, then your University equivalent back in the space.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was kind of surprising and would you know?",
                    "label": 0
                },
                {
                    "sent": "I think it's nontrivial, right?",
                    "label": 0
                },
                {
                    "sent": "This other class of mathematical objects over here is kind of the right way to set up the notion of equivalence back in loss function space for decision theory.",
                    "label": 0
                },
                {
                    "sent": "OK, and again the proof is in this paper.",
                    "label": 1
                },
                {
                    "sent": "The next, the necessary condition in this case is easy and the sufficiency is not easy.",
                    "label": 0
                },
                {
                    "sent": "And in particular, now we can go back to zero and loss, which is what our original interest in this class of problems was, and so we can start with 01 loss here.",
                    "label": 0
                },
                {
                    "sent": "And we can ask what's all classes of functions which are universally equivalent to 01 loss while we go over the course money after versions?",
                    "label": 0
                },
                {
                    "sent": "That's the variational distance.",
                    "label": 0
                },
                {
                    "sent": "Then we go back.",
                    "label": 0
                },
                {
                    "sent": "Then we do the laugh line expansion.",
                    "label": 0
                },
                {
                    "sent": "Over here we go back over.",
                    "label": 0
                },
                {
                    "sent": "Here we get a bunch of other losses which are universally equivalent to 01 loss, in particular, hinge loss is in this set universal equivalent to 01 loss.",
                    "label": 1
                },
                {
                    "sent": "Under this theory.",
                    "label": 0
                },
                {
                    "sent": "OK, so all those V losses if you use them in your learning problem will give you the same answer as as the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "You might prefer one or the other and the 01 loss.",
                    "label": 0
                },
                {
                    "sent": "For computational reasons you know etc etc.",
                    "label": 0
                },
                {
                    "sent": "There's lots of other reasons could be there, right?",
                    "label": 0
                },
                {
                    "sent": "But there are some other functions over here which are outside of this set.",
                    "label": 0
                },
                {
                    "sent": "In particularly the boosting loss, the exponential losses outside this set.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not universal equivalent to 01 loss.",
                    "label": 0
                },
                {
                    "sent": "Well, that's a problem, right?",
                    "label": 0
                },
                {
                    "sent": "That means there exists some probably division P where you're not getting the right in the same answer 01 loss.",
                    "label": 0
                },
                {
                    "sent": "Of course, the problem is the same as you were trying to do, right?",
                    "label": 0
                },
                {
                    "sent": "So that loss function if you try to use it to optimize both the quantizer in this current function under some probabilities will give you the wrong answer.",
                    "label": 0
                },
                {
                    "sent": "It's not consistent.",
                    "label": 0
                },
                {
                    "sent": "OK, now that's not true.",
                    "label": 0
                },
                {
                    "sent": "If you're just talking about learning discriminant function.",
                    "label": 0
                },
                {
                    "sent": "Boosting has some consistency proofs.",
                    "label": 0
                },
                {
                    "sent": "Alright, but it is true in this broader class of problems.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I'm gonna see if yeah.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just pause for a second there and see her questions.",
                    "label": 0
                },
                {
                    "sent": "Yes, there was a question.",
                    "label": 0
                },
                {
                    "sent": "Awesome station.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "Statues sequentially for small numbers is that also in the category of quantization for jumping, but or is it King Roger Azatian?",
                    "label": 0
                },
                {
                    "sent": "It's in the category mathematically, whether it would be the right kind of approach to that problem, whether it would be useful now whether it be useful, isn't it would be need to be looked at, and we definitely are applications of this have all been to these kind of distributed kind of problems, not just classification of others, and I would like that kind of problem that needs some more thought.",
                    "label": 0
                },
                {
                    "sent": "Kind of, you know, really what you're asking is kind of is like for problems like choosing basis functions should be viewed as an experiment design problem.",
                    "label": 0
                },
                {
                    "sent": "And certainly mathematically can be.",
                    "label": 0
                },
                {
                    "sent": "And maybe it's really actually interesting had been looked at by me and I don't think by anybody else.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "I know this kind of looks like a tutorial flavor here, so people are getting lost or whatever it want to.",
                    "label": 0
                },
                {
                    "sent": "Ask a dumb question or just make me pause for a minute.",
                    "label": 0
                },
                {
                    "sent": "I'd be happy to do that, yes?",
                    "label": 0
                },
                {
                    "sent": "Wondering is.",
                    "label": 0
                },
                {
                    "sent": "Margin, hotel view or some other cost functions?",
                    "label": 0
                },
                {
                    "sent": "And yes, can we see something which is equal something different on that because of the steep endpoint agent that you sent here?",
                    "label": 0
                },
                {
                    "sent": "Well, I may not be interesting question entirely right where you're asking.",
                    "label": 0
                },
                {
                    "sent": "What's the margin part of you?",
                    "label": 0
                },
                {
                    "sent": "All of the classifiers were looking at her margin based classifiers.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're all aware that during that families, that's what surrogate loss functions kind of means, and So what we're saying is that some lost some margin based loss functions are perfectly fine for normal, old-fashioned classification, would just get the discriminant function, but are not fine when you actually had also the experiment design problem where we're separating those out, yeah?",
                    "label": 0
                },
                {
                    "sent": "Class.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's the approximation estimation part of the story is you just have to be in a rich enough class that you drive those errors to 0, but there's nothing.",
                    "label": 0
                },
                {
                    "sent": "We're not saying anything new about in there.",
                    "label": 0
                },
                {
                    "sent": "Framework.",
                    "label": 0
                },
                {
                    "sent": "What happens to this framework?",
                    "label": 0
                },
                {
                    "sent": "Well, that's good.",
                    "label": 0
                },
                {
                    "sent": "I mean we would not have a consistency store that we drive something to zero.",
                    "label": 0
                },
                {
                    "sent": "We might have a great story that we drive something at some rate down to something jointly.",
                    "label": 0
                },
                {
                    "sent": "Ann, and you know that would be kind of a mathematical exercise that would involve some approximation theory, some estimation theory and all this convexity theory all done together, 'cause you wouldn't be able to separate them out.",
                    "label": 0
                },
                {
                    "sent": "We kind of separated that part of the problem by assuming we're rich enough that the non parametric stuff that everybody else is done is taken care of and we're now adding this ingredient of the experiment design.",
                    "label": 0
                },
                {
                    "sent": "But you're right as part of the general picture here.",
                    "label": 0
                },
                {
                    "sent": "Over time we would want a very general theory that had all those ingredients kind of competing against each other and we're just going to put in that other piece of the picture.",
                    "label": 0
                },
                {
                    "sent": "And in one regime.",
                    "label": 0
                },
                {
                    "sent": "But you're right, we should consider the other regimes as well.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "What does it mean exactly?",
                    "label": 0
                },
                {
                    "sent": "Universal definition had nothing to do with these after version, since that's the theorem that related them.",
                    "label": 0
                },
                {
                    "sent": "The definition is really pretty simple.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's it's this one and this is black.",
                    "label": 0
                },
                {
                    "sent": "I should mention, this is Blackwell's idea actually, so Blackwell actually did this 401 loss.",
                    "label": 0
                },
                {
                    "sent": "This was his definition and he showed that under this definition that put that theorem I showed about after versus under 1401 loss, and what we've really done.",
                    "label": 0
                },
                {
                    "sent": "It just take Blackwell's idea and extend it to all the surrogate loss functions.",
                    "label": 0
                },
                {
                    "sent": "And turned out, that's that's what's happening here.",
                    "label": 0
                },
                {
                    "sent": "So here this this idea, and this is the one that actually gets used.",
                    "label": 0
                },
                {
                    "sent": "A lot of economics and preference rankings and kind of the competitions using zeolite.",
                    "label": 0
                },
                {
                    "sent": "It just says that if you have a risk function based on some feed loss, that's a hinge loss and it ranks of procedure QA as being better than a procedure QB under under under risk.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you're using his lesson.",
                    "label": 0
                },
                {
                    "sent": "He over there is using exponential loss.",
                    "label": 0
                },
                {
                    "sent": "Right, and he gets exactly the same ranking for those two procedures, but not only those two, but for all pairs of procedures and then under all possible choices, prohibition P. If that happens, then those two losses are really the same and we call them universally equivalent.",
                    "label": 0
                },
                {
                    "sent": "So the universally really kind refers to this nonparametric story.",
                    "label": 0
                },
                {
                    "sent": "This should be true in all possible probably distributions 'cause you don't know.",
                    "label": 0
                },
                {
                    "sent": "The problem is not very OK, so it turns out that Ken's loss and 01 loss are equivalent.",
                    "label": 0
                },
                {
                    "sent": "This way they rank procedures exactly the same way under all possible distributions, so they can give you the same answers under all choices distribution, whereas hinge and our side 01 an exponential laws don't.",
                    "label": 0
                },
                {
                    "sent": "OK, so is that clear that definition?",
                    "label": 0
                },
                {
                    "sent": "It really just kind of ranking that these two risk functions based on those losses rank things the same way no matter what day.",
                    "label": 0
                },
                {
                    "sent": "Do you have?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Continuous feature extractors instead of quantizers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this actually is results have nothing to do with the discrete part.",
                    "label": 0
                },
                {
                    "sent": "This is just kind of presentation.",
                    "label": 0
                },
                {
                    "sent": "To make it, there's a little bit of measure theory that you kind of start to worry about, but there's really nothing.",
                    "label": 0
                },
                {
                    "sent": "It just convexity.",
                    "label": 0
                },
                {
                    "sent": "Convex analysis is driving all this nothing without discrete continuous.",
                    "label": 0
                },
                {
                    "sent": "Calico straight or constraint set notice.",
                    "label": 0
                },
                {
                    "sent": "In some places, without getting to basically anything.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's the same question individually asking is kind of we don't take rich function classes and make our estimation and approximation is not necessarily go to zero.",
                    "label": 0
                },
                {
                    "sent": "You know, do we have rates?",
                    "label": 0
                },
                {
                    "sent": "We start talking about how that trades off against our story, and again I didn't say answer.",
                    "label": 0
                },
                {
                    "sent": "OK, good thanks for those questions.",
                    "label": 0
                },
                {
                    "sent": "So I'm definitely not going to be late 'cause how kind he was to me.",
                    "label": 0
                },
                {
                    "sent": "So let me kind of just kind of mention a little bit at the end of the talk here.",
                    "label": 0
                },
                {
                    "sent": "Kind of dessert so.",
                    "label": 0
                },
                {
                    "sent": "Let me see where was I.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, OK, so this is kind of that I'm done with that part of the talk.",
                    "label": 0
                },
                {
                    "sent": "That's one paper.",
                    "label": 0
                },
                {
                    "sent": "There's another paper that you can find on our websites.",
                    "label": 0
                },
                {
                    "sent": "I don't think I mentioned my colleagues.",
                    "label": 0
                },
                {
                    "sent": "This is all done with you online again.",
                    "label": 0
                },
                {
                    "sent": "Who was a student with me and Martin Wainwright at Berkeley?",
                    "label": 0
                },
                {
                    "sent": "Who's now would be a faculty member at University of Michigan.",
                    "label": 0
                },
                {
                    "sent": "Ann with Martin Wainwright, who's my colleague at Berkeley?",
                    "label": 0
                },
                {
                    "sent": "So in another paper we have now we're kind of interesting divergences.",
                    "label": 0
                },
                {
                    "sent": "You know, we're always interested.",
                    "label": 0
                },
                {
                    "sent": "Loss functions.",
                    "label": 0
                },
                {
                    "sent": "Now we got interesting divergences.",
                    "label": 0
                },
                {
                    "sent": "And So what about divergences?",
                    "label": 0
                },
                {
                    "sent": "For example?",
                    "label": 0
                },
                {
                    "sent": "Can you estimate divergences?",
                    "label": 0
                },
                {
                    "sent": "Give me some data, and you know two distributions.",
                    "label": 0
                },
                {
                    "sent": "If I had the populations, there would be a well defined number which is the divergent between those populations.",
                    "label": 0
                },
                {
                    "sent": "If you give me a finite sample from those distributions, can you estimate the vergence?",
                    "label": 0
                },
                {
                    "sent": "Something guesstimate the entropy that Sanada vergence?",
                    "label": 0
                },
                {
                    "sent": "Can you estimate the KL divergent's?",
                    "label": 0
                },
                {
                    "sent": "So interesting problem.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you want to optimize, if you want to maximize the mutual information which lots of people want to do for various problems, you need to estimate the mutual information and that turns out to be.",
                    "label": 0
                },
                {
                    "sent": "That's hard.",
                    "label": 0
                },
                {
                    "sent": "OK, that's not a solved problem.",
                    "label": 0
                },
                {
                    "sent": "Lots of papers in information theory talk about estimating the entropy, often in discrete settings.",
                    "label": 0
                },
                {
                    "sent": "But really you want to do some continuous settings.",
                    "label": 0
                },
                {
                    "sent": "So let's in particular look at the problem estimating the KL divergent's.",
                    "label": 0
                },
                {
                    "sent": "So we have these two distributions Q&Q.",
                    "label": 0
                },
                {
                    "sent": "With densities so we can define this this scale version.",
                    "label": 0
                },
                {
                    "sent": "So here it is.",
                    "label": 0
                },
                {
                    "sent": "Now let's talk about continuous things and we'd like to estimate that giving samples from P&Q and there is a lot of work on.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best Buy a lot of well known people, so good papers by Bequelin Rd Rita Resoft.",
                    "label": 0
                },
                {
                    "sent": "Donahoe has an old paper on this bear.",
                    "label": 0
                },
                {
                    "sent": "Jamos are long and various others.",
                    "label": 0
                },
                {
                    "sent": "Ann, what's hard about this problem?",
                    "label": 0
                },
                {
                    "sent": "It's that.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functional two densities, but the real hard part is that you have a density P an you have log.",
                    "label": 0
                },
                {
                    "sent": "P&P is not unknown.",
                    "label": 0
                },
                {
                    "sent": "You're getting data alone.",
                    "label": 0
                },
                {
                    "sent": "You're getting data from P, so you could do a density estimate of P But you also have to do a density of log P. Those are on different.",
                    "label": 0
                },
                {
                    "sent": "Those are different problems and you can't find good bandwidth for both of them simultaneously.",
                    "label": 0
                },
                {
                    "sent": "That's kind of the core of the problem.",
                    "label": 0
                },
                {
                    "sent": "You have P appearing twice, not just once, but once it does density estimation, but it's twice.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so it turns out the theory I presented actually speaks directly to this so.",
                    "label": 0
                },
                {
                    "sent": "In that little variational formula, when we talk about conjugate duality, there was a way to lower bound these divergences.",
                    "label": 0
                },
                {
                    "sent": "You can write them actually equality.",
                    "label": 0
                },
                {
                    "sent": "It's in general inequality, but you get equality at a certain conditions.",
                    "label": 0
                },
                {
                    "sent": "I guess here we've got the condition right here.",
                    "label": 0
                },
                {
                    "sent": "There's some subgradient is intersect intersects the function class.",
                    "label": 1
                },
                {
                    "sent": "But anyway this is the KL divergent can be written with equality as just a variational problem.",
                    "label": 0
                },
                {
                    "sent": "It's the supremum or all functions of F average average under Q minus the conjugate dual function.",
                    "label": 1
                },
                {
                    "sent": "Phi DP OK, and so even if you haven't seen the first part of the talk, you could just plug.",
                    "label": 0
                },
                {
                    "sent": "You could just do this variational problem on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Optimize over F and you will recover the KL divergent's so that you can check that this is correct.",
                    "label": 0
                },
                {
                    "sent": "Alright, but anyway it comes out of the theory.",
                    "label": 0
                },
                {
                    "sent": "Alright, and so that's pretty cool, because now this is a variational problem and you can now.",
                    "label": 0
                },
                {
                    "sent": "Conceptualised, estimate those terms and then optimize overall so you can now turn this into an optimization problem, i.e.",
                    "label": 0
                },
                {
                    "sent": "Do in estimation, empirical risk minimization, whatever.",
                    "label": 0
                },
                {
                    "sent": "So you can now optimize over a function class F. Hopefully chosen rich enough that the optimum of that function actually gives you a good estimate of that.",
                    "label": 0
                },
                {
                    "sent": "So that's a new approach to.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Divergent estimation right?",
                    "label": 0
                },
                {
                    "sent": "So particularly the vergence that variational problem just reduces to this where Phi is chosen to be that log of mu minus one.",
                    "label": 0
                },
                {
                    "sent": "So you plug that in and this would be an even easier to exercise to verify.",
                    "label": 0
                },
                {
                    "sent": "You can just optimize on the right hand side, you will get the elder vergence.",
                    "label": 0
                },
                {
                    "sent": "And the optimas attained.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so now we have a new procedure which is that, let's say get some data from P&Q and define the empirical measures P&QN and then plug in the empirical measures into that formula, take the supremum over your function class and you get a number and let's call it the estimate of KL divergent.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so we've done that.",
                    "label": 0
                },
                {
                    "sent": "We've worked out some theory for that.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you do that as an empirical risk measure, doesn't work very well, and you need to regularize and we worked out with the right regularizer is it's this expression here, and I'm going to cut this part of the talk short, but just let you know there is a regularization precede.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For this, an without regularization procedure, we're able to get convergence rates for this.",
                    "label": 1
                },
                {
                    "sent": "So if you chose the regularization thing in a certain way, then under Helander distance you can get a certain rate of convergence of this thing.",
                    "label": 0
                },
                {
                    "sent": "And also the regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a certain rate, so we can kind of do this and but what about empirically?",
                    "label": 0
                },
                {
                    "sent": "Well, this turns out to be a successful and maybe even state of the art procedure.",
                    "label": 0
                },
                {
                    "sent": "So kind of one of the ones that's mostly widely used is this thing called WKV, which I forget who the people there free people via their do, and I forget who the first 2 or if someone can remind me.",
                    "label": 0
                },
                {
                    "sent": "This is some information through the kind of history.",
                    "label": 0
                },
                {
                    "sent": "It's an adaptive histogram kind of procedure that they have, and so that's these curves.",
                    "label": 0
                },
                {
                    "sent": "The these are estimation of the killer versus between two particular densities.",
                    "label": 0
                },
                {
                    "sent": "This is number of data points and the right answer is that straight line there, and so the convergence rates of that state of the art procedure are given by those two curves in our new procedure is converging much much faster than in particular is conversion extremely fast, so these are just particular choices of our.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variance of our procedure.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to skip to the end.",
                    "label": 0
                },
                {
                    "sent": "We just done a bunch of under those simulations and.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a theoretical result.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm done.",
                    "label": 0
                },
                {
                    "sent": "Just to reiterate what I said now several times, the main meat of this theory was that we've formulated a link between F divergences, which you probably heard about with other names around them.",
                    "label": 0
                },
                {
                    "sent": "They are very widely used, very interesting, and this object that we've been studying and learning theory for awhile now called serverless functions.",
                    "label": 0
                },
                {
                    "sent": "It gives a decision theoretic perspective on average when I give a talk this talk to eat people.",
                    "label": 0
                },
                {
                    "sent": "It's a very different talk and it's more you've been talking about for a lot of years.",
                    "label": 0
                },
                {
                    "sent": "Actually, decision theoretic formulation of them.",
                    "label": 0
                },
                {
                    "sent": "They're not just kind of heuristics that people come up with, but then back in the other direction they allow you to design equivalent classes of loss functions.",
                    "label": 1
                },
                {
                    "sent": "It's worth going there, and there's lots of possible applications.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a first paper on this topic, so if you're interested in joining, there's tons of applications not just to decentralize problems with other kinds of experiment design problems, so thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Did I get all the questions out of the previous break?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "No, I didn't turn the multiclass case at all, and that would be a wonderful problem to tackle.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In fact, I mean it would be fun.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just there's gonna be some convex analysis of the multiclass case, so they won't be after vergence anymore.",
                    "label": 0
                },
                {
                    "sent": "He'll take multiclass losses, of which there are some for some theory.",
                    "label": 0
                },
                {
                    "sent": "Now on again, Grace would be a person asked if you don't already know about this, but there's some theory on surrogate losses for multiclass that are also consistent.",
                    "label": 0
                },
                {
                    "sent": "Some of the natural ones are not consistent.",
                    "label": 0
                },
                {
                    "sent": "It's a harder problem, but there are now known some that are consistent and then you could ask if I profile them out, I'm going to class of mathematical objects will have some convexity properties.",
                    "label": 0
                },
                {
                    "sent": "They won't have a name on them like Elvis Sylvia's.",
                    "label": 0
                },
                {
                    "sent": "Then they may have been studied by some mathematician, but or you might build your own name on them, I don't know, but there will be some interesting class of functions.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Losses that map the same activities.",
                    "label": 0
                },
                {
                    "sent": "Are there any interesting also found in the same beverages with zero one month after?",
                    "label": 1
                },
                {
                    "sent": "That's why I was saying the hinge loss Maps to the same after versions of 01 loss.",
                    "label": 0
                },
                {
                    "sent": "Exact same one.",
                    "label": 0
                },
                {
                    "sent": "It's just a different G function when you're going backwards direction.",
                    "label": 0
                },
                {
                    "sent": "This degree of freedom that may be uninteresting or agency for competition reasons but not statistically interesting.",
                    "label": 0
                },
                {
                    "sent": "Or for rates maybe.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Functions like after this I mean you said cool, awesome but equal into the active agents have followed that rule that respected afterwards.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if they map to the same after versions and their equivalent kind of trivial consequence of our theorem, but it's broader class of every guys, half an expansion.",
                    "label": 0
                },
                {
                    "sent": "All of those also equivalent.",
                    "label": 0
                },
                {
                    "sent": "Say something like.",
                    "label": 0
                },
                {
                    "sent": "So also if you order within a linear factor of each other or something, and so every every word, it's kinda says the odds between two Power Distribution.",
                    "label": 0
                },
                {
                    "sent": "So we also linear factor of each other.",
                    "label": 0
                },
                {
                    "sent": "That's kind of interesting thought.",
                    "label": 0
                },
                {
                    "sent": "Try to think about that, not all after versions are kind of odds.",
                    "label": 0
                },
                {
                    "sent": "I mean they're all functions of likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "Their convex functions like the ratio.",
                    "label": 0
                },
                {
                    "sent": "Better, I haven't actually gone for intuitions here, it's just that this is work that's been kind of drive convex analysis forward, and but definitely worth trying to think about that.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's definitely there.",
                    "label": 0
                },
                {
                    "sent": "Functions likelihood ratio that's critical here that all this kind of a name in person is lurking under this.",
                    "label": 0
                },
                {
                    "sent": "All this stuff too.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Functional.",
                    "label": 0
                },
                {
                    "sent": "Gold is pronounced in character hey Elvis.",
                    "label": 0
                },
                {
                    "sent": "So you're not interested in the Mail.",
                    "label": 0
                },
                {
                    "sent": "This is almost that part that depends on when to implement, yes.",
                    "label": 0
                },
                {
                    "sent": "Better than that, so that's a great question.",
                    "label": 0
                },
                {
                    "sent": "And I'm not going to think on my feet fast enough to say that this was all done, really.",
                    "label": 0
                },
                {
                    "sent": "It's a general theory and then all the examples have been done centered around 01 loss.",
                    "label": 0
                },
                {
                    "sent": "But you know, there would be a recipe.",
                    "label": 0
                },
                {
                    "sent": "I would know the first kind of steps to take to look at that and do you have a particular paper mine which has another, the kind of the best current way to do that?",
                    "label": 0
                },
                {
                    "sent": "Accuracy over 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "OK, that might be a good another example for us to to look at.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, we have a short list of other problems.",
                    "label": 0
                },
                {
                    "sent": "Look at with profiling.",
                    "label": 0
                },
                {
                    "sent": "There's lots of kind of profile methods and stats that are that this could shed some light on, but that was a good example.",
                    "label": 0
                },
                {
                    "sent": "Where the loss functions got is governed by it's a sub part of the problem and it's yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Any cost function?",
                    "label": 1
                },
                {
                    "sent": "If two loss functions are equivalent, mean two after version or problem if the after equivalent, all the corresponding loss functions are equivalent.",
                    "label": 0
                },
                {
                    "sent": "That's that's part of the theory, so you can talk about after versions being equivalent.",
                    "label": 0
                },
                {
                    "sent": "That just means they're related by this fine thing.",
                    "label": 0
                },
                {
                    "sent": "And what is surprising is that they go backwards and induced that all the course.",
                    "label": 0
                },
                {
                    "sent": "My loss function requipment in this way.",
                    "label": 0
                },
                {
                    "sent": "But the kind that was explained to the woman in the back, the definition of equivalence is really in terms of loss functions and their ranking properties and they kind of know that induce equivalency after verses that turns out to be this fine thing.",
                    "label": 0
                },
                {
                    "sent": "But you would never defined it that way.",
                    "label": 0
                },
                {
                    "sent": "Kind of some problem domain POV.",
                    "label": 0
                },
                {
                    "sent": "It's just a mathematical definition.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be the right one.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                }
            ]
        }
    }
}