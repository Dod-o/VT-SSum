{
    "id": "rxphtneeqxhyoyyimyi3scjxmubyj5my",
    "title": "Exploring Homology Using the Concept of Three-State Entropy Vector",
    "info": {
        "author": [
            "Armando J. Pinho, Institute of Electronics and Telematics Engineering of Aveiro"
        ],
        "published": "Oct. 14, 2010",
        "recorded": "September 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/prib2010_pinho_ehuc/",
    "segmentation": [
        [
            "1st.",
            "Say something about.",
            "Or we are and.",
            "Why have we arrived at the results that I'm presenting here?",
            "So we are mostly a bunch of electronics and telecommunications engineers.",
            "Apart from to persons Sarah, which is a biologist and Vera which is a mathematician so.",
            "We have been interested in.",
            "Data compression, basically.",
            "And data compression relies on good modeling of the data.",
            "So good models explain the data and if you can find good models you will get good compression algorithms.",
            "So we got interested in computational biology and we tried to apply those models that we are using for data compression in other domains also to the problem of modeling.",
            "For example DNA sequences.",
            "So we basically.",
            "Try to use models that we call finite context models which are based on Markov, the Markov property and those models we have applied those models for unrestricted DNA and also for protein coding regions."
        ],
        [
            "The outline of my presentation is easier.",
            "I will give a short introduction.",
            "Then I will talk a little bit about what are finite context models and then the experimental results.",
            "And finally we try to.",
            "Draw some."
        ],
        [
            "Conclusions.",
            "OK. One of the characteristics of DNA, as you know, is that there are pretty cities in the data.",
            "One of the most strong of those produce.",
            "It eases the period 3 predicted that we can find in the exons of the organisms.",
            "So in fact, as you know, this produced has been used for identifying potential protein coding regions, and one of the consequences of this periodicity.",
            "Is that the entropy that we can measure various along the column?",
            "This means that if you compute the entropy at the first position of the cotton at a second, another third, the entropy is different, and in fact is different for a given speeches, and it varies also from species to species."
        ],
        [
            "So.",
            "At this observation.",
            "Provided provided as a motivation for studying for further what we could do.",
            "I think that entropy of each of the basis of the column.",
            "So in this work we, by the way, we noticed that variation when we try to develop models for compressing those protein coding parts of the DNA.",
            "We try to develop one of of the models was based on a tree state finite context Model 3 state white restate 'cause we have three bases.",
            "Base positions at the columns and therefore we tried to model to find a model for each of the three positions.",
            "These are still preliminary results, but what we have done was for now to construct vectors, entropy, vectors and they are low dimensional vectors.",
            "In fact they are vectors with 18 components only, and we found that they seem to be useful for the clustering species, so that's the.",
            "Say that the main topic."
        ],
        [
            "Of this work.",
            "What is a finite context model?",
            "For that context, model is a computational model, is a probabilistic model that assigns probabilities estimates to the symbols of an alphabet.",
            "And according to a conditional conditioning context, and that context is computed over the past K outcomes and that's why it relies on the Markov property.",
            "So is a schematic example.",
            "Say you have sequence, you are processing the sequence in that direction, so you have some past of the sequence of recent past and you try to.",
            "Find the probability of having the next symbol conditioned to the two, in this case to the five previous symbols, so the model generates.",
            "Conditional probabilities, and normally this.",
            "This was the part.",
            "That we need it after the model for encoding the data.",
            "This is the part where we provide the probabilities and they the encoder generates the bits.",
            "Normally it is and rithmetic encoder, but for this work we just stop.",
            "At this point we are only interested in the probability generated by the the model."
        ],
        [
            "When we render the model.",
            "It constructs probability models.",
            "EE based on the information he gets from the sequence is reading and therefore we can compute the average number of bits that we need for encoding the sequence.",
            "These probabilities vary.",
            "We time.",
            "And therefore for a given position an in the sequence, we can compute an average.",
            "Of the code length, which is given by this logarithm of the probability according to the two information theory if I have.",
            "An information source that generates outcomes with certain probability.",
            "The optimal number of bits I need to.",
            "To record that that outcome is minus the log of the probability and therefore here I have an average of those values and I can call this the entropy and empirical entropy of the the.",
            "Permission model.",
            "We have also to compute the estimates of the probabilities and that is then basically by counting so.",
            "We use this this probability estimator which counts the number of times a certain symbol, in this case DNA base occurs in that context and divides by the total number of symbols that occur in that context.",
            "Affected by effector here and Alpha.",
            "And here Alpha times the size of the alphabet, which has to do with the amount of probability that we assign two and seen events until that point.",
            "Because if we don't have this term here, we would run into zero, probably probabilities for the first time we are seeing the symbol and zero probably 0.",
            "Probability is not a good way to give to.",
            "I know arithmetic in code, so this based on this estimates we can compute those entropies and that's the main ideas around finite context models."
        ],
        [
            "Just to give you an idea how this is implemented, it is quite simple, it's just based on tables counting tables.",
            "You have all the possible combinations of the context for each.",
            "When context occurs, you go to the right place in the table and you just add 1 to the.",
            "To the counter and when you have to compute the probability, just put these counters in the form that I've shown to you and you have the estimate so."
        ],
        [
            "Here, for example, if you run into a context 88 J and the next symbol is a C, then you go to this counter and you increase one after using the the counters for computing the probability.",
            "So this runs in causal way Y 'cause this was intended to be used for data compression and therefore.",
            "You have to assure that what happens to in the encoder can be exactly reproduces at the decoder."
        ],
        [
            "For handling damn.",
            "3 bytes, 3 baseplate.",
            "Produce city of the protein coding regions.",
            "We use a tree state model.",
            "Basically we have one of those tables for each of the three States and we just switch switch state sequentially.",
            "And that's the difference.",
            "We provide probabilities.",
            "In jumps off tree for having the the tree."
        ],
        [
            "The information for the three states.",
            "So what we have done, we have picked some organisms.",
            "This case 5 animals for plants and for bacteria."
        ],
        [
            "And we computed those tree entropies for several context depths of those organisms and ups.",
            "And there are the plots of those entropies.",
            "The number of bits per base.",
            "Recall that the maximum number of bits is 2 if the sequence is random.",
            "And this is the context depth.",
            "So here you are looking just for the previous base here for the two previous bases and so on.",
            "You see that the entropy.",
            "It comes down when you increase the depth, but for some depth it will start tries not in this case, but in other cases you can see it here.",
            "You see the entropy for the 1st place, which is that one.",
            "For the second base.",
            "Choose that one.",
            "And finally, for the third base, so those graphics provide us a kind of signature for this PC.",
            "And the idea was to do that computation of the entropy for those species and then to construct vectors.",
            "At.",
            "With all these points, so we have 3 * 6.",
            "Real valid real values and you constructed the factors and based on those vectors we produce it the.",
            "Adenta gram, so clustering for trying to see if the speech is get clustered calling what to what we expect.",
            "For example, just to see."
        ],
        [
            "Here for example, you see that the first base as lower entropy than the second one remembers that entropy is related with the amount of information that is there.",
            "So this means that, for example, in this case, the amount of information in the first base is less than the amount of information in the cell."
        ],
        [
            "Conveys but for example, for points that's not the case.",
            "The amount of information for the first one is more than for the 2nd and final."
        ],
        [
            "For the third one is for the bacteria, and finally 'cause I'm run."
        ],
        [
            "Out of time.",
            "This is the under gram, which is not.",
            "So great, but at least see a shows that we could group the plants, the animals and more or less the bacteria.",
            "There are obviously miss placements, but."
        ],
        [
            "In for concluding.",
            "What we have seen here is that we can build those three state entropy vectors quite easily.",
            "There are vectors of low dimension, only 18 components.",
            "And we tried to cluster this species.",
            "These preliminary results seems to show that we can build more or less many meaningful dendrograms.",
            "Obviously we have to do further study, for example, in adding more specious and also having better control over the over the quality of the data.",
            "The problem is that.",
            "Dis 3 state model is very sensitive to changes.",
            "For example, in the on the reading frame.",
            "So we have to be careful with the quality of the data we are providing to the model and we have not do that yet.",
            "We have to do it.",
            "Also we have a problem when comparing species with very large data sizes cause the models the context.",
            "Depends on the number of times you see it for having reliable values for for the statistics.",
            "Therefore we have to do some normalization.",
            "We have another normalization problem here to solve, which we are also trying to to address.",
            "That's it.",
            "Thank you for your attention.",
            "But I looked at the.",
            "Channels that are really very close.",
            "How would that be?",
            "If you use more distance?",
            "That really show up in your house.",
            "OK, I don't know.",
            "We have to try it.",
            "As I said, we we just have done some exploratory work.",
            "Basic basically to see if those vectors mean something in terms of separating species, but we haven't tried yet with larger groups and more diverse group species inside.",
            "Each of the groups.",
            "Today go for Gmail.",
            "Think that haven't had any influence on TV charger mention.",
            "So Yep.",
            "Rich we yeah.",
            "We just pick up the files with the information from the databases and use it.",
            "So most of those genes.",
            "I guess they are predicted and there might be lots of noise there.",
            "I believe with more less noisy data we probably can do better clustering.",
            "I I.",
            "Really.",
            "Spell Hewitt said over, yeah, yeah you.",
            "We plan to do that because we believe due to the sensitivity of the model.",
            "For example to Frameshifts and we have to be careful with the quality of the data and we plan to do that.",
            "Russians.",
            "I think the argument OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "Say something about.",
                    "label": 0
                },
                {
                    "sent": "Or we are and.",
                    "label": 0
                },
                {
                    "sent": "Why have we arrived at the results that I'm presenting here?",
                    "label": 0
                },
                {
                    "sent": "So we are mostly a bunch of electronics and telecommunications engineers.",
                    "label": 0
                },
                {
                    "sent": "Apart from to persons Sarah, which is a biologist and Vera which is a mathematician so.",
                    "label": 0
                },
                {
                    "sent": "We have been interested in.",
                    "label": 0
                },
                {
                    "sent": "Data compression, basically.",
                    "label": 0
                },
                {
                    "sent": "And data compression relies on good modeling of the data.",
                    "label": 0
                },
                {
                    "sent": "So good models explain the data and if you can find good models you will get good compression algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we got interested in computational biology and we tried to apply those models that we are using for data compression in other domains also to the problem of modeling.",
                    "label": 0
                },
                {
                    "sent": "For example DNA sequences.",
                    "label": 0
                },
                {
                    "sent": "So we basically.",
                    "label": 0
                },
                {
                    "sent": "Try to use models that we call finite context models which are based on Markov, the Markov property and those models we have applied those models for unrestricted DNA and also for protein coding regions.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The outline of my presentation is easier.",
                    "label": 0
                },
                {
                    "sent": "I will give a short introduction.",
                    "label": 0
                },
                {
                    "sent": "Then I will talk a little bit about what are finite context models and then the experimental results.",
                    "label": 1
                },
                {
                    "sent": "And finally we try to.",
                    "label": 0
                },
                {
                    "sent": "Draw some.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusions.",
                    "label": 0
                },
                {
                    "sent": "OK. One of the characteristics of DNA, as you know, is that there are pretty cities in the data.",
                    "label": 1
                },
                {
                    "sent": "One of the most strong of those produce.",
                    "label": 1
                },
                {
                    "sent": "It eases the period 3 predicted that we can find in the exons of the organisms.",
                    "label": 1
                },
                {
                    "sent": "So in fact, as you know, this produced has been used for identifying potential protein coding regions, and one of the consequences of this periodicity.",
                    "label": 1
                },
                {
                    "sent": "Is that the entropy that we can measure various along the column?",
                    "label": 0
                },
                {
                    "sent": "This means that if you compute the entropy at the first position of the cotton at a second, another third, the entropy is different, and in fact is different for a given speeches, and it varies also from species to species.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "At this observation.",
                    "label": 0
                },
                {
                    "sent": "Provided provided as a motivation for studying for further what we could do.",
                    "label": 0
                },
                {
                    "sent": "I think that entropy of each of the basis of the column.",
                    "label": 1
                },
                {
                    "sent": "So in this work we, by the way, we noticed that variation when we try to develop models for compressing those protein coding parts of the DNA.",
                    "label": 1
                },
                {
                    "sent": "We try to develop one of of the models was based on a tree state finite context Model 3 state white restate 'cause we have three bases.",
                    "label": 1
                },
                {
                    "sent": "Base positions at the columns and therefore we tried to model to find a model for each of the three positions.",
                    "label": 0
                },
                {
                    "sent": "These are still preliminary results, but what we have done was for now to construct vectors, entropy, vectors and they are low dimensional vectors.",
                    "label": 0
                },
                {
                    "sent": "In fact they are vectors with 18 components only, and we found that they seem to be useful for the clustering species, so that's the.",
                    "label": 0
                },
                {
                    "sent": "Say that the main topic.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of this work.",
                    "label": 0
                },
                {
                    "sent": "What is a finite context model?",
                    "label": 0
                },
                {
                    "sent": "For that context, model is a computational model, is a probabilistic model that assigns probabilities estimates to the symbols of an alphabet.",
                    "label": 1
                },
                {
                    "sent": "And according to a conditional conditioning context, and that context is computed over the past K outcomes and that's why it relies on the Markov property.",
                    "label": 0
                },
                {
                    "sent": "So is a schematic example.",
                    "label": 0
                },
                {
                    "sent": "Say you have sequence, you are processing the sequence in that direction, so you have some past of the sequence of recent past and you try to.",
                    "label": 0
                },
                {
                    "sent": "Find the probability of having the next symbol conditioned to the two, in this case to the five previous symbols, so the model generates.",
                    "label": 0
                },
                {
                    "sent": "Conditional probabilities, and normally this.",
                    "label": 0
                },
                {
                    "sent": "This was the part.",
                    "label": 0
                },
                {
                    "sent": "That we need it after the model for encoding the data.",
                    "label": 0
                },
                {
                    "sent": "This is the part where we provide the probabilities and they the encoder generates the bits.",
                    "label": 0
                },
                {
                    "sent": "Normally it is and rithmetic encoder, but for this work we just stop.",
                    "label": 0
                },
                {
                    "sent": "At this point we are only interested in the probability generated by the the model.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When we render the model.",
                    "label": 0
                },
                {
                    "sent": "It constructs probability models.",
                    "label": 0
                },
                {
                    "sent": "EE based on the information he gets from the sequence is reading and therefore we can compute the average number of bits that we need for encoding the sequence.",
                    "label": 0
                },
                {
                    "sent": "These probabilities vary.",
                    "label": 0
                },
                {
                    "sent": "We time.",
                    "label": 0
                },
                {
                    "sent": "And therefore for a given position an in the sequence, we can compute an average.",
                    "label": 1
                },
                {
                    "sent": "Of the code length, which is given by this logarithm of the probability according to the two information theory if I have.",
                    "label": 1
                },
                {
                    "sent": "An information source that generates outcomes with certain probability.",
                    "label": 0
                },
                {
                    "sent": "The optimal number of bits I need to.",
                    "label": 0
                },
                {
                    "sent": "To record that that outcome is minus the log of the probability and therefore here I have an average of those values and I can call this the entropy and empirical entropy of the the.",
                    "label": 1
                },
                {
                    "sent": "Permission model.",
                    "label": 0
                },
                {
                    "sent": "We have also to compute the estimates of the probabilities and that is then basically by counting so.",
                    "label": 1
                },
                {
                    "sent": "We use this this probability estimator which counts the number of times a certain symbol, in this case DNA base occurs in that context and divides by the total number of symbols that occur in that context.",
                    "label": 0
                },
                {
                    "sent": "Affected by effector here and Alpha.",
                    "label": 0
                },
                {
                    "sent": "And here Alpha times the size of the alphabet, which has to do with the amount of probability that we assign two and seen events until that point.",
                    "label": 0
                },
                {
                    "sent": "Because if we don't have this term here, we would run into zero, probably probabilities for the first time we are seeing the symbol and zero probably 0.",
                    "label": 0
                },
                {
                    "sent": "Probability is not a good way to give to.",
                    "label": 0
                },
                {
                    "sent": "I know arithmetic in code, so this based on this estimates we can compute those entropies and that's the main ideas around finite context models.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to give you an idea how this is implemented, it is quite simple, it's just based on tables counting tables.",
                    "label": 0
                },
                {
                    "sent": "You have all the possible combinations of the context for each.",
                    "label": 0
                },
                {
                    "sent": "When context occurs, you go to the right place in the table and you just add 1 to the.",
                    "label": 0
                },
                {
                    "sent": "To the counter and when you have to compute the probability, just put these counters in the form that I've shown to you and you have the estimate so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here, for example, if you run into a context 88 J and the next symbol is a C, then you go to this counter and you increase one after using the the counters for computing the probability.",
                    "label": 1
                },
                {
                    "sent": "So this runs in causal way Y 'cause this was intended to be used for data compression and therefore.",
                    "label": 0
                },
                {
                    "sent": "You have to assure that what happens to in the encoder can be exactly reproduces at the decoder.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For handling damn.",
                    "label": 0
                },
                {
                    "sent": "3 bytes, 3 baseplate.",
                    "label": 0
                },
                {
                    "sent": "Produce city of the protein coding regions.",
                    "label": 0
                },
                {
                    "sent": "We use a tree state model.",
                    "label": 0
                },
                {
                    "sent": "Basically we have one of those tables for each of the three States and we just switch switch state sequentially.",
                    "label": 0
                },
                {
                    "sent": "And that's the difference.",
                    "label": 0
                },
                {
                    "sent": "We provide probabilities.",
                    "label": 0
                },
                {
                    "sent": "In jumps off tree for having the the tree.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The information for the three states.",
                    "label": 0
                },
                {
                    "sent": "So what we have done, we have picked some organisms.",
                    "label": 0
                },
                {
                    "sent": "This case 5 animals for plants and for bacteria.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we computed those tree entropies for several context depths of those organisms and ups.",
                    "label": 0
                },
                {
                    "sent": "And there are the plots of those entropies.",
                    "label": 0
                },
                {
                    "sent": "The number of bits per base.",
                    "label": 1
                },
                {
                    "sent": "Recall that the maximum number of bits is 2 if the sequence is random.",
                    "label": 1
                },
                {
                    "sent": "And this is the context depth.",
                    "label": 0
                },
                {
                    "sent": "So here you are looking just for the previous base here for the two previous bases and so on.",
                    "label": 0
                },
                {
                    "sent": "You see that the entropy.",
                    "label": 0
                },
                {
                    "sent": "It comes down when you increase the depth, but for some depth it will start tries not in this case, but in other cases you can see it here.",
                    "label": 0
                },
                {
                    "sent": "You see the entropy for the 1st place, which is that one.",
                    "label": 0
                },
                {
                    "sent": "For the second base.",
                    "label": 0
                },
                {
                    "sent": "Choose that one.",
                    "label": 0
                },
                {
                    "sent": "And finally, for the third base, so those graphics provide us a kind of signature for this PC.",
                    "label": 0
                },
                {
                    "sent": "And the idea was to do that computation of the entropy for those species and then to construct vectors.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "With all these points, so we have 3 * 6.",
                    "label": 0
                },
                {
                    "sent": "Real valid real values and you constructed the factors and based on those vectors we produce it the.",
                    "label": 0
                },
                {
                    "sent": "Adenta gram, so clustering for trying to see if the speech is get clustered calling what to what we expect.",
                    "label": 0
                },
                {
                    "sent": "For example, just to see.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here for example, you see that the first base as lower entropy than the second one remembers that entropy is related with the amount of information that is there.",
                    "label": 0
                },
                {
                    "sent": "So this means that, for example, in this case, the amount of information in the first base is less than the amount of information in the cell.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conveys but for example, for points that's not the case.",
                    "label": 0
                },
                {
                    "sent": "The amount of information for the first one is more than for the 2nd and final.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the third one is for the bacteria, and finally 'cause I'm run.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out of time.",
                    "label": 0
                },
                {
                    "sent": "This is the under gram, which is not.",
                    "label": 0
                },
                {
                    "sent": "So great, but at least see a shows that we could group the plants, the animals and more or less the bacteria.",
                    "label": 0
                },
                {
                    "sent": "There are obviously miss placements, but.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In for concluding.",
                    "label": 0
                },
                {
                    "sent": "What we have seen here is that we can build those three state entropy vectors quite easily.",
                    "label": 0
                },
                {
                    "sent": "There are vectors of low dimension, only 18 components.",
                    "label": 0
                },
                {
                    "sent": "And we tried to cluster this species.",
                    "label": 1
                },
                {
                    "sent": "These preliminary results seems to show that we can build more or less many meaningful dendrograms.",
                    "label": 0
                },
                {
                    "sent": "Obviously we have to do further study, for example, in adding more specious and also having better control over the over the quality of the data.",
                    "label": 1
                },
                {
                    "sent": "The problem is that.",
                    "label": 0
                },
                {
                    "sent": "Dis 3 state model is very sensitive to changes.",
                    "label": 0
                },
                {
                    "sent": "For example, in the on the reading frame.",
                    "label": 0
                },
                {
                    "sent": "So we have to be careful with the quality of the data we are providing to the model and we have not do that yet.",
                    "label": 0
                },
                {
                    "sent": "We have to do it.",
                    "label": 0
                },
                {
                    "sent": "Also we have a problem when comparing species with very large data sizes cause the models the context.",
                    "label": 0
                },
                {
                    "sent": "Depends on the number of times you see it for having reliable values for for the statistics.",
                    "label": 0
                },
                {
                    "sent": "Therefore we have to do some normalization.",
                    "label": 0
                },
                {
                    "sent": "We have another normalization problem here to solve, which we are also trying to to address.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "But I looked at the.",
                    "label": 0
                },
                {
                    "sent": "Channels that are really very close.",
                    "label": 0
                },
                {
                    "sent": "How would that be?",
                    "label": 0
                },
                {
                    "sent": "If you use more distance?",
                    "label": 0
                },
                {
                    "sent": "That really show up in your house.",
                    "label": 0
                },
                {
                    "sent": "OK, I don't know.",
                    "label": 0
                },
                {
                    "sent": "We have to try it.",
                    "label": 0
                },
                {
                    "sent": "As I said, we we just have done some exploratory work.",
                    "label": 0
                },
                {
                    "sent": "Basic basically to see if those vectors mean something in terms of separating species, but we haven't tried yet with larger groups and more diverse group species inside.",
                    "label": 0
                },
                {
                    "sent": "Each of the groups.",
                    "label": 0
                },
                {
                    "sent": "Today go for Gmail.",
                    "label": 0
                },
                {
                    "sent": "Think that haven't had any influence on TV charger mention.",
                    "label": 0
                },
                {
                    "sent": "So Yep.",
                    "label": 0
                },
                {
                    "sent": "Rich we yeah.",
                    "label": 0
                },
                {
                    "sent": "We just pick up the files with the information from the databases and use it.",
                    "label": 0
                },
                {
                    "sent": "So most of those genes.",
                    "label": 0
                },
                {
                    "sent": "I guess they are predicted and there might be lots of noise there.",
                    "label": 0
                },
                {
                    "sent": "I believe with more less noisy data we probably can do better clustering.",
                    "label": 0
                },
                {
                    "sent": "I I.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "Spell Hewitt said over, yeah, yeah you.",
                    "label": 0
                },
                {
                    "sent": "We plan to do that because we believe due to the sensitivity of the model.",
                    "label": 0
                },
                {
                    "sent": "For example to Frameshifts and we have to be careful with the quality of the data and we plan to do that.",
                    "label": 0
                },
                {
                    "sent": "Russians.",
                    "label": 0
                },
                {
                    "sent": "I think the argument OK.",
                    "label": 0
                }
            ]
        }
    }
}