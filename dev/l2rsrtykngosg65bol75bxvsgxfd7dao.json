{
    "id": "l2rsrtykngosg65bol75bxvsgxfd7dao",
    "title": "A Scalable Framework for Discovering Coherent Co-clusters in Noisy Data",
    "info": {
        "author": [
            "Meghana Deodhar, University of Texas at Austin"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml09_deodhar_sfd/",
    "segmentation": [
        [
            "I'm Magnum and my quarters are you Joe, Gunjan Gupta, Joydeep Ghosh, and rigid Dylan from UT Austin.",
            "In this talk I'm going to be focusing on a scalable framework that we have developed for finding the."
        ],
        [
            "It's coherent clusters and high dimensional noisy large datasets.",
            "OK, sure.",
            "OK, I'm going to begin by discussing the nature of clustering problems that typically arise in a lot of real life applications and which are going to be a focus for today's talk.",
            "The main part of the talk will be on proposed framework robust overlapping Co clustering and I'll go into details about the objective function and the algorithm for this."
        ],
        [
            "Finally, we look at some experimental results, mainly on microarray data.",
            "So in several applications, the datasets that arise involve very small cohesive clusters and the rest of the data or a large part of the data is typically irrelevant or not very informative.",
            "So an example of this is microarray data which you can see in the figure which is which can be represented as a matrix of genes and their expression values across experiments or conditions, and typically only a small subset of genes show coherent expression patterns.",
            "Across a small subset of the conditions and the rest of the data is not really very informative.",
            "From the biological point of view, a similar scenario arises.",
            "In E Commerce applications like Market Basket analysis and fraud detection, where again the datasets are very large and noisy, But the interesting or useful patterns occur only in small pockets of the data.",
            "So I ordered the clustering challenges that we're going to try and address.",
            "We're going to be looking at datasets which have a large number of irrelevant or non informative data points and features which actually obfuscate the true or coherent clusters existing in the data also."
        ],
        [
            "The clusters could exist in different subspaces of features, like for example in microarray data.",
            "The same subset of genes could be correlated across different subsets of experiments.",
            "Also the clusters could be overlapping like the same set of genes could participate in more than one biological processes.",
            "So we want to find overlapping clusters as well.",
            "So here's the problem, we want to find dense, arbitrarily positioned, possibly overlapping Co clusters.",
            "Existing in large noisy datasets by pruning away or discarding the relevant parts of the data and trying to focus only on the relevant subset.",
            "So there is a lot of relevant work in this area and it can be categorized into 3 broad classes of."
        ],
        [
            "Book, so firstly density based clustering algorithms have a motivation very similar to us.",
            "DV scan optics for example, and Bregman bubble clustering which was proposed recently and however these are restricted to typically 1 sided clustering."
        ],
        [
            "So in contrast, Co clustering algorithms simultaneously cluster along multiple dimensions and hence they can find clusters of data points and features.",
            "And finally, subspace clustering."
        ],
        [
            "This also find clusters existing in different subsets of the feature space.",
            "I would like to mention here that none of these previously proposed algorithms aimed to provide the full set of capabilities that we are aiming to provide.",
            "So we propose robust overlapping Co clustering."
        ],
        [
            "Which proves the relevant points and features to find dense clusters embedded in the data, and the interesting part about this is that it is it is robust to the data noise model.",
            "It can basically use any noise distribution from the regular exponential family."
        ],
        [
            "So the key idea of row CC is based on 2 steps.",
            "In the first step it adapts the Bregman Co clustering algorithm to the extreme left you see the result of Bregman Co clustering, which is the partitional clustering algorithm proposed by Banerjee at all.",
            "It is exhaustive, which means that it assigns every point in the data matrix to exactly 1 Co cluster and it gives you a partitioning of the data matrix into a grid of.",
            "Four clusters, so the first step of our OCC adapts this algorithm to cluster only a subset of the rows and columns.",
            "So we still obtain a set of Co clusters arranged in a grid, but they involve only the most relevant part of the data set and prune the rest.",
            "So that's step one.",
            "In step two, we actually prune away some of these clusters and merge the rest based on the similarity to recover the original existing overlapping clusters.",
            "So the key idea is to actually over partition the existing clusters and step one to obtain small grid based clusters and then merge them to try and recover the original ones."
        ],
        [
            "So what are the key features of fire approach that distinguish it from other pro clustering algorithms?",
            "Firstly, step one is based on a systematically developed cost function which will minimize iteratively to a local optimal.",
            "Secondly, it is applicable to a large, wide range of distance measures.",
            "That is all Bregman divergences, and depending on the data set you can choose the distance measure that's most suitable, like for instance for text data one can use I divergance or for micro.",
            "Data one can use radically in distance.",
            "A lot of times the data matrix can have missing values and this approach can handle these missing values as well.",
            "Also several times the previous Co clustering algorithms.",
            "For instance, changing churches by clustering algorithm.",
            "They identify Co clusters one at a time rather than all at once, whereas our algorithm can detect all these clusters simultaneously.",
            "And finally, it is computationally efficient than hence scalable to large high dimensional datasets."
        ],
        [
            "So let's take a closer look at the formal problem definition.",
            "We have our data matrix represented by Z consisting of M rows and columns.",
            "Let S, R&S, C represent the number of rows and columns to be clustered.",
            "Let K&L be the sets of clustered rows and columns which are obviously subsets of all the rows and columns, and we want to find keiro clusters and L column clusters.",
            "That is K * L clusters arranged in a grid, and let Rowan Gamma represent the mappings of the Rose to the row clusters in the columns to the column clusters Respec.",
            "Actively.",
            "Now we need to input the values of SRNSC and it's actually fine to just set these values conservatively to sufficiently high values, because the algorithm will do a second round of pruning.",
            "Also, since we want to over partition the clusters, we want to set candell relatively high."
        ],
        [
            "So here is the objective function that we want to minimize.",
            "We are using squared Euclidean distance as the distance measure, but one can use any Bregman divergences.",
            "So ZUV is the original value in the matrix.",
            "In row you column V&Z hat is the value that's approximated within the cluster that this entry UV belongs to.",
            "WV is a weight associated with every matrix entry and this is how we handle missing values.",
            "So WV can be set to one for the known entries and zero to effectively ignore missing entries during the modeling process.",
            "And if you take a closer look at this objective function, you'll see that it's essentially the squared error, some only over the selected S R * C elements of the matrix.",
            "So we're summing this only over the clustered parts of the matrix."
        ],
        [
            "So in step two, we want to agglomerate clusters based on their similarity.",
            "So first we need to define a distance between clusters.",
            "So let's say we have clusters CC one and CC two, and we want to merge them to form a combined cluster.",
            "The way we do this merging is by just taking a union of the rows and columns of the two clusters.",
            "So the distance between CC-1 and CC-2 is defined as the average reconstruction error of the combined cluster.",
            "So in case of squared Euclidean this would simply be the mean squared error of the combined cluster."
        ],
        [
            "At all six different ways of approximating matrix entries within the cluster that is 6 different ways of obtaining the Z hat values, and each of these lead to different cluster definitions.",
            "I'll go over to hear the 1st and the simpler one being blocked, or clusters where the entries within the cluster or approximated by the cluster mean.",
            "So in this case the approximation error would be 0 if all the entries within the cluster identical.",
            "And this technique can find blocks of similar values in the Matrix as Co clusters.",
            "The second scheme is called pattern based clusters where value within the cluster is approximated by the cluster Romine plus the column mean minus the cluster mean and this approximation scheme captures patterns or trends in the data values.",
            "It is hence most suitable for clustering microarray data.",
            "You're so rude."
        ],
        [
            "The algorithm step one is basically an iterative minimization procedure which begins with an initial clustering, basically a row and column cluster assignment, and it rates over 2 steps until convergence.",
            "The first step, update skokloster models.",
            "So depending on what the approximation scheme is, this step would involve updating the necessary Co cluster statistics.",
            "So in the simplest case of blocco clusters, this step would simply involve updating the cluster means this recomputing the means of every cluster.",
            "The next step updates the row cluster assignments.",
            "This is done by first assigning every row to the closest row cluster and then picking Sr rows which have the least error or which are closest to the arrow clusters.",
            "And note that even though we pick SRO's here, the rest of them are retained because they could be selected in future iterations.",
            "A similar step updates the column cluster assignments, so each step actually minimizes the objective function and hence convergence is guaranteed to a local optimum.",
            "So Step 1 results in K times Elko clusters arranged in a grid and because of this grid based constraint, you could have clusters which actually have large error and which are not very coherent.",
            "So Step 2 begins by pruning away identifying and pruning away some of these non coherent clusters and then it performs hierarchical operative clustering on the remaining clusters.",
            "Using some heuristics in the."
        ],
        [
            "Use of these who sticks out in the paper.",
            "So in step one, since we are using random initialization for the row and column clustering, the algorithm could be susceptible to local minimum problems and our strategy to address this is to begin by clustering all the data and then gradually shaving off data points and features till the required number of rows and columns are left.",
            "The intuition behind this is that by gradually shaving off data, we are allowing the initial clusters to move sufficiently from their original positions to the dense regions in the data matrix, and this we found in practice leads to a better local minimum and hence a better clustering solution.",
            "So step one of our algorithm is based on a very intuitive generative model consisting of a mixture of K * L exponential family distributions corresponding to the K * L. Cool clusters and uniform back."
        ],
        [
            "Ground distribution corresponding to the non informative entries in the matrix.",
            "So every element GI Jane the matrix is generated by this equation and Alpha IJ is the priors of the K times Elko clusters.",
            "Alpha Zeros the prior for the background distribution and P0 is its density.",
            "If PSI is an exponential family distribution.",
            "So note that the assignment of zij to each of these components is soft.",
            "And one can use a standard EM algorithm to fit this soft model.",
            "Let's move on to some results.",
            "We began by performing charity."
        ],
        [
            "Checks on synthetically generated microarray datasets be listed here in this table on the plot on the right hand side we are comparing our approach with Chang and churches Biclustering algorithm.",
            "The measure of comparison is the relative non intersecting area between the true clusters and the identified clusters.",
            "So the lower it is the better and you can see that our OCC is doing significantly better than by clustering on all four datasets also.",
            "Auto CC is not given the true number of clusters.",
            "It identifies the number of clusters in the data automatically."
        ],
        [
            "So here's how the data looks in the plot on the left hand side you can see the original data set.",
            "It has a noninformative background, noisy background, and three clusters embedded in it.",
            "And this matrix is permuted and given as input to the biclustering analysis algorithms and you can see that the construction by Auto CC is visually better than that of white clustering.",
            "Uh, with?"
        ],
        [
            "Then conducted experiments on 2 E microarray datasets.",
            "The lead data set and the cache data set.",
            "The ground truth on both these datasets is in the form of pairwise linkages between functionally related genes and a measure of evaluation is the overlap lift which basically measures how many times more correct gene linkages are predicted as compared to random chance.",
            "So the aim on all these datasets is to find the most coherent 150 to 200 clusters and these are going to involve only a small fraction of the data and we want to try and identify what the relevant portions are and eliminate them.",
            "So here are the results in the."
        ],
        [
            "Start on the left.",
            "We are comparing our OCC with four different Co clustering algorithms.",
            "BCC is Bregman Co clustering.",
            "It actually clusters all the data so we follow it up with the post processing step which discards rows and columns with large errors and ends up clustering only a subset of the data.",
            "By clustering, is changing churches algorithm or PSM is an order preserving Submatrix algorithm which tries to find a submatrix where the genes impose the same linear ordering across the experiments.",
            "And by Max is a simple deterministic Co clustering algorithm proposed by Pellek at all based on a simple binary data model.",
            "So this is baseline and you can see in the results that the overlap lift of our OCC is significantly better on both datasets.",
            "Also, above each bar you have the percentage of the data that's clustered by the different approaches.",
            "So in the lead data set you can actually see that our OCC clusters a larger fraction of the data set than by clustering.",
            "OPI SMN by Max.",
            "But the clusters are of better quality.",
            "The cache data set is quite noisy, it's a lot more noisy than Lee, so you can see that a very large fraction of the data has to be actually pruned away to get meaningful results like the arosi algorithm tones all but .9% of the data set to get biologically significant clusters.",
            "Now in the table on the right you can see some examples of biologically significant clusters found by our OCC on the lead data set.",
            "All of them have very low P values and they correspond quite well to categories from the genome."
        ],
        [
            "Do you hate Arky?",
            "Finally we tried to explore an application of work in performing feature selection along 1 axis while simultaneously clustering along the other and we tried this on human lung cancer data set which had around 12,000 genes and 181 human tissue samples belonging to two different lung cancer classes and the aim on this was to recover the two sample classes in an unsupervised way by clustering them.",
            "Using the jeans as features, these jeans are actually very in this data set.",
            "They are pretty non informative.",
            "A lot of them are redundant and noisy and so feature selection here becomes an important issue.",
            "We used to pre processed version of this data set where a lot of the jeans were discarded based on domain knowledge, but we wanted to apply our OCC to it to see if there was some more genes that could be discarded to improve the sample clustering accuracy.",
            "So in the figure you will see a comparison between ROTC Anne Brigman.",
            "Co clustering and K means, both of which use all the genes as features to cluster the samples and on the Y axis you see that sample clustering accuracy.",
            "So our OCC does much better.",
            "It actually achieves almost 100% accuracy, just using 10% of the genes as features."
        ],
        [
            "To conclude, we propose our OCC, an algorithm that efficiently and scalably can discard noisy regions of the data and find dense overlapping, possibly as an arbitrarily positioned clusters in noisy high dimensional datasets.",
            "We've explored applications in microarray data and shown promising results, but it would also be interesting to apply this to Market Basket analysis and text mining.",
            "Also, it seems that one needs to specify values of several parameters, but I would like to mention that the algorithm is actually not very sensitive to these choices of para meters, like the number of rows and columns need not be specified exactly, they just need to be in the right ballpark and the algorithm is robust to these values.",
            "So thank you and I can now answer questions if any."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm Magnum and my quarters are you Joe, Gunjan Gupta, Joydeep Ghosh, and rigid Dylan from UT Austin.",
                    "label": 0
                },
                {
                    "sent": "In this talk I'm going to be focusing on a scalable framework that we have developed for finding the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's coherent clusters and high dimensional noisy large datasets.",
                    "label": 0
                },
                {
                    "sent": "OK, sure.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to begin by discussing the nature of clustering problems that typically arise in a lot of real life applications and which are going to be a focus for today's talk.",
                    "label": 0
                },
                {
                    "sent": "The main part of the talk will be on proposed framework robust overlapping Co clustering and I'll go into details about the objective function and the algorithm for this.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, we look at some experimental results, mainly on microarray data.",
                    "label": 1
                },
                {
                    "sent": "So in several applications, the datasets that arise involve very small cohesive clusters and the rest of the data or a large part of the data is typically irrelevant or not very informative.",
                    "label": 0
                },
                {
                    "sent": "So an example of this is microarray data which you can see in the figure which is which can be represented as a matrix of genes and their expression values across experiments or conditions, and typically only a small subset of genes show coherent expression patterns.",
                    "label": 0
                },
                {
                    "sent": "Across a small subset of the conditions and the rest of the data is not really very informative.",
                    "label": 0
                },
                {
                    "sent": "From the biological point of view, a similar scenario arises.",
                    "label": 1
                },
                {
                    "sent": "In E Commerce applications like Market Basket analysis and fraud detection, where again the datasets are very large and noisy, But the interesting or useful patterns occur only in small pockets of the data.",
                    "label": 0
                },
                {
                    "sent": "So I ordered the clustering challenges that we're going to try and address.",
                    "label": 0
                },
                {
                    "sent": "We're going to be looking at datasets which have a large number of irrelevant or non informative data points and features which actually obfuscate the true or coherent clusters existing in the data also.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The clusters could exist in different subspaces of features, like for example in microarray data.",
                    "label": 1
                },
                {
                    "sent": "The same subset of genes could be correlated across different subsets of experiments.",
                    "label": 0
                },
                {
                    "sent": "Also the clusters could be overlapping like the same set of genes could participate in more than one biological processes.",
                    "label": 1
                },
                {
                    "sent": "So we want to find overlapping clusters as well.",
                    "label": 0
                },
                {
                    "sent": "So here's the problem, we want to find dense, arbitrarily positioned, possibly overlapping Co clusters.",
                    "label": 0
                },
                {
                    "sent": "Existing in large noisy datasets by pruning away or discarding the relevant parts of the data and trying to focus only on the relevant subset.",
                    "label": 0
                },
                {
                    "sent": "So there is a lot of relevant work in this area and it can be categorized into 3 broad classes of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Book, so firstly density based clustering algorithms have a motivation very similar to us.",
                    "label": 0
                },
                {
                    "sent": "DV scan optics for example, and Bregman bubble clustering which was proposed recently and however these are restricted to typically 1 sided clustering.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in contrast, Co clustering algorithms simultaneously cluster along multiple dimensions and hence they can find clusters of data points and features.",
                    "label": 0
                },
                {
                    "sent": "And finally, subspace clustering.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This also find clusters existing in different subsets of the feature space.",
                    "label": 0
                },
                {
                    "sent": "I would like to mention here that none of these previously proposed algorithms aimed to provide the full set of capabilities that we are aiming to provide.",
                    "label": 0
                },
                {
                    "sent": "So we propose robust overlapping Co clustering.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which proves the relevant points and features to find dense clusters embedded in the data, and the interesting part about this is that it is it is robust to the data noise model.",
                    "label": 0
                },
                {
                    "sent": "It can basically use any noise distribution from the regular exponential family.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the key idea of row CC is based on 2 steps.",
                    "label": 0
                },
                {
                    "sent": "In the first step it adapts the Bregman Co clustering algorithm to the extreme left you see the result of Bregman Co clustering, which is the partitional clustering algorithm proposed by Banerjee at all.",
                    "label": 0
                },
                {
                    "sent": "It is exhaustive, which means that it assigns every point in the data matrix to exactly 1 Co cluster and it gives you a partitioning of the data matrix into a grid of.",
                    "label": 0
                },
                {
                    "sent": "Four clusters, so the first step of our OCC adapts this algorithm to cluster only a subset of the rows and columns.",
                    "label": 0
                },
                {
                    "sent": "So we still obtain a set of Co clusters arranged in a grid, but they involve only the most relevant part of the data set and prune the rest.",
                    "label": 0
                },
                {
                    "sent": "So that's step one.",
                    "label": 0
                },
                {
                    "sent": "In step two, we actually prune away some of these clusters and merge the rest based on the similarity to recover the original existing overlapping clusters.",
                    "label": 0
                },
                {
                    "sent": "So the key idea is to actually over partition the existing clusters and step one to obtain small grid based clusters and then merge them to try and recover the original ones.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the key features of fire approach that distinguish it from other pro clustering algorithms?",
                    "label": 0
                },
                {
                    "sent": "Firstly, step one is based on a systematically developed cost function which will minimize iteratively to a local optimal.",
                    "label": 1
                },
                {
                    "sent": "Secondly, it is applicable to a large, wide range of distance measures.",
                    "label": 1
                },
                {
                    "sent": "That is all Bregman divergences, and depending on the data set you can choose the distance measure that's most suitable, like for instance for text data one can use I divergance or for micro.",
                    "label": 0
                },
                {
                    "sent": "Data one can use radically in distance.",
                    "label": 0
                },
                {
                    "sent": "A lot of times the data matrix can have missing values and this approach can handle these missing values as well.",
                    "label": 0
                },
                {
                    "sent": "Also several times the previous Co clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "For instance, changing churches by clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "They identify Co clusters one at a time rather than all at once, whereas our algorithm can detect all these clusters simultaneously.",
                    "label": 0
                },
                {
                    "sent": "And finally, it is computationally efficient than hence scalable to large high dimensional datasets.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's take a closer look at the formal problem definition.",
                    "label": 1
                },
                {
                    "sent": "We have our data matrix represented by Z consisting of M rows and columns.",
                    "label": 0
                },
                {
                    "sent": "Let S, R&S, C represent the number of rows and columns to be clustered.",
                    "label": 1
                },
                {
                    "sent": "Let K&L be the sets of clustered rows and columns which are obviously subsets of all the rows and columns, and we want to find keiro clusters and L column clusters.",
                    "label": 1
                },
                {
                    "sent": "That is K * L clusters arranged in a grid, and let Rowan Gamma represent the mappings of the Rose to the row clusters in the columns to the column clusters Respec.",
                    "label": 0
                },
                {
                    "sent": "Actively.",
                    "label": 0
                },
                {
                    "sent": "Now we need to input the values of SRNSC and it's actually fine to just set these values conservatively to sufficiently high values, because the algorithm will do a second round of pruning.",
                    "label": 0
                },
                {
                    "sent": "Also, since we want to over partition the clusters, we want to set candell relatively high.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the objective function that we want to minimize.",
                    "label": 1
                },
                {
                    "sent": "We are using squared Euclidean distance as the distance measure, but one can use any Bregman divergences.",
                    "label": 0
                },
                {
                    "sent": "So ZUV is the original value in the matrix.",
                    "label": 0
                },
                {
                    "sent": "In row you column V&Z hat is the value that's approximated within the cluster that this entry UV belongs to.",
                    "label": 0
                },
                {
                    "sent": "WV is a weight associated with every matrix entry and this is how we handle missing values.",
                    "label": 0
                },
                {
                    "sent": "So WV can be set to one for the known entries and zero to effectively ignore missing entries during the modeling process.",
                    "label": 0
                },
                {
                    "sent": "And if you take a closer look at this objective function, you'll see that it's essentially the squared error, some only over the selected S R * C elements of the matrix.",
                    "label": 1
                },
                {
                    "sent": "So we're summing this only over the clustered parts of the matrix.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in step two, we want to agglomerate clusters based on their similarity.",
                    "label": 0
                },
                {
                    "sent": "So first we need to define a distance between clusters.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have clusters CC one and CC two, and we want to merge them to form a combined cluster.",
                    "label": 0
                },
                {
                    "sent": "The way we do this merging is by just taking a union of the rows and columns of the two clusters.",
                    "label": 1
                },
                {
                    "sent": "So the distance between CC-1 and CC-2 is defined as the average reconstruction error of the combined cluster.",
                    "label": 0
                },
                {
                    "sent": "So in case of squared Euclidean this would simply be the mean squared error of the combined cluster.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At all six different ways of approximating matrix entries within the cluster that is 6 different ways of obtaining the Z hat values, and each of these lead to different cluster definitions.",
                    "label": 1
                },
                {
                    "sent": "I'll go over to hear the 1st and the simpler one being blocked, or clusters where the entries within the cluster or approximated by the cluster mean.",
                    "label": 0
                },
                {
                    "sent": "So in this case the approximation error would be 0 if all the entries within the cluster identical.",
                    "label": 0
                },
                {
                    "sent": "And this technique can find blocks of similar values in the Matrix as Co clusters.",
                    "label": 1
                },
                {
                    "sent": "The second scheme is called pattern based clusters where value within the cluster is approximated by the cluster Romine plus the column mean minus the cluster mean and this approximation scheme captures patterns or trends in the data values.",
                    "label": 0
                },
                {
                    "sent": "It is hence most suitable for clustering microarray data.",
                    "label": 0
                },
                {
                    "sent": "You're so rude.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The algorithm step one is basically an iterative minimization procedure which begins with an initial clustering, basically a row and column cluster assignment, and it rates over 2 steps until convergence.",
                    "label": 1
                },
                {
                    "sent": "The first step, update skokloster models.",
                    "label": 0
                },
                {
                    "sent": "So depending on what the approximation scheme is, this step would involve updating the necessary Co cluster statistics.",
                    "label": 0
                },
                {
                    "sent": "So in the simplest case of blocco clusters, this step would simply involve updating the cluster means this recomputing the means of every cluster.",
                    "label": 0
                },
                {
                    "sent": "The next step updates the row cluster assignments.",
                    "label": 0
                },
                {
                    "sent": "This is done by first assigning every row to the closest row cluster and then picking Sr rows which have the least error or which are closest to the arrow clusters.",
                    "label": 1
                },
                {
                    "sent": "And note that even though we pick SRO's here, the rest of them are retained because they could be selected in future iterations.",
                    "label": 0
                },
                {
                    "sent": "A similar step updates the column cluster assignments, so each step actually minimizes the objective function and hence convergence is guaranteed to a local optimum.",
                    "label": 0
                },
                {
                    "sent": "So Step 1 results in K times Elko clusters arranged in a grid and because of this grid based constraint, you could have clusters which actually have large error and which are not very coherent.",
                    "label": 0
                },
                {
                    "sent": "So Step 2 begins by pruning away identifying and pruning away some of these non coherent clusters and then it performs hierarchical operative clustering on the remaining clusters.",
                    "label": 0
                },
                {
                    "sent": "Using some heuristics in the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use of these who sticks out in the paper.",
                    "label": 0
                },
                {
                    "sent": "So in step one, since we are using random initialization for the row and column clustering, the algorithm could be susceptible to local minimum problems and our strategy to address this is to begin by clustering all the data and then gradually shaving off data points and features till the required number of rows and columns are left.",
                    "label": 1
                },
                {
                    "sent": "The intuition behind this is that by gradually shaving off data, we are allowing the initial clusters to move sufficiently from their original positions to the dense regions in the data matrix, and this we found in practice leads to a better local minimum and hence a better clustering solution.",
                    "label": 0
                },
                {
                    "sent": "So step one of our algorithm is based on a very intuitive generative model consisting of a mixture of K * L exponential family distributions corresponding to the K * L. Cool clusters and uniform back.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ground distribution corresponding to the non informative entries in the matrix.",
                    "label": 0
                },
                {
                    "sent": "So every element GI Jane the matrix is generated by this equation and Alpha IJ is the priors of the K times Elko clusters.",
                    "label": 0
                },
                {
                    "sent": "Alpha Zeros the prior for the background distribution and P0 is its density.",
                    "label": 0
                },
                {
                    "sent": "If PSI is an exponential family distribution.",
                    "label": 0
                },
                {
                    "sent": "So note that the assignment of zij to each of these components is soft.",
                    "label": 0
                },
                {
                    "sent": "And one can use a standard EM algorithm to fit this soft model.",
                    "label": 1
                },
                {
                    "sent": "Let's move on to some results.",
                    "label": 0
                },
                {
                    "sent": "We began by performing charity.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Checks on synthetically generated microarray datasets be listed here in this table on the plot on the right hand side we are comparing our approach with Chang and churches Biclustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "The measure of comparison is the relative non intersecting area between the true clusters and the identified clusters.",
                    "label": 0
                },
                {
                    "sent": "So the lower it is the better and you can see that our OCC is doing significantly better than by clustering on all four datasets also.",
                    "label": 0
                },
                {
                    "sent": "Auto CC is not given the true number of clusters.",
                    "label": 0
                },
                {
                    "sent": "It identifies the number of clusters in the data automatically.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's how the data looks in the plot on the left hand side you can see the original data set.",
                    "label": 0
                },
                {
                    "sent": "It has a noninformative background, noisy background, and three clusters embedded in it.",
                    "label": 0
                },
                {
                    "sent": "And this matrix is permuted and given as input to the biclustering analysis algorithms and you can see that the construction by Auto CC is visually better than that of white clustering.",
                    "label": 0
                },
                {
                    "sent": "Uh, with?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then conducted experiments on 2 E microarray datasets.",
                    "label": 0
                },
                {
                    "sent": "The lead data set and the cache data set.",
                    "label": 0
                },
                {
                    "sent": "The ground truth on both these datasets is in the form of pairwise linkages between functionally related genes and a measure of evaluation is the overlap lift which basically measures how many times more correct gene linkages are predicted as compared to random chance.",
                    "label": 1
                },
                {
                    "sent": "So the aim on all these datasets is to find the most coherent 150 to 200 clusters and these are going to involve only a small fraction of the data and we want to try and identify what the relevant portions are and eliminate them.",
                    "label": 0
                },
                {
                    "sent": "So here are the results in the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start on the left.",
                    "label": 0
                },
                {
                    "sent": "We are comparing our OCC with four different Co clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "BCC is Bregman Co clustering.",
                    "label": 0
                },
                {
                    "sent": "It actually clusters all the data so we follow it up with the post processing step which discards rows and columns with large errors and ends up clustering only a subset of the data.",
                    "label": 0
                },
                {
                    "sent": "By clustering, is changing churches algorithm or PSM is an order preserving Submatrix algorithm which tries to find a submatrix where the genes impose the same linear ordering across the experiments.",
                    "label": 0
                },
                {
                    "sent": "And by Max is a simple deterministic Co clustering algorithm proposed by Pellek at all based on a simple binary data model.",
                    "label": 0
                },
                {
                    "sent": "So this is baseline and you can see in the results that the overlap lift of our OCC is significantly better on both datasets.",
                    "label": 0
                },
                {
                    "sent": "Also, above each bar you have the percentage of the data that's clustered by the different approaches.",
                    "label": 0
                },
                {
                    "sent": "So in the lead data set you can actually see that our OCC clusters a larger fraction of the data set than by clustering.",
                    "label": 0
                },
                {
                    "sent": "OPI SMN by Max.",
                    "label": 0
                },
                {
                    "sent": "But the clusters are of better quality.",
                    "label": 0
                },
                {
                    "sent": "The cache data set is quite noisy, it's a lot more noisy than Lee, so you can see that a very large fraction of the data has to be actually pruned away to get meaningful results like the arosi algorithm tones all but .9% of the data set to get biologically significant clusters.",
                    "label": 0
                },
                {
                    "sent": "Now in the table on the right you can see some examples of biologically significant clusters found by our OCC on the lead data set.",
                    "label": 1
                },
                {
                    "sent": "All of them have very low P values and they correspond quite well to categories from the genome.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do you hate Arky?",
                    "label": 0
                },
                {
                    "sent": "Finally we tried to explore an application of work in performing feature selection along 1 axis while simultaneously clustering along the other and we tried this on human lung cancer data set which had around 12,000 genes and 181 human tissue samples belonging to two different lung cancer classes and the aim on this was to recover the two sample classes in an unsupervised way by clustering them.",
                    "label": 1
                },
                {
                    "sent": "Using the jeans as features, these jeans are actually very in this data set.",
                    "label": 0
                },
                {
                    "sent": "They are pretty non informative.",
                    "label": 0
                },
                {
                    "sent": "A lot of them are redundant and noisy and so feature selection here becomes an important issue.",
                    "label": 0
                },
                {
                    "sent": "We used to pre processed version of this data set where a lot of the jeans were discarded based on domain knowledge, but we wanted to apply our OCC to it to see if there was some more genes that could be discarded to improve the sample clustering accuracy.",
                    "label": 0
                },
                {
                    "sent": "So in the figure you will see a comparison between ROTC Anne Brigman.",
                    "label": 1
                },
                {
                    "sent": "Co clustering and K means, both of which use all the genes as features to cluster the samples and on the Y axis you see that sample clustering accuracy.",
                    "label": 0
                },
                {
                    "sent": "So our OCC does much better.",
                    "label": 0
                },
                {
                    "sent": "It actually achieves almost 100% accuracy, just using 10% of the genes as features.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude, we propose our OCC, an algorithm that efficiently and scalably can discard noisy regions of the data and find dense overlapping, possibly as an arbitrarily positioned clusters in noisy high dimensional datasets.",
                    "label": 1
                },
                {
                    "sent": "We've explored applications in microarray data and shown promising results, but it would also be interesting to apply this to Market Basket analysis and text mining.",
                    "label": 0
                },
                {
                    "sent": "Also, it seems that one needs to specify values of several parameters, but I would like to mention that the algorithm is actually not very sensitive to these choices of para meters, like the number of rows and columns need not be specified exactly, they just need to be in the right ballpark and the algorithm is robust to these values.",
                    "label": 0
                },
                {
                    "sent": "So thank you and I can now answer questions if any.",
                    "label": 0
                }
            ]
        }
    }
}