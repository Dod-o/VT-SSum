{
    "id": "ovscejc2t7vnihexsy5a3ag2hz7fyyxv",
    "title": "Bayesian Interpretations of RKHS Embedding Methods",
    "info": {
        "author": [
            "David Kristjanson Duvenaud, Department of Computer Science, University of Toronto"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_duvenaud_bayesian/",
    "segmentation": [
        [
            "I mean, OK, just."
        ],
        [
            "Then Gaussian process you count the fact that we're going to do an integral.",
            "So just as the online I'm going to 1st introducing a result that I gave you, I so apologize anyone who's at UI.",
            "But it's like impressed introduction to Colonel Hardy invasion, quadrature and then results showing that their optimizing the same thing and then a few results that I've come up with since then, including when I found out yesterday.",
            "Thanks for suggestion, firmly song.",
            "And then I think.",
            "I'll have to tease you with putting at the very end of the talk, so."
        ],
        [
            "Alright, so to introduce Colonel hurting invasion quadrotor.",
            "I'm first going to introduce the problem that we're trying to solve, which is quadrature.",
            "Just briefly, we're trying to find the integral of some function against some input distribution, and of course this comes up all the time invasion machine learning when we expectations or integrating nuisance parameters or computing normalization constants."
        ],
        [
            "The standard workhorse, of course for these problems is sampling, where we take samples from our input density, evaluate the function at those locations, and compute the empirical mean to estimate the integral."
        ],
        [
            "This might not be always the best thing to do for two reasons.",
            "One is that any finite sample from the input distribution, some regions are going to be over represented by examples and some are going to be underrepresented.",
            "There's no way around this example actually from the distribution."
        ],
        [
            "Second one is that if we know our function is smooth, we can take advantage of this, and in general sampling will do a little bit of wasteful computation when it computes function values that are very close to each other, because we know these would be very."
        ],
        [
            "Miller so quasi Monte Carlo methods or like high dispersion methods spread out the samples in some sensible way.",
            "Such that the convergence to the empirical mean is faster, although it requires making assumptions about the function."
        ],
        [
            "So the only the first cause I'm not calling to introduce is Colonel Hurting, which has a long and strange history which I won't go into here, but effectively it's just a sequential procedure for choosing sample locations depending on the previous."
        ],
        [
            "Locations.",
            "And it keeps the same estimate rule as.",
            "As a modern mother sure keeps the same estimate rule as Monte Carlo.",
            "Which is waiting as examples evenly."
        ],
        [
            "And it attains faster convergence, although the proof of this is still sort of in dispute.",
            "Empirically it attained something like 1 / N convergence as opposed to 1 / sqrt N and it does this by sort of spreading out the examples in some sensible ways.",
            "So here we're looking down on a 2D mixture of Gaussians.",
            "And xamples from this mixture sort of unfortunately bunched up in some regions and then missed some modes and sort of a cartoon version of why Colonel hurting can change faster convergence is that it spreads out the samples to sort of make sure that it gets every every part represented sort of portion it."
        ],
        [
            "After it was introduced, it was actually discovered to minimize this metric between distributions called maximum mean discrepancy, which I assume everyone is most people are familiar with.",
            "Essentially, it measures the difference.",
            "Difference between these two distributions in terms of how different an integral against those two distributions can be given that a function lives in some smooth classic functions.",
            "And so the MMD between P&Q will be 0 if they are exactly equal and it will be large if they.",
            "Well are different."
        ],
        [
            "Specifically, kernel hurting is minimizing the MMD between PST.",
            "Original discussion and Q of X, which is a set of point masses at all the chosen sample locations.",
            "So this is the exact form of the objective being minimized by Colonel hurting.",
            "So which is MMD between vision and the sum of direct?"
        ],
        [
            "So the next thing about kernel hurting is that if we assume that the function are interesting against lives in an arcade chest.",
            "Then MMD actually has a clue."
        ],
        [
            "Forum.",
            "And even nicer when were sequentially minimizing MMD, we only have to look at two terms.",
            "One is which.",
            "Well, yes, one encourages us to put regions.",
            "Examples in regions of high density under P and the other one discourages us from putting examples close to that place.",
            "We've already put down."
        ],
        [
            "And so just to give you like.",
            "A quick demo here we're looking down on the hurting objective function for a 2D2 D Gaussian rather and the red dot represents the point that we're going to do next.",
            "I guess I should use this more.",
            "Anne.",
            "And as we saw, after we had the first point, we can see that."
        ],
        [
            "Alright, so after he chosen one point, the next one is going to have to be in a region of high density, but far from the existing point.",
            "And actually there's a symmetric loss function, so we just."
        ],
        [
            "Only choose this point.",
            "The next one is going to be in a region of high density, but far from the existing point."
        ],
        [
            "After we've added three points, we have this horrible clown face."
        ],
        [
            "But"
        ],
        [
            "Will press on."
        ],
        [
            "And basically what?"
        ],
        [
            "I wanted to get."
        ],
        [
            "This slide is that."
        ],
        [
            "Pure."
        ],
        [
            "Hurting is going is."
        ],
        [
            "Choosing a set of."
        ],
        [
            "Who's density?"
        ],
        [
            "Roughly matches the under."
        ],
        [
            "Density, but which is spread out.",
            "So I mean, I hope you think this is a sensible summary of a 2D."
        ],
        [
            "Ocean.",
            "Alright, so just to summarize, Colonel hurting is a sequential sampling method which minimizes a worst case divergance given that FX is in an arcade.",
            "Jess Ann, like Monte Carlo, it weights all the function evaluations evenly when it's going to compute an integral against this distribution.",
            "So we can."
        ],
        [
            "Ask what if we allowed non uniform weights here?"
        ],
        [
            "Francis Bach this year looked at weighted herding strategies and showed that there was an improvement.",
            "Convergence rates was possible."
        ],
        [
            "Anne.",
            "So we decided to ask about what would be optimal weighting strategy be and in."
        ],
        [
            "To tell you that I'm going to introduce another method called Bayesian Quadrature, also known as Bayesian Monte Carlo.",
            "And this is a method that has been discovered and rediscovered many times, including by persevere konas, who is here in the probabilistic numerics."
        ],
        [
            "Shop today.",
            "Basically, the way we're going to approach integrating under something."
        ],
        [
            "And that we've only seen it a few samples by placing a distribution on the function, which here is represented by this green mean and standard deviation."
        ],
        [
            "And the idea is that as we evaluate the function at some points our posterior over the Internet or over the function is going to concentrate.",
            "And our posterior over the function implies the posterior over its integral.",
            "And in the case of a Gaussian process prior and if this posterior over the integral is actually Goshen as well."
        ],
        [
            "So we can keep adding."
        ],
        [
            "Examples and watch the."
        ],
        [
            "The distribution over the integral converge and the nice thing about this map."
        ],
        [
            "Is that we can choose samples however we want.",
            "So any questions at this point?",
            "OK."
        ],
        [
            "So as I said, the posterior versus Gaussian and it's mean, which is like the estimated_loss, is also linear in the function values, which is nice, and the waiter just given by this term that depends on the prior and also the covariance between all the points."
        ],
        [
            "Interestingly, the these weights don't sum to one and they don't have to be positive either.",
            "So here's an example of some vision quadrature weights on just a mixture of Gaussians and we can see that you know they mostly cluster around 1 / N, But some of them are even negative and they they don't quite some time."
        ],
        [
            "There.",
            "Now we can ask if we were using Bayesian quadrature estimates, integrals, how would we?"
        ],
        [
            "Examples and the natural thing we can do is.",
            "Minimize uncertainty about the integral, which for a Gaussian is equivalent to its variance.",
            "And that has a nice closed form here."
        ],
        [
            "Um?",
            "And it also has one term to favor.",
            "To favorite examples where distribution is high and another term to discourage us from putting examples close to ones that are.",
            "Correlated with the other sample locations, so it's very similar flavor to the coding objective."
        ],
        [
            "Interestingly, it does not depend on the function values.",
            "I mean almost any model of functions variance would depend on the values that you actually see, but this one doesn't.",
            "So that actually lets us choose samples ahead of time without worrying about the function actually is just like internal."
        ],
        [
            "And if we choose our samples greedily, then we call this sequential Bayesian quadrature."
        ],
        [
            "Oh shit.",
            "So.",
            "Now we can try to compare these two methods so we have kernel which minimizes this worst case bound and basic, which minimizes the posterior variance of a model.",
            "And we can ask, is there any correspondence?",
            "And it turns out we can."
        ],
        [
            "Only very strong, which is that the posterior variance of a Gaussian process conditioned on these applications is exactly equal to the squared MMD between the original distribution.",
            "And the set of directs at our sample locations weighted by the vision quadrature weights.",
            "So.",
            "The two things turn out to be equivalent."
        ],
        [
            "And we can say that the basic water is actually minimizing the exact same objective function as Colonel."
        ],
        [
            "We can also ask about the relative performance of these methods.",
            "Now that we know that they're minimizing the same thing and.",
            "Bayesian quadrature has extra freedom.",
            "The Colonel who doesn't wish they can wait these samples."
        ],
        [
            "At once.",
            "So we can ask how this affected form.",
            "And again we can say something very."
        ],
        [
            "Strong, which is that.",
            "Yeah, the basic water rates are the optimal weights for any well optimal out of any weights.",
            "For estimating these integrals, they have a minimax interpretation.",
            "Anne."
        ],
        [
            "So.",
            "So far.",
            "Our variance are partially variance all along actually had two interpretations.",
            "One was just the standard posterior variance under the model and the other one was actually a tight bound on the estimation error of Z.",
            "Given that we know the norm of the function in the arcade."
        ],
        [
            "We can actually now look at the rates of convergence of these different methods, assuming that their assumptions are true.",
            "So here is 1 / sqrt N convergence of.",
            "Monte Carlo and this is a log log plot by the way, so as a function of number examples.",
            "Inserting has empirically this sort of 1 / N convergence rate, roughly."
        ],
        [
            "And now we can ask, what if we took the hurting example locations and just re rated them according to the Asian Quadrature rules and we would see that.",
            "Well, we get some slight increase in convergence rates."
        ],
        [
            "But if we choose the example locations and the weights according to inquire with this really super fast convergence, I don't know what the form of this convergence is.",
            "It looks vaguely exponential.",
            "I'm kind of hoping that someone here will be able to point me to some literature, because I bet this has been proven in the 70s."
        ],
        [
            "Anyways.",
            "Sanity check we can actually just draw functions from the arc HS that we're assuming and compute the empirical error rates, and you know they match."
        ],
        [
            "We can also sort of do sensitivity analysis and an example functions outside of the RHS and just reassuringly the rough ordering in shape of the convergence plot stays with."
        ],
        [
            "And the coolest thing that you can do with this new result, I think, is that you can actually put this tight very quickly.",
            "Decreasing bound on the error of the vision.",
            "Quadrotor estimate.",
            "I mean you have to know the Dark Ages and the norm of the function in the RHS.",
            "I think bounds in general are like not that useful except for giving intuition on problems, but it's kind of refreshing that we can put a tight bound on the vision estimator, I think."
        ],
        [
            "Alright, so just to summarize, this part of the talk the posterior variance of the integral under the prior is exactly equivalent to the squared maximum discrepancy.",
            "And if we believe the RKS assumption, we have a tight clothes from upper bound on the visionaire."
        ],
        [
            "Beijing culture has some very fast but unknown convergence rate."
        ],
        [
            "And the optimal weighted hurting strategy is Beijing quadrotor."
        ],
        [
            "Alright, and this this part of the talk was joint work."
        ],
        [
            "Parents hazard."
        ],
        [
            "OK, so now that we've laid this groundwork, I'm going to 1st show a takeaway that I think.",
            "Of how I think that this Bayesian interpretation of Colonel Huntington actually are quadrature can inform how we can improve kernel hurting.",
            "Then I'm going to give Bayesian interpretations of mean embeddings.",
            "The Kernel 2 sample tests, HSE and determined to point processes in terms of expectations on your GPS and also.",
            "Give some hints about how we could maybe improve these methods.",
            "Based on this new understanding."
        ],
        [
            "Alright, so the first thing I want to say is that the Gaussian process assumption and the RKS assumption are actually equivalent.",
            "Now, like we can see this.",
            "I mean, I don't think this is controversial.",
            "But people often use kernel loading for inference.",
            "The thing about that is that if you're using it for inference, you have to be.",
            "Integrating against selected function and likelihood functions are kind of weird.",
            "They don't really look like something you would draw from a GP.",
            "They spent a lot of time being very close to zero and a small amount of time being very far away from zero.",
            "And this sort of heavy tailed behavior is basically impossible to model with the Gaussian process.",
            "So here if we just evaluated the function 3 points and 50 posterior to it, we would get this."
        ],
        [
            "Distribution that basically thinks nothing is going on.",
            "If we were to example in the mode somewhere, then basically the hyperparameter learning would increase our variance and shorter length scale, and we would basically have to be forever uncertain about everywhere else in the function until we sampled almost every point.",
            "So it's basically no way for GP to sort of.",
            "Tomato likelihood functions well, at least when they, when they peaked as they usually are once you."
        ],
        [
            "Little bit of data.",
            "We can imagine like."
        ],
        [
            "Some ways of doing this?",
            "We could for instance take the log of our likelihood function and assume that that's in it.",
            "I modeled by GPR NRK chess.",
            "And this turns out to be like."
        ],
        [
            "And assumption.",
            "Basically, you can say that no, the function is going to be close to 0 almost everywhere except some regions is going to be very far from there."
        ],
        [
            "So I think that the takeaway from hurting is that the yes hernias are inappropriate for doing inference, and we can maybe imagine, like assuming that the log likelihood in RHS or some other related method.",
            "But in general, I think that.",
            "GP's are powerful because they let us check our assumptions by looking at the marginal likelihood and we can actually for any given function, compare how well different RHS is actually model.",
            "The functions that we're looking at."
        ],
        [
            "Alright.",
            "Now I'm just going to give a switch tracks and give an interpretation of a bunch of.",
            "Sort of statistics in terms of Gaussian processes.",
            "So the first one which was discovered by Crick this summer."
        ],
        [
            "So is that the mean embedding of a distribution with respect to HS can actually be written as the covariance of a function at each point of the function with the integral of the function?"
        ],
        [
            "Integrated with respect to P of XI mean this is just doing algebra, rearranging things so it's.",
            "I'm not sure that this is going to be like that much more powerful of of interpretation, but it's sort of starting point.",
            "That's going to let us drive all the other results.",
            "You can actually start from here or from the kernel.",
            "Hurting equals Beijing quadrature result to drive all the next ones that I.",
            "That I'm going to show."
        ],
        [
            "So the net result is has to do with the kernel through sample test, so the kernels out to sample test is used to determine whether two distributions are the same."
        ],
        [
            "And basically, Arthur Gretton Company proposed using MMD between P&Q 2.",
            "To basically measure the."
        ],
        [
            "Extend two distributions are the same and so natural test statistic and we can actually say that just like very similar to the previous result, the MMD between two distributions is just the variance of a 0 mean GP integrated against.",
            "P of X minus integrated against QX.",
            "So we can sort of reinterpret the.",
            "Let me see.",
            "The kernel to sample test is saying that we're measuring to what extent we expect the expectation of these two.",
            "Distributions to be the same given functions drawn from GP."
        ],
        [
            "There.",
            "So the takeaway from this are a we can ask well, under what circumstances do we actually like what sort of functions we actually care about computing expectations of our populations with respect to?",
            "Like you know, these might be medical outcomes, and this might be some like quality of life.",
            "That's a function of.",
            "If X, like X is blood pressure or something?",
            "I mean it's kind of a stretch, but we can imagine actually thinking about functions that we care about and that informing our choice of kernels.",
            "The other, more practical takeaway is that we can probably have some sort of approximate method of sampling GPS and using that to compute the MMD in some.",
            "Quicker approximate way."
        ],
        [
            "A very related result is the Hilbert Schmidt independence criterion, which is attempting to measure the extent or a test used to determine whether given some samples X&Y, whether the joint distribution actually factorize."
        ],
        [
            "So the interpretation from that was used to drive it is the infinite dimensional previous norm of a cross covariance matrix of features of X&Y and."
        ],
        [
            "And it has this.",
            "Well, I would say somewhat nice form of just a bunch of expectations of kernel functions with respect to our input distribution P. Anne."
        ],
        [
            "It turns out that we can rewrite this actually very simply as the expected expected difference between function to the functions drawn from GP against the joint distribution or against the product of the marginals.",
            "So.",
            "So again, I think to take away the same as other."
        ],
        [
            "This method, but it's nice that such a method was that was driving such a roundabout way."
        ],
        [
            "He has such a simple interpretation.",
            "OK, and now here's like the most relevant one for today.",
            "Is a really simple interpretation for determining the point process."
        ],
        [
            "So.",
            "Or rather, at least for the greedy map way of adding points to a set under the DPP model.",
            "So we can say that greedy map is going to map some Max."
        ],
        [
            "Is the probability of our existing set plus a new point.",
            "It's really not that hard to show that this determinant."
        ],
        [
            "Has a new term which has this form.",
            "And that term actually is exactly equivalent to the variance.",
            "The marginal variance of function value.",
            "Given the GP prior of the new proposed new point, given all the existing exam."
        ],
        [
            "So basically, we're going to be asking which point are we most marginally uncertain about and going to add that to our set.",
            "And now this is where it relates to these dispersion methods, because if we're using GPS to compute expectations later, this is actually not quite the right thing to do.",
            "Probably you would actually want to use some sort of vision, quadrature or kernel hardening method that actually took into account the fact that it wanted to maximize the variance of integrals, not just the marginal variance of individual points.",
            "Of course, this requires us to have like a measure over X, but I think that arises naturally in your.",
            "And your problems it's not clear yet."
        ],
        [
            "Is also related to like sensor placement by Andreas Cross where he had like a similar metric adding the point that had the highest entropy in a GP.",
            "I think that's equivalent here.",
            "This is the result that I came up with yesterday so haven't thought about very much yet."
        ],
        [
            "Yeah, thanks to this song and Roman for.",
            "For this suggestion.",
            "I just wanted to summarize the connection."
        ],
        [
            "I made so basically kernel loading, minimize the same objective as patient quadrature and then all the rest of these are actually equivalent to each other.",
            "Alright."
        ],
        [
            "So there's a few possible extensions that I've already mentioned that these sort of."
        ],
        [
            "Just one is we could use some sort of log kernel hurting for inference or some more appropriate assumptions."
        ],
        [
            "The other one is like obviously Next up.",
            "I want to come up with an interpretation of conditional mean embeddings in terms of Goshen."
        ],
        [
            "Processes.",
            "The other thing that I wanted to point them to, and actually anyone who's doing these.",
            "Conditional mean embeddings work is that I see often low rank approximations and in the Gaussian process literature there's a lot of different sparse GP methods that.",
            "Have a lot of like have nice properties, especially for computing posterior variances in a nice way, so I think that if you're looking for better approximations you might just want to convert your problem into a GP and then use the existing sparsity methods."
        ],
        [
            "OK, more generally, I've kind of noticed once I started reading like the UCL papers that there's a lot of correspondences between this sort of archaic community and this sort of like beige and numerics GP community.",
            "And in fact we can say like a lot of these.",
            "There's a lot of corresponding model classes that are being solved in slightly different ways.",
            "Currency progression, same model class.",
            "Hardkernel learning turn out like same model classes.",
            "This additive Gaussian processes thing Colonel Bayes rule is kind of similar to something that might cause more.",
            "Did Beijing quadrature ratios where you try to put GPS on different functions.",
            "The conditional distribution is used for dynamics.",
            "It seems very similar to like all the model based reinforcement learning in the in the Asian community.",
            "Colonel message passing, which Arthur claims is beige and is not big enough for me.",
            "Me and Phillip and again, my customer had been trying to play around with the version of message passing that uses GPS, but it's hard to.",
            "I mean to us, well, it's hard to come up with really compelling.",
            "Oil based message passing algorithm passing algorithm that uses GPS so.",
            "In general, I think that it's I feel like these communities haven't talked to each other very much and there's a lot of like duplicated work, and that might be a good thing because, you know, we can get twice as many papers, but.",
            "I think the general weekend could be a lot closer."
        ],
        [
            "So thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, OK, just.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then Gaussian process you count the fact that we're going to do an integral.",
                    "label": 0
                },
                {
                    "sent": "So just as the online I'm going to 1st introducing a result that I gave you, I so apologize anyone who's at UI.",
                    "label": 0
                },
                {
                    "sent": "But it's like impressed introduction to Colonel Hardy invasion, quadrature and then results showing that their optimizing the same thing and then a few results that I've come up with since then, including when I found out yesterday.",
                    "label": 0
                },
                {
                    "sent": "Thanks for suggestion, firmly song.",
                    "label": 0
                },
                {
                    "sent": "And then I think.",
                    "label": 0
                },
                {
                    "sent": "I'll have to tease you with putting at the very end of the talk, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so to introduce Colonel hurting invasion quadrotor.",
                    "label": 0
                },
                {
                    "sent": "I'm first going to introduce the problem that we're trying to solve, which is quadrature.",
                    "label": 0
                },
                {
                    "sent": "Just briefly, we're trying to find the integral of some function against some input distribution, and of course this comes up all the time invasion machine learning when we expectations or integrating nuisance parameters or computing normalization constants.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The standard workhorse, of course for these problems is sampling, where we take samples from our input density, evaluate the function at those locations, and compute the empirical mean to estimate the integral.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This might not be always the best thing to do for two reasons.",
                    "label": 1
                },
                {
                    "sent": "One is that any finite sample from the input distribution, some regions are going to be over represented by examples and some are going to be underrepresented.",
                    "label": 0
                },
                {
                    "sent": "There's no way around this example actually from the distribution.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second one is that if we know our function is smooth, we can take advantage of this, and in general sampling will do a little bit of wasteful computation when it computes function values that are very close to each other, because we know these would be very.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Miller so quasi Monte Carlo methods or like high dispersion methods spread out the samples in some sensible way.",
                    "label": 0
                },
                {
                    "sent": "Such that the convergence to the empirical mean is faster, although it requires making assumptions about the function.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the only the first cause I'm not calling to introduce is Colonel Hurting, which has a long and strange history which I won't go into here, but effectively it's just a sequential procedure for choosing sample locations depending on the previous.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Locations.",
                    "label": 0
                },
                {
                    "sent": "And it keeps the same estimate rule as.",
                    "label": 1
                },
                {
                    "sent": "As a modern mother sure keeps the same estimate rule as Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Which is waiting as examples evenly.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it attains faster convergence, although the proof of this is still sort of in dispute.",
                    "label": 0
                },
                {
                    "sent": "Empirically it attained something like 1 / N convergence as opposed to 1 / sqrt N and it does this by sort of spreading out the examples in some sensible ways.",
                    "label": 1
                },
                {
                    "sent": "So here we're looking down on a 2D mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And xamples from this mixture sort of unfortunately bunched up in some regions and then missed some modes and sort of a cartoon version of why Colonel hurting can change faster convergence is that it spreads out the samples to sort of make sure that it gets every every part represented sort of portion it.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After it was introduced, it was actually discovered to minimize this metric between distributions called maximum mean discrepancy, which I assume everyone is most people are familiar with.",
                    "label": 1
                },
                {
                    "sent": "Essentially, it measures the difference.",
                    "label": 0
                },
                {
                    "sent": "Difference between these two distributions in terms of how different an integral against those two distributions can be given that a function lives in some smooth classic functions.",
                    "label": 0
                },
                {
                    "sent": "And so the MMD between P&Q will be 0 if they are exactly equal and it will be large if they.",
                    "label": 0
                },
                {
                    "sent": "Well are different.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Specifically, kernel hurting is minimizing the MMD between PST.",
                    "label": 0
                },
                {
                    "sent": "Original discussion and Q of X, which is a set of point masses at all the chosen sample locations.",
                    "label": 1
                },
                {
                    "sent": "So this is the exact form of the objective being minimized by Colonel hurting.",
                    "label": 0
                },
                {
                    "sent": "So which is MMD between vision and the sum of direct?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next thing about kernel hurting is that if we assume that the function are interesting against lives in an arcade chest.",
                    "label": 0
                },
                {
                    "sent": "Then MMD actually has a clue.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Forum.",
                    "label": 0
                },
                {
                    "sent": "And even nicer when were sequentially minimizing MMD, we only have to look at two terms.",
                    "label": 1
                },
                {
                    "sent": "One is which.",
                    "label": 0
                },
                {
                    "sent": "Well, yes, one encourages us to put regions.",
                    "label": 0
                },
                {
                    "sent": "Examples in regions of high density under P and the other one discourages us from putting examples close to that place.",
                    "label": 0
                },
                {
                    "sent": "We've already put down.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so just to give you like.",
                    "label": 0
                },
                {
                    "sent": "A quick demo here we're looking down on the hurting objective function for a 2D2 D Gaussian rather and the red dot represents the point that we're going to do next.",
                    "label": 0
                },
                {
                    "sent": "I guess I should use this more.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And as we saw, after we had the first point, we can see that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so after he chosen one point, the next one is going to have to be in a region of high density, but far from the existing point.",
                    "label": 0
                },
                {
                    "sent": "And actually there's a symmetric loss function, so we just.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only choose this point.",
                    "label": 0
                },
                {
                    "sent": "The next one is going to be in a region of high density, but far from the existing point.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After we've added three points, we have this horrible clown face.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will press on.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically what?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I wanted to get.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This slide is that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pure.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hurting is going is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Choosing a set of.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who's density?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Roughly matches the under.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Density, but which is spread out.",
                    "label": 0
                },
                {
                    "sent": "So I mean, I hope you think this is a sensible summary of a 2D.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocean.",
                    "label": 0
                },
                {
                    "sent": "Alright, so just to summarize, Colonel hurting is a sequential sampling method which minimizes a worst case divergance given that FX is in an arcade.",
                    "label": 0
                },
                {
                    "sent": "Jess Ann, like Monte Carlo, it weights all the function evaluations evenly when it's going to compute an integral against this distribution.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ask what if we allowed non uniform weights here?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Francis Bach this year looked at weighted herding strategies and showed that there was an improvement.",
                    "label": 0
                },
                {
                    "sent": "Convergence rates was possible.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So we decided to ask about what would be optimal weighting strategy be and in.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To tell you that I'm going to introduce another method called Bayesian Quadrature, also known as Bayesian Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "And this is a method that has been discovered and rediscovered many times, including by persevere konas, who is here in the probabilistic numerics.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shop today.",
                    "label": 0
                },
                {
                    "sent": "Basically, the way we're going to approach integrating under something.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that we've only seen it a few samples by placing a distribution on the function, which here is represented by this green mean and standard deviation.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the idea is that as we evaluate the function at some points our posterior over the Internet or over the function is going to concentrate.",
                    "label": 0
                },
                {
                    "sent": "And our posterior over the function implies the posterior over its integral.",
                    "label": 0
                },
                {
                    "sent": "And in the case of a Gaussian process prior and if this posterior over the integral is actually Goshen as well.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can keep adding.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples and watch the.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The distribution over the integral converge and the nice thing about this map.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that we can choose samples however we want.",
                    "label": 0
                },
                {
                    "sent": "So any questions at this point?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said, the posterior versus Gaussian and it's mean, which is like the estimated_loss, is also linear in the function values, which is nice, and the waiter just given by this term that depends on the prior and also the covariance between all the points.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interestingly, the these weights don't sum to one and they don't have to be positive either.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of some vision quadrature weights on just a mixture of Gaussians and we can see that you know they mostly cluster around 1 / N, But some of them are even negative and they they don't quite some time.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "Now we can ask if we were using Bayesian quadrature estimates, integrals, how would we?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples and the natural thing we can do is.",
                    "label": 0
                },
                {
                    "sent": "Minimize uncertainty about the integral, which for a Gaussian is equivalent to its variance.",
                    "label": 0
                },
                {
                    "sent": "And that has a nice closed form here.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And it also has one term to favor.",
                    "label": 0
                },
                {
                    "sent": "To favorite examples where distribution is high and another term to discourage us from putting examples close to ones that are.",
                    "label": 0
                },
                {
                    "sent": "Correlated with the other sample locations, so it's very similar flavor to the coding objective.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interestingly, it does not depend on the function values.",
                    "label": 0
                },
                {
                    "sent": "I mean almost any model of functions variance would depend on the values that you actually see, but this one doesn't.",
                    "label": 0
                },
                {
                    "sent": "So that actually lets us choose samples ahead of time without worrying about the function actually is just like internal.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we choose our samples greedily, then we call this sequential Bayesian quadrature.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh shit.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now we can try to compare these two methods so we have kernel which minimizes this worst case bound and basic, which minimizes the posterior variance of a model.",
                    "label": 1
                },
                {
                    "sent": "And we can ask, is there any correspondence?",
                    "label": 0
                },
                {
                    "sent": "And it turns out we can.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only very strong, which is that the posterior variance of a Gaussian process conditioned on these applications is exactly equal to the squared MMD between the original distribution.",
                    "label": 1
                },
                {
                    "sent": "And the set of directs at our sample locations weighted by the vision quadrature weights.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The two things turn out to be equivalent.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can say that the basic water is actually minimizing the exact same objective function as Colonel.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also ask about the relative performance of these methods.",
                    "label": 0
                },
                {
                    "sent": "Now that we know that they're minimizing the same thing and.",
                    "label": 0
                },
                {
                    "sent": "Bayesian quadrature has extra freedom.",
                    "label": 0
                },
                {
                    "sent": "The Colonel who doesn't wish they can wait these samples.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At once.",
                    "label": 0
                },
                {
                    "sent": "So we can ask how this affected form.",
                    "label": 0
                },
                {
                    "sent": "And again we can say something very.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strong, which is that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the basic water rates are the optimal weights for any well optimal out of any weights.",
                    "label": 0
                },
                {
                    "sent": "For estimating these integrals, they have a minimax interpretation.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So far.",
                    "label": 0
                },
                {
                    "sent": "Our variance are partially variance all along actually had two interpretations.",
                    "label": 0
                },
                {
                    "sent": "One was just the standard posterior variance under the model and the other one was actually a tight bound on the estimation error of Z.",
                    "label": 0
                },
                {
                    "sent": "Given that we know the norm of the function in the arcade.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can actually now look at the rates of convergence of these different methods, assuming that their assumptions are true.",
                    "label": 0
                },
                {
                    "sent": "So here is 1 / sqrt N convergence of.",
                    "label": 0
                },
                {
                    "sent": "Monte Carlo and this is a log log plot by the way, so as a function of number examples.",
                    "label": 0
                },
                {
                    "sent": "Inserting has empirically this sort of 1 / N convergence rate, roughly.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we can ask, what if we took the hurting example locations and just re rated them according to the Asian Quadrature rules and we would see that.",
                    "label": 0
                },
                {
                    "sent": "Well, we get some slight increase in convergence rates.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if we choose the example locations and the weights according to inquire with this really super fast convergence, I don't know what the form of this convergence is.",
                    "label": 0
                },
                {
                    "sent": "It looks vaguely exponential.",
                    "label": 0
                },
                {
                    "sent": "I'm kind of hoping that someone here will be able to point me to some literature, because I bet this has been proven in the 70s.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyways.",
                    "label": 0
                },
                {
                    "sent": "Sanity check we can actually just draw functions from the arc HS that we're assuming and compute the empirical error rates, and you know they match.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also sort of do sensitivity analysis and an example functions outside of the RHS and just reassuringly the rough ordering in shape of the convergence plot stays with.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the coolest thing that you can do with this new result, I think, is that you can actually put this tight very quickly.",
                    "label": 0
                },
                {
                    "sent": "Decreasing bound on the error of the vision.",
                    "label": 0
                },
                {
                    "sent": "Quadrotor estimate.",
                    "label": 0
                },
                {
                    "sent": "I mean you have to know the Dark Ages and the norm of the function in the RHS.",
                    "label": 0
                },
                {
                    "sent": "I think bounds in general are like not that useful except for giving intuition on problems, but it's kind of refreshing that we can put a tight bound on the vision estimator, I think.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so just to summarize, this part of the talk the posterior variance of the integral under the prior is exactly equivalent to the squared maximum discrepancy.",
                    "label": 0
                },
                {
                    "sent": "And if we believe the RKS assumption, we have a tight clothes from upper bound on the visionaire.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Beijing culture has some very fast but unknown convergence rate.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the optimal weighted hurting strategy is Beijing quadrotor.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, and this this part of the talk was joint work.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parents hazard.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now that we've laid this groundwork, I'm going to 1st show a takeaway that I think.",
                    "label": 0
                },
                {
                    "sent": "Of how I think that this Bayesian interpretation of Colonel Huntington actually are quadrature can inform how we can improve kernel hurting.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to give Bayesian interpretations of mean embeddings.",
                    "label": 0
                },
                {
                    "sent": "The Kernel 2 sample tests, HSE and determined to point processes in terms of expectations on your GPS and also.",
                    "label": 0
                },
                {
                    "sent": "Give some hints about how we could maybe improve these methods.",
                    "label": 0
                },
                {
                    "sent": "Based on this new understanding.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the first thing I want to say is that the Gaussian process assumption and the RKS assumption are actually equivalent.",
                    "label": 0
                },
                {
                    "sent": "Now, like we can see this.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't think this is controversial.",
                    "label": 0
                },
                {
                    "sent": "But people often use kernel loading for inference.",
                    "label": 0
                },
                {
                    "sent": "The thing about that is that if you're using it for inference, you have to be.",
                    "label": 0
                },
                {
                    "sent": "Integrating against selected function and likelihood functions are kind of weird.",
                    "label": 0
                },
                {
                    "sent": "They don't really look like something you would draw from a GP.",
                    "label": 0
                },
                {
                    "sent": "They spent a lot of time being very close to zero and a small amount of time being very far away from zero.",
                    "label": 0
                },
                {
                    "sent": "And this sort of heavy tailed behavior is basically impossible to model with the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So here if we just evaluated the function 3 points and 50 posterior to it, we would get this.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distribution that basically thinks nothing is going on.",
                    "label": 0
                },
                {
                    "sent": "If we were to example in the mode somewhere, then basically the hyperparameter learning would increase our variance and shorter length scale, and we would basically have to be forever uncertain about everywhere else in the function until we sampled almost every point.",
                    "label": 0
                },
                {
                    "sent": "So it's basically no way for GP to sort of.",
                    "label": 0
                },
                {
                    "sent": "Tomato likelihood functions well, at least when they, when they peaked as they usually are once you.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit of data.",
                    "label": 0
                },
                {
                    "sent": "We can imagine like.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some ways of doing this?",
                    "label": 0
                },
                {
                    "sent": "We could for instance take the log of our likelihood function and assume that that's in it.",
                    "label": 0
                },
                {
                    "sent": "I modeled by GPR NRK chess.",
                    "label": 0
                },
                {
                    "sent": "And this turns out to be like.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And assumption.",
                    "label": 0
                },
                {
                    "sent": "Basically, you can say that no, the function is going to be close to 0 almost everywhere except some regions is going to be very far from there.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think that the takeaway from hurting is that the yes hernias are inappropriate for doing inference, and we can maybe imagine, like assuming that the log likelihood in RHS or some other related method.",
                    "label": 0
                },
                {
                    "sent": "But in general, I think that.",
                    "label": 0
                },
                {
                    "sent": "GP's are powerful because they let us check our assumptions by looking at the marginal likelihood and we can actually for any given function, compare how well different RHS is actually model.",
                    "label": 0
                },
                {
                    "sent": "The functions that we're looking at.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Now I'm just going to give a switch tracks and give an interpretation of a bunch of.",
                    "label": 0
                },
                {
                    "sent": "Sort of statistics in terms of Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "So the first one which was discovered by Crick this summer.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So is that the mean embedding of a distribution with respect to HS can actually be written as the covariance of a function at each point of the function with the integral of the function?",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Integrated with respect to P of XI mean this is just doing algebra, rearranging things so it's.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure that this is going to be like that much more powerful of of interpretation, but it's sort of starting point.",
                    "label": 0
                },
                {
                    "sent": "That's going to let us drive all the other results.",
                    "label": 0
                },
                {
                    "sent": "You can actually start from here or from the kernel.",
                    "label": 0
                },
                {
                    "sent": "Hurting equals Beijing quadrature result to drive all the next ones that I.",
                    "label": 0
                },
                {
                    "sent": "That I'm going to show.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the net result is has to do with the kernel through sample test, so the kernels out to sample test is used to determine whether two distributions are the same.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically, Arthur Gretton Company proposed using MMD between P&Q 2.",
                    "label": 0
                },
                {
                    "sent": "To basically measure the.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Extend two distributions are the same and so natural test statistic and we can actually say that just like very similar to the previous result, the MMD between two distributions is just the variance of a 0 mean GP integrated against.",
                    "label": 0
                },
                {
                    "sent": "P of X minus integrated against QX.",
                    "label": 0
                },
                {
                    "sent": "So we can sort of reinterpret the.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                },
                {
                    "sent": "The kernel to sample test is saying that we're measuring to what extent we expect the expectation of these two.",
                    "label": 0
                },
                {
                    "sent": "Distributions to be the same given functions drawn from GP.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "So the takeaway from this are a we can ask well, under what circumstances do we actually like what sort of functions we actually care about computing expectations of our populations with respect to?",
                    "label": 1
                },
                {
                    "sent": "Like you know, these might be medical outcomes, and this might be some like quality of life.",
                    "label": 0
                },
                {
                    "sent": "That's a function of.",
                    "label": 0
                },
                {
                    "sent": "If X, like X is blood pressure or something?",
                    "label": 0
                },
                {
                    "sent": "I mean it's kind of a stretch, but we can imagine actually thinking about functions that we care about and that informing our choice of kernels.",
                    "label": 0
                },
                {
                    "sent": "The other, more practical takeaway is that we can probably have some sort of approximate method of sampling GPS and using that to compute the MMD in some.",
                    "label": 0
                },
                {
                    "sent": "Quicker approximate way.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A very related result is the Hilbert Schmidt independence criterion, which is attempting to measure the extent or a test used to determine whether given some samples X&Y, whether the joint distribution actually factorize.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the interpretation from that was used to drive it is the infinite dimensional previous norm of a cross covariance matrix of features of X&Y and.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it has this.",
                    "label": 0
                },
                {
                    "sent": "Well, I would say somewhat nice form of just a bunch of expectations of kernel functions with respect to our input distribution P. Anne.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It turns out that we can rewrite this actually very simply as the expected expected difference between function to the functions drawn from GP against the joint distribution or against the product of the marginals.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "So again, I think to take away the same as other.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This method, but it's nice that such a method was that was driving such a roundabout way.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He has such a simple interpretation.",
                    "label": 0
                },
                {
                    "sent": "OK, and now here's like the most relevant one for today.",
                    "label": 0
                },
                {
                    "sent": "Is a really simple interpretation for determining the point process.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Or rather, at least for the greedy map way of adding points to a set under the DPP model.",
                    "label": 0
                },
                {
                    "sent": "So we can say that greedy map is going to map some Max.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the probability of our existing set plus a new point.",
                    "label": 0
                },
                {
                    "sent": "It's really not that hard to show that this determinant.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Has a new term which has this form.",
                    "label": 0
                },
                {
                    "sent": "And that term actually is exactly equivalent to the variance.",
                    "label": 0
                },
                {
                    "sent": "The marginal variance of function value.",
                    "label": 0
                },
                {
                    "sent": "Given the GP prior of the new proposed new point, given all the existing exam.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically, we're going to be asking which point are we most marginally uncertain about and going to add that to our set.",
                    "label": 0
                },
                {
                    "sent": "And now this is where it relates to these dispersion methods, because if we're using GPS to compute expectations later, this is actually not quite the right thing to do.",
                    "label": 0
                },
                {
                    "sent": "Probably you would actually want to use some sort of vision, quadrature or kernel hardening method that actually took into account the fact that it wanted to maximize the variance of integrals, not just the marginal variance of individual points.",
                    "label": 0
                },
                {
                    "sent": "Of course, this requires us to have like a measure over X, but I think that arises naturally in your.",
                    "label": 0
                },
                {
                    "sent": "And your problems it's not clear yet.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is also related to like sensor placement by Andreas Cross where he had like a similar metric adding the point that had the highest entropy in a GP.",
                    "label": 0
                },
                {
                    "sent": "I think that's equivalent here.",
                    "label": 0
                },
                {
                    "sent": "This is the result that I came up with yesterday so haven't thought about very much yet.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, thanks to this song and Roman for.",
                    "label": 0
                },
                {
                    "sent": "For this suggestion.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to summarize the connection.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I made so basically kernel loading, minimize the same objective as patient quadrature and then all the rest of these are actually equivalent to each other.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a few possible extensions that I've already mentioned that these sort of.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just one is we could use some sort of log kernel hurting for inference or some more appropriate assumptions.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other one is like obviously Next up.",
                    "label": 0
                },
                {
                    "sent": "I want to come up with an interpretation of conditional mean embeddings in terms of Goshen.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Processes.",
                    "label": 0
                },
                {
                    "sent": "The other thing that I wanted to point them to, and actually anyone who's doing these.",
                    "label": 0
                },
                {
                    "sent": "Conditional mean embeddings work is that I see often low rank approximations and in the Gaussian process literature there's a lot of different sparse GP methods that.",
                    "label": 0
                },
                {
                    "sent": "Have a lot of like have nice properties, especially for computing posterior variances in a nice way, so I think that if you're looking for better approximations you might just want to convert your problem into a GP and then use the existing sparsity methods.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, more generally, I've kind of noticed once I started reading like the UCL papers that there's a lot of correspondences between this sort of archaic community and this sort of like beige and numerics GP community.",
                    "label": 0
                },
                {
                    "sent": "And in fact we can say like a lot of these.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of corresponding model classes that are being solved in slightly different ways.",
                    "label": 0
                },
                {
                    "sent": "Currency progression, same model class.",
                    "label": 0
                },
                {
                    "sent": "Hardkernel learning turn out like same model classes.",
                    "label": 0
                },
                {
                    "sent": "This additive Gaussian processes thing Colonel Bayes rule is kind of similar to something that might cause more.",
                    "label": 0
                },
                {
                    "sent": "Did Beijing quadrature ratios where you try to put GPS on different functions.",
                    "label": 0
                },
                {
                    "sent": "The conditional distribution is used for dynamics.",
                    "label": 0
                },
                {
                    "sent": "It seems very similar to like all the model based reinforcement learning in the in the Asian community.",
                    "label": 0
                },
                {
                    "sent": "Colonel message passing, which Arthur claims is beige and is not big enough for me.",
                    "label": 0
                },
                {
                    "sent": "Me and Phillip and again, my customer had been trying to play around with the version of message passing that uses GPS, but it's hard to.",
                    "label": 0
                },
                {
                    "sent": "I mean to us, well, it's hard to come up with really compelling.",
                    "label": 0
                },
                {
                    "sent": "Oil based message passing algorithm passing algorithm that uses GPS so.",
                    "label": 0
                },
                {
                    "sent": "In general, I think that it's I feel like these communities haven't talked to each other very much and there's a lot of like duplicated work, and that might be a good thing because, you know, we can get twice as many papers, but.",
                    "label": 0
                },
                {
                    "sent": "I think the general weekend could be a lot closer.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}