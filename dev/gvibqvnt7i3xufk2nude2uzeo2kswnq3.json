{
    "id": "gvibqvnt7i3xufk2nude2uzeo2kswnq3",
    "title": "Characterizing and Understanding the Impact of Temporal Evolution on Document Classification",
    "info": {
        "author": [
            "Wagner Meira Jr., University of Rochester"
        ],
        "published": "Feb. 25, 2008",
        "recorded": "February 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Classification",
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/wsdm08_meira_jr_cuite/",
    "segmentation": [
        [
            "So good morning this is trying to work with Fernando Leonardo Renata Cherson in Marcus at the Federal University of Miniaturizing Press you."
        ],
        [
            "So our scenario here is basically like automatic automatic document classification has several applications in like not not just in digital libraries, but also in web in general.",
            "Such as topic, Deb tagging and identification of writing styles, digital libraries, web searching, and even more recent ones such as spam filtering, detection of adult content in pleasures.",
            "All main challenge here is like considering this collections in a long term range.",
            "These documents may change significantly overtime.",
            "Since new information is created, terms fields emerge and disappear, and and so on.",
            "One simple example of such a challenge is that Pluto was considered to be a planet since 4 till mid 2006, but it's not anymore.",
            "So in some sense the use of Pluto in astronomics astrophysics documents is supposed to disappear across the time while it's using mythology, we will remain.",
            "And and again, we may also be served.",
            "Observe that most of the current techniques for automatic document classification.",
            "They do not take into account this kind of temporary solution."
        ],
        [
            "So we in this work we try to address 2 temporary related issues.",
            "The first one, how does the Tampa Revolution affect the performance of classifiers and what are the temporary related characteristics that affect the classification of the same classifiers?",
            "It's it's quite intuitive that temporal evolution does those things like affect their performance in the quality of the classifiers, but we evaluate such impact in a more rigorous.",
            "Way that was that we consider three factors that may affect the quality of the classifiers.",
            "The one is the class distribution with so well known and well we studied in in the context of classifiers and we had two more factors that terms distribution and the class similar to talk about those three."
        ],
        [
            "So the class distribution.",
            "Is that the in terms of classifier?",
            "It comes as some classes are much more frequent than other classes and then this produces usually biased models which are not accurate, mainly for less represented classes.",
            "In our case the problem gets worse becaused this class distribution may vary across time and classes may appear disappeared in and even as a consequence of new of change in the class.",
            "In the classification scheme, one simple example is in the ACM Digital Library.",
            "Is that information retrieval and artificial intelligence used to be in the same class and as a new classification scheme was adopted, they they were assigned to two different classes?"
        ],
        [
            "The second temporal effect is the term distribution.",
            "It's again, it's well known that some terms they have a better discriminative power to some classes than orders, and it has been exploited in several tasks in search and document classification and so on.",
            "But again, these terms may appear, disappear, migrate or simple, lose or gain discriminative power.",
            "The Bluetooth case is a intuitive one."
        ],
        [
            "The third one is the class similarity.",
            "Also, is intuitive to say that if you have classes that are more similar with respect, for instance their terms, the terms that occur in those classes, it's more difficult to distinguish among them.",
            "OK again.",
            "This this simulator among classes may vary across time and.",
            "It will make the process of building a classifier even more difficult again.",
            "Or we may give a simple example, like if you talk about crime in biology in like 4030 years ago, they won't be related, although nowadays they are often related becausw of DNA analysis using being used as a legal."
        ],
        [
            "Everything's.",
            "So looking at all those examples and there's several in the three factors we just mentioned, we may say that we should somehow look to reduce our training samples towards generating more robust classifiers.",
            "OK, but in this case there is another, uh, there is a tradeoff that we should be be careful about.",
            "Is that the sampling?",
            "The sampling effect may also arise, so if we do source sample too much.",
            "It may happen that we have a good sample with respect to temporary effects, but not not necessarily our representative enough sample for building a classifier.",
            "So our challenge here or the challenge that we are starting to address here is to determine a training sample where those classification concepts are stable with respect to the temporal effects and the sample is large enough for building a classifier.",
            "So we fully large too much the sample, then we it may become confusing like this.",
            "Temporary effects may become prominent or otherwise, or the sample may be too small and not relevant enough, OK?"
        ],
        [
            "So yes, temporal effects and temporal related problems have been addressed in the literature.",
            "We may divide this effort into 4 broad categories that are just glance across them fastly.",
            "Adaptive document classification and put some examples there.",
            "Adaptive information field filtering, concept drift, and the last one is basically temporal.",
            "Deputation is in the sense that the three words that are more close to.",
            "Close to ours, in the sense that somehow they address it or exploited the temporal evolution, the temporal effects to improve the classification of the documents or or in other contexts."
        ],
        [
            "But analyzing all of them we can see at least three three contributions of our work.",
            "Considering those the first one is like considering those three factors simultaneously, forsake of classification and evaluation of the impact on the classifier's accuracy.",
            "The second thing is that none of them at least considered the problem of OK, I want to classify a document in its original context, so the document was produced in 1970.",
            "So what exactly was happening there there at that time?",
            "And also we performed at an in depth investigation of the impact of those temporal effects."
        ],
        [
            "So first I'm going to present here characterization of the temporal effects, and for that we are going to use 2 document collections.",
            "The ACM Digital first set of 30 K documents from the ACM Digital Library, ranging from 1980 to 2002, spread over eleven categories and for more than 4 million documents from the Medline from 1970 to 2006, spread over 7 seven categories.",
            "So we use it just the top categories in each case.",
            "And for both collections these this category schema did not change across during the period we considered as a as a technique that we use it for evaluating.",
            "The temporary effects was a support vector machine is available in the Internet to see SVM using the radial basis function kernel and they will use it as a black box.",
            "OK, and then we are evaluation strategy comprises two basic steps.",
            "The first one is we want to somehow factor out both sampling in overall temporal effects without looking individually at each of the three individual temporary effects, and then we characterize each of them individually."
        ],
        [
            "So the first thing we did was really to check how was the sampling effect in these collections.",
            "OK, So what we did is.",
            "We picked those collections and range this sampling from 20 to 100% of the collection and check it.",
            "What was the accuracy in a 10 fold cross validation using the SVM algorithm just mentioned we can see that for both collections there is a sampling effect there.",
            "So as we increase the size of the sample the accuracy improved."
        ],
        [
            "Then we.",
            "We based on this observation we do the following procedure for all the experiments that I'm going to present next.",
            "To avoid this sampling effect, we.",
            "First, we consider this the samples in a per year basis and also we equalize it the size of the samples for both collections.",
            "In our case that we are using, so the details are described in the paper.",
            "But basically we did some statistical measures to minimize the impact of the sampling effect.",
            "OK and that may be a contribution of the paper in the sense that we always pay attention to those effects while talking about temporal effects.",
            "So then we started.",
            "Access assessing the temporal effects.",
            "First of all, we'd like to are there really temporary sets, so we started with a simple experiment where we compared the following.",
            "We generated.",
            "We picked the documents for just one year and generated classifier for like using three fold cross validation classifier for that year in debt collection.",
            "And then we compared the accuracy between like the.",
            "If we use the third like the last third of the collection for death, classified to using that classifier or using other like a sample from the whole collection, let me explain it again.",
            "So we had like.",
            "For the year 1985 in ACM for install 1990, each year we build a classifier using the documents from 1990 and we use it both 4 classified documents from only 1990 and documents samples from the whole collection.",
            "The upper line is when we use documents for just the year from which we built the classifier.",
            "The lower line we use it like documents from the whole collection, so we see that.",
            "Up from same size samples when we do a more temporally localized it's use it's we achieve better results OK?",
            "In general."
        ],
        [
            "And then we we evolve it on these and save.",
            "So let's see what happens if we instead of looking at just the specific year from which we built the classifier.",
            "We check, like, uh, how do as a model would behave for neighbor hears like 1990 would be used to classify 1990, nineteen, 89 and 1991?",
            "And then we found those those.",
            "Interesting graphs here and and using this definition of time distance we we were able to really quantify the temporal locality of such collections in the following sense.",
            "So we define this time distance that's in the X axis in each graph, which means basically the distance between the year from which the classifier was built in the year of the document that was being classified.",
            "What we can see is that as we we.",
            "Like are more distant from the year of from which the document was trained.",
            "The classifier curacy went down.",
            "This is relative accuracy.",
            "I won't give the details now, but you can find in the paper.",
            "What you can see that that's the interesting part.",
            "It's obvious that when we try to classify documents in the future like we have a 1990 T classifier trying to classify documents at 2000, it's expected that to lose accuracy.",
            "OK, because new things happen.",
            "But the interesting thing is that when we try to classify things in the past, it also loses accuracy.",
            "OK, so as you see in this in both graphs we can also see that.",
            "There is as we get more distant, mostly for the ACM collection.",
            "It the variation in terms of the accuracy also gets bigger.",
            "OK, one last observation.",
            "What happened between the time distance 15 and 20 in ICM?",
            "Basically the sample became too small and then we're kind of affected by the sampling effect at that point."
        ],
        [
            "So the second temporal effect we characterizes the class distribution.",
            "As I mentioned, this is a class, not the seven.",
            "First, the first one, OK. As I mentioned, this is a classical effect and we can see that in both collections there is a significant class variation.",
            "Each graph shows the percentage of documents assigned to each class in each of the collections across the years, and by the variation of the colors, although probably can't read the graphs, the valuation of the colors, you can see that it varies quite significantly across time."
        ],
        [
            "We we did a simple experiment to show the impact of this classic version in the sense that we created samples that on purpose had more or less documents from from a given class and what we can see here is like as we change the percentage of recurrence offered given class on the test file.",
            "The accuracy also changed significantly and we observe this for all cases, so we confirmed that in this document collections.",
            "We do have the class distribution and classification changing across time would impact our classifier."
        ],
        [
            "The second thing we, the second of effect that we evaluated is term distribution.",
            "So we basically.",
            "Evaluated how the vocabulary for each class evolved across time.",
            "How we did that we picked for each year using our information gain criteria.",
            "The 5050 most valuable terms.",
            "Best terms for detecting each class, and we sum it like we made the union of those terms across the years and the results are shown in these graphs.",
            "So we have for the ACM collection for each class.",
            "The size, the number of distinct terms of those 50 most informative terms across the years and we can see that some classes are much more dynamic than other classes.",
            "Regarding terms, that's what we call terms distribution.",
            "OK, and the same apply to the Medline collection.",
            "It's interesting to note.",
            "See how the term how AIDS classes the first column in the right graph.",
            "It was really the highest in the sense of.",
            "It has a very large number of distinctive terms across the years.",
            "OK?"
        ],
        [
            "And the.",
            "We can go even further and evaluate how the time distance affects the this.",
            "This term distribution, and again we realize that the farther it is in time, the more the term distributions.",
            "Various OK, So what we can see here is that we picked each of these 50 terms and and for each class we created this vector and we calculated the cosine similarity between.",
            "Glasses and in different time distances and we use it the model since it's the same right, it has a. I've missed the war anyway is the same and then what you can see as the time distance grows we have less similarity, meaning that the classes are further apart in terms of the terms that compose them, OK?",
            "Right most graph we can see that the line that's quite different from the others is the age line that that's an effect of the last observation in the sense that since the age class has much more terms, also they very faster than in other classes.",
            "But in general we see that that's a phenomenon that affects most classes, or all classes."
        ],
        [
            "Finally, we checked the class similarity.",
            "And how we did that?",
            "We use it the same, the same vocabulary and and.",
            "We calculated in a pairwise basis for each pair of classes how much they were similar or not, and and then across the years we calculated the standard deviation of this similarity across the whole collection.",
            "What we can see here is that some classes, they their similarity, their pairwise similarity varied a lot, reaching almost 30% in the in the ACM library, or 20%.",
            "In the Medline library, which is clearly more stable than the ACM library, again, we can see that across the years the classical era changed a lot."
        ],
        [
            "OK.",
            "So given that in the sense that we were able to in both document collections, not only two.",
            "Quantify the impact of those effects and see that they really happen there.",
            "How can we improve the classifier's accuracy using such knowledge or?",
            "Baking or our strategy here, can we determine time windows in which both at that we have like some stability regarding temporary effects and there are large enough to avoid the sampling effect.",
            "So that's or basically basically hypothesis here OK?"
        ],
        [
            "The first thing we did is like using this notion of window size.",
            "So for each year we are trying to determine a window of years from which we may create a classifier and that will give a good precision.",
            "So is.",
            "Oh, OK. And then we can see here that, as expected, as we increase the window size, the the.",
            "The size of the training set also increase in the in a linear fit."
        ],
        [
            "Just to understand then we have this this.",
            "This this experiment here that's quite interesting, so let me explain that the graph is legal messy.",
            "The two lines that are the true steady lines are the accuracy and macro F1.",
            "When we built a classifier using the whole collection.",
            "And then we.",
            "We built a classifier for various window size an and for each year we used a single window size.",
            "What we can see here is that as we increase the window size above 5, four ACM and above two or three four Medline, we surpassed the classifiers that use it the whole collection.",
            "OK, so that's the first good evidence in the sense that like using more focus, temporarily focus the windows based off."
        ],
        [
            "Then we did a second experiment in the sense that, OK, so is there an optimal window size for each year?",
            "And the answer is yes, so we exhaustively check it for each year.",
            "What would be the best window size?",
            "And we can see that the summary of the results here, in the sense that they vary a lot.",
            "So we got from 20 to 8 or 9 for ACM and 24 two 922 Medline OK, and then we again performed the same experience.",
            "But instead of using a single window size for everything, we."
        ],
        [
            "Use it the best window size OK, and then in the in this case.",
            "We've improved even more or result in the sense that we both collections.",
            "We were very close to 90% and remain remember that.",
            "OK. We're like the original baselines are 70% of soul, and then we got 2 to 90% OK. Just a moment.",
            "I'm finishing.",
            "Oh, I'm going to the wrong direction, OK?"
        ],
        [
            "So.",
            "OK, So what are the conclusions we are able.",
            "We just present the characterization methodology of treating for effects that affect document classification.",
            "OK, and we're able to quantify those three effects and and given given a new strategy for such quantification, we also assess it the tradeoff between sampling temporary effects that should be factored out so that we produce good classifiers.",
            "And we demonstrated that exploiting the knowledge about temporal effects pays off.",
            "So we like.",
            "Actually, it almost 90% accuracy for ACM 80 almost 88% for Medline, which represent up to 20% gains.",
            "Using less training data compared to the whole collection stuff, OK?"
        ],
        [
            "So what are you doing nowadays?",
            "We are applying the same methodology to web collections where time is even more dynamic, right?",
            "So what's what's the time effect for news or the time effects for a different field that has documents in the web and we are also designing novel document classification algorithms that take into account temporary effects.",
            "For instance, we have under submission work where using a KNN classifier exploiting those temporal effects like that exposure we.",
            "Match it on SVM classifier.",
            "OK, so let's basically thank you very much for your attention.",
            "Are there any questions for Wagner?",
            "Yes.",
            "Hi, have you thought of using other collections like Wikipedia where you have like a very large page, for example instance page which contains a lot of temporal references to the past, but it's also a novel page.",
            "How you classify that not so far like we're starting?",
            "We have some some collections of pages that are starting working on them, but not now.",
            "The problem with web collections is we assume that we have a robust classification scheme.",
            "So this this gives us like we we tried news, but in some cases like news mixes with other like a politics mixed with sports.",
            "And this makes things much harder to really achieve good classification.",
            "Yeah, but they didn't try this collection.",
            "That may be a good idea.",
            "Other questions, yeah.",
            "Thank you yeah in machine learning community there has been recent interest in transfer learning.",
            "I don't know if you have looked at that area and see if your problem can be solved by some of the techniques there.",
            "Use a transfer learning.",
            "Yeah, I'm really not aware of check that work, thank you.",
            "One quick question on one of the diagrams before, when you were using classifier on one particular year and then you were classifying.",
            "Items from the past and from the future.",
            "So on the ACM site, the future.",
            "10 years ahead, I think then the classification accuracy went up again, having explanation for.",
            "This one no no."
        ],
        [
            "That one, yeah.",
            "So why so?",
            "Is this?",
            "This meat of computer science that everything repeats after 15 years?",
            "This is just a sampling effect sentiment sampling effect.",
            "So yeah, there there, there, there, there.",
            "So the sampling here is not good.",
            "We when we saw this is what's wrong here.",
            "So we will check it.",
            "And it was kind of a statistical accident.",
            "OK, not a great deal of karmic reinvention so."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So good morning this is trying to work with Fernando Leonardo Renata Cherson in Marcus at the Federal University of Miniaturizing Press you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our scenario here is basically like automatic automatic document classification has several applications in like not not just in digital libraries, but also in web in general.",
                    "label": 0
                },
                {
                    "sent": "Such as topic, Deb tagging and identification of writing styles, digital libraries, web searching, and even more recent ones such as spam filtering, detection of adult content in pleasures.",
                    "label": 1
                },
                {
                    "sent": "All main challenge here is like considering this collections in a long term range.",
                    "label": 1
                },
                {
                    "sent": "These documents may change significantly overtime.",
                    "label": 1
                },
                {
                    "sent": "Since new information is created, terms fields emerge and disappear, and and so on.",
                    "label": 0
                },
                {
                    "sent": "One simple example of such a challenge is that Pluto was considered to be a planet since 4 till mid 2006, but it's not anymore.",
                    "label": 0
                },
                {
                    "sent": "So in some sense the use of Pluto in astronomics astrophysics documents is supposed to disappear across the time while it's using mythology, we will remain.",
                    "label": 1
                },
                {
                    "sent": "And and again, we may also be served.",
                    "label": 1
                },
                {
                    "sent": "Observe that most of the current techniques for automatic document classification.",
                    "label": 0
                },
                {
                    "sent": "They do not take into account this kind of temporary solution.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we in this work we try to address 2 temporary related issues.",
                    "label": 0
                },
                {
                    "sent": "The first one, how does the Tampa Revolution affect the performance of classifiers and what are the temporary related characteristics that affect the classification of the same classifiers?",
                    "label": 0
                },
                {
                    "sent": "It's it's quite intuitive that temporal evolution does those things like affect their performance in the quality of the classifiers, but we evaluate such impact in a more rigorous.",
                    "label": 0
                },
                {
                    "sent": "Way that was that we consider three factors that may affect the quality of the classifiers.",
                    "label": 0
                },
                {
                    "sent": "The one is the class distribution with so well known and well we studied in in the context of classifiers and we had two more factors that terms distribution and the class similar to talk about those three.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the class distribution.",
                    "label": 0
                },
                {
                    "sent": "Is that the in terms of classifier?",
                    "label": 0
                },
                {
                    "sent": "It comes as some classes are much more frequent than other classes and then this produces usually biased models which are not accurate, mainly for less represented classes.",
                    "label": 0
                },
                {
                    "sent": "In our case the problem gets worse becaused this class distribution may vary across time and classes may appear disappeared in and even as a consequence of new of change in the class.",
                    "label": 1
                },
                {
                    "sent": "In the classification scheme, one simple example is in the ACM Digital Library.",
                    "label": 0
                },
                {
                    "sent": "Is that information retrieval and artificial intelligence used to be in the same class and as a new classification scheme was adopted, they they were assigned to two different classes?",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second temporal effect is the term distribution.",
                    "label": 1
                },
                {
                    "sent": "It's again, it's well known that some terms they have a better discriminative power to some classes than orders, and it has been exploited in several tasks in search and document classification and so on.",
                    "label": 0
                },
                {
                    "sent": "But again, these terms may appear, disappear, migrate or simple, lose or gain discriminative power.",
                    "label": 1
                },
                {
                    "sent": "The Bluetooth case is a intuitive one.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The third one is the class similarity.",
                    "label": 1
                },
                {
                    "sent": "Also, is intuitive to say that if you have classes that are more similar with respect, for instance their terms, the terms that occur in those classes, it's more difficult to distinguish among them.",
                    "label": 0
                },
                {
                    "sent": "OK again.",
                    "label": 0
                },
                {
                    "sent": "This this simulator among classes may vary across time and.",
                    "label": 1
                },
                {
                    "sent": "It will make the process of building a classifier even more difficult again.",
                    "label": 1
                },
                {
                    "sent": "Or we may give a simple example, like if you talk about crime in biology in like 4030 years ago, they won't be related, although nowadays they are often related becausw of DNA analysis using being used as a legal.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Everything's.",
                    "label": 0
                },
                {
                    "sent": "So looking at all those examples and there's several in the three factors we just mentioned, we may say that we should somehow look to reduce our training samples towards generating more robust classifiers.",
                    "label": 0
                },
                {
                    "sent": "OK, but in this case there is another, uh, there is a tradeoff that we should be be careful about.",
                    "label": 0
                },
                {
                    "sent": "Is that the sampling?",
                    "label": 0
                },
                {
                    "sent": "The sampling effect may also arise, so if we do source sample too much.",
                    "label": 1
                },
                {
                    "sent": "It may happen that we have a good sample with respect to temporary effects, but not not necessarily our representative enough sample for building a classifier.",
                    "label": 0
                },
                {
                    "sent": "So our challenge here or the challenge that we are starting to address here is to determine a training sample where those classification concepts are stable with respect to the temporal effects and the sample is large enough for building a classifier.",
                    "label": 1
                },
                {
                    "sent": "So we fully large too much the sample, then we it may become confusing like this.",
                    "label": 0
                },
                {
                    "sent": "Temporary effects may become prominent or otherwise, or the sample may be too small and not relevant enough, OK?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yes, temporal effects and temporal related problems have been addressed in the literature.",
                    "label": 0
                },
                {
                    "sent": "We may divide this effort into 4 broad categories that are just glance across them fastly.",
                    "label": 1
                },
                {
                    "sent": "Adaptive document classification and put some examples there.",
                    "label": 1
                },
                {
                    "sent": "Adaptive information field filtering, concept drift, and the last one is basically temporal.",
                    "label": 0
                },
                {
                    "sent": "Deputation is in the sense that the three words that are more close to.",
                    "label": 0
                },
                {
                    "sent": "Close to ours, in the sense that somehow they address it or exploited the temporal evolution, the temporal effects to improve the classification of the documents or or in other contexts.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But analyzing all of them we can see at least three three contributions of our work.",
                    "label": 0
                },
                {
                    "sent": "Considering those the first one is like considering those three factors simultaneously, forsake of classification and evaluation of the impact on the classifier's accuracy.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that none of them at least considered the problem of OK, I want to classify a document in its original context, so the document was produced in 1970.",
                    "label": 1
                },
                {
                    "sent": "So what exactly was happening there there at that time?",
                    "label": 1
                },
                {
                    "sent": "And also we performed at an in depth investigation of the impact of those temporal effects.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first I'm going to present here characterization of the temporal effects, and for that we are going to use 2 document collections.",
                    "label": 1
                },
                {
                    "sent": "The ACM Digital first set of 30 K documents from the ACM Digital Library, ranging from 1980 to 2002, spread over eleven categories and for more than 4 million documents from the Medline from 1970 to 2006, spread over 7 seven categories.",
                    "label": 1
                },
                {
                    "sent": "So we use it just the top categories in each case.",
                    "label": 0
                },
                {
                    "sent": "And for both collections these this category schema did not change across during the period we considered as a as a technique that we use it for evaluating.",
                    "label": 1
                },
                {
                    "sent": "The temporary effects was a support vector machine is available in the Internet to see SVM using the radial basis function kernel and they will use it as a black box.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we are evaluation strategy comprises two basic steps.",
                    "label": 0
                },
                {
                    "sent": "The first one is we want to somehow factor out both sampling in overall temporal effects without looking individually at each of the three individual temporary effects, and then we characterize each of them individually.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first thing we did was really to check how was the sampling effect in these collections.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we did is.",
                    "label": 0
                },
                {
                    "sent": "We picked those collections and range this sampling from 20 to 100% of the collection and check it.",
                    "label": 0
                },
                {
                    "sent": "What was the accuracy in a 10 fold cross validation using the SVM algorithm just mentioned we can see that for both collections there is a sampling effect there.",
                    "label": 0
                },
                {
                    "sent": "So as we increase the size of the sample the accuracy improved.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we.",
                    "label": 0
                },
                {
                    "sent": "We based on this observation we do the following procedure for all the experiments that I'm going to present next.",
                    "label": 0
                },
                {
                    "sent": "To avoid this sampling effect, we.",
                    "label": 0
                },
                {
                    "sent": "First, we consider this the samples in a per year basis and also we equalize it the size of the samples for both collections.",
                    "label": 0
                },
                {
                    "sent": "In our case that we are using, so the details are described in the paper.",
                    "label": 0
                },
                {
                    "sent": "But basically we did some statistical measures to minimize the impact of the sampling effect.",
                    "label": 0
                },
                {
                    "sent": "OK and that may be a contribution of the paper in the sense that we always pay attention to those effects while talking about temporal effects.",
                    "label": 1
                },
                {
                    "sent": "So then we started.",
                    "label": 0
                },
                {
                    "sent": "Access assessing the temporal effects.",
                    "label": 1
                },
                {
                    "sent": "First of all, we'd like to are there really temporary sets, so we started with a simple experiment where we compared the following.",
                    "label": 0
                },
                {
                    "sent": "We generated.",
                    "label": 0
                },
                {
                    "sent": "We picked the documents for just one year and generated classifier for like using three fold cross validation classifier for that year in debt collection.",
                    "label": 0
                },
                {
                    "sent": "And then we compared the accuracy between like the.",
                    "label": 0
                },
                {
                    "sent": "If we use the third like the last third of the collection for death, classified to using that classifier or using other like a sample from the whole collection, let me explain it again.",
                    "label": 0
                },
                {
                    "sent": "So we had like.",
                    "label": 0
                },
                {
                    "sent": "For the year 1985 in ACM for install 1990, each year we build a classifier using the documents from 1990 and we use it both 4 classified documents from only 1990 and documents samples from the whole collection.",
                    "label": 0
                },
                {
                    "sent": "The upper line is when we use documents for just the year from which we built the classifier.",
                    "label": 0
                },
                {
                    "sent": "The lower line we use it like documents from the whole collection, so we see that.",
                    "label": 0
                },
                {
                    "sent": "Up from same size samples when we do a more temporally localized it's use it's we achieve better results OK?",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we we evolve it on these and save.",
                    "label": 0
                },
                {
                    "sent": "So let's see what happens if we instead of looking at just the specific year from which we built the classifier.",
                    "label": 0
                },
                {
                    "sent": "We check, like, uh, how do as a model would behave for neighbor hears like 1990 would be used to classify 1990, nineteen, 89 and 1991?",
                    "label": 0
                },
                {
                    "sent": "And then we found those those.",
                    "label": 0
                },
                {
                    "sent": "Interesting graphs here and and using this definition of time distance we we were able to really quantify the temporal locality of such collections in the following sense.",
                    "label": 1
                },
                {
                    "sent": "So we define this time distance that's in the X axis in each graph, which means basically the distance between the year from which the classifier was built in the year of the document that was being classified.",
                    "label": 0
                },
                {
                    "sent": "What we can see is that as we we.",
                    "label": 0
                },
                {
                    "sent": "Like are more distant from the year of from which the document was trained.",
                    "label": 0
                },
                {
                    "sent": "The classifier curacy went down.",
                    "label": 0
                },
                {
                    "sent": "This is relative accuracy.",
                    "label": 0
                },
                {
                    "sent": "I won't give the details now, but you can find in the paper.",
                    "label": 0
                },
                {
                    "sent": "What you can see that that's the interesting part.",
                    "label": 0
                },
                {
                    "sent": "It's obvious that when we try to classify documents in the future like we have a 1990 T classifier trying to classify documents at 2000, it's expected that to lose accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK, because new things happen.",
                    "label": 0
                },
                {
                    "sent": "But the interesting thing is that when we try to classify things in the past, it also loses accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK, so as you see in this in both graphs we can also see that.",
                    "label": 1
                },
                {
                    "sent": "There is as we get more distant, mostly for the ACM collection.",
                    "label": 0
                },
                {
                    "sent": "It the variation in terms of the accuracy also gets bigger.",
                    "label": 0
                },
                {
                    "sent": "OK, one last observation.",
                    "label": 1
                },
                {
                    "sent": "What happened between the time distance 15 and 20 in ICM?",
                    "label": 0
                },
                {
                    "sent": "Basically the sample became too small and then we're kind of affected by the sampling effect at that point.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the second temporal effect we characterizes the class distribution.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, this is a class, not the seven.",
                    "label": 0
                },
                {
                    "sent": "First, the first one, OK. As I mentioned, this is a classical effect and we can see that in both collections there is a significant class variation.",
                    "label": 0
                },
                {
                    "sent": "Each graph shows the percentage of documents assigned to each class in each of the collections across the years, and by the variation of the colors, although probably can't read the graphs, the valuation of the colors, you can see that it varies quite significantly across time.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We we did a simple experiment to show the impact of this classic version in the sense that we created samples that on purpose had more or less documents from from a given class and what we can see here is like as we change the percentage of recurrence offered given class on the test file.",
                    "label": 0
                },
                {
                    "sent": "The accuracy also changed significantly and we observe this for all cases, so we confirmed that in this document collections.",
                    "label": 0
                },
                {
                    "sent": "We do have the class distribution and classification changing across time would impact our classifier.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second thing we, the second of effect that we evaluated is term distribution.",
                    "label": 0
                },
                {
                    "sent": "So we basically.",
                    "label": 0
                },
                {
                    "sent": "Evaluated how the vocabulary for each class evolved across time.",
                    "label": 0
                },
                {
                    "sent": "How we did that we picked for each year using our information gain criteria.",
                    "label": 0
                },
                {
                    "sent": "The 5050 most valuable terms.",
                    "label": 0
                },
                {
                    "sent": "Best terms for detecting each class, and we sum it like we made the union of those terms across the years and the results are shown in these graphs.",
                    "label": 0
                },
                {
                    "sent": "So we have for the ACM collection for each class.",
                    "label": 0
                },
                {
                    "sent": "The size, the number of distinct terms of those 50 most informative terms across the years and we can see that some classes are much more dynamic than other classes.",
                    "label": 0
                },
                {
                    "sent": "Regarding terms, that's what we call terms distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, and the same apply to the Medline collection.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to note.",
                    "label": 0
                },
                {
                    "sent": "See how the term how AIDS classes the first column in the right graph.",
                    "label": 0
                },
                {
                    "sent": "It was really the highest in the sense of.",
                    "label": 0
                },
                {
                    "sent": "It has a very large number of distinctive terms across the years.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "We can go even further and evaluate how the time distance affects the this.",
                    "label": 1
                },
                {
                    "sent": "This term distribution, and again we realize that the farther it is in time, the more the term distributions.",
                    "label": 0
                },
                {
                    "sent": "Various OK, So what we can see here is that we picked each of these 50 terms and and for each class we created this vector and we calculated the cosine similarity between.",
                    "label": 1
                },
                {
                    "sent": "Glasses and in different time distances and we use it the model since it's the same right, it has a. I've missed the war anyway is the same and then what you can see as the time distance grows we have less similarity, meaning that the classes are further apart in terms of the terms that compose them, OK?",
                    "label": 0
                },
                {
                    "sent": "Right most graph we can see that the line that's quite different from the others is the age line that that's an effect of the last observation in the sense that since the age class has much more terms, also they very faster than in other classes.",
                    "label": 0
                },
                {
                    "sent": "But in general we see that that's a phenomenon that affects most classes, or all classes.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we checked the class similarity.",
                    "label": 0
                },
                {
                    "sent": "And how we did that?",
                    "label": 0
                },
                {
                    "sent": "We use it the same, the same vocabulary and and.",
                    "label": 0
                },
                {
                    "sent": "We calculated in a pairwise basis for each pair of classes how much they were similar or not, and and then across the years we calculated the standard deviation of this similarity across the whole collection.",
                    "label": 0
                },
                {
                    "sent": "What we can see here is that some classes, they their similarity, their pairwise similarity varied a lot, reaching almost 30% in the in the ACM library, or 20%.",
                    "label": 0
                },
                {
                    "sent": "In the Medline library, which is clearly more stable than the ACM library, again, we can see that across the years the classical era changed a lot.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So given that in the sense that we were able to in both document collections, not only two.",
                    "label": 0
                },
                {
                    "sent": "Quantify the impact of those effects and see that they really happen there.",
                    "label": 0
                },
                {
                    "sent": "How can we improve the classifier's accuracy using such knowledge or?",
                    "label": 1
                },
                {
                    "sent": "Baking or our strategy here, can we determine time windows in which both at that we have like some stability regarding temporary effects and there are large enough to avoid the sampling effect.",
                    "label": 1
                },
                {
                    "sent": "So that's or basically basically hypothesis here OK?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first thing we did is like using this notion of window size.",
                    "label": 0
                },
                {
                    "sent": "So for each year we are trying to determine a window of years from which we may create a classifier and that will give a good precision.",
                    "label": 0
                },
                {
                    "sent": "So is.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK. And then we can see here that, as expected, as we increase the window size, the the.",
                    "label": 0
                },
                {
                    "sent": "The size of the training set also increase in the in a linear fit.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to understand then we have this this.",
                    "label": 0
                },
                {
                    "sent": "This this experiment here that's quite interesting, so let me explain that the graph is legal messy.",
                    "label": 0
                },
                {
                    "sent": "The two lines that are the true steady lines are the accuracy and macro F1.",
                    "label": 1
                },
                {
                    "sent": "When we built a classifier using the whole collection.",
                    "label": 0
                },
                {
                    "sent": "And then we.",
                    "label": 0
                },
                {
                    "sent": "We built a classifier for various window size an and for each year we used a single window size.",
                    "label": 1
                },
                {
                    "sent": "What we can see here is that as we increase the window size above 5, four ACM and above two or three four Medline, we surpassed the classifiers that use it the whole collection.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the first good evidence in the sense that like using more focus, temporarily focus the windows based off.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we did a second experiment in the sense that, OK, so is there an optimal window size for each year?",
                    "label": 1
                },
                {
                    "sent": "And the answer is yes, so we exhaustively check it for each year.",
                    "label": 0
                },
                {
                    "sent": "What would be the best window size?",
                    "label": 1
                },
                {
                    "sent": "And we can see that the summary of the results here, in the sense that they vary a lot.",
                    "label": 0
                },
                {
                    "sent": "So we got from 20 to 8 or 9 for ACM and 24 two 922 Medline OK, and then we again performed the same experience.",
                    "label": 1
                },
                {
                    "sent": "But instead of using a single window size for everything, we.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use it the best window size OK, and then in the in this case.",
                    "label": 0
                },
                {
                    "sent": "We've improved even more or result in the sense that we both collections.",
                    "label": 0
                },
                {
                    "sent": "We were very close to 90% and remain remember that.",
                    "label": 0
                },
                {
                    "sent": "OK. We're like the original baselines are 70% of soul, and then we got 2 to 90% OK. Just a moment.",
                    "label": 0
                },
                {
                    "sent": "I'm finishing.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm going to the wrong direction, OK?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are the conclusions we are able.",
                    "label": 0
                },
                {
                    "sent": "We just present the characterization methodology of treating for effects that affect document classification.",
                    "label": 1
                },
                {
                    "sent": "OK, and we're able to quantify those three effects and and given given a new strategy for such quantification, we also assess it the tradeoff between sampling temporary effects that should be factored out so that we produce good classifiers.",
                    "label": 0
                },
                {
                    "sent": "And we demonstrated that exploiting the knowledge about temporal effects pays off.",
                    "label": 1
                },
                {
                    "sent": "So we like.",
                    "label": 0
                },
                {
                    "sent": "Actually, it almost 90% accuracy for ACM 80 almost 88% for Medline, which represent up to 20% gains.",
                    "label": 0
                },
                {
                    "sent": "Using less training data compared to the whole collection stuff, OK?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what are you doing nowadays?",
                    "label": 0
                },
                {
                    "sent": "We are applying the same methodology to web collections where time is even more dynamic, right?",
                    "label": 0
                },
                {
                    "sent": "So what's what's the time effect for news or the time effects for a different field that has documents in the web and we are also designing novel document classification algorithms that take into account temporary effects.",
                    "label": 0
                },
                {
                    "sent": "For instance, we have under submission work where using a KNN classifier exploiting those temporal effects like that exposure we.",
                    "label": 0
                },
                {
                    "sent": "Match it on SVM classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's basically thank you very much for your attention.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions for Wagner?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Hi, have you thought of using other collections like Wikipedia where you have like a very large page, for example instance page which contains a lot of temporal references to the past, but it's also a novel page.",
                    "label": 0
                },
                {
                    "sent": "How you classify that not so far like we're starting?",
                    "label": 0
                },
                {
                    "sent": "We have some some collections of pages that are starting working on them, but not now.",
                    "label": 0
                },
                {
                    "sent": "The problem with web collections is we assume that we have a robust classification scheme.",
                    "label": 0
                },
                {
                    "sent": "So this this gives us like we we tried news, but in some cases like news mixes with other like a politics mixed with sports.",
                    "label": 0
                },
                {
                    "sent": "And this makes things much harder to really achieve good classification.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but they didn't try this collection.",
                    "label": 0
                },
                {
                    "sent": "That may be a good idea.",
                    "label": 0
                },
                {
                    "sent": "Other questions, yeah.",
                    "label": 0
                },
                {
                    "sent": "Thank you yeah in machine learning community there has been recent interest in transfer learning.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you have looked at that area and see if your problem can be solved by some of the techniques there.",
                    "label": 0
                },
                {
                    "sent": "Use a transfer learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm really not aware of check that work, thank you.",
                    "label": 0
                },
                {
                    "sent": "One quick question on one of the diagrams before, when you were using classifier on one particular year and then you were classifying.",
                    "label": 0
                },
                {
                    "sent": "Items from the past and from the future.",
                    "label": 0
                },
                {
                    "sent": "So on the ACM site, the future.",
                    "label": 0
                },
                {
                    "sent": "10 years ahead, I think then the classification accuracy went up again, having explanation for.",
                    "label": 0
                },
                {
                    "sent": "This one no no.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That one, yeah.",
                    "label": 0
                },
                {
                    "sent": "So why so?",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "This meat of computer science that everything repeats after 15 years?",
                    "label": 0
                },
                {
                    "sent": "This is just a sampling effect sentiment sampling effect.",
                    "label": 0
                },
                {
                    "sent": "So yeah, there there, there, there, there.",
                    "label": 0
                },
                {
                    "sent": "So the sampling here is not good.",
                    "label": 0
                },
                {
                    "sent": "We when we saw this is what's wrong here.",
                    "label": 0
                },
                {
                    "sent": "So we will check it.",
                    "label": 0
                },
                {
                    "sent": "And it was kind of a statistical accident.",
                    "label": 0
                },
                {
                    "sent": "OK, not a great deal of karmic reinvention so.",
                    "label": 0
                }
            ]
        }
    }
}