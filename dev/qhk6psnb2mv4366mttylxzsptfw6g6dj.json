{
    "id": "qhk6psnb2mv4366mttylxzsptfw6g6dj",
    "title": "ShareBoost: Boosting for Multi-View Learning with Performance Guarantees",
    "info": {
        "author": [
            "Kannappan Palaniappan, Department of Computer Science, University of Missouri"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Boosting"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_palaniappan_shareboost/",
    "segmentation": [
        [
            "My name is Kay Palaniappan and I'm going to be talking about randomized shared boosting, which is follows very much in the flavor of the previous talks.",
            "So a lot of the sort of background I can kind of maybe speed up a little bit, and it's again a related to boosting with multiple views.",
            "And what we show is certain performance guarantees between the shared boosting, which is some previous work an randomized boosting.",
            "So the other coauthors are jingping from Montclair State University.",
            "Cost in barboo at MIT Lincoln Laboratory gonna see the Roman at the Air Force Research lab.",
            "Why fan and shampoo at IBM research?",
            "This was sort of a big collaboration.",
            "So the again."
        ],
        [
            "Innovation is similar to the previous talk when in terms of there are many problems including image, image analysis and computer vision where you want to get information from multiple sources.",
            "There could be sensors that have different measurements or measurement capabilities and you want to be able to exploit those that are more robust.",
            "You know some sensors may be more robust to noise than others, so here's a simple case where you have two views so that.",
            "There are the projected the data on the upper left as one of the views, so this is 1 view for example where the where obviously we cannot separate the two classes very easily.",
            "Here's another view again where it's a poor discrimination, but if we fuse the two views then we'll get hopefully Third view which are refused view.",
            "That gives a better separation between the classes.",
            "So here we want to find the right combination for the source of information that we're getting from different views."
        ],
        [
            "Applications are in multisensor data Fusion, where often we have many different sources of data acquisition.",
            "In this case, infrared or synthetic aperture, radar or Fleur, or many other sensors, and also biometrics, where you have a wide variety of sensors from iris data and fingerprints and audio, and here and so forth.",
            "So these are just too simple.",
            "Two categories broad categories of example."
        ],
        [
            "So in this case the shared boost algorithm, so I'm just going to kind of highlight it with, which has been previously presented, says that we can combine simple classifiers to generate a more powerful one.",
            "In this case, is the X or problem where we can't separate the the two classes easily with a single classifier, but with three classifiers we can do a pretty good job.",
            "Now so if we put this."
        ],
        [
            "Same framework in and we want to compare different types of boosting algorithms, so I'll kind of compare Adaboost to how shared boost works and then the randomized version.",
            "So this is to make kind of set the framework consistent in common framework.",
            "Here we have examples here, and these are the weights for each example and along the horizontal axis is sort of time or the number of boosting iterations.",
            "So after each iteration usually we're going to take the negative classes or the samples that are misclassified and assign greater weight to them so that they will be in the next iteration.",
            "The classifier will work harder to classify these negative samples, so that's typically how.",
            "Ada Boost works at each iteration.",
            "We're trying to improve the classification of the negative samples and so this is to show kind of different trees and what is.",
            "Why is the sizes of the trees different?",
            "Is sort of this is proportional to our kind of weight that we assign to this tree.",
            "So this tree didn't do that good of a job in the number of examples it classified correctly, so therefore it has a kind of a smaller wait.",
            "This one did a better job at classify two example.",
            "Example, so it has a higher weight and so forth.",
            "So after we've done these multiple rounds of iteration, then we could combine all of the trees together and that's what this classifier here is.",
            "This is the combined classifier where we have the weights assigned as a result from each round of boosting and we combine them all together.",
            "And hopefully that gives a better."
        ],
        [
            "Salt.",
            "The IF we have multiple views so that was for one view.",
            "So with multiple views you could generalize it in a straightforward way where you do boosting for each view in dependently.",
            "So we call that independent Adaboost and in this case this is for view one and we get a set of classifiers are a combined classifier here, and in this case we have you 2 and we get another classifier.",
            "So in this case we're not.",
            "We're not kind of sharing information.",
            "Between the views when we do boosting independently.",
            "And the weights are associated with each examples computed independently for each view.",
            "So in this case we would have four classifiers from each view, and so there would be a total of essentially 8 classifiers in the final combined fused result."
        ],
        [
            "With what we've shared boosting, it was sort of similar in spirit to the previous talk, which says, well, let's share the result.",
            "So if we make a mistake in one view, so safer in this case, for example, we've made a mistake and view one in classifying.",
            "So the classifier that was picked using View 1 using that sensor, that modality that feature set misclassified three of them, including the first one and the last one.",
            "But view 2 classifieds.",
            "The first one and the last one correctly but misclassified the second one.",
            "So it seems that we should share these views such that we give we pick the better weight from here, right?",
            "So we say, OK, the better view in this case is view too.",
            "So we're going to take the weight from view 2 as our classifier OK, and so we're going to get our tree from view 2 and our weight from view 2.",
            "And likewise, at each round of boosting, we look at the interplay between the two views and whichever view is doing a better job in classifying.",
            "That's the view that dominates for that iteration.",
            "OK, and so this is done for as many rounds as we want.",
            "And of course, the number of choices that we have will depend upon the number of views.",
            "So the final classifier is assembled from the base classifiers that provide the best classification in each round.",
            "In this case, shared boost requires fewer base classifiers and is less complex.",
            "So here we only have 4 rather than 8, so that the spirit or the hope is that if you have that, there's more generalization capability in the final classifier that results.",
            "And as we as we do these rounds of iteration of boosting, we're sort of hopefully the better view is is being used in each time, and if there are some view that dominates, then it will get used more frequently.",
            "OK, and the shared boosting you can see is what I call a greedy approach.",
            "Right, so suppose we have 10 features, 10 views.",
            "We're always looking for the best view, so it's a greedy approach.",
            "It's going to take the best view each time.",
            "OK, and so this kind of seems to make intuitive sense.",
            "Say, OK, we'll take the best view.",
            "Use that as a classifier and combine it with all the others.",
            "In further rounds.",
            "But then why don't we ask?",
            "The question, is that optimal?",
            "What happens if I take the median of this set of results?",
            "Or some other choice?",
            "Some other rank order statistic?",
            "Well."
        ],
        [
            "Instead of any statistic, why not pick the view randomly?",
            "Can we prove something about that?",
            "If you pick a view, if we pick the view not by a greedy choice of whichever one minimize the error, but just randomly?",
            "Can we show that that will converge to the same result as the shared boosting with sufficient time OK?",
            "And that's what that's what indeed we showed in this paper, so shared boosting is robust against noise, and in this case noise means the class label noise.",
            "As a result of the shared waiting mechanism, and but it's, but it has high complexity and it's proportional to so M as I said, is the number of views.",
            "So we have to perform the boosting for each view, tease the number of iterations.",
            "That was sort of our horizontal axis, so this is going to be proportional to the number of iterations and boosting Q is the number of dimensions and end is the number of training samples, so it's going to be M * T and then.",
            "Complexity proportional to the maximum of queue or log in either the either dominated by the number of training samples of the number of dimensions, times N log N kind of just building the classifier cost.",
            "So.",
            "So this is so we wanted to reduce this cost.",
            "That's why we kind of explored this idea.",
            "Well, if we just if we didn't force ourselves to evaluate every single view at each round of boosting, that would obviously reduce our cost.",
            "And so in this case we randomly pick the view.",
            "At each round, and therefore we avoid this cost M. OK, so at each iteration we randomly pick a view and that's the assigned.",
            "You know that's the view that gets picked for that round of boosting and we continue on."
        ],
        [
            "And with with.",
            "The result is that it does converge to the same.",
            "The same result as shared boosting, so randomized shared shared boost converges to the same result as shared boost.",
            "And the proof I'll leave to the paper.",
            "I won't go into the details, but it follows the sort of the multi arm bandit approach where you have you have a slot machine with multiple arms, an you pull each arm and that kind of gives you a risk or reward associated with that particular step time step.",
            "So in this case this training error is the T poles of the arm.",
            "And what we want to do is we want to minimize this training error or maximize the reward function and it says that over the M views we can find for any any such distribution over the training data, the base learners learn a classifier visa plus with edge with edge strengths, beta times V ISA plus that's greater than RO.",
            "Then, with probability at least one minus Delta, the training error of randomized share boost will become zero in polynomial time and the."
        ],
        [
            "So how did we did we sort of assign the cost or the risk that gets evaluated in this proof is we need some sort of measure performance and what we chose was this notion of regret.",
            "So what regret says is.",
            "The best choice I could have made is gmax.",
            "T. At that iteration, so at time step T, the best view produces this error value.",
            "Gmax, OK, and the one I actually chose was a algorithm.",
            "A or view a.",
            "So this is our regret or kind of our difference between our best choice that we could have made OK. And So what we want to do is we want to we want to minimize our regret, right?",
            "We want to minimize our regret and so this is the formulation, that of minimizing our total regret.",
            "And this is builds upon some previous work by Freundin Shapir in 2002."
        ],
        [
            "So with a simple illustration from."
        ],
        [
            "Classic Iris data set.",
            "So here we have two views, the sepal width and height and the petal width and height.",
            "These are two classes in black and red and what we're going to do is we're going to assign noise to a subset of the samples, so in this case are noisy samples are shown."
        ],
        [
            "In there, either there are shown in this in this blue boxes on either class label right on either the the class One or the black classes, or the red classes.",
            "Red class sample.",
            "So class one or Class 2 an so there these are noisy labels and we want to see overtime which of the noisy samples what their weight is over the boosting iterations.",
            "OK, so we start out with equal weights and there are different errors in different."
        ],
        [
            "News.",
            "And this is now view 2, so the previous one was in view one that's used in.",
            "Now we look at view 2, and in this case this is shared, boosting on the left and Ada boost on the right.",
            "Shared Boost decides that it's going to try an fix.",
            "Sample 14 the most so it gives the highest weight to those and a lot of the other miss noisy samples are not adversely affected, right?",
            "So we're not trying to.",
            "We're not.",
            "We're not increasing the weight on the noisy samples and moving our our decision plane or our classifier adversely towards the noisy labels.",
            "Because we want to kind of ignore the noisy labels, right?",
            "That's the goal.",
            "With Ada boost, it kind of it's sort of more evenly distributes the weights that it's across the noisy labels where it's going to try and improve the.",
            "The classifier result.",
            "Now this is iteration."
        ],
        [
            "Three an view one.",
            "And here were we have a different decision.",
            "Plain different set of misclassified examples and the noisy labels are still labeled are still showing the same way here and now we can see shared boost has more of the noisy labels that are getting higher weights and this looks about the same between Adaboost and shared boost on this particular view."
        ],
        [
            "But on this view it's a little bit better.",
            "I think in terms of the total number.",
            "And this is."
        ],
        [
            "Iteration 4 and now here.",
            "This is iteration 5, view 2, where we're have again a much, much less emphasis on the noisy samples in shared boost compared to Ada boost.",
            "So that's sort of the."
        ],
        [
            "What we wanted to show and then now with some actual examples from some machine learning datasets, we have four different datasets.",
            "The face data set, gender, glass and microarray gene expression data an we compared eight different algorithms, including share boost, random share, boost.",
            "These are the first 2 and kind of dark green, then independent or independent ADA boost.",
            "The one I explained that.",
            "3rd one semidefinite programming, another one that's common in the literature Adaboost, with majority Vote Adaboost with concatenated space.",
            "So that means we just concatenate all the views together and then.",
            "Stacking an as the as the last one, and so.",
            "In general, if there is no noise, right?",
            "So in the noise free case, shared boost, randomize, shared boost and independent boosting.",
            "Perform similarly for majority of the cases, except maybe in the gene one where shared boost and randomized and random share boosting perform a little bit better in the noise free case, there's not much difference between share boost and random share boost.",
            "And with the with the noisy case.",
            "So this is with 30% noise.",
            "What we showed is that the random shared boosting.",
            "And the shared boosting also have the same performance, which is good right?",
            "So we want to show we can reach the same performance as shared boost, but without the cost.",
            "So we maintain the same performance except now with noise there is.",
            "In these, in the first three datasets, it's significantly better than Adaboost independent data boost, but not as big difference in the gene microarray data."
        ],
        [
            "And here we've we're showing the.",
            "The performance over sort of iterations, so we have 150 based classifier.",
            "So in the horizontal axis is sort of our our iterations right?",
            "We're building classifiers up to 150 and we're showing the rate of convergence.",
            "Because this is an asymptotic convergence and we're showing how fast the randomized boosting which is in red is converging to the shared boost result in blue.",
            "This is for the face data."
        ],
        [
            "This is for the gender data, so the more kind of well behaved the data is, the faster the convergence."
        ],
        [
            "This is the glass data.",
            "Here we got pretty similar results, but we you can see that convergence kind of the gap is a little bit higher."
        ],
        [
            "And this was the gene data where the performance was was similar across the different classifiers, because it's a difficult data set.",
            "Once you add noise to it."
        ],
        [
            "So this so in summary, we've shown that random shared boosting converges to the same result as shared boosting the same sort of classifier performance, but it takes sort of much less complexity, so it's not proportional to the number of views and now currently sort of in machine vision and other other application areas, the number of features are views can be very large, could be hundreds of views or even thousands.",
            "And so therefore removing that dependence on the number of views is a huge savings in performance.",
            "OK, thank you.",
            "We have time for one question perhaps.",
            "What would be interesting to see if, for example, if you simulate a view that's very dominant and very good performance, and you seem like a lot of other views which are?",
            "Really not as good, and then see what happens in that case.",
            "Did you maybe experiment with that and how then could you are boost behaves are randomized?",
            "Right, yeah, yeah, that's that's an excellent example right?",
            "So the so if you have a bunch of weak views and then one strong view, what happens?",
            "So we we didn't show those results, but in those cases what happens is that if you did the greedy approach, you converge a lot faster.",
            "And then what will happen is that the randomized one will eventually catch up, but it will take many more iterations.",
            "So where's that tradeoff between the number of views right versus convergence time?",
            "So that's going to be very data dependent, but that's a good very good observation.",
            "And also is there anyway you can rank the views if you share boost, not the randomized version, but the original shareview boosts.",
            "And then if you count which one is selected as the best performing one and in the end is that really the best view?",
            "Yeah, that that we haven't done that experiment, but that one seems like it could give you more information about.",
            "It is maybe early sensing that a particular view is dominating and then to use that to avoid doing redundant calculation on other views.",
            "I think I think that's what you're getting at, right?",
            "There's a different different approach to exploit shared boosting, but more efficiently right?",
            "So you can eliminate limited right, right, right, right, right?",
            "Perhaps the same example could be used to test the second case."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Kay Palaniappan and I'm going to be talking about randomized shared boosting, which is follows very much in the flavor of the previous talks.",
                    "label": 0
                },
                {
                    "sent": "So a lot of the sort of background I can kind of maybe speed up a little bit, and it's again a related to boosting with multiple views.",
                    "label": 0
                },
                {
                    "sent": "And what we show is certain performance guarantees between the shared boosting, which is some previous work an randomized boosting.",
                    "label": 0
                },
                {
                    "sent": "So the other coauthors are jingping from Montclair State University.",
                    "label": 0
                },
                {
                    "sent": "Cost in barboo at MIT Lincoln Laboratory gonna see the Roman at the Air Force Research lab.",
                    "label": 0
                },
                {
                    "sent": "Why fan and shampoo at IBM research?",
                    "label": 0
                },
                {
                    "sent": "This was sort of a big collaboration.",
                    "label": 0
                },
                {
                    "sent": "So the again.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Innovation is similar to the previous talk when in terms of there are many problems including image, image analysis and computer vision where you want to get information from multiple sources.",
                    "label": 0
                },
                {
                    "sent": "There could be sensors that have different measurements or measurement capabilities and you want to be able to exploit those that are more robust.",
                    "label": 0
                },
                {
                    "sent": "You know some sensors may be more robust to noise than others, so here's a simple case where you have two views so that.",
                    "label": 0
                },
                {
                    "sent": "There are the projected the data on the upper left as one of the views, so this is 1 view for example where the where obviously we cannot separate the two classes very easily.",
                    "label": 0
                },
                {
                    "sent": "Here's another view again where it's a poor discrimination, but if we fuse the two views then we'll get hopefully Third view which are refused view.",
                    "label": 0
                },
                {
                    "sent": "That gives a better separation between the classes.",
                    "label": 0
                },
                {
                    "sent": "So here we want to find the right combination for the source of information that we're getting from different views.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applications are in multisensor data Fusion, where often we have many different sources of data acquisition.",
                    "label": 0
                },
                {
                    "sent": "In this case, infrared or synthetic aperture, radar or Fleur, or many other sensors, and also biometrics, where you have a wide variety of sensors from iris data and fingerprints and audio, and here and so forth.",
                    "label": 0
                },
                {
                    "sent": "So these are just too simple.",
                    "label": 0
                },
                {
                    "sent": "Two categories broad categories of example.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case the shared boost algorithm, so I'm just going to kind of highlight it with, which has been previously presented, says that we can combine simple classifiers to generate a more powerful one.",
                    "label": 0
                },
                {
                    "sent": "In this case, is the X or problem where we can't separate the the two classes easily with a single classifier, but with three classifiers we can do a pretty good job.",
                    "label": 0
                },
                {
                    "sent": "Now so if we put this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same framework in and we want to compare different types of boosting algorithms, so I'll kind of compare Adaboost to how shared boost works and then the randomized version.",
                    "label": 0
                },
                {
                    "sent": "So this is to make kind of set the framework consistent in common framework.",
                    "label": 0
                },
                {
                    "sent": "Here we have examples here, and these are the weights for each example and along the horizontal axis is sort of time or the number of boosting iterations.",
                    "label": 0
                },
                {
                    "sent": "So after each iteration usually we're going to take the negative classes or the samples that are misclassified and assign greater weight to them so that they will be in the next iteration.",
                    "label": 0
                },
                {
                    "sent": "The classifier will work harder to classify these negative samples, so that's typically how.",
                    "label": 0
                },
                {
                    "sent": "Ada Boost works at each iteration.",
                    "label": 0
                },
                {
                    "sent": "We're trying to improve the classification of the negative samples and so this is to show kind of different trees and what is.",
                    "label": 0
                },
                {
                    "sent": "Why is the sizes of the trees different?",
                    "label": 0
                },
                {
                    "sent": "Is sort of this is proportional to our kind of weight that we assign to this tree.",
                    "label": 0
                },
                {
                    "sent": "So this tree didn't do that good of a job in the number of examples it classified correctly, so therefore it has a kind of a smaller wait.",
                    "label": 0
                },
                {
                    "sent": "This one did a better job at classify two example.",
                    "label": 0
                },
                {
                    "sent": "Example, so it has a higher weight and so forth.",
                    "label": 0
                },
                {
                    "sent": "So after we've done these multiple rounds of iteration, then we could combine all of the trees together and that's what this classifier here is.",
                    "label": 0
                },
                {
                    "sent": "This is the combined classifier where we have the weights assigned as a result from each round of boosting and we combine them all together.",
                    "label": 0
                },
                {
                    "sent": "And hopefully that gives a better.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Salt.",
                    "label": 0
                },
                {
                    "sent": "The IF we have multiple views so that was for one view.",
                    "label": 0
                },
                {
                    "sent": "So with multiple views you could generalize it in a straightforward way where you do boosting for each view in dependently.",
                    "label": 0
                },
                {
                    "sent": "So we call that independent Adaboost and in this case this is for view one and we get a set of classifiers are a combined classifier here, and in this case we have you 2 and we get another classifier.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're not.",
                    "label": 0
                },
                {
                    "sent": "We're not kind of sharing information.",
                    "label": 0
                },
                {
                    "sent": "Between the views when we do boosting independently.",
                    "label": 0
                },
                {
                    "sent": "And the weights are associated with each examples computed independently for each view.",
                    "label": 0
                },
                {
                    "sent": "So in this case we would have four classifiers from each view, and so there would be a total of essentially 8 classifiers in the final combined fused result.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With what we've shared boosting, it was sort of similar in spirit to the previous talk, which says, well, let's share the result.",
                    "label": 0
                },
                {
                    "sent": "So if we make a mistake in one view, so safer in this case, for example, we've made a mistake and view one in classifying.",
                    "label": 0
                },
                {
                    "sent": "So the classifier that was picked using View 1 using that sensor, that modality that feature set misclassified three of them, including the first one and the last one.",
                    "label": 0
                },
                {
                    "sent": "But view 2 classifieds.",
                    "label": 0
                },
                {
                    "sent": "The first one and the last one correctly but misclassified the second one.",
                    "label": 0
                },
                {
                    "sent": "So it seems that we should share these views such that we give we pick the better weight from here, right?",
                    "label": 0
                },
                {
                    "sent": "So we say, OK, the better view in this case is view too.",
                    "label": 0
                },
                {
                    "sent": "So we're going to take the weight from view 2 as our classifier OK, and so we're going to get our tree from view 2 and our weight from view 2.",
                    "label": 0
                },
                {
                    "sent": "And likewise, at each round of boosting, we look at the interplay between the two views and whichever view is doing a better job in classifying.",
                    "label": 0
                },
                {
                    "sent": "That's the view that dominates for that iteration.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this is done for as many rounds as we want.",
                    "label": 0
                },
                {
                    "sent": "And of course, the number of choices that we have will depend upon the number of views.",
                    "label": 0
                },
                {
                    "sent": "So the final classifier is assembled from the base classifiers that provide the best classification in each round.",
                    "label": 0
                },
                {
                    "sent": "In this case, shared boost requires fewer base classifiers and is less complex.",
                    "label": 0
                },
                {
                    "sent": "So here we only have 4 rather than 8, so that the spirit or the hope is that if you have that, there's more generalization capability in the final classifier that results.",
                    "label": 0
                },
                {
                    "sent": "And as we as we do these rounds of iteration of boosting, we're sort of hopefully the better view is is being used in each time, and if there are some view that dominates, then it will get used more frequently.",
                    "label": 0
                },
                {
                    "sent": "OK, and the shared boosting you can see is what I call a greedy approach.",
                    "label": 0
                },
                {
                    "sent": "Right, so suppose we have 10 features, 10 views.",
                    "label": 0
                },
                {
                    "sent": "We're always looking for the best view, so it's a greedy approach.",
                    "label": 0
                },
                {
                    "sent": "It's going to take the best view each time.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this kind of seems to make intuitive sense.",
                    "label": 0
                },
                {
                    "sent": "Say, OK, we'll take the best view.",
                    "label": 0
                },
                {
                    "sent": "Use that as a classifier and combine it with all the others.",
                    "label": 0
                },
                {
                    "sent": "In further rounds.",
                    "label": 0
                },
                {
                    "sent": "But then why don't we ask?",
                    "label": 0
                },
                {
                    "sent": "The question, is that optimal?",
                    "label": 0
                },
                {
                    "sent": "What happens if I take the median of this set of results?",
                    "label": 0
                },
                {
                    "sent": "Or some other choice?",
                    "label": 0
                },
                {
                    "sent": "Some other rank order statistic?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of any statistic, why not pick the view randomly?",
                    "label": 0
                },
                {
                    "sent": "Can we prove something about that?",
                    "label": 0
                },
                {
                    "sent": "If you pick a view, if we pick the view not by a greedy choice of whichever one minimize the error, but just randomly?",
                    "label": 0
                },
                {
                    "sent": "Can we show that that will converge to the same result as the shared boosting with sufficient time OK?",
                    "label": 0
                },
                {
                    "sent": "And that's what that's what indeed we showed in this paper, so shared boosting is robust against noise, and in this case noise means the class label noise.",
                    "label": 0
                },
                {
                    "sent": "As a result of the shared waiting mechanism, and but it's, but it has high complexity and it's proportional to so M as I said, is the number of views.",
                    "label": 0
                },
                {
                    "sent": "So we have to perform the boosting for each view, tease the number of iterations.",
                    "label": 0
                },
                {
                    "sent": "That was sort of our horizontal axis, so this is going to be proportional to the number of iterations and boosting Q is the number of dimensions and end is the number of training samples, so it's going to be M * T and then.",
                    "label": 0
                },
                {
                    "sent": "Complexity proportional to the maximum of queue or log in either the either dominated by the number of training samples of the number of dimensions, times N log N kind of just building the classifier cost.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is so we wanted to reduce this cost.",
                    "label": 0
                },
                {
                    "sent": "That's why we kind of explored this idea.",
                    "label": 0
                },
                {
                    "sent": "Well, if we just if we didn't force ourselves to evaluate every single view at each round of boosting, that would obviously reduce our cost.",
                    "label": 0
                },
                {
                    "sent": "And so in this case we randomly pick the view.",
                    "label": 0
                },
                {
                    "sent": "At each round, and therefore we avoid this cost M. OK, so at each iteration we randomly pick a view and that's the assigned.",
                    "label": 0
                },
                {
                    "sent": "You know that's the view that gets picked for that round of boosting and we continue on.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with with.",
                    "label": 0
                },
                {
                    "sent": "The result is that it does converge to the same.",
                    "label": 0
                },
                {
                    "sent": "The same result as shared boosting, so randomized shared shared boost converges to the same result as shared boost.",
                    "label": 0
                },
                {
                    "sent": "And the proof I'll leave to the paper.",
                    "label": 0
                },
                {
                    "sent": "I won't go into the details, but it follows the sort of the multi arm bandit approach where you have you have a slot machine with multiple arms, an you pull each arm and that kind of gives you a risk or reward associated with that particular step time step.",
                    "label": 0
                },
                {
                    "sent": "So in this case this training error is the T poles of the arm.",
                    "label": 0
                },
                {
                    "sent": "And what we want to do is we want to minimize this training error or maximize the reward function and it says that over the M views we can find for any any such distribution over the training data, the base learners learn a classifier visa plus with edge with edge strengths, beta times V ISA plus that's greater than RO.",
                    "label": 0
                },
                {
                    "sent": "Then, with probability at least one minus Delta, the training error of randomized share boost will become zero in polynomial time and the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how did we did we sort of assign the cost or the risk that gets evaluated in this proof is we need some sort of measure performance and what we chose was this notion of regret.",
                    "label": 0
                },
                {
                    "sent": "So what regret says is.",
                    "label": 0
                },
                {
                    "sent": "The best choice I could have made is gmax.",
                    "label": 0
                },
                {
                    "sent": "T. At that iteration, so at time step T, the best view produces this error value.",
                    "label": 0
                },
                {
                    "sent": "Gmax, OK, and the one I actually chose was a algorithm.",
                    "label": 0
                },
                {
                    "sent": "A or view a.",
                    "label": 0
                },
                {
                    "sent": "So this is our regret or kind of our difference between our best choice that we could have made OK. And So what we want to do is we want to we want to minimize our regret, right?",
                    "label": 0
                },
                {
                    "sent": "We want to minimize our regret and so this is the formulation, that of minimizing our total regret.",
                    "label": 0
                },
                {
                    "sent": "And this is builds upon some previous work by Freundin Shapir in 2002.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with a simple illustration from.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classic Iris data set.",
                    "label": 0
                },
                {
                    "sent": "So here we have two views, the sepal width and height and the petal width and height.",
                    "label": 1
                },
                {
                    "sent": "These are two classes in black and red and what we're going to do is we're going to assign noise to a subset of the samples, so in this case are noisy samples are shown.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In there, either there are shown in this in this blue boxes on either class label right on either the the class One or the black classes, or the red classes.",
                    "label": 0
                },
                {
                    "sent": "Red class sample.",
                    "label": 0
                },
                {
                    "sent": "So class one or Class 2 an so there these are noisy labels and we want to see overtime which of the noisy samples what their weight is over the boosting iterations.",
                    "label": 0
                },
                {
                    "sent": "OK, so we start out with equal weights and there are different errors in different.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "News.",
                    "label": 0
                },
                {
                    "sent": "And this is now view 2, so the previous one was in view one that's used in.",
                    "label": 0
                },
                {
                    "sent": "Now we look at view 2, and in this case this is shared, boosting on the left and Ada boost on the right.",
                    "label": 0
                },
                {
                    "sent": "Shared Boost decides that it's going to try an fix.",
                    "label": 0
                },
                {
                    "sent": "Sample 14 the most so it gives the highest weight to those and a lot of the other miss noisy samples are not adversely affected, right?",
                    "label": 0
                },
                {
                    "sent": "So we're not trying to.",
                    "label": 0
                },
                {
                    "sent": "We're not.",
                    "label": 0
                },
                {
                    "sent": "We're not increasing the weight on the noisy samples and moving our our decision plane or our classifier adversely towards the noisy labels.",
                    "label": 0
                },
                {
                    "sent": "Because we want to kind of ignore the noisy labels, right?",
                    "label": 0
                },
                {
                    "sent": "That's the goal.",
                    "label": 0
                },
                {
                    "sent": "With Ada boost, it kind of it's sort of more evenly distributes the weights that it's across the noisy labels where it's going to try and improve the.",
                    "label": 0
                },
                {
                    "sent": "The classifier result.",
                    "label": 0
                },
                {
                    "sent": "Now this is iteration.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three an view one.",
                    "label": 0
                },
                {
                    "sent": "And here were we have a different decision.",
                    "label": 0
                },
                {
                    "sent": "Plain different set of misclassified examples and the noisy labels are still labeled are still showing the same way here and now we can see shared boost has more of the noisy labels that are getting higher weights and this looks about the same between Adaboost and shared boost on this particular view.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But on this view it's a little bit better.",
                    "label": 0
                },
                {
                    "sent": "I think in terms of the total number.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Iteration 4 and now here.",
                    "label": 0
                },
                {
                    "sent": "This is iteration 5, view 2, where we're have again a much, much less emphasis on the noisy samples in shared boost compared to Ada boost.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we wanted to show and then now with some actual examples from some machine learning datasets, we have four different datasets.",
                    "label": 0
                },
                {
                    "sent": "The face data set, gender, glass and microarray gene expression data an we compared eight different algorithms, including share boost, random share, boost.",
                    "label": 0
                },
                {
                    "sent": "These are the first 2 and kind of dark green, then independent or independent ADA boost.",
                    "label": 0
                },
                {
                    "sent": "The one I explained that.",
                    "label": 0
                },
                {
                    "sent": "3rd one semidefinite programming, another one that's common in the literature Adaboost, with majority Vote Adaboost with concatenated space.",
                    "label": 0
                },
                {
                    "sent": "So that means we just concatenate all the views together and then.",
                    "label": 0
                },
                {
                    "sent": "Stacking an as the as the last one, and so.",
                    "label": 0
                },
                {
                    "sent": "In general, if there is no noise, right?",
                    "label": 0
                },
                {
                    "sent": "So in the noise free case, shared boost, randomize, shared boost and independent boosting.",
                    "label": 0
                },
                {
                    "sent": "Perform similarly for majority of the cases, except maybe in the gene one where shared boost and randomized and random share boosting perform a little bit better in the noise free case, there's not much difference between share boost and random share boost.",
                    "label": 0
                },
                {
                    "sent": "And with the with the noisy case.",
                    "label": 0
                },
                {
                    "sent": "So this is with 30% noise.",
                    "label": 0
                },
                {
                    "sent": "What we showed is that the random shared boosting.",
                    "label": 0
                },
                {
                    "sent": "And the shared boosting also have the same performance, which is good right?",
                    "label": 0
                },
                {
                    "sent": "So we want to show we can reach the same performance as shared boost, but without the cost.",
                    "label": 0
                },
                {
                    "sent": "So we maintain the same performance except now with noise there is.",
                    "label": 0
                },
                {
                    "sent": "In these, in the first three datasets, it's significantly better than Adaboost independent data boost, but not as big difference in the gene microarray data.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here we've we're showing the.",
                    "label": 0
                },
                {
                    "sent": "The performance over sort of iterations, so we have 150 based classifier.",
                    "label": 0
                },
                {
                    "sent": "So in the horizontal axis is sort of our our iterations right?",
                    "label": 0
                },
                {
                    "sent": "We're building classifiers up to 150 and we're showing the rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "Because this is an asymptotic convergence and we're showing how fast the randomized boosting which is in red is converging to the shared boost result in blue.",
                    "label": 0
                },
                {
                    "sent": "This is for the face data.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is for the gender data, so the more kind of well behaved the data is, the faster the convergence.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the glass data.",
                    "label": 0
                },
                {
                    "sent": "Here we got pretty similar results, but we you can see that convergence kind of the gap is a little bit higher.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this was the gene data where the performance was was similar across the different classifiers, because it's a difficult data set.",
                    "label": 0
                },
                {
                    "sent": "Once you add noise to it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this so in summary, we've shown that random shared boosting converges to the same result as shared boosting the same sort of classifier performance, but it takes sort of much less complexity, so it's not proportional to the number of views and now currently sort of in machine vision and other other application areas, the number of features are views can be very large, could be hundreds of views or even thousands.",
                    "label": 0
                },
                {
                    "sent": "And so therefore removing that dependence on the number of views is a huge savings in performance.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "We have time for one question perhaps.",
                    "label": 0
                },
                {
                    "sent": "What would be interesting to see if, for example, if you simulate a view that's very dominant and very good performance, and you seem like a lot of other views which are?",
                    "label": 0
                },
                {
                    "sent": "Really not as good, and then see what happens in that case.",
                    "label": 0
                },
                {
                    "sent": "Did you maybe experiment with that and how then could you are boost behaves are randomized?",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, yeah, that's that's an excellent example right?",
                    "label": 0
                },
                {
                    "sent": "So the so if you have a bunch of weak views and then one strong view, what happens?",
                    "label": 0
                },
                {
                    "sent": "So we we didn't show those results, but in those cases what happens is that if you did the greedy approach, you converge a lot faster.",
                    "label": 0
                },
                {
                    "sent": "And then what will happen is that the randomized one will eventually catch up, but it will take many more iterations.",
                    "label": 0
                },
                {
                    "sent": "So where's that tradeoff between the number of views right versus convergence time?",
                    "label": 0
                },
                {
                    "sent": "So that's going to be very data dependent, but that's a good very good observation.",
                    "label": 0
                },
                {
                    "sent": "And also is there anyway you can rank the views if you share boost, not the randomized version, but the original shareview boosts.",
                    "label": 0
                },
                {
                    "sent": "And then if you count which one is selected as the best performing one and in the end is that really the best view?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that that we haven't done that experiment, but that one seems like it could give you more information about.",
                    "label": 0
                },
                {
                    "sent": "It is maybe early sensing that a particular view is dominating and then to use that to avoid doing redundant calculation on other views.",
                    "label": 0
                },
                {
                    "sent": "I think I think that's what you're getting at, right?",
                    "label": 0
                },
                {
                    "sent": "There's a different different approach to exploit shared boosting, but more efficiently right?",
                    "label": 0
                },
                {
                    "sent": "So you can eliminate limited right, right, right, right, right?",
                    "label": 0
                },
                {
                    "sent": "Perhaps the same example could be used to test the second case.",
                    "label": 0
                }
            ]
        }
    }
}