{
    "id": "n6lzb2xy4zza2yuap5vssffub7p7uvhm",
    "title": "Beyond Blacklists: Learning to Detect Malicious Web Sites from Suspicious URLs",
    "info": {
        "author": [
            "Justin Ma, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Data Mining->Security & Privacy"
        ]
    },
    "url": "http://videolectures.net/kdd09_ma_bbldmwssurl/",
    "segmentation": [
        [
            "Visors at UC San Diego Lawrence all, Stephen Savage and Geoff Voelker.",
            "And I'm going to talk about a problem that hits relatively close to home, which is that I'm detecting malicious."
        ],
        [
            "Websites now in the course of your normal surfing or checking your email, you're invariably going to run into URLs, the niceness of which is a little under question.",
            "For example, say you were asked to make a judgment about a set of URLs to see whether they were malicious or not.",
            "You might be able to tell that the top one in this example is pretty clearly malicious, but then for others, it's going to be harder to make that judgment and.",
            "You know that especially that last one right there, that looks a little suspicious.",
            "But the bottom line is that we'd like to predict what is safe to visit without committing to."
        ],
        [
            "Risky actions.",
            "And to that end, our problem in a nutshell is to use features of those URLs to identify malicious websites.",
            "Now we're going to do this without taking into account the context in which the URL appeared or the content of the web page, the landing page, and we're going to reduce this to a binary classification problem, so any kinds of Ben is spam, phishing, exploits, and so on.",
            "We're just going to give it our universal stamp of disapproval."
        ],
        [
            "Now the state of the practice, primarily what happens when people want to detect malicious URLs of the common practice is to look up whether it's in a black list or to do learning on a small set of hand tuned features.",
            "And there are some limitations to these approaches, namely because if a site is not listed in the blacklist, then you won't be able to predict that is malicious, and you won't be able to count for new features if you're only selecting handpicked few.",
            "So in this ongoing arms race with.",
            "Spammers, phishers, and so on.",
            "It would be nice to see, well, can we have a more automated approach?"
        ],
        [
            "So at the outlet for today's talk, I'm going to give an overview of our system for detecting malicious URLs.",
            "I'll briefly go over our sources of training data and the algorithms that we use, but the focus of the talk is going to be the features that we use for classification.",
            "I'll present experimental results and conclude from there."
        ],
        [
            "Now, here's an overview of our system has three components.",
            "There is a source of labeled training data.",
            "We have a malicious and benign set.",
            "From there, given the URL we want to construct a feature vector because we're just giving a URL.",
            "It's not really a mathematical parsable form yet, so we have to do some feature vector construction.",
            "And from there we train a classifier which we can use to produce our predicted ratings.",
            "For example, with linear classifier, this might be your prediction function.",
            "And I'm going to briefly describe our training data as well as the models we use for class."
        ],
        [
            "Location.",
            "Now all the training data comes with four different sources to malicious into benign.",
            "The first one is from fish tank, which is a public site for reporting phishing sites.",
            "And the second one is from spam scatter, which is a study conducted at UCSD kind of studying.",
            "It was a measurement study for sources of spam, and we draw benign URL's.",
            "The negative examples from a couple of public URL directories.",
            "And together the permutations of those two of those four datasets give us four different datasets and in total when we take into account all the features, they have roughly 30 to 50."
        ],
        [
            "5000 features now the algorithm that we're using for classification.",
            "I'll focus on results that we got using logistic regression with L1 norm regularization.",
            "We got similar results for support vector machines, and both the SVM and logistic regression.",
            "We had better classification results then with Naive Bayes."
        ],
        [
            "Now, the focus of today's talk is going to be what happened with the features that we use for classification.",
            "Now it wasn't like."
        ],
        [
            "Arrived at A at the full representation right away, but if you were to construct a feature vector, so you're giving a URL, you might go about it by saying extracting tokens or querying information based on the host of network level properties, domain name properties and such, and from there you will construct a feature vector.",
            "Some of those features are going to be real valued, a lot of them are going to be binary."
        ],
        [
            "Um?",
            "And in the course of the evaluations, we wanted to sort of build up to the final representation that is, starting with something simple like blacklists or very simple heuristics and building on that considering domain name registration information as well as host based properties and lexical properties."
        ],
        [
            "Now the blacklist is pretty straightforward.",
            "What happens is that we have a set of blacklists name maintained by various entities, and what we do from here is that we are given URL.",
            "We query blacklist and we get a yes no answer.",
            "Is it in the blacklist is a non in the blacklist and that gives us a little sort of a bit vector telling us about blacklist membership."
        ],
        [
            "Now the second set of features were manually considered by various previous work, and so we designed to evaluate that as a set on its own.",
            "They included 3 three properties, which are the IPH weather and IP addresses in the host name.",
            "The number of dots in the URL and also when the domain name was registered.",
            "Sort of.",
            "These very basic properties that would think would be intuitive to kind of pick out from malicious URLs."
        ],
        [
            "Now is a step beyond that we consider who is features that is domain name registration properties in addition to the dates of registration, expiration and so on.",
            "We also consider who registered the domain as well as who managed the registration for the domain so.",
            "You might get some information about a hint of about the maliciousness of a site based on Windows registered, but if you knew it was managed by malicious company then that could give you a little more leverage in classification."
        ],
        [
            "Now is a step beyond that we consider those features, blacklists and who is information as a subset of a broader set of properties which we call a host based features which include information about where the host is hosted, which I which Internet service provider is hosting the website.",
            "So in this case we know, say facebook.com is hosted in a reputable part of the Internet address space, whereas this other site which is a phishing site is going to be hosted in a less reputable part.",
            "Of the Internet.",
            "So we want to definitely take that into account."
        ],
        [
            "And finally we consider lexical features, including some that we mentioned before.",
            "But more importantly, we also take into account breaking the URL into tokens and using a bag of words representation so that we can classify on those.",
            "So this sort of intuitively captures the fact that malicious URLs will tend to look different than benign URLs."
        ],
        [
            "Now all these taken together, what you're seeing here is the error rates for classification over the four datasets.",
            "The X axis is showing the error rate and what we see as shown on the little table on the right is that when we have more features to classify with, we get better accuracy because the classifier has a broader set of information to draw from.",
            "Now what happens then if we combine Hostbased features with lexical features?",
            "Then we get the result as follows.",
            "It actually improves on both."
        ],
        [
            "Just host based features alone.",
            "And both lexical features alone.",
            "Interestingly, if you're concerned about, say, overhead of querying like, because who is information is not very well structured, I just want to mention quickly that the accuracy for the full feature set was between 96 and 99%, so this is pretty reasonable."
        ],
        [
            "But if you were concerned about overhead of querying because who was not structured like DNS, then if you omit who is features annual MIT, the blacklist features.",
            "You can still get competitive classification rates."
        ],
        [
            "And this brings me sort of back full circle to the title of this presentation, so we're concerned, well that blacklists may not give us a high enough detection rate, and So what happens?",
            "Comparing the arosi curves for the blacklist features versus the full feature set, we see that, say, at a fixed false positive rate of zero point 1%.",
            "Then we get a much higher detection rate using these predictive features.",
            "Then we were just rely on blacklists alone.",
            "I think the difference here is roughly 70%."
        ],
        [
            "Now there are some limitations.",
            "The false positives and false negatives of our approach typically tend to do with sort of guilt by Association.",
            "An innocence by Association.",
            "But we're optimistic that if we account for web page content in the future that we can probably solve some of these false positives and false negatives.",
            "Now, granted, this is for a system that's already achieving up to 99% accuracy, but we always want to do better so we can see we want to see if we can improve on that."
        ],
        [
            "So in conclusion, I talked about a system for detecting malicious URLs.",
            "We only use the URL, no content or context with a diverse feature set, we can achieve relatively good accuracy and there's more in the paper about model analysis, feature analysis, and I want to point out some related work that we've done where given these features that I talked about today, what happens if we scale up the training set to say millions of examples and millions of features?",
            "In that regime, we asked the question well, do online learning algorithms help with us with that?",
            "And that's been published in ICML 2009.",
            "And as future work we're looking to deploy this as a public service for classification.",
            "So how do we get this system to scale up?",
            "And with that."
        ],
        [
            "I'll take your questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visors at UC San Diego Lawrence all, Stephen Savage and Geoff Voelker.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to talk about a problem that hits relatively close to home, which is that I'm detecting malicious.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Websites now in the course of your normal surfing or checking your email, you're invariably going to run into URLs, the niceness of which is a little under question.",
                    "label": 0
                },
                {
                    "sent": "For example, say you were asked to make a judgment about a set of URLs to see whether they were malicious or not.",
                    "label": 0
                },
                {
                    "sent": "You might be able to tell that the top one in this example is pretty clearly malicious, but then for others, it's going to be harder to make that judgment and.",
                    "label": 0
                },
                {
                    "sent": "You know that especially that last one right there, that looks a little suspicious.",
                    "label": 0
                },
                {
                    "sent": "But the bottom line is that we'd like to predict what is safe to visit without committing to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Risky actions.",
                    "label": 0
                },
                {
                    "sent": "And to that end, our problem in a nutshell is to use features of those URLs to identify malicious websites.",
                    "label": 1
                },
                {
                    "sent": "Now we're going to do this without taking into account the context in which the URL appeared or the content of the web page, the landing page, and we're going to reduce this to a binary classification problem, so any kinds of Ben is spam, phishing, exploits, and so on.",
                    "label": 0
                },
                {
                    "sent": "We're just going to give it our universal stamp of disapproval.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the state of the practice, primarily what happens when people want to detect malicious URLs of the common practice is to look up whether it's in a black list or to do learning on a small set of hand tuned features.",
                    "label": 1
                },
                {
                    "sent": "And there are some limitations to these approaches, namely because if a site is not listed in the blacklist, then you won't be able to predict that is malicious, and you won't be able to count for new features if you're only selecting handpicked few.",
                    "label": 1
                },
                {
                    "sent": "So in this ongoing arms race with.",
                    "label": 0
                },
                {
                    "sent": "Spammers, phishers, and so on.",
                    "label": 1
                },
                {
                    "sent": "It would be nice to see, well, can we have a more automated approach?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at the outlet for today's talk, I'm going to give an overview of our system for detecting malicious URLs.",
                    "label": 0
                },
                {
                    "sent": "I'll briefly go over our sources of training data and the algorithms that we use, but the focus of the talk is going to be the features that we use for classification.",
                    "label": 0
                },
                {
                    "sent": "I'll present experimental results and conclude from there.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, here's an overview of our system has three components.",
                    "label": 0
                },
                {
                    "sent": "There is a source of labeled training data.",
                    "label": 0
                },
                {
                    "sent": "We have a malicious and benign set.",
                    "label": 0
                },
                {
                    "sent": "From there, given the URL we want to construct a feature vector because we're just giving a URL.",
                    "label": 0
                },
                {
                    "sent": "It's not really a mathematical parsable form yet, so we have to do some feature vector construction.",
                    "label": 0
                },
                {
                    "sent": "And from there we train a classifier which we can use to produce our predicted ratings.",
                    "label": 0
                },
                {
                    "sent": "For example, with linear classifier, this might be your prediction function.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to briefly describe our training data as well as the models we use for class.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Location.",
                    "label": 0
                },
                {
                    "sent": "Now all the training data comes with four different sources to malicious into benign.",
                    "label": 0
                },
                {
                    "sent": "The first one is from fish tank, which is a public site for reporting phishing sites.",
                    "label": 0
                },
                {
                    "sent": "And the second one is from spam scatter, which is a study conducted at UCSD kind of studying.",
                    "label": 0
                },
                {
                    "sent": "It was a measurement study for sources of spam, and we draw benign URL's.",
                    "label": 0
                },
                {
                    "sent": "The negative examples from a couple of public URL directories.",
                    "label": 0
                },
                {
                    "sent": "And together the permutations of those two of those four datasets give us four different datasets and in total when we take into account all the features, they have roughly 30 to 50.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "5000 features now the algorithm that we're using for classification.",
                    "label": 0
                },
                {
                    "sent": "I'll focus on results that we got using logistic regression with L1 norm regularization.",
                    "label": 0
                },
                {
                    "sent": "We got similar results for support vector machines, and both the SVM and logistic regression.",
                    "label": 1
                },
                {
                    "sent": "We had better classification results then with Naive Bayes.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, the focus of today's talk is going to be what happened with the features that we use for classification.",
                    "label": 0
                },
                {
                    "sent": "Now it wasn't like.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Arrived at A at the full representation right away, but if you were to construct a feature vector, so you're giving a URL, you might go about it by saying extracting tokens or querying information based on the host of network level properties, domain name properties and such, and from there you will construct a feature vector.",
                    "label": 0
                },
                {
                    "sent": "Some of those features are going to be real valued, a lot of them are going to be binary.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And in the course of the evaluations, we wanted to sort of build up to the final representation that is, starting with something simple like blacklists or very simple heuristics and building on that considering domain name registration information as well as host based properties and lexical properties.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the blacklist is pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "What happens is that we have a set of blacklists name maintained by various entities, and what we do from here is that we are given URL.",
                    "label": 0
                },
                {
                    "sent": "We query blacklist and we get a yes no answer.",
                    "label": 0
                },
                {
                    "sent": "Is it in the blacklist is a non in the blacklist and that gives us a little sort of a bit vector telling us about blacklist membership.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the second set of features were manually considered by various previous work, and so we designed to evaluate that as a set on its own.",
                    "label": 0
                },
                {
                    "sent": "They included 3 three properties, which are the IPH weather and IP addresses in the host name.",
                    "label": 0
                },
                {
                    "sent": "The number of dots in the URL and also when the domain name was registered.",
                    "label": 1
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "These very basic properties that would think would be intuitive to kind of pick out from malicious URLs.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now is a step beyond that we consider who is features that is domain name registration properties in addition to the dates of registration, expiration and so on.",
                    "label": 1
                },
                {
                    "sent": "We also consider who registered the domain as well as who managed the registration for the domain so.",
                    "label": 0
                },
                {
                    "sent": "You might get some information about a hint of about the maliciousness of a site based on Windows registered, but if you knew it was managed by malicious company then that could give you a little more leverage in classification.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now is a step beyond that we consider those features, blacklists and who is information as a subset of a broader set of properties which we call a host based features which include information about where the host is hosted, which I which Internet service provider is hosting the website.",
                    "label": 0
                },
                {
                    "sent": "So in this case we know, say facebook.com is hosted in a reputable part of the Internet address space, whereas this other site which is a phishing site is going to be hosted in a less reputable part.",
                    "label": 0
                },
                {
                    "sent": "Of the Internet.",
                    "label": 0
                },
                {
                    "sent": "So we want to definitely take that into account.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally we consider lexical features, including some that we mentioned before.",
                    "label": 1
                },
                {
                    "sent": "But more importantly, we also take into account breaking the URL into tokens and using a bag of words representation so that we can classify on those.",
                    "label": 0
                },
                {
                    "sent": "So this sort of intuitively captures the fact that malicious URLs will tend to look different than benign URLs.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now all these taken together, what you're seeing here is the error rates for classification over the four datasets.",
                    "label": 0
                },
                {
                    "sent": "The X axis is showing the error rate and what we see as shown on the little table on the right is that when we have more features to classify with, we get better accuracy because the classifier has a broader set of information to draw from.",
                    "label": 0
                },
                {
                    "sent": "Now what happens then if we combine Hostbased features with lexical features?",
                    "label": 0
                },
                {
                    "sent": "Then we get the result as follows.",
                    "label": 0
                },
                {
                    "sent": "It actually improves on both.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just host based features alone.",
                    "label": 0
                },
                {
                    "sent": "And both lexical features alone.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, if you're concerned about, say, overhead of querying like, because who is information is not very well structured, I just want to mention quickly that the accuracy for the full feature set was between 96 and 99%, so this is pretty reasonable.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if you were concerned about overhead of querying because who was not structured like DNS, then if you omit who is features annual MIT, the blacklist features.",
                    "label": 0
                },
                {
                    "sent": "You can still get competitive classification rates.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this brings me sort of back full circle to the title of this presentation, so we're concerned, well that blacklists may not give us a high enough detection rate, and So what happens?",
                    "label": 0
                },
                {
                    "sent": "Comparing the arosi curves for the blacklist features versus the full feature set, we see that, say, at a fixed false positive rate of zero point 1%.",
                    "label": 0
                },
                {
                    "sent": "Then we get a much higher detection rate using these predictive features.",
                    "label": 1
                },
                {
                    "sent": "Then we were just rely on blacklists alone.",
                    "label": 0
                },
                {
                    "sent": "I think the difference here is roughly 70%.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there are some limitations.",
                    "label": 0
                },
                {
                    "sent": "The false positives and false negatives of our approach typically tend to do with sort of guilt by Association.",
                    "label": 1
                },
                {
                    "sent": "An innocence by Association.",
                    "label": 0
                },
                {
                    "sent": "But we're optimistic that if we account for web page content in the future that we can probably solve some of these false positives and false negatives.",
                    "label": 0
                },
                {
                    "sent": "Now, granted, this is for a system that's already achieving up to 99% accuracy, but we always want to do better so we can see we want to see if we can improve on that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, I talked about a system for detecting malicious URLs.",
                    "label": 0
                },
                {
                    "sent": "We only use the URL, no content or context with a diverse feature set, we can achieve relatively good accuracy and there's more in the paper about model analysis, feature analysis, and I want to point out some related work that we've done where given these features that I talked about today, what happens if we scale up the training set to say millions of examples and millions of features?",
                    "label": 1
                },
                {
                    "sent": "In that regime, we asked the question well, do online learning algorithms help with us with that?",
                    "label": 1
                },
                {
                    "sent": "And that's been published in ICML 2009.",
                    "label": 0
                },
                {
                    "sent": "And as future work we're looking to deploy this as a public service for classification.",
                    "label": 0
                },
                {
                    "sent": "So how do we get this system to scale up?",
                    "label": 0
                },
                {
                    "sent": "And with that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll take your questions.",
                    "label": 0
                }
            ]
        }
    }
}