{
    "id": "n2atxzgynlkao6px3hojq2gdkx4kafcm",
    "title": "Sparse methods for machine learning: Theory and algorithms",
    "info": {
        "author": [
            "Guillaume Obozinski, \u00c9cole des Ponts ParisTech, MINES ParisTech"
        ],
        "published": "Nov. 16, 2010",
        "recorded": "September 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2010_obozinski_smta/",
    "segmentation": [
        [
            "So the problem that I'd like to consider is a prob."
        ],
        [
            "And of middle metastases prognosis so basically.",
            "We have a certain number of patients that have a tumor and we would like to be able to predict whether or not this tumor will metastase or not, right?",
            "And the type of information that we have available to make this prediction is we have microarray data.",
            "So the level of gene expression for a large number of genes.",
            "And the idea is that while we're predicting well, first.",
            "The typical assumption that biologists would make is that the small number of genes should be associated with the possibility of having a metastasis that not the entire set of genes are modified by the fact that the cell is timorous or is likely to metastase.",
            "So a small set of genes is likely to be predictive.",
            "In finding this month, Medici jeans has to be have two advantages.",
            "First, it is likely to increase.",
            "The predictor, improved prediction accuracy and then it would be useful to biologist to identify what are the genes that are responsible for the metastasis.",
            "So.",
            "One common issue, so one way of doing this is to build a classifier and to use some form of sparsity, which basically suggests that you should, such as like anyone.",
            "The regulation to select a small number of genes."
        ],
        [
            "But one issue that you encounter in practice is that this type of data is very noisy and Moreover genes that belong to same groups of genes that are active together that are called biological pathways.",
            "Are likely to be very correlated, so identifying the right set of group is quite difficult and one thing which is natural is to consider directly those biological pathways which are anyway the type of information that are relevant to biologists so far, biologists, if you give them a set of genes that are predictive of tumor or metastasis where they will try to understand is what are the biological pathways that these genes are.",
            "Associated to.",
            "So why am I considering this example?",
            "Well, because so.",
            "Magical pathways have the interesting property that so they gather genes that correspond to the same logical mechanism they contain, often very correlated genes that are going to be difficult to tell apart.",
            "If we're using L1 regularization and the pathway form groups that are overlapping.",
            "So in what?",
            "We have considered so far the groups when we consider groups where disjoint.",
            "So what happens where the group overlap, and in particular what the information that we have in this application is that we would like to be able to select entire pathways so that the support of our model should be a union of pathways.",
            "So a union of overlapping group.",
            "So why would that be difficult?"
        ],
        [
            "Let's step back for a second and look at what was happening for the lasso in the group lasso.",
            "So for LASSO, the L1 norm is a non differentiable regularizer and because of its structure a certain number of individual coefficients."
        ],
        [
            "Are set to 0 because essentially the.",
            "You get trapped by optimizing in the non differentiability of the function."
        ],
        [
            "In the case of the group Lasso, again a certain number of groups are."
        ],
        [
            "To 0.",
            "And therefore the support that you estimate is a union of group."
        ],
        [
            "Now if we consider the case of the group lasso with overlap where we can still use the same regularizer, the sum over a certain number of groups of WG, and So what?",
            "I denote by that now is that WG is the sub vector of W, which has entries indexed by the Group G, and so this norm is the L2 norm.",
            "Or it could possibly be another LQ norm.",
            "And I'm thinking the sum over order groups in G so I can still consider this regularization, which is the one of the group lasso or DL102 regularization.",
            "It is actually still a norm even if the different groups overlap, but what happens is that what this regulation is going to induce is that a certain number of groups are going to be set to 0."
        ],
        [
            "And so if I start setting group to."
        ],
        [
            "So like this or like this?",
            "What you notice is that basically what is left is not any longer union of the groups that I have defined."
        ],
        [
            "And but in fact it is an intersection of the complement of certain subset of the groups that have been set to 0.",
            "So how can we address this?"
        ],
        [
            "Yes, but the idea is to use.",
            "A latent group lasso.",
            "What do I mean by that?",
            "So if, let's say we're trying to solve a classification problem so.",
            "We have performed, for example logistic regression with the parameter vector W. What I can do is I can decompose this vector W in a certain number of vectors that I'm going to define as follow for each group I'm going to introduce a latent vector V1 for Group One, V2 for Group 2, and V3 for Group 3.",
            "And if you want is going to be allowed to be non zero only on Groupon and then it will be forced to be 0 everywhere else.",
            "V2 is going to be non zero only on group two and three is going to be 1 zero only on Group 3.",
            "What I do is, I'd say that W the vector W results from the superposition of these three vectors, so a linear combination.",
            "And what they will regularize into the regularising WI regularized latent parameter vector.",
            "So let's say that I want to solve my problem of.",
            "Miss test this prediction, so I might want to solve a.",
            "Learning problem which is classification problem.",
            "So my empirical risk would be the empirical risk based on the risk class for example.",
            "And within regularize is the sum of the L2 norms of each of the VI vectors, with the constraint Now that these should combine linearly to form W. So.",
            "If you believe me that basically the situation is the same as before, and that a certain number of these groups are going to be set to 0, basically what happens is that a certain number of these latent vectors are going to be set to 0.",
            "So let's say V3 is set to 0, then W results from the linear combination of V1 and V2.",
            "And it is therefore a vector that has support with the same support as the Union of the support of V1 and the support of Y two.",
            "So it's a union of groups.",
            "And So what is nice as well is that it is but now possible to select one variables without selecting all the groups that are associated that are containing it, which was not the case with the naive extension of."
        ],
        [
            "Reply so.",
            "What is perhaps even more interesting is that.",
            "Um?",
            "This formulation proposes introduces a new type of regular of regularizer on W. So let me explain this.",
            "Essentially this is the same optimization problem as the one that I presented before and what you can consider is to basically marginalized out if I can say the latent variables V by defining a function Omega of W which is.",
            "The minimum of an optimization problem on the latent variables V. So this defines for each W. This defines a function on W and I can use this function as a regularizer.",
            "In this problem is actually exactly equivalent to the to the previous problem."
        ],
        [
            "And what maybe perhaps surprising is that this actually defines a norm on W, so this is a new norm W. And to get a sense of."
        ],
        [
            "What is happening?",
            "Let me show you what the unit balls of different norms, what they look like.",
            "So if you remember, this is the L1L2 norm that forces were showing you.",
            "In the first part of the tutorial corresponding to.",
            "Um, so a tree dimensional vector W1W 2W3 and where the groups where the group of variables 1, two and the variable tree.",
            "So the parts of the world that are non smooth are the tip where W 3 is non 0 N W1W2S0 and the circle where it is the opposite.",
            "Now here is the unit ball corresponding to.",
            "Group the naive generalization of group lasso with groups 1, two and Group 2.",
            "Three where you take the sum of the L2 norm of W12 and W23.",
            "And in that case, what you see is that this the ball has very few.",
            "And very few non smooth points.",
            "In fact it has only these four corners that are non smooth that correspond to selecting alone W 3 or selecting sorting alone W one or second alone W 3.",
            "And the Norm Omega that I've just defined is actually this norm, which has the nice property that it is exactly can be shown.",
            "It is exactly the convex Hull of the two disks that correspond to the two different groups.",
            "So again, this is the norm that is corresponding to trying to obtain supports that are the Union of Group 1, two and Group 2, three, and so the group went to.",
            "Being non 0.",
            "Corresponds to.",
            "And the horizontal circle and then W30 and then W 2, three being non zero correspond to the vertical circle and so this.",
            "This unit ball is just the convex Hull of these of these circles."
        ],
        [
            "So this is an application of this type.",
            "This new form of regularization to the metastasis problem, and.",
            "What we obtained in this case is that so we compared essentially two types of regularization regularization with the L1 norm and regulation with this norm that induces sparsity at the level of pathways and in terms of misclassification error, we observe small improvement which is not significant, But if you consider the number of pathways involved in the solution, you see that the number pathways, so those those three numbers correspond to three different.",
            "Replicates of the experiments using like 3 different datasets.",
            "Examples of simple datasets, and in that case you get a very large number of pathways, whereas in this case you very often get a small number of pathways which are going to lead to something that's much more interpretable for the biologist."
        ],
        [
            "Alright, let's talk about sparse structure."
        ],
        [
            "PCA.",
            "So.",
            "OK, we talked before about sparse PCA and I mentioned the fact that sparse PCA and dictionary learning are closely related.",
            "But then somehow, now that we've started looking at.",
            "More structured regularization.",
            "We might want to add more structure to the constraint that we want to put on either on the dictionary elements or the principal components of the participant analysis or on the decomposition coefficients.",
            "So what other constraints can?"
        ],
        [
            "Consider it well.",
            "In fact, you can retrieve with different types of constraints on victory learning or or PCA.",
            "Alot of algorithms that are familiar.",
            "So if you just want to find an approximation where the good low rank approximation, you just need to use unv with a few columns.",
            "I've mentioned that before.",
            "So sparse PCA, tree learning U&V as many zeros.",
            "I've talked about that before, but you can also retrieve K means.",
            "If you enforce that you is a binary matrix such that the sum of the elements on each column is 1.",
            "And you can retrieve non negative matrix factorization if you constrains the coefficient to be positive and so on.",
            "And I'll give other examples later."
        ],
        [
            "Um?",
            "So what am I going to call structured sparse PCA?",
            "Well, so I'm now using the notations from signal processing.",
            "So in sparse PCA what we had is we had a matrix D of dictionary elements that correspond to the principle components and we were regularising.",
            "Each of these columns of D that there I denote DJ by.",
            "The N1 norm so that these.",
            "Columns are sparse and then I was constraining the coefficients to avoid to get a digital solution where the coefficients would blow up in the size of the the norm of the dictionaries with shrink to 0.",
            "So what I want to call sparse structure PCA Now is the generalization of this problem where I would like to enforce.",
            "I would like to consider more sophisticated structure on the dictionary elements.",
            "So again, this will lead to situations where the diction will not be orthogonal.",
            "Again, it's not going to be jointly convex, but convex in each of DJ and Alpha J, which will lead to efficient block coordinate descent algorithms."
        ],
        [
            "OK, um what concrete problem do I have in mind?",
            "Well, so a problem that has been of interest in the vision community is to find good decomposition for faces.",
            "So good decomposition means can can have several meanings, so can be good because you are able to perform very well supervised task based on this decomposition.",
            "And it can also mean that you retrieve some of the latent structure.",
            "Of faces.",
            "So when the type of decomposition that was used already a couple of decades ago was eigenfaces, which essentially correspond to taking the image, the database formed by the images of the faces and applying principle component analysis on it.",
            "But what was a bit disappointing for some people in the result of eigenfaces that obviously each of the eigen phases picture can be viewed as a picture that covers the entire face.",
            "And that one thing that you might want to retrieve when you decompose cases to decompose phase in natural parts like eyes, nose, mouth and things like that.",
            "So this is actually what made the idea of Lee and Seung to apply non negative matrix factorization too.",
            "Two face decomposition is very popular is that they were showing in their paper that using a factorization that was constraining both the dictionary elements to be to have positive coefficients in the decomposition coefficient to be positive, they were able to identify localized parts of the face.",
            "So eyebrows, eyes and so on.",
            "So the database that I'm showing here is the database that we used, which has a slightly higher resolution than the database they considered and."
        ],
        [
            "When we use NMF, we obtain something that looks like it tends to be sparse, but which is not so sparse.",
            "So you can see that there are some bright parts in the image which suggests that if you were doing some very aggressive thresholding you could get sparse portions that maybe for some of them might be localized.",
            "But essentially you get something that's not quite sparse and not that well localized.",
            "So one idea is to use prior knowledge to actually enforce to basically say that, well, we know that the part that we would like to find are parts that should be localized on the face.",
            "Can we use structured sparsity to enforce such a prior?"
        ],
        [
            "So here is the structure to suggesting to do this.",
            "And consider that these grids that I'm showing each of these grid is actually corresponding to an image, so it's an image that has five by five pixels by 5 pixels.",
            "And, um.",
            "I'm going to consider the following regularization on it.",
            "So each of these image you can think of as being one of the dictionary elements.",
            "So one vector in the matrix factorization problem, but represented here on the image, right?",
            "So I'm treating each image as a vector, but of course it is very natural to represent these vectors as images and in this case it's a 5 by 5 image.",
            "And I'm going to consider the regularization which is the sum of the L2 norms.",
            "Of some groups of pixels and the groups of pixels that I'm going to consider, I mean of the dictionary element values corresponding to a certain number of pixels.",
            "And the groups that consider are the groups in blue.",
            "Which are groups that are to the left of a vertical line and then their compliments.",
            "So the groups that are to the right of a vertical line and then consider also the groups that are above certain horizontal line and below horizontal line.",
            "So if I regularize by the sum of these groups a.",
            "An image that has that shape and if I set if I therefore certain number of groups to zero, what's going to happen is I'll get something situation which is like what I have on the right.",
            "So if I set this group to 0, this group to 0, this group zero and this group to 0.",
            "So the group that are above below to the left and to the right of the four lines that I've drawn.",
            "What I'm left with is a rectangular region, so this regularization with a carefully set of groups actually gives me dictionary elements who supports are exactly the rectangular, all the rectangular regions of the grid.",
            "Of the image."
        ],
        [
            "So we can go even further and we can consider now groups which are defined by diagonal lines, so I can take all the groups that are above an ascending diagonal 9 or descending down the line or below that same line and then if I combine this with the groups that I had before, then I will start to get shapes that are a little closer to general convex shapes in the image and adding more lines you would be able to approximate pretty much all of the convex shapes that you have in the image.",
            "So the idea is to use this norm in a sparse structured PCA formulation, where this is going to be the structured norm."
        ],
        [
            "And so.",
            "Again, in the formulation that I wrote before, this will correspond to using this regularization on each of the dictionary elements that I introduce.",
            "So now this regularization is.",
            "Is a bit more complicated than the ones that we've considered before in particular.",
            "If Francis said that one of the characteristic of the of the L1 norm man of the L1L2 norm, the block norms is that they are separable and that makes it that coordinate descent algorithms or block coordinate descent of them are convergent.",
            "In this case, the norm that I have introduced is not separable, so those algorithms are excluded.",
            "And using proximal algorithm in this case is also difficult.",
            "So one way to approach to tackle this problem is to use the reweighted scheme, which Fortunately is still a possibility in this case.",
            "Um?"
        ],
        [
            "So let me show you what happened.",
            "So this is a database that has the following characteristic that.",
            "So it's a bit which is made of half man half woman composed of 100 individuals, and for each individual we have 14 non included images and 12 occluded images.",
            "So the type of occlusion that we have, our glasses or scarves, or sometimes both both and.",
            "Obviously you can see one of the advantages of learning a representation of the data which is localized is that one can imagine that this representation will be robust to some extent to occlusion, because if the database is localized then sorry if the dictionary elements are localized, then the projection on these documents of the subparts of the image will not be altered by the fact that some part of the images.",
            "Is included and so one of the characteristic of this database is that it has lateral.",
            "Lateral eliminations are main factor of variance in this data.",
            "So we worked with this representation that I'm showing here, which is slightly reduced respect to the origonal."
        ],
        [
            "Representation.",
            "So what I'm showing here is the dictionaries that are obtained if you use sparse PCA on the left and structured sparse PCA on the right, so the dictionary elements are these images that are sorted from top left to bottom right.",
            "By this decreased order of explained variance.",
            "And to make this figure maybe a little more readable, let me show you in."
        ],
        [
            "Green.",
            "What are the zero coefficients?",
            "So what you can see is that sparse PCA is definitely sparse, but it's quite a bit scattered, and it's difficult to decide.",
            "You know if you obtain something that correspond to parts, whereas obviously spectral PCA by construction is retrieving.",
            "Portions of the face that are convex.",
            "So for instance, you see that the two main dictionary elements that you retrieve are the ones that main principle components are the one that explain the left right illumination and then some of these.",
            "Um components are corresponding to eyes or nose or some parts of the face, and obviously some of them are actually capturing parts that are larger.",
            "Um, so should we be happy with this?",
            "I mean, in a sense, this is a pleasant perhaps to look at, but is this a good representation of the data?",
            "Is this really a good idea to try and find parts?",
            "Well so it depends what you like to do.",
            "So if you're interested in supervised problem, maybe whether or not this is a good.",
            "Representation should be determined by the performance of combining this obtained representation with using it to solve a supervised."
        ],
        [
            "Problem so this is where we consider here where basically we try to do face recognition.",
            "On the occluded images.",
            "So we use the non included images to learn the.",
            "The dictionary on the right and then we projected the data that was included on the same dictionary and use the coefficient obtained to classify the corresponding images using nearest neighbor and.",
            "Long story short, I don't have too much time to describe in details all these curves, but as the dictionary size is increasing, the approaches in black and in red that correspond to using structured.",
            "Dictionary elements are actually performing significantly better than the other methods, including PCA, NMF, and so on."
        ],
        [
            "OK, let me.",
            "Um, tell you about.",
            "Particle dictionary learning which.",
            "Have you I view and we view as very exciting?",
            "Direction that that is a interesting way to use truck."
        ],
        [
            "Sparsity, so here is a situation where you would like to use Heartical dictionary, or specifically a hierarchical topic model.",
            "So what you're interested in is modeling text corpora so you have a database of documents, and each of these document is represented by a vector exchange that has dimension the number of.",
            "Words in the dictionary.",
            "And the entries in XJ count the number of occurrences of that word in the document.",
            "Obviously the sum of the total sum of the number of words of each type that appear is the total number of words of the document and.",
            "A natural way to model these documents represented that way is to actually say that.",
            "Those documents are viewed as as the counts of words are generated as a large multinomial distribution, but with some structure.",
            "So what kind of structure can we consider?",
            "Well, we can consider that.",
            "Um?",
            "Words in the document correspond to different topics and that topics are.",
            "Each topic has its own lexicon, its own.",
            "Lexical field.",
            "So it has its own distribution over words.",
            "And so each of these distribution over words I'm going to view them as a column of the matrix D. And each document is going to have some topic proportions, so a certain proportion of you.",
            "All of these topics, and therefore the.",
            "The resulting document will be obtained by taking a linear combination of the columns of the corresponding to using each of the specific dictionaries corresponding to each topic in the proportions of how much these topics are used in the document, and then sampling the multinomial vector from it.",
            "So.",
            "This seems to be the appropriate setting to public framework to think of a low rank factorization of a Word document matrix, and this can be viewed as some form of PCA, but associated with a multinomial likelihood.",
            "So you can call this multinomial PCA.",
            "Maybe the model that many of you are more familiar with and that attempts to model the same type of data is latent digital allocation which.",
            "Essentially is a beige and take on the same model which puts a prior on the topic proportions, namely directly prior.",
            "Since it is conjugate to the multinomial prior, which has been very popular both to model text corpora but also to model.",
            "To model images in computer vision using bag of features instead of bag of words.",
            "So what I've described here is a model that.",
            "Decomposers essentially a document into certain number of topics.",
            "But it is very natural to think of these topics as or as being organized in some sort of hierarchy of our ontology.",
            "If you will, right so when you read a document, you might find that some of the topics in that document are quite general and some are more specific, and so if we're going to build a model that automatically identifies different topics that are present in documents, then why not try and organize these these topics somehow?",
            "And So what I'm suggesting is to organize the topics in a tree.",
            "Which is something which is very natural for us, so this is been considered an.",
            "In fact it's being considered as a generalization of a latent allocation.",
            "So still in a Bayesian formulation with this case, in the nonparametric Bayesian formulation using.",
            "Objects like Harkle Chinese restaurant process and nested directly process.",
            "And which allows to basically learn by inference a topic model which has a tree which is tree structured.",
            "So what I'd like to propose or to ask is, can we obtain a similar model just using matrix factorization but adding some structure using structured regularization."
        ],
        [
            "So here is a suggestion of how to do this.",
            "What will consider the dictionary learning setting that I've mentioned now several times so far?",
            "And instead of putting string structure in the dictionary, which was the case for the modeling of faces, we put structure on the codes.",
            "So the idea is the following.",
            "So if we view this this tree here as our tree of topics, so each node is associated to a topic and at the same time you can think that the node is also associated to the coefficient that this topic has in the decomposition of a certain document, then what we would like to say is we would like to say that the topics that are at the bottom of the tree are more specific and the topics at the top.",
            "Or more general.",
            "And that for instance.",
            "If you use topic too, then you should also use topic one.",
            "You should only access the part of the dictionary if you've accessed topics that are more general than the specific topics that you want to reach.",
            "So.",
            "So this is a rule, so I want to enforce a situation where if I pick a certain topic, I want to pick all the ancestors of that topic.",
            "Cheap converse, the contrapositive of this statement is to say that if I don't use a topic, I should not not use any of this descendants, and this suggests a way to construct regularization norm which is going to be a sparsity inducing norm which has exactly the properties that we would like to have.",
            "So specifically, the groups I'm going to consider a norm on the decomposition coefficient corresponding to each.",
            "Document and this norm will be of this of this form.",
            "It's going to be a sum of L2 norms on blocks of Alphonse, some groups and the groups are going to be defined as follows, so the groups are groups that are depicted in dashed red lines.",
            "So these groups are exactly the group that are obtained by taking one node of the tree and putting in the same group all of its descendants and then doing this for all groups.",
            "Now if you think of what happens if, similar to what happened before a certain number of these groups are set to 0.",
            "So for example in this case, if the group.",
            "Containing two and four is set to 0 and the Group 6 is set to 0 while we're left with 135 which is.",
            "The path in the tree, so it's a special case of a rooted subtree of the tree, and so the regulation that we have proposed has sparsity patterns that are exactly all the rooted subtrees of the tree that we're considering."
        ],
        [
            "OK, so well, if we're going to use a regularization of this sort, then useful concern should be how do we optimize the corresponding.",
            "Victory learning problem.",
            "And, um.",
            "So this is an illustration of the fact that the methods that Francis has proposed.",
            "Are nice since the.",
            "We used in this setting and different methods, including the proximal methods that I'm going to talk again about now, can be used efficiently with that type of structured norm as well.",
            "So in particular, if we think of an alternating scheme or a block coordinate descent scheme that optimize in turn over the columns of D and over the vector of decomposition coefficients of each of the documents.",
            "So each of these Alpha eyes.",
            "Then if I focus on the optimization with respect to one of the vectors Alpha I and I use approximate methods proximal method.",
            "I don't detail the algebra here, but essentially the problem that you need to solve to use proximal methods is just to solve the little problem that I'm writing here, which is to find.",
            "The Alpha that is closest to your current point, which I call Y and which is traded off with the regularization on Alpha so.",
            "This is the proxamol problem associated to the proximal method.",
            "And if we can solve this problem efficiently, then we're in business because we will be able to use proximal methods to solve efficiently this."
        ],
        [
            "Dictionary learning problem and in fact so these groups have a tree structure and for tree structured set of groups it is possible to show.",
            "So what I could three structures of the groups instead of groups such that either two groups don't intersect or one of the two is contained in the others in the other, sorry.",
            "And in that case, the solution of the proximal problem that I showed before.",
            "Um?",
            "And is the proximal operator.",
            "So we need to compute basically this proximal operator and it can be shown that it can be obtained as a composition of simple proximal operator corresponding to each of the individual groups.",
            "So specifically what this result says, it says that if you sort the group in an order which is such that if a group is larger than another one, it comes after that first group in the order.",
            "And if you just apply.",
            "Approximate operator for the L2 norm corresponding to die group.",
            "So sort of groups of thresholding very similar to what Francis has written before and you just compose from all the for all the groups.",
            "Then you find the exact solution.",
            "What this shows is that this is not much more different than doing soft thresholding with yellow one norm.",
            "In particular, some thresholding for the one norm is cost as a cost, which is linear in the number of coefficients that you consider and hear.",
            "This algorithm is also near, so you might think that working with structured sparsity comes at a cost at algorithmic cost, which is which could be much larger than using simple sparsity.",
            "In fact, in this type of case, it's not true.",
            "We've got algorithms that are as efficient As for simple sparsity."
        ],
        [
            "OK, so let me illustrate this.",
            "So what we did is we considered the NIPS abstract from.",
            "I think a whole decade or less than that, which was 1714 documents containing 8274 different words.",
            "Then we chose a topology for the tree which is the topology that you see here with the root at the center and then four children and grandchildren.",
            "And what I'm showing here is so each of the node of the tree correspond to a dictionary element.",
            "And after we learn the dictionary, what is represented here is for each dictionary element, the five most frequent words that correspond to the associated topic.",
            "So what do we see when we see that the stop words are appearing at the root, which is fairly natural, and then according to maybe different topics of broad topics of the what can be found if you read the articles of that conference then things get split between, well, maybe what the Community, who is more focusing on.",
            "Realizing the behavior of real neurons work on.",
            "Maybe the community that works on online learning and reinforcement learning is represented in that quadrant.",
            "People who do things that look like what we've been talking about are here at the bottom left and the bottom right are people who are more interested by images and analog circuits.",
            "So I mean, this kid could give you suggestion if you need a title for your next article, just take this this.",
            "You know tree and then pick a word of the other route and then the side of a path in the tree and pick a word at each level that might be."
        ],
        [
            "Yeah, good title.",
            "So let me conclude.",
            "So here's a summary of what we've been talking about today.",
            "So first Francis talked about sparse linear estimation with L1 regularization.",
            "He's presented several.",
            "The connection framework in several algorithms, and then he's told you about various theoretical results that.",
            "Motivate or help us understand how to use these method methods.",
            "Well, then he's told you about group sparsity, presented block Norm and showed how there was a relationship between multiple kernel learning and group sparsity.",
            "In the case where the groups have infinite size.",
            "Then I told you about matrix sparsity and essentially talked about two types of sparsity, one sparsity, which is about.",
            "Our city at the on the Matrix, but taken as a structured vector which leads essentially to row sparsity, we should be distinguished in, which can be very useful in very multivariate learning problems such as multi task learning problems, and then problems that relate to low rank, which led us to consider PCA in dictionary learning.",
            "And finally I give you a couple of examples of what can be achieved with a structured sparsity with maybe a few more concrete examples which.",
            "Show what what problems can be tackled with this type of methods, and we've encountered two situation situation where the supports that we were looking at were stable by Union and some which were stable by intersection."
        ],
        [
            "Um?",
            "Maybe I repeat somehow some of the things that Francis has emphasized, but.",
            "Those would be some of the important message of this tutorial.",
            "Well, first, sparse methods are not limited to regression.",
            "There are plenty of problems where these sparsity can play an interesting role.",
            "I mean, of course, maybe this should not be pushed to the extreme the same way that you can kernelized many algorithm.",
            "You could specify many algorithms.",
            "Um?",
            "An important point is that.",
            "This type of methods will be very useful in high dimension and this is something that people don't have in mind sufficiently constantly when using this method.",
            "So the theory predicts that spot visits will perform well with very large number of predictors, and if N is larger than 100, having a number of predictors which is of.",
            "We look pretty close to 2.",
            "N gives a situation where there are basically no algorithms that can deal with this situation efficiently, at least for flat structured sparsity with with no structure in terms of performance.",
            "Francis reminded you that sparsity is not the key to get pretty performance.",
            "That will not always be the case, and that's parsed equipment to prior.",
            "So that if your problem is not is not, especially prior is not appropriate and it will not lead to increase performance.",
            "But very often it is a matter of how you look at your problem.",
            "Maybe there is a representation of the data that you're working with, a representation of the model that you want to work with, which should be sparse, but the sparsity might not be simple sparse.",
            "It might be some form of structured sparsity, and so this leads to.",
            "What was important about structured sparsity?",
            "Essentially, it gives you some models that are even more interpretable that then sparse models, and it suggests that maybe instead of using L1 or more using group norms for the specific problem that you're working on, you should construct your own norm.",
            "I construct your own sparsity inducing regulation that captures the structure of your problem."
        ],
        [
            "So finally, let me acknowledge several people so all our students in the Willow Group and their PhD student of Francis.",
            "Launch a Cup in our collaborator, John Ponce is also a carburetor and head of the group and collaborator on on many of the vision projects and Ben Taskar Matthewman Right and Michael Jordan are former mentors, so they made possible all of the things that we told you about or a large fraction of them."
        ],
        [
            "And I would like to thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem that I'd like to consider is a prob.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of middle metastases prognosis so basically.",
                    "label": 0
                },
                {
                    "sent": "We have a certain number of patients that have a tumor and we would like to be able to predict whether or not this tumor will metastase or not, right?",
                    "label": 1
                },
                {
                    "sent": "And the type of information that we have available to make this prediction is we have microarray data.",
                    "label": 1
                },
                {
                    "sent": "So the level of gene expression for a large number of genes.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that while we're predicting well, first.",
                    "label": 0
                },
                {
                    "sent": "The typical assumption that biologists would make is that the small number of genes should be associated with the possibility of having a metastasis that not the entire set of genes are modified by the fact that the cell is timorous or is likely to metastase.",
                    "label": 0
                },
                {
                    "sent": "So a small set of genes is likely to be predictive.",
                    "label": 0
                },
                {
                    "sent": "In finding this month, Medici jeans has to be have two advantages.",
                    "label": 0
                },
                {
                    "sent": "First, it is likely to increase.",
                    "label": 0
                },
                {
                    "sent": "The predictor, improved prediction accuracy and then it would be useful to biologist to identify what are the genes that are responsible for the metastasis.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One common issue, so one way of doing this is to build a classifier and to use some form of sparsity, which basically suggests that you should, such as like anyone.",
                    "label": 0
                },
                {
                    "sent": "The regulation to select a small number of genes.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But one issue that you encounter in practice is that this type of data is very noisy and Moreover genes that belong to same groups of genes that are active together that are called biological pathways.",
                    "label": 0
                },
                {
                    "sent": "Are likely to be very correlated, so identifying the right set of group is quite difficult and one thing which is natural is to consider directly those biological pathways which are anyway the type of information that are relevant to biologists so far, biologists, if you give them a set of genes that are predictive of tumor or metastasis where they will try to understand is what are the biological pathways that these genes are.",
                    "label": 0
                },
                {
                    "sent": "Associated to.",
                    "label": 0
                },
                {
                    "sent": "So why am I considering this example?",
                    "label": 0
                },
                {
                    "sent": "Well, because so.",
                    "label": 0
                },
                {
                    "sent": "Magical pathways have the interesting property that so they gather genes that correspond to the same logical mechanism they contain, often very correlated genes that are going to be difficult to tell apart.",
                    "label": 1
                },
                {
                    "sent": "If we're using L1 regularization and the pathway form groups that are overlapping.",
                    "label": 0
                },
                {
                    "sent": "So in what?",
                    "label": 0
                },
                {
                    "sent": "We have considered so far the groups when we consider groups where disjoint.",
                    "label": 0
                },
                {
                    "sent": "So what happens where the group overlap, and in particular what the information that we have in this application is that we would like to be able to select entire pathways so that the support of our model should be a union of pathways.",
                    "label": 1
                },
                {
                    "sent": "So a union of overlapping group.",
                    "label": 0
                },
                {
                    "sent": "So why would that be difficult?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's step back for a second and look at what was happening for the lasso in the group lasso.",
                    "label": 0
                },
                {
                    "sent": "So for LASSO, the L1 norm is a non differentiable regularizer and because of its structure a certain number of individual coefficients.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are set to 0 because essentially the.",
                    "label": 0
                },
                {
                    "sent": "You get trapped by optimizing in the non differentiability of the function.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the case of the group Lasso, again a certain number of groups are.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To 0.",
                    "label": 0
                },
                {
                    "sent": "And therefore the support that you estimate is a union of group.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if we consider the case of the group lasso with overlap where we can still use the same regularizer, the sum over a certain number of groups of WG, and So what?",
                    "label": 0
                },
                {
                    "sent": "I denote by that now is that WG is the sub vector of W, which has entries indexed by the Group G, and so this norm is the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "Or it could possibly be another LQ norm.",
                    "label": 0
                },
                {
                    "sent": "And I'm thinking the sum over order groups in G so I can still consider this regularization, which is the one of the group lasso or DL102 regularization.",
                    "label": 0
                },
                {
                    "sent": "It is actually still a norm even if the different groups overlap, but what happens is that what this regulation is going to induce is that a certain number of groups are going to be set to 0.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if I start setting group to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So like this or like this?",
                    "label": 0
                },
                {
                    "sent": "What you notice is that basically what is left is not any longer union of the groups that I have defined.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And but in fact it is an intersection of the complement of certain subset of the groups that have been set to 0.",
                    "label": 0
                },
                {
                    "sent": "So how can we address this?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, but the idea is to use.",
                    "label": 0
                },
                {
                    "sent": "A latent group lasso.",
                    "label": 0
                },
                {
                    "sent": "What do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "So if, let's say we're trying to solve a classification problem so.",
                    "label": 0
                },
                {
                    "sent": "We have performed, for example logistic regression with the parameter vector W. What I can do is I can decompose this vector W in a certain number of vectors that I'm going to define as follow for each group I'm going to introduce a latent vector V1 for Group One, V2 for Group 2, and V3 for Group 3.",
                    "label": 0
                },
                {
                    "sent": "And if you want is going to be allowed to be non zero only on Groupon and then it will be forced to be 0 everywhere else.",
                    "label": 0
                },
                {
                    "sent": "V2 is going to be non zero only on group two and three is going to be 1 zero only on Group 3.",
                    "label": 0
                },
                {
                    "sent": "What I do is, I'd say that W the vector W results from the superposition of these three vectors, so a linear combination.",
                    "label": 0
                },
                {
                    "sent": "And what they will regularize into the regularising WI regularized latent parameter vector.",
                    "label": 0
                },
                {
                    "sent": "So let's say that I want to solve my problem of.",
                    "label": 0
                },
                {
                    "sent": "Miss test this prediction, so I might want to solve a.",
                    "label": 0
                },
                {
                    "sent": "Learning problem which is classification problem.",
                    "label": 0
                },
                {
                    "sent": "So my empirical risk would be the empirical risk based on the risk class for example.",
                    "label": 0
                },
                {
                    "sent": "And within regularize is the sum of the L2 norms of each of the VI vectors, with the constraint Now that these should combine linearly to form W. So.",
                    "label": 0
                },
                {
                    "sent": "If you believe me that basically the situation is the same as before, and that a certain number of these groups are going to be set to 0, basically what happens is that a certain number of these latent vectors are going to be set to 0.",
                    "label": 0
                },
                {
                    "sent": "So let's say V3 is set to 0, then W results from the linear combination of V1 and V2.",
                    "label": 0
                },
                {
                    "sent": "And it is therefore a vector that has support with the same support as the Union of the support of V1 and the support of Y two.",
                    "label": 0
                },
                {
                    "sent": "So it's a union of groups.",
                    "label": 1
                },
                {
                    "sent": "And So what is nice as well is that it is but now possible to select one variables without selecting all the groups that are associated that are containing it, which was not the case with the naive extension of.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reply so.",
                    "label": 0
                },
                {
                    "sent": "What is perhaps even more interesting is that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This formulation proposes introduces a new type of regular of regularizer on W. So let me explain this.",
                    "label": 0
                },
                {
                    "sent": "Essentially this is the same optimization problem as the one that I presented before and what you can consider is to basically marginalized out if I can say the latent variables V by defining a function Omega of W which is.",
                    "label": 0
                },
                {
                    "sent": "The minimum of an optimization problem on the latent variables V. So this defines for each W. This defines a function on W and I can use this function as a regularizer.",
                    "label": 0
                },
                {
                    "sent": "In this problem is actually exactly equivalent to the to the previous problem.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what maybe perhaps surprising is that this actually defines a norm on W, so this is a new norm W. And to get a sense of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is happening?",
                    "label": 0
                },
                {
                    "sent": "Let me show you what the unit balls of different norms, what they look like.",
                    "label": 0
                },
                {
                    "sent": "So if you remember, this is the L1L2 norm that forces were showing you.",
                    "label": 0
                },
                {
                    "sent": "In the first part of the tutorial corresponding to.",
                    "label": 0
                },
                {
                    "sent": "Um, so a tree dimensional vector W1W 2W3 and where the groups where the group of variables 1, two and the variable tree.",
                    "label": 0
                },
                {
                    "sent": "So the parts of the world that are non smooth are the tip where W 3 is non 0 N W1W2S0 and the circle where it is the opposite.",
                    "label": 0
                },
                {
                    "sent": "Now here is the unit ball corresponding to.",
                    "label": 0
                },
                {
                    "sent": "Group the naive generalization of group lasso with groups 1, two and Group 2.",
                    "label": 0
                },
                {
                    "sent": "Three where you take the sum of the L2 norm of W12 and W23.",
                    "label": 0
                },
                {
                    "sent": "And in that case, what you see is that this the ball has very few.",
                    "label": 0
                },
                {
                    "sent": "And very few non smooth points.",
                    "label": 0
                },
                {
                    "sent": "In fact it has only these four corners that are non smooth that correspond to selecting alone W 3 or selecting sorting alone W one or second alone W 3.",
                    "label": 0
                },
                {
                    "sent": "And the Norm Omega that I've just defined is actually this norm, which has the nice property that it is exactly can be shown.",
                    "label": 0
                },
                {
                    "sent": "It is exactly the convex Hull of the two disks that correspond to the two different groups.",
                    "label": 0
                },
                {
                    "sent": "So again, this is the norm that is corresponding to trying to obtain supports that are the Union of Group 1, two and Group 2, three, and so the group went to.",
                    "label": 0
                },
                {
                    "sent": "Being non 0.",
                    "label": 0
                },
                {
                    "sent": "Corresponds to.",
                    "label": 0
                },
                {
                    "sent": "And the horizontal circle and then W30 and then W 2, three being non zero correspond to the vertical circle and so this.",
                    "label": 0
                },
                {
                    "sent": "This unit ball is just the convex Hull of these of these circles.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is an application of this type.",
                    "label": 0
                },
                {
                    "sent": "This new form of regularization to the metastasis problem, and.",
                    "label": 0
                },
                {
                    "sent": "What we obtained in this case is that so we compared essentially two types of regularization regularization with the L1 norm and regulation with this norm that induces sparsity at the level of pathways and in terms of misclassification error, we observe small improvement which is not significant, But if you consider the number of pathways involved in the solution, you see that the number pathways, so those those three numbers correspond to three different.",
                    "label": 1
                },
                {
                    "sent": "Replicates of the experiments using like 3 different datasets.",
                    "label": 0
                },
                {
                    "sent": "Examples of simple datasets, and in that case you get a very large number of pathways, whereas in this case you very often get a small number of pathways which are going to lead to something that's much more interpretable for the biologist.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, let's talk about sparse structure.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "PCA.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, we talked before about sparse PCA and I mentioned the fact that sparse PCA and dictionary learning are closely related.",
                    "label": 1
                },
                {
                    "sent": "But then somehow, now that we've started looking at.",
                    "label": 0
                },
                {
                    "sent": "More structured regularization.",
                    "label": 0
                },
                {
                    "sent": "We might want to add more structure to the constraint that we want to put on either on the dictionary elements or the principal components of the participant analysis or on the decomposition coefficients.",
                    "label": 0
                },
                {
                    "sent": "So what other constraints can?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Consider it well.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can retrieve with different types of constraints on victory learning or or PCA.",
                    "label": 0
                },
                {
                    "sent": "Alot of algorithms that are familiar.",
                    "label": 0
                },
                {
                    "sent": "So if you just want to find an approximation where the good low rank approximation, you just need to use unv with a few columns.",
                    "label": 0
                },
                {
                    "sent": "I've mentioned that before.",
                    "label": 0
                },
                {
                    "sent": "So sparse PCA, tree learning U&V as many zeros.",
                    "label": 1
                },
                {
                    "sent": "I've talked about that before, but you can also retrieve K means.",
                    "label": 0
                },
                {
                    "sent": "If you enforce that you is a binary matrix such that the sum of the elements on each column is 1.",
                    "label": 0
                },
                {
                    "sent": "And you can retrieve non negative matrix factorization if you constrains the coefficient to be positive and so on.",
                    "label": 1
                },
                {
                    "sent": "And I'll give other examples later.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what am I going to call structured sparse PCA?",
                    "label": 0
                },
                {
                    "sent": "Well, so I'm now using the notations from signal processing.",
                    "label": 0
                },
                {
                    "sent": "So in sparse PCA what we had is we had a matrix D of dictionary elements that correspond to the principle components and we were regularising.",
                    "label": 0
                },
                {
                    "sent": "Each of these columns of D that there I denote DJ by.",
                    "label": 0
                },
                {
                    "sent": "The N1 norm so that these.",
                    "label": 0
                },
                {
                    "sent": "Columns are sparse and then I was constraining the coefficients to avoid to get a digital solution where the coefficients would blow up in the size of the the norm of the dictionaries with shrink to 0.",
                    "label": 0
                },
                {
                    "sent": "So what I want to call sparse structure PCA Now is the generalization of this problem where I would like to enforce.",
                    "label": 0
                },
                {
                    "sent": "I would like to consider more sophisticated structure on the dictionary elements.",
                    "label": 0
                },
                {
                    "sent": "So again, this will lead to situations where the diction will not be orthogonal.",
                    "label": 0
                },
                {
                    "sent": "Again, it's not going to be jointly convex, but convex in each of DJ and Alpha J, which will lead to efficient block coordinate descent algorithms.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, um what concrete problem do I have in mind?",
                    "label": 0
                },
                {
                    "sent": "Well, so a problem that has been of interest in the vision community is to find good decomposition for faces.",
                    "label": 0
                },
                {
                    "sent": "So good decomposition means can can have several meanings, so can be good because you are able to perform very well supervised task based on this decomposition.",
                    "label": 0
                },
                {
                    "sent": "And it can also mean that you retrieve some of the latent structure.",
                    "label": 0
                },
                {
                    "sent": "Of faces.",
                    "label": 0
                },
                {
                    "sent": "So when the type of decomposition that was used already a couple of decades ago was eigenfaces, which essentially correspond to taking the image, the database formed by the images of the faces and applying principle component analysis on it.",
                    "label": 0
                },
                {
                    "sent": "But what was a bit disappointing for some people in the result of eigenfaces that obviously each of the eigen phases picture can be viewed as a picture that covers the entire face.",
                    "label": 0
                },
                {
                    "sent": "And that one thing that you might want to retrieve when you decompose cases to decompose phase in natural parts like eyes, nose, mouth and things like that.",
                    "label": 0
                },
                {
                    "sent": "So this is actually what made the idea of Lee and Seung to apply non negative matrix factorization too.",
                    "label": 1
                },
                {
                    "sent": "Two face decomposition is very popular is that they were showing in their paper that using a factorization that was constraining both the dictionary elements to be to have positive coefficients in the decomposition coefficient to be positive, they were able to identify localized parts of the face.",
                    "label": 0
                },
                {
                    "sent": "So eyebrows, eyes and so on.",
                    "label": 0
                },
                {
                    "sent": "So the database that I'm showing here is the database that we used, which has a slightly higher resolution than the database they considered and.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we use NMF, we obtain something that looks like it tends to be sparse, but which is not so sparse.",
                    "label": 0
                },
                {
                    "sent": "So you can see that there are some bright parts in the image which suggests that if you were doing some very aggressive thresholding you could get sparse portions that maybe for some of them might be localized.",
                    "label": 0
                },
                {
                    "sent": "But essentially you get something that's not quite sparse and not that well localized.",
                    "label": 0
                },
                {
                    "sent": "So one idea is to use prior knowledge to actually enforce to basically say that, well, we know that the part that we would like to find are parts that should be localized on the face.",
                    "label": 0
                },
                {
                    "sent": "Can we use structured sparsity to enforce such a prior?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the structure to suggesting to do this.",
                    "label": 1
                },
                {
                    "sent": "And consider that these grids that I'm showing each of these grid is actually corresponding to an image, so it's an image that has five by five pixels by 5 pixels.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "I'm going to consider the following regularization on it.",
                    "label": 0
                },
                {
                    "sent": "So each of these image you can think of as being one of the dictionary elements.",
                    "label": 1
                },
                {
                    "sent": "So one vector in the matrix factorization problem, but represented here on the image, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm treating each image as a vector, but of course it is very natural to represent these vectors as images and in this case it's a 5 by 5 image.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to consider the regularization which is the sum of the L2 norms.",
                    "label": 0
                },
                {
                    "sent": "Of some groups of pixels and the groups of pixels that I'm going to consider, I mean of the dictionary element values corresponding to a certain number of pixels.",
                    "label": 0
                },
                {
                    "sent": "And the groups that consider are the groups in blue.",
                    "label": 0
                },
                {
                    "sent": "Which are groups that are to the left of a vertical line and then their compliments.",
                    "label": 1
                },
                {
                    "sent": "So the groups that are to the right of a vertical line and then consider also the groups that are above certain horizontal line and below horizontal line.",
                    "label": 0
                },
                {
                    "sent": "So if I regularize by the sum of these groups a.",
                    "label": 0
                },
                {
                    "sent": "An image that has that shape and if I set if I therefore certain number of groups to zero, what's going to happen is I'll get something situation which is like what I have on the right.",
                    "label": 0
                },
                {
                    "sent": "So if I set this group to 0, this group to 0, this group zero and this group to 0.",
                    "label": 0
                },
                {
                    "sent": "So the group that are above below to the left and to the right of the four lines that I've drawn.",
                    "label": 0
                },
                {
                    "sent": "What I'm left with is a rectangular region, so this regularization with a carefully set of groups actually gives me dictionary elements who supports are exactly the rectangular, all the rectangular regions of the grid.",
                    "label": 0
                },
                {
                    "sent": "Of the image.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can go even further and we can consider now groups which are defined by diagonal lines, so I can take all the groups that are above an ascending diagonal 9 or descending down the line or below that same line and then if I combine this with the groups that I had before, then I will start to get shapes that are a little closer to general convex shapes in the image and adding more lines you would be able to approximate pretty much all of the convex shapes that you have in the image.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to use this norm in a sparse structured PCA formulation, where this is going to be the structured norm.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Again, in the formulation that I wrote before, this will correspond to using this regularization on each of the dictionary elements that I introduce.",
                    "label": 1
                },
                {
                    "sent": "So now this regularization is.",
                    "label": 0
                },
                {
                    "sent": "Is a bit more complicated than the ones that we've considered before in particular.",
                    "label": 0
                },
                {
                    "sent": "If Francis said that one of the characteristic of the of the L1 norm man of the L1L2 norm, the block norms is that they are separable and that makes it that coordinate descent algorithms or block coordinate descent of them are convergent.",
                    "label": 0
                },
                {
                    "sent": "In this case, the norm that I have introduced is not separable, so those algorithms are excluded.",
                    "label": 0
                },
                {
                    "sent": "And using proximal algorithm in this case is also difficult.",
                    "label": 0
                },
                {
                    "sent": "So one way to approach to tackle this problem is to use the reweighted scheme, which Fortunately is still a possibility in this case.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me show you what happened.",
                    "label": 0
                },
                {
                    "sent": "So this is a database that has the following characteristic that.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit which is made of half man half woman composed of 100 individuals, and for each individual we have 14 non included images and 12 occluded images.",
                    "label": 1
                },
                {
                    "sent": "So the type of occlusion that we have, our glasses or scarves, or sometimes both both and.",
                    "label": 0
                },
                {
                    "sent": "Obviously you can see one of the advantages of learning a representation of the data which is localized is that one can imagine that this representation will be robust to some extent to occlusion, because if the database is localized then sorry if the dictionary elements are localized, then the projection on these documents of the subparts of the image will not be altered by the fact that some part of the images.",
                    "label": 0
                },
                {
                    "sent": "Is included and so one of the characteristic of this database is that it has lateral.",
                    "label": 0
                },
                {
                    "sent": "Lateral eliminations are main factor of variance in this data.",
                    "label": 0
                },
                {
                    "sent": "So we worked with this representation that I'm showing here, which is slightly reduced respect to the origonal.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Representation.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing here is the dictionaries that are obtained if you use sparse PCA on the left and structured sparse PCA on the right, so the dictionary elements are these images that are sorted from top left to bottom right.",
                    "label": 0
                },
                {
                    "sent": "By this decreased order of explained variance.",
                    "label": 0
                },
                {
                    "sent": "And to make this figure maybe a little more readable, let me show you in.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Green.",
                    "label": 0
                },
                {
                    "sent": "What are the zero coefficients?",
                    "label": 0
                },
                {
                    "sent": "So what you can see is that sparse PCA is definitely sparse, but it's quite a bit scattered, and it's difficult to decide.",
                    "label": 0
                },
                {
                    "sent": "You know if you obtain something that correspond to parts, whereas obviously spectral PCA by construction is retrieving.",
                    "label": 0
                },
                {
                    "sent": "Portions of the face that are convex.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you see that the two main dictionary elements that you retrieve are the ones that main principle components are the one that explain the left right illumination and then some of these.",
                    "label": 0
                },
                {
                    "sent": "Um components are corresponding to eyes or nose or some parts of the face, and obviously some of them are actually capturing parts that are larger.",
                    "label": 0
                },
                {
                    "sent": "Um, so should we be happy with this?",
                    "label": 0
                },
                {
                    "sent": "I mean, in a sense, this is a pleasant perhaps to look at, but is this a good representation of the data?",
                    "label": 0
                },
                {
                    "sent": "Is this really a good idea to try and find parts?",
                    "label": 0
                },
                {
                    "sent": "Well so it depends what you like to do.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in supervised problem, maybe whether or not this is a good.",
                    "label": 0
                },
                {
                    "sent": "Representation should be determined by the performance of combining this obtained representation with using it to solve a supervised.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem so this is where we consider here where basically we try to do face recognition.",
                    "label": 0
                },
                {
                    "sent": "On the occluded images.",
                    "label": 0
                },
                {
                    "sent": "So we use the non included images to learn the.",
                    "label": 0
                },
                {
                    "sent": "The dictionary on the right and then we projected the data that was included on the same dictionary and use the coefficient obtained to classify the corresponding images using nearest neighbor and.",
                    "label": 0
                },
                {
                    "sent": "Long story short, I don't have too much time to describe in details all these curves, but as the dictionary size is increasing, the approaches in black and in red that correspond to using structured.",
                    "label": 0
                },
                {
                    "sent": "Dictionary elements are actually performing significantly better than the other methods, including PCA, NMF, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let me.",
                    "label": 0
                },
                {
                    "sent": "Um, tell you about.",
                    "label": 0
                },
                {
                    "sent": "Particle dictionary learning which.",
                    "label": 0
                },
                {
                    "sent": "Have you I view and we view as very exciting?",
                    "label": 0
                },
                {
                    "sent": "Direction that that is a interesting way to use truck.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sparsity, so here is a situation where you would like to use Heartical dictionary, or specifically a hierarchical topic model.",
                    "label": 0
                },
                {
                    "sent": "So what you're interested in is modeling text corpora so you have a database of documents, and each of these document is represented by a vector exchange that has dimension the number of.",
                    "label": 0
                },
                {
                    "sent": "Words in the dictionary.",
                    "label": 0
                },
                {
                    "sent": "And the entries in XJ count the number of occurrences of that word in the document.",
                    "label": 1
                },
                {
                    "sent": "Obviously the sum of the total sum of the number of words of each type that appear is the total number of words of the document and.",
                    "label": 0
                },
                {
                    "sent": "A natural way to model these documents represented that way is to actually say that.",
                    "label": 0
                },
                {
                    "sent": "Those documents are viewed as as the counts of words are generated as a large multinomial distribution, but with some structure.",
                    "label": 0
                },
                {
                    "sent": "So what kind of structure can we consider?",
                    "label": 0
                },
                {
                    "sent": "Well, we can consider that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Words in the document correspond to different topics and that topics are.",
                    "label": 0
                },
                {
                    "sent": "Each topic has its own lexicon, its own.",
                    "label": 0
                },
                {
                    "sent": "Lexical field.",
                    "label": 0
                },
                {
                    "sent": "So it has its own distribution over words.",
                    "label": 0
                },
                {
                    "sent": "And so each of these distribution over words I'm going to view them as a column of the matrix D. And each document is going to have some topic proportions, so a certain proportion of you.",
                    "label": 0
                },
                {
                    "sent": "All of these topics, and therefore the.",
                    "label": 0
                },
                {
                    "sent": "The resulting document will be obtained by taking a linear combination of the columns of the corresponding to using each of the specific dictionaries corresponding to each topic in the proportions of how much these topics are used in the document, and then sampling the multinomial vector from it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This seems to be the appropriate setting to public framework to think of a low rank factorization of a Word document matrix, and this can be viewed as some form of PCA, but associated with a multinomial likelihood.",
                    "label": 1
                },
                {
                    "sent": "So you can call this multinomial PCA.",
                    "label": 0
                },
                {
                    "sent": "Maybe the model that many of you are more familiar with and that attempts to model the same type of data is latent digital allocation which.",
                    "label": 0
                },
                {
                    "sent": "Essentially is a beige and take on the same model which puts a prior on the topic proportions, namely directly prior.",
                    "label": 0
                },
                {
                    "sent": "Since it is conjugate to the multinomial prior, which has been very popular both to model text corpora but also to model.",
                    "label": 0
                },
                {
                    "sent": "To model images in computer vision using bag of features instead of bag of words.",
                    "label": 0
                },
                {
                    "sent": "So what I've described here is a model that.",
                    "label": 0
                },
                {
                    "sent": "Decomposers essentially a document into certain number of topics.",
                    "label": 0
                },
                {
                    "sent": "But it is very natural to think of these topics as or as being organized in some sort of hierarchy of our ontology.",
                    "label": 0
                },
                {
                    "sent": "If you will, right so when you read a document, you might find that some of the topics in that document are quite general and some are more specific, and so if we're going to build a model that automatically identifies different topics that are present in documents, then why not try and organize these these topics somehow?",
                    "label": 0
                },
                {
                    "sent": "And So what I'm suggesting is to organize the topics in a tree.",
                    "label": 1
                },
                {
                    "sent": "Which is something which is very natural for us, so this is been considered an.",
                    "label": 0
                },
                {
                    "sent": "In fact it's being considered as a generalization of a latent allocation.",
                    "label": 0
                },
                {
                    "sent": "So still in a Bayesian formulation with this case, in the nonparametric Bayesian formulation using.",
                    "label": 0
                },
                {
                    "sent": "Objects like Harkle Chinese restaurant process and nested directly process.",
                    "label": 1
                },
                {
                    "sent": "And which allows to basically learn by inference a topic model which has a tree which is tree structured.",
                    "label": 0
                },
                {
                    "sent": "So what I'd like to propose or to ask is, can we obtain a similar model just using matrix factorization but adding some structure using structured regularization.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a suggestion of how to do this.",
                    "label": 0
                },
                {
                    "sent": "What will consider the dictionary learning setting that I've mentioned now several times so far?",
                    "label": 0
                },
                {
                    "sent": "And instead of putting string structure in the dictionary, which was the case for the modeling of faces, we put structure on the codes.",
                    "label": 0
                },
                {
                    "sent": "So the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "So if we view this this tree here as our tree of topics, so each node is associated to a topic and at the same time you can think that the node is also associated to the coefficient that this topic has in the decomposition of a certain document, then what we would like to say is we would like to say that the topics that are at the bottom of the tree are more specific and the topics at the top.",
                    "label": 0
                },
                {
                    "sent": "Or more general.",
                    "label": 0
                },
                {
                    "sent": "And that for instance.",
                    "label": 0
                },
                {
                    "sent": "If you use topic too, then you should also use topic one.",
                    "label": 0
                },
                {
                    "sent": "You should only access the part of the dictionary if you've accessed topics that are more general than the specific topics that you want to reach.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is a rule, so I want to enforce a situation where if I pick a certain topic, I want to pick all the ancestors of that topic.",
                    "label": 0
                },
                {
                    "sent": "Cheap converse, the contrapositive of this statement is to say that if I don't use a topic, I should not not use any of this descendants, and this suggests a way to construct regularization norm which is going to be a sparsity inducing norm which has exactly the properties that we would like to have.",
                    "label": 0
                },
                {
                    "sent": "So specifically, the groups I'm going to consider a norm on the decomposition coefficient corresponding to each.",
                    "label": 0
                },
                {
                    "sent": "Document and this norm will be of this of this form.",
                    "label": 0
                },
                {
                    "sent": "It's going to be a sum of L2 norms on blocks of Alphonse, some groups and the groups are going to be defined as follows, so the groups are groups that are depicted in dashed red lines.",
                    "label": 0
                },
                {
                    "sent": "So these groups are exactly the group that are obtained by taking one node of the tree and putting in the same group all of its descendants and then doing this for all groups.",
                    "label": 0
                },
                {
                    "sent": "Now if you think of what happens if, similar to what happened before a certain number of these groups are set to 0.",
                    "label": 0
                },
                {
                    "sent": "So for example in this case, if the group.",
                    "label": 0
                },
                {
                    "sent": "Containing two and four is set to 0 and the Group 6 is set to 0 while we're left with 135 which is.",
                    "label": 0
                },
                {
                    "sent": "The path in the tree, so it's a special case of a rooted subtree of the tree, and so the regulation that we have proposed has sparsity patterns that are exactly all the rooted subtrees of the tree that we're considering.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so well, if we're going to use a regularization of this sort, then useful concern should be how do we optimize the corresponding.",
                    "label": 0
                },
                {
                    "sent": "Victory learning problem.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "So this is an illustration of the fact that the methods that Francis has proposed.",
                    "label": 0
                },
                {
                    "sent": "Are nice since the.",
                    "label": 0
                },
                {
                    "sent": "We used in this setting and different methods, including the proximal methods that I'm going to talk again about now, can be used efficiently with that type of structured norm as well.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if we think of an alternating scheme or a block coordinate descent scheme that optimize in turn over the columns of D and over the vector of decomposition coefficients of each of the documents.",
                    "label": 0
                },
                {
                    "sent": "So each of these Alpha eyes.",
                    "label": 0
                },
                {
                    "sent": "Then if I focus on the optimization with respect to one of the vectors Alpha I and I use approximate methods proximal method.",
                    "label": 0
                },
                {
                    "sent": "I don't detail the algebra here, but essentially the problem that you need to solve to use proximal methods is just to solve the little problem that I'm writing here, which is to find.",
                    "label": 0
                },
                {
                    "sent": "The Alpha that is closest to your current point, which I call Y and which is traded off with the regularization on Alpha so.",
                    "label": 0
                },
                {
                    "sent": "This is the proxamol problem associated to the proximal method.",
                    "label": 0
                },
                {
                    "sent": "And if we can solve this problem efficiently, then we're in business because we will be able to use proximal methods to solve efficiently this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dictionary learning problem and in fact so these groups have a tree structure and for tree structured set of groups it is possible to show.",
                    "label": 1
                },
                {
                    "sent": "So what I could three structures of the groups instead of groups such that either two groups don't intersect or one of the two is contained in the others in the other, sorry.",
                    "label": 1
                },
                {
                    "sent": "And in that case, the solution of the proximal problem that I showed before.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And is the proximal operator.",
                    "label": 1
                },
                {
                    "sent": "So we need to compute basically this proximal operator and it can be shown that it can be obtained as a composition of simple proximal operator corresponding to each of the individual groups.",
                    "label": 1
                },
                {
                    "sent": "So specifically what this result says, it says that if you sort the group in an order which is such that if a group is larger than another one, it comes after that first group in the order.",
                    "label": 0
                },
                {
                    "sent": "And if you just apply.",
                    "label": 0
                },
                {
                    "sent": "Approximate operator for the L2 norm corresponding to die group.",
                    "label": 0
                },
                {
                    "sent": "So sort of groups of thresholding very similar to what Francis has written before and you just compose from all the for all the groups.",
                    "label": 0
                },
                {
                    "sent": "Then you find the exact solution.",
                    "label": 0
                },
                {
                    "sent": "What this shows is that this is not much more different than doing soft thresholding with yellow one norm.",
                    "label": 0
                },
                {
                    "sent": "In particular, some thresholding for the one norm is cost as a cost, which is linear in the number of coefficients that you consider and hear.",
                    "label": 0
                },
                {
                    "sent": "This algorithm is also near, so you might think that working with structured sparsity comes at a cost at algorithmic cost, which is which could be much larger than using simple sparsity.",
                    "label": 0
                },
                {
                    "sent": "In fact, in this type of case, it's not true.",
                    "label": 0
                },
                {
                    "sent": "We've got algorithms that are as efficient As for simple sparsity.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me illustrate this.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we considered the NIPS abstract from.",
                    "label": 0
                },
                {
                    "sent": "I think a whole decade or less than that, which was 1714 documents containing 8274 different words.",
                    "label": 1
                },
                {
                    "sent": "Then we chose a topology for the tree which is the topology that you see here with the root at the center and then four children and grandchildren.",
                    "label": 0
                },
                {
                    "sent": "And what I'm showing here is so each of the node of the tree correspond to a dictionary element.",
                    "label": 0
                },
                {
                    "sent": "And after we learn the dictionary, what is represented here is for each dictionary element, the five most frequent words that correspond to the associated topic.",
                    "label": 0
                },
                {
                    "sent": "So what do we see when we see that the stop words are appearing at the root, which is fairly natural, and then according to maybe different topics of broad topics of the what can be found if you read the articles of that conference then things get split between, well, maybe what the Community, who is more focusing on.",
                    "label": 0
                },
                {
                    "sent": "Realizing the behavior of real neurons work on.",
                    "label": 0
                },
                {
                    "sent": "Maybe the community that works on online learning and reinforcement learning is represented in that quadrant.",
                    "label": 0
                },
                {
                    "sent": "People who do things that look like what we've been talking about are here at the bottom left and the bottom right are people who are more interested by images and analog circuits.",
                    "label": 0
                },
                {
                    "sent": "So I mean, this kid could give you suggestion if you need a title for your next article, just take this this.",
                    "label": 0
                },
                {
                    "sent": "You know tree and then pick a word of the other route and then the side of a path in the tree and pick a word at each level that might be.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, good title.",
                    "label": 0
                },
                {
                    "sent": "So let me conclude.",
                    "label": 0
                },
                {
                    "sent": "So here's a summary of what we've been talking about today.",
                    "label": 0
                },
                {
                    "sent": "So first Francis talked about sparse linear estimation with L1 regularization.",
                    "label": 1
                },
                {
                    "sent": "He's presented several.",
                    "label": 0
                },
                {
                    "sent": "The connection framework in several algorithms, and then he's told you about various theoretical results that.",
                    "label": 0
                },
                {
                    "sent": "Motivate or help us understand how to use these method methods.",
                    "label": 0
                },
                {
                    "sent": "Well, then he's told you about group sparsity, presented block Norm and showed how there was a relationship between multiple kernel learning and group sparsity.",
                    "label": 1
                },
                {
                    "sent": "In the case where the groups have infinite size.",
                    "label": 1
                },
                {
                    "sent": "Then I told you about matrix sparsity and essentially talked about two types of sparsity, one sparsity, which is about.",
                    "label": 0
                },
                {
                    "sent": "Our city at the on the Matrix, but taken as a structured vector which leads essentially to row sparsity, we should be distinguished in, which can be very useful in very multivariate learning problems such as multi task learning problems, and then problems that relate to low rank, which led us to consider PCA in dictionary learning.",
                    "label": 0
                },
                {
                    "sent": "And finally I give you a couple of examples of what can be achieved with a structured sparsity with maybe a few more concrete examples which.",
                    "label": 0
                },
                {
                    "sent": "Show what what problems can be tackled with this type of methods, and we've encountered two situation situation where the supports that we were looking at were stable by Union and some which were stable by intersection.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Maybe I repeat somehow some of the things that Francis has emphasized, but.",
                    "label": 0
                },
                {
                    "sent": "Those would be some of the important message of this tutorial.",
                    "label": 0
                },
                {
                    "sent": "Well, first, sparse methods are not limited to regression.",
                    "label": 1
                },
                {
                    "sent": "There are plenty of problems where these sparsity can play an interesting role.",
                    "label": 0
                },
                {
                    "sent": "I mean, of course, maybe this should not be pushed to the extreme the same way that you can kernelized many algorithm.",
                    "label": 0
                },
                {
                    "sent": "You could specify many algorithms.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "An important point is that.",
                    "label": 0
                },
                {
                    "sent": "This type of methods will be very useful in high dimension and this is something that people don't have in mind sufficiently constantly when using this method.",
                    "label": 0
                },
                {
                    "sent": "So the theory predicts that spot visits will perform well with very large number of predictors, and if N is larger than 100, having a number of predictors which is of.",
                    "label": 0
                },
                {
                    "sent": "We look pretty close to 2.",
                    "label": 0
                },
                {
                    "sent": "N gives a situation where there are basically no algorithms that can deal with this situation efficiently, at least for flat structured sparsity with with no structure in terms of performance.",
                    "label": 0
                },
                {
                    "sent": "Francis reminded you that sparsity is not the key to get pretty performance.",
                    "label": 0
                },
                {
                    "sent": "That will not always be the case, and that's parsed equipment to prior.",
                    "label": 0
                },
                {
                    "sent": "So that if your problem is not is not, especially prior is not appropriate and it will not lead to increase performance.",
                    "label": 1
                },
                {
                    "sent": "But very often it is a matter of how you look at your problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe there is a representation of the data that you're working with, a representation of the model that you want to work with, which should be sparse, but the sparsity might not be simple sparse.",
                    "label": 1
                },
                {
                    "sent": "It might be some form of structured sparsity, and so this leads to.",
                    "label": 0
                },
                {
                    "sent": "What was important about structured sparsity?",
                    "label": 0
                },
                {
                    "sent": "Essentially, it gives you some models that are even more interpretable that then sparse models, and it suggests that maybe instead of using L1 or more using group norms for the specific problem that you're working on, you should construct your own norm.",
                    "label": 0
                },
                {
                    "sent": "I construct your own sparsity inducing regulation that captures the structure of your problem.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally, let me acknowledge several people so all our students in the Willow Group and their PhD student of Francis.",
                    "label": 0
                },
                {
                    "sent": "Launch a Cup in our collaborator, John Ponce is also a carburetor and head of the group and collaborator on on many of the vision projects and Ben Taskar Matthewman Right and Michael Jordan are former mentors, so they made possible all of the things that we told you about or a large fraction of them.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I would like to thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}