{
    "id": "2xfl7g74sx7n3ep4m7fqrazp6guojrl2",
    "title": "Magic Moments: Moment-based Approaches to Structured Output Prediction",
    "info": {
        "author": [
            "Elisa Ricci, University of Perugia"
        ],
        "published": "Dec. 14, 2007",
        "recorded": "October 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Output"
        ]
    },
    "url": "http://videolectures.net/aop07_ricci_mmm/",
    "segmentation": [
        [
            "I my name is Alyssa Ritchey and I am a PhD student.",
            "Today I will present my works that is about moment based approach for structured output prediction and we'd like to thank the person that has been involved with me in this project.",
            "They are nobodies, whether they be analogous to Nini from University of Bristol and we developed this project while I was at University of Bristol as of."
        ],
        [
            "Sitting student.",
            "And the circle I will briefly brief outline of this talk will briefly introduce what is learning in structured output space.",
            "How we do it quite quickly, because Ben Taskar did lot this morning and will introduce our approach for learning instructor doubtful space that are based on a new objective function that this core and how we present some experimental results and discuss some computational issues related to our approach.",
            "And that will conclude with some ideas."
        ],
        [
            "Or further works.",
            "OK, so today there is a lot of problems that involve structured data which can be represented by sequences, trees, or in general by graphs and then sequences, trees and graphs.",
            "So sensually model the temporal, spatial and structural dependencies between objects in the problem.",
            "And this phenomenon arise in several fields such as computer vision, computational biology and natural language processing or web."
        ],
        [
            "Analysis.",
            "So machine learning and data mining algorithms must be able to treat and to cope with this problem and to analyze efficiently and automatically this large amount of complex and structured data.",
            "That's why recently a lot of effort has been put to develop structural learning algorithms that can predict complex structures such as sequences, trees or graphs, and using traditional algorithms to cope with problems involving structured data.",
            "Is a suboptimal choice because sometimes we often we lost the information about the structure of the problem and this concept was explained by Tasker this morning.",
            "I will explain again quickly, but we thought."
        ],
        [
            "Case studies.",
            "First of all, I will briefly review what is the traditional framework, the traditional framework for supervised learning is that we have data in that are available in formal examples and their associated correct concerts.",
            "So we have a input output pairs where the input X is a vector and why is usually is a scholar and then we want to find the optimal hypothesis age taken from an opportune a hypothesis space an.",
            "Then we want the hypothesis performs well on a new test sample drawn from the same distribution of the training set.",
            "And to do that usually we impose that the hypothesis perform."
        ],
        [
            "As well, on the training set.",
            "I think we got supervised learning task is classification and in classification one should assign to object one of finished number of classes.",
            "Oh sorry.",
            "If it's better, sorry.",
            "And so I will show you an example of classification in the case of named entity recognition, named entity recognition is the task of locate name, entity into text and entity of interest can be person names, organization names, location names or indication of times or dates or whatever.",
            "So this problem can be modeled in a traditional framework by a multi class classification problem so.",
            "Example, yet we have Spanish word and for each word in the sentence as a sorry Spanish sentence and for each word in the sentence, we should assign a label and.",
            "We ought to do that.",
            "Usually we consider we encode the information about the world in the feature vector X that represent the input, and we want to assign a label to each word.",
            "So we do it separately.",
            "So for the first word, we should assign the class label or that indicates that PP is an organization.",
            "The same for the second word.",
            "The same for the third and as well for all the words in the sentence.",
            "But one can easily argue that this is a suboptimal task because we should be able to consider the correlation between."
        ],
        [
            "Recent words how to do that we should we ask, can we realize the joint labeling for all the words in the sentence and this task is usually referred as a sequence labeling.",
            "So we are given an input sequence, a sequence X and we want to reconstruct the associated the label sequence Y where both the X&Y of the equal length and.",
            "Of course, sequence labeling can be used not only for name entity recognition, but for many other tasks such as gene funding in computational biology."
        ],
        [
            "For example.",
            "Another task that involves structural data and we are analyze it, is biological sequence alignment and biological sequence.",
            "Alignment is especially useful when one to determine the similarity between biological sequences, in particular, in this circle will concentrate my attention on global alignment.",
            "So we are given to sequences Swan and there's two and the global alignment is an assignment of gaps such as to line up each letter in one sequence.",
            "Either with a gap or letter in the other SQL."
        ],
        [
            "This problem can be more the mathematically.",
            "In this way we are given a sequence pair X and we want to predict the correct sequence Y of alignment operation in the simplest model, we should just count the number of matches mismatches and gaps in each alignment, and alignment can be represented as you all probably know as paths in the Alignment graph path that goes from the upper left to the lower right corner."
        ],
        [
            "In that graph.",
            "The last problem that we analyzed that involves structured data, is there any secondary structure prediction?",
            "Is the problem that we have an RNA sequence the primary structure and we want to predict the most likely secondary structure and the secondary structure?",
            "And is for machines the sequence for the itself to effect of the hydrogen bond and it's the study of hair and a secondary structure particularly important.",
            "In understanding the function of."
        ],
        [
            "Irani, how do we model this problem?",
            "We model this problem as a sequence parsing problem.",
            "So we are given an input sequence X.",
            "We want to determine that's asserted Part 3, Y.",
            "If we have defined an appropriate context free grammar.",
            "As you probably already know, I context free grammar is 4 two player of object.",
            "So we have a set of non terminal symbols, a set of terminal symbols, a set of rules that connect terminal and non terminal symbols.",
            "And then starting symbol that should belong to the set of non terminals and for example on the right side of the screen we have an RNA sequence and we can fold it.",
            "We can model the folding in that way.",
            "Using the grammar that we have defined on the left."
        ],
        [
            "Screen.",
            "And for all the three problems that I've shown to you.",
            "These are generative generative approaches possible.",
            "For example, I will consider the most simple case in sequence labeling traditionally hidden Markov models as be used for sequence labeling.",
            "So, but there are too many two main drawbacks in using Hmm's for this task.",
            "The first drawback is that HMM relies on independence assumptions and so we assume that each observation.",
            "Is dependent only on the current state and each state is dependent only on the previous state and in this way we cannot model long term interaction between objects between words.",
            "For example, in named entity recognition and the second drawback that we try to avoid is to is that HMM is our typical entrained by maximum likelihood estimation and.",
            "This is a suboptimal way of training, since the maximum likelihood criterium is not directly related to the prediction accuracy of the model.",
            "So we should try to devise a method that is more directly related to the prediction accuracy."
        ],
        [
            "How to do that?",
            "Recently, discriminative models have been introduced and discriminative models essentially specify the probability of a possible output Y given an observation.",
            "So they consider the conditional probability rather than the joint probability, and we discriminative models.",
            "We don't not have problems of independence assumption like in generative models and the.",
            "Moreover we can consider.",
            "Arbitrary features of the observation, and this can help us to model more complex problems in a more effective way.",
            "This is the sample of conditional random fields in the example is a chain conditional random fields that Tasker show."
        ],
        [
            "This morning and so I already said that a lot of discriminative algorithms have emerged in the last few years too.",
            "Solve the problem of complex structured prediction and here I would like to show how our approach is for structured prediction and I will concentrate on the tree problem that I showed you before.",
            "So the question that I have to answer are we are given a training set of correct pair of sentences and their associated entity tags.",
            "Can we let the abstract entities from new words for new sentences or given a training set of correct biological alignment?",
            "Can we learn to align to a new sequences and finally, given a training set of correct air and a secondary structures, can we learn to determine the secondary structures?",
            "Foreign you Aaron, a sequence?",
            "Of course this is not an exhaustive list of possible application.",
            "For example, image segmentation could be another application or like gene finding by informatics, there's a lot of.",
            "Location for this."
        ],
        [
            "Framework.",
            "An like previous approach and like the approach the task are presented in this morning, we model the problem like Multi Label supervised classification.",
            "So basically in the House but we don't do not have a scholar but we have vectors.",
            "So the problem is we are given an input.",
            "We are given a training set of input, output pairs drawn IID from an unknown probability distribution and we want to find the hypothesis H. Taken from my Newport hypothesis class, an such that the optimal output Y can be reconstructed from X on a new test sample.",
            "And how to do that?",
            "We use the empirical risk minimization principle and so we impose that the function H to perform well on the training set.",
            "As a discarded and as all the pattern that work in this field usually do, the prediction amount 2, computing the argmax of a scoring function, and the scoring function is linear if parametric tries linearly in the in a feature vector fee that is adjoint representation of input and output substantially.",
            "I'm adopting the same notation of this car, but.",
            "Anne.",
            "This is was this morning was F and in my cases fee but all the framework remained."
        ],
        [
            "Same.",
            "Now we show up with.",
            "We solve that problem that is new and substantially we are analyzing this problem in three phases.",
            "The first phase is encoding, so you should.",
            "We should define a suitable feature map.",
            "The second phase is compression that we should be able to characterize the output space that asked us.",
            "Cars show is huge in a synthetic and compact way.",
            "The third step is optimization, so we should define.",
            "Not appropriate objective function and we can use it.",
            "We use it for learning."
        ],
        [
            "Let's analyze the 1st."
        ],
        [
            "Step before.",
            "So which is the main problem in defining features?",
            "Features must be defined in a way such that prediction can be computed efficiently.",
            "This is quite hard.",
            "The sense of the cardinality of the output space is typically huge out.",
            "We do that.",
            "The defining feature vector in a way such that as it decomposes as a sum of elementary features on parts and parts, are typically edges or nodes in."
        ],
        [
            "Apps.",
            "I will show some example in the three problems that I have considered before.",
            "Here is an example of a chain conditional random field with HMM features.",
            "So basically we have two types of elementary features features for that connect adjacent label and feature that connect an observation at step K with the state step with the label at the same step.",
            "OK, but this is not.",
            "This is just an example of a chain conditional random field with HMM features, but in general features can be more much more complex and can reflect long range interaction between observation and also arbitrary features of observation can be considered for example in named entity recognition could be useful to define the features that reflect the spelling properties or."
        ],
        [
            "Of the words.",
            "In sequence alignment, so essentially the the feature vector in the simplest model, the three parameters model contains the number of matches mismatches and gaps for an alignment Y, but in practice more complex models are used, and so for example in the one that I called the 4th para meters model, we consider a thin function for penalty, so we scored differently the gap.",
            "Openings and the gap is tension, and in that case the feature vector will be with count the number of matches, the number of mismatches and then number of gap opening and the number of gap as tension in the complete model, the 200 Eleven 212 para meters models, the feature vector will contain the statistic associated to all pairs of amino acids and associated to the."
        ],
        [
            "Penalties in sequence part singer.",
            "But the feature vector contains the statistics associated to the roots.",
            "For example, in this part three, that rule occurs just."
        ],
        [
            "At once.",
            "In this way, having defined this feature of prediction can be computed that specially with dynamic programming algorithms, and in the case examined, we can use for example for sequence labeling.",
            "The Viterbi algorithm for sequence alignment they need among Bush algorithm for sequence parsing the YC algorithm."
        ],
        [
            "So basically we have solved the encoding problem.",
            "Let's analyze let's face with the compression problem.",
            "So the task is to characterize the auto space in a synthetic."
        ],
        [
            "In compact way, this is not trivial since the number of possible output vector Y given an observation is typically used, for example in sequence labeling will be exponential in the length of the sequences.",
            "To characterize the distribution of the scores, we consider the mean and the variance of the distribution, and it's important to see that both the mean and the variance can be expressed as a function of parameters.",
            "For example, the mean can be expressed as a linear function of the parameters, so it's W. I suppose no where no is contains the mean as associated to the feature vectors and the variance can be expressed as W transpose CW, and in this case is C is a covariance matrix associated to the feature vectors.",
            "How do we compute the mean and variance?",
            "And this is not trivial as I saw as I told you before, since N is huge, but we divide the sum.",
            "Dynamic programming technique."
        ],
        [
            "To do that?",
            "I show just an example to try to be clear.",
            "In sequence, labeling the number of possible level sequence is exponentially in the length of the sequences.",
            "And to compute the mean and each element of the vector mu and the covariance matrix E, we use an algorithm that is similar to the forward algorithm.",
            "For example, in this algorithm on the right side of the screen we compute the mean value associated to the features which represent.",
            "The mission of a symbol Q at a state pin.",
            "Basically we are feeling a dynamic programming table that contains the value of the mean step by steps and.",
            "This is a recursive formula.",
            "Basically we compute the mean considering that the expectation at a given step is given by the expectation at the previous step plus the value of the feature that."
        ],
        [
            "Step butt.",
            "This is the basic ideas behind this formulas an the mean values are computed by considering that this petition of assume of variable bleues at step K is given by this petition of the sum of variable ball at previous step plus the spectation at current step."
        ],
        [
            "In the previous expression, just by changing this formulas and changing the initial condition, we may also compute all the other values in the mean vector and in the symmetrix the."
        ],
        [
            "Example for the variance is slightly different and basically we compute before the 2nd order moment following this principle and then variances are computed by centering this second."
        ],
        [
            "Are there moments?",
            "Which is the main problem in using this approach that when the feature space tend to growth to be large we have an AI computational cost and how to solve this problem, we substantially device to way to solve this problem.",
            "The first is simply to exploit the structure of the metrics of the covariance matrix.",
            "See that covariance matrix in most of applications tend to be very sparse.",
            "For example, I show you an example here.",
            "Of conditional random field which hmm features when the size of observation alphabet is 4 and the size of the Indian alphabet history.",
            "And for this in this case you can see that there is a lot of redundancy in these metrics in general for the sequence labeling and for HMM for conditional random field with HMM features we we can see that a number of different values in the covariance matrix.",
            "Is linear in the size of the observation alphabet.",
            "We can devise similar results for other applications.",
            "Another another way to solve the problem of the high computational cost is to use some sampling strategy, and I will discuss this later in theory and in the."
        ],
        [
            "Experimental results.",
            "So we have defined the feature vector.",
            "We have characterized the distribution of the score and now how do we use this previous step to define perform learning?"
        ],
        [
            "We should define an optimization criterium, an we present and you optimization criterion.",
            "That is the discord, and we think that this approach is particularly sweet for non separable case that always occur in practical application.",
            "So we minimize the number of output vector which score I heard then the score of the correct pace.",
            "I would like to give you an idea about that just considering one sequence in the training set.",
            "So this is the score of the optimal pace, and this is the distribution of this course.",
            "We want to maximize this discord, so we want to that the distance between the score of the optimal pair and the mean of the distribution is maximized, and this distance is normalizer in terms of standard deviation.",
            "In this way, we impose that this car off the correct pay is as far as possible from the bulk of this distribution from the bulk of them, uncorrect.",
            "Well of the anchor at."
        ],
        [
            "Output.",
            "And, importantly, the discord can be expressed as a function of the parameters.",
            "So we define the vector B that is, the difference between the feature vector associated to the optimal pair and the mean of the the features and the standard deviation is given by the square root of W transpose CW, and so we should solve for just one sequence in the training set.",
            "This optimization problem does.",
            "You see, can be OK. Just say that this problem is equivalent to, so the problem and we prove it.",
            "And by equivalent I mean that the solution is sequel up to a scaling factor.",
            "Let's have a look at the 2nd."
        ],
        [
            "Just now.",
            "Before we define a criterium that is.",
            "That is that we called the ranking loss.",
            "So basically in the ranking loss we are ranking loss is the sum of the number of scores that are larger than the score of the optimal pair.",
            "In this case I is an indicator function and we what we are doing is we are maximizing minimizing an upper bound on this quantity.",
            "Since our objective function is an upper bound on the ranking loss in this way, we are sure that the number of output vector with with ever score.",
            "I heard and the score of the correct payers is minimized."
        ],
        [
            "Why our approach is different from previous approach?",
            "Because a classical approach will be to minimize the in the training set.",
            "The number of incorrect micro labels, and in that case the loss will be that one and this is the approach followed by previous approach like conditional random fields by Lafferty DeMarco, support vector machines and the average purchase.",
            "Another approach the approach is followed by Tasker and by SVM Heiser.",
            "Is to minimize the number of incorrect micro labels.",
            "We do not use it substantially since as this car show this morning, this required that loss should decompose in the structure in the graph and with our approach we don't know."
        ],
        [
            "I have this problem.",
            "So we are back in the in the hour optimization problem and we want to extend the optimization problem in the case of an arbitrary training set with helpers in the in the input output pairs.",
            "So basically what we're doing is we are.",
            "We are minimizing the empirical risk has decided to the upper bound on the ranking loss previously defined and this is the optimization problem that we solved.",
            "We consider an equivalent formulation.",
            "Again, equivalent means that the solution is equivalent up to a scaling factors and that is this one.",
            "And as you can see is expressed in terms of the B matrix and of the vector and the metrics that we introduced before in the compression phase.",
            "And this is the algorithm that we call the structured output indiscriminant analysis, since it could be seen as an extension of the linear discriminant analysis to this setting or structured out."
        ],
        [
            "Problems.",
            "And so the problem that we have to solve, this one we are maximizing.",
            "This ratio this is an optimization where convex optimization problem.",
            "Perhaps you will see it better here and then to solve it.",
            "The first thing is we should observe that if the metric see is not positive semidefinite, we should introduce regularisation too.",
            "To solve better the problem and then we can solve the problem by simple matrix inversion and, but in theory inverting these metrics can be.",
            "Ava I competition of course, but we we use the fast, can you get the gradient methods and to speed up the optimization?"
        ],
        [
            "And face.",
            "I will just give an idea about this theoretical stuff that perhaps I don't want to bore anyone and we developed the Rademacher bound that showed that learning with our upper bounds is effectively achieved and we want that bound holds even when B&C are estimated by sampling, and we want that to Cynthia.",
            "In this way we can really speed up.",
            "The optimization, then our algorithm.",
            "So we consider two direction of sampling.",
            "The first one is that for each input output pair in the training set, we consider only a limited number of uncorrect outputs to estimate the matrices by sampling.",
            "And also we consider only a finished number of input output pairs in the training set and the bound that I will show in the next slide.",
            "Basically, we prove that empirical expectation overestimated loss.",
            "Added the scenes that we're computing B&C by sampling is a good approximate upper bound for the expected loss, and we should also remember that the upper bound that we defined before is an upper bound on the ranking loss.",
            "So effectively what we are doing is we are minimizing.",
            "Spectation of the ranking growth."
        ],
        [
            "And see.",
            "This is the bound and just to give some insight, the five terms here are.",
            "And then empirical estimate of the spectral lot of the loss determined with sampling to Rademacher quantities that decrease.",
            "Cause as the inverse of the square root of N in hell.",
            "And the other two terms here M is a cost and again goes as the inverse of the square root of N anhel.",
            "So that means that all the quantity on the right side go to zero when the number of samples in."
        ],
        [
            "Freeze.",
            "Another approach that we consider is a variant of.",
            "The previous approach is that we call the true this core approach an Araiza since we asked ourself how do we define the discovery of a training set?",
            "Another possible approach would be to consider that the contributor or feature input output pair is independent from the contributor of the orders.",
            "So it means that the means and they called and variance computed for each base can be summed in that way.",
            "We should assume to have an estimate of the true mean of the distribution for all four overal input output pair of the training set and as well for the covariance.",
            "And this is what we call the disk or approach the problem.",
            "The final problem is this one.",
            "As you can see the structure of the problem.",
            "Is similar to the one for the soda, where we again have a convex optimization problem that can be solved by simple matrix inversion.",
            "And.",
            "Basically, the result that I will show you later are quite similar.",
            "In both case, if you work it out this formula, you can see that we are minimizing kind of upper bound on the ranking loss again, so that's why perhaps that you measured are very similar.",
            "But for this second method we also try a different strategy, because basically when we maximize this, Cora must of the linear constraints.",
            "Of this kind are satisfied.",
            "That means that the score of the correct pace in most of the cases larger than the score of all incorrect ones."
        ],
        [
            "But if one want to impose is pleasantly that discover of the optimal pair should be bigger than the score of all uncorrect.",
            "One, we could add the constraint that quadratic programming problem.",
            "We can solve it by iterative algorithms in a similar way to previous approaches, and eventually in that case when the data are not linearly separable we can relax constraint by adding Slack variables."
        ],
        [
            "So the algorithm in that case will be this.",
            "So we have basically four steps in the first step we compute the moment, in the second step we maximize the discord without any constraints.",
            "Then we enter in the main loop and for each input output pair in the training set, we compute.",
            "We identify the Max violated constraints, and then if the score or associated to that is bigger.",
            "Larger than the score of the the correct input output pair we add to the set of constraints constraint that impose that the score of the input of the correct input output pair should be bigger, and so we solve the.",
            "We maximize the discourse object to that constrains.",
            "And we repeat this until the set of constraints that not change."
        ],
        [
            "Ninja in decoration.",
            "We finally show some experimental results are related to the, so the algorithm to the proper disk or approach and to the disk or approach with constraints will learn briefly show that our methods perform better than the orders in case of high level of noise.",
            "What we are doing here is we are generated random sequences by starting from a random vector of parameters.",
            "For a chain conditional random field with HMM features, we consider two kind of conditional random field, one with this when the size of the observation alphabet is 4 and the size of the Eden alphabet is 2, another one when the side of their Salvation alphabet is 5 and the sides of the hidden alphabet is 3.",
            "Sequence are length of a length of 50 and the training set side is made by 20 pairs.",
            "The test side there is made by 100 pays.",
            "All the results are generated by.",
            "Waiting at the process for one under the runs and we compare our approach with the previous approaches and we plot here the average number of incorrect labels at varying level of noise and you can see our approach are better in case of large P&P represent the probability of.",
            "Mirror represented the level of noise that we generated by flipping Eden labels in the training set."
        ],
        [
            "Another problem that I know there are simulation that we did is we consider the learning curve."
        ],
        [
            "For the second graph here, when the level of noise is zero, point."
        ],
        [
            "2.",
            "We plot the learning curve for the all the algorithms that we consider.",
            "I do not show here the discourse in the performance are very similar to soda and what we observe is that our approach performs better than conditional random fields and the perceptron and is very competitive and somehow better than the SVM Heiser that.",
            "Username in gloss.",
            "Similar to the algorithm that presented this morning by Tasker, and in this case we see that the performance in term of this error are very similar with the SVM is about when we analyze the computational cost of learning, we see that our approach is much more faster, especially for when the sides of the training set become."
        ],
        [
            "It's larger.",
            "Hello there experiments that we did.",
            "So again, with artificial data is that we consider a chain conditional random field with HMM features an the size of the hidden alphabet is 3 the size of their survey shun alphabet changes in the experiment.",
            "Here we want to show that with sampling approaches we can obtain good results.",
            "So basically we compar.",
            "Use our approach by computing metrics with dynamic programming and by computing metrics with random with estimates.",
            "Of the metrics with the approximate sampling on 50 parts and 200 parts, and we plot the results in terms of this error, and we see that all the formatters are more or less equivalent.",
            "But of course using sampling the time is much the computational cost and the time for learning is much more reduced."
        ],
        [
            "Hello, the simulation that we did again with artificial data is to show the behavior of the disk or approach when constraints are added.",
            "In this case, what we want to show is basically that our approach need much less constraints that previous iterative approach, similar approach.",
            "And as you can see here, the number of constraints needed is is much less than.",
            "The orders and but we did write you so much the disk or approach with constraints, since we see that in many problems that are noisy and linearly nonlinear with parable.",
            "So the other version are."
        ],
        [
            "Better.",
            "I showed you before the result with artificial data to understand how the algorithms works, but now I will show some results with real data on sequence labeling and in particular name entity recognition.",
            "We consider the Spanish newer heartical corpus for the special session of Natural Language Competition or 2002.",
            "We consider only 300 sentences with an average of 30 words.",
            "We use a small corpus in order.",
            "To compare our approach with other with another paper that is using the same.",
            "The same setting experimental setting.",
            "We have 9 labels that indicates no name beginning and continuation of person, organization, location, miscellaneous names, and we used to set of binary features.",
            "So in the first set, just the HMM features in the second set, the set S1 or features and the HMM feature for the previous and the next word.",
            "In this way we can model the interaction between adjacent words.",
            "In this case I plot the result for both the score I showed the result for both discord and soda.",
            "And as you can see in the previous set of features, this side upper outperform all the other methods in the second set of features is the discord that performs better."
        ],
        [
            "Add another.",
            "Mental results that I would like to show is for sequence alignment.",
            "We use artificial sequences because what we want to see is that starting from a random matrix of para meters, are we able with learning to reconstruct the.",
            "That the sequence that the metrics of para meters.",
            "Basically we have one substitution matrix generated randomly, but just imposing that the values on the diagonal should be bigger than the values out of the diagonal.",
            "And then we consider the sequence of length 100 and the various sides of the training set from 5 to 100 and we.",
            "And the size of the data set is 4 is 100.",
            "The pair of sequences and for the training set of 100 we for one simulation we see that we are able to reconstruct a more or less the structure of the metrics and Moreover if we plot the result in terms of test errors and we compare our approach with a generative approach where parameters are determined by.",
            "Boston estimates we see the test error for our approach is much lower, especially for data set for training set or small sides."
        ],
        [
            "The last experiment is to show you how our method or discord with constraints performs an is for sequence parsing.",
            "We use Hydramatic that we draw on that paper that we see is particularly adapted to model the problem of RNA secondary structure prediction.",
            "We extract five families from the FM database.",
            "And we plot the results in terms of sensitivity and specificity for all the for all the different family in the database we compare our approach with this current generative approach and the perceptron approach, and we see that both discriminative approaches so that this current perceptron outperform the generative approach and the performance of this current perceptron are more or less."
        ],
        [
            "The same.",
            "So concluding have presented to new methods for learning structured output space.",
            "We saw that accuracy is compatible with state of the art techniques we with our method are very easy to implement since they rely on dynamic programming for metrics for statistic computations for moment computation, and on a simple convex optimization problem is fast.",
            "For large training set.",
            "And the reasonable number of features since the mean and variance computation is parallelizable for large training set.",
            "And since the invention of the metrics can be performed by conjugate gradient techniques.",
            "Three application sequence labeling, sequence parsing, and sequence alignment.",
            "There's a lot of work to do.",
            "First of all, we should be able to scale our approach for a lot of features as required in many practical application and how to do that, we should exploit the bit more the sampling approach.",
            "We also need to develop a dual version.",
            "We kernels in order to deal with non separable data.",
            "And."
        ],
        [
            "Thank you.",
            "Correct somehow?",
            "Don't let your data have causing normality when you use this call you soon that's metrics at the.",
            "Somehow abortion like not really, since for example the other matter bound is like when you use Fisher discriminate that you could have the same problem.",
            "But really if you develop the Rademacher bounds you can prove that works even if we do not assume any.",
            "We do not have any assumption.",
            "Oshana of those sanity for the distribution of this course, it's a bit complex plane towards, but this is the principle rather maker bound.",
            "Show that you achieve learning even if it is not Goshen.",
            "Remember.",
            "Yeah, so the disease called hasn't definition beautiful under those assumptions, but you're not using it.",
            "Kate."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I my name is Alyssa Ritchey and I am a PhD student.",
                    "label": 0
                },
                {
                    "sent": "Today I will present my works that is about moment based approach for structured output prediction and we'd like to thank the person that has been involved with me in this project.",
                    "label": 1
                },
                {
                    "sent": "They are nobodies, whether they be analogous to Nini from University of Bristol and we developed this project while I was at University of Bristol as of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sitting student.",
                    "label": 0
                },
                {
                    "sent": "And the circle I will briefly brief outline of this talk will briefly introduce what is learning in structured output space.",
                    "label": 1
                },
                {
                    "sent": "How we do it quite quickly, because Ben Taskar did lot this morning and will introduce our approach for learning instructor doubtful space that are based on a new objective function that this core and how we present some experimental results and discuss some computational issues related to our approach.",
                    "label": 0
                },
                {
                    "sent": "And that will conclude with some ideas.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or further works.",
                    "label": 0
                },
                {
                    "sent": "OK, so today there is a lot of problems that involve structured data which can be represented by sequences, trees, or in general by graphs and then sequences, trees and graphs.",
                    "label": 1
                },
                {
                    "sent": "So sensually model the temporal, spatial and structural dependencies between objects in the problem.",
                    "label": 0
                },
                {
                    "sent": "And this phenomenon arise in several fields such as computer vision, computational biology and natural language processing or web.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Analysis.",
                    "label": 0
                },
                {
                    "sent": "So machine learning and data mining algorithms must be able to treat and to cope with this problem and to analyze efficiently and automatically this large amount of complex and structured data.",
                    "label": 1
                },
                {
                    "sent": "That's why recently a lot of effort has been put to develop structural learning algorithms that can predict complex structures such as sequences, trees or graphs, and using traditional algorithms to cope with problems involving structured data.",
                    "label": 0
                },
                {
                    "sent": "Is a suboptimal choice because sometimes we often we lost the information about the structure of the problem and this concept was explained by Tasker this morning.",
                    "label": 0
                },
                {
                    "sent": "I will explain again quickly, but we thought.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Case studies.",
                    "label": 0
                },
                {
                    "sent": "First of all, I will briefly review what is the traditional framework, the traditional framework for supervised learning is that we have data in that are available in formal examples and their associated correct concerts.",
                    "label": 1
                },
                {
                    "sent": "So we have a input output pairs where the input X is a vector and why is usually is a scholar and then we want to find the optimal hypothesis age taken from an opportune a hypothesis space an.",
                    "label": 1
                },
                {
                    "sent": "Then we want the hypothesis performs well on a new test sample drawn from the same distribution of the training set.",
                    "label": 0
                },
                {
                    "sent": "And to do that usually we impose that the hypothesis perform.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As well, on the training set.",
                    "label": 0
                },
                {
                    "sent": "I think we got supervised learning task is classification and in classification one should assign to object one of finished number of classes.",
                    "label": 1
                },
                {
                    "sent": "Oh sorry.",
                    "label": 0
                },
                {
                    "sent": "If it's better, sorry.",
                    "label": 0
                },
                {
                    "sent": "And so I will show you an example of classification in the case of named entity recognition, named entity recognition is the task of locate name, entity into text and entity of interest can be person names, organization names, location names or indication of times or dates or whatever.",
                    "label": 1
                },
                {
                    "sent": "So this problem can be modeled in a traditional framework by a multi class classification problem so.",
                    "label": 0
                },
                {
                    "sent": "Example, yet we have Spanish word and for each word in the sentence as a sorry Spanish sentence and for each word in the sentence, we should assign a label and.",
                    "label": 0
                },
                {
                    "sent": "We ought to do that.",
                    "label": 0
                },
                {
                    "sent": "Usually we consider we encode the information about the world in the feature vector X that represent the input, and we want to assign a label to each word.",
                    "label": 0
                },
                {
                    "sent": "So we do it separately.",
                    "label": 0
                },
                {
                    "sent": "So for the first word, we should assign the class label or that indicates that PP is an organization.",
                    "label": 0
                },
                {
                    "sent": "The same for the second word.",
                    "label": 0
                },
                {
                    "sent": "The same for the third and as well for all the words in the sentence.",
                    "label": 0
                },
                {
                    "sent": "But one can easily argue that this is a suboptimal task because we should be able to consider the correlation between.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recent words how to do that we should we ask, can we realize the joint labeling for all the words in the sentence and this task is usually referred as a sequence labeling.",
                    "label": 1
                },
                {
                    "sent": "So we are given an input sequence, a sequence X and we want to reconstruct the associated the label sequence Y where both the X&Y of the equal length and.",
                    "label": 0
                },
                {
                    "sent": "Of course, sequence labeling can be used not only for name entity recognition, but for many other tasks such as gene funding in computational biology.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Another task that involves structural data and we are analyze it, is biological sequence alignment and biological sequence.",
                    "label": 0
                },
                {
                    "sent": "Alignment is especially useful when one to determine the similarity between biological sequences, in particular, in this circle will concentrate my attention on global alignment.",
                    "label": 1
                },
                {
                    "sent": "So we are given to sequences Swan and there's two and the global alignment is an assignment of gaps such as to line up each letter in one sequence.",
                    "label": 1
                },
                {
                    "sent": "Either with a gap or letter in the other SQL.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This problem can be more the mathematically.",
                    "label": 0
                },
                {
                    "sent": "In this way we are given a sequence pair X and we want to predict the correct sequence Y of alignment operation in the simplest model, we should just count the number of matches mismatches and gaps in each alignment, and alignment can be represented as you all probably know as paths in the Alignment graph path that goes from the upper left to the lower right corner.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In that graph.",
                    "label": 0
                },
                {
                    "sent": "The last problem that we analyzed that involves structured data, is there any secondary structure prediction?",
                    "label": 1
                },
                {
                    "sent": "Is the problem that we have an RNA sequence the primary structure and we want to predict the most likely secondary structure and the secondary structure?",
                    "label": 1
                },
                {
                    "sent": "And is for machines the sequence for the itself to effect of the hydrogen bond and it's the study of hair and a secondary structure particularly important.",
                    "label": 0
                },
                {
                    "sent": "In understanding the function of.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Irani, how do we model this problem?",
                    "label": 0
                },
                {
                    "sent": "We model this problem as a sequence parsing problem.",
                    "label": 0
                },
                {
                    "sent": "So we are given an input sequence X.",
                    "label": 1
                },
                {
                    "sent": "We want to determine that's asserted Part 3, Y.",
                    "label": 0
                },
                {
                    "sent": "If we have defined an appropriate context free grammar.",
                    "label": 0
                },
                {
                    "sent": "As you probably already know, I context free grammar is 4 two player of object.",
                    "label": 0
                },
                {
                    "sent": "So we have a set of non terminal symbols, a set of terminal symbols, a set of rules that connect terminal and non terminal symbols.",
                    "label": 0
                },
                {
                    "sent": "And then starting symbol that should belong to the set of non terminals and for example on the right side of the screen we have an RNA sequence and we can fold it.",
                    "label": 0
                },
                {
                    "sent": "We can model the folding in that way.",
                    "label": 0
                },
                {
                    "sent": "Using the grammar that we have defined on the left.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Screen.",
                    "label": 0
                },
                {
                    "sent": "And for all the three problems that I've shown to you.",
                    "label": 0
                },
                {
                    "sent": "These are generative generative approaches possible.",
                    "label": 0
                },
                {
                    "sent": "For example, I will consider the most simple case in sequence labeling traditionally hidden Markov models as be used for sequence labeling.",
                    "label": 1
                },
                {
                    "sent": "So, but there are too many two main drawbacks in using Hmm's for this task.",
                    "label": 0
                },
                {
                    "sent": "The first drawback is that HMM relies on independence assumptions and so we assume that each observation.",
                    "label": 1
                },
                {
                    "sent": "Is dependent only on the current state and each state is dependent only on the previous state and in this way we cannot model long term interaction between objects between words.",
                    "label": 0
                },
                {
                    "sent": "For example, in named entity recognition and the second drawback that we try to avoid is to is that HMM is our typical entrained by maximum likelihood estimation and.",
                    "label": 0
                },
                {
                    "sent": "This is a suboptimal way of training, since the maximum likelihood criterium is not directly related to the prediction accuracy of the model.",
                    "label": 0
                },
                {
                    "sent": "So we should try to devise a method that is more directly related to the prediction accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How to do that?",
                    "label": 0
                },
                {
                    "sent": "Recently, discriminative models have been introduced and discriminative models essentially specify the probability of a possible output Y given an observation.",
                    "label": 1
                },
                {
                    "sent": "So they consider the conditional probability rather than the joint probability, and we discriminative models.",
                    "label": 0
                },
                {
                    "sent": "We don't not have problems of independence assumption like in generative models and the.",
                    "label": 0
                },
                {
                    "sent": "Moreover we can consider.",
                    "label": 0
                },
                {
                    "sent": "Arbitrary features of the observation, and this can help us to model more complex problems in a more effective way.",
                    "label": 0
                },
                {
                    "sent": "This is the sample of conditional random fields in the example is a chain conditional random fields that Tasker show.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This morning and so I already said that a lot of discriminative algorithms have emerged in the last few years too.",
                    "label": 0
                },
                {
                    "sent": "Solve the problem of complex structured prediction and here I would like to show how our approach is for structured prediction and I will concentrate on the tree problem that I showed you before.",
                    "label": 0
                },
                {
                    "sent": "So the question that I have to answer are we are given a training set of correct pair of sentences and their associated entity tags.",
                    "label": 1
                },
                {
                    "sent": "Can we let the abstract entities from new words for new sentences or given a training set of correct biological alignment?",
                    "label": 0
                },
                {
                    "sent": "Can we learn to align to a new sequences and finally, given a training set of correct air and a secondary structures, can we learn to determine the secondary structures?",
                    "label": 1
                },
                {
                    "sent": "Foreign you Aaron, a sequence?",
                    "label": 0
                },
                {
                    "sent": "Of course this is not an exhaustive list of possible application.",
                    "label": 0
                },
                {
                    "sent": "For example, image segmentation could be another application or like gene finding by informatics, there's a lot of.",
                    "label": 0
                },
                {
                    "sent": "Location for this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Framework.",
                    "label": 0
                },
                {
                    "sent": "An like previous approach and like the approach the task are presented in this morning, we model the problem like Multi Label supervised classification.",
                    "label": 0
                },
                {
                    "sent": "So basically in the House but we don't do not have a scholar but we have vectors.",
                    "label": 0
                },
                {
                    "sent": "So the problem is we are given an input.",
                    "label": 0
                },
                {
                    "sent": "We are given a training set of input, output pairs drawn IID from an unknown probability distribution and we want to find the hypothesis H. Taken from my Newport hypothesis class, an such that the optimal output Y can be reconstructed from X on a new test sample.",
                    "label": 1
                },
                {
                    "sent": "And how to do that?",
                    "label": 0
                },
                {
                    "sent": "We use the empirical risk minimization principle and so we impose that the function H to perform well on the training set.",
                    "label": 0
                },
                {
                    "sent": "As a discarded and as all the pattern that work in this field usually do, the prediction amount 2, computing the argmax of a scoring function, and the scoring function is linear if parametric tries linearly in the in a feature vector fee that is adjoint representation of input and output substantially.",
                    "label": 0
                },
                {
                    "sent": "I'm adopting the same notation of this car, but.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This is was this morning was F and in my cases fee but all the framework remained.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same.",
                    "label": 0
                },
                {
                    "sent": "Now we show up with.",
                    "label": 0
                },
                {
                    "sent": "We solve that problem that is new and substantially we are analyzing this problem in three phases.",
                    "label": 0
                },
                {
                    "sent": "The first phase is encoding, so you should.",
                    "label": 0
                },
                {
                    "sent": "We should define a suitable feature map.",
                    "label": 1
                },
                {
                    "sent": "The second phase is compression that we should be able to characterize the output space that asked us.",
                    "label": 0
                },
                {
                    "sent": "Cars show is huge in a synthetic and compact way.",
                    "label": 1
                },
                {
                    "sent": "The third step is optimization, so we should define.",
                    "label": 1
                },
                {
                    "sent": "Not appropriate objective function and we can use it.",
                    "label": 0
                },
                {
                    "sent": "We use it for learning.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's analyze the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Step before.",
                    "label": 0
                },
                {
                    "sent": "So which is the main problem in defining features?",
                    "label": 0
                },
                {
                    "sent": "Features must be defined in a way such that prediction can be computed efficiently.",
                    "label": 1
                },
                {
                    "sent": "This is quite hard.",
                    "label": 0
                },
                {
                    "sent": "The sense of the cardinality of the output space is typically huge out.",
                    "label": 0
                },
                {
                    "sent": "We do that.",
                    "label": 0
                },
                {
                    "sent": "The defining feature vector in a way such that as it decomposes as a sum of elementary features on parts and parts, are typically edges or nodes in.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apps.",
                    "label": 0
                },
                {
                    "sent": "I will show some example in the three problems that I have considered before.",
                    "label": 0
                },
                {
                    "sent": "Here is an example of a chain conditional random field with HMM features.",
                    "label": 0
                },
                {
                    "sent": "So basically we have two types of elementary features features for that connect adjacent label and feature that connect an observation at step K with the state step with the label at the same step.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is not.",
                    "label": 0
                },
                {
                    "sent": "This is just an example of a chain conditional random field with HMM features, but in general features can be more much more complex and can reflect long range interaction between observation and also arbitrary features of observation can be considered for example in named entity recognition could be useful to define the features that reflect the spelling properties or.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the words.",
                    "label": 0
                },
                {
                    "sent": "In sequence alignment, so essentially the the feature vector in the simplest model, the three parameters model contains the number of matches mismatches and gaps for an alignment Y, but in practice more complex models are used, and so for example in the one that I called the 4th para meters model, we consider a thin function for penalty, so we scored differently the gap.",
                    "label": 1
                },
                {
                    "sent": "Openings and the gap is tension, and in that case the feature vector will be with count the number of matches, the number of mismatches and then number of gap opening and the number of gap as tension in the complete model, the 200 Eleven 212 para meters models, the feature vector will contain the statistic associated to all pairs of amino acids and associated to the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Penalties in sequence part singer.",
                    "label": 0
                },
                {
                    "sent": "But the feature vector contains the statistics associated to the roots.",
                    "label": 1
                },
                {
                    "sent": "For example, in this part three, that rule occurs just.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At once.",
                    "label": 0
                },
                {
                    "sent": "In this way, having defined this feature of prediction can be computed that specially with dynamic programming algorithms, and in the case examined, we can use for example for sequence labeling.",
                    "label": 1
                },
                {
                    "sent": "The Viterbi algorithm for sequence alignment they need among Bush algorithm for sequence parsing the YC algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically we have solved the encoding problem.",
                    "label": 0
                },
                {
                    "sent": "Let's analyze let's face with the compression problem.",
                    "label": 0
                },
                {
                    "sent": "So the task is to characterize the auto space in a synthetic.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In compact way, this is not trivial since the number of possible output vector Y given an observation is typically used, for example in sequence labeling will be exponential in the length of the sequences.",
                    "label": 1
                },
                {
                    "sent": "To characterize the distribution of the scores, we consider the mean and the variance of the distribution, and it's important to see that both the mean and the variance can be expressed as a function of parameters.",
                    "label": 1
                },
                {
                    "sent": "For example, the mean can be expressed as a linear function of the parameters, so it's W. I suppose no where no is contains the mean as associated to the feature vectors and the variance can be expressed as W transpose CW, and in this case is C is a covariance matrix associated to the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "How do we compute the mean and variance?",
                    "label": 0
                },
                {
                    "sent": "And this is not trivial as I saw as I told you before, since N is huge, but we divide the sum.",
                    "label": 0
                },
                {
                    "sent": "Dynamic programming technique.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To do that?",
                    "label": 0
                },
                {
                    "sent": "I show just an example to try to be clear.",
                    "label": 0
                },
                {
                    "sent": "In sequence, labeling the number of possible level sequence is exponentially in the length of the sequences.",
                    "label": 1
                },
                {
                    "sent": "And to compute the mean and each element of the vector mu and the covariance matrix E, we use an algorithm that is similar to the forward algorithm.",
                    "label": 1
                },
                {
                    "sent": "For example, in this algorithm on the right side of the screen we compute the mean value associated to the features which represent.",
                    "label": 0
                },
                {
                    "sent": "The mission of a symbol Q at a state pin.",
                    "label": 0
                },
                {
                    "sent": "Basically we are feeling a dynamic programming table that contains the value of the mean step by steps and.",
                    "label": 0
                },
                {
                    "sent": "This is a recursive formula.",
                    "label": 0
                },
                {
                    "sent": "Basically we compute the mean considering that the expectation at a given step is given by the expectation at the previous step plus the value of the feature that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step butt.",
                    "label": 0
                },
                {
                    "sent": "This is the basic ideas behind this formulas an the mean values are computed by considering that this petition of assume of variable bleues at step K is given by this petition of the sum of variable ball at previous step plus the spectation at current step.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the previous expression, just by changing this formulas and changing the initial condition, we may also compute all the other values in the mean vector and in the symmetrix the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example for the variance is slightly different and basically we compute before the 2nd order moment following this principle and then variances are computed by centering this second.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are there moments?",
                    "label": 0
                },
                {
                    "sent": "Which is the main problem in using this approach that when the feature space tend to growth to be large we have an AI computational cost and how to solve this problem, we substantially device to way to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "The first is simply to exploit the structure of the metrics of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "See that covariance matrix in most of applications tend to be very sparse.",
                    "label": 0
                },
                {
                    "sent": "For example, I show you an example here.",
                    "label": 0
                },
                {
                    "sent": "Of conditional random field which hmm features when the size of observation alphabet is 4 and the size of the Indian alphabet history.",
                    "label": 0
                },
                {
                    "sent": "And for this in this case you can see that there is a lot of redundancy in these metrics in general for the sequence labeling and for HMM for conditional random field with HMM features we we can see that a number of different values in the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "Is linear in the size of the observation alphabet.",
                    "label": 1
                },
                {
                    "sent": "We can devise similar results for other applications.",
                    "label": 0
                },
                {
                    "sent": "Another another way to solve the problem of the high computational cost is to use some sampling strategy, and I will discuss this later in theory and in the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experimental results.",
                    "label": 0
                },
                {
                    "sent": "So we have defined the feature vector.",
                    "label": 0
                },
                {
                    "sent": "We have characterized the distribution of the score and now how do we use this previous step to define perform learning?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We should define an optimization criterium, an we present and you optimization criterion.",
                    "label": 0
                },
                {
                    "sent": "That is the discord, and we think that this approach is particularly sweet for non separable case that always occur in practical application.",
                    "label": 0
                },
                {
                    "sent": "So we minimize the number of output vector which score I heard then the score of the correct pace.",
                    "label": 1
                },
                {
                    "sent": "I would like to give you an idea about that just considering one sequence in the training set.",
                    "label": 0
                },
                {
                    "sent": "So this is the score of the optimal pace, and this is the distribution of this course.",
                    "label": 0
                },
                {
                    "sent": "We want to maximize this discord, so we want to that the distance between the score of the optimal pair and the mean of the distribution is maximized, and this distance is normalizer in terms of standard deviation.",
                    "label": 0
                },
                {
                    "sent": "In this way, we impose that this car off the correct pay is as far as possible from the bulk of this distribution from the bulk of them, uncorrect.",
                    "label": 0
                },
                {
                    "sent": "Well of the anchor at.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Output.",
                    "label": 0
                },
                {
                    "sent": "And, importantly, the discord can be expressed as a function of the parameters.",
                    "label": 1
                },
                {
                    "sent": "So we define the vector B that is, the difference between the feature vector associated to the optimal pair and the mean of the the features and the standard deviation is given by the square root of W transpose CW, and so we should solve for just one sequence in the training set.",
                    "label": 0
                },
                {
                    "sent": "This optimization problem does.",
                    "label": 0
                },
                {
                    "sent": "You see, can be OK. Just say that this problem is equivalent to, so the problem and we prove it.",
                    "label": 0
                },
                {
                    "sent": "And by equivalent I mean that the solution is sequel up to a scaling factor.",
                    "label": 0
                },
                {
                    "sent": "Let's have a look at the 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just now.",
                    "label": 0
                },
                {
                    "sent": "Before we define a criterium that is.",
                    "label": 0
                },
                {
                    "sent": "That is that we called the ranking loss.",
                    "label": 0
                },
                {
                    "sent": "So basically in the ranking loss we are ranking loss is the sum of the number of scores that are larger than the score of the optimal pair.",
                    "label": 1
                },
                {
                    "sent": "In this case I is an indicator function and we what we are doing is we are maximizing minimizing an upper bound on this quantity.",
                    "label": 1
                },
                {
                    "sent": "Since our objective function is an upper bound on the ranking loss in this way, we are sure that the number of output vector with with ever score.",
                    "label": 0
                },
                {
                    "sent": "I heard and the score of the correct payers is minimized.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why our approach is different from previous approach?",
                    "label": 0
                },
                {
                    "sent": "Because a classical approach will be to minimize the in the training set.",
                    "label": 0
                },
                {
                    "sent": "The number of incorrect micro labels, and in that case the loss will be that one and this is the approach followed by previous approach like conditional random fields by Lafferty DeMarco, support vector machines and the average purchase.",
                    "label": 1
                },
                {
                    "sent": "Another approach the approach is followed by Tasker and by SVM Heiser.",
                    "label": 0
                },
                {
                    "sent": "Is to minimize the number of incorrect micro labels.",
                    "label": 1
                },
                {
                    "sent": "We do not use it substantially since as this car show this morning, this required that loss should decompose in the structure in the graph and with our approach we don't know.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have this problem.",
                    "label": 0
                },
                {
                    "sent": "So we are back in the in the hour optimization problem and we want to extend the optimization problem in the case of an arbitrary training set with helpers in the in the input output pairs.",
                    "label": 0
                },
                {
                    "sent": "So basically what we're doing is we are.",
                    "label": 0
                },
                {
                    "sent": "We are minimizing the empirical risk has decided to the upper bound on the ranking loss previously defined and this is the optimization problem that we solved.",
                    "label": 1
                },
                {
                    "sent": "We consider an equivalent formulation.",
                    "label": 1
                },
                {
                    "sent": "Again, equivalent means that the solution is equivalent up to a scaling factors and that is this one.",
                    "label": 0
                },
                {
                    "sent": "And as you can see is expressed in terms of the B matrix and of the vector and the metrics that we introduced before in the compression phase.",
                    "label": 0
                },
                {
                    "sent": "And this is the algorithm that we call the structured output indiscriminant analysis, since it could be seen as an extension of the linear discriminant analysis to this setting or structured out.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problems.",
                    "label": 0
                },
                {
                    "sent": "And so the problem that we have to solve, this one we are maximizing.",
                    "label": 0
                },
                {
                    "sent": "This ratio this is an optimization where convex optimization problem.",
                    "label": 1
                },
                {
                    "sent": "Perhaps you will see it better here and then to solve it.",
                    "label": 1
                },
                {
                    "sent": "The first thing is we should observe that if the metric see is not positive semidefinite, we should introduce regularisation too.",
                    "label": 0
                },
                {
                    "sent": "To solve better the problem and then we can solve the problem by simple matrix inversion and, but in theory inverting these metrics can be.",
                    "label": 1
                },
                {
                    "sent": "Ava I competition of course, but we we use the fast, can you get the gradient methods and to speed up the optimization?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And face.",
                    "label": 0
                },
                {
                    "sent": "I will just give an idea about this theoretical stuff that perhaps I don't want to bore anyone and we developed the Rademacher bound that showed that learning with our upper bounds is effectively achieved and we want that bound holds even when B&C are estimated by sampling, and we want that to Cynthia.",
                    "label": 1
                },
                {
                    "sent": "In this way we can really speed up.",
                    "label": 0
                },
                {
                    "sent": "The optimization, then our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we consider two direction of sampling.",
                    "label": 0
                },
                {
                    "sent": "The first one is that for each input output pair in the training set, we consider only a limited number of uncorrect outputs to estimate the matrices by sampling.",
                    "label": 1
                },
                {
                    "sent": "And also we consider only a finished number of input output pairs in the training set and the bound that I will show in the next slide.",
                    "label": 0
                },
                {
                    "sent": "Basically, we prove that empirical expectation overestimated loss.",
                    "label": 0
                },
                {
                    "sent": "Added the scenes that we're computing B&C by sampling is a good approximate upper bound for the expected loss, and we should also remember that the upper bound that we defined before is an upper bound on the ranking loss.",
                    "label": 1
                },
                {
                    "sent": "So effectively what we are doing is we are minimizing.",
                    "label": 0
                },
                {
                    "sent": "Spectation of the ranking growth.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And see.",
                    "label": 0
                },
                {
                    "sent": "This is the bound and just to give some insight, the five terms here are.",
                    "label": 0
                },
                {
                    "sent": "And then empirical estimate of the spectral lot of the loss determined with sampling to Rademacher quantities that decrease.",
                    "label": 0
                },
                {
                    "sent": "Cause as the inverse of the square root of N in hell.",
                    "label": 0
                },
                {
                    "sent": "And the other two terms here M is a cost and again goes as the inverse of the square root of N anhel.",
                    "label": 0
                },
                {
                    "sent": "So that means that all the quantity on the right side go to zero when the number of samples in.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Freeze.",
                    "label": 0
                },
                {
                    "sent": "Another approach that we consider is a variant of.",
                    "label": 0
                },
                {
                    "sent": "The previous approach is that we call the true this core approach an Araiza since we asked ourself how do we define the discovery of a training set?",
                    "label": 1
                },
                {
                    "sent": "Another possible approach would be to consider that the contributor or feature input output pair is independent from the contributor of the orders.",
                    "label": 0
                },
                {
                    "sent": "So it means that the means and they called and variance computed for each base can be summed in that way.",
                    "label": 0
                },
                {
                    "sent": "We should assume to have an estimate of the true mean of the distribution for all four overal input output pair of the training set and as well for the covariance.",
                    "label": 0
                },
                {
                    "sent": "And this is what we call the disk or approach the problem.",
                    "label": 0
                },
                {
                    "sent": "The final problem is this one.",
                    "label": 0
                },
                {
                    "sent": "As you can see the structure of the problem.",
                    "label": 0
                },
                {
                    "sent": "Is similar to the one for the soda, where we again have a convex optimization problem that can be solved by simple matrix inversion.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Basically, the result that I will show you later are quite similar.",
                    "label": 0
                },
                {
                    "sent": "In both case, if you work it out this formula, you can see that we are minimizing kind of upper bound on the ranking loss again, so that's why perhaps that you measured are very similar.",
                    "label": 0
                },
                {
                    "sent": "But for this second method we also try a different strategy, because basically when we maximize this, Cora must of the linear constraints.",
                    "label": 0
                },
                {
                    "sent": "Of this kind are satisfied.",
                    "label": 0
                },
                {
                    "sent": "That means that the score of the correct pace in most of the cases larger than the score of all incorrect ones.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if one want to impose is pleasantly that discover of the optimal pair should be bigger than the score of all uncorrect.",
                    "label": 1
                },
                {
                    "sent": "One, we could add the constraint that quadratic programming problem.",
                    "label": 0
                },
                {
                    "sent": "We can solve it by iterative algorithms in a similar way to previous approaches, and eventually in that case when the data are not linearly separable we can relax constraint by adding Slack variables.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the algorithm in that case will be this.",
                    "label": 0
                },
                {
                    "sent": "So we have basically four steps in the first step we compute the moment, in the second step we maximize the discord without any constraints.",
                    "label": 0
                },
                {
                    "sent": "Then we enter in the main loop and for each input output pair in the training set, we compute.",
                    "label": 0
                },
                {
                    "sent": "We identify the Max violated constraints, and then if the score or associated to that is bigger.",
                    "label": 0
                },
                {
                    "sent": "Larger than the score of the the correct input output pair we add to the set of constraints constraint that impose that the score of the input of the correct input output pair should be bigger, and so we solve the.",
                    "label": 0
                },
                {
                    "sent": "We maximize the discourse object to that constrains.",
                    "label": 0
                },
                {
                    "sent": "And we repeat this until the set of constraints that not change.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ninja in decoration.",
                    "label": 0
                },
                {
                    "sent": "We finally show some experimental results are related to the, so the algorithm to the proper disk or approach and to the disk or approach with constraints will learn briefly show that our methods perform better than the orders in case of high level of noise.",
                    "label": 0
                },
                {
                    "sent": "What we are doing here is we are generated random sequences by starting from a random vector of parameters.",
                    "label": 0
                },
                {
                    "sent": "For a chain conditional random field with HMM features, we consider two kind of conditional random field, one with this when the size of the observation alphabet is 4 and the size of the Eden alphabet is 2, another one when the side of their Salvation alphabet is 5 and the sides of the hidden alphabet is 3.",
                    "label": 0
                },
                {
                    "sent": "Sequence are length of a length of 50 and the training set side is made by 20 pairs.",
                    "label": 1
                },
                {
                    "sent": "The test side there is made by 100 pays.",
                    "label": 0
                },
                {
                    "sent": "All the results are generated by.",
                    "label": 0
                },
                {
                    "sent": "Waiting at the process for one under the runs and we compare our approach with the previous approaches and we plot here the average number of incorrect labels at varying level of noise and you can see our approach are better in case of large P&P represent the probability of.",
                    "label": 1
                },
                {
                    "sent": "Mirror represented the level of noise that we generated by flipping Eden labels in the training set.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another problem that I know there are simulation that we did is we consider the learning curve.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the second graph here, when the level of noise is zero, point.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "We plot the learning curve for the all the algorithms that we consider.",
                    "label": 0
                },
                {
                    "sent": "I do not show here the discourse in the performance are very similar to soda and what we observe is that our approach performs better than conditional random fields and the perceptron and is very competitive and somehow better than the SVM Heiser that.",
                    "label": 0
                },
                {
                    "sent": "Username in gloss.",
                    "label": 0
                },
                {
                    "sent": "Similar to the algorithm that presented this morning by Tasker, and in this case we see that the performance in term of this error are very similar with the SVM is about when we analyze the computational cost of learning, we see that our approach is much more faster, especially for when the sides of the training set become.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's larger.",
                    "label": 0
                },
                {
                    "sent": "Hello there experiments that we did.",
                    "label": 0
                },
                {
                    "sent": "So again, with artificial data is that we consider a chain conditional random field with HMM features an the size of the hidden alphabet is 3 the size of their survey shun alphabet changes in the experiment.",
                    "label": 1
                },
                {
                    "sent": "Here we want to show that with sampling approaches we can obtain good results.",
                    "label": 0
                },
                {
                    "sent": "So basically we compar.",
                    "label": 0
                },
                {
                    "sent": "Use our approach by computing metrics with dynamic programming and by computing metrics with random with estimates.",
                    "label": 0
                },
                {
                    "sent": "Of the metrics with the approximate sampling on 50 parts and 200 parts, and we plot the results in terms of this error, and we see that all the formatters are more or less equivalent.",
                    "label": 0
                },
                {
                    "sent": "But of course using sampling the time is much the computational cost and the time for learning is much more reduced.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello, the simulation that we did again with artificial data is to show the behavior of the disk or approach when constraints are added.",
                    "label": 1
                },
                {
                    "sent": "In this case, what we want to show is basically that our approach need much less constraints that previous iterative approach, similar approach.",
                    "label": 0
                },
                {
                    "sent": "And as you can see here, the number of constraints needed is is much less than.",
                    "label": 1
                },
                {
                    "sent": "The orders and but we did write you so much the disk or approach with constraints, since we see that in many problems that are noisy and linearly nonlinear with parable.",
                    "label": 0
                },
                {
                    "sent": "So the other version are.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Better.",
                    "label": 0
                },
                {
                    "sent": "I showed you before the result with artificial data to understand how the algorithms works, but now I will show some results with real data on sequence labeling and in particular name entity recognition.",
                    "label": 0
                },
                {
                    "sent": "We consider the Spanish newer heartical corpus for the special session of Natural Language Competition or 2002.",
                    "label": 1
                },
                {
                    "sent": "We consider only 300 sentences with an average of 30 words.",
                    "label": 1
                },
                {
                    "sent": "We use a small corpus in order.",
                    "label": 0
                },
                {
                    "sent": "To compare our approach with other with another paper that is using the same.",
                    "label": 0
                },
                {
                    "sent": "The same setting experimental setting.",
                    "label": 0
                },
                {
                    "sent": "We have 9 labels that indicates no name beginning and continuation of person, organization, location, miscellaneous names, and we used to set of binary features.",
                    "label": 1
                },
                {
                    "sent": "So in the first set, just the HMM features in the second set, the set S1 or features and the HMM feature for the previous and the next word.",
                    "label": 1
                },
                {
                    "sent": "In this way we can model the interaction between adjacent words.",
                    "label": 0
                },
                {
                    "sent": "In this case I plot the result for both the score I showed the result for both discord and soda.",
                    "label": 0
                },
                {
                    "sent": "And as you can see in the previous set of features, this side upper outperform all the other methods in the second set of features is the discord that performs better.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Add another.",
                    "label": 0
                },
                {
                    "sent": "Mental results that I would like to show is for sequence alignment.",
                    "label": 1
                },
                {
                    "sent": "We use artificial sequences because what we want to see is that starting from a random matrix of para meters, are we able with learning to reconstruct the.",
                    "label": 0
                },
                {
                    "sent": "That the sequence that the metrics of para meters.",
                    "label": 0
                },
                {
                    "sent": "Basically we have one substitution matrix generated randomly, but just imposing that the values on the diagonal should be bigger than the values out of the diagonal.",
                    "label": 0
                },
                {
                    "sent": "And then we consider the sequence of length 100 and the various sides of the training set from 5 to 100 and we.",
                    "label": 1
                },
                {
                    "sent": "And the size of the data set is 4 is 100.",
                    "label": 1
                },
                {
                    "sent": "The pair of sequences and for the training set of 100 we for one simulation we see that we are able to reconstruct a more or less the structure of the metrics and Moreover if we plot the result in terms of test errors and we compare our approach with a generative approach where parameters are determined by.",
                    "label": 0
                },
                {
                    "sent": "Boston estimates we see the test error for our approach is much lower, especially for data set for training set or small sides.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last experiment is to show you how our method or discord with constraints performs an is for sequence parsing.",
                    "label": 1
                },
                {
                    "sent": "We use Hydramatic that we draw on that paper that we see is particularly adapted to model the problem of RNA secondary structure prediction.",
                    "label": 0
                },
                {
                    "sent": "We extract five families from the FM database.",
                    "label": 1
                },
                {
                    "sent": "And we plot the results in terms of sensitivity and specificity for all the for all the different family in the database we compare our approach with this current generative approach and the perceptron approach, and we see that both discriminative approaches so that this current perceptron outperform the generative approach and the performance of this current perceptron are more or less.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The same.",
                    "label": 0
                },
                {
                    "sent": "So concluding have presented to new methods for learning structured output space.",
                    "label": 1
                },
                {
                    "sent": "We saw that accuracy is compatible with state of the art techniques we with our method are very easy to implement since they rely on dynamic programming for metrics for statistic computations for moment computation, and on a simple convex optimization problem is fast.",
                    "label": 0
                },
                {
                    "sent": "For large training set.",
                    "label": 0
                },
                {
                    "sent": "And the reasonable number of features since the mean and variance computation is parallelizable for large training set.",
                    "label": 1
                },
                {
                    "sent": "And since the invention of the metrics can be performed by conjugate gradient techniques.",
                    "label": 1
                },
                {
                    "sent": "Three application sequence labeling, sequence parsing, and sequence alignment.",
                    "label": 1
                },
                {
                    "sent": "There's a lot of work to do.",
                    "label": 0
                },
                {
                    "sent": "First of all, we should be able to scale our approach for a lot of features as required in many practical application and how to do that, we should exploit the bit more the sampling approach.",
                    "label": 0
                },
                {
                    "sent": "We also need to develop a dual version.",
                    "label": 0
                },
                {
                    "sent": "We kernels in order to deal with non separable data.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Correct somehow?",
                    "label": 0
                },
                {
                    "sent": "Don't let your data have causing normality when you use this call you soon that's metrics at the.",
                    "label": 0
                },
                {
                    "sent": "Somehow abortion like not really, since for example the other matter bound is like when you use Fisher discriminate that you could have the same problem.",
                    "label": 0
                },
                {
                    "sent": "But really if you develop the Rademacher bounds you can prove that works even if we do not assume any.",
                    "label": 0
                },
                {
                    "sent": "We do not have any assumption.",
                    "label": 0
                },
                {
                    "sent": "Oshana of those sanity for the distribution of this course, it's a bit complex plane towards, but this is the principle rather maker bound.",
                    "label": 0
                },
                {
                    "sent": "Show that you achieve learning even if it is not Goshen.",
                    "label": 0
                },
                {
                    "sent": "Remember.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the disease called hasn't definition beautiful under those assumptions, but you're not using it.",
                    "label": 0
                },
                {
                    "sent": "Kate.",
                    "label": 0
                }
            ]
        }
    }
}