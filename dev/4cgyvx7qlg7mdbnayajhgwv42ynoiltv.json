{
    "id": "4cgyvx7qlg7mdbnayajhgwv42ynoiltv",
    "title": "Minimax Regret of Finite Partial-Monitoring Games in Stochastic Environments",
    "info": {
        "author": [
            "G\u00e1bor Bart\u00f3k, Department of Computer Science, ETH Zurich"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Mathematics->Game Theory"
        ]
    },
    "url": "http://videolectures.net/colt2011_bartok_games/",
    "segmentation": [
        [
            "OK, so I'll start with defining partial monitoring because it's a little different than for from the previous talk."
        ],
        [
            "So consider the learner and an environment.",
            "Repeated game in in every time step, the learner chooses an action and the environment choosing an outcome.",
            "They they give their choices to every free and they referred as the following things.",
            "The referee calculates the feedback.",
            "Based on the the action and the outcome and the feedback function, and it calculates a loss based on the loss function and the action and the outcome.",
            "And it's important to know that these these functions are known to both the learner and environment and everybody.",
            "And then in the in the next step, the referee gives the feedback to the learner an notes the loss, but it the loss is not revealed to the learner.",
            "And in this talk we will care about finite stochastic finite partial merging with finitely many actions and outcomes.",
            "And stochastic environment, meaning that the outcomes are chosen in an IID manner every time step."
        ],
        [
            "OK, some examples of partial monitoring.",
            "If the loss function and the feedback function are the same, then we talk about bandits because the learner gets exactly the loss as feedback.",
            "The next example is a full information example.",
            "Aurora expert advice example, where no matter which action the learner chooses this, this role corresponds to action one.",
            "For example, the feedback will be the outcome itself, so the outcome is basically revealed to the learner.",
            "So these are two clinical examples, but partial merging will like partial monitoring because it has some.",
            "Examples that are outside of the scope of these two examples and a good example for that is dynamic pricing, where a vendor wants to sell product at every time step and the customer comes in and wants to buy it, and the vendor sets the price and the customers had a secret maximum price he's willing to buy the product for and then the transaction happens or not.",
            "The feedback to the to the vendor who is actually the learner.",
            "The feedback is only if the the transaction happened or not and the loss is a constant loss if there was no transaction and the difference between the maximum price and the actual price.",
            "If the transaction happened and the dynamic pricing game can be that the.",
            "The discretized version of the dynamic pricing game with an actions and outcomes can be represented with these two matrices.",
            "This is a loss function.",
            "This is the feedback function."
        ],
        [
            "OK, this light is the goal slide so the the performance measure of a player or learner is as usual, the expected regret, which in the stochastic case is the difference between the are expected.",
            "Loss minus the the expected loss of the of the best action in hindsight.",
            "OK, the problem we want to solve in this paper is that if we are given a game and a pair of feedback and loss functions determine the minimax expected regret of the game.",
            "A typical result for a minimax regret is, for example, they say that we have an algorithm and we can show that the minimum expected regret or the expected regret is at most constant Times 3 to the Alpha, and some of you might remember this sentence from yesterday from troubles talk, so you you may think that I stole this sentence from Java, but this is not the truth.",
            "The truth is that David had a talk like two months ago and we both stole this sentence from him."
        ],
        [
            "OK, previous work.",
            "So based on their expected regret, people try to characterize these games.",
            "So this this table shows the gains.",
            "Ordered by their expected regret from zero, which is a trivial games that there is no no regret at all to hopeless game where there is not enough information and basically the learner cannot do anything.",
            "OK, so we know from from these people that the full information and the bandit games are the minimax regulating square root of T. And.",
            "And we know that in general, if a game is not hopeless then then Pickle mentioned on her way introduced an algorithm based on Peter always X3 algorithm that achieved 3 to 3/4 expected regret whenever the game is not hopeless.",
            "So they actually showed that there is a gap here.",
            "And then later on Nicolo Gabor NZL showed that the very same algorithm has a little better regret that pick up until her thought.",
            "And they also also showed in their paper that there exists again variant of the label efficient prediction game in which the there is a lower bound of T-22.",
            "Third, on the regret, meaning that this bound is in some sense tight in the sense that there exists at least one game where we cannot do any better.",
            "But the question is still there.",
            "Is that true for all games or what can we do in general about about the game?",
            "Yeah, so the next step was that we showed with on Tosh and some other people that that the gap here that if a game is not trivial then the expected regret will jump immediately to at least to the 1/2.",
            "So the remaining part is this Gray area, including dynamic pricing.",
            "These results are on non stochastic results, but they all apply to the stochastic case."
        ],
        [
            "OK, so we still have the same table.",
            "What can we do?",
            "We know that this Gray area that the lower bounds on the expected uses square root of T, the upper bound is tied to the 2/3.",
            "But other games would say to the 3/5.",
            "We regret minimax regret.",
            "That's what we try to figure out.",
            "And it turns out to be the answer turns out to be no, so there is no games in between, meaning that if we have a game that it will.",
            "It will be either one of these four categories and as an extra we show that the dynamic pricing game is hard there.",
            "You cannot be.",
            "You cannot do better than three to the 2/3.",
            "So the main theorem is that the minimax regret of any finite finite partial monitoring game against this stochastic opponent or environment can be 0 sqrt T we hide a little~ here, meaning that we have some extra logarithmic terms in our proofs.",
            "Did Luther or linear, meaning the hopeless game?"
        ],
        [
            "OK, so how do we do this?",
            "There we have we have these two matrices, the matrix L, the matrix age and.",
            "Well, I'll start with L explaining what we can do with them.",
            "OK, so how do we use the information from LOL is consists of a bunch of rows.",
            "Every action corresponds to a row and.",
            "And on the other hand you this is.",
            "This is the space of all the outcome distributions.",
            "So here P is a distribution over all the outcomes.",
            "And.",
            "This L matrix.",
            "These rows give us a cell, the composition of the of the probability simplex, the space of outcome distributions showing that, for example, the yellow action is optimal whenever the outcome distribution is in the yellow area.",
            "In the example where the distribution is this one, then then the orange action is the optimal action, and so on.",
            "OK. Just a little note, save it for later that the boundary between between two actions within these cells between of two actions lies in.",
            "In this subspace, where Ally is the corresponding row of the action."
        ],
        [
            "OK, what can we do with age?",
            "So age consists of symbols.",
            "Age is basically has only the information what we get as feedback, so they're not necessarily numbers, but when we when we choose action, I then in this example we get feedback a if the outcome was one or three feedback be the outcome was too and feedback.",
            "See the outcome of those four.",
            "A natural question that arises that if we if we are given an open strategy or environment strategy, P was the probability of observing these symbols.",
            "In this case, it's really obvious that the probability of observing simple A is B 1 + 3 three, and so on and so on.",
            "But how can we generalize it there to the general case?",
            "We like linear algebra, so let's let's fill this table.",
            "It's not.",
            "It's not very hard the the P1 plus please three comes if we.",
            "If we put 1010 here, it's really obvious and the 2nd row will be this and the third will be this.",
            "And now if we have a look at this matrix then then we can see that the first role corresponds to the indicator of symbol A.",
            "The 2nd row is the indicator of symbol be the third reason code or symbol C. And so because of this we we call this matrix the signal matrix of action I.",
            "If we have more than one action, we have to, we want to care about, then we can stack these simple matrixes on top of each other and we get signal matrices for more actions.",
            "And why is this important or interesting?",
            "There is 1 interesting thing to note that if if we have two outcome distributions.",
            "And we we can only choose action I and I prime and these two vectors are the same.",
            "Then we cannot distinguish between the two outcome distributions at all, no matter what we do and no matter how the outcome outcomes come.",
            "Based on these distributions, there is no way we can figure out which which outcome distribution the environment shows at the beginning.",
            "So we can say that the kernel of this matrix is is an area of danger."
        ],
        [
            "OK, so.",
            "What makes a game easy?",
            "The question arises and the answer is that the game is easy if we can.",
            "If we can figure out which action is better.",
            "By not choosing any other actions, just that too.",
            "So we want to decide the question with two neighbor actions.",
            "Which one is better?",
            "And we don't want to use any other actions because that might be costly as from the label efficient prediction game we know.",
            "So this is the main condition that will characterize the easy and hard gains.",
            "So the local observer ability condition says that every neighboring action pair for every neighboring action pair.",
            "The difference of the two loss vectors.",
            "Is in the row space of this of the signal matrix?",
            "Well, this sounds a little arbitrary right now, but it turns out that this condition enables us to estimate the expected difference of the two losses.",
            "No matter what the outcome distribution is.",
            "By choosing only these two actions."
        ],
        [
            "OK, so based on this we can have an algorithm.",
            "The algorithm is as follows.",
            "We maintain a set of alive actions.",
            "An in every round we choose each alive action.",
            "Once it's like a racing game, we choose each live action once, and then we update.",
            "We maintain an estimate or for the loss differences for every action pair and after each round we update these estimates and then if we are.",
            "If it turns out that we are, we are confident in and in that in the sign of an action allows difference.",
            "Let's say that by a large margin we can say that the difference between two losses is negative or positive.",
            "Then we can eliminate the whole house space.",
            "Let's say that it turns out that.",
            "The yellow action is significantly better than the OR injection with confidence.",
            "Then we can eliminate this whole house space and note that here not only the orange action was eliminated, but this grayish action was eliminated as well, because we can be sure that if the if we know that the outcome distribution is on this half space.",
            "But then this cannot be optimal.",
            "So we can eliminate this section and then then we do it on until we run out of time or only one action remains.",
            "And this algorithm achieves.",
            "Aldila square root of the regret.",
            "So that's good that if the condition holds then we have upper bounded, they expected regret by square root of T. Oak."
        ],
        [
            "What?",
            "What is the other case?",
            "The other case when we have two actions that are neighbors?",
            "And we don't have enough feedback, meaning that there is there is a line in the outcome distribution space where we cannot distinguish between outcome distributions.",
            "Based on actions I&J and, this is where the new space of SJ comes in.",
            "In this case, 1/3 action is needed to decide which action is better.",
            "And that's why that's why it becomes costly.",
            "And so there is a little coincidence here.",
            "When does this line exist?",
            "This is the condition when their local over really the condition doesn't hold so that the level of difference is not observable and this is the condition when the line this line of an observed billeti crosses the border, the boundary between the two, the two cells and these two conditions coincide.",
            "Luckily, so we have this scenario exactly when we cannot run the algorithm from the previous slide.",
            "And and we do the usual lower bounding proof technique we put down to distributions very close to the border on the line an we want we want the algorithm to decide which outcome distribution environment shows, and for that we need to pull pull it.",
            "This is not a bandit talk.",
            "We need to choose actions.",
            "Not I&J and that becomes costly."
        ],
        [
            "OK, so in summary we classified all finance stochastic partial monitoring games and it turns out that there is only four kinds of games trivial gains with regret zero when there is an action that that superior to every other action no matter what that issue.",
            "The outcome distribution is is against for which we can run the algorithm.",
            "Hard games where the lower bound holds and hopeless games when there is not even know local observe ability.",
            "I didn't talk about this, but the characterizing hopeless games it was done by pickled onion central, Hower, and and basically equivalent to saying that global survey ability doesn't hold there are there are distributions that are indistinguishable even by choosing all the actions.",
            "So we have the key conditions separating the easy and the hard games, which is the local observe ability condition.",
            "And.",
            "And the algorithm we designed achieves the minimus regret rate.",
            "Some logarithmic factors.",
            "There are some questions.",
            "Computational efficiency of the algorithm.",
            "There is more questions.",
            "First of all is, can we computationally efficiently verify the condition the separating condition and once we decided that the game is easy, is our algorithm efficient?",
            "And it turns out that all our efficiency questions like rely on linear programming, and since we do have polynomial time linear programming solvers, everything is sufficient.",
            "The next question is, how do we scale with the number of actions?",
            "I didn't did not mention at all about scaling with the number of actions, and while the the sad part is that, well, there's two set parts.",
            "The lower bound doesn't scale with the number of actions at all because it only uses 2 actions all the time within the proof.",
            "And the other sad part is that the upper bound is a little ugly.",
            "We hope we can get it done to linear, but that's still not not as good as the bandit where it should be square root of N. So the big question is that is is because there is a lower bound with with linear N or N to three half or or do we need a better analysis or do we need a better algorithm?",
            "Scaling with the number of outcomes.",
            "The good news is that no matter the number of outcomes, these bounds don't have the number of outcomes in them.",
            "And the last question is that what can we say about non stochastic opponents?",
            "Well, the big conjecture is that exactly the same thing holds for non stochastic opponents and it's a very very strong conjecture because any lower bound that we've seen so far was only you only used stochastic opponents so.",
            "Find a separation between adversarial and stochastic games here.",
            "That would be pretty big deal.",
            "I don't think it will happen.",
            "So the only thing we need is an algorithm for easy games in the non stochastic setting, and then we're done with the class."
        ],
        [
            "To fication.",
            "Thank you.",
            "Questions you assume that the feedback is a function of the two actions of the action.",
            "And yes, yeah, I wanted to say there is no noise involved, so if there is an action and outcome then from that point everything is deterministic.",
            "It's a function of those, yes, so can you extend this to random signals?",
            "We don't have any extension.",
            "We really strong really rely on the on these combinatorial and dinner.",
            "Algebraic structures, so probably something different would be needed to solve that case.",
            "Questions.",
            "Questions can you extend this to infinite games or with the same infinitely many actions or infinitely many outcomes?",
            "Actions with infinitely many actions?",
            "Obviously you need some extra structure structure on the actions.",
            "And, well, we hope that we will be able to extend it.",
            "I don't know how yet about outcomes if we.",
            "If we just leave this structure and say infinitely many outcomes, and then the matrix matrices become integral operators, and probably the answer is no.",
            "But if we, if we say that an outcome is a convex combination of M. An atomic outcomes, then the answer is actually yes, so this this whole thing generalizes without any modification, and then we have solutions to infinitely infinite number of outcomes.",
            "Alright.",
            "Anymore, alright, let's thank number."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'll start with defining partial monitoring because it's a little different than for from the previous talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So consider the learner and an environment.",
                    "label": 0
                },
                {
                    "sent": "Repeated game in in every time step, the learner chooses an action and the environment choosing an outcome.",
                    "label": 0
                },
                {
                    "sent": "They they give their choices to every free and they referred as the following things.",
                    "label": 0
                },
                {
                    "sent": "The referee calculates the feedback.",
                    "label": 0
                },
                {
                    "sent": "Based on the the action and the outcome and the feedback function, and it calculates a loss based on the loss function and the action and the outcome.",
                    "label": 0
                },
                {
                    "sent": "And it's important to know that these these functions are known to both the learner and environment and everybody.",
                    "label": 0
                },
                {
                    "sent": "And then in the in the next step, the referee gives the feedback to the learner an notes the loss, but it the loss is not revealed to the learner.",
                    "label": 0
                },
                {
                    "sent": "And in this talk we will care about finite stochastic finite partial merging with finitely many actions and outcomes.",
                    "label": 1
                },
                {
                    "sent": "And stochastic environment, meaning that the outcomes are chosen in an IID manner every time step.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, some examples of partial monitoring.",
                    "label": 1
                },
                {
                    "sent": "If the loss function and the feedback function are the same, then we talk about bandits because the learner gets exactly the loss as feedback.",
                    "label": 0
                },
                {
                    "sent": "The next example is a full information example.",
                    "label": 0
                },
                {
                    "sent": "Aurora expert advice example, where no matter which action the learner chooses this, this role corresponds to action one.",
                    "label": 0
                },
                {
                    "sent": "For example, the feedback will be the outcome itself, so the outcome is basically revealed to the learner.",
                    "label": 0
                },
                {
                    "sent": "So these are two clinical examples, but partial merging will like partial monitoring because it has some.",
                    "label": 0
                },
                {
                    "sent": "Examples that are outside of the scope of these two examples and a good example for that is dynamic pricing, where a vendor wants to sell product at every time step and the customer comes in and wants to buy it, and the vendor sets the price and the customers had a secret maximum price he's willing to buy the product for and then the transaction happens or not.",
                    "label": 0
                },
                {
                    "sent": "The feedback to the to the vendor who is actually the learner.",
                    "label": 0
                },
                {
                    "sent": "The feedback is only if the the transaction happened or not and the loss is a constant loss if there was no transaction and the difference between the maximum price and the actual price.",
                    "label": 1
                },
                {
                    "sent": "If the transaction happened and the dynamic pricing game can be that the.",
                    "label": 0
                },
                {
                    "sent": "The discretized version of the dynamic pricing game with an actions and outcomes can be represented with these two matrices.",
                    "label": 0
                },
                {
                    "sent": "This is a loss function.",
                    "label": 0
                },
                {
                    "sent": "This is the feedback function.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this light is the goal slide so the the performance measure of a player or learner is as usual, the expected regret, which in the stochastic case is the difference between the are expected.",
                    "label": 0
                },
                {
                    "sent": "Loss minus the the expected loss of the of the best action in hindsight.",
                    "label": 0
                },
                {
                    "sent": "OK, the problem we want to solve in this paper is that if we are given a game and a pair of feedback and loss functions determine the minimax expected regret of the game.",
                    "label": 1
                },
                {
                    "sent": "A typical result for a minimax regret is, for example, they say that we have an algorithm and we can show that the minimum expected regret or the expected regret is at most constant Times 3 to the Alpha, and some of you might remember this sentence from yesterday from troubles talk, so you you may think that I stole this sentence from Java, but this is not the truth.",
                    "label": 0
                },
                {
                    "sent": "The truth is that David had a talk like two months ago and we both stole this sentence from him.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, previous work.",
                    "label": 0
                },
                {
                    "sent": "So based on their expected regret, people try to characterize these games.",
                    "label": 0
                },
                {
                    "sent": "So this this table shows the gains.",
                    "label": 0
                },
                {
                    "sent": "Ordered by their expected regret from zero, which is a trivial games that there is no no regret at all to hopeless game where there is not enough information and basically the learner cannot do anything.",
                    "label": 0
                },
                {
                    "sent": "OK, so we know from from these people that the full information and the bandit games are the minimax regulating square root of T. And.",
                    "label": 0
                },
                {
                    "sent": "And we know that in general, if a game is not hopeless then then Pickle mentioned on her way introduced an algorithm based on Peter always X3 algorithm that achieved 3 to 3/4 expected regret whenever the game is not hopeless.",
                    "label": 0
                },
                {
                    "sent": "So they actually showed that there is a gap here.",
                    "label": 0
                },
                {
                    "sent": "And then later on Nicolo Gabor NZL showed that the very same algorithm has a little better regret that pick up until her thought.",
                    "label": 0
                },
                {
                    "sent": "And they also also showed in their paper that there exists again variant of the label efficient prediction game in which the there is a lower bound of T-22.",
                    "label": 0
                },
                {
                    "sent": "Third, on the regret, meaning that this bound is in some sense tight in the sense that there exists at least one game where we cannot do any better.",
                    "label": 0
                },
                {
                    "sent": "But the question is still there.",
                    "label": 0
                },
                {
                    "sent": "Is that true for all games or what can we do in general about about the game?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the next step was that we showed with on Tosh and some other people that that the gap here that if a game is not trivial then the expected regret will jump immediately to at least to the 1/2.",
                    "label": 0
                },
                {
                    "sent": "So the remaining part is this Gray area, including dynamic pricing.",
                    "label": 0
                },
                {
                    "sent": "These results are on non stochastic results, but they all apply to the stochastic case.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we still have the same table.",
                    "label": 0
                },
                {
                    "sent": "What can we do?",
                    "label": 0
                },
                {
                    "sent": "We know that this Gray area that the lower bounds on the expected uses square root of T, the upper bound is tied to the 2/3.",
                    "label": 0
                },
                {
                    "sent": "But other games would say to the 3/5.",
                    "label": 0
                },
                {
                    "sent": "We regret minimax regret.",
                    "label": 0
                },
                {
                    "sent": "That's what we try to figure out.",
                    "label": 0
                },
                {
                    "sent": "And it turns out to be the answer turns out to be no, so there is no games in between, meaning that if we have a game that it will.",
                    "label": 0
                },
                {
                    "sent": "It will be either one of these four categories and as an extra we show that the dynamic pricing game is hard there.",
                    "label": 0
                },
                {
                    "sent": "You cannot be.",
                    "label": 0
                },
                {
                    "sent": "You cannot do better than three to the 2/3.",
                    "label": 0
                },
                {
                    "sent": "So the main theorem is that the minimax regret of any finite finite partial monitoring game against this stochastic opponent or environment can be 0 sqrt T we hide a little~ here, meaning that we have some extra logarithmic terms in our proofs.",
                    "label": 1
                },
                {
                    "sent": "Did Luther or linear, meaning the hopeless game?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how do we do this?",
                    "label": 0
                },
                {
                    "sent": "There we have we have these two matrices, the matrix L, the matrix age and.",
                    "label": 0
                },
                {
                    "sent": "Well, I'll start with L explaining what we can do with them.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we use the information from LOL is consists of a bunch of rows.",
                    "label": 0
                },
                {
                    "sent": "Every action corresponds to a row and.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand you this is.",
                    "label": 0
                },
                {
                    "sent": "This is the space of all the outcome distributions.",
                    "label": 0
                },
                {
                    "sent": "So here P is a distribution over all the outcomes.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This L matrix.",
                    "label": 0
                },
                {
                    "sent": "These rows give us a cell, the composition of the of the probability simplex, the space of outcome distributions showing that, for example, the yellow action is optimal whenever the outcome distribution is in the yellow area.",
                    "label": 1
                },
                {
                    "sent": "In the example where the distribution is this one, then then the orange action is the optimal action, and so on.",
                    "label": 0
                },
                {
                    "sent": "OK. Just a little note, save it for later that the boundary between between two actions within these cells between of two actions lies in.",
                    "label": 0
                },
                {
                    "sent": "In this subspace, where Ally is the corresponding row of the action.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what can we do with age?",
                    "label": 0
                },
                {
                    "sent": "So age consists of symbols.",
                    "label": 0
                },
                {
                    "sent": "Age is basically has only the information what we get as feedback, so they're not necessarily numbers, but when we when we choose action, I then in this example we get feedback a if the outcome was one or three feedback be the outcome was too and feedback.",
                    "label": 0
                },
                {
                    "sent": "See the outcome of those four.",
                    "label": 0
                },
                {
                    "sent": "A natural question that arises that if we if we are given an open strategy or environment strategy, P was the probability of observing these symbols.",
                    "label": 1
                },
                {
                    "sent": "In this case, it's really obvious that the probability of observing simple A is B 1 + 3 three, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "But how can we generalize it there to the general case?",
                    "label": 0
                },
                {
                    "sent": "We like linear algebra, so let's let's fill this table.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not very hard the the P1 plus please three comes if we.",
                    "label": 0
                },
                {
                    "sent": "If we put 1010 here, it's really obvious and the 2nd row will be this and the third will be this.",
                    "label": 0
                },
                {
                    "sent": "And now if we have a look at this matrix then then we can see that the first role corresponds to the indicator of symbol A.",
                    "label": 1
                },
                {
                    "sent": "The 2nd row is the indicator of symbol be the third reason code or symbol C. And so because of this we we call this matrix the signal matrix of action I.",
                    "label": 0
                },
                {
                    "sent": "If we have more than one action, we have to, we want to care about, then we can stack these simple matrixes on top of each other and we get signal matrices for more actions.",
                    "label": 0
                },
                {
                    "sent": "And why is this important or interesting?",
                    "label": 0
                },
                {
                    "sent": "There is 1 interesting thing to note that if if we have two outcome distributions.",
                    "label": 0
                },
                {
                    "sent": "And we we can only choose action I and I prime and these two vectors are the same.",
                    "label": 0
                },
                {
                    "sent": "Then we cannot distinguish between the two outcome distributions at all, no matter what we do and no matter how the outcome outcomes come.",
                    "label": 0
                },
                {
                    "sent": "Based on these distributions, there is no way we can figure out which which outcome distribution the environment shows at the beginning.",
                    "label": 1
                },
                {
                    "sent": "So we can say that the kernel of this matrix is is an area of danger.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What makes a game easy?",
                    "label": 1
                },
                {
                    "sent": "The question arises and the answer is that the game is easy if we can.",
                    "label": 1
                },
                {
                    "sent": "If we can figure out which action is better.",
                    "label": 1
                },
                {
                    "sent": "By not choosing any other actions, just that too.",
                    "label": 0
                },
                {
                    "sent": "So we want to decide the question with two neighbor actions.",
                    "label": 0
                },
                {
                    "sent": "Which one is better?",
                    "label": 0
                },
                {
                    "sent": "And we don't want to use any other actions because that might be costly as from the label efficient prediction game we know.",
                    "label": 0
                },
                {
                    "sent": "So this is the main condition that will characterize the easy and hard gains.",
                    "label": 1
                },
                {
                    "sent": "So the local observer ability condition says that every neighboring action pair for every neighboring action pair.",
                    "label": 0
                },
                {
                    "sent": "The difference of the two loss vectors.",
                    "label": 0
                },
                {
                    "sent": "Is in the row space of this of the signal matrix?",
                    "label": 1
                },
                {
                    "sent": "Well, this sounds a little arbitrary right now, but it turns out that this condition enables us to estimate the expected difference of the two losses.",
                    "label": 0
                },
                {
                    "sent": "No matter what the outcome distribution is.",
                    "label": 0
                },
                {
                    "sent": "By choosing only these two actions.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so based on this we can have an algorithm.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is as follows.",
                    "label": 0
                },
                {
                    "sent": "We maintain a set of alive actions.",
                    "label": 1
                },
                {
                    "sent": "An in every round we choose each alive action.",
                    "label": 1
                },
                {
                    "sent": "Once it's like a racing game, we choose each live action once, and then we update.",
                    "label": 0
                },
                {
                    "sent": "We maintain an estimate or for the loss differences for every action pair and after each round we update these estimates and then if we are.",
                    "label": 0
                },
                {
                    "sent": "If it turns out that we are, we are confident in and in that in the sign of an action allows difference.",
                    "label": 0
                },
                {
                    "sent": "Let's say that by a large margin we can say that the difference between two losses is negative or positive.",
                    "label": 0
                },
                {
                    "sent": "Then we can eliminate the whole house space.",
                    "label": 0
                },
                {
                    "sent": "Let's say that it turns out that.",
                    "label": 0
                },
                {
                    "sent": "The yellow action is significantly better than the OR injection with confidence.",
                    "label": 0
                },
                {
                    "sent": "Then we can eliminate this whole house space and note that here not only the orange action was eliminated, but this grayish action was eliminated as well, because we can be sure that if the if we know that the outcome distribution is on this half space.",
                    "label": 0
                },
                {
                    "sent": "But then this cannot be optimal.",
                    "label": 0
                },
                {
                    "sent": "So we can eliminate this section and then then we do it on until we run out of time or only one action remains.",
                    "label": 0
                },
                {
                    "sent": "And this algorithm achieves.",
                    "label": 0
                },
                {
                    "sent": "Aldila square root of the regret.",
                    "label": 0
                },
                {
                    "sent": "So that's good that if the condition holds then we have upper bounded, they expected regret by square root of T. Oak.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What is the other case?",
                    "label": 0
                },
                {
                    "sent": "The other case when we have two actions that are neighbors?",
                    "label": 0
                },
                {
                    "sent": "And we don't have enough feedback, meaning that there is there is a line in the outcome distribution space where we cannot distinguish between outcome distributions.",
                    "label": 0
                },
                {
                    "sent": "Based on actions I&J and, this is where the new space of SJ comes in.",
                    "label": 0
                },
                {
                    "sent": "In this case, 1/3 action is needed to decide which action is better.",
                    "label": 0
                },
                {
                    "sent": "And that's why that's why it becomes costly.",
                    "label": 0
                },
                {
                    "sent": "And so there is a little coincidence here.",
                    "label": 0
                },
                {
                    "sent": "When does this line exist?",
                    "label": 1
                },
                {
                    "sent": "This is the condition when their local over really the condition doesn't hold so that the level of difference is not observable and this is the condition when the line this line of an observed billeti crosses the border, the boundary between the two, the two cells and these two conditions coincide.",
                    "label": 0
                },
                {
                    "sent": "Luckily, so we have this scenario exactly when we cannot run the algorithm from the previous slide.",
                    "label": 0
                },
                {
                    "sent": "And and we do the usual lower bounding proof technique we put down to distributions very close to the border on the line an we want we want the algorithm to decide which outcome distribution environment shows, and for that we need to pull pull it.",
                    "label": 0
                },
                {
                    "sent": "This is not a bandit talk.",
                    "label": 0
                },
                {
                    "sent": "We need to choose actions.",
                    "label": 0
                },
                {
                    "sent": "Not I&J and that becomes costly.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in summary we classified all finance stochastic partial monitoring games and it turns out that there is only four kinds of games trivial gains with regret zero when there is an action that that superior to every other action no matter what that issue.",
                    "label": 0
                },
                {
                    "sent": "The outcome distribution is is against for which we can run the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Hard games where the lower bound holds and hopeless games when there is not even know local observe ability.",
                    "label": 0
                },
                {
                    "sent": "I didn't talk about this, but the characterizing hopeless games it was done by pickled onion central, Hower, and and basically equivalent to saying that global survey ability doesn't hold there are there are distributions that are indistinguishable even by choosing all the actions.",
                    "label": 0
                },
                {
                    "sent": "So we have the key conditions separating the easy and the hard games, which is the local observe ability condition.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm we designed achieves the minimus regret rate.",
                    "label": 0
                },
                {
                    "sent": "Some logarithmic factors.",
                    "label": 0
                },
                {
                    "sent": "There are some questions.",
                    "label": 0
                },
                {
                    "sent": "Computational efficiency of the algorithm.",
                    "label": 1
                },
                {
                    "sent": "There is more questions.",
                    "label": 0
                },
                {
                    "sent": "First of all is, can we computationally efficiently verify the condition the separating condition and once we decided that the game is easy, is our algorithm efficient?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that all our efficiency questions like rely on linear programming, and since we do have polynomial time linear programming solvers, everything is sufficient.",
                    "label": 0
                },
                {
                    "sent": "The next question is, how do we scale with the number of actions?",
                    "label": 0
                },
                {
                    "sent": "I didn't did not mention at all about scaling with the number of actions, and while the the sad part is that, well, there's two set parts.",
                    "label": 0
                },
                {
                    "sent": "The lower bound doesn't scale with the number of actions at all because it only uses 2 actions all the time within the proof.",
                    "label": 1
                },
                {
                    "sent": "And the other sad part is that the upper bound is a little ugly.",
                    "label": 0
                },
                {
                    "sent": "We hope we can get it done to linear, but that's still not not as good as the bandit where it should be square root of N. So the big question is that is is because there is a lower bound with with linear N or N to three half or or do we need a better analysis or do we need a better algorithm?",
                    "label": 0
                },
                {
                    "sent": "Scaling with the number of outcomes.",
                    "label": 1
                },
                {
                    "sent": "The good news is that no matter the number of outcomes, these bounds don't have the number of outcomes in them.",
                    "label": 0
                },
                {
                    "sent": "And the last question is that what can we say about non stochastic opponents?",
                    "label": 0
                },
                {
                    "sent": "Well, the big conjecture is that exactly the same thing holds for non stochastic opponents and it's a very very strong conjecture because any lower bound that we've seen so far was only you only used stochastic opponents so.",
                    "label": 0
                },
                {
                    "sent": "Find a separation between adversarial and stochastic games here.",
                    "label": 0
                },
                {
                    "sent": "That would be pretty big deal.",
                    "label": 0
                },
                {
                    "sent": "I don't think it will happen.",
                    "label": 0
                },
                {
                    "sent": "So the only thing we need is an algorithm for easy games in the non stochastic setting, and then we're done with the class.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To fication.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions you assume that the feedback is a function of the two actions of the action.",
                    "label": 0
                },
                {
                    "sent": "And yes, yeah, I wanted to say there is no noise involved, so if there is an action and outcome then from that point everything is deterministic.",
                    "label": 0
                },
                {
                    "sent": "It's a function of those, yes, so can you extend this to random signals?",
                    "label": 0
                },
                {
                    "sent": "We don't have any extension.",
                    "label": 0
                },
                {
                    "sent": "We really strong really rely on the on these combinatorial and dinner.",
                    "label": 0
                },
                {
                    "sent": "Algebraic structures, so probably something different would be needed to solve that case.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Questions can you extend this to infinite games or with the same infinitely many actions or infinitely many outcomes?",
                    "label": 0
                },
                {
                    "sent": "Actions with infinitely many actions?",
                    "label": 0
                },
                {
                    "sent": "Obviously you need some extra structure structure on the actions.",
                    "label": 0
                },
                {
                    "sent": "And, well, we hope that we will be able to extend it.",
                    "label": 0
                },
                {
                    "sent": "I don't know how yet about outcomes if we.",
                    "label": 0
                },
                {
                    "sent": "If we just leave this structure and say infinitely many outcomes, and then the matrix matrices become integral operators, and probably the answer is no.",
                    "label": 0
                },
                {
                    "sent": "But if we, if we say that an outcome is a convex combination of M. An atomic outcomes, then the answer is actually yes, so this this whole thing generalizes without any modification, and then we have solutions to infinitely infinite number of outcomes.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Anymore, alright, let's thank number.",
                    "label": 0
                }
            ]
        }
    }
}