{
    "id": "r3zogzywbze5fk5xenxsywwdgkvw7t4l",
    "title": "What is the Optimal Number of Features? A learning theoretic perspective",
    "info": {
        "author": [
            "Amir Navot, The Hebrew University of Jerusalem"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "February 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Preprocessing",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/slsfs05_navot_wonfl/",
    "segmentation": [
        [
            "Quickly about the connection between feature selection and generalization accuracy.",
            "And this is a joint work with the Wrangler back lock, if not both and Tony kiss me.",
            "So what is feature selection?",
            "I'm sure that you all know."
        ],
        [
            "After the morning talk, so we go over it quickly.",
            "We are walking in the supervised learning framework.",
            "Therefore we assume that labeled sample training set is given, and we also assume that there is initial set of features and that each instance in the training set is represented by.",
            "A vector of values for all the features in the feature set.",
            "In feature selection is the task of choosing.",
            "And good typically small subset of features out of the given set and good subset in our context is a subset that enables us to be good classifiers.",
            "And therefore feature selection is a special form of dimensionality reduction, as was explained in details in the morning talk.",
            "So there are several."
        ],
        [
            "Reasons to do feature selection.",
            "One of them is reduced computational complexity.",
            "Another one is economy.",
            "As feature selection saves us the cost of measuring unselected features.",
            "And another one is the many cases.",
            "The features themselves for interest and say.",
            "Can provide us some insight about the nature of the problem, but overall these reasons feature selection can also improve the generalization accuracy and in this talk we talk about we focus.",
            "On their list last reason.",
            "Then there is a.",
            "A great deal of empirical evidence that feature selection can improve the classification accuracy, and it is widely believed that using too many weekly relevant features can hurt the performance.",
            "However, we have not been able to find many studies that analyze this phenomenon directly.",
            "And we try to do a first step in this direction."
        ],
        [
            "And the main question you ask is, under which conditions can feature selection improve the classification accuracy?",
            "We also ask what is the optimal number of features from the classification accuracy standpoint and how that this number depends on the training set size.",
            "And then.",
            "We try to discuss this question by by analyzing one simple setting.",
            "The setting is very simple, but it provides us with some interns."
        ],
        [
            "Insights.",
            "How we look on a binary classification task bar in North where N is a very large number?",
            "We assume that the distribution of the points in the positive class is the N dimensional guys, one Gaussian with mean vector mu and unit covariance matrix, and that the distributions of the point in the negative classes the same but with mean equals to minus mu.",
            "Then in this setting, our initial set of features is simply the coordinates in our end.",
            "And.",
            "And since we assume same covariance matrix, the two Gaussians are intersecting and hyperplane in the optimal classifier is simply the sign of the dot product between mu and X.",
            "And if we use only a subset of features F, then the optimal classifier, given this subset is the same.",
            "But when mu and X are first projected on the subset of features.",
            "And we want to ask which subset of features?",
            "And indeed also minimal generalization error, so it is quite clear that if you is known.",
            "Therefore, we know the true distribution of the features.",
            "Then using all the features is optimal.",
            "Because any additional features will only improve the separation between the Gaussian if it equals to zero.",
            "It doesn't matter if you use it or not, but it can't hurt if you know the distribution.",
            "However, we are interested."
        ],
        [
            "In the situation of you is not known and IT system is estimated from a finite sample of size M. And as we see shortly in this situation, using all the feature might not be optimal.",
            "Then given an estimator mu head from you.",
            "In the subset of features.",
            "We consider the induced classifier.",
            "And we want to look on the generalization error of this classifier, but the generalization error is depends on the specific sample as the estimate depends on this sample and therefore we know can the average randomization or will the average is taken with respect to all possible samples of size M and now we want to find the subset of features that minimize.",
            "This this error.",
            "And we also assume, without loss of generality, that the coordinates of mu or order in a descending order according their absolute value, and therefore if we choose to use on the end features and using the first features is optimal and therefore the problem reduces to find only the optimal number of features, not the optimal set of features.",
            "And then the following we consider the minimum variance unbiased estimators formula, which is simply the empirical mean of the training set."
        ],
        [
            "Before I'm going to present the results, I want to show short instruction to make the setting more clear, so we look in R2 and assume this mute.",
            "So the optimal classifier is this dashed red line.",
            "And.",
            "First we look on situation with you is known.",
            "Then if you is known and we use only the first features X access, then we get this classifier.",
            "If we use only the.",
            "Second feature we get this cluster, which is obvious, obviously wrong.",
            "And if we use.",
            "They both features.",
            "We get the optimal classifier, so we see that as we expect, using all the mu is known using all the features optimal.",
            "However, if MU is estimated, for example by this mu hat.",
            "Then using all the features.",
            "If this classifier.",
            "But using only the first features give this classifier, which is better.",
            "So here we see that using all the feature is not optimal and this happens 'cause.",
            "New head fails in estimating the sign of the second feature.",
            "So.",
            "This demonstration suggests that if we estimate mu from.",
            "Small samples from a small sample and therefore estimator has large variance.",
            "Then it might be played there not to use features with two small with a value that is too close to 0.",
            "And now we try to formulate this observation and as we see it a little bit more complicated than just a threshold."
        ],
        [
            "So.",
            "The main result says that the number of feature or features that minimizes average error.",
            "For training set of size N in the end that maximize this term.",
            "And before I describe the proof of this statement statement, let's see what we can learn from it about our questions.",
            "Yeah.",
            "What?",
            "This is this is the truth values NJJ.",
            "Yeah it's a true values of Community Mew Mew is is not known but fixed.",
            "So what we can?"
        ],
        [
            "Learn from this term.",
            "So first we see that if we add the features that equals to zero, it adds nothing to the numerator.",
            "And and sqrt 1 / M to the denominator and therefore it reduces the value of the whole term.",
            "So it.",
            "And this we can conclude that if the values of the coordinates of new approaches to ONJ grows, then there is a nontrivial optimal end on trivial mean, not the maximum possible end.",
            "This way we get for this setting sufficient condition for the situation of feature selection can improve security.",
            "Another thing that we see here is that if you.",
            "Then goes to Infinity, then this whole term reduces to just the square root of the numerator and then using all the features is optimal.",
            "Is any additional feature can just increase the value of the numerator as we expect?",
            "And then other things that we see here, that if we had a feature then it can increase both the value of the numerator and the denominator and therefore the effect on the whole term depends on the current value of the numerator and the denominator.",
            "And he doesn't want to decision on any features.",
            "Depends not only on its value, but also on the values of the features that we already selected.",
            "And this may be surprising as the features here are a statistical independent.",
            "Get here some kind of apparent dependence between independent features.",
            "Rim.",
            "We can also solve this maximization problem for specific choices of mu.",
            "And here we see that if we take you to the continent of mu to decrease exponentially.",
            "Then we get that the optimal number of features is in the same order of magnitude.",
            "Is Logan or M is the size of the training set, and if we take a quantum using Visa harmonic series?",
            "We get optimal number of features is in the order of magnitude square root of M and if we take the square root of the harmonic series we get.",
            "Only of magnitude as a training for size.",
            "And.",
            "We can."
        ],
        [
            "Also, Luke wants the generalization error itself.",
            "These choices of new and force where we see it for three choices of training set size.",
            "The X axis here is a number of selected features.",
            "And then solid lines shows the generalization error of our classifier.",
            "It was a calculating by.",
            "Of arranging one 200 repeats of.",
            "Fay we selecting the training set.",
            "And then we can see the clear minima in all these nine conditions, the clear minimizer?",
            "With the values that support our theoretic results.",
            "Which is the negative effects to make new right?",
            "You estimate you buy that unbiased.",
            "Yeah, we estimate nearby.",
            "Here.",
            "This is estimated."
        ],
        [
            "Yeah, so just because of the symmetric and use all the symmetry is not very important, it is just make all the innovation more simple.",
            "Take it wherever in the spaces to Gaussian.",
            "See as long as they have the same covariance matrix or.",
            "Or so."
        ],
        [
            "The dashed line here is is John Isation over when you is known and we can see that it decreases constantly as we.",
            "And."
        ],
        [
            "OK and then.",
            "Now we will describe the proof briefly very complicated.",
            "Therefore give an estimate on your head.",
            "We look on the generalization error produce classifier.",
            "And this is simply the definition where PP plus.",
            "Denotes the probability given the positive plus minus plus and because of the same symmetrization.",
            "Is this a problem?",
            "The generation that was simply the probability with respect to X that the dot product of knew not X is negative.",
            "Since you had both fixed by itself, is a 1 dimensional Gaussian, then this probability is simply simplify of this term will find the CDF function of the standard option.",
            "May substitute the expectation and variance of X.",
            "We get the this term cause the expectation of X is you.",
            "In the suffix is 1.",
            "Is is unique matrix which coordinates.",
            "Last line is that only one of the classes picking.",
            "No, it's more info.",
            "It may be a little bit confusing here be cause.",
            "It is quite clear that given the positive class, this is a long term but but the problem is completely symmetric, so it is the translation or given the negative classes.",
            "So no matter what is the prior on the classes you take, the average of two equal terms.",
            "X distribution because it's symmetric.",
            "The expectational based stuff is zero, no?",
            "It's probably true that if you didn't know the that you were looking at the text origin, no, I assumed."
        ],
        [
            "I know I work there.",
            "I know the model, I just don't know the music.",
            "So.",
            "Now the the now we want to find the.",
            "The end the number of features that.",
            "Let's minimize this this.",
            "The average this term."
        ],
        [
            "Seen walking zebra this term.",
            "And then for doing this we use this lemma that tells us that at least when M is large enough, it is OK to move the expectation inside inside inside the quotient and insides square root.",
            "I'm not going to describe in details to the proof of this lemma, but it.",
            "It follows from the from the fact that to.",
            "Quantities are concentrated around the mean, or in other words, the variance.",
            "And goes to 0 fast when Ambrose and the fact that the meaning of this term is not zero, and using that I is 1 left its function.",
            "The derivative of lies.",
            "And bounded by Upper bounded by 1.",
            "So.",
            "Now we can look on this there.",
            "And."
        ],
        [
            "Therefore, minimum we conclude using them as minimizing the generalization is every generalization or is equivalent to maximizing this term.",
            "Because if we.",
            "If we want to minimize this term, we have to.",
            "To maximize.",
            "The argument of fee because it's in - so.",
            "And finally substituting the.",
            "Expectation and the second moment of our estimator.",
            "We obtained the statement.",
            "How?"
        ],
        [
            "And.",
            "Here is another demonstration of the lemma instead of precise proof, we just demonstrated empirically and what we see here again for the same three choices of mule and same three choices of their training set size.",
            "Successes against a number of selected features in blue we see the true generalization error.",
            "And then the dashed red line we see the value of the.",
            "Of the lemmas approximation and we can see that.",
            "The two terms of, at least for these examples, the terms are very close even for small values of M."
        ],
        [
            "In the.",
            "One more thing I want to show.",
            "In order to verify that our result is not specific to the classifier, we chose to use, which is first estimate mu.",
            "As best as we can and then use the optimal classifier.",
            "Given this estimation, we also check what is empirically checked, what is a error of linear SVM?",
            "On a function of the number of selected features.",
            "And again we need it by averaging on 200 repeats and we can see for all the cases we checked it.",
            "In and SVM Ann.",
            "Behave in the same.",
            "Qualitatively, it behaves the same.",
            "It's also have a clearer.",
            "A clear minima.",
            "I think the reason I had one of those and I showed that in the morning.",
            "So I got the impression performs in graphs that feature selection can improve the results if you use Naive Bayes.",
            "But if you use SVM it doesn't matter.",
            "You always get the better performance with all the features, so at least.",
            "Who is this example?",
            "It is not true.",
            "A.",
            "What you mean special Dan?",
            "Well, because you know you have your model with zero model.",
            "And no, I was in general this again but it finds the threshold to be almost zero always.",
            "I need some tuning by cross validation of the parameters of the SVM.",
            "That's the best I could get playing with it.",
            "Is DSDM curve significantly below the blue one?",
            "I didn't check this significant.",
            "I don't know.",
            "I didn't check the significant thing in this.",
            "But I can say that they did it many times and my impression is that it is not significant.",
            "You know you see in different lines when you."
        ],
        [
            "So to summarize photos, the main conclusion or my POV, at least in this world.",
            "Yvette so we see that very obvious that if the model is known or we have we have infinite in number of training instances, then using all the features is optimal from the point of view of classification, appearance or other reasons to do feature selection.",
            "But this is not the situation when the model is estimated for final submission.",
            "We saw that even when all the features carry information and are independent, using also features may be suboptimal.",
            "In despite of the independency between the features, the decision to add features depends on the not only value, but the value.",
            "Values of the rest of the pictures as well.",
            "And in addition, we saw that the optimal number of features depend crucially on the sample size we get here in this simple setting, a clear tradeoff between training set size and the optimal number of features.",
            "And.",
            "And I think that they say that we should remember it when we talk about optimal number of features and always we see this graph of performance as function of number of features and.",
            "We should remember that the shape of this graph is a crucially dependent on the training success.",
            "When we look on this graph.",
            "That's all."
        ],
        [
            "Question.",
            "Which features to use?",
            "Was that always a prefix or estimated from the sample as well?",
            "I assume that the features are ordering descending order.",
            "Yeah, I assume I don't think it's a limited the result in something 'cause you know the order of the feature is arbitrate.",
            "The result here is not overrated.",
            "How to order them?",
            "I just want to.",
            "Wanted to.",
            "I guess what I'm wondering is if your procedure for deciding which features to keep in touch throughout uses the same sample that your algorithm for classification is using.",
            "Then you can make mistakes and assigning put in these features and not use those from the final example, yes, it's not totally clear whether you want to separate out first pass, decide the picosecond pass, run this other classifier or just do the whole thing, yeah?",
            "At least for me, it's not very clear how to make an algorithm in this result, so this I don't consider it is allegoric results or how to do things, just I was there I have.",
            "The motivation for this work is many arguments with people features that there is justification from the point of view of classification accuracy to feature selection, why not to use all the features?",
            "So.",
            "That's what I tried to show here, and it's only a first step.",
            "I think that we have here at play shown here afraid of the theme similar to the usual tradeoff between model complexity and sample size 1.",
            "Man.",
            "So we we want to try to fully discover the relationship more general setting.",
            "If I wanted to make this an algorithm and say OK, I've gotta do cross validation, so let me stop without interrupting segments or something.",
            "Look at the curtains that appears in each of these cases and do some statistical tests to see which coefficient seems to be likely to be swapping sign for that.",
            "Internet is another feature selection.",
            "The distribution is going to be like this, so you know conflict.",
            "Probably possible we didn't try to.",
            "Just to clarify, whatever mask as well what you meant was that the features were ordered at Peoria.",
            "You knew the first feature is the largest, the 2nd.",
            "So you when you said I took with the five features it was literally feature 12345.",
            "It wasn't the ones that had the largest norm in the largest size on the estimate.",
            "Yes, so you didn't try it using the estimate and you had to choose which features to include, no?",
            "When you took those expectations, really inside the CDN.",
            "Here.",
            "Question does it give you?"
        ],
        [
            "Anne.",
            "You're right, they not only did in his driveway lemma as a proof precisely, I didn't state the lemma precisely, but.",
            "You can.",
            "You can write this as as we used to from generalization interacts with the epsilon and Delta as they see the.",
            "But the exact amount would always depends on the specific.",
            "That's the reason.",
            "I think it's not very interesting here for the general statement, but it's very easy to see that they just look on the variance.",
            "You see the terms, but it goes to 0.",
            "When Ambrose and this.",
            "There is a place for large enough N can walk you can.",
            "You can write it for precisely with epsilons and it will depends on the next behavior.",
            "OK, come this time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quickly about the connection between feature selection and generalization accuracy.",
                    "label": 0
                },
                {
                    "sent": "And this is a joint work with the Wrangler back lock, if not both and Tony kiss me.",
                    "label": 1
                },
                {
                    "sent": "So what is feature selection?",
                    "label": 0
                },
                {
                    "sent": "I'm sure that you all know.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After the morning talk, so we go over it quickly.",
                    "label": 0
                },
                {
                    "sent": "We are walking in the supervised learning framework.",
                    "label": 0
                },
                {
                    "sent": "Therefore we assume that labeled sample training set is given, and we also assume that there is initial set of features and that each instance in the training set is represented by.",
                    "label": 0
                },
                {
                    "sent": "A vector of values for all the features in the feature set.",
                    "label": 0
                },
                {
                    "sent": "In feature selection is the task of choosing.",
                    "label": 0
                },
                {
                    "sent": "And good typically small subset of features out of the given set and good subset in our context is a subset that enables us to be good classifiers.",
                    "label": 1
                },
                {
                    "sent": "And therefore feature selection is a special form of dimensionality reduction, as was explained in details in the morning talk.",
                    "label": 0
                },
                {
                    "sent": "So there are several.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reasons to do feature selection.",
                    "label": 1
                },
                {
                    "sent": "One of them is reduced computational complexity.",
                    "label": 0
                },
                {
                    "sent": "Another one is economy.",
                    "label": 1
                },
                {
                    "sent": "As feature selection saves us the cost of measuring unselected features.",
                    "label": 0
                },
                {
                    "sent": "And another one is the many cases.",
                    "label": 0
                },
                {
                    "sent": "The features themselves for interest and say.",
                    "label": 1
                },
                {
                    "sent": "Can provide us some insight about the nature of the problem, but overall these reasons feature selection can also improve the generalization accuracy and in this talk we talk about we focus.",
                    "label": 0
                },
                {
                    "sent": "On their list last reason.",
                    "label": 0
                },
                {
                    "sent": "Then there is a.",
                    "label": 0
                },
                {
                    "sent": "A great deal of empirical evidence that feature selection can improve the classification accuracy, and it is widely believed that using too many weekly relevant features can hurt the performance.",
                    "label": 0
                },
                {
                    "sent": "However, we have not been able to find many studies that analyze this phenomenon directly.",
                    "label": 0
                },
                {
                    "sent": "And we try to do a first step in this direction.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the main question you ask is, under which conditions can feature selection improve the classification accuracy?",
                    "label": 1
                },
                {
                    "sent": "We also ask what is the optimal number of features from the classification accuracy standpoint and how that this number depends on the training set size.",
                    "label": 1
                },
                {
                    "sent": "And then.",
                    "label": 1
                },
                {
                    "sent": "We try to discuss this question by by analyzing one simple setting.",
                    "label": 0
                },
                {
                    "sent": "The setting is very simple, but it provides us with some interns.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Insights.",
                    "label": 0
                },
                {
                    "sent": "How we look on a binary classification task bar in North where N is a very large number?",
                    "label": 0
                },
                {
                    "sent": "We assume that the distribution of the points in the positive class is the N dimensional guys, one Gaussian with mean vector mu and unit covariance matrix, and that the distributions of the point in the negative classes the same but with mean equals to minus mu.",
                    "label": 0
                },
                {
                    "sent": "Then in this setting, our initial set of features is simply the coordinates in our end.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And since we assume same covariance matrix, the two Gaussians are intersecting and hyperplane in the optimal classifier is simply the sign of the dot product between mu and X.",
                    "label": 0
                },
                {
                    "sent": "And if we use only a subset of features F, then the optimal classifier, given this subset is the same.",
                    "label": 0
                },
                {
                    "sent": "But when mu and X are first projected on the subset of features.",
                    "label": 0
                },
                {
                    "sent": "And we want to ask which subset of features?",
                    "label": 0
                },
                {
                    "sent": "And indeed also minimal generalization error, so it is quite clear that if you is known.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we know the true distribution of the features.",
                    "label": 0
                },
                {
                    "sent": "Then using all the features is optimal.",
                    "label": 1
                },
                {
                    "sent": "Because any additional features will only improve the separation between the Gaussian if it equals to zero.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter if you use it or not, but it can't hurt if you know the distribution.",
                    "label": 0
                },
                {
                    "sent": "However, we are interested.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the situation of you is not known and IT system is estimated from a finite sample of size M. And as we see shortly in this situation, using all the feature might not be optimal.",
                    "label": 1
                },
                {
                    "sent": "Then given an estimator mu head from you.",
                    "label": 0
                },
                {
                    "sent": "In the subset of features.",
                    "label": 0
                },
                {
                    "sent": "We consider the induced classifier.",
                    "label": 1
                },
                {
                    "sent": "And we want to look on the generalization error of this classifier, but the generalization error is depends on the specific sample as the estimate depends on this sample and therefore we know can the average randomization or will the average is taken with respect to all possible samples of size M and now we want to find the subset of features that minimize.",
                    "label": 1
                },
                {
                    "sent": "This this error.",
                    "label": 0
                },
                {
                    "sent": "And we also assume, without loss of generality, that the coordinates of mu or order in a descending order according their absolute value, and therefore if we choose to use on the end features and using the first features is optimal and therefore the problem reduces to find only the optimal number of features, not the optimal set of features.",
                    "label": 0
                },
                {
                    "sent": "And then the following we consider the minimum variance unbiased estimators formula, which is simply the empirical mean of the training set.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before I'm going to present the results, I want to show short instruction to make the setting more clear, so we look in R2 and assume this mute.",
                    "label": 0
                },
                {
                    "sent": "So the optimal classifier is this dashed red line.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "First we look on situation with you is known.",
                    "label": 0
                },
                {
                    "sent": "Then if you is known and we use only the first features X access, then we get this classifier.",
                    "label": 0
                },
                {
                    "sent": "If we use only the.",
                    "label": 0
                },
                {
                    "sent": "Second feature we get this cluster, which is obvious, obviously wrong.",
                    "label": 0
                },
                {
                    "sent": "And if we use.",
                    "label": 0
                },
                {
                    "sent": "They both features.",
                    "label": 0
                },
                {
                    "sent": "We get the optimal classifier, so we see that as we expect, using all the mu is known using all the features optimal.",
                    "label": 0
                },
                {
                    "sent": "However, if MU is estimated, for example by this mu hat.",
                    "label": 0
                },
                {
                    "sent": "Then using all the features.",
                    "label": 0
                },
                {
                    "sent": "If this classifier.",
                    "label": 0
                },
                {
                    "sent": "But using only the first features give this classifier, which is better.",
                    "label": 0
                },
                {
                    "sent": "So here we see that using all the feature is not optimal and this happens 'cause.",
                    "label": 0
                },
                {
                    "sent": "New head fails in estimating the sign of the second feature.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This demonstration suggests that if we estimate mu from.",
                    "label": 0
                },
                {
                    "sent": "Small samples from a small sample and therefore estimator has large variance.",
                    "label": 0
                },
                {
                    "sent": "Then it might be played there not to use features with two small with a value that is too close to 0.",
                    "label": 0
                },
                {
                    "sent": "And now we try to formulate this observation and as we see it a little bit more complicated than just a threshold.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The main result says that the number of feature or features that minimizes average error.",
                    "label": 1
                },
                {
                    "sent": "For training set of size N in the end that maximize this term.",
                    "label": 0
                },
                {
                    "sent": "And before I describe the proof of this statement statement, let's see what we can learn from it about our questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "This is this is the truth values NJJ.",
                    "label": 0
                },
                {
                    "sent": "Yeah it's a true values of Community Mew Mew is is not known but fixed.",
                    "label": 0
                },
                {
                    "sent": "So what we can?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learn from this term.",
                    "label": 0
                },
                {
                    "sent": "So first we see that if we add the features that equals to zero, it adds nothing to the numerator.",
                    "label": 0
                },
                {
                    "sent": "And and sqrt 1 / M to the denominator and therefore it reduces the value of the whole term.",
                    "label": 0
                },
                {
                    "sent": "So it.",
                    "label": 0
                },
                {
                    "sent": "And this we can conclude that if the values of the coordinates of new approaches to ONJ grows, then there is a nontrivial optimal end on trivial mean, not the maximum possible end.",
                    "label": 0
                },
                {
                    "sent": "This way we get for this setting sufficient condition for the situation of feature selection can improve security.",
                    "label": 0
                },
                {
                    "sent": "Another thing that we see here is that if you.",
                    "label": 0
                },
                {
                    "sent": "Then goes to Infinity, then this whole term reduces to just the square root of the numerator and then using all the features is optimal.",
                    "label": 0
                },
                {
                    "sent": "Is any additional feature can just increase the value of the numerator as we expect?",
                    "label": 0
                },
                {
                    "sent": "And then other things that we see here, that if we had a feature then it can increase both the value of the numerator and the denominator and therefore the effect on the whole term depends on the current value of the numerator and the denominator.",
                    "label": 0
                },
                {
                    "sent": "And he doesn't want to decision on any features.",
                    "label": 0
                },
                {
                    "sent": "Depends not only on its value, but also on the values of the features that we already selected.",
                    "label": 0
                },
                {
                    "sent": "And this may be surprising as the features here are a statistical independent.",
                    "label": 0
                },
                {
                    "sent": "Get here some kind of apparent dependence between independent features.",
                    "label": 0
                },
                {
                    "sent": "Rim.",
                    "label": 0
                },
                {
                    "sent": "We can also solve this maximization problem for specific choices of mu.",
                    "label": 0
                },
                {
                    "sent": "And here we see that if we take you to the continent of mu to decrease exponentially.",
                    "label": 0
                },
                {
                    "sent": "Then we get that the optimal number of features is in the same order of magnitude.",
                    "label": 0
                },
                {
                    "sent": "Is Logan or M is the size of the training set, and if we take a quantum using Visa harmonic series?",
                    "label": 0
                },
                {
                    "sent": "We get optimal number of features is in the order of magnitude square root of M and if we take the square root of the harmonic series we get.",
                    "label": 0
                },
                {
                    "sent": "Only of magnitude as a training for size.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, Luke wants the generalization error itself.",
                    "label": 0
                },
                {
                    "sent": "These choices of new and force where we see it for three choices of training set size.",
                    "label": 0
                },
                {
                    "sent": "The X axis here is a number of selected features.",
                    "label": 1
                },
                {
                    "sent": "And then solid lines shows the generalization error of our classifier.",
                    "label": 0
                },
                {
                    "sent": "It was a calculating by.",
                    "label": 0
                },
                {
                    "sent": "Of arranging one 200 repeats of.",
                    "label": 0
                },
                {
                    "sent": "Fay we selecting the training set.",
                    "label": 0
                },
                {
                    "sent": "And then we can see the clear minima in all these nine conditions, the clear minimizer?",
                    "label": 0
                },
                {
                    "sent": "With the values that support our theoretic results.",
                    "label": 0
                },
                {
                    "sent": "Which is the negative effects to make new right?",
                    "label": 0
                },
                {
                    "sent": "You estimate you buy that unbiased.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we estimate nearby.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "This is estimated.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so just because of the symmetric and use all the symmetry is not very important, it is just make all the innovation more simple.",
                    "label": 0
                },
                {
                    "sent": "Take it wherever in the spaces to Gaussian.",
                    "label": 0
                },
                {
                    "sent": "See as long as they have the same covariance matrix or.",
                    "label": 0
                },
                {
                    "sent": "Or so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The dashed line here is is John Isation over when you is known and we can see that it decreases constantly as we.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK and then.",
                    "label": 0
                },
                {
                    "sent": "Now we will describe the proof briefly very complicated.",
                    "label": 0
                },
                {
                    "sent": "Therefore give an estimate on your head.",
                    "label": 0
                },
                {
                    "sent": "We look on the generalization error produce classifier.",
                    "label": 0
                },
                {
                    "sent": "And this is simply the definition where PP plus.",
                    "label": 0
                },
                {
                    "sent": "Denotes the probability given the positive plus minus plus and because of the same symmetrization.",
                    "label": 0
                },
                {
                    "sent": "Is this a problem?",
                    "label": 0
                },
                {
                    "sent": "The generation that was simply the probability with respect to X that the dot product of knew not X is negative.",
                    "label": 0
                },
                {
                    "sent": "Since you had both fixed by itself, is a 1 dimensional Gaussian, then this probability is simply simplify of this term will find the CDF function of the standard option.",
                    "label": 1
                },
                {
                    "sent": "May substitute the expectation and variance of X.",
                    "label": 0
                },
                {
                    "sent": "We get the this term cause the expectation of X is you.",
                    "label": 0
                },
                {
                    "sent": "In the suffix is 1.",
                    "label": 0
                },
                {
                    "sent": "Is is unique matrix which coordinates.",
                    "label": 0
                },
                {
                    "sent": "Last line is that only one of the classes picking.",
                    "label": 0
                },
                {
                    "sent": "No, it's more info.",
                    "label": 0
                },
                {
                    "sent": "It may be a little bit confusing here be cause.",
                    "label": 0
                },
                {
                    "sent": "It is quite clear that given the positive class, this is a long term but but the problem is completely symmetric, so it is the translation or given the negative classes.",
                    "label": 0
                },
                {
                    "sent": "So no matter what is the prior on the classes you take, the average of two equal terms.",
                    "label": 0
                },
                {
                    "sent": "X distribution because it's symmetric.",
                    "label": 0
                },
                {
                    "sent": "The expectational based stuff is zero, no?",
                    "label": 0
                },
                {
                    "sent": "It's probably true that if you didn't know the that you were looking at the text origin, no, I assumed.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I know I work there.",
                    "label": 0
                },
                {
                    "sent": "I know the model, I just don't know the music.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now the the now we want to find the.",
                    "label": 0
                },
                {
                    "sent": "The end the number of features that.",
                    "label": 0
                },
                {
                    "sent": "Let's minimize this this.",
                    "label": 0
                },
                {
                    "sent": "The average this term.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seen walking zebra this term.",
                    "label": 0
                },
                {
                    "sent": "And then for doing this we use this lemma that tells us that at least when M is large enough, it is OK to move the expectation inside inside inside the quotient and insides square root.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to describe in details to the proof of this lemma, but it.",
                    "label": 0
                },
                {
                    "sent": "It follows from the from the fact that to.",
                    "label": 0
                },
                {
                    "sent": "Quantities are concentrated around the mean, or in other words, the variance.",
                    "label": 0
                },
                {
                    "sent": "And goes to 0 fast when Ambrose and the fact that the meaning of this term is not zero, and using that I is 1 left its function.",
                    "label": 0
                },
                {
                    "sent": "The derivative of lies.",
                    "label": 0
                },
                {
                    "sent": "And bounded by Upper bounded by 1.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now we can look on this there.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Therefore, minimum we conclude using them as minimizing the generalization is every generalization or is equivalent to maximizing this term.",
                    "label": 0
                },
                {
                    "sent": "Because if we.",
                    "label": 0
                },
                {
                    "sent": "If we want to minimize this term, we have to.",
                    "label": 1
                },
                {
                    "sent": "To maximize.",
                    "label": 0
                },
                {
                    "sent": "The argument of fee because it's in - so.",
                    "label": 0
                },
                {
                    "sent": "And finally substituting the.",
                    "label": 0
                },
                {
                    "sent": "Expectation and the second moment of our estimator.",
                    "label": 0
                },
                {
                    "sent": "We obtained the statement.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Here is another demonstration of the lemma instead of precise proof, we just demonstrated empirically and what we see here again for the same three choices of mule and same three choices of their training set size.",
                    "label": 0
                },
                {
                    "sent": "Successes against a number of selected features in blue we see the true generalization error.",
                    "label": 0
                },
                {
                    "sent": "And then the dashed red line we see the value of the.",
                    "label": 0
                },
                {
                    "sent": "Of the lemmas approximation and we can see that.",
                    "label": 1
                },
                {
                    "sent": "The two terms of, at least for these examples, the terms are very close even for small values of M.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the.",
                    "label": 0
                },
                {
                    "sent": "One more thing I want to show.",
                    "label": 0
                },
                {
                    "sent": "In order to verify that our result is not specific to the classifier, we chose to use, which is first estimate mu.",
                    "label": 0
                },
                {
                    "sent": "As best as we can and then use the optimal classifier.",
                    "label": 0
                },
                {
                    "sent": "Given this estimation, we also check what is empirically checked, what is a error of linear SVM?",
                    "label": 0
                },
                {
                    "sent": "On a function of the number of selected features.",
                    "label": 1
                },
                {
                    "sent": "And again we need it by averaging on 200 repeats and we can see for all the cases we checked it.",
                    "label": 1
                },
                {
                    "sent": "In and SVM Ann.",
                    "label": 0
                },
                {
                    "sent": "Behave in the same.",
                    "label": 0
                },
                {
                    "sent": "Qualitatively, it behaves the same.",
                    "label": 0
                },
                {
                    "sent": "It's also have a clearer.",
                    "label": 0
                },
                {
                    "sent": "A clear minima.",
                    "label": 0
                },
                {
                    "sent": "I think the reason I had one of those and I showed that in the morning.",
                    "label": 0
                },
                {
                    "sent": "So I got the impression performs in graphs that feature selection can improve the results if you use Naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "But if you use SVM it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "You always get the better performance with all the features, so at least.",
                    "label": 0
                },
                {
                    "sent": "Who is this example?",
                    "label": 0
                },
                {
                    "sent": "It is not true.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "What you mean special Dan?",
                    "label": 0
                },
                {
                    "sent": "Well, because you know you have your model with zero model.",
                    "label": 0
                },
                {
                    "sent": "And no, I was in general this again but it finds the threshold to be almost zero always.",
                    "label": 0
                },
                {
                    "sent": "I need some tuning by cross validation of the parameters of the SVM.",
                    "label": 0
                },
                {
                    "sent": "That's the best I could get playing with it.",
                    "label": 0
                },
                {
                    "sent": "Is DSDM curve significantly below the blue one?",
                    "label": 0
                },
                {
                    "sent": "I didn't check this significant.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I didn't check the significant thing in this.",
                    "label": 0
                },
                {
                    "sent": "But I can say that they did it many times and my impression is that it is not significant.",
                    "label": 0
                },
                {
                    "sent": "You know you see in different lines when you.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize photos, the main conclusion or my POV, at least in this world.",
                    "label": 0
                },
                {
                    "sent": "Yvette so we see that very obvious that if the model is known or we have we have infinite in number of training instances, then using all the features is optimal from the point of view of classification, appearance or other reasons to do feature selection.",
                    "label": 0
                },
                {
                    "sent": "But this is not the situation when the model is estimated for final submission.",
                    "label": 0
                },
                {
                    "sent": "We saw that even when all the features carry information and are independent, using also features may be suboptimal.",
                    "label": 1
                },
                {
                    "sent": "In despite of the independency between the features, the decision to add features depends on the not only value, but the value.",
                    "label": 0
                },
                {
                    "sent": "Values of the rest of the pictures as well.",
                    "label": 0
                },
                {
                    "sent": "And in addition, we saw that the optimal number of features depend crucially on the sample size we get here in this simple setting, a clear tradeoff between training set size and the optimal number of features.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And I think that they say that we should remember it when we talk about optimal number of features and always we see this graph of performance as function of number of features and.",
                    "label": 0
                },
                {
                    "sent": "We should remember that the shape of this graph is a crucially dependent on the training success.",
                    "label": 0
                },
                {
                    "sent": "When we look on this graph.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Which features to use?",
                    "label": 0
                },
                {
                    "sent": "Was that always a prefix or estimated from the sample as well?",
                    "label": 0
                },
                {
                    "sent": "I assume that the features are ordering descending order.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I assume I don't think it's a limited the result in something 'cause you know the order of the feature is arbitrate.",
                    "label": 0
                },
                {
                    "sent": "The result here is not overrated.",
                    "label": 0
                },
                {
                    "sent": "How to order them?",
                    "label": 0
                },
                {
                    "sent": "I just want to.",
                    "label": 0
                },
                {
                    "sent": "Wanted to.",
                    "label": 0
                },
                {
                    "sent": "I guess what I'm wondering is if your procedure for deciding which features to keep in touch throughout uses the same sample that your algorithm for classification is using.",
                    "label": 0
                },
                {
                    "sent": "Then you can make mistakes and assigning put in these features and not use those from the final example, yes, it's not totally clear whether you want to separate out first pass, decide the picosecond pass, run this other classifier or just do the whole thing, yeah?",
                    "label": 0
                },
                {
                    "sent": "At least for me, it's not very clear how to make an algorithm in this result, so this I don't consider it is allegoric results or how to do things, just I was there I have.",
                    "label": 0
                },
                {
                    "sent": "The motivation for this work is many arguments with people features that there is justification from the point of view of classification accuracy to feature selection, why not to use all the features?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's what I tried to show here, and it's only a first step.",
                    "label": 0
                },
                {
                    "sent": "I think that we have here at play shown here afraid of the theme similar to the usual tradeoff between model complexity and sample size 1.",
                    "label": 0
                },
                {
                    "sent": "Man.",
                    "label": 0
                },
                {
                    "sent": "So we we want to try to fully discover the relationship more general setting.",
                    "label": 0
                },
                {
                    "sent": "If I wanted to make this an algorithm and say OK, I've gotta do cross validation, so let me stop without interrupting segments or something.",
                    "label": 0
                },
                {
                    "sent": "Look at the curtains that appears in each of these cases and do some statistical tests to see which coefficient seems to be likely to be swapping sign for that.",
                    "label": 0
                },
                {
                    "sent": "Internet is another feature selection.",
                    "label": 0
                },
                {
                    "sent": "The distribution is going to be like this, so you know conflict.",
                    "label": 0
                },
                {
                    "sent": "Probably possible we didn't try to.",
                    "label": 0
                },
                {
                    "sent": "Just to clarify, whatever mask as well what you meant was that the features were ordered at Peoria.",
                    "label": 0
                },
                {
                    "sent": "You knew the first feature is the largest, the 2nd.",
                    "label": 1
                },
                {
                    "sent": "So you when you said I took with the five features it was literally feature 12345.",
                    "label": 0
                },
                {
                    "sent": "It wasn't the ones that had the largest norm in the largest size on the estimate.",
                    "label": 0
                },
                {
                    "sent": "Yes, so you didn't try it using the estimate and you had to choose which features to include, no?",
                    "label": 0
                },
                {
                    "sent": "When you took those expectations, really inside the CDN.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Question does it give you?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "You're right, they not only did in his driveway lemma as a proof precisely, I didn't state the lemma precisely, but.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "You can write this as as we used to from generalization interacts with the epsilon and Delta as they see the.",
                    "label": 0
                },
                {
                    "sent": "But the exact amount would always depends on the specific.",
                    "label": 0
                },
                {
                    "sent": "That's the reason.",
                    "label": 0
                },
                {
                    "sent": "I think it's not very interesting here for the general statement, but it's very easy to see that they just look on the variance.",
                    "label": 0
                },
                {
                    "sent": "You see the terms, but it goes to 0.",
                    "label": 0
                },
                {
                    "sent": "When Ambrose and this.",
                    "label": 0
                },
                {
                    "sent": "There is a place for large enough N can walk you can.",
                    "label": 1
                },
                {
                    "sent": "You can write it for precisely with epsilons and it will depends on the next behavior.",
                    "label": 0
                },
                {
                    "sent": "OK, come this time.",
                    "label": 0
                }
            ]
        }
    }
}