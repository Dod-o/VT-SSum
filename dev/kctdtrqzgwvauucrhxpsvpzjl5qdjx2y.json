{
    "id": "kctdtrqzgwvauucrhxpsvpzjl5qdjx2y",
    "title": "Mixture Models for Learning Low-dimensional Roles in High-dimensional Data",
    "info": {
        "author": [
            "Manas Somaiya, Department of Computer and Information Science and Engineering, University of Florida"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/kdd2010_somaiya_mml/",
    "segmentation": [
        [
            "Good afternoon everyone.",
            "My name is Manus.",
            "I'm from University of Florida.",
            "This is joint work with Chris Jermaine and Sanjay Ranka.",
            "Chris has now moved to Rice University.",
            "So the topic of our talk is mixture models for learning low dimensional roles in high dimensional data.",
            "So at a high level, we are interested in only learning low dimensional patterns in very high dimensional data using mixture models."
        ],
        [
            "The talk this is outline for the talk.",
            "I will very quickly introduce mixture models.",
            "Give a motivating example talk a little bit about our model, how we can learn our model an quickly show one of the datasets that we have worked on.",
            "An more details can be found with our poster and our paper."
        ],
        [
            "In statistics and probability, a mixture model is nothing but a convex combination of many probability distributions.",
            "Here is an example where you have a random variable X which is a mixture of N component random variables Y one through YN and the mixing proportions basically tell you how this probability distributions are mixing with each other and the constraint being that sum over mixing proportions is 1.",
            "Very classical example would be high distribution of a male and female students in a class.",
            "Is actually a mixture which will define the height for all the students in the class.",
            "Mix."
        ],
        [
            "Models are nothing new.",
            "I think the first one was by Pearson as early as 1894.",
            "Typically, one of the most famous mixture model is the Gaussian mixture model, where the data is seen as being produced by K Gaussians and typically you have K components as shown here and you assume that the data point was generated by a two step process.",
            "First you pick one of the components from the K mixture components and then you actually generate the data point defined by the probability distribution function for that component.",
            "The chief advantages is that you can very accurately model very complex data distributions using very simple components."
        ],
        [
            "As soon as we have high dimensional data, there are certain shortcomings of this kind of a mixture model.",
            "Since a data point is being generated by just a single component, that component needs to provide all the information for all the attributes in the data space.",
            "This will typically conflict with underlying reality of many high dimensional datasets cause multiple generative components may actually be influencing the data point.",
            "A single component may have influence only over subset of some of the data.",
            "And since you have influence or only a subset of the data space, you may have varying influence over different data attributes and we will come up with a good example on the next slide."
        ],
        [
            "So let's take this real life scenario.",
            "Let's assume you have a Department store.",
            "You have many items and you have many customers and your goal is to build some kind of an informative model which describes the buying patterns of different classes of customers under the classical mixture model, each customer would only belong to one class, and in that case that class will have to describe all the buying patterns of all the members over all the different kinds of items that are sold in the store.",
            "This is very unrealistic considering the diversity of the customers, and if you have a large departmental store, the diversity of the items that are available for sale."
        ],
        [
            "It is probably more natural and more accurate to explain the behavior of each customer as coming from influence of several different classes.",
            "Here is a tangible example.",
            "Let's say you are trying to buy a DVD or a Blu Ray of a movie.",
            "For example, the customer could be.",
            "An action movie fan art, horror movie fan and a parent.",
            "And let's say one of the items that is available for sale is the animated movie Teenage Mutant Ninja Turtles.",
            "So it seems intuitive that being a member of the parent class will have a stronger influence on the purchase of this item as compared to the other two classes.",
            "And after the parent class is probably removed from the discussion, being a member of an action movie class would probably have a higher influence on the purchase of this item than being a horror movie fan.",
            "So the goal is that you want to have very high precision explanation for each data point.",
            "However, you want to learn very general classes such as parent that are very important, but they do that.",
            "Do not describe any particular data point or any particular customer completely, so you want to model the customer as overlap of many such classes, because in real life the customer is nothing but an overlap of such classes like parent, wife, small business, user, Doctor, Lawyer.",
            "You know sports fan, so and so forth."
        ],
        [
            "So enter our model.",
            "We call it the power model.",
            "Probabilistic waited an symbol of roles model, and there is another reason for picking this name, which will become clear on the next slide.",
            "So under under our proposed model we have K components associated with each component.",
            "You have a Bernoulli appearance probability which tells you how likely this component is to be present in any of the data points and if we have the number of data attributes, then you have a D dimensional parameter vector.",
            "Where this will actually be used by the probability distribution to actually generate the data attribute.",
            "One of the key features of our model is that the model just describes how all these components interact to generate the data, and we are data type agnostic, which means you can pretty much plug in any type of probability distribution that you like, which will match very well with your underlying data set.",
            "For example, if you are trying to model data as normal random variable, then this data is there would be nothing but the mean and the standard deviation.",
            "For this probability distribution.",
            "We also introduce a vector called the weight vector, which basically the component can use to specify how it wants to influence different data attributes.",
            "So WIJ would basically indicate the strength of component CI or attribute AJ and the next is the condition that we introduced to make sure that a single component does not hog all the data attributes.",
            "So basically each component has a certain amount of weight that it can use up.",
            "And you know it can pick and choose which attributes it wants to favor in which it does not want to favor.",
            "So given this setup, this is how we set up the data generation under our MoD."
        ],
        [
            "Each data point is generated by this three step process.",
            "In the first step, one or more of the key components are marked as active by performing a Bernoulli trial using their appearance probabilities.",
            "So this step actually also means that each data point could be generated actually by sampling from the powerset of the K components, and that's why one more reason to pick the name power model.",
            "In the second step, for each attribute we select the dominant component by performing a weighted multinomial trial using the weights that I described on the previous slide.",
            "Among all the active components selected for this data point.",
            "And finally, in the third step, each data attribute is actually generated using the parameterized density function by using the parameters specified by the dominant component for that data attribute."
        ],
        [
            "Clearly there there is an issue here because if you are going to do Burnley trial over K variables.",
            "Then there is a chance that you may not select any component.",
            "So solution for this is that we make one of the components of special default component that is always selected.",
            "So basically we make its appearance probability to be one.",
            "So this component is basically going to act as your default component or a catchall component that describes the background distribution.",
            "Going back to our retail store example, this would be probably a customer that you cannot classify or associate with any of the classes and it just describes your general customer.",
            "Um?",
            "We do not want the default component to actually generate any data attribute.",
            "It should only generate a data attribute when none of the other components can an we do this by setting the default parameters weights to be some very small user defined constant epsilon and the user can tune this parameter to make the default stronger or weaker.",
            "We set up.",
            "The model that I described in a hierarchical Bayesian fashion.",
            "Unfortunately, I cannot describe the entire model right now, but we will be at poster #17 today evening and please feel free to stop by."
        ],
        [
            "So so the we set up the model innovation fashion and we use Gibbs sampling algorithm to actually do the inference."
        ],
        [
            "Here, here is a slide which describes all the prime."
        ],
        [
            "Here's an derivation of conditionals."
        ],
        [
            "So the challenges that we faced were assigning proper prior distributions and deriving the analytical expressions because we did not have nice conjugacy.",
            "Update to the parimeter weights was very slow becausw we had to actually use an adaptive rejection sampling scheme.",
            "We came up with a nice beta PDF approximation.",
            "Again, all the details are in the paper.",
            "Similarly we had to design a nice way to post process.",
            "All our results be 'cause you have all this data para meters.",
            "All this W parameters and it's difficult to visualize what is important and what is not.",
            "And we came up with an innovative scheme that uses scale diversions.",
            "Quickly go over one of the results.",
            "One of the day."
        ],
        [
            "Assets that we considered NIPS papers datasets from UC Irvine machine Learning Repository 1500.",
            "Papers more than 12,000 unique words total.",
            "More than 6 million words.",
            "We considered the nontrivial stemmed most frequently used top 1000 words and converted them to an absence presence kind of data, not bag of words, but absence presence back off or is also possible.",
            "So essentially the input matrix is 1500 by 1000, zero one matrix, and we learned at 21 component model with Bernoulli generators and.",
            "Very small epsilon weight is basically 1 / 1000 with 2000 Gibbs interactions and the results are over the last 1000.",
            "All the details are in this technical report."
        ],
        [
            "And here is a high level summary of the results.",
            "So these are some of the components and Alpha is the appearance probability of each component.",
            "So the first component basically talks about words related to proofs in theory.",
            "Second one is related to sound processing.",
            "The fifth one is related to.",
            "Or brain.",
            "6 one is related to neural networks.",
            "7th one is related to robotics and motion."
        ],
        [
            "920 is related to classification, 13th is related to computer vision and image processing.",
            "14th is related to statistical methods.",
            "Specific calibration methods, which is what this talk is about.",
            "17th is related to electronics and 19th one is very interesting and the bold font here actually indicates that this component suppresses this words and basically so this component is basically telling that when this component is active you will not see these words which are normally related to academia because people are going to write the grants that supported their work and stuff like that.",
            "So basically this model gave you a very nice tour of you know different sub areas of what you would expect in NIPS paper."
        ],
        [
            "Related work is some of our prior work, which was indicated in 2008 and other mixture models.",
            "Indian buffet process, parsimonious mixtures and somewhat related to LDA topic models.",
            "That's it.",
            "Thanks.",
            "Do you have any questions?",
            "Please feel free to ask."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is Manus.",
                    "label": 0
                },
                {
                    "sent": "I'm from University of Florida.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with Chris Jermaine and Sanjay Ranka.",
                    "label": 0
                },
                {
                    "sent": "Chris has now moved to Rice University.",
                    "label": 0
                },
                {
                    "sent": "So the topic of our talk is mixture models for learning low dimensional roles in high dimensional data.",
                    "label": 1
                },
                {
                    "sent": "So at a high level, we are interested in only learning low dimensional patterns in very high dimensional data using mixture models.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The talk this is outline for the talk.",
                    "label": 0
                },
                {
                    "sent": "I will very quickly introduce mixture models.",
                    "label": 1
                },
                {
                    "sent": "Give a motivating example talk a little bit about our model, how we can learn our model an quickly show one of the datasets that we have worked on.",
                    "label": 0
                },
                {
                    "sent": "An more details can be found with our poster and our paper.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In statistics and probability, a mixture model is nothing but a convex combination of many probability distributions.",
                    "label": 1
                },
                {
                    "sent": "Here is an example where you have a random variable X which is a mixture of N component random variables Y one through YN and the mixing proportions basically tell you how this probability distributions are mixing with each other and the constraint being that sum over mixing proportions is 1.",
                    "label": 1
                },
                {
                    "sent": "Very classical example would be high distribution of a male and female students in a class.",
                    "label": 0
                },
                {
                    "sent": "Is actually a mixture which will define the height for all the students in the class.",
                    "label": 0
                },
                {
                    "sent": "Mix.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Models are nothing new.",
                    "label": 0
                },
                {
                    "sent": "I think the first one was by Pearson as early as 1894.",
                    "label": 0
                },
                {
                    "sent": "Typically, one of the most famous mixture model is the Gaussian mixture model, where the data is seen as being produced by K Gaussians and typically you have K components as shown here and you assume that the data point was generated by a two step process.",
                    "label": 1
                },
                {
                    "sent": "First you pick one of the components from the K mixture components and then you actually generate the data point defined by the probability distribution function for that component.",
                    "label": 0
                },
                {
                    "sent": "The chief advantages is that you can very accurately model very complex data distributions using very simple components.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As soon as we have high dimensional data, there are certain shortcomings of this kind of a mixture model.",
                    "label": 0
                },
                {
                    "sent": "Since a data point is being generated by just a single component, that component needs to provide all the information for all the attributes in the data space.",
                    "label": 0
                },
                {
                    "sent": "This will typically conflict with underlying reality of many high dimensional datasets cause multiple generative components may actually be influencing the data point.",
                    "label": 1
                },
                {
                    "sent": "A single component may have influence only over subset of some of the data.",
                    "label": 0
                },
                {
                    "sent": "And since you have influence or only a subset of the data space, you may have varying influence over different data attributes and we will come up with a good example on the next slide.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's take this real life scenario.",
                    "label": 0
                },
                {
                    "sent": "Let's assume you have a Department store.",
                    "label": 0
                },
                {
                    "sent": "You have many items and you have many customers and your goal is to build some kind of an informative model which describes the buying patterns of different classes of customers under the classical mixture model, each customer would only belong to one class, and in that case that class will have to describe all the buying patterns of all the members over all the different kinds of items that are sold in the store.",
                    "label": 1
                },
                {
                    "sent": "This is very unrealistic considering the diversity of the customers, and if you have a large departmental store, the diversity of the items that are available for sale.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is probably more natural and more accurate to explain the behavior of each customer as coming from influence of several different classes.",
                    "label": 1
                },
                {
                    "sent": "Here is a tangible example.",
                    "label": 0
                },
                {
                    "sent": "Let's say you are trying to buy a DVD or a Blu Ray of a movie.",
                    "label": 0
                },
                {
                    "sent": "For example, the customer could be.",
                    "label": 0
                },
                {
                    "sent": "An action movie fan art, horror movie fan and a parent.",
                    "label": 1
                },
                {
                    "sent": "And let's say one of the items that is available for sale is the animated movie Teenage Mutant Ninja Turtles.",
                    "label": 1
                },
                {
                    "sent": "So it seems intuitive that being a member of the parent class will have a stronger influence on the purchase of this item as compared to the other two classes.",
                    "label": 1
                },
                {
                    "sent": "And after the parent class is probably removed from the discussion, being a member of an action movie class would probably have a higher influence on the purchase of this item than being a horror movie fan.",
                    "label": 0
                },
                {
                    "sent": "So the goal is that you want to have very high precision explanation for each data point.",
                    "label": 0
                },
                {
                    "sent": "However, you want to learn very general classes such as parent that are very important, but they do that.",
                    "label": 0
                },
                {
                    "sent": "Do not describe any particular data point or any particular customer completely, so you want to model the customer as overlap of many such classes, because in real life the customer is nothing but an overlap of such classes like parent, wife, small business, user, Doctor, Lawyer.",
                    "label": 0
                },
                {
                    "sent": "You know sports fan, so and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So enter our model.",
                    "label": 0
                },
                {
                    "sent": "We call it the power model.",
                    "label": 1
                },
                {
                    "sent": "Probabilistic waited an symbol of roles model, and there is another reason for picking this name, which will become clear on the next slide.",
                    "label": 0
                },
                {
                    "sent": "So under under our proposed model we have K components associated with each component.",
                    "label": 1
                },
                {
                    "sent": "You have a Bernoulli appearance probability which tells you how likely this component is to be present in any of the data points and if we have the number of data attributes, then you have a D dimensional parameter vector.",
                    "label": 0
                },
                {
                    "sent": "Where this will actually be used by the probability distribution to actually generate the data attribute.",
                    "label": 0
                },
                {
                    "sent": "One of the key features of our model is that the model just describes how all these components interact to generate the data, and we are data type agnostic, which means you can pretty much plug in any type of probability distribution that you like, which will match very well with your underlying data set.",
                    "label": 0
                },
                {
                    "sent": "For example, if you are trying to model data as normal random variable, then this data is there would be nothing but the mean and the standard deviation.",
                    "label": 1
                },
                {
                    "sent": "For this probability distribution.",
                    "label": 0
                },
                {
                    "sent": "We also introduce a vector called the weight vector, which basically the component can use to specify how it wants to influence different data attributes.",
                    "label": 0
                },
                {
                    "sent": "So WIJ would basically indicate the strength of component CI or attribute AJ and the next is the condition that we introduced to make sure that a single component does not hog all the data attributes.",
                    "label": 1
                },
                {
                    "sent": "So basically each component has a certain amount of weight that it can use up.",
                    "label": 0
                },
                {
                    "sent": "And you know it can pick and choose which attributes it wants to favor in which it does not want to favor.",
                    "label": 0
                },
                {
                    "sent": "So given this setup, this is how we set up the data generation under our MoD.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each data point is generated by this three step process.",
                    "label": 1
                },
                {
                    "sent": "In the first step, one or more of the key components are marked as active by performing a Bernoulli trial using their appearance probabilities.",
                    "label": 1
                },
                {
                    "sent": "So this step actually also means that each data point could be generated actually by sampling from the powerset of the K components, and that's why one more reason to pick the name power model.",
                    "label": 0
                },
                {
                    "sent": "In the second step, for each attribute we select the dominant component by performing a weighted multinomial trial using the weights that I described on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "Among all the active components selected for this data point.",
                    "label": 1
                },
                {
                    "sent": "And finally, in the third step, each data attribute is actually generated using the parameterized density function by using the parameters specified by the dominant component for that data attribute.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clearly there there is an issue here because if you are going to do Burnley trial over K variables.",
                    "label": 0
                },
                {
                    "sent": "Then there is a chance that you may not select any component.",
                    "label": 0
                },
                {
                    "sent": "So solution for this is that we make one of the components of special default component that is always selected.",
                    "label": 1
                },
                {
                    "sent": "So basically we make its appearance probability to be one.",
                    "label": 0
                },
                {
                    "sent": "So this component is basically going to act as your default component or a catchall component that describes the background distribution.",
                    "label": 0
                },
                {
                    "sent": "Going back to our retail store example, this would be probably a customer that you cannot classify or associate with any of the classes and it just describes your general customer.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "We do not want the default component to actually generate any data attribute.",
                    "label": 0
                },
                {
                    "sent": "It should only generate a data attribute when none of the other components can an we do this by setting the default parameters weights to be some very small user defined constant epsilon and the user can tune this parameter to make the default stronger or weaker.",
                    "label": 0
                },
                {
                    "sent": "We set up.",
                    "label": 0
                },
                {
                    "sent": "The model that I described in a hierarchical Bayesian fashion.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, I cannot describe the entire model right now, but we will be at poster #17 today evening and please feel free to stop by.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so the we set up the model innovation fashion and we use Gibbs sampling algorithm to actually do the inference.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, here is a slide which describes all the prime.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an derivation of conditionals.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the challenges that we faced were assigning proper prior distributions and deriving the analytical expressions because we did not have nice conjugacy.",
                    "label": 1
                },
                {
                    "sent": "Update to the parimeter weights was very slow becausw we had to actually use an adaptive rejection sampling scheme.",
                    "label": 1
                },
                {
                    "sent": "We came up with a nice beta PDF approximation.",
                    "label": 0
                },
                {
                    "sent": "Again, all the details are in the paper.",
                    "label": 1
                },
                {
                    "sent": "Similarly we had to design a nice way to post process.",
                    "label": 1
                },
                {
                    "sent": "All our results be 'cause you have all this data para meters.",
                    "label": 0
                },
                {
                    "sent": "All this W parameters and it's difficult to visualize what is important and what is not.",
                    "label": 0
                },
                {
                    "sent": "And we came up with an innovative scheme that uses scale diversions.",
                    "label": 0
                },
                {
                    "sent": "Quickly go over one of the results.",
                    "label": 0
                },
                {
                    "sent": "One of the day.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Assets that we considered NIPS papers datasets from UC Irvine machine Learning Repository 1500.",
                    "label": 0
                },
                {
                    "sent": "Papers more than 12,000 unique words total.",
                    "label": 0
                },
                {
                    "sent": "More than 6 million words.",
                    "label": 0
                },
                {
                    "sent": "We considered the nontrivial stemmed most frequently used top 1000 words and converted them to an absence presence kind of data, not bag of words, but absence presence back off or is also possible.",
                    "label": 0
                },
                {
                    "sent": "So essentially the input matrix is 1500 by 1000, zero one matrix, and we learned at 21 component model with Bernoulli generators and.",
                    "label": 1
                },
                {
                    "sent": "Very small epsilon weight is basically 1 / 1000 with 2000 Gibbs interactions and the results are over the last 1000.",
                    "label": 1
                },
                {
                    "sent": "All the details are in this technical report.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is a high level summary of the results.",
                    "label": 1
                },
                {
                    "sent": "So these are some of the components and Alpha is the appearance probability of each component.",
                    "label": 1
                },
                {
                    "sent": "So the first component basically talks about words related to proofs in theory.",
                    "label": 0
                },
                {
                    "sent": "Second one is related to sound processing.",
                    "label": 0
                },
                {
                    "sent": "The fifth one is related to.",
                    "label": 0
                },
                {
                    "sent": "Or brain.",
                    "label": 0
                },
                {
                    "sent": "6 one is related to neural networks.",
                    "label": 0
                },
                {
                    "sent": "7th one is related to robotics and motion.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "920 is related to classification, 13th is related to computer vision and image processing.",
                    "label": 0
                },
                {
                    "sent": "14th is related to statistical methods.",
                    "label": 0
                },
                {
                    "sent": "Specific calibration methods, which is what this talk is about.",
                    "label": 0
                },
                {
                    "sent": "17th is related to electronics and 19th one is very interesting and the bold font here actually indicates that this component suppresses this words and basically so this component is basically telling that when this component is active you will not see these words which are normally related to academia because people are going to write the grants that supported their work and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So basically this model gave you a very nice tour of you know different sub areas of what you would expect in NIPS paper.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Related work is some of our prior work, which was indicated in 2008 and other mixture models.",
                    "label": 0
                },
                {
                    "sent": "Indian buffet process, parsimonious mixtures and somewhat related to LDA topic models.",
                    "label": 1
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Do you have any questions?",
                    "label": 0
                },
                {
                    "sent": "Please feel free to ask.",
                    "label": 0
                }
            ]
        }
    }
}