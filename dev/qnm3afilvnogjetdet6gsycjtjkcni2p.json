{
    "id": "qnm3afilvnogjetdet6gsycjtjkcni2p",
    "title": "Detecting Anomalous Records in Categorical Datasets",
    "info": {
        "author": [
            "Kaustav Das, Carnegie Mellon University"
        ],
        "published": "Sept. 14, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Data Mining->Anomaly Detection"
        ]
    },
    "url": "http://videolectures.net/kdd07_das_dar/",
    "segmentation": [
        [
            "So I'm going to talk about detecting anomaly detection in categorical datasets, and this is joint work with my advisor Jeff Snyder at Carnegie Mellon."
        ],
        [
            "This is a brief outline of the talk, so I'll start with the problem motivation follow with a brief description of the related work and followed by two animal detection methods that I described here.",
            "The conditional animali and the marginal animal methods, and followed by some datasets and results."
        ],
        [
            "Comparisons of these methods.",
            "So to give a motivation of animal detection in real life in a lot of domains, huge amount of data is collected regularly and there is no need to detect unusual observations which we call animales.",
            "So just for a small sample of examples in the list of import containers being imported into the country we are interested in detecting animals records which can detect illegal activities such as smuggling.",
            "Also, in computer networks animals can detect network intrusions in astronomical data.",
            "Animals can lead us to new astronomical discoveries and then in emergency Department data or healthcare data, animals can detect disease outbreaks or epidemics.",
            "So this is just a small sample of domains where we have the data and we actually have applied or we plan to apply our methods."
        ],
        [
            "So to give a more formal overview of the problem, we have two sets of data.",
            "The first set is the training data and we focus solely on categorical data.",
            "The reason being that the motivating data set, which is the import of containers data set, was mainly categorical in nature, and we categorize any real valued attributes to some discrete levels.",
            "The training data can have a large number of records in the range of 100,000 to 1,000,000 records.",
            "And one important aspect of the problem is that the training data is unlabeled.",
            "It means that this is unsupervised problem, but we can handle a small fraction of the training data.",
            "Being animal is.",
            "So these animals records should not be very high.",
            "It should be in the range of 1 to 2%.",
            "But and it's unlabeled.",
            "One important factor in this data set is that attributes can have a lot of values in the range of 5000 to 10,000 possible values, and that raises some important issues which will address in the talk.",
            "The test data has the same properties, but it can have any fraction of records animals and our goal here is that we want to detect records in the test set which are actually animals given the training set, and in general we would like to score records in the test set with a degree or the score of animal asnos and then flag records based on a desired false positive."
        ],
        [
            "Great.",
            "So just to give an example of the motivating data set, this is the PS data which has 10 attributes.",
            "Most of them take categorical values and there are three attributes which take real values, but we categorize them to some five discrete levels.",
            "I'll be using this example data set to demonstrate."
        ],
        [
            "In the rest of my talk, briefly give overview of related work in this field.",
            "There has been a lot of work of animal detection using real valued attributes or in a supervised setting, but for categorical valued attributes.",
            "In unsupervised setting, one of the popular methods is to use a likelihood based method, where essentially first we learn a probability distribution from the training data, and then we for each test record we calculate the record likelihood and if the record likelihood is low, then we say that the record is.",
            "Analysts and people have used different algorithms such as based dependency trees and Bayes network for generating the MoD."
        ],
        [
            "Another another method that has been used if you were here for the first talk here is the using Association rule learners to detect anamolies.",
            "Andali Red is a system that has been developed and it learns rules of the Form X implies Y where X&Y are possible attribute values and it learns rules confident rules of this form an if any record X is present.",
            "But why is not present then the animal score depends on the probability of not seeing Y given X.",
            "One problem with this method is that this learns rules with high degree of positive correlation.",
            "But as we see now a data set we have a lot of possible values and such positive correlations are rare.",
            "Rather negative correlations are much more common, so our method will tend to focus on high negative correlations and one big problem with the likelihood based methods is that it learns model over all the possible attributes.",
            "So if in a small subset of attributes there is some signal of animale.",
            "Then it can get washed out by the other noise in the other."
        ],
        [
            "Attributes.",
            "So now I'll describe."
        ],
        [
            "Conditional anomaly which we define in this work.",
            "So I'll start with giving an example of a problem using the base network likelihood method.",
            "So for example we learn Bayes network on our training data and say the conditional probability commodity given country.",
            "This is a factor in the Bayes network and we see a test record importing gold from China and we see that the conditional probability is .01 which is very small.",
            "So the question is, is this small value?",
            "Can we treat this as animal because?",
            "This small value is going to make the record likelihood in the Bayes network to be very small.",
            "So if we dig further, we see that the marginal probability of gold is also very small .01, which means that this is a rare commodity.",
            "So the fact that gold is rare explains the fact that importing gold from China is also rare, so we should not be surprised by seeing this."
        ],
        [
            "Record so in order to, in order to compensate for this, we define this quantity R value, which is our value of the community of record T and country of record T, which is a ratio of the conditional with a marginal.",
            "It turns out to be one which is no longer a small value, so this R value is the main concept behind condition."
        ],
        [
            "Animal is and if you see another example, suppose we see a record which imports copper from China.",
            "The copper is not a rare commodity, but the probability of copper from China is still low.",
            "So in this case the R value is .01, which is a small quantity and we are surprised at seeing such a small quantity of the R value so."
        ],
        [
            "To give a more formal definition of our value, it is defined over 2 attribute values at and BT and our value is defined to be the ratio of this conditional to the marginal, which can be written in a symmetric form, and it can be written in this form where the numerator is the joint probability of occurrence of the two values and the denominator is the probability of occurrence of the two values if they were independent.",
            "So having a smaller value means that there is a high negative correlation between these two occurrence of these two values.",
            "So if they occur together and our values.",
            "Much, much less than one.",
            "Then we can say that TT is animal is."
        ],
        [
            "And in general we extend this definition to a set of more than one attributes.",
            "So a can be reset community and we can be the set US foreign country and we can define our value in this form.",
            "There are exponential number of subsets possible, so we restrict the number of attributes in each set to a constant K. So we consider up to two K attributes wilcon."
        ],
        [
            "Bring such our value.",
            "So now I give the algorithm for testing a particular record T. What we do is we take each mutually exclusive pair of attribute subsets A&B, and then we compute the corresponding R value.",
            "There is exponential number of such possible R values, but so we do a very efficient calculation of these probability values.",
            "M is the total number of attributes and K is the number of attributes in each subset.",
            "So now after we calculate all the R values for a given record, we need to assign a score to the record.",
            "One heuristic is to assign the minimum our value as the score.",
            "Then we ignore the information from all the other attributes which are not considered in the R value.",
            "So another heuristic is to combine evidence from different different subsets and we do this by taking a product of selected our values.",
            "I'll not be describing this in detail."
        ],
        [
            "Given in the paper.",
            "Now we need to estimate this probability values in efficient manner.",
            "So first of all, in order to estimate them we can use a maximum likelihood estimation which gives a ratio of the counts which match the particular values in the training set and the total number of training cases.",
            "But one problem with usually using the maximum likelihood estimation is if C of 80 zero.",
            "That is, it doesn't have any any matching instances in the training data set.",
            "Then it can be divided by zero, or if this is 0R value can become zero.",
            "So if we want to avoid that and so we use a lot less moving and we use.",
            "This turns out to be estimate of the probability and the R value turns out to be in this form.",
            "And these are the counts where.",
            "The number of training instances matching these particular values.",
            "So now we need to test."
        ],
        [
            "Discounts an in order to do them efficiently, we use first trick where we see that rare values of attributes can be ignored.",
            "What this means is that suppose we are interested in lower values which are less than a threshold Alpha.",
            "Then it can be shown that the particular count C of 80 and see of BT needs to be greater than one by Alpha.",
            "So for example, if Alpha is something like .01, it means that at least hundred training instances must match 80 NBT in order for for the R value to be less than .01.",
            "So what we can do, we can ignore all the rare values of any attribute and we can replace all those rare values of the generic real value, and that drastically decreases the arity of many of the attributes, and it gives us a."
        ],
        [
            "Timesaving another speedup trick is to use a very efficient data structure called 83.",
            "Edit Re is a caching data structure which precomputes most of the accounts for most queries and it requires very small computation time for some queries.",
            "So we construct a D3 on the reduced entity attributes in order to retrieve the."
        ],
        [
            "Once.",
            "So now I'll be described.",
            "I'll be describing another anomaly detection method that marginal animale, which accounts for some deficiencies in the conditional animally.",
            "So the main."
        ],
        [
            "Motivation for looking at marginal animals is we asked the question, what about the rare values?",
            "So in conditional anomalies we ignore all the rare values, but in some cases.",
            "For example, if we are importing plutonium or we see a container of worth 1 million, we might be interested in detecting records of this kind, which are being ignored by the previous method.",
            "So the question we ask here is that what is the probability of seeing something as rare as this or even rarer?",
            "So this is actually kind of parallel to the definition of P values for real valued attributes, and we define a quantity called Q value of 80.",
            "The attribute value of a in the record T. The Q value of 80 is the sum of the probabilities of all values of a which are equally rare or rarer than 80."
        ],
        [
            "So just to illustrate using an example, suppose we have two cases, attribute A and attribute B and these plots give the probability of occurrence for each of the possible values.",
            "50 values of A and nine values of B.",
            "And suppose in case one we see the value a four of A and case two we see value B9 of B.",
            "Now both these values have very fairly small probability of occurrence and they could be treated as animals.",
            "But then we compute the Q value which gives us the probability of seeing something as rare as a four.",
            "It is .47, which means that there is a 47% chance in this case of seeing a value as rare as a four.",
            "So we should not really be surprised by seeing something that is so rare.",
            "But in case 2.",
            "We see that the Q value is .01, which means there's a 1% chance of seeing something this rare.",
            "So we're actually surprised in this case.",
            "So what we need to do is we need to compute the Q value over all possible attributes or attributes sets for a particular test trick or T and then we need to come up with the score for the record T based on all these Q values.",
            "I'll not be explaining the details here, they are given in the paper and you can come to the poster for details and we do some efficient computation in order to calculate the Q values."
        ],
        [
            "For each record now I'll be presenting the data sets that we have used and some empirical results on them.",
            "So the first data set as I mentioned previously, is the peers data set which has a list of containers being imported into the country.",
            "It has to."
        ],
        [
            "Possible attributes an as you can see, some of the attributes have very high arity more than 5000 possible values.",
            "And we discretize the real value attributes to."
        ],
        [
            "Five values.",
            "One property of this data set is that there is no labeled anomalies in this data set.",
            "So in order to do an evaluation of the different methods we need to generate animals and the first method that we use to generate animals is to select a test record that we want to modify, and then we randomly choose a particular attribute.",
            "We flip the value of this attribute by drawing from the marginal distribution of the attribute.",
            "So the assumption here is that when we flip the value of the attribute, the relationship of this.",
            "Attribute with other attributes gets destroyed, so this record should stand out to be animals.",
            "Another method of animal generation that we tried was to insert records from a different time period into the test record.",
            "So if the test record corresponds to a particular time period, we insert a record from a different time period.",
            "Assuming that this other record comes from a different distribution.",
            "So in our experiments we have used 100,000 training records and 10,000 test records, and 10% of the test recorder created to be analysed using either of these methods."
        ],
        [
            "This gives a competitive performance of four methods, the conditional and marginal methods that we proposed.",
            "The base network likelihood method and the Lee Rod, which is sufficient rule learner method.",
            "The X axis is a precision recall curve.",
            "The X axis.",
            "Actually the recall or the detection rate, which gives the proportion of true animals which have been detected so far and the Y axis is the proportion of true positives to the proportion of total number of positives that have been predicted by the algorithm.",
            "So any curve which is towards the top or right part is better than any curve which is below and we see that both the conditional and marginal methods perform significantly better than the leader and Bayes net methods.",
            "This is for the animal generation method, one where we randomly flip one attribute value."
        ],
        [
            "So we see at another result.",
            "This is for when we insert records from a different month.",
            "Here again we see that both the conditional and the marginal methods perform better than the Lee Rod and basic methods.",
            "We see that the marginal method performs best in this case, and this can be explained by the fact that for records inserted from a different month, many of the many of them correspond to rare values which do not occur in the training data set because the training data set was from the original month.",
            "So the conditional method effectively ignores all these rare values, but the marginal method is able to take advantage of the fact of this rare values."
        ],
        [
            "Another data set that we have applied our algorithms is on the KDD Cup 99 data set, which is a data set of data set of network sessions.",
            "It has 41 features and most of them actually take real values in this data set, so we discretize them to 5 levels to get a categorical data set, and this data set is actually labeled.",
            "So we select six attack types in order to evaluate our algorithms."
        ],
        [
            "And in this result we just give a competitive performance between the conditional and the base net methods.",
            "So the X axis the same detection rate and the Y axis.",
            "Actually the difference between the performance of the conditional to the performance of the base net.",
            "So any positive value indicates a better performance by the conditional method.",
            "So all the six different attack types are shown here with a 95% confidence bar.",
            "By using 20 random runs, we see that in most cases the performance is significantly above 0, which means.",
            "The conditional method is performing better, but except for the case of guest password, in which case the Bay State performs better.",
            "And when we looked at the data set, these records actually corresponded to some very rare values of attributes and that resulted in a worse performance by their condition."
        ],
        [
            "Method.",
            "So to summarize, the talk for detecting anomalies using a single probability probability distribution and computing the whole record likelihood, there are some problems.",
            "The first of them is if we some of the attributes have very high arity, then what it tends to do is it tends to always detect rare values of those attributes irrespective of if they're really animals or not, and the signal in some of the features can get washed out when we compute the complete log likelihood.",
            "By noise in rest of the features and also animals can highlight mistakes in model learning, so usually animals tend to lie to the outlier of the probability distribution and for the model learning they don't learn the model so well.",
            "In the outlier cases.",
            "So we propose a new approach to solve this and we consider subsets of features up to some size which is similar to the rule learning method.",
            "And then we define a quantity called R value which can indicate anomalies arising out of a high negative correlation between values.",
            "And we show that empirical results on real datasets demonstrate improved performance and the time and memory requirements are actually compatible.",
            "It's slightly higher, but it's comparable to the baseline methods."
        ],
        [
            "Thank you and my other poster board is number 39 so please visit us."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about detecting anomaly detection in categorical datasets, and this is joint work with my advisor Jeff Snyder at Carnegie Mellon.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a brief outline of the talk, so I'll start with the problem motivation follow with a brief description of the related work and followed by two animal detection methods that I described here.",
                    "label": 0
                },
                {
                    "sent": "The conditional animali and the marginal animal methods, and followed by some datasets and results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comparisons of these methods.",
                    "label": 0
                },
                {
                    "sent": "So to give a motivation of animal detection in real life in a lot of domains, huge amount of data is collected regularly and there is no need to detect unusual observations which we call animales.",
                    "label": 0
                },
                {
                    "sent": "So just for a small sample of examples in the list of import containers being imported into the country we are interested in detecting animals records which can detect illegal activities such as smuggling.",
                    "label": 0
                },
                {
                    "sent": "Also, in computer networks animals can detect network intrusions in astronomical data.",
                    "label": 1
                },
                {
                    "sent": "Animals can lead us to new astronomical discoveries and then in emergency Department data or healthcare data, animals can detect disease outbreaks or epidemics.",
                    "label": 0
                },
                {
                    "sent": "So this is just a small sample of domains where we have the data and we actually have applied or we plan to apply our methods.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to give a more formal overview of the problem, we have two sets of data.",
                    "label": 0
                },
                {
                    "sent": "The first set is the training data and we focus solely on categorical data.",
                    "label": 0
                },
                {
                    "sent": "The reason being that the motivating data set, which is the import of containers data set, was mainly categorical in nature, and we categorize any real valued attributes to some discrete levels.",
                    "label": 0
                },
                {
                    "sent": "The training data can have a large number of records in the range of 100,000 to 1,000,000 records.",
                    "label": 1
                },
                {
                    "sent": "And one important aspect of the problem is that the training data is unlabeled.",
                    "label": 1
                },
                {
                    "sent": "It means that this is unsupervised problem, but we can handle a small fraction of the training data.",
                    "label": 0
                },
                {
                    "sent": "Being animal is.",
                    "label": 0
                },
                {
                    "sent": "So these animals records should not be very high.",
                    "label": 0
                },
                {
                    "sent": "It should be in the range of 1 to 2%.",
                    "label": 0
                },
                {
                    "sent": "But and it's unlabeled.",
                    "label": 0
                },
                {
                    "sent": "One important factor in this data set is that attributes can have a lot of values in the range of 5000 to 10,000 possible values, and that raises some important issues which will address in the talk.",
                    "label": 0
                },
                {
                    "sent": "The test data has the same properties, but it can have any fraction of records animals and our goal here is that we want to detect records in the test set which are actually animals given the training set, and in general we would like to score records in the test set with a degree or the score of animal asnos and then flag records based on a desired false positive.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "So just to give an example of the motivating data set, this is the PS data which has 10 attributes.",
                    "label": 0
                },
                {
                    "sent": "Most of them take categorical values and there are three attributes which take real values, but we categorize them to some five discrete levels.",
                    "label": 0
                },
                {
                    "sent": "I'll be using this example data set to demonstrate.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the rest of my talk, briefly give overview of related work in this field.",
                    "label": 1
                },
                {
                    "sent": "There has been a lot of work of animal detection using real valued attributes or in a supervised setting, but for categorical valued attributes.",
                    "label": 0
                },
                {
                    "sent": "In unsupervised setting, one of the popular methods is to use a likelihood based method, where essentially first we learn a probability distribution from the training data, and then we for each test record we calculate the record likelihood and if the record likelihood is low, then we say that the record is.",
                    "label": 1
                },
                {
                    "sent": "Analysts and people have used different algorithms such as based dependency trees and Bayes network for generating the MoD.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another another method that has been used if you were here for the first talk here is the using Association rule learners to detect anamolies.",
                    "label": 0
                },
                {
                    "sent": "Andali Red is a system that has been developed and it learns rules of the Form X implies Y where X&Y are possible attribute values and it learns rules confident rules of this form an if any record X is present.",
                    "label": 1
                },
                {
                    "sent": "But why is not present then the animal score depends on the probability of not seeing Y given X.",
                    "label": 0
                },
                {
                    "sent": "One problem with this method is that this learns rules with high degree of positive correlation.",
                    "label": 0
                },
                {
                    "sent": "But as we see now a data set we have a lot of possible values and such positive correlations are rare.",
                    "label": 0
                },
                {
                    "sent": "Rather negative correlations are much more common, so our method will tend to focus on high negative correlations and one big problem with the likelihood based methods is that it learns model over all the possible attributes.",
                    "label": 0
                },
                {
                    "sent": "So if in a small subset of attributes there is some signal of animale.",
                    "label": 0
                },
                {
                    "sent": "Then it can get washed out by the other noise in the other.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attributes.",
                    "label": 0
                },
                {
                    "sent": "So now I'll describe.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conditional anomaly which we define in this work.",
                    "label": 0
                },
                {
                    "sent": "So I'll start with giving an example of a problem using the base network likelihood method.",
                    "label": 0
                },
                {
                    "sent": "So for example we learn Bayes network on our training data and say the conditional probability commodity given country.",
                    "label": 0
                },
                {
                    "sent": "This is a factor in the Bayes network and we see a test record importing gold from China and we see that the conditional probability is .01 which is very small.",
                    "label": 1
                },
                {
                    "sent": "So the question is, is this small value?",
                    "label": 0
                },
                {
                    "sent": "Can we treat this as animal because?",
                    "label": 0
                },
                {
                    "sent": "This small value is going to make the record likelihood in the Bayes network to be very small.",
                    "label": 0
                },
                {
                    "sent": "So if we dig further, we see that the marginal probability of gold is also very small .01, which means that this is a rare commodity.",
                    "label": 0
                },
                {
                    "sent": "So the fact that gold is rare explains the fact that importing gold from China is also rare, so we should not be surprised by seeing this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Record so in order to, in order to compensate for this, we define this quantity R value, which is our value of the community of record T and country of record T, which is a ratio of the conditional with a marginal.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be one which is no longer a small value, so this R value is the main concept behind condition.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Animal is and if you see another example, suppose we see a record which imports copper from China.",
                    "label": 0
                },
                {
                    "sent": "The copper is not a rare commodity, but the probability of copper from China is still low.",
                    "label": 0
                },
                {
                    "sent": "So in this case the R value is .01, which is a small quantity and we are surprised at seeing such a small quantity of the R value so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To give a more formal definition of our value, it is defined over 2 attribute values at and BT and our value is defined to be the ratio of this conditional to the marginal, which can be written in a symmetric form, and it can be written in this form where the numerator is the joint probability of occurrence of the two values and the denominator is the probability of occurrence of the two values if they were independent.",
                    "label": 1
                },
                {
                    "sent": "So having a smaller value means that there is a high negative correlation between these two occurrence of these two values.",
                    "label": 0
                },
                {
                    "sent": "So if they occur together and our values.",
                    "label": 0
                },
                {
                    "sent": "Much, much less than one.",
                    "label": 0
                },
                {
                    "sent": "Then we can say that TT is animal is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in general we extend this definition to a set of more than one attributes.",
                    "label": 1
                },
                {
                    "sent": "So a can be reset community and we can be the set US foreign country and we can define our value in this form.",
                    "label": 0
                },
                {
                    "sent": "There are exponential number of subsets possible, so we restrict the number of attributes in each set to a constant K. So we consider up to two K attributes wilcon.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bring such our value.",
                    "label": 0
                },
                {
                    "sent": "So now I give the algorithm for testing a particular record T. What we do is we take each mutually exclusive pair of attribute subsets A&B, and then we compute the corresponding R value.",
                    "label": 1
                },
                {
                    "sent": "There is exponential number of such possible R values, but so we do a very efficient calculation of these probability values.",
                    "label": 0
                },
                {
                    "sent": "M is the total number of attributes and K is the number of attributes in each subset.",
                    "label": 1
                },
                {
                    "sent": "So now after we calculate all the R values for a given record, we need to assign a score to the record.",
                    "label": 1
                },
                {
                    "sent": "One heuristic is to assign the minimum our value as the score.",
                    "label": 0
                },
                {
                    "sent": "Then we ignore the information from all the other attributes which are not considered in the R value.",
                    "label": 0
                },
                {
                    "sent": "So another heuristic is to combine evidence from different different subsets and we do this by taking a product of selected our values.",
                    "label": 0
                },
                {
                    "sent": "I'll not be describing this in detail.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given in the paper.",
                    "label": 0
                },
                {
                    "sent": "Now we need to estimate this probability values in efficient manner.",
                    "label": 1
                },
                {
                    "sent": "So first of all, in order to estimate them we can use a maximum likelihood estimation which gives a ratio of the counts which match the particular values in the training set and the total number of training cases.",
                    "label": 1
                },
                {
                    "sent": "But one problem with usually using the maximum likelihood estimation is if C of 80 zero.",
                    "label": 0
                },
                {
                    "sent": "That is, it doesn't have any any matching instances in the training data set.",
                    "label": 0
                },
                {
                    "sent": "Then it can be divided by zero, or if this is 0R value can become zero.",
                    "label": 0
                },
                {
                    "sent": "So if we want to avoid that and so we use a lot less moving and we use.",
                    "label": 0
                },
                {
                    "sent": "This turns out to be estimate of the probability and the R value turns out to be in this form.",
                    "label": 0
                },
                {
                    "sent": "And these are the counts where.",
                    "label": 0
                },
                {
                    "sent": "The number of training instances matching these particular values.",
                    "label": 0
                },
                {
                    "sent": "So now we need to test.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discounts an in order to do them efficiently, we use first trick where we see that rare values of attributes can be ignored.",
                    "label": 1
                },
                {
                    "sent": "What this means is that suppose we are interested in lower values which are less than a threshold Alpha.",
                    "label": 0
                },
                {
                    "sent": "Then it can be shown that the particular count C of 80 and see of BT needs to be greater than one by Alpha.",
                    "label": 0
                },
                {
                    "sent": "So for example, if Alpha is something like .01, it means that at least hundred training instances must match 80 NBT in order for for the R value to be less than .01.",
                    "label": 0
                },
                {
                    "sent": "So what we can do, we can ignore all the rare values of any attribute and we can replace all those rare values of the generic real value, and that drastically decreases the arity of many of the attributes, and it gives us a.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Timesaving another speedup trick is to use a very efficient data structure called 83.",
                    "label": 1
                },
                {
                    "sent": "Edit Re is a caching data structure which precomputes most of the accounts for most queries and it requires very small computation time for some queries.",
                    "label": 1
                },
                {
                    "sent": "So we construct a D3 on the reduced entity attributes in order to retrieve the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once.",
                    "label": 0
                },
                {
                    "sent": "So now I'll be described.",
                    "label": 0
                },
                {
                    "sent": "I'll be describing another anomaly detection method that marginal animale, which accounts for some deficiencies in the conditional animally.",
                    "label": 0
                },
                {
                    "sent": "So the main.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Motivation for looking at marginal animals is we asked the question, what about the rare values?",
                    "label": 1
                },
                {
                    "sent": "So in conditional anomalies we ignore all the rare values, but in some cases.",
                    "label": 0
                },
                {
                    "sent": "For example, if we are importing plutonium or we see a container of worth 1 million, we might be interested in detecting records of this kind, which are being ignored by the previous method.",
                    "label": 0
                },
                {
                    "sent": "So the question we ask here is that what is the probability of seeing something as rare as this or even rarer?",
                    "label": 1
                },
                {
                    "sent": "So this is actually kind of parallel to the definition of P values for real valued attributes, and we define a quantity called Q value of 80.",
                    "label": 0
                },
                {
                    "sent": "The attribute value of a in the record T. The Q value of 80 is the sum of the probabilities of all values of a which are equally rare or rarer than 80.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to illustrate using an example, suppose we have two cases, attribute A and attribute B and these plots give the probability of occurrence for each of the possible values.",
                    "label": 1
                },
                {
                    "sent": "50 values of A and nine values of B.",
                    "label": 1
                },
                {
                    "sent": "And suppose in case one we see the value a four of A and case two we see value B9 of B.",
                    "label": 0
                },
                {
                    "sent": "Now both these values have very fairly small probability of occurrence and they could be treated as animals.",
                    "label": 0
                },
                {
                    "sent": "But then we compute the Q value which gives us the probability of seeing something as rare as a four.",
                    "label": 0
                },
                {
                    "sent": "It is .47, which means that there is a 47% chance in this case of seeing a value as rare as a four.",
                    "label": 0
                },
                {
                    "sent": "So we should not really be surprised by seeing something that is so rare.",
                    "label": 0
                },
                {
                    "sent": "But in case 2.",
                    "label": 1
                },
                {
                    "sent": "We see that the Q value is .01, which means there's a 1% chance of seeing something this rare.",
                    "label": 0
                },
                {
                    "sent": "So we're actually surprised in this case.",
                    "label": 0
                },
                {
                    "sent": "So what we need to do is we need to compute the Q value over all possible attributes or attributes sets for a particular test trick or T and then we need to come up with the score for the record T based on all these Q values.",
                    "label": 0
                },
                {
                    "sent": "I'll not be explaining the details here, they are given in the paper and you can come to the poster for details and we do some efficient computation in order to calculate the Q values.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each record now I'll be presenting the data sets that we have used and some empirical results on them.",
                    "label": 0
                },
                {
                    "sent": "So the first data set as I mentioned previously, is the peers data set which has a list of containers being imported into the country.",
                    "label": 0
                },
                {
                    "sent": "It has to.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Possible attributes an as you can see, some of the attributes have very high arity more than 5000 possible values.",
                    "label": 0
                },
                {
                    "sent": "And we discretize the real value attributes to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Five values.",
                    "label": 0
                },
                {
                    "sent": "One property of this data set is that there is no labeled anomalies in this data set.",
                    "label": 0
                },
                {
                    "sent": "So in order to do an evaluation of the different methods we need to generate animals and the first method that we use to generate animals is to select a test record that we want to modify, and then we randomly choose a particular attribute.",
                    "label": 0
                },
                {
                    "sent": "We flip the value of this attribute by drawing from the marginal distribution of the attribute.",
                    "label": 1
                },
                {
                    "sent": "So the assumption here is that when we flip the value of the attribute, the relationship of this.",
                    "label": 0
                },
                {
                    "sent": "Attribute with other attributes gets destroyed, so this record should stand out to be animals.",
                    "label": 1
                },
                {
                    "sent": "Another method of animal generation that we tried was to insert records from a different time period into the test record.",
                    "label": 0
                },
                {
                    "sent": "So if the test record corresponds to a particular time period, we insert a record from a different time period.",
                    "label": 0
                },
                {
                    "sent": "Assuming that this other record comes from a different distribution.",
                    "label": 0
                },
                {
                    "sent": "So in our experiments we have used 100,000 training records and 10,000 test records, and 10% of the test recorder created to be analysed using either of these methods.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This gives a competitive performance of four methods, the conditional and marginal methods that we proposed.",
                    "label": 1
                },
                {
                    "sent": "The base network likelihood method and the Lee Rod, which is sufficient rule learner method.",
                    "label": 0
                },
                {
                    "sent": "The X axis is a precision recall curve.",
                    "label": 0
                },
                {
                    "sent": "The X axis.",
                    "label": 0
                },
                {
                    "sent": "Actually the recall or the detection rate, which gives the proportion of true animals which have been detected so far and the Y axis is the proportion of true positives to the proportion of total number of positives that have been predicted by the algorithm.",
                    "label": 1
                },
                {
                    "sent": "So any curve which is towards the top or right part is better than any curve which is below and we see that both the conditional and marginal methods perform significantly better than the leader and Bayes net methods.",
                    "label": 0
                },
                {
                    "sent": "This is for the animal generation method, one where we randomly flip one attribute value.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we see at another result.",
                    "label": 0
                },
                {
                    "sent": "This is for when we insert records from a different month.",
                    "label": 0
                },
                {
                    "sent": "Here again we see that both the conditional and the marginal methods perform better than the Lee Rod and basic methods.",
                    "label": 0
                },
                {
                    "sent": "We see that the marginal method performs best in this case, and this can be explained by the fact that for records inserted from a different month, many of the many of them correspond to rare values which do not occur in the training data set because the training data set was from the original month.",
                    "label": 1
                },
                {
                    "sent": "So the conditional method effectively ignores all these rare values, but the marginal method is able to take advantage of the fact of this rare values.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another data set that we have applied our algorithms is on the KDD Cup 99 data set, which is a data set of data set of network sessions.",
                    "label": 1
                },
                {
                    "sent": "It has 41 features and most of them actually take real values in this data set, so we discretize them to 5 levels to get a categorical data set, and this data set is actually labeled.",
                    "label": 1
                },
                {
                    "sent": "So we select six attack types in order to evaluate our algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this result we just give a competitive performance between the conditional and the base net methods.",
                    "label": 1
                },
                {
                    "sent": "So the X axis the same detection rate and the Y axis.",
                    "label": 0
                },
                {
                    "sent": "Actually the difference between the performance of the conditional to the performance of the base net.",
                    "label": 0
                },
                {
                    "sent": "So any positive value indicates a better performance by the conditional method.",
                    "label": 0
                },
                {
                    "sent": "So all the six different attack types are shown here with a 95% confidence bar.",
                    "label": 0
                },
                {
                    "sent": "By using 20 random runs, we see that in most cases the performance is significantly above 0, which means.",
                    "label": 0
                },
                {
                    "sent": "The conditional method is performing better, but except for the case of guest password, in which case the Bay State performs better.",
                    "label": 0
                },
                {
                    "sent": "And when we looked at the data set, these records actually corresponded to some very rare values of attributes and that resulted in a worse performance by their condition.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Method.",
                    "label": 0
                },
                {
                    "sent": "So to summarize, the talk for detecting anomalies using a single probability probability distribution and computing the whole record likelihood, there are some problems.",
                    "label": 1
                },
                {
                    "sent": "The first of them is if we some of the attributes have very high arity, then what it tends to do is it tends to always detect rare values of those attributes irrespective of if they're really animals or not, and the signal in some of the features can get washed out when we compute the complete log likelihood.",
                    "label": 0
                },
                {
                    "sent": "By noise in rest of the features and also animals can highlight mistakes in model learning, so usually animals tend to lie to the outlier of the probability distribution and for the model learning they don't learn the model so well.",
                    "label": 1
                },
                {
                    "sent": "In the outlier cases.",
                    "label": 1
                },
                {
                    "sent": "So we propose a new approach to solve this and we consider subsets of features up to some size which is similar to the rule learning method.",
                    "label": 1
                },
                {
                    "sent": "And then we define a quantity called R value which can indicate anomalies arising out of a high negative correlation between values.",
                    "label": 1
                },
                {
                    "sent": "And we show that empirical results on real datasets demonstrate improved performance and the time and memory requirements are actually compatible.",
                    "label": 0
                },
                {
                    "sent": "It's slightly higher, but it's comparable to the baseline methods.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you and my other poster board is number 39 so please visit us.",
                    "label": 0
                }
            ]
        }
    }
}