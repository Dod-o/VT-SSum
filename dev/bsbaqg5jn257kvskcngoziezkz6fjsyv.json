{
    "id": "bsbaqg5jn257kvskcngoziezkz6fjsyv",
    "title": "Fast Clustering based on Kernel Density Estimation",
    "info": {
        "author": [
            "Alexander Hinneburg, Institute for Informatics, Martin-Luther University"
        ],
        "published": "Oct. 8, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering",
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Density Estimation"
        ]
    },
    "url": "http://videolectures.net/ida07_hinneburg_dfc/",
    "segmentation": [
        [
            "This dental this then pointed include 2.0 that has nothing to do with Web 2.0.",
            "It's just the second version of this algorithm and I thought we got some interesting.",
            "Lancement of the original algorithm which was published on 98.",
            "Katie D conference so.",
            "This work is collaborative work.",
            "This unsettling Gabriel just former diploma diploma student."
        ],
        [
            "Of mine and overview looks like this.",
            "I first introduced the density based clustering and then clue 1.0.",
            "I lay out the problem.",
            "The problem which is Hill climbing and be reformulated, climbing as an expectation maximization algorithm.",
            "Then there are some minor issues about the identification of local Maxima which has to do with identifying the clusterings.",
            "And then there's some some broader issues come in like you could apply general techniques to this new algorithms and then I present some XP."
        ],
        [
            "Comments, so when we talk about density based clustering, we have the assumption that the classes are regions of high density in the data and we assume the attributes are.",
            "In this case the numeric attributes from a dimensional space and we have none of those dimensional data points.",
            "And the question is how to estimate density.",
            "There are several answers to this.",
            "First, you can say we do parametric models and like mixture models and things like this.",
            "This is something I'm not going to talk about then.",
            "They're nonparametric models like histograms.",
            "Or is it more advanced stuff like kernel density?"
        ],
        [
            "Estimation.",
            "The idea of constant kernel density estimation is too.",
            "The models influence of a data point by Colonel or just setting.",
            "You could also say by a generalization you call membership function and the density.",
            "Then it's a normalized sum of all these kernels and then there's a parameter coming into play.",
            "This smoothing parameter called H and depends on the width.",
            "Working depends on the on the value of this parameter.",
            "How these kernels smooth surface data choose is very small.",
            "Then you have basically peaks and you have many local maximums and if you choose it a big bigger.",
            "Then you smooth data points together and you get less local maxima's.",
            "Some just this formulas are from the textbook.",
            "In this case we are.",
            "I'm talking about Gaussian kernels.",
            "Yeah, written like this until you see some over the data points and you put in.",
            "And you apply the kernel function and you have this data points going into the kernel funk."
        ],
        [
            "So the clustering in Dental 1.0 are defined by the local maximas so cluster is.",
            "As the points which correspond to this same local maximum.",
            "So basically these two points are one cluster.",
            "These are another cluster, and we originally we had an additional parameter PSI which used to separate between outliers.",
            "But this is not of main importance.",
            "You can choose imagine this to set 0.",
            "Originally we thought we did the following approach to find the local Maxima.",
            "We did hillclimbing.",
            "We computed the gradient of this density function like this here and then we started an iteration starting on a data point, computing the gradient and moving a bit into in the direction of the gradient, and we choose this data to be constant.",
            "So this is a constant step size and."
        ],
        [
            "Created some problems.",
            "The one problem is that it's not very efficient.",
            "You make many unnecessary unnecessarily small steps.",
            "In the beginning.",
            "You see here you need many steps to approach the local maximum.",
            "And the second thing is that it does not converge to the local maximum in the end if you zoom in here, you jump around and you don't.",
            "You don't converge to something, so you just come close.",
            "So this is something some things which are not too nice and base."
        ],
        [
            "These are the problems which are solved by the new approach.",
            "The new approach can be if you don't want the whole theory, you can imagine like this that you take the density function and you basically differentiate and set it to 0.",
            "You can cancel out these constants here and rearrange and you get this equation, but this is not.",
            "This is not a solution because there is still here and the kernel the kernel, but you can think I do not have a solution, but I can iterate and you do it like iteration and you can iterative formula."
        ],
        [
            "And this does a nice thing.",
            "What we want, it automatically adjusts the step size.",
            "Basically at next."
        ],
        [
            "Cost if you look back here, have the same sum over the data points which we had before here in the gradient.",
            "That is, some appears twice, doesn't matter it."
        ],
        [
            "Same computation.",
            "So the same costs and you have much fewer steps and you really converge.",
            "So now the question, why is this going to work?",
            "So this slightly fast introduction is no proof, but."
        ],
        [
            "You can imagine that you cast the problem of maximizing a kernel density function by rewriting it you just put.",
            "And here's the kernel function and then you reformulated it and you can sing.",
            "It looks like a mixture model.",
            "Here is N components.",
            "And what you have something some constraints CPS are fixed and as a muse are set to the original data points.",
            "So you can think of the density as a kind of likelihood of a mixture model and now the different stuff is here that we don't maximize this luggage over the para meters, mu and Sigma and \u03c0, but we maximize it over there.",
            "The X, but this is just doesn't make a big problem because here in this formula you can see that you can replace this XD by muti that this is interchangeable because of this square here.",
            "So basically you do the same stuff as normal mixture models.",
            "You introduce a hidden variable which here is 1 if the density at this point in SpaceX is explained by a certain component which is the kernel.",
            "Well, basically kernel offered of a certain data point and zero else wise and then you do the."
        ],
        [
            "Yeah, I'm stuff and you come up with this.",
            "The step and if you plug this status into the M step you get the nice formula where I've shown you before, so this shows it well when you maximize.",
            "Is a complete lie to you also maximize the data likelihood, which is basically in this case a kernel density estimate, and when starting the with.",
            "Well, at the data point you do the Hill climbing for that data point and so we have got a nice formulation.",
            "For our dental algorithm and had a much."
        ],
        [
            "Faster hit climbing procedure and then slide problems come into play.",
            "Now we have to identify the local maximum slightly differently as we did before.",
            "The EM algorithm iterates and iterates into rates and sometimes and from one point it converges to a point and you detect this.",
            "Usually by this kind of this formula you put threshold epsilon and if the target function doesn't change much then you stop and we call.",
            "The endpoints were reached by this.",
            "Iteration X star T, and we also recorded the sizes of the last steps.",
            "If you converge to the fixed point where these steps become closer becomes smaller.",
            "And so you have kind of sense of scale.",
            "What you are dealing at.",
            "So in the assumption we are taking to identify the local Maxima is that's a true local Maxima ball around X star and the ball has a radius of St. Anthem we associate two points to be in the same cluster of there.",
            "Endpoints of the distance between the endpoints is smaller than the sum between ST&T prime and then we say zetaboards overlaps in the resize.",
            "It belong to the same maximum.",
            "This may create some cases non unique assignment in this case which we rarely happens, you just iterate a few steps further and then your ex star convergence, little bit closer to the local maximum and then you can resolve this.",
            "I'm nonunique assignments."
        ],
        [
            "So far for the algorithm.",
            "I'm.",
            "The algorithm still, and it's in each iteration some overall data points, which is quite expensive.",
            "We look for some more general methods to speed this up, and one thing is to do a sparse EM.",
            "This is a paper by Neal and Hinton, 99 quite general paper and basically just say you update only percent of the points with the largest posteriors.",
            "Basically those points really, which has the largest kernel values which produce large kernel value.",
            "Within one iteration and save the rest of computation officers after the first iteration.",
            "You need to do one iteration to sort by the prior by the posterior, and then you can save it.",
            "And we did the following that.",
            "We just identified a few points which really contributes to the local maximum and then we basically did the other points or not.",
            "We didn't update the other points.",
            "This is slightly over there at deviation from the original approach, pioneering hintons.",
            "I also update this set, but since we converge so fast, it wasn't really necessary to update the points.",
            "Instead L, which not so much contribute, and the other approach is much more cross.",
            "We just reduced data set first step.",
            "Very simple to buy random sampling if you have endpoints, you just sample P percent of as representative points.",
            "Or you run K means on this end points and take their centroids as representative points and take some kernels over those represent."
        ],
        [
            "With points.",
            "So the experiments.",
            "Basically confirmed our assumption that we need much less iterations, number of iterations as shown here for all points.",
            "If you have this.",
            "Previous approach of 1.0 at this and here you have put different values for this epsilon where we stop the iteration.",
            "If you put a zero there.",
            "Well, it's not useful, but you see, you do not much more work.",
            "But if you put a reasonable value like this, you do much less iterations and just to identify the correct clustering.",
            "This was artificial data here."
        ],
        [
            "So you read it in a moment.",
            "To compare the acceleration methods.",
            "Content terms of speed this.",
            "Spartium is a bit more expensive since you have to do one full iteration and then you can reduce the reduce the cost and tear.",
            "The sampling approaches.",
            "Doesn't want a big difference in business speak."
        ],
        [
            "With respect to quality, there's still artificial data, and we measured clear clustering quality with respect to normalized mutual information.",
            "We have here on the X axis sample size, so here we take the full sample which is not shown here.",
            "80% of the data OK, 20% of the data and you see here for low dimensional data, uniquely identify your clustering with a small error.",
            "But you only need 20% of your points.",
            "But this is low dimensional data like the equals 16 and a few of the higher like D 128.",
            "Or this elbows not so nicely shown anymore and you also need."
        ],
        [
            "Larger smoothing.",
            "For real data, we compared to K means and.",
            "Sir is not accelerated methods which is here.",
            "Operated this a like self step step size adjusting.",
            "This has some similar various SKU means or bit better in terms of this normalized mutual information.",
            "Here are shown the size of the sample size and here's the standard deviation is a semi this standard deviation and you see here the sampling cost you something but not too much and on real data.",
            "And you also can compare.",
            "But basically this shows that.",
            "This is a reasonable method compared to other methods you could ask for more comparisons, but our main goal here was to show that we improved over the original method in Q 1.0 and it also shows this nice connection between kernel density estimate."
        ],
        [
            "And M algorithm.",
            "So I consider this the main contribution, not the experimental part.",
            "In future work we were looking for automatic setting of this smoothing parameter.",
            "So far it was tuned manually.",
            "It wasn't so difficult.",
            "You start.",
            "Just start with a large value and then make it smaller until.",
            "The number of clusters gets to be reasonable low.",
            "We want to find a way to set this automatic without so much tuning.",
            "So."
        ],
        [
            "That's so far for my talk.",
            "Thank you for attention and I'm ready to take questions.",
            "Question.",
            "Confess with me, she following which also explores the same problem to find the local min shift algorithms.",
            "So says C means.",
            "Well, I do not know this.",
            "Love.",
            "I don't know this this algorithm.",
            "Local maximum of nasty.",
            "Essentially goes.",
            "Oh mean shift.",
            "Is there obvious point me to a lot of interesting work in this community?",
            "I haven't been aware of, like mountain methods and.",
            "Subtractive clustering.",
            "Polly Oh no there yeah.",
            "Set the age.",
            "By no means.",
            "Hi Debbie.",
            "Yeah, well if you have to put a higher higher smoothing in high dimensional space since space inherently becomes sparse so you have to smooth the data point over more portions of the space, which is also a bit problematic because then you that it depends.",
            "You cannot vary it so nicely depends on small variations, how their results are this.",
            "In high dimension it becomes a problematic this approach.",
            "If you have really high dimensions, but low dimensions, cuadro works quite well.",
            "Well, how dimension is one point?",
            "What about scalability?",
            "Bob Cooper yeah, so a sampling is 1 strategy as I showed.",
            "Yeah, so basically you could also try combined approaches like parameter clustering.",
            "You could use a parametric model instead of this, this kernels.",
            "Joseph, many ideas.",
            "Also, histograms could be used to identify dense parts.",
            "We didn't want to go into detail of this because this is a combination of several ideas, and if you really have a schedule scalability problem then you are free to explore these combinations.",
            "And but over here it's depends on this sampling rate or you still have to pay this high price so something of all data points.",
            "Check it again.",
            "Your approach.",
            "Interesting approach against the TV scan DB scan.",
            "Our regional approach be we tried yes, and then we combine it with histograms and it turned out to be.",
            "Also scalable, I know they are now also scalability methods around for DB scan like this and data bubbles like things.",
            "They called it like this.",
            "They also reduce the data 1st and then they find their classes on this reduced data set.",
            "I think that you could apply the same techniques here as well.",
            "Another question."
        ],
        [
            "You do some kind of cross validation which is very common in the classroom.",
            "I mean, just taking out some data and then compare.",
            "Catherine results review get?",
            "I mean they should be similar?",
            "In sense, it's like cross validation.",
            "No, we didn't do this because in this data set we have a ground truth.",
            "These are from this machine learning repository's.",
            "I didn't mention this, I think.",
            "Is this iris data set is quite well known and yes, but in the end I mean this is a classified data set.",
            "So yeah, the classes do not have to have a kind of don't have to be denser, right?",
            "It's the classes do not have to correspond to clusters necessary.",
            "That's that's correct.",
            "We didn't check this assumption here, but.",
            "This would be the approach then.",
            "OK, there are no other questions.",
            "Thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This dental this then pointed include 2.0 that has nothing to do with Web 2.0.",
                    "label": 0
                },
                {
                    "sent": "It's just the second version of this algorithm and I thought we got some interesting.",
                    "label": 0
                },
                {
                    "sent": "Lancement of the original algorithm which was published on 98.",
                    "label": 0
                },
                {
                    "sent": "Katie D conference so.",
                    "label": 0
                },
                {
                    "sent": "This work is collaborative work.",
                    "label": 0
                },
                {
                    "sent": "This unsettling Gabriel just former diploma diploma student.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of mine and overview looks like this.",
                    "label": 0
                },
                {
                    "sent": "I first introduced the density based clustering and then clue 1.0.",
                    "label": 0
                },
                {
                    "sent": "I lay out the problem.",
                    "label": 0
                },
                {
                    "sent": "The problem which is Hill climbing and be reformulated, climbing as an expectation maximization algorithm.",
                    "label": 0
                },
                {
                    "sent": "Then there are some minor issues about the identification of local Maxima which has to do with identifying the clusterings.",
                    "label": 1
                },
                {
                    "sent": "And then there's some some broader issues come in like you could apply general techniques to this new algorithms and then I present some XP.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comments, so when we talk about density based clustering, we have the assumption that the classes are regions of high density in the data and we assume the attributes are.",
                    "label": 1
                },
                {
                    "sent": "In this case the numeric attributes from a dimensional space and we have none of those dimensional data points.",
                    "label": 1
                },
                {
                    "sent": "And the question is how to estimate density.",
                    "label": 0
                },
                {
                    "sent": "There are several answers to this.",
                    "label": 0
                },
                {
                    "sent": "First, you can say we do parametric models and like mixture models and things like this.",
                    "label": 1
                },
                {
                    "sent": "This is something I'm not going to talk about then.",
                    "label": 0
                },
                {
                    "sent": "They're nonparametric models like histograms.",
                    "label": 0
                },
                {
                    "sent": "Or is it more advanced stuff like kernel density?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Estimation.",
                    "label": 0
                },
                {
                    "sent": "The idea of constant kernel density estimation is too.",
                    "label": 1
                },
                {
                    "sent": "The models influence of a data point by Colonel or just setting.",
                    "label": 1
                },
                {
                    "sent": "You could also say by a generalization you call membership function and the density.",
                    "label": 0
                },
                {
                    "sent": "Then it's a normalized sum of all these kernels and then there's a parameter coming into play.",
                    "label": 0
                },
                {
                    "sent": "This smoothing parameter called H and depends on the width.",
                    "label": 0
                },
                {
                    "sent": "Working depends on the on the value of this parameter.",
                    "label": 0
                },
                {
                    "sent": "How these kernels smooth surface data choose is very small.",
                    "label": 0
                },
                {
                    "sent": "Then you have basically peaks and you have many local maximums and if you choose it a big bigger.",
                    "label": 0
                },
                {
                    "sent": "Then you smooth data points together and you get less local maxima's.",
                    "label": 0
                },
                {
                    "sent": "Some just this formulas are from the textbook.",
                    "label": 0
                },
                {
                    "sent": "In this case we are.",
                    "label": 0
                },
                {
                    "sent": "I'm talking about Gaussian kernels.",
                    "label": 0
                },
                {
                    "sent": "Yeah, written like this until you see some over the data points and you put in.",
                    "label": 0
                },
                {
                    "sent": "And you apply the kernel function and you have this data points going into the kernel funk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the clustering in Dental 1.0 are defined by the local maximas so cluster is.",
                    "label": 1
                },
                {
                    "sent": "As the points which correspond to this same local maximum.",
                    "label": 0
                },
                {
                    "sent": "So basically these two points are one cluster.",
                    "label": 0
                },
                {
                    "sent": "These are another cluster, and we originally we had an additional parameter PSI which used to separate between outliers.",
                    "label": 0
                },
                {
                    "sent": "But this is not of main importance.",
                    "label": 0
                },
                {
                    "sent": "You can choose imagine this to set 0.",
                    "label": 1
                },
                {
                    "sent": "Originally we thought we did the following approach to find the local Maxima.",
                    "label": 0
                },
                {
                    "sent": "We did hillclimbing.",
                    "label": 0
                },
                {
                    "sent": "We computed the gradient of this density function like this here and then we started an iteration starting on a data point, computing the gradient and moving a bit into in the direction of the gradient, and we choose this data to be constant.",
                    "label": 1
                },
                {
                    "sent": "So this is a constant step size and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Created some problems.",
                    "label": 0
                },
                {
                    "sent": "The one problem is that it's not very efficient.",
                    "label": 0
                },
                {
                    "sent": "You make many unnecessary unnecessarily small steps.",
                    "label": 1
                },
                {
                    "sent": "In the beginning.",
                    "label": 0
                },
                {
                    "sent": "You see here you need many steps to approach the local maximum.",
                    "label": 0
                },
                {
                    "sent": "And the second thing is that it does not converge to the local maximum in the end if you zoom in here, you jump around and you don't.",
                    "label": 1
                },
                {
                    "sent": "You don't converge to something, so you just come close.",
                    "label": 0
                },
                {
                    "sent": "So this is something some things which are not too nice and base.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are the problems which are solved by the new approach.",
                    "label": 0
                },
                {
                    "sent": "The new approach can be if you don't want the whole theory, you can imagine like this that you take the density function and you basically differentiate and set it to 0.",
                    "label": 1
                },
                {
                    "sent": "You can cancel out these constants here and rearrange and you get this equation, but this is not.",
                    "label": 0
                },
                {
                    "sent": "This is not a solution because there is still here and the kernel the kernel, but you can think I do not have a solution, but I can iterate and you do it like iteration and you can iterative formula.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this does a nice thing.",
                    "label": 0
                },
                {
                    "sent": "What we want, it automatically adjusts the step size.",
                    "label": 1
                },
                {
                    "sent": "Basically at next.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cost if you look back here, have the same sum over the data points which we had before here in the gradient.",
                    "label": 0
                },
                {
                    "sent": "That is, some appears twice, doesn't matter it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same computation.",
                    "label": 0
                },
                {
                    "sent": "So the same costs and you have much fewer steps and you really converge.",
                    "label": 0
                },
                {
                    "sent": "So now the question, why is this going to work?",
                    "label": 0
                },
                {
                    "sent": "So this slightly fast introduction is no proof, but.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can imagine that you cast the problem of maximizing a kernel density function by rewriting it you just put.",
                    "label": 1
                },
                {
                    "sent": "And here's the kernel function and then you reformulated it and you can sing.",
                    "label": 1
                },
                {
                    "sent": "It looks like a mixture model.",
                    "label": 0
                },
                {
                    "sent": "Here is N components.",
                    "label": 0
                },
                {
                    "sent": "And what you have something some constraints CPS are fixed and as a muse are set to the original data points.",
                    "label": 0
                },
                {
                    "sent": "So you can think of the density as a kind of likelihood of a mixture model and now the different stuff is here that we don't maximize this luggage over the para meters, mu and Sigma and \u03c0, but we maximize it over there.",
                    "label": 0
                },
                {
                    "sent": "The X, but this is just doesn't make a big problem because here in this formula you can see that you can replace this XD by muti that this is interchangeable because of this square here.",
                    "label": 0
                },
                {
                    "sent": "So basically you do the same stuff as normal mixture models.",
                    "label": 0
                },
                {
                    "sent": "You introduce a hidden variable which here is 1 if the density at this point in SpaceX is explained by a certain component which is the kernel.",
                    "label": 0
                },
                {
                    "sent": "Well, basically kernel offered of a certain data point and zero else wise and then you do the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, I'm stuff and you come up with this.",
                    "label": 0
                },
                {
                    "sent": "The step and if you plug this status into the M step you get the nice formula where I've shown you before, so this shows it well when you maximize.",
                    "label": 0
                },
                {
                    "sent": "Is a complete lie to you also maximize the data likelihood, which is basically in this case a kernel density estimate, and when starting the with.",
                    "label": 1
                },
                {
                    "sent": "Well, at the data point you do the Hill climbing for that data point and so we have got a nice formulation.",
                    "label": 0
                },
                {
                    "sent": "For our dental algorithm and had a much.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Faster hit climbing procedure and then slide problems come into play.",
                    "label": 0
                },
                {
                    "sent": "Now we have to identify the local maximum slightly differently as we did before.",
                    "label": 0
                },
                {
                    "sent": "The EM algorithm iterates and iterates into rates and sometimes and from one point it converges to a point and you detect this.",
                    "label": 0
                },
                {
                    "sent": "Usually by this kind of this formula you put threshold epsilon and if the target function doesn't change much then you stop and we call.",
                    "label": 0
                },
                {
                    "sent": "The endpoints were reached by this.",
                    "label": 0
                },
                {
                    "sent": "Iteration X star T, and we also recorded the sizes of the last steps.",
                    "label": 0
                },
                {
                    "sent": "If you converge to the fixed point where these steps become closer becomes smaller.",
                    "label": 0
                },
                {
                    "sent": "And so you have kind of sense of scale.",
                    "label": 0
                },
                {
                    "sent": "What you are dealing at.",
                    "label": 0
                },
                {
                    "sent": "So in the assumption we are taking to identify the local Maxima is that's a true local Maxima ball around X star and the ball has a radius of St. Anthem we associate two points to be in the same cluster of there.",
                    "label": 0
                },
                {
                    "sent": "Endpoints of the distance between the endpoints is smaller than the sum between ST&T prime and then we say zetaboards overlaps in the resize.",
                    "label": 0
                },
                {
                    "sent": "It belong to the same maximum.",
                    "label": 1
                },
                {
                    "sent": "This may create some cases non unique assignment in this case which we rarely happens, you just iterate a few steps further and then your ex star convergence, little bit closer to the local maximum and then you can resolve this.",
                    "label": 0
                },
                {
                    "sent": "I'm nonunique assignments.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far for the algorithm.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "The algorithm still, and it's in each iteration some overall data points, which is quite expensive.",
                    "label": 0
                },
                {
                    "sent": "We look for some more general methods to speed this up, and one thing is to do a sparse EM.",
                    "label": 0
                },
                {
                    "sent": "This is a paper by Neal and Hinton, 99 quite general paper and basically just say you update only percent of the points with the largest posteriors.",
                    "label": 1
                },
                {
                    "sent": "Basically those points really, which has the largest kernel values which produce large kernel value.",
                    "label": 0
                },
                {
                    "sent": "Within one iteration and save the rest of computation officers after the first iteration.",
                    "label": 0
                },
                {
                    "sent": "You need to do one iteration to sort by the prior by the posterior, and then you can save it.",
                    "label": 0
                },
                {
                    "sent": "And we did the following that.",
                    "label": 0
                },
                {
                    "sent": "We just identified a few points which really contributes to the local maximum and then we basically did the other points or not.",
                    "label": 0
                },
                {
                    "sent": "We didn't update the other points.",
                    "label": 0
                },
                {
                    "sent": "This is slightly over there at deviation from the original approach, pioneering hintons.",
                    "label": 0
                },
                {
                    "sent": "I also update this set, but since we converge so fast, it wasn't really necessary to update the points.",
                    "label": 0
                },
                {
                    "sent": "Instead L, which not so much contribute, and the other approach is much more cross.",
                    "label": 0
                },
                {
                    "sent": "We just reduced data set first step.",
                    "label": 0
                },
                {
                    "sent": "Very simple to buy random sampling if you have endpoints, you just sample P percent of as representative points.",
                    "label": 1
                },
                {
                    "sent": "Or you run K means on this end points and take their centroids as representative points and take some kernels over those represent.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With points.",
                    "label": 0
                },
                {
                    "sent": "So the experiments.",
                    "label": 0
                },
                {
                    "sent": "Basically confirmed our assumption that we need much less iterations, number of iterations as shown here for all points.",
                    "label": 0
                },
                {
                    "sent": "If you have this.",
                    "label": 0
                },
                {
                    "sent": "Previous approach of 1.0 at this and here you have put different values for this epsilon where we stop the iteration.",
                    "label": 0
                },
                {
                    "sent": "If you put a zero there.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not useful, but you see, you do not much more work.",
                    "label": 0
                },
                {
                    "sent": "But if you put a reasonable value like this, you do much less iterations and just to identify the correct clustering.",
                    "label": 1
                },
                {
                    "sent": "This was artificial data here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you read it in a moment.",
                    "label": 0
                },
                {
                    "sent": "To compare the acceleration methods.",
                    "label": 1
                },
                {
                    "sent": "Content terms of speed this.",
                    "label": 0
                },
                {
                    "sent": "Spartium is a bit more expensive since you have to do one full iteration and then you can reduce the reduce the cost and tear.",
                    "label": 0
                },
                {
                    "sent": "The sampling approaches.",
                    "label": 0
                },
                {
                    "sent": "Doesn't want a big difference in business speak.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With respect to quality, there's still artificial data, and we measured clear clustering quality with respect to normalized mutual information.",
                    "label": 1
                },
                {
                    "sent": "We have here on the X axis sample size, so here we take the full sample which is not shown here.",
                    "label": 0
                },
                {
                    "sent": "80% of the data OK, 20% of the data and you see here for low dimensional data, uniquely identify your clustering with a small error.",
                    "label": 0
                },
                {
                    "sent": "But you only need 20% of your points.",
                    "label": 0
                },
                {
                    "sent": "But this is low dimensional data like the equals 16 and a few of the higher like D 128.",
                    "label": 0
                },
                {
                    "sent": "Or this elbows not so nicely shown anymore and you also need.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Larger smoothing.",
                    "label": 0
                },
                {
                    "sent": "For real data, we compared to K means and.",
                    "label": 1
                },
                {
                    "sent": "Sir is not accelerated methods which is here.",
                    "label": 0
                },
                {
                    "sent": "Operated this a like self step step size adjusting.",
                    "label": 0
                },
                {
                    "sent": "This has some similar various SKU means or bit better in terms of this normalized mutual information.",
                    "label": 0
                },
                {
                    "sent": "Here are shown the size of the sample size and here's the standard deviation is a semi this standard deviation and you see here the sampling cost you something but not too much and on real data.",
                    "label": 0
                },
                {
                    "sent": "And you also can compare.",
                    "label": 0
                },
                {
                    "sent": "But basically this shows that.",
                    "label": 0
                },
                {
                    "sent": "This is a reasonable method compared to other methods you could ask for more comparisons, but our main goal here was to show that we improved over the original method in Q 1.0 and it also shows this nice connection between kernel density estimate.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And M algorithm.",
                    "label": 0
                },
                {
                    "sent": "So I consider this the main contribution, not the experimental part.",
                    "label": 0
                },
                {
                    "sent": "In future work we were looking for automatic setting of this smoothing parameter.",
                    "label": 1
                },
                {
                    "sent": "So far it was tuned manually.",
                    "label": 0
                },
                {
                    "sent": "It wasn't so difficult.",
                    "label": 0
                },
                {
                    "sent": "You start.",
                    "label": 0
                },
                {
                    "sent": "Just start with a large value and then make it smaller until.",
                    "label": 0
                },
                {
                    "sent": "The number of clusters gets to be reasonable low.",
                    "label": 0
                },
                {
                    "sent": "We want to find a way to set this automatic without so much tuning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's so far for my talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you for attention and I'm ready to take questions.",
                    "label": 1
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Confess with me, she following which also explores the same problem to find the local min shift algorithms.",
                    "label": 0
                },
                {
                    "sent": "So says C means.",
                    "label": 0
                },
                {
                    "sent": "Well, I do not know this.",
                    "label": 0
                },
                {
                    "sent": "Love.",
                    "label": 0
                },
                {
                    "sent": "I don't know this this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Local maximum of nasty.",
                    "label": 0
                },
                {
                    "sent": "Essentially goes.",
                    "label": 0
                },
                {
                    "sent": "Oh mean shift.",
                    "label": 0
                },
                {
                    "sent": "Is there obvious point me to a lot of interesting work in this community?",
                    "label": 0
                },
                {
                    "sent": "I haven't been aware of, like mountain methods and.",
                    "label": 0
                },
                {
                    "sent": "Subtractive clustering.",
                    "label": 0
                },
                {
                    "sent": "Polly Oh no there yeah.",
                    "label": 0
                },
                {
                    "sent": "Set the age.",
                    "label": 0
                },
                {
                    "sent": "By no means.",
                    "label": 0
                },
                {
                    "sent": "Hi Debbie.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well if you have to put a higher higher smoothing in high dimensional space since space inherently becomes sparse so you have to smooth the data point over more portions of the space, which is also a bit problematic because then you that it depends.",
                    "label": 0
                },
                {
                    "sent": "You cannot vary it so nicely depends on small variations, how their results are this.",
                    "label": 0
                },
                {
                    "sent": "In high dimension it becomes a problematic this approach.",
                    "label": 0
                },
                {
                    "sent": "If you have really high dimensions, but low dimensions, cuadro works quite well.",
                    "label": 0
                },
                {
                    "sent": "Well, how dimension is one point?",
                    "label": 0
                },
                {
                    "sent": "What about scalability?",
                    "label": 0
                },
                {
                    "sent": "Bob Cooper yeah, so a sampling is 1 strategy as I showed.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so basically you could also try combined approaches like parameter clustering.",
                    "label": 0
                },
                {
                    "sent": "You could use a parametric model instead of this, this kernels.",
                    "label": 0
                },
                {
                    "sent": "Joseph, many ideas.",
                    "label": 0
                },
                {
                    "sent": "Also, histograms could be used to identify dense parts.",
                    "label": 0
                },
                {
                    "sent": "We didn't want to go into detail of this because this is a combination of several ideas, and if you really have a schedule scalability problem then you are free to explore these combinations.",
                    "label": 0
                },
                {
                    "sent": "And but over here it's depends on this sampling rate or you still have to pay this high price so something of all data points.",
                    "label": 0
                },
                {
                    "sent": "Check it again.",
                    "label": 0
                },
                {
                    "sent": "Your approach.",
                    "label": 0
                },
                {
                    "sent": "Interesting approach against the TV scan DB scan.",
                    "label": 0
                },
                {
                    "sent": "Our regional approach be we tried yes, and then we combine it with histograms and it turned out to be.",
                    "label": 0
                },
                {
                    "sent": "Also scalable, I know they are now also scalability methods around for DB scan like this and data bubbles like things.",
                    "label": 0
                },
                {
                    "sent": "They called it like this.",
                    "label": 0
                },
                {
                    "sent": "They also reduce the data 1st and then they find their classes on this reduced data set.",
                    "label": 0
                },
                {
                    "sent": "I think that you could apply the same techniques here as well.",
                    "label": 0
                },
                {
                    "sent": "Another question.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You do some kind of cross validation which is very common in the classroom.",
                    "label": 0
                },
                {
                    "sent": "I mean, just taking out some data and then compare.",
                    "label": 0
                },
                {
                    "sent": "Catherine results review get?",
                    "label": 0
                },
                {
                    "sent": "I mean they should be similar?",
                    "label": 0
                },
                {
                    "sent": "In sense, it's like cross validation.",
                    "label": 0
                },
                {
                    "sent": "No, we didn't do this because in this data set we have a ground truth.",
                    "label": 0
                },
                {
                    "sent": "These are from this machine learning repository's.",
                    "label": 0
                },
                {
                    "sent": "I didn't mention this, I think.",
                    "label": 0
                },
                {
                    "sent": "Is this iris data set is quite well known and yes, but in the end I mean this is a classified data set.",
                    "label": 0
                },
                {
                    "sent": "So yeah, the classes do not have to have a kind of don't have to be denser, right?",
                    "label": 0
                },
                {
                    "sent": "It's the classes do not have to correspond to clusters necessary.",
                    "label": 0
                },
                {
                    "sent": "That's that's correct.",
                    "label": 0
                },
                {
                    "sent": "We didn't check this assumption here, but.",
                    "label": 0
                },
                {
                    "sent": "This would be the approach then.",
                    "label": 0
                },
                {
                    "sent": "OK, there are no other questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you again.",
                    "label": 0
                }
            ]
        }
    }
}