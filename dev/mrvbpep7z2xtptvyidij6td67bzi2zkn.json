{
    "id": "mrvbpep7z2xtptvyidij6td67bzi2zkn",
    "title": "Vector-Valued Property Elicitation",
    "info": {
        "author": [
            "Rafael M. Frongillo, Harvard School of Engineering and Applied Sciences, Harvard University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_frongillo_property_elicitation/",
    "segmentation": [
        [
            "This is joint work with Ian Cash, who's in the audience."
        ],
        [
            "So an empirical risk minimization you think about minimizing a loss over the data in here?",
            "Maybe so, why isn't the data?",
            "Maybe we've conditioned on some X and we find the hypothesis that minimizes the loss over the data.",
            "The total loss called that W hat.",
            "Property elicitation instead.",
            "We think about minimizing expected loss, so here we think of our data is being drawn from some distribution P and the minimizer of the expected loss is the property gamma P. So we express the minimizer W hat as a function of the underlying distribution.",
            "Of course, this distribution could be the empirical distribution, and then obviously this is the same.",
            "So, so property elicitation is a way of thinking about the minimizer of the loss.",
            "And we say that allosso lisitsa property if this equality holds, that property is the minimizer of the expected loss for all distributions P."
        ],
        [
            "And so there are many well known examples.",
            "Obviously the squared loss.",
            "So this is the mean absolute loss lists the median an you can only say quantiles and expect tiles and much more exotic things.",
            "And in general there are two very fundamental questions about empirical risk minimization that you can express in terms of properties.",
            "In the."
        ],
        [
            "These are which properties are listenable, so if you dream up some statistic, can you come up with a loss where that statistic will be the minimizer?",
            "And what what?",
            "Surrogate losses are consistent with with a particular property?",
            "And to summarize the previous work, basically the real valued case and the expected value case are well understood, so the real valued case.",
            "This is where the property in question is can be expressed in a single real number, and then the illicit aghbal properties are those for which the level sets of the property a convex maximal, meaning that the set of distributions that share the same value.",
            "Of the property can be written as an affine set intersected with the set of distributions that you're interested in.",
            "For the expected value case, this is where the property is some mean of some potentially vector valued random variable.",
            "Here these are always a listable and the set of surrogate losses are the Bregman divergences, so these are the two sort of well known cases and in general other vector valued cases are pretty much wide open.",
            "So what we do is."
        ],
        [
            "We take some initial steps in the vector valued case.",
            "We first unify the results in the expected value case by removing differentiability assumptions.",
            "We then look at separable losses.",
            "In particular ask so loss is separable if you can just decompose it coordinate wise across your across the dimensions.",
            "And.",
            "It turns out there are some cases where this is the only thing that you can do, and this is sort of relevant for various things in practice.",
            "We also"
        ],
        [
            "Look at the construction of identification functions, which are relevant to this literature, which you can think of as derivatives of losses.",
            "And finally, we show that this condition of convex maximality of the level sets is not actually sufficient in the general case.",
            "And in particular, that last one suggests that the full characterization is actually out of reach at the moment, and so there are many interesting questions which I think are fundamental to machine learning.",
            "So you should come by the poster."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with Ian Cash, who's in the audience.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So an empirical risk minimization you think about minimizing a loss over the data in here?",
                    "label": 1
                },
                {
                    "sent": "Maybe so, why isn't the data?",
                    "label": 0
                },
                {
                    "sent": "Maybe we've conditioned on some X and we find the hypothesis that minimizes the loss over the data.",
                    "label": 0
                },
                {
                    "sent": "The total loss called that W hat.",
                    "label": 0
                },
                {
                    "sent": "Property elicitation instead.",
                    "label": 0
                },
                {
                    "sent": "We think about minimizing expected loss, so here we think of our data is being drawn from some distribution P and the minimizer of the expected loss is the property gamma P. So we express the minimizer W hat as a function of the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "Of course, this distribution could be the empirical distribution, and then obviously this is the same.",
                    "label": 0
                },
                {
                    "sent": "So, so property elicitation is a way of thinking about the minimizer of the loss.",
                    "label": 1
                },
                {
                    "sent": "And we say that allosso lisitsa property if this equality holds, that property is the minimizer of the expected loss for all distributions P.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so there are many well known examples.",
                    "label": 0
                },
                {
                    "sent": "Obviously the squared loss.",
                    "label": 0
                },
                {
                    "sent": "So this is the mean absolute loss lists the median an you can only say quantiles and expect tiles and much more exotic things.",
                    "label": 0
                },
                {
                    "sent": "And in general there are two very fundamental questions about empirical risk minimization that you can express in terms of properties.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are which properties are listenable, so if you dream up some statistic, can you come up with a loss where that statistic will be the minimizer?",
                    "label": 1
                },
                {
                    "sent": "And what what?",
                    "label": 0
                },
                {
                    "sent": "Surrogate losses are consistent with with a particular property?",
                    "label": 0
                },
                {
                    "sent": "And to summarize the previous work, basically the real valued case and the expected value case are well understood, so the real valued case.",
                    "label": 0
                },
                {
                    "sent": "This is where the property in question is can be expressed in a single real number, and then the illicit aghbal properties are those for which the level sets of the property a convex maximal, meaning that the set of distributions that share the same value.",
                    "label": 0
                },
                {
                    "sent": "Of the property can be written as an affine set intersected with the set of distributions that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "For the expected value case, this is where the property is some mean of some potentially vector valued random variable.",
                    "label": 0
                },
                {
                    "sent": "Here these are always a listable and the set of surrogate losses are the Bregman divergences, so these are the two sort of well known cases and in general other vector valued cases are pretty much wide open.",
                    "label": 0
                },
                {
                    "sent": "So what we do is.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We take some initial steps in the vector valued case.",
                    "label": 0
                },
                {
                    "sent": "We first unify the results in the expected value case by removing differentiability assumptions.",
                    "label": 0
                },
                {
                    "sent": "We then look at separable losses.",
                    "label": 0
                },
                {
                    "sent": "In particular ask so loss is separable if you can just decompose it coordinate wise across your across the dimensions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It turns out there are some cases where this is the only thing that you can do, and this is sort of relevant for various things in practice.",
                    "label": 0
                },
                {
                    "sent": "We also",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at the construction of identification functions, which are relevant to this literature, which you can think of as derivatives of losses.",
                    "label": 1
                },
                {
                    "sent": "And finally, we show that this condition of convex maximality of the level sets is not actually sufficient in the general case.",
                    "label": 0
                },
                {
                    "sent": "And in particular, that last one suggests that the full characterization is actually out of reach at the moment, and so there are many interesting questions which I think are fundamental to machine learning.",
                    "label": 0
                },
                {
                    "sent": "So you should come by the poster.",
                    "label": 0
                }
            ]
        }
    }
}