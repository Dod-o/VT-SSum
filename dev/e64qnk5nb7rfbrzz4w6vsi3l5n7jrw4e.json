{
    "id": "e64qnk5nb7rfbrzz4w6vsi3l5n7jrw4e",
    "title": "Correlation Clustering: From Theory to Practice",
    "info": {
        "author": [
            "Edo Liberty, Yahoo! Research",
            "David Garcia-Soriano, Yahoo! Research Barcelona",
            "Francesco Bonchi, Yahoo! Research Barcelona"
        ],
        "published": "Oct. 15, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_bonchi_garcia_soriano_liberty_clustering/",
    "segmentation": [
        [
            "First of all, welcome.",
            "Nice to see you.",
            "Making it back after lunch.",
            "For the jet legs, just like people, we appreciate you staying up.",
            "So we're going to talk today about correlation clustering.",
            "This talk will be given in three parts."
        ],
        [
            "I will start with an introduction of the fundamental results and some just a glimpse into the theory and the algorithms, then Francesco Bonchi, my colleague from Barcelona will talk about different variants of this problem and some applications and then.",
            "David Garcia also from Barcelona, also from Yahoo Labs, will talk about issues of scalability.",
            "Running these algorithms in.",
            "Real life in large scale and things they will encounter inside Yahoo."
        ],
        [
            "So.",
            "Without further ado, I want to kind of jump in and start.",
            "The discussion about correlation clustering."
        ],
        [
            "So before I before I really describe the problem, I want to take a step back and think about what clustering is and kind of clear up some of the confusion that I see in general in the tension between practitioners and theoreticians.",
            "OK, so as a practitioner.",
            "End of the theoretician.",
            "Clustering is really the art of taking a collection of item items and putting putting them in groups.",
            "In some sense where the similar things are put together in the dissimilar.",
            "So things are set apart.",
            "OK, and the notion of similarity and the kind of groups that you allow and all these things go into the types of objects that you have and what you can do with them.",
            "So the way I see it as a theoretician is that you have the setting.",
            "What objects do I have from that?",
            "You derive an objective function saying how do I score a given partitioning, and then you define an algorithm on top of that.",
            "So let me give you an example and why I think there is some confusion.",
            "But I'll try to keep this setting so."
        ],
        [
            "The most well known setting is the Euclidean setting.",
            "OK, well we have a bunch of spoiled points.",
            "OK so I my data points are represented by vectors and the similarity is represented by the distance between them.",
            "So small distance means similar.",
            "I."
        ],
        [
            "And think about groups of those items as the clusters and I."
        ],
        [
            "And endow each of these clusters by cluster center.",
            "And then I can measure my Michael.",
            "So this is the setting.",
            "OK, so my setting is vectors in space and vector and groups and centers."
        ],
        [
            "And now we can define an objective, and the objective is for example, the K means objective very well known.",
            "Just sum up the square the square distances between each point as a, send the corresponding center of the cluster."
        ],
        [
            "OK, if you remove the square you get the K means objective, so you just sum up the distances.",
            "That turns out to be a different computational problem.",
            "It sounds it looks very similar, but actually it turns out to be.",
            "Quite different algorithms wise."
        ],
        [
            "You can also look at the case centers problem.",
            "Essentially I don't care about the same things, I just don't want to have.",
            "I want them.",
            "I'm going to minimize the farthest point away from its center, so in some sense, like a worst case situation.",
            "So you want to put centers.",
            "That way, OK, so these are three different objectives for the same setting and mind you, I'm not talking at all about the algorithms, so you can talk about Lloyd's algorithm and so on.",
            "But you know those not talking at all about the algorithm.",
            "This is just the objectives."
        ],
        [
            "OK, so now I want to break away from the Euclidean setting or the metric setting and talk about the graph setting where my data is represented by a graph.",
            "Each, each node is corresponds to an item in my data or some object that I need to cluster, and I have similarities which are edges in this graph.",
            "OK, now I can represent metric data in this way by saying every every two points with distance less than some threshold are have an edge and otherwise note or have some kind of smooth interpolation of these things?",
            "So I can represent many kinds of things as graphs, but in the end the algorithm sees a graph OK."
        ],
        [
            "And I can think about clustering as sorry, so these are edges.",
            "I would represent edges of a set of objects as all the sum of all the weights of the edges inside.",
            "If the runway is just the number, if their weight, it's the summer."
        ],
        [
            "Wait?",
            "And the edges of SS complement, as is the saddest compliment, is all the nodes except for S. And I would.",
            "I would denote by SE of SS compliment just the edges going across.",
            "OK so.",
            "So, so in general, you would think about about clustering of graphs, trying to somehow make the edges inside clusters heavy and edges going across, you know, as light as possible.",
            "OK, that's the that's the idea."
        ],
        [
            "But you can formalize them in different ways, and let's kind of look at a few different well known graph clustering objectives.",
            "I can just try to take.",
            "I can minimize, I can try to split into two groups and minimize.",
            "Minimize the edges going across between the clusters and then divide by the sizes of the clusters.",
            "OK, why is this necessary?",
            "Because if I don't do that, I might carve out just one node with them.",
            "You know, with very local dinner, very few neighbors, and then I split the graph into right.",
            "But then the denominator would be small and I want to make it big, so minimizing this objective function received.",
            "I know how many 10s of publications in the in the computer science theory community.",
            "I mean it's it's.",
            "Computationally hard to actually solve this problem, but there are approximations."
        ],
        [
            "You can look at the expansion sets or find a set of cardinality.",
            "Smaller than.",
            "1/2 of the nodes the minimizes the sum of edges going outside of it.",
            "OK, again, dividing by the size of the set so you don't want to take small sets you want take sets.",
            "You can."
        ],
        [
            "Look at the graph conductance and the reason is called conductances, because it's highly related to electronic circuitry.",
            "So you can look at the edges going across divided by the sum of edges inside the click inside the clusters again.",
            "You have to make sure it's less than half 'cause you can take everything."
        ],
        [
            "But you can try to think about things that have to do with.",
            "You can try to break graphs into K different things.",
            "So in K median case centers we know you know just splitting into two is not enough.",
            "Sometimes want K clusters you know, so you can try to break at graphing K different parts and you can say OK, one reasonable object objective is create K components roughly equally sized and minimize the edges going across.",
            "That's the K balanced partition problem."
        ],
        [
            "Um?",
            "You can generalize the the the minimal conductance problem by minimizing the Max over your parts over the edges going from the part out of a divided by the weight of the.",
            "Of the cluster and so on."
        ],
        [
            "And I wanted to give you this kind of.",
            "Introduction just to show you that you could have.",
            "You can split things into or in K for graphs or for metric spaces or for Euclidean space, which is usually the easiest, but we here we embarked we slightly.",
            "Switch gears and we switch to the objective function of correlation clustering.",
            "Finally OK, so we have a graph OK and we want to break it into a collection of clicks."
        ],
        [
            "Which are the clusters such that I just air the least?",
            "With respect to the original graph, in some sense.",
            "I have my original graph and I have my partitioning it of into graphs.",
            "I will pay one so the one on the left you see the redundant edge.",
            "So there is an edge going between the clusters or extraneous edge.",
            "It's not really redundant, it's just I mean I disagree on that edge with the graph, so I would claim that it shouldn't be there.",
            "And on the on the right upper corner there's a.",
            "There's an edge missing between two nodes that I put together in the cluster.",
            "OK, so my objective is just to find any partitioning and I'm just going to pay for every edge I get wrong.",
            "OK."
        ],
        [
            "So we can formalize this in several ways.",
            "We can look at the minimal disagreement problem so.",
            "Um?",
            "So at the top I would I would write CIJ as my clustering function so it's one if I&J are put together in my in the same cluster and 0 otherwise in my output and EJ is 1 if there is an edge between I&J in the original graph and 0 otherwise.",
            "And you can think about fractional edges, so without something could be an edge with some like you know.",
            "Similarity between zero and one, but even let's think about just a simple graph.",
            "Edges are either there or not.",
            "Wait one so you can minimize the disagreement.",
            "So CIJ times 1 -- Y IJ is only one if one is zero and the other one is 1 right?",
            "And the bottom one just gives you a flip case.",
            "OK, so if you minimize the sum of RIJ over these things.",
            "It's essentially how many times do I disagree with the original graph?",
            "OK, and you try to minimize the disagreement.",
            "You can also try to maximize the number of agreements saying which is the obvious corollary to that.",
            "And you can think about the weighted case where some edges I really don't want to be wrong on right?",
            "So some things I just I guess I they should be connected, but some things that I'm almost positive so I give them a highway.",
            "The minimal agreement at the minimal disagree, and the maximal agree.",
            "Should a an optimal solution so cheerfully you could look at the problem and say, oh up to some constant factor the same.",
            "OK, not multiplicative, so there is some additive factor the same, so they should be sold by the same thing, but the so that would be a true and correct observation.",
            "But the value of the objective is different.",
            "OK, so a 10% error in one is not a 10% error in the other.",
            "So if you try to minimize one problem, if you look at them in disagreement problem from a theory.",
            "Standpoint it would be different than the Max agree."
        ],
        [
            "So for the rest of this talk, I just want to focus on the unweighted case so we have no edge weights and the Cardinal the similarities are integer, so there's 01 things are either similar or dissimilar?",
            "OK, we have variants that Francesca is going to talk about later.",
            "But this already encapsulates most of the difficult instances and somehow is already very difficult to solve.",
            "Alright, go on.",
            "Ejs one if in the original graph you have an edge between I&J and 0 otherwise.",
            "OK, and CIJ is just whether there's an I put them together in the graph, put them together in the cluster or not.",
            "If you want.",
            "If you want to.",
            "Think about this differently.",
            "If I take if I create a collection of clicks OK, I just paid the symmetric difference between the two graphs.",
            "So distance in between the two graphs.",
            "Questions up until now, and.",
            "OK, there's a setting is is clear."
        ],
        [
            "So I want to point out something very important about correlation clustering that's going to come back later.",
            "There is no limitation on the number of clusters, and there is no limitation on their sizes.",
            "OK, so it might very well be that the best solution is to just put everything together in one giant cluster.",
            "There's not clustering at all.",
            "I just put just slumped everything together.",
            "OK, oh, I might have just split everything to Singleton so everything every single thing should be like alone in a cluster that's not also also not very interesting, but there might be the optimal solution to this problem, OK?",
            "I want to convince you that this is still interesting and somehow a big part of the reason why we chose to do this tutorial is to do.",
            "Point out that in many cases, in many situations this is the most natural notion of clustering."
        ],
        [
            "So here's an example.",
            "If your inbox looks anything like mine.",
            "You'd have emails like the one at the bottom right that is just emails that I send to you and you reply back.",
            "Those are very unique, those are just the one offs right though, so there are part of no cluster.",
            "I might have some.",
            "You know promotional email from some forum that I'm registered to and I might get 20 emails from Dan today or one a day or one a week, but I keep getting those and I have hundreds or thousands of those.",
            "OK and."
        ],
        [
            "The emails that I get from these companies, all the emails that I get from these forums or receipts of itineraries or whatnot are not identical.",
            "So if I look at 2 emails that I have two things that I bought on Amazon, the emails or slightly different.",
            "I mean maybe the date or the shipping address or the thing that I actually bought is going to be different so I can't just lump them together OK."
        ],
        [
            "So now I I.",
            "So now I need to if I wanted to cluster them together.",
            "If I just want to have a rule all of the emails from this forum just put them in this bin.",
            "OK, I need to cluster them.",
            "Now you might say OK if they are so similar I can just do some connected component thing.",
            "So just find everything that's similar, just carve it out as a connected component and put it there, but it's not, so that's not so clear, right?",
            "So sometimes these similarities are non transitive.",
            "OK, and that's where free emails might have this similarity graph.",
            "OK, or if you look at the if you think about generating graphs on data, OK, we said you might take for example a truncated distance says everything of distance less than one has an edge.",
            "Everything this is more than one does not have an edge.",
            "OK, it might be a very large connected component.",
            "OK, there's no.",
            "You can just cut this graph the trivial way."
        ],
        [
            "So this is.",
            "And again, if you like me, unfortunately our life is somehow we get pinged from this hundreds or thousands of different vendors or organizations or individuals.",
            "So the number of clusters is huge.",
            "I can't decide what K is to begin with.",
            "I just want to say, you know, just choose the I mean, I just want the algorithm to cluster things that should be together and not class of things that should be together.",
            "And there might be thousands or 10s of thousands of different clusters."
        ],
        [
            "OK. One more motivation is comes from machine learning, and it actually is one of the original motivations behind correlation clustering.",
            "It appears already in the 1st paper that describes correlation.",
            "Clustering is, say you build a classifier that knows how to tell breeds of dogs apart.",
            "OK.",
            "I don't know why.",
            "So this I mean, I know the answer here because I looked it up.",
            "So I mean I wouldn't know otherwise.",
            "So the four ones on the right are Irish setters and the three on the left are Golden settles that are two types of settles.",
            "But different enough to be considered to different breeds.",
            "So if you want to cluster this, if you want to build a collection of Irish setters and you don't know how to do it, you might just decide.",
            "OK, I'm just going to build a classifier that knows whether two kinds of pictures of two different dogs are the same breed or different.",
            "That's it.",
            "I don't know anything else.",
            "I don't know anything about dogs, I just want to say this similar or different OK?"
        ],
        [
            "Then the result of this classifier could be your graph.",
            "And then you could cluster the result of the classifier OK. And if you look at it from a geometric point.",
            "Formed in a geometric wave, the input graph is an object.",
            "OK, so it has all these like classifications.",
            "The there is some unknown ground truth that you're trusting.",
            "Classifier is trying to get at and there is the output of the clustering algorithm and just by the triangle inequality, if you minimize the disagreements between.",
            "The output of your clustering and the input.",
            "You also minimize the disagreements between the output and the ground truth, which is what you would write like to do from the machine learning standpoint so.",
            "In other words, if your classifier is only good, your result clustering is as good.",
            "And sum up to some.",
            "Small photo."
        ],
        [
            "OK, so up until now we defined the problem.",
            "And we motivate it and hopefully I can.",
            "I convince you that correlation clustering is A is a good thing to try to solve.",
            "Unfortunately, it's unsolvable.",
            "OK, so even again in the 1st paper that defined the problem it was already.",
            "Prove that it's it's hard, it was later proved to be approximation hard, so there is some small constant.",
            "I think it was, it's.",
            "I forget the number.",
            "I think it's I think it's one more than 1.5, but I don't want to.",
            "You can definitely not solve it up to approximation constant apps epsilon so or to arbitrary precision.",
            "OK, so there is some fixed constant.",
            "Which getting them anymore errors than the optimal solution is NP hard.",
            "OK, so in some sense we're hosed, right?",
            "OK, hope."
        ],
        [
            "Theoretically, we can do anything but the answer, but of course we can do something.",
            "So I will define an algorithm to be a C approximation to this problem.",
            "If the number of mistakes returned by the clustering of the algorithm is at most C times the best possible in the best possible answer.",
            "OK, so Al would be the cost of the algorithm or the number of mistakes of the.",
            "Made by the resulting by the Returned Clustering and OPT is the best you could do.",
            "OK.",
            "So the first algorithm by Bansal, Blum and Caola in oh 902 already proposed that 20,000 factor approximation.",
            "This problem might not be very interesting, but it was.",
            "It was it was a conceptual breakthrough and they define the problem and this algorithm is actually not very efficient but doable OK?",
            "So all of N squared and N is the number of nodes in your graph.",
            "Later to align the works that actually happened simultaneously by Divine Emmanuelle Fiat anymore, like I know Six and Charlie Congresswoman worth in 03, both suggested solutions based on linear programming rounding which we will see in a second, which gave better performance.",
            "So this like full again approximation is.",
            "Arguably better or worse than 20,000 if you ask a theoretician, it's worse if you ask any.",
            "Practitioner, that's like way better.",
            "Unless N is like more than the atoms, the number of atoms in the universe, then you know, then probably for Logan is better.",
            "And of course, this was improved into a constant factor for then in 05 a work by a lone Charikar Newman.",
            "Launch Icon human Atlanta.",
            "Right, is it in my?",
            "Launch Oracle would end his Newman, right?",
            "Good OK, I just want to make sure.",
            "Gave it 2.5 approximation ratio which I will show you again also in a bit.",
            "And then we'll talk about their solution, which is much more efficient.",
            "Slightly after that, so the rest of the talk I'm going to focus on the algorithms that achieve these coefficients achieve these approximations.",
            "The idea is really to present the algorithms and some of the intuition behind them.",
            "I'm not going to show the proofs.",
            "You might be disappointed or happy.",
            "I mean, it depends on your if you want to be happy to go in as you know, to jump down the rabbit hole.",
            "But then.",
            "When people start falling a falling asleep, I will have to come out OK, so let's start."
        ],
        [
            "Let's start with a warm up exercise.",
            "This was given as a kind of also as a warm up exercise in Bunzl, Blumen Chow line O2 and they notice the very simple following."
        ],
        [
            "OK, so let's say I have a graph."
        ],
        [
            "And let's say I only care about correlation clustering that returns only two clusters.",
            "This is an exercise is not a general solution, but they assume that the best solution for correlation clustering returns two clusters OK. Can I even do something in that case?",
            "OK, and it turns out, yes, you can do something extremely trivial in that case, OK?"
        ],
        [
            "So I'm going to look at a node."
        ],
        [
            "In the graph and its neighborhood neighborhood, it's everybody that's connected to and itself including and.",
            "I'm going to only consider clustering of the form.",
            "The neighborhood of the node and everything else.",
            "OK. OK, that sounds very would umentary, but surprisingly enough it can only already do something.",
            "OK, so if you look at so, let's define the number of nodes that they.",
            "That unable disagrees with the best clustering is essentially the set into the set difference between the two.",
            "OK.",
            "So here this node circled in red is in the neighborhood of V, but is not in the same cluster as V in the in the optimal solution.",
            "Now I clearly I don't know the optimal solution to begin with, but it exists nevertheless."
        ],
        [
            "OK, so let's call this number D. The number of disagreements.",
            "So I want to say that the optimal solution must air on at least 10 * D / 2 edges.",
            "Why's that so the so D is the so the number of disagreements over the node that has the minimum number of mistakes is D. OK, so every node has at least D neighbors that have edges going across the optimal solution.",
            "OK, so that those are edges I pay for OK, but I have nodes on both sides, so I kind of counted twice.",
            "OK, so each one of them contributes D / 2 errors.",
            "OK, so the optimal solution needs to be at least 10 * D / 2."
        ],
        [
            "OK, but what does the algorithm do?",
            "OK, the algorithm takes the entire neighborhood of this node, so it takes those D nodes that were.",
            "It takes the nodes that did not include.",
            "They were not included in the original graph or they or does not take once they were included and puts them in the cluster.",
            "Those nodes are connected to at most N different things.",
            "I mean, N is the entire size of the graph, so the number of errors.",
            "That algorithm seizes whatever the best is, plus N * D. So the D errors each one of them contributes at most.",
            "Sorry, the errors is one of them contributed most in more errors, so it's 10 times the."
        ],
        [
            "If you just put the two together, the top is at least N D / 2 and the algorithm is.",
            "Uh.",
            "At most optimal, I sometimes do you get that algae is smaller than three opt OK so this is like very simple in some sense.",
            "You disappointing almost right.",
            "But remember that.",
            "This problem is hard.",
            "OK, so even this is something OK. Later in a show in a paper that I'm not showing, it was shown that for the case of two centers and actually any fixed number of clusters, you could get Iapetus, but I'm not going to talk about that in this talk."
        ],
        [
            "So.",
            "In the remainder of the talk, I want to kind of show LP based solution so.",
            "I'm going to so the first 2 works happens to me.",
            "Tanias Lee and I reference them in the beginning and then the loan Charikar.",
            "Newman, I'm missing somebody here.",
            "So why do I have two letters?",
            "It was just a typo.",
            "OK. OK.",
            "Anyway, so the so the last paper also is LP based and let's let's see what I'm."
        ],
        [
            "OK, so let's go back to the let's go back to the definition of correlation clustering.",
            "We had this sum over disagreements and agreements in the graph.",
            "I can think about it as this way let me just define distances between pairs I&J OK. And these distances I'm going to restrict to be either zero or one OK, and I'm going to pay.",
            "One for every edge in the graph for which I put the two nodes in distance one, and I'm going to pay one for every time two nodes, or don't have an edge between them, but I put them in distance 0.",
            "OK, so this is exactly equal to the definition that I had before.",
            "OK, and notice that for distances, if I didn't have the last condition, then I could just sit all the everything that has an edge to distance zero and everything doesn't have an edge distance one now would be done OK, but distances abide in the triangle inequality, which means that if I have if I if the distance with INK so I and J&J and K0 then the distance between I and K must be 0 as well.",
            "So if you look at if you stare hard at this thing, you'll see that this actually gives you a clustering distances of 01 is A is identical to having connecting components OK, so the immediate thing we can try to do.",
            "Is."
        ],
        [
            "Just relax this problem so we cannot solve the integer problem, but we can solve the linear problem and we can do this by just replacing the constraints on DJ to be in the range 01.",
            "OK, so now there's another Floating Points.",
            "And this becomes a linear problem program.",
            "So the objective is linear.",
            "The constraints are linear in the IJ, and so this is polynomial time solvable.",
            "OK. What is E?",
            "Is the edge set either capitoli?",
            "Yeah, it's it's the edge set.",
            "Yeah, there original input edge set.",
            "OK so everything yeah.",
            "OK, so this gives, so the result of this thing is is good in some sense.",
            "I optimally solve a more general problem, so the objective function is reduced, or at the very least not increased, but the results DJ that I get our fractional so the distances between zero and one.",
            "Now we need to round them."
        ],
        [
            "So a common technique for rounding LP.",
            "Solutions is called rich and growing and let me kind of explain to you how that goes.",
            "You start with some setting where you have real distances now, so now you have embedded your points into this space where you really have.",
            "Distances.",
            "I mean, I put it here into the.",
            "Of course it's not today, it's not even Euclidean.",
            "Usually it's just a metric, OK?"
        ],
        [
            "You pick an arbitrary point.",
            "It doesn't matter which one.",
            "OK."
        ],
        [
            "You pick an arbitrary point and you start growing a ball around it.",
            "What does that mean?",
            "Just take all the points that lie with distance less than the radius into it."
        ],
        [
            "And you keep growing it until some condition holds.",
            "OK, what is that condition?",
            "I'm not going to go into that for a second because that depending on which publication might be slightly no tacious but.",
            "It's a condition that's easy to check.",
            "OK, you grow the ball.",
            "You reach some condition when you do, you stop.",
            "You take all the points in that cluster.",
            "You take all the points inside the ball and you make the intercluster you."
        ],
        [
            "Use more points you keep growing balls until you until you exhaust the entire graph, and then you're done.",
            "OK."
        ],
        [
            "So the.",
            "There there the good news about it is that.",
            "Is it actually probably does something interesting, so this is the terministic algorithm.",
            "It gives a log and approximation for any weighted graph or functional edges.",
            "OK, login is over.",
            "Logan is pretty good.",
            "It gives a full approximation if you have the unweighted complete graph setting which we talked about which they were trying to solve an in the four approximation.",
            "You actually have a very simple ago that we don't even have to grow the graphs.",
            "You don't even have to grow those balls.",
            "You can just take balls of radius 1/2.",
            "And kind of check whether you know the average distance inside your cluster is like more than 1/4 more than 1/4.",
            "If it is you, you don't take this cluster and you make the node into a single tone.",
            "If it's less, you take it, and that's it.",
            "So it's very simple, very efficient."
        ],
        [
            "So this could be so the rounding technique could be improved.",
            "You want to.",
            "I want to present the pivot algorithm presented by a launcher.",
            "Karen Newman."
        ],
        [
            "So again, we start with the result of the LP, so I have.",
            "I have solved this problem.",
            "I have pairwise distances and now I need to run them.",
            "So the way I do it, I choose one."
        ],
        [
            "Old, uniformly random.",
            "I mean I chose the same one as before.",
            "Just you know, for the presentation sake, but I this is not arbitrary now.",
            "Now it's a uniform choice over the items.",
            "It's important that's the case."
        ],
        [
            "Then for every other node in the graph, or for every other item I added to this cluster with probability 1 minus DJ.",
            "OK.",
            "So if the distance is 1, the probability is zero and the distance is 0, the probability is one.",
            "That's a good sanity check, but for everything in the middle it's kind of.",
            "It's a coin toss.",
            "OK.",
            "So you started with.",
            "You started with the original graph.",
            "You solve this let me skip back.",
            "OK, so you solve this linear program used by Jr variables.",
            "OK, you minimize over all possible instantiation DRJ.",
            "Or you know this objective OK?",
            "Where the distance is if you try to kind of squinted it, this is a choice to be as close as possible to 04 edges that for edges in the graph is close to possible to one for non edges.",
            "How close can I get to the 01 graph while still being a metric?",
            "OK, so the result of that are variables DJ that you have now."
        ],
        [
            "OK, and that's the input to the rounding.",
            "OK, so that's the input of the rounding.",
            "Now what do you do with it?",
            "You said you choose one note at random.",
            "Then you do this like pairwise thing.",
            "You either lump them together or disconnect them.",
            "And then you cut away the cluster you get and repeat.",
            "So whatever is left, you choose a random and you repeat until you exhaust the entire graph.",
            "And amazingly enough."
        ],
        [
            "This is probably better.",
            "OK, so this gives a 2.5 approximation.",
            "That's the best known approximation ratio for the unweighted case of correlation clustering.",
            "And as a side note, I'm not going to talk about that, but if you're.",
            "If your edges are fractional, this actually might be better, so 2.5 is the worst case, and it turns out that the unweighted 01 edges are the worst case that these are the hardest case.",
            "If you have fractional things that actually makes a fairly bitter.",
            "And the most.",
            "The worst news around about this algorithm is that you have to solve this ginormous LP, and I'm assuming we have many practitioners in this room.",
            "Think about your own graphs.",
            "You think OK, N is like.",
            "Well, let's let's say more than 100.",
            "OK, probably in the many thousands or millions, but at the very least, let's say 1000 OK?",
            "The linear program that we have to solve needs to make sure that all three nodes, every triplet, actually the triangle inequality holds for it.",
            "So that's N ^3 / N ^3 constraints.",
            "That's horrendous.",
            "OK, that's a billion constraints.",
            "I don't know what LP solvers you use, but that's not acceptable in most most places, OK?"
        ],
        [
            "OK, so now we're back to the drawing table, so we have almost we have the best.",
            "We've explained.",
            "The best known approximation ratio.",
            "But it's unusable so.",
            "I want to present the vote also known as quick cluster.",
            "And it was also showed in a launch Areekara Newman.",
            "What happens if you skip the LP?",
            "Essentially run pivot like we said before on the original graph."
        ],
        [
            "What do you get?",
            "So let me just show how that works."
        ],
        [
            "You again, you pick a unit."
        ],
        [
            "Full node, you take all its neighbors.",
            "You put them in a graph.",
            "You put them in a cluster.",
            "You cut it away and you."
        ],
        [
            "Rickers OK."
        ],
        [
            "Until you are done with the graph, this is this is very efficient.",
            "This is almost trivial.",
            "OK.",
            "The amazing thing is."
        ],
        [
            "That this already gives a factor 3 approximation.",
            "OK, so.",
            "The so let me kind of give you an intuition of why why this happens OK.",
            "So if you think back to the the toy example that we had from bunzl Bloom caola about choosing the node that minimizes disagreement with the graph with the final clustering, Now we replace it with instead of the one that minimizes disagreement, just replaced with a uniform, just choose one node at random.",
            "The number of mistakes that is going to make is it's number of disagreements with the best clustering times N like we did before, right?",
            "But if you think about it, the optimal doesn't pay the optimal pays.",
            "The expected number of disagreements with every node or the sum of them actually divided by two, so the same the same argument that we had before about the node minimizing the disagreement is actually true about a uniformly uniformly chosen random node with expectation.",
            "So now this is a randomized thing, but still the expectation is correct, so you can repeat a few times and just take the best one.",
            "OK, but unfortunately this also only works for unweighted graphs, and now we need to somehow we're not going to do it now, but we I think this is a major next step to do so.",
            "This is quick cluster and the field in his part of the talk is going to say more about that in.",
            "Extend, so let me just.",
            "Recap and give you a few more."
        ],
        [
            "A few more references.",
            "These are the references I talked about, but I want to so this is a bit small, but."
        ],
        [
            "We see in one word what each one of them does, because I think this is a if you you know if you want to go out of this talk and start kind of digging into what kind of variance we have and whatever it's like kind of was standard literature.",
            "Then go to Sen. Google Swami in 0906 asked what happens if you have a fixed number of clusters.",
            "It turns out you have a pee test for that.",
            "OK, so you can do better if you know in advance how many clusters you have.",
            "Clearly that number that running time grows as the number of clusters and so if you.",
            "If you set the number, you can just go all the way from one to N and solve the NP hard problem also.",
            "It's going to be cheating.",
            "Together with near Elan I worked on.",
            "I worked on the problem of so remember this triangle with the machine learning, so I said you have the ground truth, the input and the output and the question is what is the relation between that and turns out that you could derive most of the resources you can get for quick cluster just by the reduction of the machine learning case and it turns out that the distance to the ground truth is actually smaller.",
            "Then the distance to the graph.",
            "So this is surprising.",
            "Von Zullen Williamson actually 2 quick cluster and derandomized it so now if you have if you're unhappy with the fact that quick cluster is randomized algorithm which gives you a good result in expectation but somehow.",
            "Might with post more verbally to give you a bad answer, you can be randomized and guarantee a factor approximation.",
            "Matthew Sanquin Schutte showed that you can actually do online correlation clustering to some extent, and Francesco's going to talk about that.",
            "Oh, wait a second.",
            "Sorry they that's skip one, so Matthew and shooting alone.",
            "I had a very interesting paper about what happens if the mistakes are random.",
            "So I take a graph that is a collection of clicks and now I flip edges with some probability P. That's a much more.",
            "That's potentially a problem with much more conducive of actually finding the solution instead of an arbitrary graph, and indeed they show that's the case.",
            "So even with huge amount of noise, if you flip edges probability almost 1/2, you can actually recover the original graph in with very high probability, or at least clusters that are big enough.",
            "And I'm actually going to skip the last two I think want to leave some time for questions.",
            "That's that for now.",
            "I'll be happy to.",
            "Take questions now."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, welcome.",
                    "label": 0
                },
                {
                    "sent": "Nice to see you.",
                    "label": 0
                },
                {
                    "sent": "Making it back after lunch.",
                    "label": 0
                },
                {
                    "sent": "For the jet legs, just like people, we appreciate you staying up.",
                    "label": 0
                },
                {
                    "sent": "So we're going to talk today about correlation clustering.",
                    "label": 1
                },
                {
                    "sent": "This talk will be given in three parts.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will start with an introduction of the fundamental results and some just a glimpse into the theory and the algorithms, then Francesco Bonchi, my colleague from Barcelona will talk about different variants of this problem and some applications and then.",
                    "label": 0
                },
                {
                    "sent": "David Garcia also from Barcelona, also from Yahoo Labs, will talk about issues of scalability.",
                    "label": 0
                },
                {
                    "sent": "Running these algorithms in.",
                    "label": 0
                },
                {
                    "sent": "Real life in large scale and things they will encounter inside Yahoo.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Without further ado, I want to kind of jump in and start.",
                    "label": 0
                },
                {
                    "sent": "The discussion about correlation clustering.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before I before I really describe the problem, I want to take a step back and think about what clustering is and kind of clear up some of the confusion that I see in general in the tension between practitioners and theoreticians.",
                    "label": 0
                },
                {
                    "sent": "OK, so as a practitioner.",
                    "label": 0
                },
                {
                    "sent": "End of the theoretician.",
                    "label": 0
                },
                {
                    "sent": "Clustering is really the art of taking a collection of item items and putting putting them in groups.",
                    "label": 0
                },
                {
                    "sent": "In some sense where the similar things are put together in the dissimilar.",
                    "label": 0
                },
                {
                    "sent": "So things are set apart.",
                    "label": 1
                },
                {
                    "sent": "OK, and the notion of similarity and the kind of groups that you allow and all these things go into the types of objects that you have and what you can do with them.",
                    "label": 0
                },
                {
                    "sent": "So the way I see it as a theoretician is that you have the setting.",
                    "label": 0
                },
                {
                    "sent": "What objects do I have from that?",
                    "label": 0
                },
                {
                    "sent": "You derive an objective function saying how do I score a given partitioning, and then you define an algorithm on top of that.",
                    "label": 0
                },
                {
                    "sent": "So let me give you an example and why I think there is some confusion.",
                    "label": 0
                },
                {
                    "sent": "But I'll try to keep this setting so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The most well known setting is the Euclidean setting.",
                    "label": 0
                },
                {
                    "sent": "OK, well we have a bunch of spoiled points.",
                    "label": 0
                },
                {
                    "sent": "OK so I my data points are represented by vectors and the similarity is represented by the distance between them.",
                    "label": 0
                },
                {
                    "sent": "So small distance means similar.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And think about groups of those items as the clusters and I.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And endow each of these clusters by cluster center.",
                    "label": 1
                },
                {
                    "sent": "And then I can measure my Michael.",
                    "label": 0
                },
                {
                    "sent": "So this is the setting.",
                    "label": 0
                },
                {
                    "sent": "OK, so my setting is vectors in space and vector and groups and centers.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we can define an objective, and the objective is for example, the K means objective very well known.",
                    "label": 0
                },
                {
                    "sent": "Just sum up the square the square distances between each point as a, send the corresponding center of the cluster.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, if you remove the square you get the K means objective, so you just sum up the distances.",
                    "label": 0
                },
                {
                    "sent": "That turns out to be a different computational problem.",
                    "label": 0
                },
                {
                    "sent": "It sounds it looks very similar, but actually it turns out to be.",
                    "label": 0
                },
                {
                    "sent": "Quite different algorithms wise.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also look at the case centers problem.",
                    "label": 0
                },
                {
                    "sent": "Essentially I don't care about the same things, I just don't want to have.",
                    "label": 0
                },
                {
                    "sent": "I want them.",
                    "label": 0
                },
                {
                    "sent": "I'm going to minimize the farthest point away from its center, so in some sense, like a worst case situation.",
                    "label": 0
                },
                {
                    "sent": "So you want to put centers.",
                    "label": 0
                },
                {
                    "sent": "That way, OK, so these are three different objectives for the same setting and mind you, I'm not talking at all about the algorithms, so you can talk about Lloyd's algorithm and so on.",
                    "label": 0
                },
                {
                    "sent": "But you know those not talking at all about the algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is just the objectives.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I want to break away from the Euclidean setting or the metric setting and talk about the graph setting where my data is represented by a graph.",
                    "label": 1
                },
                {
                    "sent": "Each, each node is corresponds to an item in my data or some object that I need to cluster, and I have similarities which are edges in this graph.",
                    "label": 0
                },
                {
                    "sent": "OK, now I can represent metric data in this way by saying every every two points with distance less than some threshold are have an edge and otherwise note or have some kind of smooth interpolation of these things?",
                    "label": 0
                },
                {
                    "sent": "So I can represent many kinds of things as graphs, but in the end the algorithm sees a graph OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I can think about clustering as sorry, so these are edges.",
                    "label": 0
                },
                {
                    "sent": "I would represent edges of a set of objects as all the sum of all the weights of the edges inside.",
                    "label": 0
                },
                {
                    "sent": "If the runway is just the number, if their weight, it's the summer.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "And the edges of SS complement, as is the saddest compliment, is all the nodes except for S. And I would.",
                    "label": 0
                },
                {
                    "sent": "I would denote by SE of SS compliment just the edges going across.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "So, so in general, you would think about about clustering of graphs, trying to somehow make the edges inside clusters heavy and edges going across, you know, as light as possible.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the that's the idea.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you can formalize them in different ways, and let's kind of look at a few different well known graph clustering objectives.",
                    "label": 0
                },
                {
                    "sent": "I can just try to take.",
                    "label": 0
                },
                {
                    "sent": "I can minimize, I can try to split into two groups and minimize.",
                    "label": 0
                },
                {
                    "sent": "Minimize the edges going across between the clusters and then divide by the sizes of the clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, why is this necessary?",
                    "label": 0
                },
                {
                    "sent": "Because if I don't do that, I might carve out just one node with them.",
                    "label": 0
                },
                {
                    "sent": "You know, with very local dinner, very few neighbors, and then I split the graph into right.",
                    "label": 0
                },
                {
                    "sent": "But then the denominator would be small and I want to make it big, so minimizing this objective function received.",
                    "label": 0
                },
                {
                    "sent": "I know how many 10s of publications in the in the computer science theory community.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it's.",
                    "label": 0
                },
                {
                    "sent": "Computationally hard to actually solve this problem, but there are approximations.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can look at the expansion sets or find a set of cardinality.",
                    "label": 0
                },
                {
                    "sent": "Smaller than.",
                    "label": 0
                },
                {
                    "sent": "1/2 of the nodes the minimizes the sum of edges going outside of it.",
                    "label": 0
                },
                {
                    "sent": "OK, again, dividing by the size of the set so you don't want to take small sets you want take sets.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at the graph conductance and the reason is called conductances, because it's highly related to electronic circuitry.",
                    "label": 1
                },
                {
                    "sent": "So you can look at the edges going across divided by the sum of edges inside the click inside the clusters again.",
                    "label": 0
                },
                {
                    "sent": "You have to make sure it's less than half 'cause you can take everything.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you can try to think about things that have to do with.",
                    "label": 0
                },
                {
                    "sent": "You can try to break graphs into K different things.",
                    "label": 0
                },
                {
                    "sent": "So in K median case centers we know you know just splitting into two is not enough.",
                    "label": 0
                },
                {
                    "sent": "Sometimes want K clusters you know, so you can try to break at graphing K different parts and you can say OK, one reasonable object objective is create K components roughly equally sized and minimize the edges going across.",
                    "label": 0
                },
                {
                    "sent": "That's the K balanced partition problem.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You can generalize the the the minimal conductance problem by minimizing the Max over your parts over the edges going from the part out of a divided by the weight of the.",
                    "label": 0
                },
                {
                    "sent": "Of the cluster and so on.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I wanted to give you this kind of.",
                    "label": 0
                },
                {
                    "sent": "Introduction just to show you that you could have.",
                    "label": 0
                },
                {
                    "sent": "You can split things into or in K for graphs or for metric spaces or for Euclidean space, which is usually the easiest, but we here we embarked we slightly.",
                    "label": 0
                },
                {
                    "sent": "Switch gears and we switch to the objective function of correlation clustering.",
                    "label": 1
                },
                {
                    "sent": "Finally OK, so we have a graph OK and we want to break it into a collection of clicks.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which are the clusters such that I just air the least?",
                    "label": 0
                },
                {
                    "sent": "With respect to the original graph, in some sense.",
                    "label": 0
                },
                {
                    "sent": "I have my original graph and I have my partitioning it of into graphs.",
                    "label": 0
                },
                {
                    "sent": "I will pay one so the one on the left you see the redundant edge.",
                    "label": 1
                },
                {
                    "sent": "So there is an edge going between the clusters or extraneous edge.",
                    "label": 0
                },
                {
                    "sent": "It's not really redundant, it's just I mean I disagree on that edge with the graph, so I would claim that it shouldn't be there.",
                    "label": 0
                },
                {
                    "sent": "And on the on the right upper corner there's a.",
                    "label": 0
                },
                {
                    "sent": "There's an edge missing between two nodes that I put together in the cluster.",
                    "label": 1
                },
                {
                    "sent": "OK, so my objective is just to find any partitioning and I'm just going to pay for every edge I get wrong.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can formalize this in several ways.",
                    "label": 0
                },
                {
                    "sent": "We can look at the minimal disagreement problem so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So at the top I would I would write CIJ as my clustering function so it's one if I&J are put together in my in the same cluster and 0 otherwise in my output and EJ is 1 if there is an edge between I&J in the original graph and 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "And you can think about fractional edges, so without something could be an edge with some like you know.",
                    "label": 0
                },
                {
                    "sent": "Similarity between zero and one, but even let's think about just a simple graph.",
                    "label": 0
                },
                {
                    "sent": "Edges are either there or not.",
                    "label": 0
                },
                {
                    "sent": "Wait one so you can minimize the disagreement.",
                    "label": 0
                },
                {
                    "sent": "So CIJ times 1 -- Y IJ is only one if one is zero and the other one is 1 right?",
                    "label": 0
                },
                {
                    "sent": "And the bottom one just gives you a flip case.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you minimize the sum of RIJ over these things.",
                    "label": 0
                },
                {
                    "sent": "It's essentially how many times do I disagree with the original graph?",
                    "label": 0
                },
                {
                    "sent": "OK, and you try to minimize the disagreement.",
                    "label": 0
                },
                {
                    "sent": "You can also try to maximize the number of agreements saying which is the obvious corollary to that.",
                    "label": 0
                },
                {
                    "sent": "And you can think about the weighted case where some edges I really don't want to be wrong on right?",
                    "label": 0
                },
                {
                    "sent": "So some things I just I guess I they should be connected, but some things that I'm almost positive so I give them a highway.",
                    "label": 0
                },
                {
                    "sent": "The minimal agreement at the minimal disagree, and the maximal agree.",
                    "label": 0
                },
                {
                    "sent": "Should a an optimal solution so cheerfully you could look at the problem and say, oh up to some constant factor the same.",
                    "label": 0
                },
                {
                    "sent": "OK, not multiplicative, so there is some additive factor the same, so they should be sold by the same thing, but the so that would be a true and correct observation.",
                    "label": 0
                },
                {
                    "sent": "But the value of the objective is different.",
                    "label": 0
                },
                {
                    "sent": "OK, so a 10% error in one is not a 10% error in the other.",
                    "label": 0
                },
                {
                    "sent": "So if you try to minimize one problem, if you look at them in disagreement problem from a theory.",
                    "label": 0
                },
                {
                    "sent": "Standpoint it would be different than the Max agree.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the rest of this talk, I just want to focus on the unweighted case so we have no edge weights and the Cardinal the similarities are integer, so there's 01 things are either similar or dissimilar?",
                    "label": 0
                },
                {
                    "sent": "OK, we have variants that Francesca is going to talk about later.",
                    "label": 0
                },
                {
                    "sent": "But this already encapsulates most of the difficult instances and somehow is already very difficult to solve.",
                    "label": 0
                },
                {
                    "sent": "Alright, go on.",
                    "label": 0
                },
                {
                    "sent": "Ejs one if in the original graph you have an edge between I&J and 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "OK, and CIJ is just whether there's an I put them together in the graph, put them together in the cluster or not.",
                    "label": 0
                },
                {
                    "sent": "If you want.",
                    "label": 0
                },
                {
                    "sent": "If you want to.",
                    "label": 0
                },
                {
                    "sent": "Think about this differently.",
                    "label": 0
                },
                {
                    "sent": "If I take if I create a collection of clicks OK, I just paid the symmetric difference between the two graphs.",
                    "label": 0
                },
                {
                    "sent": "So distance in between the two graphs.",
                    "label": 0
                },
                {
                    "sent": "Questions up until now, and.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a setting is is clear.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to point out something very important about correlation clustering that's going to come back later.",
                    "label": 0
                },
                {
                    "sent": "There is no limitation on the number of clusters, and there is no limitation on their sizes.",
                    "label": 1
                },
                {
                    "sent": "OK, so it might very well be that the best solution is to just put everything together in one giant cluster.",
                    "label": 0
                },
                {
                    "sent": "There's not clustering at all.",
                    "label": 0
                },
                {
                    "sent": "I just put just slumped everything together.",
                    "label": 0
                },
                {
                    "sent": "OK, oh, I might have just split everything to Singleton so everything every single thing should be like alone in a cluster that's not also also not very interesting, but there might be the optimal solution to this problem, OK?",
                    "label": 0
                },
                {
                    "sent": "I want to convince you that this is still interesting and somehow a big part of the reason why we chose to do this tutorial is to do.",
                    "label": 0
                },
                {
                    "sent": "Point out that in many cases, in many situations this is the most natural notion of clustering.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "If your inbox looks anything like mine.",
                    "label": 0
                },
                {
                    "sent": "You'd have emails like the one at the bottom right that is just emails that I send to you and you reply back.",
                    "label": 0
                },
                {
                    "sent": "Those are very unique, those are just the one offs right though, so there are part of no cluster.",
                    "label": 0
                },
                {
                    "sent": "I might have some.",
                    "label": 0
                },
                {
                    "sent": "You know promotional email from some forum that I'm registered to and I might get 20 emails from Dan today or one a day or one a week, but I keep getting those and I have hundreds or thousands of those.",
                    "label": 0
                },
                {
                    "sent": "OK and.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The emails that I get from these companies, all the emails that I get from these forums or receipts of itineraries or whatnot are not identical.",
                    "label": 1
                },
                {
                    "sent": "So if I look at 2 emails that I have two things that I bought on Amazon, the emails or slightly different.",
                    "label": 0
                },
                {
                    "sent": "I mean maybe the date or the shipping address or the thing that I actually bought is going to be different so I can't just lump them together OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I I.",
                    "label": 0
                },
                {
                    "sent": "So now I need to if I wanted to cluster them together.",
                    "label": 0
                },
                {
                    "sent": "If I just want to have a rule all of the emails from this forum just put them in this bin.",
                    "label": 0
                },
                {
                    "sent": "OK, I need to cluster them.",
                    "label": 0
                },
                {
                    "sent": "Now you might say OK if they are so similar I can just do some connected component thing.",
                    "label": 0
                },
                {
                    "sent": "So just find everything that's similar, just carve it out as a connected component and put it there, but it's not, so that's not so clear, right?",
                    "label": 0
                },
                {
                    "sent": "So sometimes these similarities are non transitive.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's where free emails might have this similarity graph.",
                    "label": 0
                },
                {
                    "sent": "OK, or if you look at the if you think about generating graphs on data, OK, we said you might take for example a truncated distance says everything of distance less than one has an edge.",
                    "label": 0
                },
                {
                    "sent": "Everything this is more than one does not have an edge.",
                    "label": 0
                },
                {
                    "sent": "OK, it might be a very large connected component.",
                    "label": 0
                },
                {
                    "sent": "OK, there's no.",
                    "label": 0
                },
                {
                    "sent": "You can just cut this graph the trivial way.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "And again, if you like me, unfortunately our life is somehow we get pinged from this hundreds or thousands of different vendors or organizations or individuals.",
                    "label": 0
                },
                {
                    "sent": "So the number of clusters is huge.",
                    "label": 0
                },
                {
                    "sent": "I can't decide what K is to begin with.",
                    "label": 0
                },
                {
                    "sent": "I just want to say, you know, just choose the I mean, I just want the algorithm to cluster things that should be together and not class of things that should be together.",
                    "label": 0
                },
                {
                    "sent": "And there might be thousands or 10s of thousands of different clusters.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. One more motivation is comes from machine learning, and it actually is one of the original motivations behind correlation clustering.",
                    "label": 1
                },
                {
                    "sent": "It appears already in the 1st paper that describes correlation.",
                    "label": 0
                },
                {
                    "sent": "Clustering is, say you build a classifier that knows how to tell breeds of dogs apart.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I don't know why.",
                    "label": 0
                },
                {
                    "sent": "So this I mean, I know the answer here because I looked it up.",
                    "label": 0
                },
                {
                    "sent": "So I mean I wouldn't know otherwise.",
                    "label": 0
                },
                {
                    "sent": "So the four ones on the right are Irish setters and the three on the left are Golden settles that are two types of settles.",
                    "label": 0
                },
                {
                    "sent": "But different enough to be considered to different breeds.",
                    "label": 0
                },
                {
                    "sent": "So if you want to cluster this, if you want to build a collection of Irish setters and you don't know how to do it, you might just decide.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm just going to build a classifier that knows whether two kinds of pictures of two different dogs are the same breed or different.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "I don't know anything else.",
                    "label": 0
                },
                {
                    "sent": "I don't know anything about dogs, I just want to say this similar or different OK?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the result of this classifier could be your graph.",
                    "label": 1
                },
                {
                    "sent": "And then you could cluster the result of the classifier OK. And if you look at it from a geometric point.",
                    "label": 1
                },
                {
                    "sent": "Formed in a geometric wave, the input graph is an object.",
                    "label": 0
                },
                {
                    "sent": "OK, so it has all these like classifications.",
                    "label": 0
                },
                {
                    "sent": "The there is some unknown ground truth that you're trusting.",
                    "label": 0
                },
                {
                    "sent": "Classifier is trying to get at and there is the output of the clustering algorithm and just by the triangle inequality, if you minimize the disagreements between.",
                    "label": 1
                },
                {
                    "sent": "The output of your clustering and the input.",
                    "label": 0
                },
                {
                    "sent": "You also minimize the disagreements between the output and the ground truth, which is what you would write like to do from the machine learning standpoint so.",
                    "label": 0
                },
                {
                    "sent": "In other words, if your classifier is only good, your result clustering is as good.",
                    "label": 0
                },
                {
                    "sent": "And sum up to some.",
                    "label": 0
                },
                {
                    "sent": "Small photo.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so up until now we defined the problem.",
                    "label": 0
                },
                {
                    "sent": "And we motivate it and hopefully I can.",
                    "label": 0
                },
                {
                    "sent": "I convince you that correlation clustering is A is a good thing to try to solve.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it's unsolvable.",
                    "label": 0
                },
                {
                    "sent": "OK, so even again in the 1st paper that defined the problem it was already.",
                    "label": 0
                },
                {
                    "sent": "Prove that it's it's hard, it was later proved to be approximation hard, so there is some small constant.",
                    "label": 0
                },
                {
                    "sent": "I think it was, it's.",
                    "label": 0
                },
                {
                    "sent": "I forget the number.",
                    "label": 0
                },
                {
                    "sent": "I think it's I think it's one more than 1.5, but I don't want to.",
                    "label": 0
                },
                {
                    "sent": "You can definitely not solve it up to approximation constant apps epsilon so or to arbitrary precision.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is some fixed constant.",
                    "label": 0
                },
                {
                    "sent": "Which getting them anymore errors than the optimal solution is NP hard.",
                    "label": 0
                },
                {
                    "sent": "OK, so in some sense we're hosed, right?",
                    "label": 0
                },
                {
                    "sent": "OK, hope.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Theoretically, we can do anything but the answer, but of course we can do something.",
                    "label": 0
                },
                {
                    "sent": "So I will define an algorithm to be a C approximation to this problem.",
                    "label": 0
                },
                {
                    "sent": "If the number of mistakes returned by the clustering of the algorithm is at most C times the best possible in the best possible answer.",
                    "label": 0
                },
                {
                    "sent": "OK, so Al would be the cost of the algorithm or the number of mistakes of the.",
                    "label": 0
                },
                {
                    "sent": "Made by the resulting by the Returned Clustering and OPT is the best you could do.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the first algorithm by Bansal, Blum and Caola in oh 902 already proposed that 20,000 factor approximation.",
                    "label": 0
                },
                {
                    "sent": "This problem might not be very interesting, but it was.",
                    "label": 0
                },
                {
                    "sent": "It was it was a conceptual breakthrough and they define the problem and this algorithm is actually not very efficient but doable OK?",
                    "label": 0
                },
                {
                    "sent": "So all of N squared and N is the number of nodes in your graph.",
                    "label": 0
                },
                {
                    "sent": "Later to align the works that actually happened simultaneously by Divine Emmanuelle Fiat anymore, like I know Six and Charlie Congresswoman worth in 03, both suggested solutions based on linear programming rounding which we will see in a second, which gave better performance.",
                    "label": 0
                },
                {
                    "sent": "So this like full again approximation is.",
                    "label": 0
                },
                {
                    "sent": "Arguably better or worse than 20,000 if you ask a theoretician, it's worse if you ask any.",
                    "label": 0
                },
                {
                    "sent": "Practitioner, that's like way better.",
                    "label": 0
                },
                {
                    "sent": "Unless N is like more than the atoms, the number of atoms in the universe, then you know, then probably for Logan is better.",
                    "label": 0
                },
                {
                    "sent": "And of course, this was improved into a constant factor for then in 05 a work by a lone Charikar Newman.",
                    "label": 0
                },
                {
                    "sent": "Launch Icon human Atlanta.",
                    "label": 0
                },
                {
                    "sent": "Right, is it in my?",
                    "label": 0
                },
                {
                    "sent": "Launch Oracle would end his Newman, right?",
                    "label": 0
                },
                {
                    "sent": "Good OK, I just want to make sure.",
                    "label": 0
                },
                {
                    "sent": "Gave it 2.5 approximation ratio which I will show you again also in a bit.",
                    "label": 0
                },
                {
                    "sent": "And then we'll talk about their solution, which is much more efficient.",
                    "label": 0
                },
                {
                    "sent": "Slightly after that, so the rest of the talk I'm going to focus on the algorithms that achieve these coefficients achieve these approximations.",
                    "label": 0
                },
                {
                    "sent": "The idea is really to present the algorithms and some of the intuition behind them.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to show the proofs.",
                    "label": 0
                },
                {
                    "sent": "You might be disappointed or happy.",
                    "label": 0
                },
                {
                    "sent": "I mean, it depends on your if you want to be happy to go in as you know, to jump down the rabbit hole.",
                    "label": 0
                },
                {
                    "sent": "But then.",
                    "label": 0
                },
                {
                    "sent": "When people start falling a falling asleep, I will have to come out OK, so let's start.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's start with a warm up exercise.",
                    "label": 0
                },
                {
                    "sent": "This was given as a kind of also as a warm up exercise in Bunzl, Blumen Chow line O2 and they notice the very simple following.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's say I have a graph.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let's say I only care about correlation clustering that returns only two clusters.",
                    "label": 0
                },
                {
                    "sent": "This is an exercise is not a general solution, but they assume that the best solution for correlation clustering returns two clusters OK. Can I even do something in that case?",
                    "label": 0
                },
                {
                    "sent": "OK, and it turns out, yes, you can do something extremely trivial in that case, OK?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to look at a node.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the graph and its neighborhood neighborhood, it's everybody that's connected to and itself including and.",
                    "label": 0
                },
                {
                    "sent": "I'm going to only consider clustering of the form.",
                    "label": 0
                },
                {
                    "sent": "The neighborhood of the node and everything else.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, that sounds very would umentary, but surprisingly enough it can only already do something.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you look at so, let's define the number of nodes that they.",
                    "label": 0
                },
                {
                    "sent": "That unable disagrees with the best clustering is essentially the set into the set difference between the two.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here this node circled in red is in the neighborhood of V, but is not in the same cluster as V in the in the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "Now I clearly I don't know the optimal solution to begin with, but it exists nevertheless.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's call this number D. The number of disagreements.",
                    "label": 0
                },
                {
                    "sent": "So I want to say that the optimal solution must air on at least 10 * D / 2 edges.",
                    "label": 0
                },
                {
                    "sent": "Why's that so the so D is the so the number of disagreements over the node that has the minimum number of mistakes is D. OK, so every node has at least D neighbors that have edges going across the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "OK, so that those are edges I pay for OK, but I have nodes on both sides, so I kind of counted twice.",
                    "label": 0
                },
                {
                    "sent": "OK, so each one of them contributes D / 2 errors.",
                    "label": 0
                },
                {
                    "sent": "OK, so the optimal solution needs to be at least 10 * D / 2.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but what does the algorithm do?",
                    "label": 0
                },
                {
                    "sent": "OK, the algorithm takes the entire neighborhood of this node, so it takes those D nodes that were.",
                    "label": 0
                },
                {
                    "sent": "It takes the nodes that did not include.",
                    "label": 0
                },
                {
                    "sent": "They were not included in the original graph or they or does not take once they were included and puts them in the cluster.",
                    "label": 0
                },
                {
                    "sent": "Those nodes are connected to at most N different things.",
                    "label": 1
                },
                {
                    "sent": "I mean, N is the entire size of the graph, so the number of errors.",
                    "label": 0
                },
                {
                    "sent": "That algorithm seizes whatever the best is, plus N * D. So the D errors each one of them contributes at most.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the errors is one of them contributed most in more errors, so it's 10 times the.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you just put the two together, the top is at least N D / 2 and the algorithm is.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "At most optimal, I sometimes do you get that algae is smaller than three opt OK so this is like very simple in some sense.",
                    "label": 0
                },
                {
                    "sent": "You disappointing almost right.",
                    "label": 0
                },
                {
                    "sent": "But remember that.",
                    "label": 0
                },
                {
                    "sent": "This problem is hard.",
                    "label": 0
                },
                {
                    "sent": "OK, so even this is something OK. Later in a show in a paper that I'm not showing, it was shown that for the case of two centers and actually any fixed number of clusters, you could get Iapetus, but I'm not going to talk about that in this talk.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the remainder of the talk, I want to kind of show LP based solution so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to so the first 2 works happens to me.",
                    "label": 0
                },
                {
                    "sent": "Tanias Lee and I reference them in the beginning and then the loan Charikar.",
                    "label": 0
                },
                {
                    "sent": "Newman, I'm missing somebody here.",
                    "label": 0
                },
                {
                    "sent": "So why do I have two letters?",
                    "label": 0
                },
                {
                    "sent": "It was just a typo.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so the so the last paper also is LP based and let's let's see what I'm.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's go back to the let's go back to the definition of correlation clustering.",
                    "label": 0
                },
                {
                    "sent": "We had this sum over disagreements and agreements in the graph.",
                    "label": 0
                },
                {
                    "sent": "I can think about it as this way let me just define distances between pairs I&J OK. And these distances I'm going to restrict to be either zero or one OK, and I'm going to pay.",
                    "label": 0
                },
                {
                    "sent": "One for every edge in the graph for which I put the two nodes in distance one, and I'm going to pay one for every time two nodes, or don't have an edge between them, but I put them in distance 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is exactly equal to the definition that I had before.",
                    "label": 0
                },
                {
                    "sent": "OK, and notice that for distances, if I didn't have the last condition, then I could just sit all the everything that has an edge to distance zero and everything doesn't have an edge distance one now would be done OK, but distances abide in the triangle inequality, which means that if I have if I if the distance with INK so I and J&J and K0 then the distance between I and K must be 0 as well.",
                    "label": 0
                },
                {
                    "sent": "So if you look at if you stare hard at this thing, you'll see that this actually gives you a clustering distances of 01 is A is identical to having connecting components OK, so the immediate thing we can try to do.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just relax this problem so we cannot solve the integer problem, but we can solve the linear problem and we can do this by just replacing the constraints on DJ to be in the range 01.",
                    "label": 0
                },
                {
                    "sent": "OK, so now there's another Floating Points.",
                    "label": 0
                },
                {
                    "sent": "And this becomes a linear problem program.",
                    "label": 0
                },
                {
                    "sent": "So the objective is linear.",
                    "label": 0
                },
                {
                    "sent": "The constraints are linear in the IJ, and so this is polynomial time solvable.",
                    "label": 0
                },
                {
                    "sent": "OK. What is E?",
                    "label": 0
                },
                {
                    "sent": "Is the edge set either capitoli?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's it's the edge set.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there original input edge set.",
                    "label": 0
                },
                {
                    "sent": "OK so everything yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so this gives, so the result of this thing is is good in some sense.",
                    "label": 0
                },
                {
                    "sent": "I optimally solve a more general problem, so the objective function is reduced, or at the very least not increased, but the results DJ that I get our fractional so the distances between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Now we need to round them.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a common technique for rounding LP.",
                    "label": 0
                },
                {
                    "sent": "Solutions is called rich and growing and let me kind of explain to you how that goes.",
                    "label": 0
                },
                {
                    "sent": "You start with some setting where you have real distances now, so now you have embedded your points into this space where you really have.",
                    "label": 0
                },
                {
                    "sent": "Distances.",
                    "label": 0
                },
                {
                    "sent": "I mean, I put it here into the.",
                    "label": 0
                },
                {
                    "sent": "Of course it's not today, it's not even Euclidean.",
                    "label": 0
                },
                {
                    "sent": "Usually it's just a metric, OK?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You pick an arbitrary point.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter which one.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You pick an arbitrary point and you start growing a ball around it.",
                    "label": 1
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "Just take all the points that lie with distance less than the radius into it.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you keep growing it until some condition holds.",
                    "label": 0
                },
                {
                    "sent": "OK, what is that condition?",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into that for a second because that depending on which publication might be slightly no tacious but.",
                    "label": 0
                },
                {
                    "sent": "It's a condition that's easy to check.",
                    "label": 0
                },
                {
                    "sent": "OK, you grow the ball.",
                    "label": 0
                },
                {
                    "sent": "You reach some condition when you do, you stop.",
                    "label": 0
                },
                {
                    "sent": "You take all the points in that cluster.",
                    "label": 0
                },
                {
                    "sent": "You take all the points inside the ball and you make the intercluster you.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use more points you keep growing balls until you until you exhaust the entire graph, and then you're done.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "There there the good news about it is that.",
                    "label": 1
                },
                {
                    "sent": "Is it actually probably does something interesting, so this is the terministic algorithm.",
                    "label": 0
                },
                {
                    "sent": "It gives a log and approximation for any weighted graph or functional edges.",
                    "label": 0
                },
                {
                    "sent": "OK, login is over.",
                    "label": 0
                },
                {
                    "sent": "Logan is pretty good.",
                    "label": 0
                },
                {
                    "sent": "It gives a full approximation if you have the unweighted complete graph setting which we talked about which they were trying to solve an in the four approximation.",
                    "label": 0
                },
                {
                    "sent": "You actually have a very simple ago that we don't even have to grow the graphs.",
                    "label": 0
                },
                {
                    "sent": "You don't even have to grow those balls.",
                    "label": 0
                },
                {
                    "sent": "You can just take balls of radius 1/2.",
                    "label": 0
                },
                {
                    "sent": "And kind of check whether you know the average distance inside your cluster is like more than 1/4 more than 1/4.",
                    "label": 0
                },
                {
                    "sent": "If it is you, you don't take this cluster and you make the node into a single tone.",
                    "label": 0
                },
                {
                    "sent": "If it's less, you take it, and that's it.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple, very efficient.",
                    "label": 1
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this could be so the rounding technique could be improved.",
                    "label": 0
                },
                {
                    "sent": "You want to.",
                    "label": 0
                },
                {
                    "sent": "I want to present the pivot algorithm presented by a launcher.",
                    "label": 0
                },
                {
                    "sent": "Karen Newman.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, we start with the result of the LP, so I have.",
                    "label": 0
                },
                {
                    "sent": "I have solved this problem.",
                    "label": 0
                },
                {
                    "sent": "I have pairwise distances and now I need to run them.",
                    "label": 0
                },
                {
                    "sent": "So the way I do it, I choose one.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Old, uniformly random.",
                    "label": 0
                },
                {
                    "sent": "I mean I chose the same one as before.",
                    "label": 0
                },
                {
                    "sent": "Just you know, for the presentation sake, but I this is not arbitrary now.",
                    "label": 0
                },
                {
                    "sent": "Now it's a uniform choice over the items.",
                    "label": 0
                },
                {
                    "sent": "It's important that's the case.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then for every other node in the graph, or for every other item I added to this cluster with probability 1 minus DJ.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if the distance is 1, the probability is zero and the distance is 0, the probability is one.",
                    "label": 0
                },
                {
                    "sent": "That's a good sanity check, but for everything in the middle it's kind of.",
                    "label": 0
                },
                {
                    "sent": "It's a coin toss.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you started with.",
                    "label": 0
                },
                {
                    "sent": "You started with the original graph.",
                    "label": 0
                },
                {
                    "sent": "You solve this let me skip back.",
                    "label": 0
                },
                {
                    "sent": "OK, so you solve this linear program used by Jr variables.",
                    "label": 0
                },
                {
                    "sent": "OK, you minimize over all possible instantiation DRJ.",
                    "label": 0
                },
                {
                    "sent": "Or you know this objective OK?",
                    "label": 0
                },
                {
                    "sent": "Where the distance is if you try to kind of squinted it, this is a choice to be as close as possible to 04 edges that for edges in the graph is close to possible to one for non edges.",
                    "label": 0
                },
                {
                    "sent": "How close can I get to the 01 graph while still being a metric?",
                    "label": 0
                },
                {
                    "sent": "OK, so the result of that are variables DJ that you have now.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and that's the input to the rounding.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the input of the rounding.",
                    "label": 1
                },
                {
                    "sent": "Now what do you do with it?",
                    "label": 0
                },
                {
                    "sent": "You said you choose one note at random.",
                    "label": 0
                },
                {
                    "sent": "Then you do this like pairwise thing.",
                    "label": 0
                },
                {
                    "sent": "You either lump them together or disconnect them.",
                    "label": 0
                },
                {
                    "sent": "And then you cut away the cluster you get and repeat.",
                    "label": 0
                },
                {
                    "sent": "So whatever is left, you choose a random and you repeat until you exhaust the entire graph.",
                    "label": 0
                },
                {
                    "sent": "And amazingly enough.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is probably better.",
                    "label": 0
                },
                {
                    "sent": "OK, so this gives a 2.5 approximation.",
                    "label": 0
                },
                {
                    "sent": "That's the best known approximation ratio for the unweighted case of correlation clustering.",
                    "label": 0
                },
                {
                    "sent": "And as a side note, I'm not going to talk about that, but if you're.",
                    "label": 0
                },
                {
                    "sent": "If your edges are fractional, this actually might be better, so 2.5 is the worst case, and it turns out that the unweighted 01 edges are the worst case that these are the hardest case.",
                    "label": 0
                },
                {
                    "sent": "If you have fractional things that actually makes a fairly bitter.",
                    "label": 0
                },
                {
                    "sent": "And the most.",
                    "label": 0
                },
                {
                    "sent": "The worst news around about this algorithm is that you have to solve this ginormous LP, and I'm assuming we have many practitioners in this room.",
                    "label": 0
                },
                {
                    "sent": "Think about your own graphs.",
                    "label": 0
                },
                {
                    "sent": "You think OK, N is like.",
                    "label": 0
                },
                {
                    "sent": "Well, let's let's say more than 100.",
                    "label": 0
                },
                {
                    "sent": "OK, probably in the many thousands or millions, but at the very least, let's say 1000 OK?",
                    "label": 0
                },
                {
                    "sent": "The linear program that we have to solve needs to make sure that all three nodes, every triplet, actually the triangle inequality holds for it.",
                    "label": 0
                },
                {
                    "sent": "So that's N ^3 / N ^3 constraints.",
                    "label": 0
                },
                {
                    "sent": "That's horrendous.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a billion constraints.",
                    "label": 0
                },
                {
                    "sent": "I don't know what LP solvers you use, but that's not acceptable in most most places, OK?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we're back to the drawing table, so we have almost we have the best.",
                    "label": 0
                },
                {
                    "sent": "We've explained.",
                    "label": 0
                },
                {
                    "sent": "The best known approximation ratio.",
                    "label": 0
                },
                {
                    "sent": "But it's unusable so.",
                    "label": 0
                },
                {
                    "sent": "I want to present the vote also known as quick cluster.",
                    "label": 0
                },
                {
                    "sent": "And it was also showed in a launch Areekara Newman.",
                    "label": 0
                },
                {
                    "sent": "What happens if you skip the LP?",
                    "label": 1
                },
                {
                    "sent": "Essentially run pivot like we said before on the original graph.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What do you get?",
                    "label": 0
                },
                {
                    "sent": "So let me just show how that works.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You again, you pick a unit.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Full node, you take all its neighbors.",
                    "label": 1
                },
                {
                    "sent": "You put them in a graph.",
                    "label": 0
                },
                {
                    "sent": "You put them in a cluster.",
                    "label": 0
                },
                {
                    "sent": "You cut it away and you.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rickers OK.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Until you are done with the graph, this is this is very efficient.",
                    "label": 1
                },
                {
                    "sent": "This is almost trivial.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The amazing thing is.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That this already gives a factor 3 approximation.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The so let me kind of give you an intuition of why why this happens OK.",
                    "label": 0
                },
                {
                    "sent": "So if you think back to the the toy example that we had from bunzl Bloom caola about choosing the node that minimizes disagreement with the graph with the final clustering, Now we replace it with instead of the one that minimizes disagreement, just replaced with a uniform, just choose one node at random.",
                    "label": 0
                },
                {
                    "sent": "The number of mistakes that is going to make is it's number of disagreements with the best clustering times N like we did before, right?",
                    "label": 0
                },
                {
                    "sent": "But if you think about it, the optimal doesn't pay the optimal pays.",
                    "label": 0
                },
                {
                    "sent": "The expected number of disagreements with every node or the sum of them actually divided by two, so the same the same argument that we had before about the node minimizing the disagreement is actually true about a uniformly uniformly chosen random node with expectation.",
                    "label": 0
                },
                {
                    "sent": "So now this is a randomized thing, but still the expectation is correct, so you can repeat a few times and just take the best one.",
                    "label": 0
                },
                {
                    "sent": "OK, but unfortunately this also only works for unweighted graphs, and now we need to somehow we're not going to do it now, but we I think this is a major next step to do so.",
                    "label": 0
                },
                {
                    "sent": "This is quick cluster and the field in his part of the talk is going to say more about that in.",
                    "label": 0
                },
                {
                    "sent": "Extend, so let me just.",
                    "label": 0
                },
                {
                    "sent": "Recap and give you a few more.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A few more references.",
                    "label": 0
                },
                {
                    "sent": "These are the references I talked about, but I want to so this is a bit small, but.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We see in one word what each one of them does, because I think this is a if you you know if you want to go out of this talk and start kind of digging into what kind of variance we have and whatever it's like kind of was standard literature.",
                    "label": 0
                },
                {
                    "sent": "Then go to Sen. Google Swami in 0906 asked what happens if you have a fixed number of clusters.",
                    "label": 1
                },
                {
                    "sent": "It turns out you have a pee test for that.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can do better if you know in advance how many clusters you have.",
                    "label": 0
                },
                {
                    "sent": "Clearly that number that running time grows as the number of clusters and so if you.",
                    "label": 0
                },
                {
                    "sent": "If you set the number, you can just go all the way from one to N and solve the NP hard problem also.",
                    "label": 0
                },
                {
                    "sent": "It's going to be cheating.",
                    "label": 0
                },
                {
                    "sent": "Together with near Elan I worked on.",
                    "label": 0
                },
                {
                    "sent": "I worked on the problem of so remember this triangle with the machine learning, so I said you have the ground truth, the input and the output and the question is what is the relation between that and turns out that you could derive most of the resources you can get for quick cluster just by the reduction of the machine learning case and it turns out that the distance to the ground truth is actually smaller.",
                    "label": 0
                },
                {
                    "sent": "Then the distance to the graph.",
                    "label": 0
                },
                {
                    "sent": "So this is surprising.",
                    "label": 0
                },
                {
                    "sent": "Von Zullen Williamson actually 2 quick cluster and derandomized it so now if you have if you're unhappy with the fact that quick cluster is randomized algorithm which gives you a good result in expectation but somehow.",
                    "label": 0
                },
                {
                    "sent": "Might with post more verbally to give you a bad answer, you can be randomized and guarantee a factor approximation.",
                    "label": 1
                },
                {
                    "sent": "Matthew Sanquin Schutte showed that you can actually do online correlation clustering to some extent, and Francesco's going to talk about that.",
                    "label": 0
                },
                {
                    "sent": "Oh, wait a second.",
                    "label": 0
                },
                {
                    "sent": "Sorry they that's skip one, so Matthew and shooting alone.",
                    "label": 0
                },
                {
                    "sent": "I had a very interesting paper about what happens if the mistakes are random.",
                    "label": 0
                },
                {
                    "sent": "So I take a graph that is a collection of clicks and now I flip edges with some probability P. That's a much more.",
                    "label": 0
                },
                {
                    "sent": "That's potentially a problem with much more conducive of actually finding the solution instead of an arbitrary graph, and indeed they show that's the case.",
                    "label": 0
                },
                {
                    "sent": "So even with huge amount of noise, if you flip edges probability almost 1/2, you can actually recover the original graph in with very high probability, or at least clusters that are big enough.",
                    "label": 0
                },
                {
                    "sent": "And I'm actually going to skip the last two I think want to leave some time for questions.",
                    "label": 0
                },
                {
                    "sent": "That's that for now.",
                    "label": 0
                },
                {
                    "sent": "I'll be happy to.",
                    "label": 0
                },
                {
                    "sent": "Take questions now.",
                    "label": 0
                }
            ]
        }
    }
}