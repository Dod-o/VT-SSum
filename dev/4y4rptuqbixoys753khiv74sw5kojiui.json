{
    "id": "4y4rptuqbixoys753khiv74sw5kojiui",
    "title": "Structured Regularization for MKL",
    "info": {
        "author": [
            "Guillaume Obozinski, \u00c9cole des Ponts ParisTech, MINES ParisTech"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_obozinski_srf/",
    "segmentation": [
        [
            "This talk will be about relationship between centralization and.",
            "Multiple kernel learning.",
            "What I'll do is I'll return first too.",
            "Maybe first principles or historical questions about multiple kernel learning.",
            "Then I'll talk a bit about structured regularization and how it can be applied to multiple kernel learning.",
            "Maybe a bit of both algorithms and then conclude.",
            "So let me first so return."
        ],
        [
            "To the question of what is multiple kernel learning, I don't claim that the answer that I will give is a unique one, but I think there is one thing which is important in multiple learning is that there are several different aspects in this problem and that they sometimes seem to be forgotten or only one of the views seems to be privileged so.",
            "I would think that there are four different views of multiple kernel learning.",
            "The first one is that it allows to learn metric in a."
        ],
        [
            "Supervised problem, the second one is that it's related to sparsity.",
            "It's sort of a functional space version of sparse method.",
            "The third one is."
        ],
        [
            "It allows data Fusion which is yet something completely different and the 4th."
        ],
        [
            "One is that it allows you to introduce structure in a functional space.",
            "So."
        ],
        [
            "The first formulation that.",
            "Is interesting to consider too.",
            "To introduce the way multiple multiple kernel learning was introduced historically is to consider a regularised supervised learning problem, where the regularization is a Archaea chest norm.",
            "In that case, applying the.",
            "Representor theorem and computing the dual.",
            "Within a function F of K, which turns out to be a convex function of the kernel matrix.",
            "So obviously once you've got a convex function is very tempting to optimize the function F over the kernel K. But in some ways now that we.",
            "Think of this a few years later this might.",
            "This may be a bit criticized because if you think about it, K is coming from the rewrite, regularizer and cake and potentially have.",
            "Maybe many degrees of freedom.",
            "In this formulation, there is probably no regularization for K. Nonetheless, this was the first formulation."
        ],
        [
            "And.",
            "If you express K as a linear combination of a certain number of kernel, then you can formulate the multiple kernel learning as the.",
            "Um?",
            "As the SDP in the."
        ],
        [
            "Case where the loss that you consider is that the one of the support vector machine, then if you want to get rid of the.",
            "Positive constraint.",
            "Considering convex combination is leading to a QCQP.",
            "Let's go to the second point."
        ],
        [
            "If you have multiple kernel learning, well a couple of.",
            "Years later, back ET al have proposed a primal formulation that correspond to the multiple kernel learning and which is.",
            "Relating multiple kernel learning with the L1L2 norm.",
            "So the first proof of these results was rather long, but that's nice thing in science that after some time we're able to explain things with proof that are much shorter.",
            "So here is a four line proof that multiple kernel learning an regularization with DL1L2 norm are dual to each other.",
            "If you consider a problem regularised by the square.",
            "Of the two norm.",
            "So the sum of the two norms of some groups of W and you notice that this norm actually has a version of form.",
            "So you can write it as the minimization over some ADA in our P. An 8 appealing to the simplex of the function on the right.",
            "After a change of variable you recognize on the left feature map, which corresponds to a linear combination of kernel and applying the first duality result that I presented in the first slide, you obtain that.",
            "Sorry, there's an Alpha that's missing on this line, you obtain that.",
            "This is multiple kernel learning."
        ],
        [
            "So from this we see that multiple kernel learning is directly related to the one L1 and two norm, and that means the special case day one norm and so we should keep.",
            "Keep in mind that it's related to sparsity.",
            "Then there is a functional interpretation.",
            "Um?",
            "Multiple kernel learning can be viewed as learning a function which is a linear combination of function belonging to several different properties in kernel Hilbert space, and to selecting among those functions.",
            "And finally, this was the."
        ],
        [
            "Major selling point of multiple kernel learning.",
            "Multiple kernel learning provides an appropriate way of embedding different types of data in the same space and to find the appropriate weightings.",
            "So I think it's important to keep in mind is 4 different."
        ],
        [
            "Interpretations.",
            "So what is the goal of machine multiple kernel learning in the end?",
            "Is it really data Fusion or is it aggregation?",
            "Or is it selection actually depending on each of the aspects that you considered?",
            "For each of the view, it seems to be doing things that are different.",
            "So in the first position it was claimed that the solution is sparse, and indeed, since it's related to the L1 and two norm solution is sparse, and that it therefore discards irrelevant information.",
            "Today we know that sparseness is work well if the task of selection is presented in a context where there is either a large number of variables to separate from or in the case of multiple kernel learning where there's a large number of kernel project specific from, and where there's a large number of kernel to discard in particular.",
            "So of course this was not clear when people started working with MKL and apply it and started applying it to problems in practice.",
            "And it led to a certain number of disappointments because MCL didn't seem to be able to learn a good combination of kernels when you were considering 10 kernels, 20 kernels, and I will advocate that multiple kernel learning should be considered either."
        ],
        [
            "As a sparse method, and in that case used in the context where you have a large number of kernels and when you are able to organize this corn kernels in a certain way to select them, or if you have a small number of kernels, then you should use more recent formulation of multiple kernel learning which are non sparse formulations such as the formulation that are based on LP norms that have been proposed by closed and flow."
        ],
        [
            "Um?",
            "So.",
            "Let's talk about structured sparsity.",
            "Usual sparsity is interested in constraining the cardinality of the support of the model that you're going to learn, so you know selecting a certain number of variables or non zero parameters.",
            "And going beyond usual."
        ],
        [
            "Are structured sparsity.",
            "Which is the most recently through the works of many authors, not only constraint the cardinality of the support, but also constrained the structure of the sparsity pattern that.",
            "You're allowed to use and.",
            "Give a few."
        ],
        [
            "Examples and I get into more concrete examples of.",
            "Norms that induce structured sparsity.",
            "Well, you might want that.",
            "In addition to selecting a small number of variables, variables should be selected in groups or perhaps variables or parameters line hierarchy and should be selected in this while respecting a certain partial order.",
            "Or you can think that variables are your parameters lie on some graph that model some prior information that you have about the problem and that connected variables should be.",
            "Are likely to be simultaneously relevant, so."
        ],
        [
            "Um?",
            "If we start from.",
            "The group LASSO, which is dual to multiple kernel learning its original version.",
            "So the group LASSO considers a partition of the variables into a certain number of disjoint groups.",
            "And regularizes the parameter as a sum of LQ norms of these blocks of parameters.",
            "So it's it's 2 zero groups of variables and as a result this support is a union of group of variable in terms of multiple kernel.",
            "This each of these groups corresponds to one kernel, and so the support will be.",
            "Sparse subset of kernels.",
            "Now going to.",
            "Structured case."
        ],
        [
            "If we start considering that groups are overlapping, we can still keep the same formulation.",
            "Omega is still a norm.",
            "And the sets of zero that you will obtain as solutions of problem, that regularised with Omega of W. Will still be unions of groups of G, but as a result the sparsity pattern will not be a union group, but rather it will be the complement of a union of group.",
            "So it will be an intersection of compliments of the groups that you have used to build the regularization, and in fact it is possible to show and this was shown by Jonathan.",
            "At all that for any family of supports that is stable by intersection, it is possible to construct a set of groups G such that the sparsity patterns that are allowed are exactly the sparsity patterns that.",
            "Are obtained by this norm."
        ],
        [
            "Another way.",
            "Of extending the group lasso to the case where.",
            "Um?",
            "Where the groups overlap is.",
            "To use what I will call a latent group lasso to distinguish it from the other one.",
            "And there the idea is to introduce a latent variable VG for each group.",
            "Each of these latent variable has a support which is included in the corresponding group.",
            "And W is constrained to be the linear combination of these latent variable.",
            "And then instead of applying another one and two norm to W, we actually apply and then one in two norm to the latent variable.",
            "But adding a constraint that the latent variable are superposing are summing to BW, so."
        ],
        [
            "W is this linear superposition of variables that are constrained to live on the corresponding groups.",
            "The interest the reason why this this norm is interesting is that as opposed to the previous norm, which allowed for sparsity patterns that were families stable by intersection, it is actually easy to see that this norm induces sparsity patterns that are stable by Union.",
            "So you can view this enormous somehow complementary of the previous one.",
            "And let me mention that in particular.",
            "Both this norm and the previous one can be applied to.",
            "Um?",
            "To obtain what can be called a graph last so that is to regularize.",
            "Parameters so that.",
            "So that if you have an underlying graph of variables, the coefficients that are nonzero are actually more densely connected on the graph."
        ],
        [
            "Another cheap type of structured norm which.",
            "Has a.",
            "Really interesting applications are hierarchical norms, so given a directed graph such as the one that I'm represented on the on the right.",
            "You can consider groups that are formed of nodes and all the dissident of a given node and consider the group regularization which is the sum of all the sum of the L2 norms of each of the individual groups.",
            "And in particular."
        ],
        [
            "If your directed graph is the tree, then.",
            "But the regularization that your pain is going to be a tree structured regulation for which efficient algorithm can be obtained and which can be used for different things, including learning harkle dictionaries."
        ],
        [
            "So.",
            "What is the connection that we can make between these structured regularization and multiple kernel learning?",
            "Are there multiple kernel learning formulation counterparts for these norms and?"
        ],
        [
            "Um?",
            "To answer this question, I would point out the fact that a large number of the norms that we know have a very variational form which looks like the ones of the L1 norm and the one for the LP norm.",
            "So it's the minimization of an objective which is jointly convex in W an ADA.",
            "And where Ada is constrained to belong to some convex set so."
        ],
        [
            "If I abstract from this formulation, those are norms that are of the following form.",
            "There, there square is exactly the minimization of again the same objective, but now with Ada constraint to be in a convex set."
        ],
        [
            "Each.",
            "So if we concern because there are learning problem with L loss function and."
        ],
        [
            "With an ORM Omega of the form that I just presented is not different."
        ],
        [
            "To see that the dual norm is.",
            "As also a version of form.",
            "And I realized that I have a typo.",
            "There should not be any square here.",
            "And so it seems the dual norm has disform."
        ],
        [
            "If we consider now learning problem regularised by that norm?",
            "And we."
        ],
        [
            "Transform this.",
            "Transform this formulation in Lagrangian a Cup?"
        ],
        [
            "Derivations using Fenchel duality lead to a dual problem in which the dual norm is applied to X transpose Alpha.",
            "Now, if we use the fact that the dual norm."
        ],
        [
            "Has the version of formulation that I wrote above.",
            "Well, we just get a multiple kernel learning formulation.",
            "Except that it differs from the the."
        ],
        [
            "Vanilla multiple counter learning in that the minimum here is not over 8 as belonging to a simplex, but 28 as belonging to a convex set H. So I obviously I should mention that this formulation that I'm convinced here is extremely close to the one that Massimiliano presented this morning."
        ],
        [
            "And So what I?"
        ],
        [
            "Drive here is with a rank 1 Colonel, but we can essentially do the same thing."
        ],
        [
            "Reproducing kernel Hilbert space and for general kernels.",
            "So if we consider a product."
        ],
        [
            "The product of reproducing kernel Hilbert space and parameter vectors and feature Maps that belong to that space.",
            "If we define the the corresponding loss."
        ],
        [
            "And if we consider a norm which is of the same form as the one."
        ],
        [
            "I considered before.",
            "Then all the objectives that are of the former loss plus a certain norm applied to the vector of reproducing kernel Hilbert space norms of each of the parameters corresponding to each of the subspaces.",
            "Well, this formulation."
        ],
        [
            "Will exactly lead to a multiple kernel learning formulation with a dog belonging to some convex set which is associated with the Norm Omega.",
            "Um?",
            "So in fact."
        ],
        [
            "Older norms that I've presented to you before are special cases of this, and so I guess the case of the two norm, one norm and the LP norm have already been mentioned today.",
            "The case of the generalized transition, sorry, generalization of the group lasso to overlapping groups, well is just.",
            "Is just the EO written virtually as a as a weighted sum of the WG squared, and we have similar formulations for latent group LASSO and which sometimes simply find to give relatively intuitive and simple expressions.",
            "So for example, if you consider the special case of the latent group lasso where groups form a tree.",
            "Then the formulation is just constraining the weights associated to each of the different parameters to be ordered."
        ],
        [
            "So the fact that we have all these version formulation means that we have corresponding multiple learning, multiple kernel learning schemes.",
            "And so again, the first few schemes that I'm listing here are ones that we've talked about.",
            "Let me make a few comments on the learning scheme that arise from group lasso with overlap, and from formulations that are latent group lasso so.",
            "Essentially what happens here is that the groups that we've considered before become in the multiple kernel learning formulation.",
            "Groups of kernels, and that the multiple kernel learning formulation can be written either in terms of individual kernels associated to the original kernels, or it can be written as a function of new kernels that are associated with groups.",
            "So for instance, in the case of the group lasso.",
            "If you write it in terms of the individual kernels Ki, then the corresponding weights become the harmonic mean of all the weights that correspond to all the groups that contain a certain kernel.",
            "If you consider the latent group lasso, I could have written a formulation where instead of taking the harmonic mean, I take the arithmetic mean, but it's even simpler to write the multiple kernel learning scheme as a linear combination of kernels that are associated with each group, and now the kernel cagey is just a linear combination of the kernels that are contained in the groups, so there is a very nice.",
            "Um?",
            "Algebra that you can play with by combining kernels, your ordinal kernels and by creating from them kernels that are associated with groups, and this suggests that there is an interesting language that can possibly potentially be used.",
            "So let me be, let me give an example where."
        ],
        [
            "After norms have been used in the multiple kernel learning setting to lead to a heartical kernel learning formulation.",
            "So the idea is is to decompose kernels in a large number of atomic kernels.",
            "Um?",
            "That are indexed by some set and two.",
            "To deal with the size of the set by organizing all these kernels in some sort of hierarchy.",
            "So if you consider.",
            "The Gaussian kernel, a Gaussian kernel, can be expanded via those formulas as a sum over all the possible subsets of variables as.",
            "So so."
        ],
        [
            "Sorry, so if we if we have a kernel of this form which is composed of individual scalar Gaussian kernels, we get an expansion of Gaussian kernels that use as base variable.",
            "The older based variable are contained in a set J for all the possible set J.",
            "So.",
            "It is.",
            "It is tempting in this formulation to add view this as individual kernel and to add weights so as to select kernels that pick particular subset of the variables.",
            "Now the number of such kernels is 2 to the P. If P is your number of variables.",
            "And clearly it's not going to be possible to deal with this time of setting with plane with plain multiple kernel learning formulation so.",
            "The idea that was suggested by office is to actually organize the variables in.",
            "In has diagram that corresponds to the inclusion of the different sets of variable and to use a group regularization which is of the of the form that I presented before, which is a harkle norm which correspond exactly to this graph.",
            "So this leads to multiple kernel learning formulation which selects kernels are quickly.",
            "I don't have time to get into the details."
        ],
        [
            "In the in the last few minutes, what I would like to comment upon is to say few words about algorithms.",
            "So.",
            "Given the fact that now we have a generic way of constructing multiple kernel learning formulations, well, what are what are generally algorithms that we can use?",
            "I mean we have to create a new new optimization schemes for each of these formulations?",
            "Or are there generic algorithmic tools that have emerged to solve this problem?",
            "So there are four cases that you can distinguish."
        ],
        [
            "According to weather, the loss is smooth or the regular regularization is smooth, or both are smooth.",
            "So if both are smooth, essentially the problem is simple.",
            "You can use plenty of different methods if there exist and is non smooth but simple.",
            "Then proximate methods are applicable.",
            "If the loss is non smooth, well you can think that you maybe you could apply proximal method.",
            "It turns out that you know.",
            "El Nonsmooth is typically support vector machines, and in that case it's typically coding a decent schemes that have been.",
            "Most efficient, and in particular the Smosh algorithm.",
            "And it's interesting to see that DSM algorithm is emerges as the right way to approach this problem in the multiple kernel learning formulation.",
            "And it's not so obvious to.",
            "To consider this type of formulation, if you only have the primer and of course the case where L and Omega are both non smooth is hard and This is why there was so much difficulty in finding efficient algorithm for learning is because the two forms are not smooth.",
            "But if we restrict ourselves for the case where the last L is smooth and there plenty of interesting clauses.",
            "That are smooth.",
            "Then we can basically solve a large number of multiple kernel learning formulation by kernel Ising proximal methods.",
            "So the idea is the following is that approximate methods can be applied directly in the reproducing kernel Hilbert space.",
            "Um?",
            "So what I just say is I don't have time to, you know, present approximate methods, but I'd say that the main.",
            "The main problem subproblem that you have to solve when you want to apply approximate algorithm is to solve the proxamol problem, which is to minimize the sum of two terms, one which is a squared squares missing a squared distance as measured by the norm of the reproducing kernel Hilbert space, and then the norm that you want to use, which in this case is the composition of normal mega with.",
            "Norms of the individual represents color space.",
            "And it turns out that this process personal problem can be solved exactly with the same with the same tools as in a Euclidean space.",
            "So applying the Representer theorem.",
            "We can rewrite this proximal problem as follows, and what we see is essentially that it looks like a usual proxamol problem, except that the metric which is used here is not the norm on the dual variables Alpha, but it is the norm which is associated with the kernel."
        ],
        [
            "All the old approximal methods developed for structured sparse terms are directly applicable in.",
            "In the corresponding multiple kernel learning settings, and this was pointed, I guess first by Rosasco ET al.",
            "How much time do you still have two minutes?",
            "OK, so if we're going to consider structured sparse multiple kernel learning, there is an issue, obviously, which is that I told you earlier that since this is a sparse formulation, it should be only efficient or relevant in the case where the number of features is large.",
            "In this case the features come.",
            "As kernels, and so it would be interesting in the case where we have a very large number of kernels Now the issue is that.",
            "You need to have a way to generate the large number or number of kernels, and that suggests to consider combinatorial feature space and commercial function space.",
            "And from a practical point of view, it's going to require very efficient scheme computationally to cache kernel an recompute them efficiently from maybe a set of basis kernels.",
            "It's."
        ],
        [
            "What time would like to suggest is that.",
            "The right way to use structured sparsity in multiple kernel learning setting is to crack the kernel.",
            "That is, if you think of all the the algorithms that have been developed to compute efficiently kernels.",
            "To avoid to have to represent explicitly the feature space, well, those usually are efficient dynamic programs that combine by via sum and product individual kernels that are very simple to compute in that are associated with simple features.",
            "So the idea here is that."
        ],
        [
            "If you consider kernels such as the old subset kernel or the polynomial kernel or string kernels, graph kernels pyramid match kernel, all these kernels have a certain structure of.",
            "And can be computed by dynamic programs that can be exploited to explore the space of kernel efficiently.",
            "And now you can think of expanding of expanding the those kernels and trying starting to do feature selection inside of the kernel, and that's what I mean by cracking the kernel."
        ],
        [
            "So the idea is to integrate multiple kernel learning inside of the kernel itself."
        ],
        [
            "To conclude.",
            "What is multiple kernel learning?",
            "Well, it's a formula to learn in structured.",
            "It can be.",
            "It can be seen in the context of structured sparsity as a way to learn in structured reproducing kernel airspace and opportunity to encode protein structure of the function space.",
            "I tried to illustrate that structure sparsity is directly applicable to multiple kernel learning that there are algorithms that can be applied directly, and I think that one interesting new direction for multiple kernel learning is to design algorithms that.",
            "Explore dramatically feature spaces.",
            "Very large feature spaces.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This talk will be about relationship between centralization and.",
                    "label": 0
                },
                {
                    "sent": "Multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "What I'll do is I'll return first too.",
                    "label": 0
                },
                {
                    "sent": "Maybe first principles or historical questions about multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "Then I'll talk a bit about structured regularization and how it can be applied to multiple kernel learning.",
                    "label": 1
                },
                {
                    "sent": "Maybe a bit of both algorithms and then conclude.",
                    "label": 0
                },
                {
                    "sent": "So let me first so return.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the question of what is multiple kernel learning, I don't claim that the answer that I will give is a unique one, but I think there is one thing which is important in multiple learning is that there are several different aspects in this problem and that they sometimes seem to be forgotten or only one of the views seems to be privileged so.",
                    "label": 0
                },
                {
                    "sent": "I would think that there are four different views of multiple kernel learning.",
                    "label": 1
                },
                {
                    "sent": "The first one is that it allows to learn metric in a.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Supervised problem, the second one is that it's related to sparsity.",
                    "label": 0
                },
                {
                    "sent": "It's sort of a functional space version of sparse method.",
                    "label": 1
                },
                {
                    "sent": "The third one is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It allows data Fusion which is yet something completely different and the 4th.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is that it allows you to introduce structure in a functional space.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first formulation that.",
                    "label": 0
                },
                {
                    "sent": "Is interesting to consider too.",
                    "label": 0
                },
                {
                    "sent": "To introduce the way multiple multiple kernel learning was introduced historically is to consider a regularised supervised learning problem, where the regularization is a Archaea chest norm.",
                    "label": 1
                },
                {
                    "sent": "In that case, applying the.",
                    "label": 0
                },
                {
                    "sent": "Representor theorem and computing the dual.",
                    "label": 0
                },
                {
                    "sent": "Within a function F of K, which turns out to be a convex function of the kernel matrix.",
                    "label": 1
                },
                {
                    "sent": "So obviously once you've got a convex function is very tempting to optimize the function F over the kernel K. But in some ways now that we.",
                    "label": 0
                },
                {
                    "sent": "Think of this a few years later this might.",
                    "label": 0
                },
                {
                    "sent": "This may be a bit criticized because if you think about it, K is coming from the rewrite, regularizer and cake and potentially have.",
                    "label": 0
                },
                {
                    "sent": "Maybe many degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "In this formulation, there is probably no regularization for K. Nonetheless, this was the first formulation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If you express K as a linear combination of a certain number of kernel, then you can formulate the multiple kernel learning as the.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "As the SDP in the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case where the loss that you consider is that the one of the support vector machine, then if you want to get rid of the.",
                    "label": 0
                },
                {
                    "sent": "Positive constraint.",
                    "label": 0
                },
                {
                    "sent": "Considering convex combination is leading to a QCQP.",
                    "label": 0
                },
                {
                    "sent": "Let's go to the second point.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you have multiple kernel learning, well a couple of.",
                    "label": 0
                },
                {
                    "sent": "Years later, back ET al have proposed a primal formulation that correspond to the multiple kernel learning and which is.",
                    "label": 0
                },
                {
                    "sent": "Relating multiple kernel learning with the L1L2 norm.",
                    "label": 0
                },
                {
                    "sent": "So the first proof of these results was rather long, but that's nice thing in science that after some time we're able to explain things with proof that are much shorter.",
                    "label": 0
                },
                {
                    "sent": "So here is a four line proof that multiple kernel learning an regularization with DL1L2 norm are dual to each other.",
                    "label": 0
                },
                {
                    "sent": "If you consider a problem regularised by the square.",
                    "label": 0
                },
                {
                    "sent": "Of the two norm.",
                    "label": 0
                },
                {
                    "sent": "So the sum of the two norms of some groups of W and you notice that this norm actually has a version of form.",
                    "label": 0
                },
                {
                    "sent": "So you can write it as the minimization over some ADA in our P. An 8 appealing to the simplex of the function on the right.",
                    "label": 0
                },
                {
                    "sent": "After a change of variable you recognize on the left feature map, which corresponds to a linear combination of kernel and applying the first duality result that I presented in the first slide, you obtain that.",
                    "label": 0
                },
                {
                    "sent": "Sorry, there's an Alpha that's missing on this line, you obtain that.",
                    "label": 0
                },
                {
                    "sent": "This is multiple kernel learning.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So from this we see that multiple kernel learning is directly related to the one L1 and two norm, and that means the special case day one norm and so we should keep.",
                    "label": 0
                },
                {
                    "sent": "Keep in mind that it's related to sparsity.",
                    "label": 0
                },
                {
                    "sent": "Then there is a functional interpretation.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Multiple kernel learning can be viewed as learning a function which is a linear combination of function belonging to several different properties in kernel Hilbert space, and to selecting among those functions.",
                    "label": 0
                },
                {
                    "sent": "And finally, this was the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Major selling point of multiple kernel learning.",
                    "label": 1
                },
                {
                    "sent": "Multiple kernel learning provides an appropriate way of embedding different types of data in the same space and to find the appropriate weightings.",
                    "label": 1
                },
                {
                    "sent": "So I think it's important to keep in mind is 4 different.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interpretations.",
                    "label": 0
                },
                {
                    "sent": "So what is the goal of machine multiple kernel learning in the end?",
                    "label": 0
                },
                {
                    "sent": "Is it really data Fusion or is it aggregation?",
                    "label": 1
                },
                {
                    "sent": "Or is it selection actually depending on each of the aspects that you considered?",
                    "label": 0
                },
                {
                    "sent": "For each of the view, it seems to be doing things that are different.",
                    "label": 0
                },
                {
                    "sent": "So in the first position it was claimed that the solution is sparse, and indeed, since it's related to the L1 and two norm solution is sparse, and that it therefore discards irrelevant information.",
                    "label": 1
                },
                {
                    "sent": "Today we know that sparseness is work well if the task of selection is presented in a context where there is either a large number of variables to separate from or in the case of multiple kernel learning where there's a large number of kernel project specific from, and where there's a large number of kernel to discard in particular.",
                    "label": 0
                },
                {
                    "sent": "So of course this was not clear when people started working with MKL and apply it and started applying it to problems in practice.",
                    "label": 0
                },
                {
                    "sent": "And it led to a certain number of disappointments because MCL didn't seem to be able to learn a good combination of kernels when you were considering 10 kernels, 20 kernels, and I will advocate that multiple kernel learning should be considered either.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a sparse method, and in that case used in the context where you have a large number of kernels and when you are able to organize this corn kernels in a certain way to select them, or if you have a small number of kernels, then you should use more recent formulation of multiple kernel learning which are non sparse formulations such as the formulation that are based on LP norms that have been proposed by closed and flow.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about structured sparsity.",
                    "label": 0
                },
                {
                    "sent": "Usual sparsity is interested in constraining the cardinality of the support of the model that you're going to learn, so you know selecting a certain number of variables or non zero parameters.",
                    "label": 1
                },
                {
                    "sent": "And going beyond usual.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are structured sparsity.",
                    "label": 0
                },
                {
                    "sent": "Which is the most recently through the works of many authors, not only constraint the cardinality of the support, but also constrained the structure of the sparsity pattern that.",
                    "label": 1
                },
                {
                    "sent": "You're allowed to use and.",
                    "label": 0
                },
                {
                    "sent": "Give a few.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Examples and I get into more concrete examples of.",
                    "label": 0
                },
                {
                    "sent": "Norms that induce structured sparsity.",
                    "label": 0
                },
                {
                    "sent": "Well, you might want that.",
                    "label": 0
                },
                {
                    "sent": "In addition to selecting a small number of variables, variables should be selected in groups or perhaps variables or parameters line hierarchy and should be selected in this while respecting a certain partial order.",
                    "label": 1
                },
                {
                    "sent": "Or you can think that variables are your parameters lie on some graph that model some prior information that you have about the problem and that connected variables should be.",
                    "label": 1
                },
                {
                    "sent": "Are likely to be simultaneously relevant, so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "If we start from.",
                    "label": 0
                },
                {
                    "sent": "The group LASSO, which is dual to multiple kernel learning its original version.",
                    "label": 0
                },
                {
                    "sent": "So the group LASSO considers a partition of the variables into a certain number of disjoint groups.",
                    "label": 1
                },
                {
                    "sent": "And regularizes the parameter as a sum of LQ norms of these blocks of parameters.",
                    "label": 0
                },
                {
                    "sent": "So it's it's 2 zero groups of variables and as a result this support is a union of group of variable in terms of multiple kernel.",
                    "label": 1
                },
                {
                    "sent": "This each of these groups corresponds to one kernel, and so the support will be.",
                    "label": 0
                },
                {
                    "sent": "Sparse subset of kernels.",
                    "label": 0
                },
                {
                    "sent": "Now going to.",
                    "label": 0
                },
                {
                    "sent": "Structured case.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we start considering that groups are overlapping, we can still keep the same formulation.",
                    "label": 0
                },
                {
                    "sent": "Omega is still a norm.",
                    "label": 1
                },
                {
                    "sent": "And the sets of zero that you will obtain as solutions of problem, that regularised with Omega of W. Will still be unions of groups of G, but as a result the sparsity pattern will not be a union group, but rather it will be the complement of a union of group.",
                    "label": 1
                },
                {
                    "sent": "So it will be an intersection of compliments of the groups that you have used to build the regularization, and in fact it is possible to show and this was shown by Jonathan.",
                    "label": 0
                },
                {
                    "sent": "At all that for any family of supports that is stable by intersection, it is possible to construct a set of groups G such that the sparsity patterns that are allowed are exactly the sparsity patterns that.",
                    "label": 0
                },
                {
                    "sent": "Are obtained by this norm.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another way.",
                    "label": 0
                },
                {
                    "sent": "Of extending the group lasso to the case where.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Where the groups overlap is.",
                    "label": 0
                },
                {
                    "sent": "To use what I will call a latent group lasso to distinguish it from the other one.",
                    "label": 0
                },
                {
                    "sent": "And there the idea is to introduce a latent variable VG for each group.",
                    "label": 1
                },
                {
                    "sent": "Each of these latent variable has a support which is included in the corresponding group.",
                    "label": 0
                },
                {
                    "sent": "And W is constrained to be the linear combination of these latent variable.",
                    "label": 0
                },
                {
                    "sent": "And then instead of applying another one and two norm to W, we actually apply and then one in two norm to the latent variable.",
                    "label": 0
                },
                {
                    "sent": "But adding a constraint that the latent variable are superposing are summing to BW, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "W is this linear superposition of variables that are constrained to live on the corresponding groups.",
                    "label": 0
                },
                {
                    "sent": "The interest the reason why this this norm is interesting is that as opposed to the previous norm, which allowed for sparsity patterns that were families stable by intersection, it is actually easy to see that this norm induces sparsity patterns that are stable by Union.",
                    "label": 0
                },
                {
                    "sent": "So you can view this enormous somehow complementary of the previous one.",
                    "label": 0
                },
                {
                    "sent": "And let me mention that in particular.",
                    "label": 0
                },
                {
                    "sent": "Both this norm and the previous one can be applied to.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "To obtain what can be called a graph last so that is to regularize.",
                    "label": 0
                },
                {
                    "sent": "Parameters so that.",
                    "label": 0
                },
                {
                    "sent": "So that if you have an underlying graph of variables, the coefficients that are nonzero are actually more densely connected on the graph.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another cheap type of structured norm which.",
                    "label": 0
                },
                {
                    "sent": "Has a.",
                    "label": 0
                },
                {
                    "sent": "Really interesting applications are hierarchical norms, so given a directed graph such as the one that I'm represented on the on the right.",
                    "label": 1
                },
                {
                    "sent": "You can consider groups that are formed of nodes and all the dissident of a given node and consider the group regularization which is the sum of all the sum of the L2 norms of each of the individual groups.",
                    "label": 0
                },
                {
                    "sent": "And in particular.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If your directed graph is the tree, then.",
                    "label": 0
                },
                {
                    "sent": "But the regularization that your pain is going to be a tree structured regulation for which efficient algorithm can be obtained and which can be used for different things, including learning harkle dictionaries.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What is the connection that we can make between these structured regularization and multiple kernel learning?",
                    "label": 0
                },
                {
                    "sent": "Are there multiple kernel learning formulation counterparts for these norms and?",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "To answer this question, I would point out the fact that a large number of the norms that we know have a very variational form which looks like the ones of the L1 norm and the one for the LP norm.",
                    "label": 0
                },
                {
                    "sent": "So it's the minimization of an objective which is jointly convex in W an ADA.",
                    "label": 0
                },
                {
                    "sent": "And where Ada is constrained to belong to some convex set so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I abstract from this formulation, those are norms that are of the following form.",
                    "label": 0
                },
                {
                    "sent": "There, there square is exactly the minimization of again the same objective, but now with Ada constraint to be in a convex set.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each.",
                    "label": 0
                },
                {
                    "sent": "So if we concern because there are learning problem with L loss function and.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With an ORM Omega of the form that I just presented is not different.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To see that the dual norm is.",
                    "label": 0
                },
                {
                    "sent": "As also a version of form.",
                    "label": 0
                },
                {
                    "sent": "And I realized that I have a typo.",
                    "label": 0
                },
                {
                    "sent": "There should not be any square here.",
                    "label": 0
                },
                {
                    "sent": "And so it seems the dual norm has disform.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we consider now learning problem regularised by that norm?",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transform this.",
                    "label": 0
                },
                {
                    "sent": "Transform this formulation in Lagrangian a Cup?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Derivations using Fenchel duality lead to a dual problem in which the dual norm is applied to X transpose Alpha.",
                    "label": 0
                },
                {
                    "sent": "Now, if we use the fact that the dual norm.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Has the version of formulation that I wrote above.",
                    "label": 0
                },
                {
                    "sent": "Well, we just get a multiple kernel learning formulation.",
                    "label": 0
                },
                {
                    "sent": "Except that it differs from the the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vanilla multiple counter learning in that the minimum here is not over 8 as belonging to a simplex, but 28 as belonging to a convex set H. So I obviously I should mention that this formulation that I'm convinced here is extremely close to the one that Massimiliano presented this morning.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what I?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Drive here is with a rank 1 Colonel, but we can essentially do the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reproducing kernel Hilbert space and for general kernels.",
                    "label": 0
                },
                {
                    "sent": "So if we consider a product.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The product of reproducing kernel Hilbert space and parameter vectors and feature Maps that belong to that space.",
                    "label": 0
                },
                {
                    "sent": "If we define the the corresponding loss.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we consider a norm which is of the same form as the one.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I considered before.",
                    "label": 0
                },
                {
                    "sent": "Then all the objectives that are of the former loss plus a certain norm applied to the vector of reproducing kernel Hilbert space norms of each of the parameters corresponding to each of the subspaces.",
                    "label": 0
                },
                {
                    "sent": "Well, this formulation.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will exactly lead to a multiple kernel learning formulation with a dog belonging to some convex set which is associated with the Norm Omega.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So in fact.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Older norms that I've presented to you before are special cases of this, and so I guess the case of the two norm, one norm and the LP norm have already been mentioned today.",
                    "label": 0
                },
                {
                    "sent": "The case of the generalized transition, sorry, generalization of the group lasso to overlapping groups, well is just.",
                    "label": 0
                },
                {
                    "sent": "Is just the EO written virtually as a as a weighted sum of the WG squared, and we have similar formulations for latent group LASSO and which sometimes simply find to give relatively intuitive and simple expressions.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you consider the special case of the latent group lasso where groups form a tree.",
                    "label": 0
                },
                {
                    "sent": "Then the formulation is just constraining the weights associated to each of the different parameters to be ordered.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the fact that we have all these version formulation means that we have corresponding multiple learning, multiple kernel learning schemes.",
                    "label": 1
                },
                {
                    "sent": "And so again, the first few schemes that I'm listing here are ones that we've talked about.",
                    "label": 0
                },
                {
                    "sent": "Let me make a few comments on the learning scheme that arise from group lasso with overlap, and from formulations that are latent group lasso so.",
                    "label": 0
                },
                {
                    "sent": "Essentially what happens here is that the groups that we've considered before become in the multiple kernel learning formulation.",
                    "label": 0
                },
                {
                    "sent": "Groups of kernels, and that the multiple kernel learning formulation can be written either in terms of individual kernels associated to the original kernels, or it can be written as a function of new kernels that are associated with groups.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in the case of the group lasso.",
                    "label": 0
                },
                {
                    "sent": "If you write it in terms of the individual kernels Ki, then the corresponding weights become the harmonic mean of all the weights that correspond to all the groups that contain a certain kernel.",
                    "label": 0
                },
                {
                    "sent": "If you consider the latent group lasso, I could have written a formulation where instead of taking the harmonic mean, I take the arithmetic mean, but it's even simpler to write the multiple kernel learning scheme as a linear combination of kernels that are associated with each group, and now the kernel cagey is just a linear combination of the kernels that are contained in the groups, so there is a very nice.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Algebra that you can play with by combining kernels, your ordinal kernels and by creating from them kernels that are associated with groups, and this suggests that there is an interesting language that can possibly potentially be used.",
                    "label": 0
                },
                {
                    "sent": "So let me be, let me give an example where.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After norms have been used in the multiple kernel learning setting to lead to a heartical kernel learning formulation.",
                    "label": 0
                },
                {
                    "sent": "So the idea is is to decompose kernels in a large number of atomic kernels.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "That are indexed by some set and two.",
                    "label": 0
                },
                {
                    "sent": "To deal with the size of the set by organizing all these kernels in some sort of hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So if you consider.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian kernel, a Gaussian kernel, can be expanded via those formulas as a sum over all the possible subsets of variables as.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, so if we if we have a kernel of this form which is composed of individual scalar Gaussian kernels, we get an expansion of Gaussian kernels that use as base variable.",
                    "label": 0
                },
                {
                    "sent": "The older based variable are contained in a set J for all the possible set J.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "It is tempting in this formulation to add view this as individual kernel and to add weights so as to select kernels that pick particular subset of the variables.",
                    "label": 0
                },
                {
                    "sent": "Now the number of such kernels is 2 to the P. If P is your number of variables.",
                    "label": 0
                },
                {
                    "sent": "And clearly it's not going to be possible to deal with this time of setting with plane with plain multiple kernel learning formulation so.",
                    "label": 0
                },
                {
                    "sent": "The idea that was suggested by office is to actually organize the variables in.",
                    "label": 0
                },
                {
                    "sent": "In has diagram that corresponds to the inclusion of the different sets of variable and to use a group regularization which is of the of the form that I presented before, which is a harkle norm which correspond exactly to this graph.",
                    "label": 0
                },
                {
                    "sent": "So this leads to multiple kernel learning formulation which selects kernels are quickly.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to get into the details.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the in the last few minutes, what I would like to comment upon is to say few words about algorithms.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Given the fact that now we have a generic way of constructing multiple kernel learning formulations, well, what are what are generally algorithms that we can use?",
                    "label": 0
                },
                {
                    "sent": "I mean we have to create a new new optimization schemes for each of these formulations?",
                    "label": 0
                },
                {
                    "sent": "Or are there generic algorithmic tools that have emerged to solve this problem?",
                    "label": 0
                },
                {
                    "sent": "So there are four cases that you can distinguish.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "According to weather, the loss is smooth or the regular regularization is smooth, or both are smooth.",
                    "label": 0
                },
                {
                    "sent": "So if both are smooth, essentially the problem is simple.",
                    "label": 0
                },
                {
                    "sent": "You can use plenty of different methods if there exist and is non smooth but simple.",
                    "label": 0
                },
                {
                    "sent": "Then proximate methods are applicable.",
                    "label": 0
                },
                {
                    "sent": "If the loss is non smooth, well you can think that you maybe you could apply proximal method.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you know.",
                    "label": 0
                },
                {
                    "sent": "El Nonsmooth is typically support vector machines, and in that case it's typically coding a decent schemes that have been.",
                    "label": 0
                },
                {
                    "sent": "Most efficient, and in particular the Smosh algorithm.",
                    "label": 0
                },
                {
                    "sent": "And it's interesting to see that DSM algorithm is emerges as the right way to approach this problem in the multiple kernel learning formulation.",
                    "label": 0
                },
                {
                    "sent": "And it's not so obvious to.",
                    "label": 0
                },
                {
                    "sent": "To consider this type of formulation, if you only have the primer and of course the case where L and Omega are both non smooth is hard and This is why there was so much difficulty in finding efficient algorithm for learning is because the two forms are not smooth.",
                    "label": 0
                },
                {
                    "sent": "But if we restrict ourselves for the case where the last L is smooth and there plenty of interesting clauses.",
                    "label": 0
                },
                {
                    "sent": "That are smooth.",
                    "label": 0
                },
                {
                    "sent": "Then we can basically solve a large number of multiple kernel learning formulation by kernel Ising proximal methods.",
                    "label": 0
                },
                {
                    "sent": "So the idea is the following is that approximate methods can be applied directly in the reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what I just say is I don't have time to, you know, present approximate methods, but I'd say that the main.",
                    "label": 0
                },
                {
                    "sent": "The main problem subproblem that you have to solve when you want to apply approximate algorithm is to solve the proxamol problem, which is to minimize the sum of two terms, one which is a squared squares missing a squared distance as measured by the norm of the reproducing kernel Hilbert space, and then the norm that you want to use, which in this case is the composition of normal mega with.",
                    "label": 0
                },
                {
                    "sent": "Norms of the individual represents color space.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this process personal problem can be solved exactly with the same with the same tools as in a Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So applying the Representer theorem.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite this proximal problem as follows, and what we see is essentially that it looks like a usual proxamol problem, except that the metric which is used here is not the norm on the dual variables Alpha, but it is the norm which is associated with the kernel.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All the old approximal methods developed for structured sparse terms are directly applicable in.",
                    "label": 0
                },
                {
                    "sent": "In the corresponding multiple kernel learning settings, and this was pointed, I guess first by Rosasco ET al.",
                    "label": 0
                },
                {
                    "sent": "How much time do you still have two minutes?",
                    "label": 0
                },
                {
                    "sent": "OK, so if we're going to consider structured sparse multiple kernel learning, there is an issue, obviously, which is that I told you earlier that since this is a sparse formulation, it should be only efficient or relevant in the case where the number of features is large.",
                    "label": 0
                },
                {
                    "sent": "In this case the features come.",
                    "label": 0
                },
                {
                    "sent": "As kernels, and so it would be interesting in the case where we have a very large number of kernels Now the issue is that.",
                    "label": 1
                },
                {
                    "sent": "You need to have a way to generate the large number or number of kernels, and that suggests to consider combinatorial feature space and commercial function space.",
                    "label": 1
                },
                {
                    "sent": "And from a practical point of view, it's going to require very efficient scheme computationally to cache kernel an recompute them efficiently from maybe a set of basis kernels.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What time would like to suggest is that.",
                    "label": 0
                },
                {
                    "sent": "The right way to use structured sparsity in multiple kernel learning setting is to crack the kernel.",
                    "label": 1
                },
                {
                    "sent": "That is, if you think of all the the algorithms that have been developed to compute efficiently kernels.",
                    "label": 0
                },
                {
                    "sent": "To avoid to have to represent explicitly the feature space, well, those usually are efficient dynamic programs that combine by via sum and product individual kernels that are very simple to compute in that are associated with simple features.",
                    "label": 1
                },
                {
                    "sent": "So the idea here is that.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you consider kernels such as the old subset kernel or the polynomial kernel or string kernels, graph kernels pyramid match kernel, all these kernels have a certain structure of.",
                    "label": 1
                },
                {
                    "sent": "And can be computed by dynamic programs that can be exploited to explore the space of kernel efficiently.",
                    "label": 0
                },
                {
                    "sent": "And now you can think of expanding of expanding the those kernels and trying starting to do feature selection inside of the kernel, and that's what I mean by cracking the kernel.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea is to integrate multiple kernel learning inside of the kernel itself.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude.",
                    "label": 0
                },
                {
                    "sent": "What is multiple kernel learning?",
                    "label": 1
                },
                {
                    "sent": "Well, it's a formula to learn in structured.",
                    "label": 0
                },
                {
                    "sent": "It can be.",
                    "label": 0
                },
                {
                    "sent": "It can be seen in the context of structured sparsity as a way to learn in structured reproducing kernel airspace and opportunity to encode protein structure of the function space.",
                    "label": 1
                },
                {
                    "sent": "I tried to illustrate that structure sparsity is directly applicable to multiple kernel learning that there are algorithms that can be applied directly, and I think that one interesting new direction for multiple kernel learning is to design algorithms that.",
                    "label": 0
                },
                {
                    "sent": "Explore dramatically feature spaces.",
                    "label": 0
                },
                {
                    "sent": "Very large feature spaces.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}