{
    "id": "4nh4rdkho4irsnlchirqwcbjqls4qysg",
    "title": "Deep Learning with Multiplicative Interactions",
    "info": {
        "author": [
            "Geoffrey E. Hinton, Department of Computer Science, University of Toronto"
        ],
        "published": "Jan. 20, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/nips09_hinton_dlmi/",
    "segmentation": [
        [
            "First of, our plasma electricity.",
            "This year Jeff Hinton going back in his current little bit.",
            "Jeff moved to UCSD in the early 1980s and became part of the parallel distributed, or PDP research Group he Co invented Boltzmann machines with Terry Sinofsky in 1983 and then one of the researchers who introduced back propagation algorithm for training multilayer neural networks.",
            "The PDP books in 1986 led to an explosion in the growth of neural networks and actually the NIPS.",
            "Conference started in 1987.",
            "In that kind of that wave of excitement around those books, Jeffers received many awards, including the Fellowship, the Royal Society.",
            "In 1998, the David Rumelhart Prize in 2001, and the Sky Award for Research Excellence in 2005.",
            "Since the 1980s, Jeffers continued to be highly innovative with contributions such as variation, learning the Helmholtz machine in the product of experts architecture.",
            "Most recently, he's been he's been working on deep belief networks for unsupervised learning, and today he'll be talking about deep learning with multiplicative interactions.",
            "Thank you, Jack, thank you.",
            "So today I'm going to spend the first 10 minutes or so, giving you a background about deep learning.",
            "I'm afraid many of you know this stuff, but I want to give the general background and then I'm going to show you one recent example where the ideology of deep learning worked perfectly."
        ],
        [
            "I'm then going to talk about how to make the basic learning module work better by introducing multiplicative interactions into a restricted Boltzmann machine.",
            "I'm going to say why we need to do that and how we keep the number of parameters under control and show you how the inference and learning is still straight, relatively straightforward, and then you describe 3 examples from students in my lab of using the multiplicative interactions one is learning to transform images, that is, learning how images change every time another is having a model of human motion and using a style label to transform that model so it can cope with many different."
        ],
        [
            "Nations and the last is modeling the covariance structure of images so that you can do better discrimination.",
            "So restricted Boltzmann machine consists of two layers of neuron like units.",
            "Their binary stochastic units and there's no connections between the hidden units and also no connections between the visible units.",
            "Given the states of the visible units, which is where you put the data, the hidden units are conditionally independent, so it's very easy to do inference in this kind of machine, and that's good news.",
            "The rule for turning on a hidden unit is simply, you compute.",
            "The input is getting from the visible units.",
            "Put that through the logistic function and that tells you the probability of turning it on, and then you make a stochastic decision about whether it should be on and their independent scattered."
        ],
        [
            "Visions from the hidden units.",
            "This network obeza Hopfield energy function, where the energy if I tell you the states of the visible and hidden units, the energy of the network is quadratic in those states for all pairs of a visible and hidden both on you take the weight between you add up all those weights.",
            "And that tells you.",
            "The negative energy low energy is good.",
            "The derivative of the energy function is nice and simple, it's just the product of the visible in activity.",
            "That's the derivative with respect to weight, which you need for learning."
        ],
        [
            "You can use those energies to define probabilities over joint configurations of visible and hidden units.",
            "And the probability of a joint configuration is proportional to each of the minus energy, but that's normalized by the probability of all possible configurations.",
            "That's called the partition function, and it makes learning tricky.",
            "To know the probability that you assign to a visible vector V, you have to add up over all possible states of the hidden units.",
            "That's the bottom line."
        ],
        [
            "The maximum likelihood learning algorithm for restricted Boltzmann machine is conceptually very simple, but takes a long time.",
            "You start with data on the visible units at time T0 and then using whatever weights you currently have, you activate the hidden units stochastically and then if you were to do it right, she would then activate the visible units again stochastically and you go backwards and forwards.",
            "This is a Markov chain, it's doing alternating block Gibbs sampling and when you run long enough you forgotten where you started and you're now sampling from.",
            "The visible patterns that the model believes in and for those visible patterns that distribution model believes in, you compute the pairwise statistics.",
            "You compute how often a pixel and feature take drawn together and the learning rule is just the difference between those pairwise statistics with data and with fantasies from the model.",
            "Unfortunately, you have to run the Markov chain for a long time.",
            "So I discovered empirically that if you're on the market."
        ],
        [
            "Change for a short time.",
            "It still works pretty well and it's much quicker.",
            "So what we're going to do is just activate the hidden units.",
            "Allow the hidden units to reconstruct a patent on the visible units, typically going to be quite like the data.",
            "Then activate the hidden units again and the difference of those two power statistics will tell you how to change your weight, and then we'll just do stochastic gradient descent to try and get a good set of weights.",
            "The main reason restricted Boltzmann machines interesting is because you can use it recursively."
        ],
        [
            "To learn a deep net, it turns out that the deep net you end up learning is a directed network, which is a big surprise to people in graphical models, and you're learning a directed network one layer at a time.",
            "I'm not going to go into that any further, I'll just give you the sort of how you use it.",
            "You train one layer of features and then you treat the activations of those features as data when you train another layer and you just keep going like that for as many layers as you can stand.",
            "And when you finished the top two layers, a restricted Boltzmann machine, that's an associative memory and all the layers below can be thought of as a directed graphical model directed so that you can generate data from the associative memory via some transformations in the other hidden layers.",
            "We'd like to prove that every time you add another hidden layer, you get a better model of the data.",
            "We can't quite prove that, but we can almost prove it every time you add another hidden layer.",
            "If you do it right, you improve a variational bound on the log probability that the model will generate the training data.",
            "And that's good enough to show you doing something sensible.",
            "Off"
        ],
        [
            "Steve run multiple layers.",
            "You can then stick some label units on top.",
            "And do back propagation.",
            "Now, backpropagation is now not having to design features.",
            "The features already designed by the unsupervised learning.",
            "The multiple lesser Boltzmann machines.",
            "Backpropagation is doing is slightly changing the weights, it hardly changes of each of the tool so that the boundaries between classes are in the right places.",
            "So it's slightly adjusting the features to be better for discrimination, but it's not having to discover the features, so the unsupervised learning gets you into a good part of white space and then the back propagation within that good part of white space does local myopic steepest descent to tune up the weights for discrimination.",
            "Experiments in Benjy's lab show that this wins in two ways.",
            "The unsupervised learning gets you to a better part of white space than you would have got too if you just did supervised learning.",
            "And you actually.",
            "Then get better generalization as well, so you're winning both on the optimization and speed and the generaliza."
        ],
        [
            "This is probably the most important slide in the talk because it sort of challenges what most people in machine learning at least used to believe.",
            "So it uses a technical concept that I got from Donald Rumsfeld called stuff.",
            "And so the way the world works is the stuff it happens and stuff produces images.",
            "And if you then believe that labels are produced from images and so the label is conditionally independent of the stuff given the image, then the right way to do machine learning to discriminate objects and images for example, is to try and learn the function from images to labels.",
            "But almost nobody believes that.",
            "What you ought to believe is that stuff creates images and the same stuff creates labels.",
            "What's more, the bandwidth between stuff and images is high.",
            "That is, if I show you an image, you can tell me a lot about the stuff that created it, like if it's a dog, you can tell me how big the dog is and what color it is and what direction is facing and whether it's jumping up in the air.",
            "If I give you a label like dog, you can tell me hardly anything about the stuff that created it, so that's a very low bandwidth path.",
            "So now if you want to get from image to label, you're completely crazy to try and learn a direct mapping from image to label what you want to do is learn how to get the stuff from the image.",
            "How to invert that high bandwidth path 'cause the image specifies this stuff quite well.",
            "Once you've learned that, then learn how to get the label from the stuff and maybe slightly tune up the way in which you get stuff from images.",
            "That's a different view of how you're going to do.",
            "Discriminative machine learning.",
            "You're first going to figure out what's going on, and then you're going to do the learning and the interesting bit is figuring out what's going on.",
            "OK."
        ],
        [
            "So I want to describe one application which demonstrates that that approach works.",
            "My contribution to this was to thoroughly indoctrinate one very smart student.",
            "He then met another student who knew all about speech and told him about the TIMIT phone recognition problem, and he said, OK, what do I use as input?",
            "And I'm sort of simplifying this a bit, but both students are here so they can correct it later.",
            "Use the standard input, which is 39 coefficients per frame and 11 frames.",
            "Then you have to predict the phone label of the middle frame.",
            "Once you predicted those phone labels, they're going to go into a post process so that user bigram model of phones.",
            "To try and predict what's really going on.",
            "So the student said, well, why don't I take these 39 * 11 input variables and use 2000 hidden units and learn that unsupervised, and then another 2010 units?",
            "And that unsupervised and then another 2000 units are not unsupervised?",
            "If he told me, I'd tell him to stop there, but he said and another 2010 units and learn that unsupervised, then stick 100 units on top and from those or learn all of that unsupervised and then go to 183 context specific phone labels, which is very standard.",
            "An but do fine tuning.",
            "Fine tuning takes a few days on a very fast GPU board, but it turns out that just wipes out the record on the TIMIT recognition problem.",
            "The previous record was 24.4% and this gets 23%.",
            "More importantly, all variations he tries that are along these lines get between 23 and 24%, so they will beat the record.",
            "Andrew Yang has a student called Hung Likely who's recently applied similar techniques to the classification task where you're given the boundaries of the phone and you told all these frames of the same phone.",
            "Can you classify it and he's round about the record for."
        ],
        [
            "Classification task, although I think Michael Collins has something a little bit better.",
            "So this is the net retrained.",
            "It couldn't be a better fit to the ideology.",
            "You just take the input.",
            "You have lots and lots of hidden layers.",
            "You need a big data set of course.",
            "Only the last layer weights is not pre trained and that's why the penultimate layer needs to be small to keep that number small.",
            "And then you just train the hell out of it with back prop and it wins."
        ],
        [
            "Now, in order to do that, they had to be able to model."
        ],
        [
            "The input, which was real valued, Mel caps from coefficients and so the bolts machine.",
            "I told you about has binary visible units and binary hidden units.",
            "You can revise that to have real valued visible units and binary hidden units.",
            "We simply call that the Gaussian RBM."
        ],
        [
            "You make a very naive assumption, which is that given the states of the hidden units, the visible units are conditionally independent, so it's not good for data with high covariances in But the Mail capturing coefficients are designed to get rid of the covariances, so it's not so bad for that data.",
            "You can write down an energy function.",
            "The pictures better around each visible unit you put a parabolic containment function.",
            "You say it's energy to move away from where that visible unit would like to be, and then the effect of the hidden units is to impose an energy gradient that's trying to move it away, and so the lowest energy point will be where the gradient of the blue line is equal and opposite to the gradient of the red line, and so it just moves the parabola sideways.",
            "That's the simple model, and that's not a very good model of data with covariances in at the end of the talk, I'll come back come to a much better."
        ],
        [
            "So the new idea is that the basic module that has either binary or real value visible units and binary hidden units have something deeply wrong with it.",
            "Which is, it can't deal with multiplicative interactions and the world is full of multiplicative interactions.",
            "Preeminent enbom along time ago had a paper on style and content which is about multiplicative interactions.",
            "But all the work on tensor faces about multiplicative interactions.",
            "Whenever you get 2 Gaussian things in the world, two Gaussian distributed variables and you multiply them together, you get a variable that's very non Gaussian distributed.",
            "It's got very heavy tails, and that's probably why the world is full of heavy tailed things.",
            "Even if you believe in ghosts in distributed things."
        ],
        [
            "I want to give you one example of why I want these multiplicative interactions, and it's from a generative perspective.",
            "If you're building generative models that are multi layer and you want to generate some interesting data.",
            "Let's consider the problem of generating an image of a shape.",
            "When I just tell you that it's a square and it's got particular post parameters where it is and what orientation it said.",
            "Well, you could generate where the parts are the edges of the square very accurately.",
            "If you had accurate post parameters, and then maybe you can maintain the constraints between the parts.",
            "But if you do any kind of sloppy generation of the parts, then the edges won't quite meet and the corners won't quite be right angles and you'll get a very bad looking square.",
            "So if you use a top down graphical model.",
            "With.",
            "Basically Gaussian model and you try and generate me praying like this, it's an unlikely configuration, but there you are.",
            "You're liable to generate something like this or like this unless you have very very accurate generation at each stage.",
            "Um?"
        ],
        [
            "The alternative is to sloppy generation, but also specify how to clean things up.",
            "And that's a much better way to go.",
            "So here's the picture.",
            "You know the shape and you know the pose.",
            "You do sloppy generation and probably have a redundant set of parts.",
            "And then you specify how the part should interact, and now you let that thing settle down.",
            "So in generation it's going to be slow.",
            "It's going to settle down and now you get a very clean structure.",
            "It's like when an officer wants to get a bunch of soldiers on the parade ground to stand and eat rectangle.",
            "He could give every soldier his GPS coordinates down to within a few inches and that will be kind of hard work.",
            "Or you could tell them roughly where to stand and then tell each soldier what distance should be from his neighbor.",
            "That's a much better thing to do."
        ],
        [
            "So we're going to try and make a more powerful module for doing this.",
            "Deep learning, and in this more powerful module we'd like the units in one layer to modify to modulate the interactions in the layer below.",
            "So in the level we want to have something like random field, and we want the units in the lower back to specify the weights in that market random field.",
            "Some previous work in NIPS a few years ago with Simon and Arrow had had a fixed Markov random field with units in the layer, but just specifying the biases, and that's already a big win.",
            "But we're now after something more ambitious to actually modulate that Markov random field.",
            "To do that, we need higher order Boltzmann machines."
        ],
        [
            "So I hired a Boltzmann machine, Terry Sinofsky, first thought about these in 1986 I think, or thereabouts.",
            "Has interaction terms that are more than quadratic.",
            "So the second equation there is for three way, one, and that's all we're going to use for now.",
            "And if you think about this, the state of Unit K that SK in the second equation when that's off.",
            "The term doesn't apply when it's on, when it's got a state of 1.",
            "Now you've got a pairwise interaction between I&J, so the state of SK is gating pairwise interactions between I&J.",
            "On each unit can be viewed like that is gating powers interactions between the other guys."
        ],
        [
            "So one thing we could use a hard about some machine for is modeling image transformations.",
            "I could give you pairs of images.",
            "The simple case is when there's a global transformation.",
            "The more interesting case is when there's more local transformations and several transformations going on at once in the same image, and we can cope with that just fine.",
            "So I give you pairs of images.",
            "And the hidden units are now going to gate interactions between pixels, so where those little triangle symbols are will have a 3 way wait.",
            "That gives you an energy as a function of the states of the two pixels and the state of the hidden unit.",
            "And that red hidden unit likes the two transformations of pixels that sees 'cause they're consistent.",
            "They correspond to a consistent translation, and so if you have the image of time T in the image of time T + 1.",
            "Both those pairings will provide evidence about hidden units should be on, and so you'll tend to say that's a consistent transformation.",
            "Conversely, if you have the image at time T and you turn on that hidden unit, it will tell you where the Pixel should translate to in the image in time T + 1.",
            "So Rolla Mesa vision.",
            "I had a paper that uses this and has a huge number of weights because you see, those triangle symbols have three indices.",
            "They connect three different units, and so there's cubically many of them.",
            "So the sort of central trick to this talk is that we're going to take that cubically many parameters, and we're going to just use factorization, 'cause we don't."
        ],
        [
            "Actually believe that we want hidden units to be able to cause arbitrary permutations of the pixels.",
            "We believe this regular structure there, so we don't really need that many parameters.",
            "So we're going to take the three way."
        ],
        [
            "Right wih and we're going to factor it as a product.",
            "Of three weights WIFWJF&WHF.",
            "Per factor, we're going to have a bunch of factors.",
            "So it looks like we've got four indices now, and that's true, but notice we haven't got anything that has more than two indices on it.",
            "So basically we've turned a cube into three pairwise things.",
            "That's a win.",
            "We only get linearly many parameters per factor.",
            "And we have about as many factors as there are hidden units or visible units.",
            "So we basically turn something this cubic into something that's roughly quadratic.",
            "Here's a picture of what we're doing.",
            "Each factor corresponds to a tensor weights.",
            "This only got rank one.",
            "'cause it's the outer product of three vectors.",
            "And then you just add up a bunch of those tensors.",
            "Now if I give you enough tensors, then you can add up to make any tensor you like.",
            "Any 3 way thing you like.",
            "But if I give you a limited number, you can only make things that have sort of regular structure.",
            "This is like PCA, but for three 3 dimensional things instead of two dimensional things."
        ],
        [
            "So as an equation we.",
            "We turned the three weight into a product of 3.",
            "Factor specific weights.",
            "And then we can factorize that top line equation and we see that the energy contributed by a given factor F is just the product of three weighted sums.",
            "For each group of units you're connecting, you take a weighted sum.",
            "Since you apply linear filter and then the energy is the product of all those three linear filters.",
            "So now for a hidden unit age, if you wanted to know whether I should turn it on or not, you'd need to know the difference in the energy of the whole system as a function of whether H is on or off.",
            "So just considering the energy contributed by factor F, What does factor F need to tell hidden unit H in order to reach to decide how to adopt A state that will give the system low energy?",
            "Well, H needs to know the difference in the energy of the whole system as a function, whether it's on or off.",
            "Which is going to be the sum of the energies from all the factors of the energy contributed by factor.",
            "So factor F. We want to know the difference in energy contributed by factor F when H is on and off.",
            "And so you take the right hand side on the top line.",
            "And just differentiate it with respect to SH.",
            "That will give you the right chance even that's the wrong thing to do, and what you get is that the WHF comes out the front and so you see that what hidden unit H needs to see is the product of the."
        ],
        [
            "Other two weighted sums, multiplied by the weight on the connection between the factor and H. So we can draw that in a picture.",
            "The factor at each of its three vertices computes a weighted sum.",
            "And what H needs to see is the product of the weighted sums at the bottom two vertices and that needs to be sent to H and on its way to H it needs to be multiplied by WHF in authority.",
            "Connectionist way all hidden units need to see that same message, but each way to buy the weight on the connection.",
            "If you look at learning."
        ],
        [
            "Then that message that goes from the factor to H. Is what you need for doing learning and the learning algorithm.",
            "The Bolts machine learning outcome stays just the same.",
            "The learning algorithm basically says I want to lower the energy.",
            "When I'm looking at data and I want to raise the energy when I'm generating fantasies from the model.",
            "And the derivative of the energy respect to work when I'm looking at data.",
            "Is just the product of the state of a hidden unit and the message that comes to it from a factor.",
            "And the derivative of the energy when you generated from the model is that same product.",
            "But when you generated from the model and so the difference of those two products is what you need in order to learn the weight.",
            "So who I'm ahead of 2:30?",
            "That's great so.",
            "You get exactly the same pairwise learning rule, so we took these three way interactions.",
            "We factored everything and now everything becomes pairwise, but with these messages that are products of weighted sums, and that's where the multiply gets into the act."
        ],
        [
            "So now I'm going to show you an example for translating images.",
            "We're going to have a pre image and we're going to post image and we're going to train it up on lots of pairs of images and to begin with, the images will just be random dot sparse random dots and all will do is just translate the random dots will have a number of different translations and that's the training data.",
            "And we'll learn the weights to the factors will have a whole bunch of factors, will learn the weights and factors and the weights between factors in units will learn everything.",
            "We start with small random weights and just learn it.",
            "And after it's learned what I'm going to show you is.",
            "The weights that a factor has to the pre image and the weights that are factor has to post image.",
            "That is these two filters that multiplied together and sent to a hidden unit.",
            "And of course, there's going to be a lot of factors, so I'm going to show you the weights for the pre and post image for all."
        ],
        [
            "Practice, so here's the weights to the preimage of all the factors.",
            "For those of you know about translation and free analysis and stuff like this, this should be."
        ],
        [
            "Not a surprise.",
            "And here's the weights to the post image."
        ],
        [
            "Pre image."
        ],
        [
            "First image, pre image post image, pre image post image."
        ],
        [
            "It's learning basically the Fourier basis, and it's learning pairs of ratings that are about 90 degrees out of phase and which seems like a sensible thing to do, and now it can represent translations very nicely.",
            "Instead of translating images, we could rotate images so we take the same random dot patterns and we rotate them.",
            "So there."
        ],
        [
            "The pre image."
        ],
        [
            "And there's the post image.",
            "Pre image."
        ],
        [
            "Postimage"
        ],
        [
            "If you look about halfway down the left hand side is learned about Yang and Yang, whoops."
        ],
        [
            "Wait?",
            "That's the post image.",
            "Pre image post image pre image.",
            "You might call this seeing machine, but that would be confusing.",
            "OK, so now what we did Alan Jackson then suggested something.",
            "He said well, you've trained it on sparse translations.",
            "Why don't you give it transparent images so you give it sparse?",
            "Depends, but some dots translate one way and some dots translate another way.",
            "And we know what happens when you show those to people.",
            "If the two directions of translation is quite similar, within about 30 degrees they see one motion.",
            "If there are more than about 30 degrees, they see two motions and the motions are repelled from each other, just slightly.",
            "So Luckily the Roland didn't know that when he did this work and he was slightly irritated that they were slightly repelled in his data, but that was good news really.",
            "It's good news having a model that doesn't work if you're a psychologist because maybe people don't work the same way.",
            "So we train it on translation, but just one translation of time.",
            "Then we had a second hidden layer which we encourage to be sparse.",
            "All unsupervised II inlay learns units that are tuned to direction of motion.",
            "So each unit has a preferred direction in the georgopoulos kind of away, and you can then decide what this network perceiving by taking all those units in the second in there that have tuned directions and adding up the beliefs of all the ones that are on.",
            "Actually, you wait how?",
            "How much they want to be on by what they believe, and then you're going to distribution over directions it believes in, but everything is totally unsupervised and the answer is when you presented two dot patterns, if there within about 30 degrees it sees one motion, and if there are more than about 30 degrees apart, it sees two motions.",
            "It gets 2 peaks in what it believes and they're slightly repelled, and so that proves that this is how the brain works.",
            "OK, I'm now going to go to."
        ],
        [
            "Time series models.",
            "That's a very.",
            "That's one kind of time series model.",
            "Most people in our community when they want to model time series would actually like to use nonlinear distributed representations, 'cause they're much more powerful and interesting, but they're kind of hard to learn, especially if you use directed models.",
            "And So what they do is they fall back on the old faithfuls which are give up on distributed and run the hidden Markov model will give up on nonlinear nonlinear dynamical system.",
            "Now you could go a little bit further.",
            "Another mixture of linear ankle systems that will get you a little bit further.",
            "But what we're going to do is learn a.",
            "We're going to give up on mathematics and learn a thoroughly nonlinear distributed representation.",
            "So it's going to look like this."
        ],
        [
            "On the right hand side you see the standard restricted Boltzmann machine.",
            "It has some visible units and some hidden units.",
            "And the states of those units are conditioned on previous time slices.",
            "So we we regard the history as fixed and not changeable anymore those previous time slices have conditioning connections which in effect change the biases of the visible units.",
            "That's called an autoregressive model.",
            "If you use linear visible units.",
            "And they also have conditioning connections, which change the biases of the hidden units.",
            "So those are now dynamic biases that depend on the previous data.",
            "You can learn this by you put data in given the data.",
            "The previous data in the current data, the hidden units, all independent so you can sample their states.",
            "You then reconstruct just the current visible data.",
            "And then you activate the hidden units again and use the differences in statistics and you take the learning rule.",
            "You would have used to change the biases of the hidden units and visible units and you back propagate that derivative to change the weights coming from previous states of visible units.",
            "So you can run all of these connections quite simply using."
        ],
        [
            "Contrastive divergent.",
            "Once you've learned you can generate from the model and the way you generate from the model is you have to initialize it with a few sensible frames and then given the previous frames what they do is simply determine the biases of the visible and hidden units, and then you do alternating Gibb sampling.",
            "But you don't have to go for very long, 'cause with strongly determined biases there aren't many places it can really go to.",
            "It has to fit in with what just happened, so you don't have to go for huge lengths of time and then you just sample the state of the visible units and that's your next visible frame and so you."
        ],
        [
            "Keep sampling.",
            "You can actually stack these models up so you can learn one model.",
            "You now have states for the hidden units.",
            "You treat those as data.",
            "Once their data, you can now put in auto regressive connections between them.",
            "And add another hidden layer and learn the next layer.",
            "And if you do that, you'll get a better model.",
            "Anne Graham Taylor shown that if you're doing this for modeling workout data, adding a second layer makes it work much better.",
            "That's not what we're going to do, though.",
            "We"
        ],
        [
            "You apply this kind of modeling to motion capture data, but we're going to do it with these three way interactions, which allow you multiplicative things.",
            "So you can capture human motion by putting markers on the joints and having infrared cameras and figure out where they were in space and figure out the joint angles and so.",
            "Then each frame is going to be a bunch of joint angles which are real values, so we're going to use a Gaussian binary RBM to begin with.",
            "Well, that's where I'm used to begin with, but we got something better now.",
            "Actually.",
            "I retract that remark.",
            "We're basically using it Gaussian binary RBM for the visible units.",
            "We also add in the translation of the base of the spine and the rotation of the base of the spine for rotation we do the actual angles for the pitch and the roll, but the change in angle for the euro, 'cause gravity doesn't care about the absolute value of your.",
            "And so now we're going to do a 3 way version of the model I showed you before.",
            "What we're going to do?"
        ],
        [
            "Take the basic model and modulate all the interaction matrices using a style variable so the style variable is a one event style.",
            "That gets expanded into a real valued vector of 100 style features.",
            "Those style features are used as one of the inputs to a factor, so each factor is looking at some linear combination of style features, some weighted linear combination of those.",
            "And then the.",
            "Those style features are in effect gating the interactions between the earlier frames.",
            "In the hidden units and between the earlier frames and the current visible frame.",
            "I think if you make the current visible frame just be one linear value.",
            "And you make the style label just be 1 number.",
            "And it just changes that interaction.",
            "That's called a heteroskedastic model and you get a Nobel Prize for it.",
            "But I think that's what model is.",
            "I think this is a generalization of that where you're having high dimensional data and you're getting a whole bunch of matrices this way.",
            "Modulation matrices.",
            "OK, so Grand Taylor trained up a model like that and I'm going to show you some samples from the model after it's learned and what we're going to be doing is just generating from the model, anybody?",
            "Linear ankle systems knows that it's kind of hard to generate from them.",
            "They tend to die or explode, and it's certainly very hard to get them to sort of walk a bit and then stop and then start walking again.",
            "So here's some samples from the model."
        ],
        [
            "So this is it generating normal walking.",
            "This is all data from a student at CMU who did various styles of things.",
            "So it's deciding which way to turn.",
            "And it's also deciding how fast to walk and it's walking at a sort of fairly constant speed there.",
            "Now we're going to tell it to walk in a drunken way.",
            "It's the very same model, it's just so style labels engaging the interaction matrices and you'll notice that it actually stops.",
            "And then starts again.",
            "That's very hard to do with our linear domical system.",
            "Um?",
            "You can tell him to walk like a gangly teenager.",
            "Being a computer science student, he's good at that.",
            "He remembers that.",
            "I make all these character assertions about this poor student who provided us the data.",
            "I've never met him.",
            "No idea who he is, but he's easy to make jokes about.",
            "That's what you get from remaining data.",
            "Because it's all one model with this style label, it can walk in one style and then you can switch the style label.",
            "It's never seen transitions between styles, but it'll do plausable transitions.",
            "Of course it doesn't understand physics, so there may be some momentum lost in this transition.",
            "It just models what it sees.",
            "But we're telling to work in a sexy way.",
            "And then to switch to normal.",
            "Notice is the same model going in a very different speed.",
            "That's normal, and then we'll switch to sexy again.",
            "I think.",
            "There he goes.",
            "It's censored at this point.",
            "Right?",
            "Since I've got a bit of spare time with my schedule, I'll show you one more.",
            "This is just walking like a cat.",
            "And Graham can get this model to do all sorts of things.",
            "He can't get it to do backflips but he can get to all sorts of things including slowing down and speeding up.",
            "OK, that's it.",
            "For the mokap example.",
            "And now I want to go for the most come."
        ],
        [
            "Great example.",
            "What we're going to do is take this 3 way thing, and we're going to use it for modeling a static image by making two copies of the same image.",
            "This is one way to think about it.",
            "We make 2 copies of the same image.",
            "And we say that each factor has exactly the same weight to those two copies, but other than that it's going to be just like what we did before.",
            "So now if you think about what the belief propagation rule is, the factor should take the weighted sum.",
            "It gets on the 1st copy, take the weighted sum, it gets on the second copy, which is the same weighted sum.",
            "Multiply the two together that is, square the output of a linear filter and send that message to the hidden units.",
            "Well, that's exactly the standard model of a simple cell sending stuff to complex cells.",
            "So the claim here is that the model of simple cells and complex cells are quite popular in the vision literature and with neuro scientist two just dropped out of taking a 3 way energy function and factoring it.",
            "The advantage of taking the three rare in G function factoring it is that it becomes obvious how to do the learning, and it becomes.",
            "Obviously you've got a generative model."
        ],
        [
            "So the advantage of modeling of having these three way interaction is you can model covariances between pixels rather than pixels, and that's a much better thing to model.",
            "So if I say if I say what's a vertical edge, one vertical edge doesn't really mean is bright this side and outside, or it's far this side and near this side.",
            "What are vertical edge really means if it's including edges, do not interpolate across this edge.",
            "It's a statement about covariance structure.",
            "And that's what we'd like, a representation of a vertical edge to do.",
            "Other models another model against this is Gaussian scale mixture models.",
            "They get this same property.",
            "You don't want to interpolate horizontally across a vertical edge.",
            "That gives you some translation invariants, a lot of invariants to brightness and contrast, and it makes it look a bit more like a complex cell."
        ],
        [
            "So here's another way of thinking about it.",
            "If you take 2 pixels and you want to model the joint distribution, that will typically have a strong covariance.",
            "And so the two square things are meant to be the two pixels, and what we could do is have two linear filters which we're going to learn.",
            "That's those red blobs and green blobs.",
            "The learned weights."
        ],
        [
            "And if we can learn those two linear filters to line up with the axes of the ellipse.",
            "Then we can take the output of a linear filter.",
            "We can square it, we can wait it.",
            "For directions in which the... elongated, you want a small weight and for directions in which the... precise you want a big weight, and then that energy function.",
            "Represents the negative log probability under a full covariance matrix, where sort of 1 standard deviation of the full covariance matrix is shown by that ellipse.",
            "So then you just make the probabilities proportional to each of my side energy and you're sampling from that full covariance matrix, but it has zero mean."
        ],
        [
            "OK, you can think of each of those learn factors as a parabolic trough.",
            "So there's one parabolic troughs that goes along the ellipse and that is fairly sharp.",
            "It's got fairly high precision that corresponds to the green filter, and there's one parabolic troughs that goes across the ellipse and is very gentle and that corresponds to the red filter.",
            "That's why it has small weights and we synthesized the covariance matrix or the precision matrix.",
            "By adding together these two parabolic troughs.",
            "So what we'd really like to do is be able to synthesize the covariance matrix on the fly by adding together 1 dimensional ingredients like this.",
            "But if we can do that, then for each image we can create an appropriate covariance matrix for that image instead of just having one covariance matrix that applies to everything.",
            "And for images you don't want one covariance matrix because when the covariance breaks down, it breaks down big time and you couldn't possibly have a covariance that was heavy tailed enough to cope with that.",
            "If you're being Gaussian.",
            "So here's what we do.",
            "We take the outputs of those linear filters.",
            "Square them.",
            "And send them to a hidden unit.",
            "So this is just a re description of the three way model and we give that hidden unit a big positive bias.",
            "So now when the linear filter isn't producing any output, that's going to be because we have a nice smooth image and you win by plus B.",
            "That is your free energies minus B.",
            "As the filter starts producing output, you get a bit less smooth, so you start losing and you lose as you get less and less smooth.",
            "But when you start losing big time, you just turn off the hidden units so you just pay fixed price.",
            "So you say it's no longer smooth.",
            "Forget about smoothness.",
            "Don't add that contribute."
        ],
        [
            "To the precision matrix.",
            "Inference is still very simple.",
            "The hidden units are all independent given the pixel intensities, so we can infer the states of these hidden units that are modulating the precision matrix given the states of the hidden units.",
            "However, the pixels have correlations.",
            "That was the whole point of the hidden units, and so you can't simply do a reconstruction previously with the MOCAP data and the image transformation data we conditioned on the previous image.",
            "We didn't try and reconstruct, it was fixed.",
            "If you condition on the previous image life, simple, but if the two images are identical copies.",
            "So when you're trying to model the variance of a single image, you have to do something else to reconstruct, and we actually do."
        ],
        [
            "Hybrid Monte Carlo, that is, you could actually just invert the precision matrix, but it's a different one for every data point, so for big images that would take a long time.",
            "So we do hybrid Monte Carlo.",
            "What that amounts to is starting at the."
        ],
        [
            "Later.",
            "You going to?",
            "Run along a free energy surface that you get by integrating out the hidden units and you're going to simulate a particle that starts with some random momentum and moves along the free energy surface for 20 steps.",
            "And if you do the steps right where you can get numerical errors, cancel out and then when you got to the end you're going to say did I get numerical errors if I did throw it away and try again.",
            "If I didn't, that's good, and so I've now got a sample from 20 projects or hybrid Monte Carlo and will use that as the reconstruction.",
            "Then, given that reconstruction we activate the hidden units again and measure their statistics again, and the learning is just the pairwise statistics, you get between factors and units with the data and the pairwise statistics.",
            "Yet between factors and units after doing these repro steps, so it's just the same learning algorithm."
        ],
        [
            "And you could just use a 3 way PBM.",
            "But what the through PBM does is it learns the covariance structure nicely, but it doesn't learn the means.",
            "It always wants 0 mean, so it thinks the most likely image is just a completely flat image.",
            "Or maybe a completely sloping image, but very smooth image.",
            "It doesn't like to have complexity in the image, whereas the other PBM, the ones modeling, means it works not by getting unhappy when it sees smoothness being violated, but by getting happy when it sees familiar features.",
            "So it likes images that are busy with lots going on.",
            "Lots of familiar features.",
            "These two work very well together, so we're going to have some hidden units that use this 3 way RBM and model the precision matrix of the image and they get unhappy when they see violations of smoothness, other hidden units, the model, the means, and they're happy when they see familiar things.",
            "So on 16 by 16 natural image patches, these are the receptive fields of the hidden units, the model, the means, and they're very blurry.",
            "They sort of saying the image is sort of reddish hair and bluish there.",
            "The kind of thing you'd like if someone had drawn all the boundaries for you and you just had to sort of explain someone how to color it in.",
            "So to make it ready, share mega Bush over.",
            "There is sort of like water covering.",
            "That's what the means are doing.",
            "This is what the covariance units are doing, something totally different.",
            "And what I'm showing you here is the receptive fields of the factors.",
            "They learn typically high frequency filters.",
            "The high frequency filters are almost exactly pure pure Gray level.",
            "That is, all of these filters are actually produced by using."
        ],
        [
            "Using RGB values.",
            "Still very good and.",
            "It decides to have exactly the same values for RGB for almost all of the filters.",
            "But it has some low frequency filters that are colored.",
            "It uses red green opponent ones and yellow blue account ones, and it puts those all in the same space in the traffic map.",
            "And this looks quite a lot like what goes on in brains of animals, some animals.",
            "The map is typographic because the way Marcario learned this was to layout the factors in 2D allow the hidden units in 2D and then connect to hidden unit to factors that have similar locations.",
            "So every fact is connected to all the pixels, so the receptive fields are local.",
            "Becausw it learned that.",
            "But it forms the trouble graphic map because we wired in topography.",
            "But then it's not actually allowed to learn for each hidden unit.",
            "Which of the things that it can see it would like to."
        ],
        [
            "Really look at how much you would like to look at it, so it's actually learning everything in the end.",
            "If you take an image and you activate the hidden units for the means and you activate the hidden units for the correct precision matrix and then from those hidden units you reconstruct exact samples many times, you do that by inverting that precision matrix and sampling from the Gaussian.",
            "Then what's interesting is all these samples are rather different from one another.",
            "If your idea of similarity is squared, difference between pixel intensities, but they're all very much the same in terms of what they look like to us.",
            "So if you look at the middle Rd there.",
            "What's happening is it's got the same regions with the same edges in roughly the same locations, but is coloring in the regions a bit differently.",
            "What it knows is that if anyone region.",
            "That should be smooth.",
            "The color in that region should be the same all over more or less, but it can change what their color is.",
            "So again, if it's like the watercolor model of perception where you have edges and then you coloring regions, sort of roughly.",
            "But the coloring into the regions doesn't need to know exactly where the edges are.",
            "So the last thing we."
        ],
        [
            "It shows it's good for discrimination because as we all know, machine learning is about the scores you get in discrimination competitions.",
            "So here's the data set that's quite difficult.",
            "It's 32 by 32 color images.",
            "Um?",
            "Yeah, the square actually, but the somehow they've got non square.",
            "If you look at the first column that's planes and there's quite a lot of variation in the planes, look at the one just before the end.",
            "That's a very small plane on the plane.",
            "At the top has been writing in it, or look at the third column.",
            "That's birds.",
            "You get a sort of close up of an ostrich.",
            "That's a bird.",
            "You get birds in normal poses.",
            "You get a chick in.",
            "It's a difficult data set.",
            "These are images gleaned from the web.",
            "The images by using bird as a search term.",
            "Or sub categories of bird.",
            "The images are then shown to.",
            "Students in Toronto who have to say, if I saw this image, is there a single dominant object and there's a reasonable chance I would use the word bird to describe that object.",
            "And if there is, we say that's a good one.",
            "So now we can train on there's 80,000,000 unlabeled images and we train on a few million of those to learn early laser vision.",
            "And then we have 50,000 labeled ones.",
            "There's 5000 label ones of each category and then 1000 labeled test ones."
        ],
        [
            "So.",
            "What we'd like to have done is learned on the whole 32 by 32 Patch, but this was done a few weeks ago when the research was not as far along, and so we learned on 8 by 8 patches.",
            "This mean and covariance RBM and then just replicated across the image.",
            "If you get a postdoc from Youngs lab, he's always going to do convolutional things.",
            "It takes a long time for that to die out.",
            "So after you replicated you get all these hidden units.",
            "And.",
            "We then just simply give those to logistic regression.",
            "Obviously we could use something better, but for now we want to assess different ways of getting those hidden units.",
            "And so we're going to compare."
        ],
        [
            "The Gaussian binary RBM?",
            "That's the one that works fine if the pixels are decorrelated like they are in speech.",
            "We then compare using the same number of hidden units with some binary RBM.",
            "Well, that number of hidden units with just covariances, or that number of Indians with means anchor variances and will also try adding another layer to see if the greedy learning ideas still works.",
            "If you stack it on top."
        ],
        [
            "So if you just do a Gaussian binary RBM, you get just under 60% correct the through DBM, just for covariances get 62%.",
            "If you use a lot more factors.",
            "So each hidden unit is pooling.",
            "Many of these factors you get something more like a complex cell, and that gives you another 5%.",
            "If you add the means as well, but reduce the number of hidden units being used for the covariances.",
            "So it's always the same number of hidden units.",
            "You do better, and if you then take those binary vectors and you just do the sort of.",
            "Dumb deeplearning idea of let's learn another layer of 8000 units.",
            "Then you do even better.",
            "Now, I must admit that if you add another layer, you don't do any better than that, but you will.",
            "Once we've learned these 32 by 32 images instead of eight Berry patches, I believe.",
            "So, at least on one very difficult task, these three way interactions aren't just good for modeling the covariance structure and image.",
            "They actually give you a representation that's better for discrimination.",
            "So in summary."
        ],
        [
            "I started by showing you how it's relatively easy to learn a deep generative model by stacking up these restricted balls machines.",
            "I didn't sort of explain while that works, but it does.",
            "I then said we can actually make a more complicated module that has multiplicative interactions.",
            "If we do it in the naive way, we get cubically many parameters, but we're going to factor that.",
            "And now we can get a reasonable number of parameters, and once we factor it, the inference and the learning are all pairwise things, and they look just like a normal Boltzmann machine, except for this, multiplying together the outputs of two linear filters.",
            "And then I showed you that you can use that to get representations that are good for object recognition in small color images.",
            "And I'm done."
        ],
        [
            "With.",
            "Questions.",
            "Very loosely.",
            "It is, but my hearing is bad.",
            "By following.",
            "It seems that the more complexity you add, the better the system before you have any effects of over fitting into certain point, it becomes too complex and begin to deteriorate.",
            "OK, so the question was you using a whole load of parameters?",
            "Do you have problems overfitting?",
            "The answer is most of the parameters are trained as a generative model, and so we win in two ways.",
            "There we can train on unlabeled data so we can train on the whole labeled whole lot of data so we can use millions of unlabeled images.",
            "So that's one way to deal with overfitting.",
            "Also, for each training example.",
            "We are getting much more constraint because the model is trying to model what's going on in the image, not just this label.",
            "So getting much more constraint per case that helps with overfitting, but then in addition to all that, it looks like you can train about half as many parameters as you have pixels in your entire training set, and so it seems to be resistant to overfitting for other reasons I don't understand, But the biggest win is that you can train on lots of unlabeled data and so you can discover your multiple layers of feature detectors that needing to know anything about labels.",
            "And then the labels are just needed for fine tuning.",
            "You can use a small number of labels.",
            "That's not true for the speech example.",
            "The speech example that I told you didn't have any extra unlabeled data and it should work much better with extra unlabeled data.",
            "You can ask the question here alright, can you just say something about the computational complexity of training these things?",
            "Like how much time does it take?",
            "OK, how much time does it take?",
            "Well, the good news is everything is linear.",
            "That is, as you add more training data, there's nothing quadratic in the size of the training data, and as you add more hidden units apart from the connection matrices which quadratically with the size of the layers in terms of the number of connections, everything is linear.",
            "But you have to train it for a long time.",
            "So the speech example.",
            "Took several days on a GPU board.",
            "The image example probably takes about a day on a GPU board and a day on a GPU board is about a month on a single core.",
            "So basically if you want to do research in this kind of machine learning, you have to get yourself a workstation that has a Generation 2 PCI 16 slot and you have to plug in an NVIDIA GTX 285 board and then you're in business.",
            "And I told him very if it gave me a free one.",
            "I say all this, but they didn't give me a free one.",
            "But it's such a good board that I'm saying it anyway.",
            "Yeah, if there's no other questions, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of, our plasma electricity.",
                    "label": 0
                },
                {
                    "sent": "This year Jeff Hinton going back in his current little bit.",
                    "label": 0
                },
                {
                    "sent": "Jeff moved to UCSD in the early 1980s and became part of the parallel distributed, or PDP research Group he Co invented Boltzmann machines with Terry Sinofsky in 1983 and then one of the researchers who introduced back propagation algorithm for training multilayer neural networks.",
                    "label": 0
                },
                {
                    "sent": "The PDP books in 1986 led to an explosion in the growth of neural networks and actually the NIPS.",
                    "label": 0
                },
                {
                    "sent": "Conference started in 1987.",
                    "label": 0
                },
                {
                    "sent": "In that kind of that wave of excitement around those books, Jeffers received many awards, including the Fellowship, the Royal Society.",
                    "label": 0
                },
                {
                    "sent": "In 1998, the David Rumelhart Prize in 2001, and the Sky Award for Research Excellence in 2005.",
                    "label": 0
                },
                {
                    "sent": "Since the 1980s, Jeffers continued to be highly innovative with contributions such as variation, learning the Helmholtz machine in the product of experts architecture.",
                    "label": 0
                },
                {
                    "sent": "Most recently, he's been he's been working on deep belief networks for unsupervised learning, and today he'll be talking about deep learning with multiplicative interactions.",
                    "label": 1
                },
                {
                    "sent": "Thank you, Jack, thank you.",
                    "label": 0
                },
                {
                    "sent": "So today I'm going to spend the first 10 minutes or so, giving you a background about deep learning.",
                    "label": 0
                },
                {
                    "sent": "I'm afraid many of you know this stuff, but I want to give the general background and then I'm going to show you one recent example where the ideology of deep learning worked perfectly.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm then going to talk about how to make the basic learning module work better by introducing multiplicative interactions into a restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say why we need to do that and how we keep the number of parameters under control and show you how the inference and learning is still straight, relatively straightforward, and then you describe 3 examples from students in my lab of using the multiplicative interactions one is learning to transform images, that is, learning how images change every time another is having a model of human motion and using a style label to transform that model so it can cope with many different.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nations and the last is modeling the covariance structure of images so that you can do better discrimination.",
                    "label": 0
                },
                {
                    "sent": "So restricted Boltzmann machine consists of two layers of neuron like units.",
                    "label": 1
                },
                {
                    "sent": "Their binary stochastic units and there's no connections between the hidden units and also no connections between the visible units.",
                    "label": 1
                },
                {
                    "sent": "Given the states of the visible units, which is where you put the data, the hidden units are conditionally independent, so it's very easy to do inference in this kind of machine, and that's good news.",
                    "label": 1
                },
                {
                    "sent": "The rule for turning on a hidden unit is simply, you compute.",
                    "label": 0
                },
                {
                    "sent": "The input is getting from the visible units.",
                    "label": 0
                },
                {
                    "sent": "Put that through the logistic function and that tells you the probability of turning it on, and then you make a stochastic decision about whether it should be on and their independent scattered.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Visions from the hidden units.",
                    "label": 1
                },
                {
                    "sent": "This network obeza Hopfield energy function, where the energy if I tell you the states of the visible and hidden units, the energy of the network is quadratic in those states for all pairs of a visible and hidden both on you take the weight between you add up all those weights.",
                    "label": 1
                },
                {
                    "sent": "And that tells you.",
                    "label": 0
                },
                {
                    "sent": "The negative energy low energy is good.",
                    "label": 0
                },
                {
                    "sent": "The derivative of the energy function is nice and simple, it's just the product of the visible in activity.",
                    "label": 0
                },
                {
                    "sent": "That's the derivative with respect to weight, which you need for learning.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can use those energies to define probabilities over joint configurations of visible and hidden units.",
                    "label": 1
                },
                {
                    "sent": "And the probability of a joint configuration is proportional to each of the minus energy, but that's normalized by the probability of all possible configurations.",
                    "label": 1
                },
                {
                    "sent": "That's called the partition function, and it makes learning tricky.",
                    "label": 0
                },
                {
                    "sent": "To know the probability that you assign to a visible vector V, you have to add up over all possible states of the hidden units.",
                    "label": 0
                },
                {
                    "sent": "That's the bottom line.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The maximum likelihood learning algorithm for restricted Boltzmann machine is conceptually very simple, but takes a long time.",
                    "label": 1
                },
                {
                    "sent": "You start with data on the visible units at time T0 and then using whatever weights you currently have, you activate the hidden units stochastically and then if you were to do it right, she would then activate the visible units again stochastically and you go backwards and forwards.",
                    "label": 1
                },
                {
                    "sent": "This is a Markov chain, it's doing alternating block Gibbs sampling and when you run long enough you forgotten where you started and you're now sampling from.",
                    "label": 0
                },
                {
                    "sent": "The visible patterns that the model believes in and for those visible patterns that distribution model believes in, you compute the pairwise statistics.",
                    "label": 0
                },
                {
                    "sent": "You compute how often a pixel and feature take drawn together and the learning rule is just the difference between those pairwise statistics with data and with fantasies from the model.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, you have to run the Markov chain for a long time.",
                    "label": 0
                },
                {
                    "sent": "So I discovered empirically that if you're on the market.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Change for a short time.",
                    "label": 0
                },
                {
                    "sent": "It still works pretty well and it's much quicker.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is just activate the hidden units.",
                    "label": 0
                },
                {
                    "sent": "Allow the hidden units to reconstruct a patent on the visible units, typically going to be quite like the data.",
                    "label": 1
                },
                {
                    "sent": "Then activate the hidden units again and the difference of those two power statistics will tell you how to change your weight, and then we'll just do stochastic gradient descent to try and get a good set of weights.",
                    "label": 0
                },
                {
                    "sent": "The main reason restricted Boltzmann machines interesting is because you can use it recursively.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To learn a deep net, it turns out that the deep net you end up learning is a directed network, which is a big surprise to people in graphical models, and you're learning a directed network one layer at a time.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into that any further, I'll just give you the sort of how you use it.",
                    "label": 0
                },
                {
                    "sent": "You train one layer of features and then you treat the activations of those features as data when you train another layer and you just keep going like that for as many layers as you can stand.",
                    "label": 1
                },
                {
                    "sent": "And when you finished the top two layers, a restricted Boltzmann machine, that's an associative memory and all the layers below can be thought of as a directed graphical model directed so that you can generate data from the associative memory via some transformations in the other hidden layers.",
                    "label": 1
                },
                {
                    "sent": "We'd like to prove that every time you add another hidden layer, you get a better model of the data.",
                    "label": 0
                },
                {
                    "sent": "We can't quite prove that, but we can almost prove it every time you add another hidden layer.",
                    "label": 0
                },
                {
                    "sent": "If you do it right, you improve a variational bound on the log probability that the model will generate the training data.",
                    "label": 1
                },
                {
                    "sent": "And that's good enough to show you doing something sensible.",
                    "label": 0
                },
                {
                    "sent": "Off",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Steve run multiple layers.",
                    "label": 0
                },
                {
                    "sent": "You can then stick some label units on top.",
                    "label": 1
                },
                {
                    "sent": "And do back propagation.",
                    "label": 0
                },
                {
                    "sent": "Now, backpropagation is now not having to design features.",
                    "label": 0
                },
                {
                    "sent": "The features already designed by the unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "The multiple lesser Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "Backpropagation is doing is slightly changing the weights, it hardly changes of each of the tool so that the boundaries between classes are in the right places.",
                    "label": 1
                },
                {
                    "sent": "So it's slightly adjusting the features to be better for discrimination, but it's not having to discover the features, so the unsupervised learning gets you into a good part of white space and then the back propagation within that good part of white space does local myopic steepest descent to tune up the weights for discrimination.",
                    "label": 1
                },
                {
                    "sent": "Experiments in Benjy's lab show that this wins in two ways.",
                    "label": 0
                },
                {
                    "sent": "The unsupervised learning gets you to a better part of white space than you would have got too if you just did supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And you actually.",
                    "label": 0
                },
                {
                    "sent": "Then get better generalization as well, so you're winning both on the optimization and speed and the generaliza.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is probably the most important slide in the talk because it sort of challenges what most people in machine learning at least used to believe.",
                    "label": 0
                },
                {
                    "sent": "So it uses a technical concept that I got from Donald Rumsfeld called stuff.",
                    "label": 0
                },
                {
                    "sent": "And so the way the world works is the stuff it happens and stuff produces images.",
                    "label": 0
                },
                {
                    "sent": "And if you then believe that labels are produced from images and so the label is conditionally independent of the stuff given the image, then the right way to do machine learning to discriminate objects and images for example, is to try and learn the function from images to labels.",
                    "label": 1
                },
                {
                    "sent": "But almost nobody believes that.",
                    "label": 0
                },
                {
                    "sent": "What you ought to believe is that stuff creates images and the same stuff creates labels.",
                    "label": 0
                },
                {
                    "sent": "What's more, the bandwidth between stuff and images is high.",
                    "label": 0
                },
                {
                    "sent": "That is, if I show you an image, you can tell me a lot about the stuff that created it, like if it's a dog, you can tell me how big the dog is and what color it is and what direction is facing and whether it's jumping up in the air.",
                    "label": 1
                },
                {
                    "sent": "If I give you a label like dog, you can tell me hardly anything about the stuff that created it, so that's a very low bandwidth path.",
                    "label": 1
                },
                {
                    "sent": "So now if you want to get from image to label, you're completely crazy to try and learn a direct mapping from image to label what you want to do is learn how to get the stuff from the image.",
                    "label": 0
                },
                {
                    "sent": "How to invert that high bandwidth path 'cause the image specifies this stuff quite well.",
                    "label": 0
                },
                {
                    "sent": "Once you've learned that, then learn how to get the label from the stuff and maybe slightly tune up the way in which you get stuff from images.",
                    "label": 0
                },
                {
                    "sent": "That's a different view of how you're going to do.",
                    "label": 0
                },
                {
                    "sent": "Discriminative machine learning.",
                    "label": 0
                },
                {
                    "sent": "You're first going to figure out what's going on, and then you're going to do the learning and the interesting bit is figuring out what's going on.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to describe one application which demonstrates that that approach works.",
                    "label": 0
                },
                {
                    "sent": "My contribution to this was to thoroughly indoctrinate one very smart student.",
                    "label": 0
                },
                {
                    "sent": "He then met another student who knew all about speech and told him about the TIMIT phone recognition problem, and he said, OK, what do I use as input?",
                    "label": 0
                },
                {
                    "sent": "And I'm sort of simplifying this a bit, but both students are here so they can correct it later.",
                    "label": 0
                },
                {
                    "sent": "Use the standard input, which is 39 coefficients per frame and 11 frames.",
                    "label": 1
                },
                {
                    "sent": "Then you have to predict the phone label of the middle frame.",
                    "label": 0
                },
                {
                    "sent": "Once you predicted those phone labels, they're going to go into a post process so that user bigram model of phones.",
                    "label": 0
                },
                {
                    "sent": "To try and predict what's really going on.",
                    "label": 0
                },
                {
                    "sent": "So the student said, well, why don't I take these 39 * 11 input variables and use 2000 hidden units and learn that unsupervised, and then another 2010 units?",
                    "label": 0
                },
                {
                    "sent": "And that unsupervised and then another 2000 units are not unsupervised?",
                    "label": 0
                },
                {
                    "sent": "If he told me, I'd tell him to stop there, but he said and another 2010 units and learn that unsupervised, then stick 100 units on top and from those or learn all of that unsupervised and then go to 183 context specific phone labels, which is very standard.",
                    "label": 0
                },
                {
                    "sent": "An but do fine tuning.",
                    "label": 0
                },
                {
                    "sent": "Fine tuning takes a few days on a very fast GPU board, but it turns out that just wipes out the record on the TIMIT recognition problem.",
                    "label": 1
                },
                {
                    "sent": "The previous record was 24.4% and this gets 23%.",
                    "label": 0
                },
                {
                    "sent": "More importantly, all variations he tries that are along these lines get between 23 and 24%, so they will beat the record.",
                    "label": 0
                },
                {
                    "sent": "Andrew Yang has a student called Hung Likely who's recently applied similar techniques to the classification task where you're given the boundaries of the phone and you told all these frames of the same phone.",
                    "label": 0
                },
                {
                    "sent": "Can you classify it and he's round about the record for.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classification task, although I think Michael Collins has something a little bit better.",
                    "label": 0
                },
                {
                    "sent": "So this is the net retrained.",
                    "label": 0
                },
                {
                    "sent": "It couldn't be a better fit to the ideology.",
                    "label": 0
                },
                {
                    "sent": "You just take the input.",
                    "label": 0
                },
                {
                    "sent": "You have lots and lots of hidden layers.",
                    "label": 0
                },
                {
                    "sent": "You need a big data set of course.",
                    "label": 0
                },
                {
                    "sent": "Only the last layer weights is not pre trained and that's why the penultimate layer needs to be small to keep that number small.",
                    "label": 0
                },
                {
                    "sent": "And then you just train the hell out of it with back prop and it wins.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, in order to do that, they had to be able to model.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The input, which was real valued, Mel caps from coefficients and so the bolts machine.",
                    "label": 0
                },
                {
                    "sent": "I told you about has binary visible units and binary hidden units.",
                    "label": 1
                },
                {
                    "sent": "You can revise that to have real valued visible units and binary hidden units.",
                    "label": 0
                },
                {
                    "sent": "We simply call that the Gaussian RBM.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You make a very naive assumption, which is that given the states of the hidden units, the visible units are conditionally independent, so it's not good for data with high covariances in But the Mail capturing coefficients are designed to get rid of the covariances, so it's not so bad for that data.",
                    "label": 1
                },
                {
                    "sent": "You can write down an energy function.",
                    "label": 0
                },
                {
                    "sent": "The pictures better around each visible unit you put a parabolic containment function.",
                    "label": 1
                },
                {
                    "sent": "You say it's energy to move away from where that visible unit would like to be, and then the effect of the hidden units is to impose an energy gradient that's trying to move it away, and so the lowest energy point will be where the gradient of the blue line is equal and opposite to the gradient of the red line, and so it just moves the parabola sideways.",
                    "label": 0
                },
                {
                    "sent": "That's the simple model, and that's not a very good model of data with covariances in at the end of the talk, I'll come back come to a much better.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the new idea is that the basic module that has either binary or real value visible units and binary hidden units have something deeply wrong with it.",
                    "label": 0
                },
                {
                    "sent": "Which is, it can't deal with multiplicative interactions and the world is full of multiplicative interactions.",
                    "label": 1
                },
                {
                    "sent": "Preeminent enbom along time ago had a paper on style and content which is about multiplicative interactions.",
                    "label": 0
                },
                {
                    "sent": "But all the work on tensor faces about multiplicative interactions.",
                    "label": 1
                },
                {
                    "sent": "Whenever you get 2 Gaussian things in the world, two Gaussian distributed variables and you multiply them together, you get a variable that's very non Gaussian distributed.",
                    "label": 0
                },
                {
                    "sent": "It's got very heavy tails, and that's probably why the world is full of heavy tailed things.",
                    "label": 0
                },
                {
                    "sent": "Even if you believe in ghosts in distributed things.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to give you one example of why I want these multiplicative interactions, and it's from a generative perspective.",
                    "label": 0
                },
                {
                    "sent": "If you're building generative models that are multi layer and you want to generate some interesting data.",
                    "label": 0
                },
                {
                    "sent": "Let's consider the problem of generating an image of a shape.",
                    "label": 0
                },
                {
                    "sent": "When I just tell you that it's a square and it's got particular post parameters where it is and what orientation it said.",
                    "label": 0
                },
                {
                    "sent": "Well, you could generate where the parts are the edges of the square very accurately.",
                    "label": 1
                },
                {
                    "sent": "If you had accurate post parameters, and then maybe you can maintain the constraints between the parts.",
                    "label": 1
                },
                {
                    "sent": "But if you do any kind of sloppy generation of the parts, then the edges won't quite meet and the corners won't quite be right angles and you'll get a very bad looking square.",
                    "label": 0
                },
                {
                    "sent": "So if you use a top down graphical model.",
                    "label": 0
                },
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "Basically Gaussian model and you try and generate me praying like this, it's an unlikely configuration, but there you are.",
                    "label": 0
                },
                {
                    "sent": "You're liable to generate something like this or like this unless you have very very accurate generation at each stage.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The alternative is to sloppy generation, but also specify how to clean things up.",
                    "label": 0
                },
                {
                    "sent": "And that's a much better way to go.",
                    "label": 0
                },
                {
                    "sent": "So here's the picture.",
                    "label": 0
                },
                {
                    "sent": "You know the shape and you know the pose.",
                    "label": 0
                },
                {
                    "sent": "You do sloppy generation and probably have a redundant set of parts.",
                    "label": 0
                },
                {
                    "sent": "And then you specify how the part should interact, and now you let that thing settle down.",
                    "label": 0
                },
                {
                    "sent": "So in generation it's going to be slow.",
                    "label": 0
                },
                {
                    "sent": "It's going to settle down and now you get a very clean structure.",
                    "label": 0
                },
                {
                    "sent": "It's like when an officer wants to get a bunch of soldiers on the parade ground to stand and eat rectangle.",
                    "label": 1
                },
                {
                    "sent": "He could give every soldier his GPS coordinates down to within a few inches and that will be kind of hard work.",
                    "label": 0
                },
                {
                    "sent": "Or you could tell them roughly where to stand and then tell each soldier what distance should be from his neighbor.",
                    "label": 0
                },
                {
                    "sent": "That's a much better thing to do.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're going to try and make a more powerful module for doing this.",
                    "label": 0
                },
                {
                    "sent": "Deep learning, and in this more powerful module we'd like the units in one layer to modify to modulate the interactions in the layer below.",
                    "label": 1
                },
                {
                    "sent": "So in the level we want to have something like random field, and we want the units in the lower back to specify the weights in that market random field.",
                    "label": 0
                },
                {
                    "sent": "Some previous work in NIPS a few years ago with Simon and Arrow had had a fixed Markov random field with units in the layer, but just specifying the biases, and that's already a big win.",
                    "label": 0
                },
                {
                    "sent": "But we're now after something more ambitious to actually modulate that Markov random field.",
                    "label": 1
                },
                {
                    "sent": "To do that, we need higher order Boltzmann machines.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I hired a Boltzmann machine, Terry Sinofsky, first thought about these in 1986 I think, or thereabouts.",
                    "label": 0
                },
                {
                    "sent": "Has interaction terms that are more than quadratic.",
                    "label": 0
                },
                {
                    "sent": "So the second equation there is for three way, one, and that's all we're going to use for now.",
                    "label": 0
                },
                {
                    "sent": "And if you think about this, the state of Unit K that SK in the second equation when that's off.",
                    "label": 1
                },
                {
                    "sent": "The term doesn't apply when it's on, when it's got a state of 1.",
                    "label": 0
                },
                {
                    "sent": "Now you've got a pairwise interaction between I&J, so the state of SK is gating pairwise interactions between I&J.",
                    "label": 1
                },
                {
                    "sent": "On each unit can be viewed like that is gating powers interactions between the other guys.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing we could use a hard about some machine for is modeling image transformations.",
                    "label": 1
                },
                {
                    "sent": "I could give you pairs of images.",
                    "label": 0
                },
                {
                    "sent": "The simple case is when there's a global transformation.",
                    "label": 1
                },
                {
                    "sent": "The more interesting case is when there's more local transformations and several transformations going on at once in the same image, and we can cope with that just fine.",
                    "label": 0
                },
                {
                    "sent": "So I give you pairs of images.",
                    "label": 0
                },
                {
                    "sent": "And the hidden units are now going to gate interactions between pixels, so where those little triangle symbols are will have a 3 way wait.",
                    "label": 0
                },
                {
                    "sent": "That gives you an energy as a function of the states of the two pixels and the state of the hidden unit.",
                    "label": 0
                },
                {
                    "sent": "And that red hidden unit likes the two transformations of pixels that sees 'cause they're consistent.",
                    "label": 0
                },
                {
                    "sent": "They correspond to a consistent translation, and so if you have the image of time T in the image of time T + 1.",
                    "label": 0
                },
                {
                    "sent": "Both those pairings will provide evidence about hidden units should be on, and so you'll tend to say that's a consistent transformation.",
                    "label": 0
                },
                {
                    "sent": "Conversely, if you have the image at time T and you turn on that hidden unit, it will tell you where the Pixel should translate to in the image in time T + 1.",
                    "label": 0
                },
                {
                    "sent": "So Rolla Mesa vision.",
                    "label": 0
                },
                {
                    "sent": "I had a paper that uses this and has a huge number of weights because you see, those triangle symbols have three indices.",
                    "label": 0
                },
                {
                    "sent": "They connect three different units, and so there's cubically many of them.",
                    "label": 0
                },
                {
                    "sent": "So the sort of central trick to this talk is that we're going to take that cubically many parameters, and we're going to just use factorization, 'cause we don't.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually believe that we want hidden units to be able to cause arbitrary permutations of the pixels.",
                    "label": 0
                },
                {
                    "sent": "We believe this regular structure there, so we don't really need that many parameters.",
                    "label": 0
                },
                {
                    "sent": "So we're going to take the three way.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right wih and we're going to factor it as a product.",
                    "label": 0
                },
                {
                    "sent": "Of three weights WIFWJF&WHF.",
                    "label": 0
                },
                {
                    "sent": "Per factor, we're going to have a bunch of factors.",
                    "label": 0
                },
                {
                    "sent": "So it looks like we've got four indices now, and that's true, but notice we haven't got anything that has more than two indices on it.",
                    "label": 0
                },
                {
                    "sent": "So basically we've turned a cube into three pairwise things.",
                    "label": 0
                },
                {
                    "sent": "That's a win.",
                    "label": 0
                },
                {
                    "sent": "We only get linearly many parameters per factor.",
                    "label": 1
                },
                {
                    "sent": "And we have about as many factors as there are hidden units or visible units.",
                    "label": 0
                },
                {
                    "sent": "So we basically turn something this cubic into something that's roughly quadratic.",
                    "label": 0
                },
                {
                    "sent": "Here's a picture of what we're doing.",
                    "label": 0
                },
                {
                    "sent": "Each factor corresponds to a tensor weights.",
                    "label": 0
                },
                {
                    "sent": "This only got rank one.",
                    "label": 0
                },
                {
                    "sent": "'cause it's the outer product of three vectors.",
                    "label": 0
                },
                {
                    "sent": "And then you just add up a bunch of those tensors.",
                    "label": 0
                },
                {
                    "sent": "Now if I give you enough tensors, then you can add up to make any tensor you like.",
                    "label": 0
                },
                {
                    "sent": "Any 3 way thing you like.",
                    "label": 0
                },
                {
                    "sent": "But if I give you a limited number, you can only make things that have sort of regular structure.",
                    "label": 0
                },
                {
                    "sent": "This is like PCA, but for three 3 dimensional things instead of two dimensional things.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as an equation we.",
                    "label": 0
                },
                {
                    "sent": "We turned the three weight into a product of 3.",
                    "label": 0
                },
                {
                    "sent": "Factor specific weights.",
                    "label": 0
                },
                {
                    "sent": "And then we can factorize that top line equation and we see that the energy contributed by a given factor F is just the product of three weighted sums.",
                    "label": 0
                },
                {
                    "sent": "For each group of units you're connecting, you take a weighted sum.",
                    "label": 0
                },
                {
                    "sent": "Since you apply linear filter and then the energy is the product of all those three linear filters.",
                    "label": 0
                },
                {
                    "sent": "So now for a hidden unit age, if you wanted to know whether I should turn it on or not, you'd need to know the difference in the energy of the whole system as a function of whether H is on or off.",
                    "label": 0
                },
                {
                    "sent": "So just considering the energy contributed by factor F, What does factor F need to tell hidden unit H in order to reach to decide how to adopt A state that will give the system low energy?",
                    "label": 1
                },
                {
                    "sent": "Well, H needs to know the difference in the energy of the whole system as a function, whether it's on or off.",
                    "label": 0
                },
                {
                    "sent": "Which is going to be the sum of the energies from all the factors of the energy contributed by factor.",
                    "label": 1
                },
                {
                    "sent": "So factor F. We want to know the difference in energy contributed by factor F when H is on and off.",
                    "label": 0
                },
                {
                    "sent": "And so you take the right hand side on the top line.",
                    "label": 0
                },
                {
                    "sent": "And just differentiate it with respect to SH.",
                    "label": 0
                },
                {
                    "sent": "That will give you the right chance even that's the wrong thing to do, and what you get is that the WHF comes out the front and so you see that what hidden unit H needs to see is the product of the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other two weighted sums, multiplied by the weight on the connection between the factor and H. So we can draw that in a picture.",
                    "label": 0
                },
                {
                    "sent": "The factor at each of its three vertices computes a weighted sum.",
                    "label": 1
                },
                {
                    "sent": "And what H needs to see is the product of the weighted sums at the bottom two vertices and that needs to be sent to H and on its way to H it needs to be multiplied by WHF in authority.",
                    "label": 1
                },
                {
                    "sent": "Connectionist way all hidden units need to see that same message, but each way to buy the weight on the connection.",
                    "label": 0
                },
                {
                    "sent": "If you look at learning.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then that message that goes from the factor to H. Is what you need for doing learning and the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "The Bolts machine learning outcome stays just the same.",
                    "label": 0
                },
                {
                    "sent": "The learning algorithm basically says I want to lower the energy.",
                    "label": 0
                },
                {
                    "sent": "When I'm looking at data and I want to raise the energy when I'm generating fantasies from the model.",
                    "label": 0
                },
                {
                    "sent": "And the derivative of the energy respect to work when I'm looking at data.",
                    "label": 0
                },
                {
                    "sent": "Is just the product of the state of a hidden unit and the message that comes to it from a factor.",
                    "label": 0
                },
                {
                    "sent": "And the derivative of the energy when you generated from the model is that same product.",
                    "label": 0
                },
                {
                    "sent": "But when you generated from the model and so the difference of those two products is what you need in order to learn the weight.",
                    "label": 0
                },
                {
                    "sent": "So who I'm ahead of 2:30?",
                    "label": 0
                },
                {
                    "sent": "That's great so.",
                    "label": 0
                },
                {
                    "sent": "You get exactly the same pairwise learning rule, so we took these three way interactions.",
                    "label": 0
                },
                {
                    "sent": "We factored everything and now everything becomes pairwise, but with these messages that are products of weighted sums, and that's where the multiply gets into the act.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm going to show you an example for translating images.",
                    "label": 0
                },
                {
                    "sent": "We're going to have a pre image and we're going to post image and we're going to train it up on lots of pairs of images and to begin with, the images will just be random dot sparse random dots and all will do is just translate the random dots will have a number of different translations and that's the training data.",
                    "label": 0
                },
                {
                    "sent": "And we'll learn the weights to the factors will have a whole bunch of factors, will learn the weights and factors and the weights between factors in units will learn everything.",
                    "label": 0
                },
                {
                    "sent": "We start with small random weights and just learn it.",
                    "label": 0
                },
                {
                    "sent": "And after it's learned what I'm going to show you is.",
                    "label": 0
                },
                {
                    "sent": "The weights that a factor has to the pre image and the weights that are factor has to post image.",
                    "label": 1
                },
                {
                    "sent": "That is these two filters that multiplied together and sent to a hidden unit.",
                    "label": 0
                },
                {
                    "sent": "And of course, there's going to be a lot of factors, so I'm going to show you the weights for the pre and post image for all.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Practice, so here's the weights to the preimage of all the factors.",
                    "label": 0
                },
                {
                    "sent": "For those of you know about translation and free analysis and stuff like this, this should be.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not a surprise.",
                    "label": 0
                },
                {
                    "sent": "And here's the weights to the post image.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pre image.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First image, pre image post image, pre image post image.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's learning basically the Fourier basis, and it's learning pairs of ratings that are about 90 degrees out of phase and which seems like a sensible thing to do, and now it can represent translations very nicely.",
                    "label": 0
                },
                {
                    "sent": "Instead of translating images, we could rotate images so we take the same random dot patterns and we rotate them.",
                    "label": 1
                },
                {
                    "sent": "So there.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The pre image.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's the post image.",
                    "label": 0
                },
                {
                    "sent": "Pre image.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Postimage",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look about halfway down the left hand side is learned about Yang and Yang, whoops.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "That's the post image.",
                    "label": 0
                },
                {
                    "sent": "Pre image post image pre image.",
                    "label": 0
                },
                {
                    "sent": "You might call this seeing machine, but that would be confusing.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what we did Alan Jackson then suggested something.",
                    "label": 0
                },
                {
                    "sent": "He said well, you've trained it on sparse translations.",
                    "label": 0
                },
                {
                    "sent": "Why don't you give it transparent images so you give it sparse?",
                    "label": 0
                },
                {
                    "sent": "Depends, but some dots translate one way and some dots translate another way.",
                    "label": 0
                },
                {
                    "sent": "And we know what happens when you show those to people.",
                    "label": 0
                },
                {
                    "sent": "If the two directions of translation is quite similar, within about 30 degrees they see one motion.",
                    "label": 1
                },
                {
                    "sent": "If there are more than about 30 degrees, they see two motions and the motions are repelled from each other, just slightly.",
                    "label": 0
                },
                {
                    "sent": "So Luckily the Roland didn't know that when he did this work and he was slightly irritated that they were slightly repelled in his data, but that was good news really.",
                    "label": 0
                },
                {
                    "sent": "It's good news having a model that doesn't work if you're a psychologist because maybe people don't work the same way.",
                    "label": 0
                },
                {
                    "sent": "So we train it on translation, but just one translation of time.",
                    "label": 0
                },
                {
                    "sent": "Then we had a second hidden layer which we encourage to be sparse.",
                    "label": 1
                },
                {
                    "sent": "All unsupervised II inlay learns units that are tuned to direction of motion.",
                    "label": 1
                },
                {
                    "sent": "So each unit has a preferred direction in the georgopoulos kind of away, and you can then decide what this network perceiving by taking all those units in the second in there that have tuned directions and adding up the beliefs of all the ones that are on.",
                    "label": 0
                },
                {
                    "sent": "Actually, you wait how?",
                    "label": 0
                },
                {
                    "sent": "How much they want to be on by what they believe, and then you're going to distribution over directions it believes in, but everything is totally unsupervised and the answer is when you presented two dot patterns, if there within about 30 degrees it sees one motion, and if there are more than about 30 degrees apart, it sees two motions.",
                    "label": 1
                },
                {
                    "sent": "It gets 2 peaks in what it believes and they're slightly repelled, and so that proves that this is how the brain works.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm now going to go to.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Time series models.",
                    "label": 0
                },
                {
                    "sent": "That's a very.",
                    "label": 0
                },
                {
                    "sent": "That's one kind of time series model.",
                    "label": 1
                },
                {
                    "sent": "Most people in our community when they want to model time series would actually like to use nonlinear distributed representations, 'cause they're much more powerful and interesting, but they're kind of hard to learn, especially if you use directed models.",
                    "label": 1
                },
                {
                    "sent": "And So what they do is they fall back on the old faithfuls which are give up on distributed and run the hidden Markov model will give up on nonlinear nonlinear dynamical system.",
                    "label": 0
                },
                {
                    "sent": "Now you could go a little bit further.",
                    "label": 0
                },
                {
                    "sent": "Another mixture of linear ankle systems that will get you a little bit further.",
                    "label": 0
                },
                {
                    "sent": "But what we're going to do is learn a.",
                    "label": 0
                },
                {
                    "sent": "We're going to give up on mathematics and learn a thoroughly nonlinear distributed representation.",
                    "label": 0
                },
                {
                    "sent": "So it's going to look like this.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the right hand side you see the standard restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "It has some visible units and some hidden units.",
                    "label": 0
                },
                {
                    "sent": "And the states of those units are conditioned on previous time slices.",
                    "label": 0
                },
                {
                    "sent": "So we we regard the history as fixed and not changeable anymore those previous time slices have conditioning connections which in effect change the biases of the visible units.",
                    "label": 0
                },
                {
                    "sent": "That's called an autoregressive model.",
                    "label": 0
                },
                {
                    "sent": "If you use linear visible units.",
                    "label": 0
                },
                {
                    "sent": "And they also have conditioning connections, which change the biases of the hidden units.",
                    "label": 1
                },
                {
                    "sent": "So those are now dynamic biases that depend on the previous data.",
                    "label": 1
                },
                {
                    "sent": "You can learn this by you put data in given the data.",
                    "label": 1
                },
                {
                    "sent": "The previous data in the current data, the hidden units, all independent so you can sample their states.",
                    "label": 0
                },
                {
                    "sent": "You then reconstruct just the current visible data.",
                    "label": 0
                },
                {
                    "sent": "And then you activate the hidden units again and use the differences in statistics and you take the learning rule.",
                    "label": 0
                },
                {
                    "sent": "You would have used to change the biases of the hidden units and visible units and you back propagate that derivative to change the weights coming from previous states of visible units.",
                    "label": 0
                },
                {
                    "sent": "So you can run all of these connections quite simply using.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Contrastive divergent.",
                    "label": 0
                },
                {
                    "sent": "Once you've learned you can generate from the model and the way you generate from the model is you have to initialize it with a few sensible frames and then given the previous frames what they do is simply determine the biases of the visible and hidden units, and then you do alternating Gibb sampling.",
                    "label": 1
                },
                {
                    "sent": "But you don't have to go for very long, 'cause with strongly determined biases there aren't many places it can really go to.",
                    "label": 0
                },
                {
                    "sent": "It has to fit in with what just happened, so you don't have to go for huge lengths of time and then you just sample the state of the visible units and that's your next visible frame and so you.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Keep sampling.",
                    "label": 0
                },
                {
                    "sent": "You can actually stack these models up so you can learn one model.",
                    "label": 0
                },
                {
                    "sent": "You now have states for the hidden units.",
                    "label": 1
                },
                {
                    "sent": "You treat those as data.",
                    "label": 1
                },
                {
                    "sent": "Once their data, you can now put in auto regressive connections between them.",
                    "label": 0
                },
                {
                    "sent": "And add another hidden layer and learn the next layer.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, you'll get a better model.",
                    "label": 0
                },
                {
                    "sent": "Anne Graham Taylor shown that if you're doing this for modeling workout data, adding a second layer makes it work much better.",
                    "label": 1
                },
                {
                    "sent": "That's not what we're going to do, though.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You apply this kind of modeling to motion capture data, but we're going to do it with these three way interactions, which allow you multiplicative things.",
                    "label": 0
                },
                {
                    "sent": "So you can capture human motion by putting markers on the joints and having infrared cameras and figure out where they were in space and figure out the joint angles and so.",
                    "label": 1
                },
                {
                    "sent": "Then each frame is going to be a bunch of joint angles which are real values, so we're going to use a Gaussian binary RBM to begin with.",
                    "label": 0
                },
                {
                    "sent": "Well, that's where I'm used to begin with, but we got something better now.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "I retract that remark.",
                    "label": 0
                },
                {
                    "sent": "We're basically using it Gaussian binary RBM for the visible units.",
                    "label": 1
                },
                {
                    "sent": "We also add in the translation of the base of the spine and the rotation of the base of the spine for rotation we do the actual angles for the pitch and the roll, but the change in angle for the euro, 'cause gravity doesn't care about the absolute value of your.",
                    "label": 0
                },
                {
                    "sent": "And so now we're going to do a 3 way version of the model I showed you before.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Take the basic model and modulate all the interaction matrices using a style variable so the style variable is a one event style.",
                    "label": 1
                },
                {
                    "sent": "That gets expanded into a real valued vector of 100 style features.",
                    "label": 0
                },
                {
                    "sent": "Those style features are used as one of the inputs to a factor, so each factor is looking at some linear combination of style features, some weighted linear combination of those.",
                    "label": 0
                },
                {
                    "sent": "And then the.",
                    "label": 0
                },
                {
                    "sent": "Those style features are in effect gating the interactions between the earlier frames.",
                    "label": 0
                },
                {
                    "sent": "In the hidden units and between the earlier frames and the current visible frame.",
                    "label": 1
                },
                {
                    "sent": "I think if you make the current visible frame just be one linear value.",
                    "label": 0
                },
                {
                    "sent": "And you make the style label just be 1 number.",
                    "label": 0
                },
                {
                    "sent": "And it just changes that interaction.",
                    "label": 0
                },
                {
                    "sent": "That's called a heteroskedastic model and you get a Nobel Prize for it.",
                    "label": 0
                },
                {
                    "sent": "But I think that's what model is.",
                    "label": 0
                },
                {
                    "sent": "I think this is a generalization of that where you're having high dimensional data and you're getting a whole bunch of matrices this way.",
                    "label": 0
                },
                {
                    "sent": "Modulation matrices.",
                    "label": 0
                },
                {
                    "sent": "OK, so Grand Taylor trained up a model like that and I'm going to show you some samples from the model after it's learned and what we're going to be doing is just generating from the model, anybody?",
                    "label": 0
                },
                {
                    "sent": "Linear ankle systems knows that it's kind of hard to generate from them.",
                    "label": 0
                },
                {
                    "sent": "They tend to die or explode, and it's certainly very hard to get them to sort of walk a bit and then stop and then start walking again.",
                    "label": 0
                },
                {
                    "sent": "So here's some samples from the model.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is it generating normal walking.",
                    "label": 0
                },
                {
                    "sent": "This is all data from a student at CMU who did various styles of things.",
                    "label": 1
                },
                {
                    "sent": "So it's deciding which way to turn.",
                    "label": 0
                },
                {
                    "sent": "And it's also deciding how fast to walk and it's walking at a sort of fairly constant speed there.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to tell it to walk in a drunken way.",
                    "label": 0
                },
                {
                    "sent": "It's the very same model, it's just so style labels engaging the interaction matrices and you'll notice that it actually stops.",
                    "label": 0
                },
                {
                    "sent": "And then starts again.",
                    "label": 0
                },
                {
                    "sent": "That's very hard to do with our linear domical system.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You can tell him to walk like a gangly teenager.",
                    "label": 0
                },
                {
                    "sent": "Being a computer science student, he's good at that.",
                    "label": 0
                },
                {
                    "sent": "He remembers that.",
                    "label": 0
                },
                {
                    "sent": "I make all these character assertions about this poor student who provided us the data.",
                    "label": 0
                },
                {
                    "sent": "I've never met him.",
                    "label": 0
                },
                {
                    "sent": "No idea who he is, but he's easy to make jokes about.",
                    "label": 0
                },
                {
                    "sent": "That's what you get from remaining data.",
                    "label": 0
                },
                {
                    "sent": "Because it's all one model with this style label, it can walk in one style and then you can switch the style label.",
                    "label": 0
                },
                {
                    "sent": "It's never seen transitions between styles, but it'll do plausable transitions.",
                    "label": 0
                },
                {
                    "sent": "Of course it doesn't understand physics, so there may be some momentum lost in this transition.",
                    "label": 0
                },
                {
                    "sent": "It just models what it sees.",
                    "label": 0
                },
                {
                    "sent": "But we're telling to work in a sexy way.",
                    "label": 0
                },
                {
                    "sent": "And then to switch to normal.",
                    "label": 0
                },
                {
                    "sent": "Notice is the same model going in a very different speed.",
                    "label": 0
                },
                {
                    "sent": "That's normal, and then we'll switch to sexy again.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "There he goes.",
                    "label": 0
                },
                {
                    "sent": "It's censored at this point.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Since I've got a bit of spare time with my schedule, I'll show you one more.",
                    "label": 0
                },
                {
                    "sent": "This is just walking like a cat.",
                    "label": 0
                },
                {
                    "sent": "And Graham can get this model to do all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "He can't get it to do backflips but he can get to all sorts of things including slowing down and speeding up.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "For the mokap example.",
                    "label": 0
                },
                {
                    "sent": "And now I want to go for the most come.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great example.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is take this 3 way thing, and we're going to use it for modeling a static image by making two copies of the same image.",
                    "label": 1
                },
                {
                    "sent": "This is one way to think about it.",
                    "label": 0
                },
                {
                    "sent": "We make 2 copies of the same image.",
                    "label": 0
                },
                {
                    "sent": "And we say that each factor has exactly the same weight to those two copies, but other than that it's going to be just like what we did before.",
                    "label": 0
                },
                {
                    "sent": "So now if you think about what the belief propagation rule is, the factor should take the weighted sum.",
                    "label": 0
                },
                {
                    "sent": "It gets on the 1st copy, take the weighted sum, it gets on the second copy, which is the same weighted sum.",
                    "label": 0
                },
                {
                    "sent": "Multiply the two together that is, square the output of a linear filter and send that message to the hidden units.",
                    "label": 1
                },
                {
                    "sent": "Well, that's exactly the standard model of a simple cell sending stuff to complex cells.",
                    "label": 0
                },
                {
                    "sent": "So the claim here is that the model of simple cells and complex cells are quite popular in the vision literature and with neuro scientist two just dropped out of taking a 3 way energy function and factoring it.",
                    "label": 0
                },
                {
                    "sent": "The advantage of taking the three rare in G function factoring it is that it becomes obvious how to do the learning, and it becomes.",
                    "label": 0
                },
                {
                    "sent": "Obviously you've got a generative model.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the advantage of modeling of having these three way interaction is you can model covariances between pixels rather than pixels, and that's a much better thing to model.",
                    "label": 1
                },
                {
                    "sent": "So if I say if I say what's a vertical edge, one vertical edge doesn't really mean is bright this side and outside, or it's far this side and near this side.",
                    "label": 0
                },
                {
                    "sent": "What are vertical edge really means if it's including edges, do not interpolate across this edge.",
                    "label": 0
                },
                {
                    "sent": "It's a statement about covariance structure.",
                    "label": 0
                },
                {
                    "sent": "And that's what we'd like, a representation of a vertical edge to do.",
                    "label": 0
                },
                {
                    "sent": "Other models another model against this is Gaussian scale mixture models.",
                    "label": 0
                },
                {
                    "sent": "They get this same property.",
                    "label": 0
                },
                {
                    "sent": "You don't want to interpolate horizontally across a vertical edge.",
                    "label": 0
                },
                {
                    "sent": "That gives you some translation invariants, a lot of invariants to brightness and contrast, and it makes it look a bit more like a complex cell.",
                    "label": 1
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's another way of thinking about it.",
                    "label": 0
                },
                {
                    "sent": "If you take 2 pixels and you want to model the joint distribution, that will typically have a strong covariance.",
                    "label": 1
                },
                {
                    "sent": "And so the two square things are meant to be the two pixels, and what we could do is have two linear filters which we're going to learn.",
                    "label": 0
                },
                {
                    "sent": "That's those red blobs and green blobs.",
                    "label": 0
                },
                {
                    "sent": "The learned weights.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if we can learn those two linear filters to line up with the axes of the ellipse.",
                    "label": 1
                },
                {
                    "sent": "Then we can take the output of a linear filter.",
                    "label": 0
                },
                {
                    "sent": "We can square it, we can wait it.",
                    "label": 0
                },
                {
                    "sent": "For directions in which the... elongated, you want a small weight and for directions in which the... precise you want a big weight, and then that energy function.",
                    "label": 0
                },
                {
                    "sent": "Represents the negative log probability under a full covariance matrix, where sort of 1 standard deviation of the full covariance matrix is shown by that ellipse.",
                    "label": 1
                },
                {
                    "sent": "So then you just make the probabilities proportional to each of my side energy and you're sampling from that full covariance matrix, but it has zero mean.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, you can think of each of those learn factors as a parabolic trough.",
                    "label": 0
                },
                {
                    "sent": "So there's one parabolic troughs that goes along the ellipse and that is fairly sharp.",
                    "label": 0
                },
                {
                    "sent": "It's got fairly high precision that corresponds to the green filter, and there's one parabolic troughs that goes across the ellipse and is very gentle and that corresponds to the red filter.",
                    "label": 0
                },
                {
                    "sent": "That's why it has small weights and we synthesized the covariance matrix or the precision matrix.",
                    "label": 0
                },
                {
                    "sent": "By adding together these two parabolic troughs.",
                    "label": 0
                },
                {
                    "sent": "So what we'd really like to do is be able to synthesize the covariance matrix on the fly by adding together 1 dimensional ingredients like this.",
                    "label": 0
                },
                {
                    "sent": "But if we can do that, then for each image we can create an appropriate covariance matrix for that image instead of just having one covariance matrix that applies to everything.",
                    "label": 0
                },
                {
                    "sent": "And for images you don't want one covariance matrix because when the covariance breaks down, it breaks down big time and you couldn't possibly have a covariance that was heavy tailed enough to cope with that.",
                    "label": 0
                },
                {
                    "sent": "If you're being Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So here's what we do.",
                    "label": 0
                },
                {
                    "sent": "We take the outputs of those linear filters.",
                    "label": 0
                },
                {
                    "sent": "Square them.",
                    "label": 0
                },
                {
                    "sent": "And send them to a hidden unit.",
                    "label": 0
                },
                {
                    "sent": "So this is just a re description of the three way model and we give that hidden unit a big positive bias.",
                    "label": 1
                },
                {
                    "sent": "So now when the linear filter isn't producing any output, that's going to be because we have a nice smooth image and you win by plus B.",
                    "label": 0
                },
                {
                    "sent": "That is your free energies minus B.",
                    "label": 0
                },
                {
                    "sent": "As the filter starts producing output, you get a bit less smooth, so you start losing and you lose as you get less and less smooth.",
                    "label": 0
                },
                {
                    "sent": "But when you start losing big time, you just turn off the hidden units so you just pay fixed price.",
                    "label": 1
                },
                {
                    "sent": "So you say it's no longer smooth.",
                    "label": 0
                },
                {
                    "sent": "Forget about smoothness.",
                    "label": 0
                },
                {
                    "sent": "Don't add that contribute.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the precision matrix.",
                    "label": 0
                },
                {
                    "sent": "Inference is still very simple.",
                    "label": 0
                },
                {
                    "sent": "The hidden units are all independent given the pixel intensities, so we can infer the states of these hidden units that are modulating the precision matrix given the states of the hidden units.",
                    "label": 1
                },
                {
                    "sent": "However, the pixels have correlations.",
                    "label": 0
                },
                {
                    "sent": "That was the whole point of the hidden units, and so you can't simply do a reconstruction previously with the MOCAP data and the image transformation data we conditioned on the previous image.",
                    "label": 0
                },
                {
                    "sent": "We didn't try and reconstruct, it was fixed.",
                    "label": 0
                },
                {
                    "sent": "If you condition on the previous image life, simple, but if the two images are identical copies.",
                    "label": 0
                },
                {
                    "sent": "So when you're trying to model the variance of a single image, you have to do something else to reconstruct, and we actually do.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hybrid Monte Carlo, that is, you could actually just invert the precision matrix, but it's a different one for every data point, so for big images that would take a long time.",
                    "label": 1
                },
                {
                    "sent": "So we do hybrid Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "What that amounts to is starting at the.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "You going to?",
                    "label": 0
                },
                {
                    "sent": "Run along a free energy surface that you get by integrating out the hidden units and you're going to simulate a particle that starts with some random momentum and moves along the free energy surface for 20 steps.",
                    "label": 1
                },
                {
                    "sent": "And if you do the steps right where you can get numerical errors, cancel out and then when you got to the end you're going to say did I get numerical errors if I did throw it away and try again.",
                    "label": 0
                },
                {
                    "sent": "If I didn't, that's good, and so I've now got a sample from 20 projects or hybrid Monte Carlo and will use that as the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "Then, given that reconstruction we activate the hidden units again and measure their statistics again, and the learning is just the pairwise statistics, you get between factors and units with the data and the pairwise statistics.",
                    "label": 0
                },
                {
                    "sent": "Yet between factors and units after doing these repro steps, so it's just the same learning algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you could just use a 3 way PBM.",
                    "label": 1
                },
                {
                    "sent": "But what the through PBM does is it learns the covariance structure nicely, but it doesn't learn the means.",
                    "label": 0
                },
                {
                    "sent": "It always wants 0 mean, so it thinks the most likely image is just a completely flat image.",
                    "label": 0
                },
                {
                    "sent": "Or maybe a completely sloping image, but very smooth image.",
                    "label": 0
                },
                {
                    "sent": "It doesn't like to have complexity in the image, whereas the other PBM, the ones modeling, means it works not by getting unhappy when it sees smoothness being violated, but by getting happy when it sees familiar features.",
                    "label": 0
                },
                {
                    "sent": "So it likes images that are busy with lots going on.",
                    "label": 0
                },
                {
                    "sent": "Lots of familiar features.",
                    "label": 0
                },
                {
                    "sent": "These two work very well together, so we're going to have some hidden units that use this 3 way RBM and model the precision matrix of the image and they get unhappy when they see violations of smoothness, other hidden units, the model, the means, and they're happy when they see familiar things.",
                    "label": 1
                },
                {
                    "sent": "So on 16 by 16 natural image patches, these are the receptive fields of the hidden units, the model, the means, and they're very blurry.",
                    "label": 0
                },
                {
                    "sent": "They sort of saying the image is sort of reddish hair and bluish there.",
                    "label": 0
                },
                {
                    "sent": "The kind of thing you'd like if someone had drawn all the boundaries for you and you just had to sort of explain someone how to color it in.",
                    "label": 0
                },
                {
                    "sent": "So to make it ready, share mega Bush over.",
                    "label": 0
                },
                {
                    "sent": "There is sort of like water covering.",
                    "label": 0
                },
                {
                    "sent": "That's what the means are doing.",
                    "label": 1
                },
                {
                    "sent": "This is what the covariance units are doing, something totally different.",
                    "label": 1
                },
                {
                    "sent": "And what I'm showing you here is the receptive fields of the factors.",
                    "label": 0
                },
                {
                    "sent": "They learn typically high frequency filters.",
                    "label": 0
                },
                {
                    "sent": "The high frequency filters are almost exactly pure pure Gray level.",
                    "label": 0
                },
                {
                    "sent": "That is, all of these filters are actually produced by using.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using RGB values.",
                    "label": 0
                },
                {
                    "sent": "Still very good and.",
                    "label": 0
                },
                {
                    "sent": "It decides to have exactly the same values for RGB for almost all of the filters.",
                    "label": 0
                },
                {
                    "sent": "But it has some low frequency filters that are colored.",
                    "label": 0
                },
                {
                    "sent": "It uses red green opponent ones and yellow blue account ones, and it puts those all in the same space in the traffic map.",
                    "label": 0
                },
                {
                    "sent": "And this looks quite a lot like what goes on in brains of animals, some animals.",
                    "label": 0
                },
                {
                    "sent": "The map is typographic because the way Marcario learned this was to layout the factors in 2D allow the hidden units in 2D and then connect to hidden unit to factors that have similar locations.",
                    "label": 1
                },
                {
                    "sent": "So every fact is connected to all the pixels, so the receptive fields are local.",
                    "label": 0
                },
                {
                    "sent": "Becausw it learned that.",
                    "label": 0
                },
                {
                    "sent": "But it forms the trouble graphic map because we wired in topography.",
                    "label": 1
                },
                {
                    "sent": "But then it's not actually allowed to learn for each hidden unit.",
                    "label": 0
                },
                {
                    "sent": "Which of the things that it can see it would like to.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really look at how much you would like to look at it, so it's actually learning everything in the end.",
                    "label": 0
                },
                {
                    "sent": "If you take an image and you activate the hidden units for the means and you activate the hidden units for the correct precision matrix and then from those hidden units you reconstruct exact samples many times, you do that by inverting that precision matrix and sampling from the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Then what's interesting is all these samples are rather different from one another.",
                    "label": 0
                },
                {
                    "sent": "If your idea of similarity is squared, difference between pixel intensities, but they're all very much the same in terms of what they look like to us.",
                    "label": 1
                },
                {
                    "sent": "So if you look at the middle Rd there.",
                    "label": 0
                },
                {
                    "sent": "What's happening is it's got the same regions with the same edges in roughly the same locations, but is coloring in the regions a bit differently.",
                    "label": 0
                },
                {
                    "sent": "What it knows is that if anyone region.",
                    "label": 0
                },
                {
                    "sent": "That should be smooth.",
                    "label": 0
                },
                {
                    "sent": "The color in that region should be the same all over more or less, but it can change what their color is.",
                    "label": 0
                },
                {
                    "sent": "So again, if it's like the watercolor model of perception where you have edges and then you coloring regions, sort of roughly.",
                    "label": 0
                },
                {
                    "sent": "But the coloring into the regions doesn't need to know exactly where the edges are.",
                    "label": 0
                },
                {
                    "sent": "So the last thing we.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It shows it's good for discrimination because as we all know, machine learning is about the scores you get in discrimination competitions.",
                    "label": 0
                },
                {
                    "sent": "So here's the data set that's quite difficult.",
                    "label": 0
                },
                {
                    "sent": "It's 32 by 32 color images.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the square actually, but the somehow they've got non square.",
                    "label": 0
                },
                {
                    "sent": "If you look at the first column that's planes and there's quite a lot of variation in the planes, look at the one just before the end.",
                    "label": 0
                },
                {
                    "sent": "That's a very small plane on the plane.",
                    "label": 0
                },
                {
                    "sent": "At the top has been writing in it, or look at the third column.",
                    "label": 0
                },
                {
                    "sent": "That's birds.",
                    "label": 0
                },
                {
                    "sent": "You get a sort of close up of an ostrich.",
                    "label": 0
                },
                {
                    "sent": "That's a bird.",
                    "label": 0
                },
                {
                    "sent": "You get birds in normal poses.",
                    "label": 0
                },
                {
                    "sent": "You get a chick in.",
                    "label": 0
                },
                {
                    "sent": "It's a difficult data set.",
                    "label": 0
                },
                {
                    "sent": "These are images gleaned from the web.",
                    "label": 1
                },
                {
                    "sent": "The images by using bird as a search term.",
                    "label": 0
                },
                {
                    "sent": "Or sub categories of bird.",
                    "label": 0
                },
                {
                    "sent": "The images are then shown to.",
                    "label": 0
                },
                {
                    "sent": "Students in Toronto who have to say, if I saw this image, is there a single dominant object and there's a reasonable chance I would use the word bird to describe that object.",
                    "label": 0
                },
                {
                    "sent": "And if there is, we say that's a good one.",
                    "label": 0
                },
                {
                    "sent": "So now we can train on there's 80,000,000 unlabeled images and we train on a few million of those to learn early laser vision.",
                    "label": 0
                },
                {
                    "sent": "And then we have 50,000 labeled ones.",
                    "label": 0
                },
                {
                    "sent": "There's 5000 label ones of each category and then 1000 labeled test ones.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we'd like to have done is learned on the whole 32 by 32 Patch, but this was done a few weeks ago when the research was not as far along, and so we learned on 8 by 8 patches.",
                    "label": 0
                },
                {
                    "sent": "This mean and covariance RBM and then just replicated across the image.",
                    "label": 0
                },
                {
                    "sent": "If you get a postdoc from Youngs lab, he's always going to do convolutional things.",
                    "label": 0
                },
                {
                    "sent": "It takes a long time for that to die out.",
                    "label": 0
                },
                {
                    "sent": "So after you replicated you get all these hidden units.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We then just simply give those to logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Obviously we could use something better, but for now we want to assess different ways of getting those hidden units.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to compare.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Gaussian binary RBM?",
                    "label": 0
                },
                {
                    "sent": "That's the one that works fine if the pixels are decorrelated like they are in speech.",
                    "label": 0
                },
                {
                    "sent": "We then compare using the same number of hidden units with some binary RBM.",
                    "label": 1
                },
                {
                    "sent": "Well, that number of hidden units with just covariances, or that number of Indians with means anchor variances and will also try adding another layer to see if the greedy learning ideas still works.",
                    "label": 0
                },
                {
                    "sent": "If you stack it on top.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you just do a Gaussian binary RBM, you get just under 60% correct the through DBM, just for covariances get 62%.",
                    "label": 0
                },
                {
                    "sent": "If you use a lot more factors.",
                    "label": 0
                },
                {
                    "sent": "So each hidden unit is pooling.",
                    "label": 0
                },
                {
                    "sent": "Many of these factors you get something more like a complex cell, and that gives you another 5%.",
                    "label": 0
                },
                {
                    "sent": "If you add the means as well, but reduce the number of hidden units being used for the covariances.",
                    "label": 1
                },
                {
                    "sent": "So it's always the same number of hidden units.",
                    "label": 0
                },
                {
                    "sent": "You do better, and if you then take those binary vectors and you just do the sort of.",
                    "label": 1
                },
                {
                    "sent": "Dumb deeplearning idea of let's learn another layer of 8000 units.",
                    "label": 0
                },
                {
                    "sent": "Then you do even better.",
                    "label": 0
                },
                {
                    "sent": "Now, I must admit that if you add another layer, you don't do any better than that, but you will.",
                    "label": 0
                },
                {
                    "sent": "Once we've learned these 32 by 32 images instead of eight Berry patches, I believe.",
                    "label": 0
                },
                {
                    "sent": "So, at least on one very difficult task, these three way interactions aren't just good for modeling the covariance structure and image.",
                    "label": 0
                },
                {
                    "sent": "They actually give you a representation that's better for discrimination.",
                    "label": 0
                },
                {
                    "sent": "So in summary.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I started by showing you how it's relatively easy to learn a deep generative model by stacking up these restricted balls machines.",
                    "label": 1
                },
                {
                    "sent": "I didn't sort of explain while that works, but it does.",
                    "label": 1
                },
                {
                    "sent": "I then said we can actually make a more complicated module that has multiplicative interactions.",
                    "label": 0
                },
                {
                    "sent": "If we do it in the naive way, we get cubically many parameters, but we're going to factor that.",
                    "label": 0
                },
                {
                    "sent": "And now we can get a reasonable number of parameters, and once we factor it, the inference and the learning are all pairwise things, and they look just like a normal Boltzmann machine, except for this, multiplying together the outputs of two linear filters.",
                    "label": 1
                },
                {
                    "sent": "And then I showed you that you can use that to get representations that are good for object recognition in small color images.",
                    "label": 0
                },
                {
                    "sent": "And I'm done.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Very loosely.",
                    "label": 0
                },
                {
                    "sent": "It is, but my hearing is bad.",
                    "label": 0
                },
                {
                    "sent": "By following.",
                    "label": 0
                },
                {
                    "sent": "It seems that the more complexity you add, the better the system before you have any effects of over fitting into certain point, it becomes too complex and begin to deteriorate.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question was you using a whole load of parameters?",
                    "label": 0
                },
                {
                    "sent": "Do you have problems overfitting?",
                    "label": 0
                },
                {
                    "sent": "The answer is most of the parameters are trained as a generative model, and so we win in two ways.",
                    "label": 0
                },
                {
                    "sent": "There we can train on unlabeled data so we can train on the whole labeled whole lot of data so we can use millions of unlabeled images.",
                    "label": 0
                },
                {
                    "sent": "So that's one way to deal with overfitting.",
                    "label": 0
                },
                {
                    "sent": "Also, for each training example.",
                    "label": 0
                },
                {
                    "sent": "We are getting much more constraint because the model is trying to model what's going on in the image, not just this label.",
                    "label": 0
                },
                {
                    "sent": "So getting much more constraint per case that helps with overfitting, but then in addition to all that, it looks like you can train about half as many parameters as you have pixels in your entire training set, and so it seems to be resistant to overfitting for other reasons I don't understand, But the biggest win is that you can train on lots of unlabeled data and so you can discover your multiple layers of feature detectors that needing to know anything about labels.",
                    "label": 0
                },
                {
                    "sent": "And then the labels are just needed for fine tuning.",
                    "label": 0
                },
                {
                    "sent": "You can use a small number of labels.",
                    "label": 0
                },
                {
                    "sent": "That's not true for the speech example.",
                    "label": 0
                },
                {
                    "sent": "The speech example that I told you didn't have any extra unlabeled data and it should work much better with extra unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "You can ask the question here alright, can you just say something about the computational complexity of training these things?",
                    "label": 0
                },
                {
                    "sent": "Like how much time does it take?",
                    "label": 0
                },
                {
                    "sent": "OK, how much time does it take?",
                    "label": 0
                },
                {
                    "sent": "Well, the good news is everything is linear.",
                    "label": 0
                },
                {
                    "sent": "That is, as you add more training data, there's nothing quadratic in the size of the training data, and as you add more hidden units apart from the connection matrices which quadratically with the size of the layers in terms of the number of connections, everything is linear.",
                    "label": 0
                },
                {
                    "sent": "But you have to train it for a long time.",
                    "label": 0
                },
                {
                    "sent": "So the speech example.",
                    "label": 0
                },
                {
                    "sent": "Took several days on a GPU board.",
                    "label": 0
                },
                {
                    "sent": "The image example probably takes about a day on a GPU board and a day on a GPU board is about a month on a single core.",
                    "label": 0
                },
                {
                    "sent": "So basically if you want to do research in this kind of machine learning, you have to get yourself a workstation that has a Generation 2 PCI 16 slot and you have to plug in an NVIDIA GTX 285 board and then you're in business.",
                    "label": 0
                },
                {
                    "sent": "And I told him very if it gave me a free one.",
                    "label": 0
                },
                {
                    "sent": "I say all this, but they didn't give me a free one.",
                    "label": 0
                },
                {
                    "sent": "But it's such a good board that I'm saying it anyway.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if there's no other questions, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}