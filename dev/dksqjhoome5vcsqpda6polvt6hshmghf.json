{
    "id": "dksqjhoome5vcsqpda6polvt6hshmghf",
    "title": "CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples",
    "info": {
        "author": [
            "Filip Radenovi\u0107, Department of Cybernetics, Czech Technical University in Prague"
        ],
        "published": "Oct. 24, 2016",
        "recorded": "October 2016",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2016_radenovic_cnn_image/",
    "segmentation": [
        [
            "Hi, my name is Philip Damage and I will present our work on CNN Image retrieval that learns from bag of words.",
            "So let's start with the title it."
        ],
        [
            "Briefly summarizes the talk.",
            "We are interested in CNN."
        ],
        [
            "Image retrieval.",
            "So we start with a fully convolutional network followed by global Max pooling, which means that for each image we have a mapping from an image to a vector and we get compact image descriptors.",
            "Then we can use nearest neighbor or approximate nearest neighbor for a fast image search.",
            "We saw."
        ],
        [
            "With a pre trained network that's trained on a different but similar task, and then we do a learning or fine tuning on the data that is relevant to our task.",
            "To generate this data."
        ],
        [
            "We use state of the art image retrieval methods based on bag of words.",
            "People have worked on megahertz for more than 10 years now, so they actually know which parts are helping and what works well, which is not the case yet.",
            "With CNN's an backwards couples well with structure for motion.",
            "When?"
        ],
        [
            "We say unsupervised we we mean that from the beginning of the process of training until the end there is no human interaction at all.",
            "And."
        ],
        [
            "We are using only hard examples because using hard examples is as good as using all of the examples, but it's more efficient."
        ],
        [
            "So let's speak about the challenges that appear in image retrieval.",
            "We want to retrieve the same object under various viewpoint and scale changes Megaforce deals with this by using affine covariant local features, and invariant descriptors, while CNN deals with this by using lots of training examples."
        ],
        [
            "Illumination illumination changes handled in Megaforce by color normal normalization of local features.",
            "CNN lots of training examples."
        ],
        [
            "Versions are handled by locality of the features.",
            "An geometric verification in bag of words.",
            "CNN once again.",
            "Lots of training examples an at the end."
        ],
        [
            "What we don't want happening is confusing similar objects with our query image back over its users discriminate discriminate ability of the features, an geometric verification, CNN guess what?",
            "Lots of training?"
        ],
        [
            "Examples.",
            "So."
        ],
        [
            "I hope that you are getting lots of training examples.",
            "A strong motive here."
        ],
        [
            "Because if you have lots of training, lots of images, lots of unordered images.",
            "In order to do the training properly, we actually need to have annotations for specific tasks.",
            "The straightforward way is going through."
        ],
        [
            "Michael Turk, which is not very accurate and it takes time."
        ],
        [
            "What you can do is you can go through the datasets by ourselves, do the manual cleaning, which is super slow and it's a very expensive waste of researchers time.",
            "What we propose is."
        ],
        [
            "Automated extraction of training data that is very accurate and it comes for free.",
            "So."
        ],
        [
            "So something that is commonly known as off the shelf application of CNN's is basically taking a CNN training on classification task and Imagenet data set and then directly applying it to other tasks like fine grained classification, object detection and even image retrieval.",
            "However, this might not be the best thing to do for immaterial.",
            "Let's take building classes and."
        ],
        [
            "Sample so all different buildings belong to the same class and network didn't train to distinguish between them.",
            "This is not the behavior that we want imagery.",
            "We actually want to get the exact instance of a building.",
            "What you can do is you can prepare a data set that is more relevant to your task.",
            "So banquet all the TC 2014 create a data set where class where one class is 1 landmark.",
            "This is better but still 2 images from the same landmark don't necessarily need to have overlapping views, they don't have to observe the same object.",
            "Also, they trained for classification, not for imaginary work.",
            "There is one more way you can generate the cheap in achieve way a data set that has we connotations meaning GPS tags.",
            "So there is a method verison metal net flood Byron Jelovich at all that actually uses this kind of a data set and they train directly for image retrieval.",
            "However, here the spatially closest images don't necessarily need to be positive.",
            "Images don't necessarily need to be looking at the same object and we don't know this because we don't know, rotation, so we have to handle this during the training.",
            "What we propose is an automatic way to get annotations for images, and these annotations are strong enough that you don't have to care about anything during the CNN training."
        ],
        [
            "So how do we get these limitations?",
            "We actually use our recently proposed state of the art image retrieval and structure from motion images real tightly coupled with structure for motion.",
            "So we take advantage of imagery with significant viewpoints and scale change so that we get very complete and many very detailed 3D models.",
            "And we do this for the whole data set, so for the whole data set we get all non overlapping 3D models that we can find.",
            "And."
        ],
        [
            "Now we know all the camera positions, camera rotations, and we know the number of inliers between each pairs of images.",
            "Given this information, we can proceed in an easy way to extract the positive and negative training examples.",
            "So."
        ],
        [
            "For negative negative images are basically the images that are from different city models.",
            "Then the query image and hard negative images are closest negative examples for the query in the CNN descriptor space.",
            "So the hardest negative."
        ],
        [
            "Image is the most similar image in the in the scene descriptor space.",
            "Usually you don't just want one hard negative image.",
            "You want to get more than one, and if you proceed in a naive way by."
        ],
        [
            "Bing top kimages in the CNN descriptor space.",
            "You will end up with near duplicates.",
            "I mean near duplicates with the hardest hard negative image.",
            "You don't want this.",
            "This is redundant data.",
            "So what we propose is getting dive."
        ],
        [
            "This hard negatives, which means that we get still we get K hard negative images, but we get only one image for 3D model.",
            "Although hard negative mining for metric learning and object detection was a standard so far, this is not the case."
        ],
        [
            "For positive images and we showed that getting hard positive images actually helps an improve the performance.",
            "So positive images.",
            "Positive examples are images that share 3D points with the query image and hard positive images are positive examples that are not close enough to the query in the CNN descriptor space.",
            "Again, naive way to get a positive example is getting the most similar image in the CNN descriptor space.",
            "Which is not good due to the reason that this image is already matched well with the query with the current CNN.",
            "So the CNN will not learn much from it.",
            "This was used in, for example, not flood the recent method, but we propose.",
            "Is getting harder positives, So what you can do is something that worked well in bag of words, imagery.",
            "We can get the nearest neighbor in the back of our descriptor space or you can even go harder.",
            "You can get a random image from Tip top K images in the bag of words.",
            "Descriptor space.",
            "When we have the training pairs, when we generated them we can proceed with the learning.",
            "We do the learning in a Siamese fashion which."
        ],
        [
            "Means that we have two branch."
        ],
        [
            "Network, one branch for the query image and the other branch for the respective positive or negative image.",
            "The two branches share the weights, which means that the weights are updated equally during the training."
        ],
        [
            "We minimize contrastive loss, which is very simple.",
            "We want to penalize positive image."
        ],
        [
            "Yes, if they are far away.",
            "And we want to penalize negative images if."
        ],
        [
            "They are not far away enough.",
            "We also tried."
        ],
        [
            "Split or ranking loss contrastive loss is more strict and it requires more very accurate data which we do provide.",
            "So in our setting contrast was always work better than triplet loss.",
            "In recently reporting."
        ],
        [
            "Recently proposed CNN images drivel works that also works that work with compact image representations.",
            "It has been shown that whitening benefits the performance a lot.",
            "So what they did so far was learning whitening PCA whitening on an independent set of descriptors.",
            "What we propose."
        ],
        [
            "Is do learning widening in a supervised way using our training data and linear discriminant projections?",
            "But we also tried."
        ],
        [
            "Is doing the whitening not as a post processing step but in adding whitening in end to end learning however?",
            "This performance of this was compatible or worse than supervised widening.",
            "Learning in post processing.",
            "So we decided to end it."
        ],
        [
            "Slowing down the convergence.",
            "So we decided to stick with the post processing step, learning the widening in the supervised way with our labeled training data."
        ],
        [
            "For the experiments we use standard datasets that are used in image retrieval, so Oxford and Paris buildings and Holidays data set.",
            "We use a standard protocol mean, average precision and very important."
        ],
        [
            "In the training 3D models, there is no landmark from testing data sets.",
            "So our."
        ],
        [
            "Show that the careful choice of positive and negative images makes a difference starting from off the show."
        ],
        [
            "Network each one of our can't."
        ],
        [
            "Buttions"
        ],
        [
            "Or positive and negative."
        ],
        [
            "Mining."
        ],
        [
            "Improves the performance.",
            "Gives higher performance."
        ],
        [
            "The end our learned lightening gives additional boost to the final results.",
            "In another."
        ],
        [
            "Element we wanted to test the overfitting of our network so we this time we added and only for this experiment we added testing landmarks during the training procedure and repeated the whole fine tuning process.",
            "We observe."
        ],
        [
            "A very small performance change, so we conclude that our network don't overfit to the training date."
        ],
        [
            "We also do extensive comparison with state of the art.",
            "So we basically compare with every image retrieval method that we are aware of.",
            "CNN based, an bag of word based.",
            "And I can go slowly through the all the results, but I don't think that would be a good idea.",
            "So what I would like to point out is."
        ],
        [
            "Is that state of the art on 256 dimensions?",
            "Previous state of the art net flat on Oxford 5K is outperformed by our fine to network with only 32 dimensions I would."
        ],
        [
            "I would also like to mention that on this conference there is a concurrent work that also does CNN image retrieval with compact codes.",
            "At the end I would like."
        ],
        [
            "We compared the teacher and the student teacher being begger for its image retrieval that we use to generate our data and student being our fine tune, fine tune, CNN, Network.",
            "So student is worse.",
            "It's compareable.",
            "It's very close but it's worse which is still a very nice achievement if you consider that student has that CNN's have compact image representations, which means much smaller memory footprint and much faster search.",
            "Also after adding rerank."
        ],
        [
            "And query expansion for the compiled codes.",
            "We get higher student surpasses the teacher on all of the testing datasets.",
            "Stay."
        ],
        [
            "There is space for improvement.",
            "So for example, if you would like to search a very small object, meaning an object that appears in a small partition of an image.",
            "Usually like Apple logo due to its low."
        ],
        [
            "Quality nature bag of words will still be able to find the small objects very well, however."
        ],
        [
            "CNN struggles, which is kind of expected because CNN methods have compact codes.",
            "Anna global representation.",
            "One more case."
        ],
        [
            "Is for example painting of Mona Lisa, our data set."
        ],
        [
            "Contains a lot of instances of this painting and bag of words is able to find them however, CNN."
        ],
        [
            "Struggles, so it seems that fine tune network does not easily forget its past and tries to generalize whenever possible, so fine tuning."
        ],
        [
            "Would not be enough.",
            "To come."
        ],
        [
            "Loot.",
            "We propose.",
            "Unsupervised way to generate generate the training.",
            "Lots of training examples that is necessary for CNN training.",
            "We also propose methods to use these data and to create hard, negative and hard positive examples.",
            "Also, we propose supervised widening learning step that in most of the cases improve results.",
            "Our training data and our train models are publicly available, so please feel free to use them.",
            "And if you want to find out more details about the poster, visit us at the post session that's on the rooftop garden.",
            "Thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, my name is Philip Damage and I will present our work on CNN Image retrieval that learns from bag of words.",
                    "label": 0
                },
                {
                    "sent": "So let's start with the title it.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Briefly summarizes the talk.",
                    "label": 0
                },
                {
                    "sent": "We are interested in CNN.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Image retrieval.",
                    "label": 0
                },
                {
                    "sent": "So we start with a fully convolutional network followed by global Max pooling, which means that for each image we have a mapping from an image to a vector and we get compact image descriptors.",
                    "label": 1
                },
                {
                    "sent": "Then we can use nearest neighbor or approximate nearest neighbor for a fast image search.",
                    "label": 0
                },
                {
                    "sent": "We saw.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With a pre trained network that's trained on a different but similar task, and then we do a learning or fine tuning on the data that is relevant to our task.",
                    "label": 0
                },
                {
                    "sent": "To generate this data.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use state of the art image retrieval methods based on bag of words.",
                    "label": 1
                },
                {
                    "sent": "People have worked on megahertz for more than 10 years now, so they actually know which parts are helping and what works well, which is not the case yet.",
                    "label": 1
                },
                {
                    "sent": "With CNN's an backwards couples well with structure for motion.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We say unsupervised we we mean that from the beginning of the process of training until the end there is no human interaction at all.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are using only hard examples because using hard examples is as good as using all of the examples, but it's more efficient.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's speak about the challenges that appear in image retrieval.",
                    "label": 0
                },
                {
                    "sent": "We want to retrieve the same object under various viewpoint and scale changes Megaforce deals with this by using affine covariant local features, and invariant descriptors, while CNN deals with this by using lots of training examples.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Illumination illumination changes handled in Megaforce by color normal normalization of local features.",
                    "label": 0
                },
                {
                    "sent": "CNN lots of training examples.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Versions are handled by locality of the features.",
                    "label": 1
                },
                {
                    "sent": "An geometric verification in bag of words.",
                    "label": 0
                },
                {
                    "sent": "CNN once again.",
                    "label": 1
                },
                {
                    "sent": "Lots of training examples an at the end.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we don't want happening is confusing similar objects with our query image back over its users discriminate discriminate ability of the features, an geometric verification, CNN guess what?",
                    "label": 0
                },
                {
                    "sent": "Lots of training?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I hope that you are getting lots of training examples.",
                    "label": 0
                },
                {
                    "sent": "A strong motive here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because if you have lots of training, lots of images, lots of unordered images.",
                    "label": 1
                },
                {
                    "sent": "In order to do the training properly, we actually need to have annotations for specific tasks.",
                    "label": 0
                },
                {
                    "sent": "The straightforward way is going through.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Michael Turk, which is not very accurate and it takes time.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you can do is you can go through the datasets by ourselves, do the manual cleaning, which is super slow and it's a very expensive waste of researchers time.",
                    "label": 0
                },
                {
                    "sent": "What we propose is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Automated extraction of training data that is very accurate and it comes for free.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So something that is commonly known as off the shelf application of CNN's is basically taking a CNN training on classification task and Imagenet data set and then directly applying it to other tasks like fine grained classification, object detection and even image retrieval.",
                    "label": 1
                },
                {
                    "sent": "However, this might not be the best thing to do for immaterial.",
                    "label": 0
                },
                {
                    "sent": "Let's take building classes and.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sample so all different buildings belong to the same class and network didn't train to distinguish between them.",
                    "label": 0
                },
                {
                    "sent": "This is not the behavior that we want imagery.",
                    "label": 0
                },
                {
                    "sent": "We actually want to get the exact instance of a building.",
                    "label": 0
                },
                {
                    "sent": "What you can do is you can prepare a data set that is more relevant to your task.",
                    "label": 0
                },
                {
                    "sent": "So banquet all the TC 2014 create a data set where class where one class is 1 landmark.",
                    "label": 0
                },
                {
                    "sent": "This is better but still 2 images from the same landmark don't necessarily need to have overlapping views, they don't have to observe the same object.",
                    "label": 0
                },
                {
                    "sent": "Also, they trained for classification, not for imaginary work.",
                    "label": 1
                },
                {
                    "sent": "There is one more way you can generate the cheap in achieve way a data set that has we connotations meaning GPS tags.",
                    "label": 0
                },
                {
                    "sent": "So there is a method verison metal net flood Byron Jelovich at all that actually uses this kind of a data set and they train directly for image retrieval.",
                    "label": 1
                },
                {
                    "sent": "However, here the spatially closest images don't necessarily need to be positive.",
                    "label": 0
                },
                {
                    "sent": "Images don't necessarily need to be looking at the same object and we don't know this because we don't know, rotation, so we have to handle this during the training.",
                    "label": 0
                },
                {
                    "sent": "What we propose is an automatic way to get annotations for images, and these annotations are strong enough that you don't have to care about anything during the CNN training.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we get these limitations?",
                    "label": 0
                },
                {
                    "sent": "We actually use our recently proposed state of the art image retrieval and structure from motion images real tightly coupled with structure for motion.",
                    "label": 0
                },
                {
                    "sent": "So we take advantage of imagery with significant viewpoints and scale change so that we get very complete and many very detailed 3D models.",
                    "label": 0
                },
                {
                    "sent": "And we do this for the whole data set, so for the whole data set we get all non overlapping 3D models that we can find.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we know all the camera positions, camera rotations, and we know the number of inliers between each pairs of images.",
                    "label": 1
                },
                {
                    "sent": "Given this information, we can proceed in an easy way to extract the positive and negative training examples.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For negative negative images are basically the images that are from different city models.",
                    "label": 0
                },
                {
                    "sent": "Then the query image and hard negative images are closest negative examples for the query in the CNN descriptor space.",
                    "label": 1
                },
                {
                    "sent": "So the hardest negative.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Image is the most similar image in the in the scene descriptor space.",
                    "label": 1
                },
                {
                    "sent": "Usually you don't just want one hard negative image.",
                    "label": 0
                },
                {
                    "sent": "You want to get more than one, and if you proceed in a naive way by.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bing top kimages in the CNN descriptor space.",
                    "label": 0
                },
                {
                    "sent": "You will end up with near duplicates.",
                    "label": 0
                },
                {
                    "sent": "I mean near duplicates with the hardest hard negative image.",
                    "label": 0
                },
                {
                    "sent": "You don't want this.",
                    "label": 0
                },
                {
                    "sent": "This is redundant data.",
                    "label": 0
                },
                {
                    "sent": "So what we propose is getting dive.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This hard negatives, which means that we get still we get K hard negative images, but we get only one image for 3D model.",
                    "label": 0
                },
                {
                    "sent": "Although hard negative mining for metric learning and object detection was a standard so far, this is not the case.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For positive images and we showed that getting hard positive images actually helps an improve the performance.",
                    "label": 0
                },
                {
                    "sent": "So positive images.",
                    "label": 0
                },
                {
                    "sent": "Positive examples are images that share 3D points with the query image and hard positive images are positive examples that are not close enough to the query in the CNN descriptor space.",
                    "label": 1
                },
                {
                    "sent": "Again, naive way to get a positive example is getting the most similar image in the CNN descriptor space.",
                    "label": 0
                },
                {
                    "sent": "Which is not good due to the reason that this image is already matched well with the query with the current CNN.",
                    "label": 0
                },
                {
                    "sent": "So the CNN will not learn much from it.",
                    "label": 0
                },
                {
                    "sent": "This was used in, for example, not flood the recent method, but we propose.",
                    "label": 0
                },
                {
                    "sent": "Is getting harder positives, So what you can do is something that worked well in bag of words, imagery.",
                    "label": 0
                },
                {
                    "sent": "We can get the nearest neighbor in the back of our descriptor space or you can even go harder.",
                    "label": 0
                },
                {
                    "sent": "You can get a random image from Tip top K images in the bag of words.",
                    "label": 0
                },
                {
                    "sent": "Descriptor space.",
                    "label": 0
                },
                {
                    "sent": "When we have the training pairs, when we generated them we can proceed with the learning.",
                    "label": 0
                },
                {
                    "sent": "We do the learning in a Siamese fashion which.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Means that we have two branch.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Network, one branch for the query image and the other branch for the respective positive or negative image.",
                    "label": 0
                },
                {
                    "sent": "The two branches share the weights, which means that the weights are updated equally during the training.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We minimize contrastive loss, which is very simple.",
                    "label": 0
                },
                {
                    "sent": "We want to penalize positive image.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, if they are far away.",
                    "label": 0
                },
                {
                    "sent": "And we want to penalize negative images if.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They are not far away enough.",
                    "label": 0
                },
                {
                    "sent": "We also tried.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Split or ranking loss contrastive loss is more strict and it requires more very accurate data which we do provide.",
                    "label": 1
                },
                {
                    "sent": "So in our setting contrast was always work better than triplet loss.",
                    "label": 0
                },
                {
                    "sent": "In recently reporting.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recently proposed CNN images drivel works that also works that work with compact image representations.",
                    "label": 0
                },
                {
                    "sent": "It has been shown that whitening benefits the performance a lot.",
                    "label": 0
                },
                {
                    "sent": "So what they did so far was learning whitening PCA whitening on an independent set of descriptors.",
                    "label": 1
                },
                {
                    "sent": "What we propose.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is do learning widening in a supervised way using our training data and linear discriminant projections?",
                    "label": 0
                },
                {
                    "sent": "But we also tried.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is doing the whitening not as a post processing step but in adding whitening in end to end learning however?",
                    "label": 0
                },
                {
                    "sent": "This performance of this was compatible or worse than supervised widening.",
                    "label": 1
                },
                {
                    "sent": "Learning in post processing.",
                    "label": 0
                },
                {
                    "sent": "So we decided to end it.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slowing down the convergence.",
                    "label": 0
                },
                {
                    "sent": "So we decided to stick with the post processing step, learning the widening in the supervised way with our labeled training data.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the experiments we use standard datasets that are used in image retrieval, so Oxford and Paris buildings and Holidays data set.",
                    "label": 0
                },
                {
                    "sent": "We use a standard protocol mean, average precision and very important.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the training 3D models, there is no landmark from testing data sets.",
                    "label": 0
                },
                {
                    "sent": "So our.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show that the careful choice of positive and negative images makes a difference starting from off the show.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Network each one of our can't.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Buttions",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or positive and negative.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mining.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Improves the performance.",
                    "label": 0
                },
                {
                    "sent": "Gives higher performance.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The end our learned lightening gives additional boost to the final results.",
                    "label": 0
                },
                {
                    "sent": "In another.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Element we wanted to test the overfitting of our network so we this time we added and only for this experiment we added testing landmarks during the training procedure and repeated the whole fine tuning process.",
                    "label": 0
                },
                {
                    "sent": "We observe.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A very small performance change, so we conclude that our network don't overfit to the training date.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also do extensive comparison with state of the art.",
                    "label": 0
                },
                {
                    "sent": "So we basically compare with every image retrieval method that we are aware of.",
                    "label": 0
                },
                {
                    "sent": "CNN based, an bag of word based.",
                    "label": 0
                },
                {
                    "sent": "And I can go slowly through the all the results, but I don't think that would be a good idea.",
                    "label": 0
                },
                {
                    "sent": "So what I would like to point out is.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that state of the art on 256 dimensions?",
                    "label": 0
                },
                {
                    "sent": "Previous state of the art net flat on Oxford 5K is outperformed by our fine to network with only 32 dimensions I would.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would also like to mention that on this conference there is a concurrent work that also does CNN image retrieval with compact codes.",
                    "label": 0
                },
                {
                    "sent": "At the end I would like.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compared the teacher and the student teacher being begger for its image retrieval that we use to generate our data and student being our fine tune, fine tune, CNN, Network.",
                    "label": 0
                },
                {
                    "sent": "So student is worse.",
                    "label": 0
                },
                {
                    "sent": "It's compareable.",
                    "label": 0
                },
                {
                    "sent": "It's very close but it's worse which is still a very nice achievement if you consider that student has that CNN's have compact image representations, which means much smaller memory footprint and much faster search.",
                    "label": 0
                },
                {
                    "sent": "Also after adding rerank.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And query expansion for the compiled codes.",
                    "label": 1
                },
                {
                    "sent": "We get higher student surpasses the teacher on all of the testing datasets.",
                    "label": 1
                },
                {
                    "sent": "Stay.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is space for improvement.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you would like to search a very small object, meaning an object that appears in a small partition of an image.",
                    "label": 0
                },
                {
                    "sent": "Usually like Apple logo due to its low.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quality nature bag of words will still be able to find the small objects very well, however.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "CNN struggles, which is kind of expected because CNN methods have compact codes.",
                    "label": 0
                },
                {
                    "sent": "Anna global representation.",
                    "label": 0
                },
                {
                    "sent": "One more case.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is for example painting of Mona Lisa, our data set.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Contains a lot of instances of this painting and bag of words is able to find them however, CNN.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Struggles, so it seems that fine tune network does not easily forget its past and tries to generalize whenever possible, so fine tuning.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would not be enough.",
                    "label": 0
                },
                {
                    "sent": "To come.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Loot.",
                    "label": 0
                },
                {
                    "sent": "We propose.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised way to generate generate the training.",
                    "label": 1
                },
                {
                    "sent": "Lots of training examples that is necessary for CNN training.",
                    "label": 1
                },
                {
                    "sent": "We also propose methods to use these data and to create hard, negative and hard positive examples.",
                    "label": 1
                },
                {
                    "sent": "Also, we propose supervised widening learning step that in most of the cases improve results.",
                    "label": 0
                },
                {
                    "sent": "Our training data and our train models are publicly available, so please feel free to use them.",
                    "label": 0
                },
                {
                    "sent": "And if you want to find out more details about the poster, visit us at the post session that's on the rooftop garden.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}