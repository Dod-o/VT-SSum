{
    "id": "7lnjvpcgybkcybxjzp7kim2l5y5kzjlx",
    "title": "Minimum Error Entropy Principle for Learning",
    "info": {
        "author": [
            "Ding-Xuan Zhou, City University of Hong Kong"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_zhou_learning/",
    "segmentation": [
        [
            "So I want to talk about minimum error entropy principle for learning and."
        ],
        [
            "My talk consists of essentially two parts.",
            "The first part is about least squares regular regression.",
            "With with features generated by Kernel PCA and then I will talk about minimum error entropy principle.",
            "OK so."
        ],
        [
            "And we started with the first part, review a little bit about the least square regularised regression problem.",
            "So the purpose of least square regression is try to learn a function F. Which is defined on a compact metric space with various in the set of real numbers.",
            "OK, and we are giving a set of samples XI.",
            "Why I from one to N and as usual we assume in our model that there is a probability major role defined on this product SpaceX cross Y.",
            "Which governs the sampling process.",
            "So by that we mean that from this role we have the marginal distribution on X and the sampling points is a random sample joint according to this marginal distribution and also for each point X in the metric SpaceX we have conditional distribution whose mean is nothing but the value of the regression function defined at this point.",
            "So our purpose is trying to learn this function F roll called.",
            "Regression function from the set of samples.",
            "The reason is why I is approximately equal to F Ro XI."
        ],
        [
            "Now we all know that the regression problem is ideal in the sense that the regression function F ro minimizes the least squares error generalization error defined in this way.",
            "This is because for any other function F, the excess generalization error can be written exactly as the.",
            "Uh, Norm Square of in the space L2 with respect to the marginal discretion Rolex.",
            "So once you minimize this least square generalization error, then you'll get the regression function.",
            "A classical algorithm is the empirical risk minimization, which says that we don't know this distribution role, but we have a set of samples drawing according to this distribution.",
            "So then we can replace this integral by taking this empirical mean.",
            "By discretisation process and then we minimize this empirical error among a set of functions, usually a set of continuous functions edge and if H is compared, then such a minimization process always makes sense.",
            "And when can?",
            "Take this output function as an approximation of the regression function, so there is a large literature in learning theory.",
            "Essentially, the analysis is divided into 2 parts.",
            "The so called sample error and then approximation error sample error says that if you replace the generalization error by this empirical mean, then the difference can be controlled uniformly for F runs over the set of the whole set of.",
            "R. Space on Edge in a uniform way, and then the approximation error says that if you choose the hypothesis space in a good way, then the regression function can be well approximated by this.",
            "Of course, if the hypothesis the hypothesis space is not so good in the sense that Afro is pretty far away from this space, then the approximation error could be a very large.",
            "So we have to choose a very good hypothesis space.",
            "In the EM algorithm."
        ],
        [
            "Now we all know that this algorithm has been well developed.",
            "Let me just mention a classical example saying that if we take the hypothesis space to be a bowl of radius R of the sovereign space of indexes with greater than half and so that space consists of continuous functions by embedding theorem, then if it happens that aggression function lies in this.",
            "High positive space.",
            "Then we know that the output function FZ from the EM algorithm actually approximates the integration function, where in a sense that the norm square decays to zero.",
            "In this rate, M to the minus 1 / 1 + N / 2 X where N is the dimension of the input.",
            "SpaceX and S is the smoothness index for the hypothesis space, so we know that if S is very large, meaning that here the functions are very smooth.",
            "Making this hypothesis space to be smaller than this can be arbitrarily close to one.",
            "This is a standard result.",
            "We know that it holds when the output random variable is uniformly bounded, almost surely, or if it decays exponentially fast in some way, OK, and.",
            "A recent interest in statistics is to consider the case when the noises help.",
            "Noise has heavy tail, meaning that why does not decay exponentially in anyway.",
            "One of the recent results is made under the assumption that the 4th moment of the output random variable is bonded, and in our study of minimal error entropy algorithms byproduct is to get.",
            "Error bounds of this kind under a weaker assumption that we only require output render valuable if you take the power of Q for any Q greater than two, then analysis works well, which is weaker than the previous work.",
            "But this is only by product in mathematics now."
        ],
        [
            "Yeah, so the first algorithm I'm going to discuss about is the regularised least square regression.",
            "We all know that we're giving a Mercer kernel, which is continuous, symmetric, positive, semi definite.",
            "Then we can generate a reproducing kernel Hilbert space with these fundamental functions and then here is the regularization scheme which is run over the whole space HK.",
            "With this penalty term OK, and we all know that because of the Representer theorem.",
            "The output function must lie in.",
            "This must have this form which lies in the funny dimensional space generated by fundamental functions.",
            "With this T taking the sampling of points OK, and these confessions can be solved by a linear system, but in general there's no sparsity in this form.",
            "Now here I want to talk about this.",
            "Regularised least square regression with features learned from kernel PCA.",
            "So let me."
        ],
        [
            "Remind you of PCA.",
            "We're giving EM samples of vectors in RN, so this is a sampling metrics.",
            "And then we can define.",
            "We want to actually approximate the data points.",
            "By vectors from affine space which is constant plus U times better where you consists of K column vectors which are orthogonal, then we can approximate each of the data points by vector.",
            "From this find space which is freedom of dimension K and we know that the constant term is easy.",
            "Just take the empirical mean and then you can be found by the sample covariance matrix.",
            "After you'll find the same eigenvalues eigenvectors, then you'll get this matrix you with columns being the orthogonal.",
            "Orthogonal basis OK. Now in kernel PCA we do almost the same as before.",
            "That is, we replace the previous covariance matrix.",
            "Here we do not take this normalization covariance matrix by."
        ],
        [
            "An empirical version of the integral operator, so we know that once K is a Mercer kernel, then we can define an integral operator on this reproducing kernel Hilbert space, and we can approximate this.",
            "Integral operator in a finite form using these sampling points XI I from one to N by discretized integration by this finite summation.",
            "Because FX is equal to F, inner product with KX, that's the reproducing property.",
            "Now this place the role as the covariance matrix and then we can use its eigenvalues and eigenvectors eigenfunctions to as features OK so.",
            "If you have eigenvalues, say Lambda X.",
            "Here the eigenvalue of course depends on the sampling set X, and then the eigenfunctions Fire IX also depends on the sampling set.",
            "OK, now of course this is functional analysis approach.",
            "You can also do in the other way.",
            "That is, we start with the gram gram matrix kxi, XJ, J from one to MM is the sample size.",
            "And then you take the eigenvalues and eigen vectors.",
            "But then for each eigenvector you'll take its components and as a linear combination of the fundamental functions.",
            "These are exactly the empirical features we want to use for our kernel.",
            "PCA algorithms.",
            "OK, so once we have these empirical features?",
            "Yeah.",
            "Find IX they actually they approximated the eigen functions of the integral operator in a nice way, but we need to handle it in a uniform way that is convergents of 1 function Phi X25I is not enough.",
            "We need the uniform convergence."
        ],
        [
            "So once we have this empirical features find IX, they are constructed from the sampling points X sampling said.",
            "Then we can use their linear combinations of this kind as.",
            "A target function to learn the regression function in our regression scheme.",
            "So here is the least square empirical riskware era and plus a penalty term.",
            "So here the penalty function is a universal function.",
            "Each time we act it on the confession CJ4J from one to Infinity.",
            "But actually we only need until N because all the others are.",
            "These will disappear.",
            "So naturally when Jay goes from N + 1 to Infinity, all the confession CJ will be 0.",
            "Kind of algorithm has appeared in the literature like in the kernel projection machines in a worker swap, and so on, where they take a finite number of a fixed number of empirical samples, some special cases if you take Omega to be their square function, then this is exactly the.",
            "Icicle Ridge regression.",
            "And this actually is the same as the normal as the classical regularised least square regression in the space HK.",
            "So K Norm square corresponding to the square norm in the penalty.",
            "OK.",
            "But for the purpose of sparsity, usually we can you say the Q norm when Q is less than or equal to 1.",
            "This has been used many times in statistics, and we can also use the so-called sketch.",
            "Penalty, which is almost the same as when Q equals to one but controls at Infinity.",
            "OK."
        ],
        [
            "Now.",
            "What's?"
        ],
        [
            "Nice about this algorithm is we do not need to solve any optimization problem, we do, but it's reduced into a very simple one.",
            "Because although it looks that you have infinitely many coefficients, actually only M coefficients, but you can, because of the discrete orthogonality of this empirical features, this actually can be separated out into each individual coefficients.",
            "So at the end, what."
        ],
        [
            "We have is for each CJ.",
            "We can solve a univariate.",
            "Function minimization problem.",
            "So this is a function of a single variable C. All these are coefficients.",
            "So the function depends on C in the quadratic form, and then in this penalty form, and we only need to minimize such function.",
            "So this SIZ is the coefficients depending on XI, an Yi and of course the empirical features.",
            "But once the samples are given then SIZ can be computed explicitly and you have eigenvalues.",
            "So therefore you solve this.",
            "Minimize this universal function.",
            "You'll get the coefficients CI zed for each fixed I you solve for such a, you minimize such a univariate function.",
            "So the optimization problem is trivial.",
            "In particular, if you take for example, you take the penalty to be see to the Q when Q is half, for example, then this is.",
            "The minimizer is realized by.",
            "The load of a cubic polynomial which has can be solved explicitly for the other queue.",
            "Of course you have to solve this.",
            "Minimize this univariate function OK."
        ],
        [
            "Now the second part I want to talk about here is about the concave for penalty function.",
            "The concave property actually will give us sparsity so.",
            "This is a theorem mathematical theorem saying that if Omega is a univariate concave function with various on the positive real axis, then.",
            "Um?",
            "From zero to 1, the function has a lower bound of C and from one to Infinity.",
            "The function has upper bound of C, so for sure it is lower bounded by this.",
            "As long as this is a non zero penalty function.",
            "Of course we require this to be concave.",
            "OK, concavity plays an important role.",
            "Um?",
            "Yes, I'm all over our.",
            "We can show that if you consider penalty functions like this, for example, if it's E to the power of Q with Q greater than one, then the derivative at origin equals to zero.",
            "In this case, sparsity is hard to obtain.",
            "The reason is if you want the ice coefficients to be 0, then you have to require either the eigenvalue to be 0 or the constant we defined before it's 0.",
            "So which is hard to obtain, so that's why.",
            "We say that if you take the penalty to be the square, then it's difficult to get sparsity.",
            "In general.",
            "We can, for example, for the Q normal penalty.",
            "We're curious between zero and one.",
            "Then we know that such an upper bound property holds for C between zero and one, and like the scared penalty, it has also this property with Q equals to one.",
            "So that's why we introduce such a concave exponent 4.",
            "A penalty function Omega which killed between zero and one with such a property then."
        ],
        [
            "Our analysis shows that if the kernel has certain smoothness, for example, it has some software smoothness and we know that in this case the eigenvalues, usually the eigenvalues of the integral operator, OK usually decays polynomially fast, so we assume this, and we assume the approximation error condition saying that the function want to learn actually lies in the Earth power of of the integral operator LK.",
            "With some, of course.",
            "Import function zero also in HK because this we define this integral operator on HK.",
            "Now under this two conditions.",
            "Then if we choose regularization parameter in this way.",
            "Of course this is other visual choice with some priority condition, but then we get sparsity and approximation error.",
            "So meaning that remember that CIZ might be 0 for I from one to N. Here we say that if I is greater than or equal to M to this power plus one, this power is less than one, then the confession must be 0.",
            "So that means you have at most this amount of non zero coefficients which compared with the sample size M is much smaller because the power is less than one and the convergence rate is also M to some positive power which is complicated depending on this concave.",
            "Exponent so if we have the kill penalty then we have this Q and from this expression actually we see that if Q becomes smaller and smaller, the learning rate becomes worse and worse.",
            "OK, so if Q is 1, the learning rate in this form is the best.",
            "OK, on the other hand this.",
            "Yeah, this sparsity results.",
            "It seems that it does not depend on the concave parameter Q, but in our simulation.",
            "Actually it depends, as in a slightly.",
            "Away but."
        ],
        [
            "So let me explain to you about some simulation so that you have some ideas about the algorithm.",
            "So we did some artificial data experiment and the result is very very good.",
            "Like one example we use is we use the Gaussian kernel with the regression function, also in Gaussian way then.",
            "Take the uniform noise, then turns out that the sparsity is like 2 two 6%, which is very good, but I want to talk about this.",
            "Real data is from human knowledge, so here are the data consists of 14 groups.",
            "Each group corresponds to one HALAA, and for each array we have a group of peptides PA.",
            "The number of our peptides.",
            "Is at least 420.",
            "Could be a few 1000 so it's hundreds or thousands for each LA.",
            "So you can consider this to be XI I from one to M. So the sample size here M is a few 100 or thousands.",
            "Then for each layer we have this so called affinity so that you can consider this to be the regression problem.",
            "That is, for each XI we have Yi so.",
            "This YP for each peptide P continues as the regression value Yi.",
            "It varies between zero and one called affinity.",
            "Now this is.",
            "A benchmark problem in my knowledge.",
            "Until 2009, Nielsen and alone used some artificial neural network based algorithm called NN alignment to give the best result until then.",
            "And then since Steve Smell Joint City in 2009, he established a seminar on if knowledge and a lot of people are involved and they use a string kernel, they denoted as K had three and apply the least square regular delegation, the one everyone uses, and they get slightly better result than the.",
            "State of art result given by Nielsen alone at 2000.",
            "That's the intent 812.",
            "And even a group of people from our seminar attended a machine learning competition in immunology, and they they won the competition in one of the two categories, OK?",
            "So we want to use this benchmark data for our algorithm based on kernel PCA an all result is even slightly better than other results."
        ],
        [
            "Let me explain to you what do we mean by slightly better.",
            "We use two measurements to measure the error.",
            "One measurement is root mean square error.",
            "So our output our algorithm provides for each peptide.",
            "An affinity YP theater and from previous branch benchmark data we have YP the real affinity number, and then we take the difference.",
            "And that's we called root mean square error of the affinity, and we also use another."
        ],
        [
            "One called an area under the curve.",
            "Number which say that in this data.",
            "In this surreal data.",
            "Peptide binding affinity problem.",
            "We use this threshold of 0.426.",
            "We say that if the affinity is bigger than this then that means this peptide is binding to the earlier.",
            "So remember that a is LA and this yeah.",
            "Yeah, P is the peptide.",
            "OK so all these peptides are considered to be binding and then all the others are unbonding.",
            "OK, so if the algorithm is good when then we expect that if P is binding but P prime is not, then the predicted binding affinity should be bigger than the bonding one.",
            "So so we we count how many type pairs satisfy this condition and.",
            "Compared with the total number, this number is considered to be is defined as AUC.",
            "It must be between zero and one.",
            "And of course, if the number is higher than the algorithm is better.",
            "OK, so here is our."
        ],
        [
            "Result.",
            "It's rather complicated, so like for each area we have, this is the number of peptides in corresponding to this area.",
            "It's 5000 something and then for the second one 1000 and then wait a few 102 hundred 420.",
            "OK."
        ],
        [
            "Yes, altogether there are 14 groups."
        ],
        [
            "And the classical result are the out of state of the art result in 2009.",
            "They have this use neural network alignment.",
            "They have these results and their result is is better in five out of 14 groups and the results of Stevens Mill and his collaborators.",
            "They are better in four.",
            "9 out of 14 groups an hour result when we use the empirical feature based regularization.",
            "Is better than even Stevens smells without 45 out of the."
        ],
        [
            "Party groups, so this is the average result, so we know that for least square regular celebration, the classical one, there's no sparsity, and this percentage is the percentage of non zero coefficients.",
            "You can see that as far as it is not so strong.",
            "OK, and the AUC error.",
            "You can see that the classical one is 0.79 and the group of Stevens Mail there improved by.",
            "Like 0.55% OK, we improved further a little bit by 0.11%, which seems."
        ],
        [
            "Be very little, let me make some observation about this so.",
            "The regularly square improved by 0.55% and we improved by 0.11% only.",
            "And yeah, it seems that the improvement is is very mild.",
            "However, we found that.",
            "In the classical medical literature, this dissimilarly metric, called Blossom 62, is well established.",
            "This is the dissimilarity metric for the set of 20ML assets.",
            "And there is a large medical literature.",
            "You can see 62 meaning that there are hundreds of such dissimilarity matrices and each is based on large statistical analysis.",
            "So the kernel is very good.",
            "So no matter which method you use, actually the result is quite similar and in fact the regularization parameter we obtained by cross validation is like 10 to the minus 4.",
            "Which is a very small so meaning that even if you do not use regularization, result could be a pretty good.",
            "So that's why this real data problem.",
            "It seems that."
        ],
        [
            "We can only improve a little bit and then recently we are doing some other problem.",
            "One is to study the so-called path again.",
            "You know pathogen genus election problem for influenza virus and a friend of mine has already made some medical experiments pretty expensive and then former state of mind has done some.",
            "Learning theory analysis using mutual information analyst.",
            "Will regular celebration anelastic Nate.",
            "But the result is not that great and I cannot report here.",
            "The correlation is only like 0.6, which is not.",
            "Not so sad."
        ],
        [
            "Factory and so our idea is probably in these data involving peptides or protein structures may be the least square error is not a good idea, so using the difference of numbers is probably not.",
            "A good measurements because over there the patterns are more important than like here we use.",
            "Affinity, it's a real number which may not be a good idea, so the our idea is maybe we can use some methods from theoretical learning.",
            "Yeah.",
            "Information, theoretical learning literature and to introduce some measurements for problems involving purpis or approaching structures.",
            "OK, so I will choose one which is called a minimum error entropy principle and to explain to you some ideas why these might be used for learning OK.",
            "There's a large literature already developed for more than 10 years, essentially by the Group of Prince side and many other people.",
            "But sometimes people from different fields encountered entropies like your hand has interesting paper.",
            "To say that entropy is minimized in some cases, so we want to study this minimal error entropy principle in a systematic way to give a general framework.",
            "About this mathematical analysis, because so far there are minimally applied at work in this direction, they apply to this principle to different kind of applications like adaptive system training, a blind source separation, clustering and classification, flying deconvolution and so on.",
            "The the original idea is trying to extract from data as much information as possible about the data generating systems by minimizing error entropies instead of minimizing the least square errors.",
            "OK, so we replace the least square errors by entropy error entropies.",
            "So to remind you about the classical entropy, Shannon's entropy is defined if you have a random variable E with PDF then.",
            "Is defining this way essentially is the expected minus expectation of lager Father PDF and Arraigns entropy is simply replace this one by PE to the power of Alpha minus one actually?",
            "Yeah, and normalized.",
            "This depends on the parameter Alpha an for Alpha not equal to 1.",
            "Now if Alpha goes to one, then rains entropy will approach the channels entropy.",
            "So we will consider a very special case of rains.",
            "Entropy of order two, where R equals to two, then the entropy is defined by law Gov.",
            "Expectation of the PDF PE, which is a very simple.",
            "1."
        ],
        [
            "Now remember that this entropy is defined.",
            "By means of the PDF of a random variable, he hear the random variable E is Y minus FX4 hour regression problem?",
            "OK, so we consider the difference between predicted value and output Y.",
            "And so we need to have the PDF of this random variable.",
            "Once you have the PDF, then we know that the least square error is defined actually as the square expected value of the square of this random variable.",
            "If you're writing this in terms of the PDF, then it's E square PE.",
            "Take the integration.",
            "So here you can see that only the second moment of the PDF is involved when you look at the least squares error.",
            "Now the idea of the.",
            "Mde minimal error entropy is to consider not the error of not this early square form, but in a different form.",
            "For example, if we use the rains entropy of order two, which we will study here, then the.",
            "Yeah, that that's that's full.",
            "OK, so it's minus log integration of PEP, so if you consider this to be the expectation, then we replace this E square by the PDF.",
            "OK, so here we only consider E square.",
            "Here we consider a function P which involves all kinds of moments.",
            "So that's why if we use the entropy concepts in measuring the error.",
            "The moments of all orders are might be involved and which may handle on more complicated noise.",
            "We all know that if the noise is Gaussian, then Lee Square is is very nice because it involves the second moment.",
            "But if you have a non Gaussian noise then probably minimal error entropy may give good result.",
            "That's the mayor."
        ],
        [
            "OK, now we use kernel approximation in the process because we know."
        ],
        [
            "So that this one, actually you cannot compute because we don't know this PE.",
            "So what we can do is we approximate this.",
            "Yeah, we approximate this one by taking its discretized form, so that means the integration of PE with respect to this probability measure we replace it."
        ],
        [
            "By this one, so the integration is replaced by the empirical sun.",
            "OK empirical mean.",
            "Now what is this P = P?"
        ],
        [
            "He is unknown.",
            "We need to approximate this probability density function of this error right?"
        ],
        [
            "Available so therefore we here we use kernel approximation that is, in order to approximate PE we use a PE head.",
            "To use a set of samples EI, remember that each E is y -- F XOESYI minus FX I, so we use this set of samples for I from one to N and then we approximate the PDF.",
            "The probability density function by this.",
            "Passing windows for I from one to N. Here is GE minus EI Square over 2 square and take the average.",
            "OK so he rages console called the scaling parameter in opposing Windows process.",
            "Typical example is if you take G to BE to the minus T then that's the classical Gaussian windowing and once we have these two process, why is discredited discretization?",
            "The other one is approximated the PDF by this empirical one.",
            "Then we have this complicated form involving two layers of summation.",
            "One is I from one to N, the other one is inside PE head which also for J2 from J from one to M. So you can see that we have two layers summation I from one to NJ from one to N and then replace EI by why I minus FX I and then EJ by this one.",
            "So this algorithm, actually former student of mine who.",
            "Yeah, it's a coauthor.",
            "Mine in this work who are refereed a paper, I think for neural computation or some other journals, he notice that the paper gives very interesting observations, but no mathematical analysis, and in particular the application there says that.",
            "In order to apply this algorithm, you have to take first age to be large, and then you're left edge to be smaller and smaller.",
            "Moreover, they observed that if H is large then the result is pretty good.",
            "If Edge is more than the result may be strange.",
            "This is very strange because important windows we expect that age should go to zero in order for this function to approximate the PDF very well.",
            "OK, so why this happens?",
            "He discussed it with me and then."
        ],
        [
            "We thought of this for awhile and then we made some analysis for this Amy algorithm.",
            "OK, so the analysis consists of two parts.",
            "The first part has just been published by Journal Machine Learning Research and it says that if H is large then the algorithm works well for the regression problem.",
            "If Edge is more than something strange may happen.",
            "So this is consistent to what people have observed from practical applications.",
            "In system identification.",
            "So here we assume that as I said before, we assume the output random valuable only requires such a weak condition, not uniformly bounded for sure.",
            "And you can consider this window function to be just E to the minus, T Gaussian windowing, and we assume for this."
        ],
        [
            "I positive space in the algorithm.",
            "This is EM algorithm but containing a pair of samples here, not like the usual only one layer submission."
        ],
        [
            "We assume that the covering number of the high positive space increase only epsilon to the minus P for some positive P. After you take log.",
            "This is always true.",
            "If H is a bowl of the server space with index greater than half of the input space dimension, and if it's a, it's above the reproducing kernel Hilbert space generated by Gaussian kernel, then this for sure is satisfy even for an arbitrarily small P."
        ],
        [
            "Now, once we have this assumption, then our result says that the variance between the output function and the regression function decays like this power plus some approximation error term.",
            "So if the regression function lies in the hypothesis space then disappear, this term disappears and then we only have this sample error term.",
            "Now, if you assume a stronger condition about uniform boundedness for the output random variable, then our result is even better is empty.",
            "The minus 1 / 1 + P, which this index can be arbitrarily close to one.",
            "If P is a small enough like the reproducing kernel Hilbert space is generated by Gaussian kernels."
        ],
        [
            "OK, now the method.",
            "The main idea when I thought of this problem is I thought if H is large then we can just take the Taylor series.",
            "And this is actually is really the case.",
            "We can approximate this generalization error by the least squares error and then all the analysis follows.",
            "Of course the."
        ],
        [
            "The analysis is not that easy, but that's the main idea.",
            "Now the second part of our analysis is when H is more.",
            "This is for small parameter.",
            "Here we only consider this Gaussian window in case now we consider two kinds of consistency.",
            "Why is the regression consistency?",
            "If you consider only the regression problem, we assume that our output function should approximate the regulation function well, so that's why we consider the error between the.",
            "Output function and the regression function up to a constant and we have very easy estimates for this.",
            "This converges to 0.",
            "Then we say regression consistency.",
            "The other way is called Angel be consistency saying that the Alpha function may not go to the regression function here too, but it approximates the minimizer of the entropy among this among the set of functions OK.",
            "This we call entropy consistency."
        ],
        [
            "So our first result says that the entropy consistency always holds.",
            "That is, if you measure the error by entropy instead of the least square error, then the minimal error entropy algorithm always works very well because the entropy consistency always holds under the assumption that H go to Infinity or no edge Godzilla.",
            "As I said an HPS Times Square root of N go to Infinity, so it should go to.",
            "Zero, but in us in a slow way, then the consistency holds.",
            "Entropy.",
            "Consistency holds an we can have error learning rate."
        ],
        [
            "Now the.",
            "The other part is about regression consistency.",
            "So for regression consistency, we divide the discussion into two parts.",
            "1st part is for homogeneous noise called Homeless Skeptic model saying that this noise is independent of X, which is often used in statistics.",
            "Then the regression consistency holds.",
            "However, if."
        ],
        [
            "You consider the non homogeneous noise.",
            "That's the analogy."
        ],
        [
            "Yes, I want to skip."
        ],
        [
            "And if it's a for the Heathrow, scared that stick models that it means the noise is not uniform.",
            "Here is 1 example only interval zero to half the noise is is uniform from minus half to half an on these other part then the noise is uniform on two separated intervals.",
            "The total length is also one.",
            "So it seems that the meeting should be 0.",
            "However, it turns out that the minimizer of the entropy Reigns entropy is not realized by zero, is realized by something else.",
            "Such a function, which has since one among these two intervals OK, and the regression function, which is 0, the anchovy, actually is not.",
            "It's not at the minimum.",
            "OK, it's pretty far away from the minimum, so that explains why people have observed in practical application that if you consider this any algorithm for the purpose of regression.",
            "It might happen that some strange phenomenon will be found, like here.",
            "Actually, the output function does not converge to the regression function at all.",
            "But if you look at the error by means of entropy, then the result is still very good.",
            "So this is."
        ],
        [
            "Only theoretical study and we're trying to use the algorithm for so for the real data problems.",
            "As I said before, especially for influenza virus problems, but.",
            "At this moment we haven't started that optimization problem yet because any algorithm is the optimization problem is rather complicated is not as easy as what we have in kernel method.",
            "OK, thank you very much for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to talk about minimum error entropy principle for learning and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My talk consists of essentially two parts.",
                    "label": 0
                },
                {
                    "sent": "The first part is about least squares regular regression.",
                    "label": 0
                },
                {
                    "sent": "With with features generated by Kernel PCA and then I will talk about minimum error entropy principle.",
                    "label": 1
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we started with the first part, review a little bit about the least square regularised regression problem.",
                    "label": 1
                },
                {
                    "sent": "So the purpose of least square regression is try to learn a function F. Which is defined on a compact metric space with various in the set of real numbers.",
                    "label": 1
                },
                {
                    "sent": "OK, and we are giving a set of samples XI.",
                    "label": 0
                },
                {
                    "sent": "Why I from one to N and as usual we assume in our model that there is a probability major role defined on this product SpaceX cross Y.",
                    "label": 1
                },
                {
                    "sent": "Which governs the sampling process.",
                    "label": 0
                },
                {
                    "sent": "So by that we mean that from this role we have the marginal distribution on X and the sampling points is a random sample joint according to this marginal distribution and also for each point X in the metric SpaceX we have conditional distribution whose mean is nothing but the value of the regression function defined at this point.",
                    "label": 1
                },
                {
                    "sent": "So our purpose is trying to learn this function F roll called.",
                    "label": 0
                },
                {
                    "sent": "Regression function from the set of samples.",
                    "label": 0
                },
                {
                    "sent": "The reason is why I is approximately equal to F Ro XI.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we all know that the regression problem is ideal in the sense that the regression function F ro minimizes the least squares error generalization error defined in this way.",
                    "label": 1
                },
                {
                    "sent": "This is because for any other function F, the excess generalization error can be written exactly as the.",
                    "label": 0
                },
                {
                    "sent": "Uh, Norm Square of in the space L2 with respect to the marginal discretion Rolex.",
                    "label": 0
                },
                {
                    "sent": "So once you minimize this least square generalization error, then you'll get the regression function.",
                    "label": 0
                },
                {
                    "sent": "A classical algorithm is the empirical risk minimization, which says that we don't know this distribution role, but we have a set of samples drawing according to this distribution.",
                    "label": 1
                },
                {
                    "sent": "So then we can replace this integral by taking this empirical mean.",
                    "label": 0
                },
                {
                    "sent": "By discretisation process and then we minimize this empirical error among a set of functions, usually a set of continuous functions edge and if H is compared, then such a minimization process always makes sense.",
                    "label": 0
                },
                {
                    "sent": "And when can?",
                    "label": 0
                },
                {
                    "sent": "Take this output function as an approximation of the regression function, so there is a large literature in learning theory.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the analysis is divided into 2 parts.",
                    "label": 0
                },
                {
                    "sent": "The so called sample error and then approximation error sample error says that if you replace the generalization error by this empirical mean, then the difference can be controlled uniformly for F runs over the set of the whole set of.",
                    "label": 0
                },
                {
                    "sent": "R. Space on Edge in a uniform way, and then the approximation error says that if you choose the hypothesis space in a good way, then the regression function can be well approximated by this.",
                    "label": 1
                },
                {
                    "sent": "Of course, if the hypothesis the hypothesis space is not so good in the sense that Afro is pretty far away from this space, then the approximation error could be a very large.",
                    "label": 0
                },
                {
                    "sent": "So we have to choose a very good hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "In the EM algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we all know that this algorithm has been well developed.",
                    "label": 0
                },
                {
                    "sent": "Let me just mention a classical example saying that if we take the hypothesis space to be a bowl of radius R of the sovereign space of indexes with greater than half and so that space consists of continuous functions by embedding theorem, then if it happens that aggression function lies in this.",
                    "label": 0
                },
                {
                    "sent": "High positive space.",
                    "label": 0
                },
                {
                    "sent": "Then we know that the output function FZ from the EM algorithm actually approximates the integration function, where in a sense that the norm square decays to zero.",
                    "label": 0
                },
                {
                    "sent": "In this rate, M to the minus 1 / 1 + N / 2 X where N is the dimension of the input.",
                    "label": 0
                },
                {
                    "sent": "SpaceX and S is the smoothness index for the hypothesis space, so we know that if S is very large, meaning that here the functions are very smooth.",
                    "label": 0
                },
                {
                    "sent": "Making this hypothesis space to be smaller than this can be arbitrarily close to one.",
                    "label": 0
                },
                {
                    "sent": "This is a standard result.",
                    "label": 0
                },
                {
                    "sent": "We know that it holds when the output random variable is uniformly bounded, almost surely, or if it decays exponentially fast in some way, OK, and.",
                    "label": 1
                },
                {
                    "sent": "A recent interest in statistics is to consider the case when the noises help.",
                    "label": 0
                },
                {
                    "sent": "Noise has heavy tail, meaning that why does not decay exponentially in anyway.",
                    "label": 1
                },
                {
                    "sent": "One of the recent results is made under the assumption that the 4th moment of the output random variable is bonded, and in our study of minimal error entropy algorithms byproduct is to get.",
                    "label": 1
                },
                {
                    "sent": "Error bounds of this kind under a weaker assumption that we only require output render valuable if you take the power of Q for any Q greater than two, then analysis works well, which is weaker than the previous work.",
                    "label": 0
                },
                {
                    "sent": "But this is only by product in mathematics now.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so the first algorithm I'm going to discuss about is the regularised least square regression.",
                    "label": 0
                },
                {
                    "sent": "We all know that we're giving a Mercer kernel, which is continuous, symmetric, positive, semi definite.",
                    "label": 0
                },
                {
                    "sent": "Then we can generate a reproducing kernel Hilbert space with these fundamental functions and then here is the regularization scheme which is run over the whole space HK.",
                    "label": 0
                },
                {
                    "sent": "With this penalty term OK, and we all know that because of the Representer theorem.",
                    "label": 1
                },
                {
                    "sent": "The output function must lie in.",
                    "label": 0
                },
                {
                    "sent": "This must have this form which lies in the funny dimensional space generated by fundamental functions.",
                    "label": 0
                },
                {
                    "sent": "With this T taking the sampling of points OK, and these confessions can be solved by a linear system, but in general there's no sparsity in this form.",
                    "label": 1
                },
                {
                    "sent": "Now here I want to talk about this.",
                    "label": 0
                },
                {
                    "sent": "Regularised least square regression with features learned from kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "So let me.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Remind you of PCA.",
                    "label": 0
                },
                {
                    "sent": "We're giving EM samples of vectors in RN, so this is a sampling metrics.",
                    "label": 1
                },
                {
                    "sent": "And then we can define.",
                    "label": 0
                },
                {
                    "sent": "We want to actually approximate the data points.",
                    "label": 1
                },
                {
                    "sent": "By vectors from affine space which is constant plus U times better where you consists of K column vectors which are orthogonal, then we can approximate each of the data points by vector.",
                    "label": 1
                },
                {
                    "sent": "From this find space which is freedom of dimension K and we know that the constant term is easy.",
                    "label": 0
                },
                {
                    "sent": "Just take the empirical mean and then you can be found by the sample covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "After you'll find the same eigenvalues eigenvectors, then you'll get this matrix you with columns being the orthogonal.",
                    "label": 0
                },
                {
                    "sent": "Orthogonal basis OK. Now in kernel PCA we do almost the same as before.",
                    "label": 0
                },
                {
                    "sent": "That is, we replace the previous covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Here we do not take this normalization covariance matrix by.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An empirical version of the integral operator, so we know that once K is a Mercer kernel, then we can define an integral operator on this reproducing kernel Hilbert space, and we can approximate this.",
                    "label": 1
                },
                {
                    "sent": "Integral operator in a finite form using these sampling points XI I from one to N by discretized integration by this finite summation.",
                    "label": 0
                },
                {
                    "sent": "Because FX is equal to F, inner product with KX, that's the reproducing property.",
                    "label": 0
                },
                {
                    "sent": "Now this place the role as the covariance matrix and then we can use its eigenvalues and eigenvectors eigenfunctions to as features OK so.",
                    "label": 0
                },
                {
                    "sent": "If you have eigenvalues, say Lambda X.",
                    "label": 0
                },
                {
                    "sent": "Here the eigenvalue of course depends on the sampling set X, and then the eigenfunctions Fire IX also depends on the sampling set.",
                    "label": 0
                },
                {
                    "sent": "OK, now of course this is functional analysis approach.",
                    "label": 0
                },
                {
                    "sent": "You can also do in the other way.",
                    "label": 1
                },
                {
                    "sent": "That is, we start with the gram gram matrix kxi, XJ, J from one to MM is the sample size.",
                    "label": 0
                },
                {
                    "sent": "And then you take the eigenvalues and eigen vectors.",
                    "label": 0
                },
                {
                    "sent": "But then for each eigenvector you'll take its components and as a linear combination of the fundamental functions.",
                    "label": 1
                },
                {
                    "sent": "These are exactly the empirical features we want to use for our kernel.",
                    "label": 0
                },
                {
                    "sent": "PCA algorithms.",
                    "label": 1
                },
                {
                    "sent": "OK, so once we have these empirical features?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Find IX they actually they approximated the eigen functions of the integral operator in a nice way, but we need to handle it in a uniform way that is convergents of 1 function Phi X25I is not enough.",
                    "label": 0
                },
                {
                    "sent": "We need the uniform convergence.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once we have this empirical features find IX, they are constructed from the sampling points X sampling said.",
                    "label": 0
                },
                {
                    "sent": "Then we can use their linear combinations of this kind as.",
                    "label": 0
                },
                {
                    "sent": "A target function to learn the regression function in our regression scheme.",
                    "label": 0
                },
                {
                    "sent": "So here is the least square empirical riskware era and plus a penalty term.",
                    "label": 0
                },
                {
                    "sent": "So here the penalty function is a universal function.",
                    "label": 0
                },
                {
                    "sent": "Each time we act it on the confession CJ4J from one to Infinity.",
                    "label": 0
                },
                {
                    "sent": "But actually we only need until N because all the others are.",
                    "label": 0
                },
                {
                    "sent": "These will disappear.",
                    "label": 0
                },
                {
                    "sent": "So naturally when Jay goes from N + 1 to Infinity, all the confession CJ will be 0.",
                    "label": 0
                },
                {
                    "sent": "Kind of algorithm has appeared in the literature like in the kernel projection machines in a worker swap, and so on, where they take a finite number of a fixed number of empirical samples, some special cases if you take Omega to be their square function, then this is exactly the.",
                    "label": 0
                },
                {
                    "sent": "Icicle Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "And this actually is the same as the normal as the classical regularised least square regression in the space HK.",
                    "label": 0
                },
                {
                    "sent": "So K Norm square corresponding to the square norm in the penalty.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But for the purpose of sparsity, usually we can you say the Q norm when Q is less than or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "This has been used many times in statistics, and we can also use the so-called sketch.",
                    "label": 0
                },
                {
                    "sent": "Penalty, which is almost the same as when Q equals to one but controls at Infinity.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What's?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nice about this algorithm is we do not need to solve any optimization problem, we do, but it's reduced into a very simple one.",
                    "label": 0
                },
                {
                    "sent": "Because although it looks that you have infinitely many coefficients, actually only M coefficients, but you can, because of the discrete orthogonality of this empirical features, this actually can be separated out into each individual coefficients.",
                    "label": 0
                },
                {
                    "sent": "So at the end, what.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have is for each CJ.",
                    "label": 0
                },
                {
                    "sent": "We can solve a univariate.",
                    "label": 0
                },
                {
                    "sent": "Function minimization problem.",
                    "label": 0
                },
                {
                    "sent": "So this is a function of a single variable C. All these are coefficients.",
                    "label": 0
                },
                {
                    "sent": "So the function depends on C in the quadratic form, and then in this penalty form, and we only need to minimize such function.",
                    "label": 0
                },
                {
                    "sent": "So this SIZ is the coefficients depending on XI, an Yi and of course the empirical features.",
                    "label": 0
                },
                {
                    "sent": "But once the samples are given then SIZ can be computed explicitly and you have eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So therefore you solve this.",
                    "label": 0
                },
                {
                    "sent": "Minimize this universal function.",
                    "label": 0
                },
                {
                    "sent": "You'll get the coefficients CI zed for each fixed I you solve for such a, you minimize such a univariate function.",
                    "label": 0
                },
                {
                    "sent": "So the optimization problem is trivial.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you take for example, you take the penalty to be see to the Q when Q is half, for example, then this is.",
                    "label": 0
                },
                {
                    "sent": "The minimizer is realized by.",
                    "label": 0
                },
                {
                    "sent": "The load of a cubic polynomial which has can be solved explicitly for the other queue.",
                    "label": 0
                },
                {
                    "sent": "Of course you have to solve this.",
                    "label": 0
                },
                {
                    "sent": "Minimize this univariate function OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the second part I want to talk about here is about the concave for penalty function.",
                    "label": 0
                },
                {
                    "sent": "The concave property actually will give us sparsity so.",
                    "label": 0
                },
                {
                    "sent": "This is a theorem mathematical theorem saying that if Omega is a univariate concave function with various on the positive real axis, then.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "From zero to 1, the function has a lower bound of C and from one to Infinity.",
                    "label": 0
                },
                {
                    "sent": "The function has upper bound of C, so for sure it is lower bounded by this.",
                    "label": 0
                },
                {
                    "sent": "As long as this is a non zero penalty function.",
                    "label": 0
                },
                {
                    "sent": "Of course we require this to be concave.",
                    "label": 0
                },
                {
                    "sent": "OK, concavity plays an important role.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm all over our.",
                    "label": 0
                },
                {
                    "sent": "We can show that if you consider penalty functions like this, for example, if it's E to the power of Q with Q greater than one, then the derivative at origin equals to zero.",
                    "label": 1
                },
                {
                    "sent": "In this case, sparsity is hard to obtain.",
                    "label": 1
                },
                {
                    "sent": "The reason is if you want the ice coefficients to be 0, then you have to require either the eigenvalue to be 0 or the constant we defined before it's 0.",
                    "label": 0
                },
                {
                    "sent": "So which is hard to obtain, so that's why.",
                    "label": 0
                },
                {
                    "sent": "We say that if you take the penalty to be the square, then it's difficult to get sparsity.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "We can, for example, for the Q normal penalty.",
                    "label": 1
                },
                {
                    "sent": "We're curious between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Then we know that such an upper bound property holds for C between zero and one, and like the scared penalty, it has also this property with Q equals to one.",
                    "label": 0
                },
                {
                    "sent": "So that's why we introduce such a concave exponent 4.",
                    "label": 0
                },
                {
                    "sent": "A penalty function Omega which killed between zero and one with such a property then.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our analysis shows that if the kernel has certain smoothness, for example, it has some software smoothness and we know that in this case the eigenvalues, usually the eigenvalues of the integral operator, OK usually decays polynomially fast, so we assume this, and we assume the approximation error condition saying that the function want to learn actually lies in the Earth power of of the integral operator LK.",
                    "label": 0
                },
                {
                    "sent": "With some, of course.",
                    "label": 0
                },
                {
                    "sent": "Import function zero also in HK because this we define this integral operator on HK.",
                    "label": 0
                },
                {
                    "sent": "Now under this two conditions.",
                    "label": 0
                },
                {
                    "sent": "Then if we choose regularization parameter in this way.",
                    "label": 0
                },
                {
                    "sent": "Of course this is other visual choice with some priority condition, but then we get sparsity and approximation error.",
                    "label": 0
                },
                {
                    "sent": "So meaning that remember that CIZ might be 0 for I from one to N. Here we say that if I is greater than or equal to M to this power plus one, this power is less than one, then the confession must be 0.",
                    "label": 0
                },
                {
                    "sent": "So that means you have at most this amount of non zero coefficients which compared with the sample size M is much smaller because the power is less than one and the convergence rate is also M to some positive power which is complicated depending on this concave.",
                    "label": 0
                },
                {
                    "sent": "Exponent so if we have the kill penalty then we have this Q and from this expression actually we see that if Q becomes smaller and smaller, the learning rate becomes worse and worse.",
                    "label": 0
                },
                {
                    "sent": "OK, so if Q is 1, the learning rate in this form is the best.",
                    "label": 0
                },
                {
                    "sent": "OK, on the other hand this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this sparsity results.",
                    "label": 0
                },
                {
                    "sent": "It seems that it does not depend on the concave parameter Q, but in our simulation.",
                    "label": 0
                },
                {
                    "sent": "Actually it depends, as in a slightly.",
                    "label": 0
                },
                {
                    "sent": "Away but.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me explain to you about some simulation so that you have some ideas about the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we did some artificial data experiment and the result is very very good.",
                    "label": 0
                },
                {
                    "sent": "Like one example we use is we use the Gaussian kernel with the regression function, also in Gaussian way then.",
                    "label": 1
                },
                {
                    "sent": "Take the uniform noise, then turns out that the sparsity is like 2 two 6%, which is very good, but I want to talk about this.",
                    "label": 0
                },
                {
                    "sent": "Real data is from human knowledge, so here are the data consists of 14 groups.",
                    "label": 1
                },
                {
                    "sent": "Each group corresponds to one HALAA, and for each array we have a group of peptides PA.",
                    "label": 0
                },
                {
                    "sent": "The number of our peptides.",
                    "label": 0
                },
                {
                    "sent": "Is at least 420.",
                    "label": 0
                },
                {
                    "sent": "Could be a few 1000 so it's hundreds or thousands for each LA.",
                    "label": 0
                },
                {
                    "sent": "So you can consider this to be XI I from one to M. So the sample size here M is a few 100 or thousands.",
                    "label": 0
                },
                {
                    "sent": "Then for each layer we have this so called affinity so that you can consider this to be the regression problem.",
                    "label": 0
                },
                {
                    "sent": "That is, for each XI we have Yi so.",
                    "label": 0
                },
                {
                    "sent": "This YP for each peptide P continues as the regression value Yi.",
                    "label": 0
                },
                {
                    "sent": "It varies between zero and one called affinity.",
                    "label": 1
                },
                {
                    "sent": "Now this is.",
                    "label": 0
                },
                {
                    "sent": "A benchmark problem in my knowledge.",
                    "label": 0
                },
                {
                    "sent": "Until 2009, Nielsen and alone used some artificial neural network based algorithm called NN alignment to give the best result until then.",
                    "label": 0
                },
                {
                    "sent": "And then since Steve Smell Joint City in 2009, he established a seminar on if knowledge and a lot of people are involved and they use a string kernel, they denoted as K had three and apply the least square regular delegation, the one everyone uses, and they get slightly better result than the.",
                    "label": 1
                },
                {
                    "sent": "State of art result given by Nielsen alone at 2000.",
                    "label": 0
                },
                {
                    "sent": "That's the intent 812.",
                    "label": 0
                },
                {
                    "sent": "And even a group of people from our seminar attended a machine learning competition in immunology, and they they won the competition in one of the two categories, OK?",
                    "label": 0
                },
                {
                    "sent": "So we want to use this benchmark data for our algorithm based on kernel PCA an all result is even slightly better than other results.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me explain to you what do we mean by slightly better.",
                    "label": 0
                },
                {
                    "sent": "We use two measurements to measure the error.",
                    "label": 0
                },
                {
                    "sent": "One measurement is root mean square error.",
                    "label": 0
                },
                {
                    "sent": "So our output our algorithm provides for each peptide.",
                    "label": 0
                },
                {
                    "sent": "An affinity YP theater and from previous branch benchmark data we have YP the real affinity number, and then we take the difference.",
                    "label": 0
                },
                {
                    "sent": "And that's we called root mean square error of the affinity, and we also use another.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One called an area under the curve.",
                    "label": 1
                },
                {
                    "sent": "Number which say that in this data.",
                    "label": 0
                },
                {
                    "sent": "In this surreal data.",
                    "label": 0
                },
                {
                    "sent": "Peptide binding affinity problem.",
                    "label": 0
                },
                {
                    "sent": "We use this threshold of 0.426.",
                    "label": 0
                },
                {
                    "sent": "We say that if the affinity is bigger than this then that means this peptide is binding to the earlier.",
                    "label": 0
                },
                {
                    "sent": "So remember that a is LA and this yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, P is the peptide.",
                    "label": 0
                },
                {
                    "sent": "OK so all these peptides are considered to be binding and then all the others are unbonding.",
                    "label": 0
                },
                {
                    "sent": "OK, so if the algorithm is good when then we expect that if P is binding but P prime is not, then the predicted binding affinity should be bigger than the bonding one.",
                    "label": 0
                },
                {
                    "sent": "So so we we count how many type pairs satisfy this condition and.",
                    "label": 1
                },
                {
                    "sent": "Compared with the total number, this number is considered to be is defined as AUC.",
                    "label": 0
                },
                {
                    "sent": "It must be between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And of course, if the number is higher than the algorithm is better.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is our.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Result.",
                    "label": 0
                },
                {
                    "sent": "It's rather complicated, so like for each area we have, this is the number of peptides in corresponding to this area.",
                    "label": 0
                },
                {
                    "sent": "It's 5000 something and then for the second one 1000 and then wait a few 102 hundred 420.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, altogether there are 14 groups.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the classical result are the out of state of the art result in 2009.",
                    "label": 0
                },
                {
                    "sent": "They have this use neural network alignment.",
                    "label": 0
                },
                {
                    "sent": "They have these results and their result is is better in five out of 14 groups and the results of Stevens Mill and his collaborators.",
                    "label": 0
                },
                {
                    "sent": "They are better in four.",
                    "label": 0
                },
                {
                    "sent": "9 out of 14 groups an hour result when we use the empirical feature based regularization.",
                    "label": 0
                },
                {
                    "sent": "Is better than even Stevens smells without 45 out of the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Party groups, so this is the average result, so we know that for least square regular celebration, the classical one, there's no sparsity, and this percentage is the percentage of non zero coefficients.",
                    "label": 0
                },
                {
                    "sent": "You can see that as far as it is not so strong.",
                    "label": 0
                },
                {
                    "sent": "OK, and the AUC error.",
                    "label": 0
                },
                {
                    "sent": "You can see that the classical one is 0.79 and the group of Stevens Mail there improved by.",
                    "label": 0
                },
                {
                    "sent": "Like 0.55% OK, we improved further a little bit by 0.11%, which seems.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be very little, let me make some observation about this so.",
                    "label": 0
                },
                {
                    "sent": "The regularly square improved by 0.55% and we improved by 0.11% only.",
                    "label": 0
                },
                {
                    "sent": "And yeah, it seems that the improvement is is very mild.",
                    "label": 1
                },
                {
                    "sent": "However, we found that.",
                    "label": 1
                },
                {
                    "sent": "In the classical medical literature, this dissimilarly metric, called Blossom 62, is well established.",
                    "label": 1
                },
                {
                    "sent": "This is the dissimilarity metric for the set of 20ML assets.",
                    "label": 0
                },
                {
                    "sent": "And there is a large medical literature.",
                    "label": 0
                },
                {
                    "sent": "You can see 62 meaning that there are hundreds of such dissimilarity matrices and each is based on large statistical analysis.",
                    "label": 0
                },
                {
                    "sent": "So the kernel is very good.",
                    "label": 0
                },
                {
                    "sent": "So no matter which method you use, actually the result is quite similar and in fact the regularization parameter we obtained by cross validation is like 10 to the minus 4.",
                    "label": 0
                },
                {
                    "sent": "Which is a very small so meaning that even if you do not use regularization, result could be a pretty good.",
                    "label": 0
                },
                {
                    "sent": "So that's why this real data problem.",
                    "label": 1
                },
                {
                    "sent": "It seems that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can only improve a little bit and then recently we are doing some other problem.",
                    "label": 0
                },
                {
                    "sent": "One is to study the so-called path again.",
                    "label": 0
                },
                {
                    "sent": "You know pathogen genus election problem for influenza virus and a friend of mine has already made some medical experiments pretty expensive and then former state of mind has done some.",
                    "label": 0
                },
                {
                    "sent": "Learning theory analysis using mutual information analyst.",
                    "label": 0
                },
                {
                    "sent": "Will regular celebration anelastic Nate.",
                    "label": 0
                },
                {
                    "sent": "But the result is not that great and I cannot report here.",
                    "label": 0
                },
                {
                    "sent": "The correlation is only like 0.6, which is not.",
                    "label": 0
                },
                {
                    "sent": "Not so sad.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Factory and so our idea is probably in these data involving peptides or protein structures may be the least square error is not a good idea, so using the difference of numbers is probably not.",
                    "label": 0
                },
                {
                    "sent": "A good measurements because over there the patterns are more important than like here we use.",
                    "label": 0
                },
                {
                    "sent": "Affinity, it's a real number which may not be a good idea, so the our idea is maybe we can use some methods from theoretical learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Information, theoretical learning literature and to introduce some measurements for problems involving purpis or approaching structures.",
                    "label": 0
                },
                {
                    "sent": "OK, so I will choose one which is called a minimum error entropy principle and to explain to you some ideas why these might be used for learning OK.",
                    "label": 0
                },
                {
                    "sent": "There's a large literature already developed for more than 10 years, essentially by the Group of Prince side and many other people.",
                    "label": 0
                },
                {
                    "sent": "But sometimes people from different fields encountered entropies like your hand has interesting paper.",
                    "label": 0
                },
                {
                    "sent": "To say that entropy is minimized in some cases, so we want to study this minimal error entropy principle in a systematic way to give a general framework.",
                    "label": 0
                },
                {
                    "sent": "About this mathematical analysis, because so far there are minimally applied at work in this direction, they apply to this principle to different kind of applications like adaptive system training, a blind source separation, clustering and classification, flying deconvolution and so on.",
                    "label": 0
                },
                {
                    "sent": "The the original idea is trying to extract from data as much information as possible about the data generating systems by minimizing error entropies instead of minimizing the least square errors.",
                    "label": 1
                },
                {
                    "sent": "OK, so we replace the least square errors by entropy error entropies.",
                    "label": 1
                },
                {
                    "sent": "So to remind you about the classical entropy, Shannon's entropy is defined if you have a random variable E with PDF then.",
                    "label": 0
                },
                {
                    "sent": "Is defining this way essentially is the expected minus expectation of lager Father PDF and Arraigns entropy is simply replace this one by PE to the power of Alpha minus one actually?",
                    "label": 0
                },
                {
                    "sent": "Yeah, and normalized.",
                    "label": 0
                },
                {
                    "sent": "This depends on the parameter Alpha an for Alpha not equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Now if Alpha goes to one, then rains entropy will approach the channels entropy.",
                    "label": 0
                },
                {
                    "sent": "So we will consider a very special case of rains.",
                    "label": 0
                },
                {
                    "sent": "Entropy of order two, where R equals to two, then the entropy is defined by law Gov.",
                    "label": 0
                },
                {
                    "sent": "Expectation of the PDF PE, which is a very simple.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now remember that this entropy is defined.",
                    "label": 0
                },
                {
                    "sent": "By means of the PDF of a random variable, he hear the random variable E is Y minus FX4 hour regression problem?",
                    "label": 1
                },
                {
                    "sent": "OK, so we consider the difference between predicted value and output Y.",
                    "label": 0
                },
                {
                    "sent": "And so we need to have the PDF of this random variable.",
                    "label": 0
                },
                {
                    "sent": "Once you have the PDF, then we know that the least square error is defined actually as the square expected value of the square of this random variable.",
                    "label": 0
                },
                {
                    "sent": "If you're writing this in terms of the PDF, then it's E square PE.",
                    "label": 0
                },
                {
                    "sent": "Take the integration.",
                    "label": 0
                },
                {
                    "sent": "So here you can see that only the second moment of the PDF is involved when you look at the least squares error.",
                    "label": 0
                },
                {
                    "sent": "Now the idea of the.",
                    "label": 0
                },
                {
                    "sent": "Mde minimal error entropy is to consider not the error of not this early square form, but in a different form.",
                    "label": 0
                },
                {
                    "sent": "For example, if we use the rains entropy of order two, which we will study here, then the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that that's that's full.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's minus log integration of PEP, so if you consider this to be the expectation, then we replace this E square by the PDF.",
                    "label": 1
                },
                {
                    "sent": "OK, so here we only consider E square.",
                    "label": 0
                },
                {
                    "sent": "Here we consider a function P which involves all kinds of moments.",
                    "label": 1
                },
                {
                    "sent": "So that's why if we use the entropy concepts in measuring the error.",
                    "label": 1
                },
                {
                    "sent": "The moments of all orders are might be involved and which may handle on more complicated noise.",
                    "label": 0
                },
                {
                    "sent": "We all know that if the noise is Gaussian, then Lee Square is is very nice because it involves the second moment.",
                    "label": 0
                },
                {
                    "sent": "But if you have a non Gaussian noise then probably minimal error entropy may give good result.",
                    "label": 0
                },
                {
                    "sent": "That's the mayor.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now we use kernel approximation in the process because we know.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that this one, actually you cannot compute because we don't know this PE.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we approximate this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we approximate this one by taking its discretized form, so that means the integration of PE with respect to this probability measure we replace it.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By this one, so the integration is replaced by the empirical sun.",
                    "label": 0
                },
                {
                    "sent": "OK empirical mean.",
                    "label": 0
                },
                {
                    "sent": "Now what is this P = P?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He is unknown.",
                    "label": 0
                },
                {
                    "sent": "We need to approximate this probability density function of this error right?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Available so therefore we here we use kernel approximation that is, in order to approximate PE we use a PE head.",
                    "label": 0
                },
                {
                    "sent": "To use a set of samples EI, remember that each E is y -- F XOESYI minus FX I, so we use this set of samples for I from one to N and then we approximate the PDF.",
                    "label": 0
                },
                {
                    "sent": "The probability density function by this.",
                    "label": 0
                },
                {
                    "sent": "Passing windows for I from one to N. Here is GE minus EI Square over 2 square and take the average.",
                    "label": 0
                },
                {
                    "sent": "OK so he rages console called the scaling parameter in opposing Windows process.",
                    "label": 0
                },
                {
                    "sent": "Typical example is if you take G to BE to the minus T then that's the classical Gaussian windowing and once we have these two process, why is discredited discretization?",
                    "label": 0
                },
                {
                    "sent": "The other one is approximated the PDF by this empirical one.",
                    "label": 0
                },
                {
                    "sent": "Then we have this complicated form involving two layers of summation.",
                    "label": 0
                },
                {
                    "sent": "One is I from one to N, the other one is inside PE head which also for J2 from J from one to M. So you can see that we have two layers summation I from one to NJ from one to N and then replace EI by why I minus FX I and then EJ by this one.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm, actually former student of mine who.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a coauthor.",
                    "label": 0
                },
                {
                    "sent": "Mine in this work who are refereed a paper, I think for neural computation or some other journals, he notice that the paper gives very interesting observations, but no mathematical analysis, and in particular the application there says that.",
                    "label": 0
                },
                {
                    "sent": "In order to apply this algorithm, you have to take first age to be large, and then you're left edge to be smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "Moreover, they observed that if H is large then the result is pretty good.",
                    "label": 0
                },
                {
                    "sent": "If Edge is more than the result may be strange.",
                    "label": 0
                },
                {
                    "sent": "This is very strange because important windows we expect that age should go to zero in order for this function to approximate the PDF very well.",
                    "label": 0
                },
                {
                    "sent": "OK, so why this happens?",
                    "label": 0
                },
                {
                    "sent": "He discussed it with me and then.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We thought of this for awhile and then we made some analysis for this Amy algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so the analysis consists of two parts.",
                    "label": 0
                },
                {
                    "sent": "The first part has just been published by Journal Machine Learning Research and it says that if H is large then the algorithm works well for the regression problem.",
                    "label": 0
                },
                {
                    "sent": "If Edge is more than something strange may happen.",
                    "label": 0
                },
                {
                    "sent": "So this is consistent to what people have observed from practical applications.",
                    "label": 0
                },
                {
                    "sent": "In system identification.",
                    "label": 0
                },
                {
                    "sent": "So here we assume that as I said before, we assume the output random valuable only requires such a weak condition, not uniformly bounded for sure.",
                    "label": 0
                },
                {
                    "sent": "And you can consider this window function to be just E to the minus, T Gaussian windowing, and we assume for this.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I positive space in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is EM algorithm but containing a pair of samples here, not like the usual only one layer submission.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We assume that the covering number of the high positive space increase only epsilon to the minus P for some positive P. After you take log.",
                    "label": 1
                },
                {
                    "sent": "This is always true.",
                    "label": 0
                },
                {
                    "sent": "If H is a bowl of the server space with index greater than half of the input space dimension, and if it's a, it's above the reproducing kernel Hilbert space generated by Gaussian kernel, then this for sure is satisfy even for an arbitrarily small P.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, once we have this assumption, then our result says that the variance between the output function and the regression function decays like this power plus some approximation error term.",
                    "label": 0
                },
                {
                    "sent": "So if the regression function lies in the hypothesis space then disappear, this term disappears and then we only have this sample error term.",
                    "label": 0
                },
                {
                    "sent": "Now, if you assume a stronger condition about uniform boundedness for the output random variable, then our result is even better is empty.",
                    "label": 0
                },
                {
                    "sent": "The minus 1 / 1 + P, which this index can be arbitrarily close to one.",
                    "label": 0
                },
                {
                    "sent": "If P is a small enough like the reproducing kernel Hilbert space is generated by Gaussian kernels.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now the method.",
                    "label": 0
                },
                {
                    "sent": "The main idea when I thought of this problem is I thought if H is large then we can just take the Taylor series.",
                    "label": 0
                },
                {
                    "sent": "And this is actually is really the case.",
                    "label": 0
                },
                {
                    "sent": "We can approximate this generalization error by the least squares error and then all the analysis follows.",
                    "label": 0
                },
                {
                    "sent": "Of course the.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The analysis is not that easy, but that's the main idea.",
                    "label": 0
                },
                {
                    "sent": "Now the second part of our analysis is when H is more.",
                    "label": 0
                },
                {
                    "sent": "This is for small parameter.",
                    "label": 1
                },
                {
                    "sent": "Here we only consider this Gaussian window in case now we consider two kinds of consistency.",
                    "label": 1
                },
                {
                    "sent": "Why is the regression consistency?",
                    "label": 0
                },
                {
                    "sent": "If you consider only the regression problem, we assume that our output function should approximate the regulation function well, so that's why we consider the error between the.",
                    "label": 0
                },
                {
                    "sent": "Output function and the regression function up to a constant and we have very easy estimates for this.",
                    "label": 0
                },
                {
                    "sent": "This converges to 0.",
                    "label": 0
                },
                {
                    "sent": "Then we say regression consistency.",
                    "label": 1
                },
                {
                    "sent": "The other way is called Angel be consistency saying that the Alpha function may not go to the regression function here too, but it approximates the minimizer of the entropy among this among the set of functions OK.",
                    "label": 0
                },
                {
                    "sent": "This we call entropy consistency.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our first result says that the entropy consistency always holds.",
                    "label": 0
                },
                {
                    "sent": "That is, if you measure the error by entropy instead of the least square error, then the minimal error entropy algorithm always works very well because the entropy consistency always holds under the assumption that H go to Infinity or no edge Godzilla.",
                    "label": 1
                },
                {
                    "sent": "As I said an HPS Times Square root of N go to Infinity, so it should go to.",
                    "label": 0
                },
                {
                    "sent": "Zero, but in us in a slow way, then the consistency holds.",
                    "label": 0
                },
                {
                    "sent": "Entropy.",
                    "label": 1
                },
                {
                    "sent": "Consistency holds an we can have error learning rate.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the.",
                    "label": 0
                },
                {
                    "sent": "The other part is about regression consistency.",
                    "label": 0
                },
                {
                    "sent": "So for regression consistency, we divide the discussion into two parts.",
                    "label": 0
                },
                {
                    "sent": "1st part is for homogeneous noise called Homeless Skeptic model saying that this noise is independent of X, which is often used in statistics.",
                    "label": 1
                },
                {
                    "sent": "Then the regression consistency holds.",
                    "label": 1
                },
                {
                    "sent": "However, if.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You consider the non homogeneous noise.",
                    "label": 0
                },
                {
                    "sent": "That's the analogy.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, I want to skip.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if it's a for the Heathrow, scared that stick models that it means the noise is not uniform.",
                    "label": 0
                },
                {
                    "sent": "Here is 1 example only interval zero to half the noise is is uniform from minus half to half an on these other part then the noise is uniform on two separated intervals.",
                    "label": 1
                },
                {
                    "sent": "The total length is also one.",
                    "label": 0
                },
                {
                    "sent": "So it seems that the meeting should be 0.",
                    "label": 0
                },
                {
                    "sent": "However, it turns out that the minimizer of the entropy Reigns entropy is not realized by zero, is realized by something else.",
                    "label": 1
                },
                {
                    "sent": "Such a function, which has since one among these two intervals OK, and the regression function, which is 0, the anchovy, actually is not.",
                    "label": 0
                },
                {
                    "sent": "It's not at the minimum.",
                    "label": 0
                },
                {
                    "sent": "OK, it's pretty far away from the minimum, so that explains why people have observed in practical application that if you consider this any algorithm for the purpose of regression.",
                    "label": 0
                },
                {
                    "sent": "It might happen that some strange phenomenon will be found, like here.",
                    "label": 0
                },
                {
                    "sent": "Actually, the output function does not converge to the regression function at all.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the error by means of entropy, then the result is still very good.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only theoretical study and we're trying to use the algorithm for so for the real data problems.",
                    "label": 0
                },
                {
                    "sent": "As I said before, especially for influenza virus problems, but.",
                    "label": 0
                },
                {
                    "sent": "At this moment we haven't started that optimization problem yet because any algorithm is the optimization problem is rather complicated is not as easy as what we have in kernel method.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much for your attention.",
                    "label": 0
                }
            ]
        }
    }
}