{
    "id": "46z5ixfrjnvpf3zh4lexpixnqjoftkez",
    "title": "An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning",
    "info": {
        "author": [
            "Andreas Maurer, Stemmer Imaging"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_maurer_sparsity/",
    "segmentation": [
        [
            "So according to the can you hear me, yeah, according to the previous speaker, I will be doing many things wrong because the talk will be about concentration.",
            "I will make many assumptions and I will also consider a very simple situation."
        ],
        [
            "A Villa.",
            "Unfortunately, this is difficult to see.",
            "The main subject will be rather minor complexities and how to bound them, and so the typical situation we consider in machine learning is said we or supervised learning is that we have a sample of observations, independent sample and then we have a class of loss functions and we want to pick loss function which has a small or even minimal.",
            "If we can empirical error.",
            "On the observations and then we hope that the expected error will also be smaller.",
            "And a nice way to guarantee this.",
            "Our priority before we see the the data is that if we have some kind of theorem that guarantees for our class of loss functions so that we get a uniform bound.",
            "Unfortunately, the contrast is poor.",
            "It was better on my screen.",
            "And this theorem where also the previous speaker is.",
            "One author hearing of this paper is a guarantee is a very nice result is that you can guarantee with high probability you have a uniform bound for all functions in the last class and given in terms of something which I will ignore in the rest of this talk and the Rademacher complexity or adima average, which measures how the function class can be, say, worst case aligned to a random sequence of uniformly minus one one.",
            "Distributed variables and this adima averages will be the object of this this talk, and there's a two important properties.",
            "One is that.",
            "I feed the function of the loss function so that my function class through a Lipschitz function.",
            "Then I can bound the rather my average in terms of the Lipschitz constant and the the originel there adima average of the original function class in another one.",
            "This is more difficult to establish.",
            "And this very obvious ones.",
            "They said the average of a class is the same as the average of its convex Hull, which you can see, because this expression here.",
            "The Rademacher process is linear in the functions, so the supremum will always be for any value of the epsilons will be attained at the at the extreme points."
        ],
        [
            "And one another assumption that I'm going to make.",
            "You said I can bound my the average of my class by bounding the average for a union of other classes, which hopefully are easier to bound.",
            "And this happens very often is what I argue is is in.",
            "Situation into which we run very often, so we have a big class.",
            "And now because the the average of the big class or the complexity of the big class can be bounded in terms of the complexity of the extreme points.",
            "So if in this case I could just consider the union of these four classes here, and if I'm able to bound that and I also get a bound on the big class, this happens very often when the set of extreme points is contained in a union of other classes.",
            "And in practice, there are many algorithms that used it like multiple kernel learning.",
            "There's the group lasso, which I can also be looked at it in this way.",
            "And then there's some more recent technique, and it can also be applied to multitask diction value learning.",
            "Aura.",
            "Actually, in in some more cases that I probably won't be able to go into because of the time constraints that I have.",
            "These unions of classes are not difficult, so this will be very."
        ],
        [
            "Simple mathematically, not very simple and very difficult to bound the effect.",
            "It's so simple that maybe I can.",
            "I can just derive the our main result rather than stating it.",
            "So as the average of the Union is just the expected supremum of all this, which I break apart into a maximum.",
            "Of the expected maximum of the components of PRIMA.",
            "If we call this the components of Prima here after process which I let me call these functions FM.",
            "Which is, you know, it can be extended to a function on our end, and it's on the argument set here so.",
            "One thing that you see right away said inset if M is a convex function.",
            "And also if you are, it's also Lipschitz because right subtract 2 of these and use the fact that if the difference of Suprema is always bounded by the supremum of the differences and then the coaching Swartz inequality, then I can see that it is a Lipschitz function.",
            "And each one of the FM says this Lipschitz constant, and which I can again bound by the global maximum.",
            "Over the entire class.",
            "So there is.",
            "So these are what we have is the expectation of a map of the maximum of a collection of finite collection of convex Lipschitz functions."
        ],
        [
            "And there is a beautiful theorem.",
            "Which is.",
            "The part where concentration comes in goes back to Talagrand.",
            "I think that if I have a function on the cube.",
            "Which is a convex Lipschitz function and I apply to independent random variables.",
            "Then it is concentrated in.",
            "I express the concentration here, not in terms of a tail bound, but in terms of a bound on the moment generating functions for the estimation difference.",
            "And did.",
            "This is the sound very simple bound on the moment generating function in terms of the Lipschitz.",
            "Constant."
        ],
        [
            "For.",
            "This is say in this book.",
            "The book is very nice because it also contains the simple trick how to proceed.",
            "No, no, no.",
            "There's no minus.",
            "Better take.",
            "The tail bound would have a minus."
        ],
        [
            "Home So what?",
            "I'm looking at the maximum overall components of the estimation difference for the component component Maxima or suprima.",
            "And.",
            "That the expectation is something I want to bound multiplied with the parameter, take the exponentially now to pound this, I can use Jensen's inequality to bring the explanation at the expectation outside of the exponential, and also because it is there all positive members left because beta is positive.",
            "I can use bring the maximum outside.",
            "Then I bound the maximum of bring the maximum outside because it's increasing.",
            "OK, and then I bound the maximum.",
            "There are some.",
            "Bring some outside the expectation and then I used the previous inequality on the moment generating funk."
        ],
        [
            "Then I can take the logarithm of all this it divide by beta.",
            "That's going to give me on the right hand side this.",
            "And on the left hand side it will be this and then I just use the fact that the that the difference of Maxima is bounded by the maximum after differences.",
            "So I get this inequality.",
            "Optimize in beta and then I obtain this.",
            "Finally."
        ],
        [
            "The site which I translate back to it adima averages.",
            "So this is also the main result.",
            "Affair, it's really very simple here.",
            "It says that my average of the Union of classes is bounded by the the average of the worst class, or for which we can, and this remaining term extends depends on the logarithm of the number of classes and this which kind of looks like an Euclidean norm.",
            "And.",
            "This is.",
            "Argument like polygons, chaining arguments.",
            "Yes, you could actually this.",
            "It is he uses it in the derivation of the the.",
            "Yeah, that's right, there is a connection to it.",
            "Lou"
        ],
        [
            "This is our main result.",
            "I will not go in.",
            "It will not become more complicated than it already is, so this is our bound.",
            "Yeah, I'm.",
            "Makes it make up symbols and words names for these two quantities.",
            "Here in the bound.",
            "Because I'm going to use them over and over, use of over and over in applications.",
            "So S is the strong parameter, which is just the complexity of the worst class.",
            "And W is the weak parameter, which is this quantity.",
            "Here it has a very nice property because that's the only one that's coupled to the number of classes.",
            "So in competing results in the application, so it happens very often is you see the complexity of the worst class times the logarithm of the number of classes, and it's the advantage of this method is that it decouples it, and this is typically much much smaller than the weak parameter and for."
        ],
        [
            "Each feature map I look at the function class, which is just a linear function class defined by functionals of norm listen equal to 1.",
            "And.",
            "Here one could consider different more like this could be some.",
            "Some some LP little LP norm that I use here, but this is.",
            "For our applications we only use the simplest case.",
            "And now for each component here for each feature map I have a.",
            "If I have a sample, then I get correspondingly represented sample that defines a covariance operator.",
            "There.",
            "And so it turns out that the strong parameter.",
            "Now what I want to bound is this, and I'm going to use the recite before that.",
            "The strong parameter, which is the one that doesn't couple to the number of to the number M capital M, is.",
            "This is a standard kind of method to show this is bounded by two the bounded by the largest trace of all the covariances.",
            "In involved.",
            "Why is the weak parameter is actually identical in this case?",
            "Just by writing out its definition to the largest, largest largest eigenvalue.",
            "Of the covariance variance covariance operators, so this is can be if in a high dimensional setting, if the data is high dimensional, this will be much smaller than this.",
            "Evidently so.",
            "If I plug these into our.",
            "Put into our main inequality, then I get this result and you can see that the number of classes is only covered to the largest eigenvalue.",
            "So the."
        ],
        [
            "This is an exotic function and exotic loss class here, but if I take the convex Hull it is not exotic anymore because it becomes, you know, depending on what I do it it becomes multiple kernel learning.",
            "If you know I have feature Maps which are are induced by positive definite kernels or it.",
            "Or the group lasso?",
            "Or there are many many other cases where it because in fact multiple kernel learning in a way that continues to the group lasso.",
            "In all these cases I have.",
            "This study inequality."
        ],
        [
            "In A to abbreviate a little bit to multitask dictionary.",
            "Learning where the method can also be applied, I have."
        ],
        [
            "Kinda Hilbert space.",
            "A selection of dictionaries from which a collection from which I am going to select one.",
            "Then I get a representation in our children K and then I have a collection of matrices, one with one linear classifier for each task.",
            "And I'm looking at the composition of the two, so I want to choose arbitrarily from from D and arbitrary from W. Then I get the same.",
            "If the number of extreme points of the set of matrices is.",
            "It is finite.",
            "Then I can use the same trick."
        ],
        [
            "And.",
            "In this case, in the working class case, very often what happens is that the strong parameter goes to 0.",
            "As a as the number of posts goes to Infinity.",
            "And and the and the weak parameter is related to some kind of.",
            "Yeah, some some largest eigenvalue or fake oberia."
        ],
        [
            "So there is an example here, which is I'm going to skip."
        ],
        [
            "So this is, yeah, this is really really it.",
            "We also applied it to independent sparse wait where there is the last so class for each task.",
            "Or subspace learning in subspace learning, we discretize the the number of matrices.",
            "Multitask subspace, learning Richard here and there are several possibilities.",
            "You just use the simplest applications of what is really a very simple trick for maybe have to apologize for this implicitly.",
            "If it's thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So according to the can you hear me, yeah, according to the previous speaker, I will be doing many things wrong because the talk will be about concentration.",
                    "label": 0
                },
                {
                    "sent": "I will make many assumptions and I will also consider a very simple situation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A Villa.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, this is difficult to see.",
                    "label": 0
                },
                {
                    "sent": "The main subject will be rather minor complexities and how to bound them, and so the typical situation we consider in machine learning is said we or supervised learning is that we have a sample of observations, independent sample and then we have a class of loss functions and we want to pick loss function which has a small or even minimal.",
                    "label": 0
                },
                {
                    "sent": "If we can empirical error.",
                    "label": 0
                },
                {
                    "sent": "On the observations and then we hope that the expected error will also be smaller.",
                    "label": 0
                },
                {
                    "sent": "And a nice way to guarantee this.",
                    "label": 0
                },
                {
                    "sent": "Our priority before we see the the data is that if we have some kind of theorem that guarantees for our class of loss functions so that we get a uniform bound.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, the contrast is poor.",
                    "label": 0
                },
                {
                    "sent": "It was better on my screen.",
                    "label": 0
                },
                {
                    "sent": "And this theorem where also the previous speaker is.",
                    "label": 0
                },
                {
                    "sent": "One author hearing of this paper is a guarantee is a very nice result is that you can guarantee with high probability you have a uniform bound for all functions in the last class and given in terms of something which I will ignore in the rest of this talk and the Rademacher complexity or adima average, which measures how the function class can be, say, worst case aligned to a random sequence of uniformly minus one one.",
                    "label": 0
                },
                {
                    "sent": "Distributed variables and this adima averages will be the object of this this talk, and there's a two important properties.",
                    "label": 0
                },
                {
                    "sent": "One is that.",
                    "label": 0
                },
                {
                    "sent": "I feed the function of the loss function so that my function class through a Lipschitz function.",
                    "label": 0
                },
                {
                    "sent": "Then I can bound the rather my average in terms of the Lipschitz constant and the the originel there adima average of the original function class in another one.",
                    "label": 0
                },
                {
                    "sent": "This is more difficult to establish.",
                    "label": 0
                },
                {
                    "sent": "And this very obvious ones.",
                    "label": 0
                },
                {
                    "sent": "They said the average of a class is the same as the average of its convex Hull, which you can see, because this expression here.",
                    "label": 0
                },
                {
                    "sent": "The Rademacher process is linear in the functions, so the supremum will always be for any value of the epsilons will be attained at the at the extreme points.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one another assumption that I'm going to make.",
                    "label": 0
                },
                {
                    "sent": "You said I can bound my the average of my class by bounding the average for a union of other classes, which hopefully are easier to bound.",
                    "label": 0
                },
                {
                    "sent": "And this happens very often is what I argue is is in.",
                    "label": 0
                },
                {
                    "sent": "Situation into which we run very often, so we have a big class.",
                    "label": 0
                },
                {
                    "sent": "And now because the the average of the big class or the complexity of the big class can be bounded in terms of the complexity of the extreme points.",
                    "label": 0
                },
                {
                    "sent": "So if in this case I could just consider the union of these four classes here, and if I'm able to bound that and I also get a bound on the big class, this happens very often when the set of extreme points is contained in a union of other classes.",
                    "label": 0
                },
                {
                    "sent": "And in practice, there are many algorithms that used it like multiple kernel learning.",
                    "label": 1
                },
                {
                    "sent": "There's the group lasso, which I can also be looked at it in this way.",
                    "label": 0
                },
                {
                    "sent": "And then there's some more recent technique, and it can also be applied to multitask diction value learning.",
                    "label": 0
                },
                {
                    "sent": "Aura.",
                    "label": 0
                },
                {
                    "sent": "Actually, in in some more cases that I probably won't be able to go into because of the time constraints that I have.",
                    "label": 1
                },
                {
                    "sent": "These unions of classes are not difficult, so this will be very.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple mathematically, not very simple and very difficult to bound the effect.",
                    "label": 0
                },
                {
                    "sent": "It's so simple that maybe I can.",
                    "label": 0
                },
                {
                    "sent": "I can just derive the our main result rather than stating it.",
                    "label": 0
                },
                {
                    "sent": "So as the average of the Union is just the expected supremum of all this, which I break apart into a maximum.",
                    "label": 0
                },
                {
                    "sent": "Of the expected maximum of the components of PRIMA.",
                    "label": 0
                },
                {
                    "sent": "If we call this the components of Prima here after process which I let me call these functions FM.",
                    "label": 0
                },
                {
                    "sent": "Which is, you know, it can be extended to a function on our end, and it's on the argument set here so.",
                    "label": 0
                },
                {
                    "sent": "One thing that you see right away said inset if M is a convex function.",
                    "label": 0
                },
                {
                    "sent": "And also if you are, it's also Lipschitz because right subtract 2 of these and use the fact that if the difference of Suprema is always bounded by the supremum of the differences and then the coaching Swartz inequality, then I can see that it is a Lipschitz function.",
                    "label": 0
                },
                {
                    "sent": "And each one of the FM says this Lipschitz constant, and which I can again bound by the global maximum.",
                    "label": 0
                },
                {
                    "sent": "Over the entire class.",
                    "label": 0
                },
                {
                    "sent": "So there is.",
                    "label": 0
                },
                {
                    "sent": "So these are what we have is the expectation of a map of the maximum of a collection of finite collection of convex Lipschitz functions.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there is a beautiful theorem.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "The part where concentration comes in goes back to Talagrand.",
                    "label": 0
                },
                {
                    "sent": "I think that if I have a function on the cube.",
                    "label": 0
                },
                {
                    "sent": "Which is a convex Lipschitz function and I apply to independent random variables.",
                    "label": 1
                },
                {
                    "sent": "Then it is concentrated in.",
                    "label": 0
                },
                {
                    "sent": "I express the concentration here, not in terms of a tail bound, but in terms of a bound on the moment generating functions for the estimation difference.",
                    "label": 0
                },
                {
                    "sent": "And did.",
                    "label": 0
                },
                {
                    "sent": "This is the sound very simple bound on the moment generating function in terms of the Lipschitz.",
                    "label": 0
                },
                {
                    "sent": "Constant.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "This is say in this book.",
                    "label": 0
                },
                {
                    "sent": "The book is very nice because it also contains the simple trick how to proceed.",
                    "label": 0
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "There's no minus.",
                    "label": 0
                },
                {
                    "sent": "Better take.",
                    "label": 0
                },
                {
                    "sent": "The tail bound would have a minus.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Home So what?",
                    "label": 0
                },
                {
                    "sent": "I'm looking at the maximum overall components of the estimation difference for the component component Maxima or suprima.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That the expectation is something I want to bound multiplied with the parameter, take the exponentially now to pound this, I can use Jensen's inequality to bring the explanation at the expectation outside of the exponential, and also because it is there all positive members left because beta is positive.",
                    "label": 0
                },
                {
                    "sent": "I can use bring the maximum outside.",
                    "label": 0
                },
                {
                    "sent": "Then I bound the maximum of bring the maximum outside because it's increasing.",
                    "label": 0
                },
                {
                    "sent": "OK, and then I bound the maximum.",
                    "label": 0
                },
                {
                    "sent": "There are some.",
                    "label": 0
                },
                {
                    "sent": "Bring some outside the expectation and then I used the previous inequality on the moment generating funk.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I can take the logarithm of all this it divide by beta.",
                    "label": 0
                },
                {
                    "sent": "That's going to give me on the right hand side this.",
                    "label": 0
                },
                {
                    "sent": "And on the left hand side it will be this and then I just use the fact that the that the difference of Maxima is bounded by the maximum after differences.",
                    "label": 0
                },
                {
                    "sent": "So I get this inequality.",
                    "label": 0
                },
                {
                    "sent": "Optimize in beta and then I obtain this.",
                    "label": 0
                },
                {
                    "sent": "Finally.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The site which I translate back to it adima averages.",
                    "label": 0
                },
                {
                    "sent": "So this is also the main result.",
                    "label": 0
                },
                {
                    "sent": "Affair, it's really very simple here.",
                    "label": 0
                },
                {
                    "sent": "It says that my average of the Union of classes is bounded by the the average of the worst class, or for which we can, and this remaining term extends depends on the logarithm of the number of classes and this which kind of looks like an Euclidean norm.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Argument like polygons, chaining arguments.",
                    "label": 0
                },
                {
                    "sent": "Yes, you could actually this.",
                    "label": 0
                },
                {
                    "sent": "It is he uses it in the derivation of the the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right, there is a connection to it.",
                    "label": 0
                },
                {
                    "sent": "Lou",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is our main result.",
                    "label": 1
                },
                {
                    "sent": "I will not go in.",
                    "label": 0
                },
                {
                    "sent": "It will not become more complicated than it already is, so this is our bound.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm.",
                    "label": 0
                },
                {
                    "sent": "Makes it make up symbols and words names for these two quantities.",
                    "label": 0
                },
                {
                    "sent": "Here in the bound.",
                    "label": 0
                },
                {
                    "sent": "Because I'm going to use them over and over, use of over and over in applications.",
                    "label": 1
                },
                {
                    "sent": "So S is the strong parameter, which is just the complexity of the worst class.",
                    "label": 1
                },
                {
                    "sent": "And W is the weak parameter, which is this quantity.",
                    "label": 0
                },
                {
                    "sent": "Here it has a very nice property because that's the only one that's coupled to the number of classes.",
                    "label": 0
                },
                {
                    "sent": "So in competing results in the application, so it happens very often is you see the complexity of the worst class times the logarithm of the number of classes, and it's the advantage of this method is that it decouples it, and this is typically much much smaller than the weak parameter and for.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each feature map I look at the function class, which is just a linear function class defined by functionals of norm listen equal to 1.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Here one could consider different more like this could be some.",
                    "label": 0
                },
                {
                    "sent": "Some some LP little LP norm that I use here, but this is.",
                    "label": 0
                },
                {
                    "sent": "For our applications we only use the simplest case.",
                    "label": 0
                },
                {
                    "sent": "And now for each component here for each feature map I have a.",
                    "label": 0
                },
                {
                    "sent": "If I have a sample, then I get correspondingly represented sample that defines a covariance operator.",
                    "label": 0
                },
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "And so it turns out that the strong parameter.",
                    "label": 0
                },
                {
                    "sent": "Now what I want to bound is this, and I'm going to use the recite before that.",
                    "label": 0
                },
                {
                    "sent": "The strong parameter, which is the one that doesn't couple to the number of to the number M capital M, is.",
                    "label": 0
                },
                {
                    "sent": "This is a standard kind of method to show this is bounded by two the bounded by the largest trace of all the covariances.",
                    "label": 0
                },
                {
                    "sent": "In involved.",
                    "label": 0
                },
                {
                    "sent": "Why is the weak parameter is actually identical in this case?",
                    "label": 0
                },
                {
                    "sent": "Just by writing out its definition to the largest, largest largest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Of the covariance variance covariance operators, so this is can be if in a high dimensional setting, if the data is high dimensional, this will be much smaller than this.",
                    "label": 0
                },
                {
                    "sent": "Evidently so.",
                    "label": 0
                },
                {
                    "sent": "If I plug these into our.",
                    "label": 0
                },
                {
                    "sent": "Put into our main inequality, then I get this result and you can see that the number of classes is only covered to the largest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is an exotic function and exotic loss class here, but if I take the convex Hull it is not exotic anymore because it becomes, you know, depending on what I do it it becomes multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "If you know I have feature Maps which are are induced by positive definite kernels or it.",
                    "label": 1
                },
                {
                    "sent": "Or the group lasso?",
                    "label": 0
                },
                {
                    "sent": "Or there are many many other cases where it because in fact multiple kernel learning in a way that continues to the group lasso.",
                    "label": 1
                },
                {
                    "sent": "In all these cases I have.",
                    "label": 0
                },
                {
                    "sent": "This study inequality.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In A to abbreviate a little bit to multitask dictionary.",
                    "label": 0
                },
                {
                    "sent": "Learning where the method can also be applied, I have.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kinda Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "A selection of dictionaries from which a collection from which I am going to select one.",
                    "label": 0
                },
                {
                    "sent": "Then I get a representation in our children K and then I have a collection of matrices, one with one linear classifier for each task.",
                    "label": 0
                },
                {
                    "sent": "And I'm looking at the composition of the two, so I want to choose arbitrarily from from D and arbitrary from W. Then I get the same.",
                    "label": 0
                },
                {
                    "sent": "If the number of extreme points of the set of matrices is.",
                    "label": 0
                },
                {
                    "sent": "It is finite.",
                    "label": 0
                },
                {
                    "sent": "Then I can use the same trick.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In this case, in the working class case, very often what happens is that the strong parameter goes to 0.",
                    "label": 0
                },
                {
                    "sent": "As a as the number of posts goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "And and the and the weak parameter is related to some kind of.",
                    "label": 1
                },
                {
                    "sent": "Yeah, some some largest eigenvalue or fake oberia.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is an example here, which is I'm going to skip.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is, yeah, this is really really it.",
                    "label": 0
                },
                {
                    "sent": "We also applied it to independent sparse wait where there is the last so class for each task.",
                    "label": 1
                },
                {
                    "sent": "Or subspace learning in subspace learning, we discretize the the number of matrices.",
                    "label": 1
                },
                {
                    "sent": "Multitask subspace, learning Richard here and there are several possibilities.",
                    "label": 0
                },
                {
                    "sent": "You just use the simplest applications of what is really a very simple trick for maybe have to apologize for this implicitly.",
                    "label": 0
                },
                {
                    "sent": "If it's thank you.",
                    "label": 0
                }
            ]
        }
    }
}