{
    "id": "cffxuxakttx7ptfqo5h7zk63u57oimq5",
    "title": "Learning with similarity functions",
    "info": {
        "author": [
            "Maria-Florina Balcan, College of Computing, Georgia Institute of Technology"
        ],
        "published": "July 20, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/icml2010_balcan_lwsf/",
    "segmentation": [
        [
            "So let me start with."
        ],
        [
            "High level picture and brief overview of my talk, so let's consider a generic classification problem.",
            "So for instance, given set of images, let's say that we want to try to learn a rule to distinguish images of man from those of women and our natural approach would be to try to use a standard classification technique.",
            "So, for example, we might try to use.",
            "And our linear separator learning algorithm.",
            "However, the Pixel representation might not be so good in the sense that the data might not be linearly separable in this natural pixel representation.",
            "Now, instead of powerful technique which has been.",
            "Extremely popular in recent years in machine learning is to use a kernel and two kernel eyes are linear separate or learning algorithm.",
            "And I'll define in the middle.",
            "That means I'm sure that you already know, but for now, let me just mention that kernel functions have become extremely popular in recent years in machine learning, in part due to the fact that they do have a very nice and solid theoretical foundation.",
            "And the theory talks about the effectiveness of our given common functions in terms of margins and implicit high dimensional feature spaces.",
            "Now, however, while quite elegantly, theory does not necessarily correspond to sometimes a common intuition with a good current function is in fact one, but after the good measure of similarity, and Moreover and more importantly, the requirement that the given similarity function is a legal kernel function.",
            "My simply rule out some of the most natural similarity functions for our learning problem, and so we so motivated by this what we do in our work.",
            "We develop a theory of learning with general similarity functions.",
            "That matches, say the common intuition with a good kernel function is on, But Act is a good measure of similarity and also again more importantly, we show how these methods have such methods can be applied to even more general similarity functions that are not necessarily legal corner functions, and so this second point especially seems to fit the the main theme of this workshop.",
            "Alright, so this is a high level overview of my talk now, going to details."
        ],
        [
            "Just to quickly remind you a kernel K pairwise function K is a legal sanction case.",
            "Illegal kernel function, if it corresponds to a legal definition of that product.",
            "So in other words, a pairwise function K is legal kernel function if there exists an implicit mapping Phi into a possibly very high dimensional feature space with the property, but K of XY is the product of 5X.",
            "And five Y in that space.",
            "So for instance, this function right here.",
            "So we spend assumption.",
            "Here is a kernel.",
            "It's called the polynomial kernel and it corresponds to legal dot product into a higher dimensional feature space where the features are given by monomials of degree up to D on the base features.",
            "So this is not obvious, but one can verify.",
            "OK, now the point is that many.",
            "Learning algorithm many classic learning algorithms such as LCM or Perceptron can be written so that they interact with the data, but only by the product.",
            "And so that means that if we now replace dot product computation with the kernel computation.",
            "When our algorithm act implicitly as if the data was in the hard drive itself into space, and it does so without having to pay high price computationally, so without having to ever going to the higher dimensional feature space.",
            "And hopefully some nicer things are going to happen in high dimensional spaces.",
            "Penn.com back to this point."
        ],
        [
            "And here is the classic picture or example, probably taken from Alex is book Alex Miller book.",
            "So data right here on the left is not linearly separable in the original 2 dimensional representation.",
            "However, it is separable by an ellipse, and it is also separable in the file space induced by the polynomial kernel of degree, and so in higher dimensional feature space."
        ],
        [
            "This is much easier to make the data linearly separable and Moreover.",
            "If the data is separable, linearly separable by a large margin.",
            "Then we also get good generalization bounds.",
            "We also get good translation guarantees.",
            "So for instance standard bounded say.",
            "But if the data is separable by merging gamma in the file space, then we only need roughly one over gamma squared labeled examples to learn well.",
            "So we only need one over gamma squared labeled examples in order to come up with a rule.",
            "But as well on new examples coming from the same source.",
            "Alright so."
        ],
        [
            "Now, going back to a more generic view, as you probably all know, kernel functions have proven very useful in practice for dealing with all sorts of data.",
            "And as I mentioned, there is also this very nice and elegant super nice theory about it makes a given kernel function a good kernel function for a given learning problem, however."
        ],
        [
            "The existing theory talks in terms of margins in implicit high dimensional feature spaces, which sometimes are difficult to maybe characterize.",
            "It also does not correspond to, say, a common intuition with a good kernel function is in fact one that acts like a good measure of similarity, so it might not be necessarily for certain people the best for intuition, but more importantly, the requirement that the similarity function is in fact the legal kernel function may simply rule out some of the most natural similarity functions for certain domain.",
            "So for instance, in the context of protein sequence classification, some of the most well known, biologically motivated similarity functions are simply not legal with products.",
            "And so this brings up the question."
        ],
        [
            "Can one develop a alternativ, perhaps more intuitive, but more importantly broader theory of learning with similarity functions right?",
            "And the answer Luckily is yes, so in our work we provide theory of learning with more general similarity functions, but are not necessarily legal kernel function.",
            "Illegal kernel functions, and it turns out that this theory also formalizes a common intuition with a good kernel function, is in fact one that acts as a good measure of similarity and much more concretely what we do."
        ],
        [
            "So we're doing this work more concretely.",
            "We develop a notion of a good similarity function for a given learning problem, which satisfies the following desirable properties.",
            "First of all, it talks in terms of natural direct quantities of the data.",
            "It does not reference implicit high dimensional feature space is and it has no requirement that the given similarity function corresponds to illegal dot product, so it's a legal kernel function.",
            "Also, if we had the similarity function satisfying our notion, then we could use it to learn well, so such a similarity function has good implications for learning.",
            "Now second, we also show that our notion is broad in the sense that it probably includes original notion of a good kernel function, and here by a good kernel function I mean one with induces a large margin in the file space, so it's broad.",
            "And the interesting also we can show up at our notion is strictly more general in a formal sense, in the sense that we can exhibit classes of functions for which a single kernel function exists, but a single good similarity function exists.",
            "But there is no single good kind of function exists, so our theory is also probably strictly more general.",
            "To understand you correctly that you want to have sort of an axiomatization of what similarity is not, axiomatization is more like.",
            "No, so it's more like analyzing so natural conditions of a given similarity function that has good implications for learning, so it's more in the directions and wanted to measurement process.",
            "So in many applications it's much simpler and much simpler to test for identity or to judge the two.",
            "Things are very similar rather than establishing precise measurements of very dissimilar things.",
            "And I think this is this is a very strong argument for for judging similarity based functions, whether they're useful.",
            "Right, so our goal here.",
            "OK, that's why.",
            "So you could go in many directions, but our goal here is to generalize the usual notion of a good kernel function.",
            "While still getting good implications for learning, I'm not to suggest would be just an alternative.",
            "Might perhaps orthogonal way too.",
            "Try to formalize learning of similarity functions.",
            "You can do a better job with quite similar.",
            "I think it's also for your application.",
            "It's more important to be precise on very similar objects and distances between very distant knology doesn't matter so much for that.",
            "I will have to really think about it.",
            "OK, before saying yes or no, but definitely OK.",
            "So technically speaking, our goal is to generalize the usual notion of a good kernel function.",
            "OK, so this is our technical goal.",
            "While still allowing.",
            "The algorithm to use non legal corner functions.",
            "I mean obviously you can develop a theory of learning of dissimilarity functions and analyze conditions that would work under the situation that you have in mind.",
            "Because on one hand you want to define what similarities and on the other hand, no.",
            "No, I just don't you see.",
            "OK, so we'll go, we'll go OK, we'll go for details.",
            "So then you understand precisely.",
            "So this is just at the high level.",
            "We want to define proposal, notion of wood similarity function that can be used.",
            "To learn well and the sticky generalize visual notion of a good kernel function.",
            "OK and actually maybe my next slide will be extremely helpful for dictation.",
            "OK, so."
        ],
        [
            "Actually, before I even talk about our main ocean just to.",
            "Give an example of the type of thing I'm thinking about.",
            "I'm going to start with something simpler that does not satisfies all the three requirements.",
            "Not broad enough in particular, but is which is simple enough.",
            "And you don't even get efficient learning algorithm and then in a few slides I modified to make it broad enough so the simpler definition.",
            "But I'm going to start.",
            "It doesn't capture all the good kernel functions, but then in a few slides are modified to capture all good kernel functions.",
            "OK, so I'll assume throughout this talk what we're trying to do binary classification and without our examples come according to a distribution fixed unknown distribution over labeled examples.",
            "And our goal is to achieve low classification at all respect to the distribution.",
            "And I'll probably clarify your question little bit.",
            "OK, so our first notion again, it's not bad enough, but our foreign notion of a good similarity function is the following.",
            "We say that K is a good similarity function for our given any problem.",
            "If most of the points X on average more similar to points of round label when the point of the opposite library.",
            "So formally we save it K and epsilon gamma good similarity function for our learning problem.",
            "If at least one epsilon fraction of the examples X.",
            "Satisfy the requirement.",
            "But that obvious similarity to a point X is the same label.",
            "Is greater by at least.",
            "A certain gap, then the average, similar to the point of the opposite label.",
            "And this gap gamma right here defines the goodness of our given similarity function.",
            "This is an example of a notion of a good semantic function."
        ],
        [
            "So for instance, say that the similarity tend to positive examples that laser .2 the similarity between end to negative between negative examples that laser pointer and the similarity between a positive example and the negative example is a random number in minus one one, then this.",
            "Left hand side would be at least three point to the right hand side would be 0 and so we should satisfy our notion for epsilon zero and gamma 0.2.",
            "Just a simple example.",
            "And actually, if you think about it.",
            "The instant space is large enough there is a nontrivial chance that this similarity function will in fact not going to be a little corner function.",
            "I mean, so this looks like a very reasonable notion about.",
            "Dude asks for a good similarity function, and Moreover if you had a similarity function satisfying this very simple condition where."
        ],
        [
            "We also have a very trivial efficient learning algorithm, so all we have to do in this case is to just draw a bunch of positive examples.",
            "A bunch of negative examples, and then to simply output the classification rule, but classified as an example X as positive.",
            "It's an average more similar to examples in this plus and this negative otherwise, so this is not.",
            "This definition is not bad enough to keep the robot kernel functions for now, just as simple as start point.",
            "OK, so let me I'll come to the main ocean in a bit.",
            "I'm not claiming that we this is the notion that captures a good kind of functions.",
            "You see an example in a second."
        ],
        [
            "OK, so in any case, so if we had the similarity function satisfying this condition when we had, we have a, we can use this simple language that I described.",
            "We draw a bunch of positive examples, a bunch of negative examples, and then I'll put the classification rule that classifieds a new example X if it's on average, more similar to examples in this plus into examples in this month, and as a negative otherwise, and then the guarantee we get is that if sets S plus in this minus have size at least one over gamma squared.",
            "One of our Delta Times epsilon prime, then with high probability at least someone is Delta.",
            "The classification rule that well put has there are at most epsilon plus epsilon prime and so notice that we get error rate which is arbitrarily close.",
            "To the fraction of the examples that failed to satisfy this requirement, but there on average more similar to examples of our own label when they have examples of the opposite level.",
            "OK, and since the professor is not very complicated, I'm going to quickly go for it.",
            "Um?",
            "So the idea is actually very simple.",
            "So first let's fix a good example X.",
            "So let's fix an example, but satisfies this condition right here.",
            "Now if we then imagine sampling the setters.",
            "Plus in this minus when we can show that the probability of error respect are given.",
            "A good example X is at most at times epsilon prime, and this follows from a simple application of a standard construction inequality like hugging inequality.",
            "Now, since this is true for any given fixed good example X, but then we said they expected that our of our procedure over the set of good examples is at most Delta times epsilon prime and so that means that by Markov inequality we get there is at most Delta chance, but the error rate over the set of good examples is greater than epsilon prime and then by adding back the fraction of the examples failing to satisfy this inequality, we get the desired guarantee that the error rate of the classification.",
            "The output is at most epsilon plus epsilon prime, so if we had the similarity function at design, this condition would have a trivial learning algorithm.",
            "However, it turns out, as you might expect, but not all good kind of functions are going to satisfy this condition, and you wouldn't.",
            "Yeah, and basically it would be hard to imagine that all with kernel function satisfy this condition, because otherwise we wouldn't really need interesting learning algorithm like SM or Perception.",
            "We could just use this trivial learning algorithm in order to learn, well, OK.",
            "But here is a conch."
        ],
        [
            "Example showing that showing that not all good counter functions are good similarity functions.",
            "Evidence.",
            "OK, so.",
            "So for this example, imagine that the point without examples are pointing the dimensional equivalent space and with the similarity function that we can see that the kernel function, we can see there is a standard of product in this 2 dimensional.",
            "In space, and we measure that the our distribution looks like this one here, so the.",
            "Positive examples are split equally among upper left and upper right and the negative examples lie all here.",
            "And also just for simplicity, imagine whether distribution is 5050 so.",
            "Half of the examples are positive and half of the examples are negative.",
            "Now if we.",
            "Consider an appropriate angle right here.",
            "In particular, if you get an angle of 30 degrees here, then clearly this kernel function with similarity function we're going to have a very large margin, so the margin we're going to be actually constant there going to be 1/2.",
            "However.",
            "I can easily verify that this similarity function fails to satisfy this condition over here.",
            "And it does so even for epsilon or constant, even for epsilon, say 1 / 4 Y, because a large fraction of the examples are from the positive examples, namely all the positive examples right here have a harder productive random negative example when they have random positive example.",
            "So in particular the dot product of a random negative example is we can verify the half while the DOT product or random.",
            "Positive example would be 1/2 * 1 + 1/2 times minus the heart, so this is 1/4.",
            "OK, so this is an example where we have a similarity function.",
            "Legal kernel function.",
            "A good kernel function not only good kernel function, it has a good large margin, but fails to satisfy our simplest notion of a good similarity function.",
            "However, notice that if in this simple example, right?"
        ],
        [
            "Here we simply ignored all the positive examples here in the upper left.",
            "When computing these expectations.",
            "Then we would be fine.",
            "And so these men suggests trying to broaden the definition as follows.",
            "We're going to say that our similarity function is a good similarity function for our learning problem.",
            "If there exists a large enough set of.",
            "So.",
            "Reasonable points such that most of the examples X are on average more similar to reasonable points of their own label when that's reasonable points of the opposite label.",
            "OK, now that we don't even need to know this set are in advance.",
            "We simply have to hope that it exists."
        ],
        [
            "Alright, so this is precisely our main definition that turns out to be broad enough.",
            "So formally we say that K our our similarity function is an epsilon gamma at how?",
            "Good similarity function if there exists set are of reasonable points such that most of the examples X-17 fraction of the examples X satisfy the property.",
            "But we're expecting similarity to reasonable point of wrong label is greater by at least a certain gap.",
            ", and expect the similarity to reasonable points of the opposite level.",
            "And we also require that we have at least top probability mass of reasonable positive examples.",
            "And at least upper body mass of reasonable negative examples.",
            "OK, so this is our main definition or main motion, right?",
            "So this is clearly in our broader definition.",
            "We have to show that we can still be learning and obviously to be more interesting here than in the previous case.",
            "And here is basically the main idea.",
            "So what we do is the following.",
            "If first draw a large enough set of so we can show you the following this.",
            "But we we draw a large enough set of landmarks or representative points.",
            "So if we draw the roughly one over gamma squared times 1 / 1000 Max and if we use this landmarks to really represent new data points, then basically a similar argument is that I've done before for the simpler definition shows that in the new feature space we're going to have a lawyer or large L1L Infinity.",
            "Separator.",
            "So basically if we re represent each point X by mapping into a D dimensional feature vector where each NTIS similarity of reference point I then kind of essentially the same algorithm had before for the simple definition shows that if these rough one over gamma squared times one over to, then we very high probability in the new feature space.",
            "We're going to have a lower or linear separator.",
            "In fact, we're going to have any separator that has a large L1L Infinity margin, and why?",
            "Because if we draw a 3D one over gamma squared times over Tau landmarks, then we have probability we're going to have at least one over gamma squared reasonable positive examples and at least one over Gamma Square.",
            "Reasonable negative examples.",
            "Now, of course, we don't really know which those reasonable points are, but their existence implies the existence of a good linear separator.",
            "In this new feature space, and in particular, can easily show that this white vector defines a linear separator.",
            "So here I put White, zero, an unreasonable points and I put weight 1 / N plus or reasonable positive examples and white 1 / -- 1 / N minus or reasonable negative examples.",
            "And if you think about it, when this positive way to help us to compute an empirical estimate of this expectation right here and this negative whites are going to help us computer.",
            "Estimation empirical estimate of this expectation right here and if.",
            "N plus and minus are large enough and the same argument that I had earlier shows that this expectation will be close enough to overestimate.",
            "This estimate will be closing after virtue expectations, and so this one implies that we have a good linear separator.",
            "In fact, the good L1L Infinity linear separator.",
            "OK, so I've just.",
            "Hopefully convinced you that would be a supporter exist.",
            "Now of course we want to also be able to find it, but now the point is that this is we are back to the problem of learning a linear separator for which we have really good learning algorithms.",
            "OK, so basically in order to learn well what we have to do is."
        ],
        [
            "The following we first draw a large enough set of landmarks.",
            "As I discussed.",
            "We use this landmass to represent new data points, and in particular what we do with taken you fresh set of labeled examples.",
            "We map them into this new feature space and then we run an L1L Infinity.",
            "Large margin separator algorithm and then we simply output the classification rule produced and so for example, in the case on epsilon is zero.",
            "We can even run, we know and so this this right here would be the sample complexity of our algorithm.",
            "So the number of unlabeled examples given by our landmarks would be roughly over gamma squared times one over to, while the number of labeled examples would be roughly over gamma squared, times epsilon accuracy, desired, accuracy level times, log of the OK.",
            "So if we had the similarity function satisfying this condition but is now hide it.",
            "We we can use it to learn well.",
            "Why an interesting learning algorithm?",
            "Alright."
        ],
        [
            "Now, in addition to learnability, we can also show actually a very nice and desirable property.",
            "We can show that any good kernel function in the traditional sense is a good similarity function in our sense.",
            "And we do get in the worst case gap is the translation.",
            "We do get a penalty in the translation, in particular the gap gamma get squared, but modulo is penalty, which basically brings up gives us a formal way of thinking about good kernel functions as being good similarity functions and just to give an example of the type of statement.",
            "So for example you can show that if K is a zero gamma good kernel, then for any tile K is that how gamma squared target similarity function in our sense?",
            "This is just to give you a feel about the type of result to get.",
            "OK, so basically we show that any good kernel function is a good similarity function in our sense.",
            "And Moreover, clearly our algorithms also apply to similarity functions that are not necessarily legal kind of functions.",
            "Which I think is very nice, and so I mean, this suggests that our future is strictly more general, and in fact we can even."
        ],
        [
            "Formally prove it.",
            "We do our notion of a good similarity function is strictly more general than a notion of a good kernel function, so you can show a strict separation of automatic separation, and in particular we can show that for any class of North pairwise uncorrelated functions, there exists a similarity function that is simultaneously good for all the functions in our class.",
            "But now single good kernel function exists.",
            "So obviously formal is strictly more general.",
            "Is it hard to do in some kind of embedding into another Hilbert space so?",
            "You are now representing each point by its distance to other points in the space, right?",
            "Yeah, so."
        ],
        [
            "Can we do an embedding?",
            "But I guess they want one interesting point here to get our best translation guarantees.",
            "We run at L1L Infinity learning algorithm.",
            "So we don't in the original definition.",
            "We had used an L2L2 learning algorithm to get much.",
            "We got much worse bounds, much worse translation.",
            "It says that for most.",
            "Semi conscious films intervention that cannot be invented by any good care.",
            "What you're saying is that you do.",
            "Heading into space and still can't get.",
            "No, no, but it's it's a key point that we use an L1L Infinity learning algorithm so it's different so we don't use an L2L2 learning algorithm linear separator.",
            "Learning algorithm, is it allowed us to get the translation to get the separation essentially and in the original paper we had a different notion of a good similarity function, but was using an L2L2 learning algorithm and for vet and we couldn't get separation.",
            "But because of this in the writing, the second piece of work because of the salon Infinity with there is no contradiction along the lines you mentioned.",
            "But the key point.",
            "Well, one I can see if I know that my similarity measure when you invented it well, well, well, so I mean so so right so, OK.",
            "So first of all, I don't think it's L1 in the usual algorithmics sense.",
            "It's more in our sense.",
            "But you are right.",
            "But are there exists obviously similarity function but not going to satisfy our main condition.",
            "Yeah, so that's definitely true.",
            "But what we show formally is that any good kernel function in the traditional sense satisfies our notion.",
            "We've maybe a certain loss in the parameters in the worst case.",
            "What?",
            "So for example, I mean it depends.",
            "For example, in the right in the case when epsilon is zero, you can just run window.",
            "If you do have.",
            "If you don't have a good kernel function, we don't say that you should really use our algorithm.",
            "So if it happens that you do have a good kernel function, you should use the SVM.",
            "So I'm just saying, but if you don't have a good legal kernel function.",
            "The sample complexity goes up in the worst case, right?",
            "So that's why if you can prove to me but for your learning problem, have a legal kernel function, then you might be better off using SVM, don't necessarily.",
            "In the number of samples.",
            "So the gamma, the gap gamma gives you the number of samples.",
            "Well, the other cell phone it has implications that I'm potentially yeah but most important on the number of samples.",
            "OK, and let me also mention that one can also."
        ],
        [
            "So obviously so, given all this framework, we can also talk about learning multiple with multiple similarity functions.",
            "So assume that for example, you have K1K2K R which are based similarity functions such with the whole But some unknown convex combination of EM is good, but it turns out that we can also learn in this case pretty well.",
            "And basically we can just use a similar algorithm that as well, but I had before, and the only difference is that now we're going to map our examples X into an R * D dimensional feature space.",
            "Wait and be given by the similarity.",
            "One of our similarities between our new point and a reference point.",
            "And then we can show that.",
            "Basically, with the guarantee that is high probability, then use distribution in this new feature space we're going to have a good error, large larger one and pretty linear separator.",
            "An interesting fact to note is that actually in this case, the sample complexity only goes up by lorgar factor because of the L1 NPT guarantee, so that's kind of a cute thing."
        ],
        [
            "So to conclude.",
            "OK, so in this work we develop a fury of learning with similarity functions, but are not necessarily legal kernel functions and our framework provides a formal way of understanding good kernel functions as good similarity functions, and one very important point is that our algorithms work for similarity functions that are not necessarily illegal kernel functions.",
            "And so one concrete algorithmic implication is that if you have a similarity function but is not necessarily illegal kernel function, but it's good, perhaps in our sense, when there is really no need to transform it in a distribution independent way into a legal kind of function and plug it in into SVM, you can just simply use our learning algorithm.",
            "So this is 1 concrete algorithmic implication, yes.",
            "It says hardest, OK as far as I know this is hard as learning, so it's an interesting question actually.",
            "Interesting theoretical question.",
            "Can you check with your labeled examples?",
            "But I'd like to know the answer.",
            "Let me know if you start now in terms of interesting concrete open."
        ],
        [
            "Options or semi concrete open questions.",
            "It will be very nice to analyze our notions of a good similarity function with is still useful for learning and in particular as I mentioned in our work, we are focusing on developing a notion of a good similarity function that encapsulates the usual notion of a good kernel function.",
            "But one could imagine trying to analyze, develop other notion of a good similarity function that is perhaps orthogonal to the notion of a good kernel function.",
            "Or you can also imagine trying to formalize learning with this similarity functions, and this is the answer.",
            "Wait?",
            "Your other question.",
            "Alright, thank you.",
            "Maybe?",
            "Find the scenario, if it exists, it doesn't.",
            "It's about the point is that you only need to help, but it exists.",
            "No, no, the algorithm implicitly you are going to apply linear separator learning algorithm.",
            "The algorithm to implicitly find it so you don't need to know the set R. You'll need to hope that it exists, but very important.",
            "If you knew that when you didn't need to apply linear separator learning algorithm, you can just use again a voting rule, but we don't need to know the set R and that's why sometimes the linear separator learning algorithm in the new feature space we're going to maybe implicitly find it."
        ],
        [
            "This is very nice and I understand your motivation.",
            "Problems.",
            "Kernels, can you give some examples of applications like biology classification problems for instance?",
            "So most of the sequence, most of the scores of alignment between sequences are simply not there, not legal dot products, and they have been optimized for years by biologists.",
            "You know, like something with something called blast.",
            "You know they are based on dynamic programming algorithm, necessarily legal kernel functions and the classic approach.",
            "In the past few years in the machine learning community has been to transform to come up with.",
            "In the distribution independent way we've legal dot product that is closing some distribution dependent way to the original similarity function.",
            "But our point is that, well, if it originally similarity functions optimized biologist for many years help us to satisfy requirement.",
            "There is no need to transform it into a legal dot product and then use SVM.",
            "You can just directly use our learning algorithm.",
            "So this is actually one of the best applications biology classification problems.",
            "Yes.",
            "Quickly.",
            "So there was this paper couple years ago.",
            "Basically used in which mental short segments by section doing it search, yeah.",
            "And then basically those results were used for similarity measure.",
            "This stuff looks pretty much like big versus what you're doing.",
            "It's pretty much.",
            "It looks pretty much like a precursor to what you're doing.",
            "Uh, huh?",
            "They do the same thing to take a similarity function, find the nearest points.",
            "So did I prove that the legal kind of function?",
            "I mean so just.",
            "The main point here is to be formal and probably in a more general and legal separation.",
            "Yeah.",
            "Instruction.",
            "In the stuff where you.",
            "Essentially, represent your points using this new vector.",
            "Yeah, that itself is.",
            "So in some sense, haven't you?",
            "So in the end again.",
            "So again we use an L1 Infinity learning algorithms, so we don't use an L2L2 learning algorithm, so that's a big difference, right?",
            "So, OK, so I guess not.",
            "I think now right?",
            "Alex is not.",
            "OK, you can be OK right?"
        ],
        [
            "So you can define here the DOT product to be the dot product usually dot product here, but then give you a bad magic and continue to get our in order to get our learning guarantees really need to use that one Infinity match.",
            "Great so I guess.",
            "My question was you say that you don't have to transform your similarity measurement about kernel, but isn't this stuff actually transformation over?",
            "Not really.",
            "So now, because again we use an L1 and Infinity, separate or not.",
            "Implicitly, implicitly, here I assume, for example, so I have this domain definition implicitly.",
            "Assume that they are normalized, but care of XX is 1.",
            "For example, that right because?"
        ],
        [
            "This wouldn't make sense.",
            "I assume that gamma is something between minus one and one.",
            "This is between say, minus one and one, so implicitly.",
            "Assume that, for example, the kernel is normalized.",
            "Connection to outliers.",
            "They are property holds for most points, but I can identify outliers.",
            "So first OK. One point is that we forgiven learning problem.",
            "We might have multiple sets of reasonable points.",
            "OK for our positive guarantees, only need with a good set exists and I will for ASCII.",
            "You are asking if there are no reasonable points are outliers.",
            "I don't really think about them that way to be honest.",
            "Can I find out?",
            "Testing it out.",
            "OK, so that's a great question.",
            "As I mentioned.",
            "OK, so that's good.",
            "Open question to think about.",
            "Can you test with the function is a good function in our sense with fewer examples.",
            "Would actually learn goodness separator.",
            "If one exists.",
            "It's a great question for property testing people and non property.",
            "If your teacher is unreliable but not random, can you still can you extend your framework again?",
            "Random.",
            "All the latest which were given to yeah we can also deal with the agnostic case.",
            "We cannot.",
            "This is biology.",
            "Yeah, we also deal with the agnostic case as well, right so?",
            "Possibly stepped on the slides.",
            "The actually didn't even I epsilon.",
            "Actually here epsilon is maybe epsilon captures.",
            "Epsilon can be non zero rights only.",
            "Most of the points at all of the points at inside the condition and then but maybe with capture viewer.",
            "I'm not sure what I'm saying is URLs are given, yeah?",
            "And some of them are wrong.",
            "Yeah, and I'm saying.",
            "But I mean, this will correspond to those lastic learning setting, and I think that we can also capture this by epsilon to be non 0.",
            "This is one way.",
            "OK, there might be a better way."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me start with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "High level picture and brief overview of my talk, so let's consider a generic classification problem.",
                    "label": 1
                },
                {
                    "sent": "So for instance, given set of images, let's say that we want to try to learn a rule to distinguish images of man from those of women and our natural approach would be to try to use a standard classification technique.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we might try to use.",
                    "label": 0
                },
                {
                    "sent": "And our linear separator learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "However, the Pixel representation might not be so good in the sense that the data might not be linearly separable in this natural pixel representation.",
                    "label": 1
                },
                {
                    "sent": "Now, instead of powerful technique which has been.",
                    "label": 1
                },
                {
                    "sent": "Extremely popular in recent years in machine learning is to use a kernel and two kernel eyes are linear separate or learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "And I'll define in the middle.",
                    "label": 0
                },
                {
                    "sent": "That means I'm sure that you already know, but for now, let me just mention that kernel functions have become extremely popular in recent years in machine learning, in part due to the fact that they do have a very nice and solid theoretical foundation.",
                    "label": 1
                },
                {
                    "sent": "And the theory talks about the effectiveness of our given common functions in terms of margins and implicit high dimensional feature spaces.",
                    "label": 0
                },
                {
                    "sent": "Now, however, while quite elegantly, theory does not necessarily correspond to sometimes a common intuition with a good current function is in fact one, but after the good measure of similarity, and Moreover and more importantly, the requirement that the given similarity function is a legal kernel function.",
                    "label": 0
                },
                {
                    "sent": "My simply rule out some of the most natural similarity functions for our learning problem, and so we so motivated by this what we do in our work.",
                    "label": 0
                },
                {
                    "sent": "We develop a theory of learning with general similarity functions.",
                    "label": 1
                },
                {
                    "sent": "That matches, say the common intuition with a good kernel function is on, But Act is a good measure of similarity and also again more importantly, we show how these methods have such methods can be applied to even more general similarity functions that are not necessarily legal corner functions, and so this second point especially seems to fit the the main theme of this workshop.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is a high level overview of my talk now, going to details.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to quickly remind you a kernel K pairwise function K is a legal sanction case.",
                    "label": 0
                },
                {
                    "sent": "Illegal kernel function, if it corresponds to a legal definition of that product.",
                    "label": 0
                },
                {
                    "sent": "So in other words, a pairwise function K is legal kernel function if there exists an implicit mapping Phi into a possibly very high dimensional feature space with the property, but K of XY is the product of 5X.",
                    "label": 0
                },
                {
                    "sent": "And five Y in that space.",
                    "label": 0
                },
                {
                    "sent": "So for instance, this function right here.",
                    "label": 0
                },
                {
                    "sent": "So we spend assumption.",
                    "label": 0
                },
                {
                    "sent": "Here is a kernel.",
                    "label": 0
                },
                {
                    "sent": "It's called the polynomial kernel and it corresponds to legal dot product into a higher dimensional feature space where the features are given by monomials of degree up to D on the base features.",
                    "label": 0
                },
                {
                    "sent": "So this is not obvious, but one can verify.",
                    "label": 0
                },
                {
                    "sent": "OK, now the point is that many.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithm many classic learning algorithms such as LCM or Perceptron can be written so that they interact with the data, but only by the product.",
                    "label": 0
                },
                {
                    "sent": "And so that means that if we now replace dot product computation with the kernel computation.",
                    "label": 0
                },
                {
                    "sent": "When our algorithm act implicitly as if the data was in the hard drive itself into space, and it does so without having to pay high price computationally, so without having to ever going to the higher dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "And hopefully some nicer things are going to happen in high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "Penn.com back to this point.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the classic picture or example, probably taken from Alex is book Alex Miller book.",
                    "label": 0
                },
                {
                    "sent": "So data right here on the left is not linearly separable in the original 2 dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "However, it is separable by an ellipse, and it is also separable in the file space induced by the polynomial kernel of degree, and so in higher dimensional feature space.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is much easier to make the data linearly separable and Moreover.",
                    "label": 0
                },
                {
                    "sent": "If the data is separable, linearly separable by a large margin.",
                    "label": 1
                },
                {
                    "sent": "Then we also get good generalization bounds.",
                    "label": 0
                },
                {
                    "sent": "We also get good translation guarantees.",
                    "label": 0
                },
                {
                    "sent": "So for instance standard bounded say.",
                    "label": 0
                },
                {
                    "sent": "But if the data is separable by merging gamma in the file space, then we only need roughly one over gamma squared labeled examples to learn well.",
                    "label": 0
                },
                {
                    "sent": "So we only need one over gamma squared labeled examples in order to come up with a rule.",
                    "label": 0
                },
                {
                    "sent": "But as well on new examples coming from the same source.",
                    "label": 0
                },
                {
                    "sent": "Alright so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, going back to a more generic view, as you probably all know, kernel functions have proven very useful in practice for dealing with all sorts of data.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned, there is also this very nice and elegant super nice theory about it makes a given kernel function a good kernel function for a given learning problem, however.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The existing theory talks in terms of margins in implicit high dimensional feature spaces, which sometimes are difficult to maybe characterize.",
                    "label": 1
                },
                {
                    "sent": "It also does not correspond to, say, a common intuition with a good kernel function is in fact one that acts like a good measure of similarity, so it might not be necessarily for certain people the best for intuition, but more importantly, the requirement that the similarity function is in fact the legal kernel function may simply rule out some of the most natural similarity functions for certain domain.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in the context of protein sequence classification, some of the most well known, biologically motivated similarity functions are simply not legal with products.",
                    "label": 0
                },
                {
                    "sent": "And so this brings up the question.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can one develop a alternativ, perhaps more intuitive, but more importantly broader theory of learning with similarity functions right?",
                    "label": 0
                },
                {
                    "sent": "And the answer Luckily is yes, so in our work we provide theory of learning with more general similarity functions, but are not necessarily legal kernel function.",
                    "label": 1
                },
                {
                    "sent": "Illegal kernel functions, and it turns out that this theory also formalizes a common intuition with a good kernel function, is in fact one that acts as a good measure of similarity and much more concretely what we do.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're doing this work more concretely.",
                    "label": 0
                },
                {
                    "sent": "We develop a notion of a good similarity function for a given learning problem, which satisfies the following desirable properties.",
                    "label": 0
                },
                {
                    "sent": "First of all, it talks in terms of natural direct quantities of the data.",
                    "label": 1
                },
                {
                    "sent": "It does not reference implicit high dimensional feature space is and it has no requirement that the given similarity function corresponds to illegal dot product, so it's a legal kernel function.",
                    "label": 0
                },
                {
                    "sent": "Also, if we had the similarity function satisfying our notion, then we could use it to learn well, so such a similarity function has good implications for learning.",
                    "label": 0
                },
                {
                    "sent": "Now second, we also show that our notion is broad in the sense that it probably includes original notion of a good kernel function, and here by a good kernel function I mean one with induces a large margin in the file space, so it's broad.",
                    "label": 0
                },
                {
                    "sent": "And the interesting also we can show up at our notion is strictly more general in a formal sense, in the sense that we can exhibit classes of functions for which a single kernel function exists, but a single good similarity function exists.",
                    "label": 0
                },
                {
                    "sent": "But there is no single good kind of function exists, so our theory is also probably strictly more general.",
                    "label": 0
                },
                {
                    "sent": "To understand you correctly that you want to have sort of an axiomatization of what similarity is not, axiomatization is more like.",
                    "label": 0
                },
                {
                    "sent": "No, so it's more like analyzing so natural conditions of a given similarity function that has good implications for learning, so it's more in the directions and wanted to measurement process.",
                    "label": 0
                },
                {
                    "sent": "So in many applications it's much simpler and much simpler to test for identity or to judge the two.",
                    "label": 0
                },
                {
                    "sent": "Things are very similar rather than establishing precise measurements of very dissimilar things.",
                    "label": 0
                },
                {
                    "sent": "And I think this is this is a very strong argument for for judging similarity based functions, whether they're useful.",
                    "label": 0
                },
                {
                    "sent": "Right, so our goal here.",
                    "label": 0
                },
                {
                    "sent": "OK, that's why.",
                    "label": 0
                },
                {
                    "sent": "So you could go in many directions, but our goal here is to generalize the usual notion of a good kernel function.",
                    "label": 0
                },
                {
                    "sent": "While still getting good implications for learning, I'm not to suggest would be just an alternative.",
                    "label": 0
                },
                {
                    "sent": "Might perhaps orthogonal way too.",
                    "label": 1
                },
                {
                    "sent": "Try to formalize learning of similarity functions.",
                    "label": 0
                },
                {
                    "sent": "You can do a better job with quite similar.",
                    "label": 0
                },
                {
                    "sent": "I think it's also for your application.",
                    "label": 0
                },
                {
                    "sent": "It's more important to be precise on very similar objects and distances between very distant knology doesn't matter so much for that.",
                    "label": 0
                },
                {
                    "sent": "I will have to really think about it.",
                    "label": 0
                },
                {
                    "sent": "OK, before saying yes or no, but definitely OK.",
                    "label": 0
                },
                {
                    "sent": "So technically speaking, our goal is to generalize the usual notion of a good kernel function.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is our technical goal.",
                    "label": 0
                },
                {
                    "sent": "While still allowing.",
                    "label": 0
                },
                {
                    "sent": "The algorithm to use non legal corner functions.",
                    "label": 0
                },
                {
                    "sent": "I mean obviously you can develop a theory of learning of dissimilarity functions and analyze conditions that would work under the situation that you have in mind.",
                    "label": 0
                },
                {
                    "sent": "Because on one hand you want to define what similarities and on the other hand, no.",
                    "label": 0
                },
                {
                    "sent": "No, I just don't you see.",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll go, we'll go OK, we'll go for details.",
                    "label": 0
                },
                {
                    "sent": "So then you understand precisely.",
                    "label": 0
                },
                {
                    "sent": "So this is just at the high level.",
                    "label": 1
                },
                {
                    "sent": "We want to define proposal, notion of wood similarity function that can be used.",
                    "label": 0
                },
                {
                    "sent": "To learn well and the sticky generalize visual notion of a good kernel function.",
                    "label": 1
                },
                {
                    "sent": "OK and actually maybe my next slide will be extremely helpful for dictation.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, before I even talk about our main ocean just to.",
                    "label": 0
                },
                {
                    "sent": "Give an example of the type of thing I'm thinking about.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start with something simpler that does not satisfies all the three requirements.",
                    "label": 0
                },
                {
                    "sent": "Not broad enough in particular, but is which is simple enough.",
                    "label": 0
                },
                {
                    "sent": "And you don't even get efficient learning algorithm and then in a few slides I modified to make it broad enough so the simpler definition.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to start.",
                    "label": 0
                },
                {
                    "sent": "It doesn't capture all the good kernel functions, but then in a few slides are modified to capture all good kernel functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll assume throughout this talk what we're trying to do binary classification and without our examples come according to a distribution fixed unknown distribution over labeled examples.",
                    "label": 1
                },
                {
                    "sent": "And our goal is to achieve low classification at all respect to the distribution.",
                    "label": 0
                },
                {
                    "sent": "And I'll probably clarify your question little bit.",
                    "label": 0
                },
                {
                    "sent": "OK, so our first notion again, it's not bad enough, but our foreign notion of a good similarity function is the following.",
                    "label": 0
                },
                {
                    "sent": "We say that K is a good similarity function for our given any problem.",
                    "label": 0
                },
                {
                    "sent": "If most of the points X on average more similar to points of round label when the point of the opposite library.",
                    "label": 1
                },
                {
                    "sent": "So formally we save it K and epsilon gamma good similarity function for our learning problem.",
                    "label": 0
                },
                {
                    "sent": "If at least one epsilon fraction of the examples X.",
                    "label": 0
                },
                {
                    "sent": "Satisfy the requirement.",
                    "label": 1
                },
                {
                    "sent": "But that obvious similarity to a point X is the same label.",
                    "label": 0
                },
                {
                    "sent": "Is greater by at least.",
                    "label": 0
                },
                {
                    "sent": "A certain gap, then the average, similar to the point of the opposite label.",
                    "label": 0
                },
                {
                    "sent": "And this gap gamma right here defines the goodness of our given similarity function.",
                    "label": 0
                },
                {
                    "sent": "This is an example of a notion of a good semantic function.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for instance, say that the similarity tend to positive examples that laser .2 the similarity between end to negative between negative examples that laser pointer and the similarity between a positive example and the negative example is a random number in minus one one, then this.",
                    "label": 0
                },
                {
                    "sent": "Left hand side would be at least three point to the right hand side would be 0 and so we should satisfy our notion for epsilon zero and gamma 0.2.",
                    "label": 0
                },
                {
                    "sent": "Just a simple example.",
                    "label": 0
                },
                {
                    "sent": "And actually, if you think about it.",
                    "label": 0
                },
                {
                    "sent": "The instant space is large enough there is a nontrivial chance that this similarity function will in fact not going to be a little corner function.",
                    "label": 0
                },
                {
                    "sent": "I mean, so this looks like a very reasonable notion about.",
                    "label": 0
                },
                {
                    "sent": "Dude asks for a good similarity function, and Moreover if you had a similarity function satisfying this very simple condition where.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have a very trivial efficient learning algorithm, so all we have to do in this case is to just draw a bunch of positive examples.",
                    "label": 0
                },
                {
                    "sent": "A bunch of negative examples, and then to simply output the classification rule, but classified as an example X as positive.",
                    "label": 0
                },
                {
                    "sent": "It's an average more similar to examples in this plus and this negative otherwise, so this is not.",
                    "label": 0
                },
                {
                    "sent": "This definition is not bad enough to keep the robot kernel functions for now, just as simple as start point.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me I'll come to the main ocean in a bit.",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming that we this is the notion that captures a good kind of functions.",
                    "label": 0
                },
                {
                    "sent": "You see an example in a second.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in any case, so if we had the similarity function satisfying this condition when we had, we have a, we can use this simple language that I described.",
                    "label": 0
                },
                {
                    "sent": "We draw a bunch of positive examples, a bunch of negative examples, and then I'll put the classification rule that classifieds a new example X if it's on average, more similar to examples in this plus into examples in this month, and as a negative otherwise, and then the guarantee we get is that if sets S plus in this minus have size at least one over gamma squared.",
                    "label": 1
                },
                {
                    "sent": "One of our Delta Times epsilon prime, then with high probability at least someone is Delta.",
                    "label": 0
                },
                {
                    "sent": "The classification rule that well put has there are at most epsilon plus epsilon prime and so notice that we get error rate which is arbitrarily close.",
                    "label": 0
                },
                {
                    "sent": "To the fraction of the examples that failed to satisfy this requirement, but there on average more similar to examples of our own label when they have examples of the opposite level.",
                    "label": 0
                },
                {
                    "sent": "OK, and since the professor is not very complicated, I'm going to quickly go for it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the idea is actually very simple.",
                    "label": 0
                },
                {
                    "sent": "So first let's fix a good example X.",
                    "label": 0
                },
                {
                    "sent": "So let's fix an example, but satisfies this condition right here.",
                    "label": 0
                },
                {
                    "sent": "Now if we then imagine sampling the setters.",
                    "label": 0
                },
                {
                    "sent": "Plus in this minus when we can show that the probability of error respect are given.",
                    "label": 0
                },
                {
                    "sent": "A good example X is at most at times epsilon prime, and this follows from a simple application of a standard construction inequality like hugging inequality.",
                    "label": 0
                },
                {
                    "sent": "Now, since this is true for any given fixed good example X, but then we said they expected that our of our procedure over the set of good examples is at most Delta times epsilon prime and so that means that by Markov inequality we get there is at most Delta chance, but the error rate over the set of good examples is greater than epsilon prime and then by adding back the fraction of the examples failing to satisfy this inequality, we get the desired guarantee that the error rate of the classification.",
                    "label": 1
                },
                {
                    "sent": "The output is at most epsilon plus epsilon prime, so if we had the similarity function at design, this condition would have a trivial learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "However, it turns out, as you might expect, but not all good kind of functions are going to satisfy this condition, and you wouldn't.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and basically it would be hard to imagine that all with kernel function satisfy this condition, because otherwise we wouldn't really need interesting learning algorithm like SM or Perception.",
                    "label": 0
                },
                {
                    "sent": "We could just use this trivial learning algorithm in order to learn, well, OK.",
                    "label": 0
                },
                {
                    "sent": "But here is a conch.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example showing that showing that not all good counter functions are good similarity functions.",
                    "label": 0
                },
                {
                    "sent": "Evidence.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So for this example, imagine that the point without examples are pointing the dimensional equivalent space and with the similarity function that we can see that the kernel function, we can see there is a standard of product in this 2 dimensional.",
                    "label": 0
                },
                {
                    "sent": "In space, and we measure that the our distribution looks like this one here, so the.",
                    "label": 0
                },
                {
                    "sent": "Positive examples are split equally among upper left and upper right and the negative examples lie all here.",
                    "label": 0
                },
                {
                    "sent": "And also just for simplicity, imagine whether distribution is 5050 so.",
                    "label": 0
                },
                {
                    "sent": "Half of the examples are positive and half of the examples are negative.",
                    "label": 0
                },
                {
                    "sent": "Now if we.",
                    "label": 0
                },
                {
                    "sent": "Consider an appropriate angle right here.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you get an angle of 30 degrees here, then clearly this kernel function with similarity function we're going to have a very large margin, so the margin we're going to be actually constant there going to be 1/2.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "I can easily verify that this similarity function fails to satisfy this condition over here.",
                    "label": 0
                },
                {
                    "sent": "And it does so even for epsilon or constant, even for epsilon, say 1 / 4 Y, because a large fraction of the examples are from the positive examples, namely all the positive examples right here have a harder productive random negative example when they have random positive example.",
                    "label": 0
                },
                {
                    "sent": "So in particular the dot product of a random negative example is we can verify the half while the DOT product or random.",
                    "label": 0
                },
                {
                    "sent": "Positive example would be 1/2 * 1 + 1/2 times minus the heart, so this is 1/4.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an example where we have a similarity function.",
                    "label": 0
                },
                {
                    "sent": "Legal kernel function.",
                    "label": 0
                },
                {
                    "sent": "A good kernel function not only good kernel function, it has a good large margin, but fails to satisfy our simplest notion of a good similarity function.",
                    "label": 1
                },
                {
                    "sent": "However, notice that if in this simple example, right?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we simply ignored all the positive examples here in the upper left.",
                    "label": 0
                },
                {
                    "sent": "When computing these expectations.",
                    "label": 0
                },
                {
                    "sent": "Then we would be fine.",
                    "label": 0
                },
                {
                    "sent": "And so these men suggests trying to broaden the definition as follows.",
                    "label": 0
                },
                {
                    "sent": "We're going to say that our similarity function is a good similarity function for our learning problem.",
                    "label": 0
                },
                {
                    "sent": "If there exists a large enough set of.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Reasonable points such that most of the examples X are on average more similar to reasonable points of their own label when that's reasonable points of the opposite label.",
                    "label": 1
                },
                {
                    "sent": "OK, now that we don't even need to know this set are in advance.",
                    "label": 0
                },
                {
                    "sent": "We simply have to hope that it exists.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so this is precisely our main definition that turns out to be broad enough.",
                    "label": 0
                },
                {
                    "sent": "So formally we say that K our our similarity function is an epsilon gamma at how?",
                    "label": 0
                },
                {
                    "sent": "Good similarity function if there exists set are of reasonable points such that most of the examples X-17 fraction of the examples X satisfy the property.",
                    "label": 1
                },
                {
                    "sent": "But we're expecting similarity to reasonable point of wrong label is greater by at least a certain gap.",
                    "label": 0
                },
                {
                    "sent": ", and expect the similarity to reasonable points of the opposite level.",
                    "label": 0
                },
                {
                    "sent": "And we also require that we have at least top probability mass of reasonable positive examples.",
                    "label": 1
                },
                {
                    "sent": "And at least upper body mass of reasonable negative examples.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is our main definition or main motion, right?",
                    "label": 0
                },
                {
                    "sent": "So this is clearly in our broader definition.",
                    "label": 0
                },
                {
                    "sent": "We have to show that we can still be learning and obviously to be more interesting here than in the previous case.",
                    "label": 0
                },
                {
                    "sent": "And here is basically the main idea.",
                    "label": 0
                },
                {
                    "sent": "So what we do is the following.",
                    "label": 1
                },
                {
                    "sent": "If first draw a large enough set of so we can show you the following this.",
                    "label": 0
                },
                {
                    "sent": "But we we draw a large enough set of landmarks or representative points.",
                    "label": 0
                },
                {
                    "sent": "So if we draw the roughly one over gamma squared times 1 / 1000 Max and if we use this landmarks to really represent new data points, then basically a similar argument is that I've done before for the simpler definition shows that in the new feature space we're going to have a lawyer or large L1L Infinity.",
                    "label": 0
                },
                {
                    "sent": "Separator.",
                    "label": 0
                },
                {
                    "sent": "So basically if we re represent each point X by mapping into a D dimensional feature vector where each NTIS similarity of reference point I then kind of essentially the same algorithm had before for the simple definition shows that if these rough one over gamma squared times one over to, then we very high probability in the new feature space.",
                    "label": 0
                },
                {
                    "sent": "We're going to have a lower or linear separator.",
                    "label": 0
                },
                {
                    "sent": "In fact, we're going to have any separator that has a large L1L Infinity margin, and why?",
                    "label": 0
                },
                {
                    "sent": "Because if we draw a 3D one over gamma squared times over Tau landmarks, then we have probability we're going to have at least one over gamma squared reasonable positive examples and at least one over Gamma Square.",
                    "label": 0
                },
                {
                    "sent": "Reasonable negative examples.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, we don't really know which those reasonable points are, but their existence implies the existence of a good linear separator.",
                    "label": 0
                },
                {
                    "sent": "In this new feature space, and in particular, can easily show that this white vector defines a linear separator.",
                    "label": 0
                },
                {
                    "sent": "So here I put White, zero, an unreasonable points and I put weight 1 / N plus or reasonable positive examples and white 1 / -- 1 / N minus or reasonable negative examples.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it, when this positive way to help us to compute an empirical estimate of this expectation right here and this negative whites are going to help us computer.",
                    "label": 1
                },
                {
                    "sent": "Estimation empirical estimate of this expectation right here and if.",
                    "label": 0
                },
                {
                    "sent": "N plus and minus are large enough and the same argument that I had earlier shows that this expectation will be close enough to overestimate.",
                    "label": 0
                },
                {
                    "sent": "This estimate will be closing after virtue expectations, and so this one implies that we have a good linear separator.",
                    "label": 0
                },
                {
                    "sent": "In fact, the good L1L Infinity linear separator.",
                    "label": 0
                },
                {
                    "sent": "OK, so I've just.",
                    "label": 0
                },
                {
                    "sent": "Hopefully convinced you that would be a supporter exist.",
                    "label": 0
                },
                {
                    "sent": "Now of course we want to also be able to find it, but now the point is that this is we are back to the problem of learning a linear separator for which we have really good learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically in order to learn well what we have to do is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The following we first draw a large enough set of landmarks.",
                    "label": 1
                },
                {
                    "sent": "As I discussed.",
                    "label": 1
                },
                {
                    "sent": "We use this landmass to represent new data points, and in particular what we do with taken you fresh set of labeled examples.",
                    "label": 1
                },
                {
                    "sent": "We map them into this new feature space and then we run an L1L Infinity.",
                    "label": 0
                },
                {
                    "sent": "Large margin separator algorithm and then we simply output the classification rule produced and so for example, in the case on epsilon is zero.",
                    "label": 0
                },
                {
                    "sent": "We can even run, we know and so this this right here would be the sample complexity of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the number of unlabeled examples given by our landmarks would be roughly over gamma squared times one over to, while the number of labeled examples would be roughly over gamma squared, times epsilon accuracy, desired, accuracy level times, log of the OK.",
                    "label": 0
                },
                {
                    "sent": "So if we had the similarity function satisfying this condition but is now hide it.",
                    "label": 0
                },
                {
                    "sent": "We we can use it to learn well.",
                    "label": 0
                },
                {
                    "sent": "Why an interesting learning algorithm?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, in addition to learnability, we can also show actually a very nice and desirable property.",
                    "label": 0
                },
                {
                    "sent": "We can show that any good kernel function in the traditional sense is a good similarity function in our sense.",
                    "label": 0
                },
                {
                    "sent": "And we do get in the worst case gap is the translation.",
                    "label": 0
                },
                {
                    "sent": "We do get a penalty in the translation, in particular the gap gamma get squared, but modulo is penalty, which basically brings up gives us a formal way of thinking about good kernel functions as being good similarity functions and just to give an example of the type of statement.",
                    "label": 0
                },
                {
                    "sent": "So for example you can show that if K is a zero gamma good kernel, then for any tile K is that how gamma squared target similarity function in our sense?",
                    "label": 1
                },
                {
                    "sent": "This is just to give you a feel about the type of result to get.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically we show that any good kernel function is a good similarity function in our sense.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, clearly our algorithms also apply to similarity functions that are not necessarily legal kind of functions.",
                    "label": 0
                },
                {
                    "sent": "Which I think is very nice, and so I mean, this suggests that our future is strictly more general, and in fact we can even.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formally prove it.",
                    "label": 0
                },
                {
                    "sent": "We do our notion of a good similarity function is strictly more general than a notion of a good kernel function, so you can show a strict separation of automatic separation, and in particular we can show that for any class of North pairwise uncorrelated functions, there exists a similarity function that is simultaneously good for all the functions in our class.",
                    "label": 1
                },
                {
                    "sent": "But now single good kernel function exists.",
                    "label": 0
                },
                {
                    "sent": "So obviously formal is strictly more general.",
                    "label": 0
                },
                {
                    "sent": "Is it hard to do in some kind of embedding into another Hilbert space so?",
                    "label": 0
                },
                {
                    "sent": "You are now representing each point by its distance to other points in the space, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can we do an embedding?",
                    "label": 0
                },
                {
                    "sent": "But I guess they want one interesting point here to get our best translation guarantees.",
                    "label": 0
                },
                {
                    "sent": "We run at L1L Infinity learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we don't in the original definition.",
                    "label": 0
                },
                {
                    "sent": "We had used an L2L2 learning algorithm to get much.",
                    "label": 0
                },
                {
                    "sent": "We got much worse bounds, much worse translation.",
                    "label": 0
                },
                {
                    "sent": "It says that for most.",
                    "label": 0
                },
                {
                    "sent": "Semi conscious films intervention that cannot be invented by any good care.",
                    "label": 0
                },
                {
                    "sent": "What you're saying is that you do.",
                    "label": 0
                },
                {
                    "sent": "Heading into space and still can't get.",
                    "label": 0
                },
                {
                    "sent": "No, no, but it's it's a key point that we use an L1L Infinity learning algorithm so it's different so we don't use an L2L2 learning algorithm linear separator.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithm, is it allowed us to get the translation to get the separation essentially and in the original paper we had a different notion of a good similarity function, but was using an L2L2 learning algorithm and for vet and we couldn't get separation.",
                    "label": 0
                },
                {
                    "sent": "But because of this in the writing, the second piece of work because of the salon Infinity with there is no contradiction along the lines you mentioned.",
                    "label": 0
                },
                {
                    "sent": "But the key point.",
                    "label": 0
                },
                {
                    "sent": "Well, one I can see if I know that my similarity measure when you invented it well, well, well, so I mean so so right so, OK.",
                    "label": 0
                },
                {
                    "sent": "So first of all, I don't think it's L1 in the usual algorithmics sense.",
                    "label": 0
                },
                {
                    "sent": "It's more in our sense.",
                    "label": 0
                },
                {
                    "sent": "But you are right.",
                    "label": 0
                },
                {
                    "sent": "But are there exists obviously similarity function but not going to satisfy our main condition.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's definitely true.",
                    "label": 0
                },
                {
                    "sent": "But what we show formally is that any good kernel function in the traditional sense satisfies our notion.",
                    "label": 0
                },
                {
                    "sent": "We've maybe a certain loss in the parameters in the worst case.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "So for example, I mean it depends.",
                    "label": 0
                },
                {
                    "sent": "For example, in the right in the case when epsilon is zero, you can just run window.",
                    "label": 0
                },
                {
                    "sent": "If you do have.",
                    "label": 0
                },
                {
                    "sent": "If you don't have a good kernel function, we don't say that you should really use our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So if it happens that you do have a good kernel function, you should use the SVM.",
                    "label": 0
                },
                {
                    "sent": "So I'm just saying, but if you don't have a good legal kernel function.",
                    "label": 0
                },
                {
                    "sent": "The sample complexity goes up in the worst case, right?",
                    "label": 0
                },
                {
                    "sent": "So that's why if you can prove to me but for your learning problem, have a legal kernel function, then you might be better off using SVM, don't necessarily.",
                    "label": 0
                },
                {
                    "sent": "In the number of samples.",
                    "label": 0
                },
                {
                    "sent": "So the gamma, the gap gamma gives you the number of samples.",
                    "label": 0
                },
                {
                    "sent": "Well, the other cell phone it has implications that I'm potentially yeah but most important on the number of samples.",
                    "label": 0
                },
                {
                    "sent": "OK, and let me also mention that one can also.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So obviously so, given all this framework, we can also talk about learning multiple with multiple similarity functions.",
                    "label": 1
                },
                {
                    "sent": "So assume that for example, you have K1K2K R which are based similarity functions such with the whole But some unknown convex combination of EM is good, but it turns out that we can also learn in this case pretty well.",
                    "label": 1
                },
                {
                    "sent": "And basically we can just use a similar algorithm that as well, but I had before, and the only difference is that now we're going to map our examples X into an R * D dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "Wait and be given by the similarity.",
                    "label": 0
                },
                {
                    "sent": "One of our similarities between our new point and a reference point.",
                    "label": 0
                },
                {
                    "sent": "And then we can show that.",
                    "label": 0
                },
                {
                    "sent": "Basically, with the guarantee that is high probability, then use distribution in this new feature space we're going to have a good error, large larger one and pretty linear separator.",
                    "label": 0
                },
                {
                    "sent": "An interesting fact to note is that actually in this case, the sample complexity only goes up by lorgar factor because of the L1 NPT guarantee, so that's kind of a cute thing.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this work we develop a fury of learning with similarity functions, but are not necessarily legal kernel functions and our framework provides a formal way of understanding good kernel functions as good similarity functions, and one very important point is that our algorithms work for similarity functions that are not necessarily illegal kernel functions.",
                    "label": 1
                },
                {
                    "sent": "And so one concrete algorithmic implication is that if you have a similarity function but is not necessarily illegal kernel function, but it's good, perhaps in our sense, when there is really no need to transform it in a distribution independent way into a legal kind of function and plug it in into SVM, you can just simply use our learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 concrete algorithmic implication, yes.",
                    "label": 0
                },
                {
                    "sent": "It says hardest, OK as far as I know this is hard as learning, so it's an interesting question actually.",
                    "label": 0
                },
                {
                    "sent": "Interesting theoretical question.",
                    "label": 0
                },
                {
                    "sent": "Can you check with your labeled examples?",
                    "label": 0
                },
                {
                    "sent": "But I'd like to know the answer.",
                    "label": 0
                },
                {
                    "sent": "Let me know if you start now in terms of interesting concrete open.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Options or semi concrete open questions.",
                    "label": 1
                },
                {
                    "sent": "It will be very nice to analyze our notions of a good similarity function with is still useful for learning and in particular as I mentioned in our work, we are focusing on developing a notion of a good similarity function that encapsulates the usual notion of a good kernel function.",
                    "label": 1
                },
                {
                    "sent": "But one could imagine trying to analyze, develop other notion of a good similarity function that is perhaps orthogonal to the notion of a good kernel function.",
                    "label": 0
                },
                {
                    "sent": "Or you can also imagine trying to formalize learning with this similarity functions, and this is the answer.",
                    "label": 0
                },
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "Your other question.",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Find the scenario, if it exists, it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It's about the point is that you only need to help, but it exists.",
                    "label": 0
                },
                {
                    "sent": "No, no, the algorithm implicitly you are going to apply linear separator learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "The algorithm to implicitly find it so you don't need to know the set R. You'll need to hope that it exists, but very important.",
                    "label": 0
                },
                {
                    "sent": "If you knew that when you didn't need to apply linear separator learning algorithm, you can just use again a voting rule, but we don't need to know the set R and that's why sometimes the linear separator learning algorithm in the new feature space we're going to maybe implicitly find it.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is very nice and I understand your motivation.",
                    "label": 0
                },
                {
                    "sent": "Problems.",
                    "label": 0
                },
                {
                    "sent": "Kernels, can you give some examples of applications like biology classification problems for instance?",
                    "label": 0
                },
                {
                    "sent": "So most of the sequence, most of the scores of alignment between sequences are simply not there, not legal dot products, and they have been optimized for years by biologists.",
                    "label": 0
                },
                {
                    "sent": "You know, like something with something called blast.",
                    "label": 0
                },
                {
                    "sent": "You know they are based on dynamic programming algorithm, necessarily legal kernel functions and the classic approach.",
                    "label": 0
                },
                {
                    "sent": "In the past few years in the machine learning community has been to transform to come up with.",
                    "label": 0
                },
                {
                    "sent": "In the distribution independent way we've legal dot product that is closing some distribution dependent way to the original similarity function.",
                    "label": 0
                },
                {
                    "sent": "But our point is that, well, if it originally similarity functions optimized biologist for many years help us to satisfy requirement.",
                    "label": 0
                },
                {
                    "sent": "There is no need to transform it into a legal dot product and then use SVM.",
                    "label": 0
                },
                {
                    "sent": "You can just directly use our learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is actually one of the best applications biology classification problems.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Quickly.",
                    "label": 0
                },
                {
                    "sent": "So there was this paper couple years ago.",
                    "label": 0
                },
                {
                    "sent": "Basically used in which mental short segments by section doing it search, yeah.",
                    "label": 0
                },
                {
                    "sent": "And then basically those results were used for similarity measure.",
                    "label": 0
                },
                {
                    "sent": "This stuff looks pretty much like big versus what you're doing.",
                    "label": 0
                },
                {
                    "sent": "It's pretty much.",
                    "label": 0
                },
                {
                    "sent": "It looks pretty much like a precursor to what you're doing.",
                    "label": 0
                },
                {
                    "sent": "Uh, huh?",
                    "label": 0
                },
                {
                    "sent": "They do the same thing to take a similarity function, find the nearest points.",
                    "label": 0
                },
                {
                    "sent": "So did I prove that the legal kind of function?",
                    "label": 0
                },
                {
                    "sent": "I mean so just.",
                    "label": 0
                },
                {
                    "sent": "The main point here is to be formal and probably in a more general and legal separation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Instruction.",
                    "label": 0
                },
                {
                    "sent": "In the stuff where you.",
                    "label": 0
                },
                {
                    "sent": "Essentially, represent your points using this new vector.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that itself is.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, haven't you?",
                    "label": 0
                },
                {
                    "sent": "So in the end again.",
                    "label": 0
                },
                {
                    "sent": "So again we use an L1 Infinity learning algorithms, so we don't use an L2L2 learning algorithm, so that's a big difference, right?",
                    "label": 0
                },
                {
                    "sent": "So, OK, so I guess not.",
                    "label": 0
                },
                {
                    "sent": "I think now right?",
                    "label": 0
                },
                {
                    "sent": "Alex is not.",
                    "label": 0
                },
                {
                    "sent": "OK, you can be OK right?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can define here the DOT product to be the dot product usually dot product here, but then give you a bad magic and continue to get our in order to get our learning guarantees really need to use that one Infinity match.",
                    "label": 0
                },
                {
                    "sent": "Great so I guess.",
                    "label": 0
                },
                {
                    "sent": "My question was you say that you don't have to transform your similarity measurement about kernel, but isn't this stuff actually transformation over?",
                    "label": 0
                },
                {
                    "sent": "Not really.",
                    "label": 0
                },
                {
                    "sent": "So now, because again we use an L1 and Infinity, separate or not.",
                    "label": 0
                },
                {
                    "sent": "Implicitly, implicitly, here I assume, for example, so I have this domain definition implicitly.",
                    "label": 0
                },
                {
                    "sent": "Assume that they are normalized, but care of XX is 1.",
                    "label": 0
                },
                {
                    "sent": "For example, that right because?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This wouldn't make sense.",
                    "label": 0
                },
                {
                    "sent": "I assume that gamma is something between minus one and one.",
                    "label": 0
                },
                {
                    "sent": "This is between say, minus one and one, so implicitly.",
                    "label": 0
                },
                {
                    "sent": "Assume that, for example, the kernel is normalized.",
                    "label": 0
                },
                {
                    "sent": "Connection to outliers.",
                    "label": 0
                },
                {
                    "sent": "They are property holds for most points, but I can identify outliers.",
                    "label": 0
                },
                {
                    "sent": "So first OK. One point is that we forgiven learning problem.",
                    "label": 0
                },
                {
                    "sent": "We might have multiple sets of reasonable points.",
                    "label": 0
                },
                {
                    "sent": "OK for our positive guarantees, only need with a good set exists and I will for ASCII.",
                    "label": 0
                },
                {
                    "sent": "You are asking if there are no reasonable points are outliers.",
                    "label": 0
                },
                {
                    "sent": "I don't really think about them that way to be honest.",
                    "label": 0
                },
                {
                    "sent": "Can I find out?",
                    "label": 0
                },
                {
                    "sent": "Testing it out.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a great question.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's good.",
                    "label": 0
                },
                {
                    "sent": "Open question to think about.",
                    "label": 0
                },
                {
                    "sent": "Can you test with the function is a good function in our sense with fewer examples.",
                    "label": 0
                },
                {
                    "sent": "Would actually learn goodness separator.",
                    "label": 0
                },
                {
                    "sent": "If one exists.",
                    "label": 0
                },
                {
                    "sent": "It's a great question for property testing people and non property.",
                    "label": 0
                },
                {
                    "sent": "If your teacher is unreliable but not random, can you still can you extend your framework again?",
                    "label": 0
                },
                {
                    "sent": "Random.",
                    "label": 0
                },
                {
                    "sent": "All the latest which were given to yeah we can also deal with the agnostic case.",
                    "label": 0
                },
                {
                    "sent": "We cannot.",
                    "label": 0
                },
                {
                    "sent": "This is biology.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we also deal with the agnostic case as well, right so?",
                    "label": 0
                },
                {
                    "sent": "Possibly stepped on the slides.",
                    "label": 0
                },
                {
                    "sent": "The actually didn't even I epsilon.",
                    "label": 0
                },
                {
                    "sent": "Actually here epsilon is maybe epsilon captures.",
                    "label": 0
                },
                {
                    "sent": "Epsilon can be non zero rights only.",
                    "label": 0
                },
                {
                    "sent": "Most of the points at all of the points at inside the condition and then but maybe with capture viewer.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure what I'm saying is URLs are given, yeah?",
                    "label": 0
                },
                {
                    "sent": "And some of them are wrong.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and I'm saying.",
                    "label": 0
                },
                {
                    "sent": "But I mean, this will correspond to those lastic learning setting, and I think that we can also capture this by epsilon to be non 0.",
                    "label": 0
                },
                {
                    "sent": "This is one way.",
                    "label": 0
                },
                {
                    "sent": "OK, there might be a better way.",
                    "label": 0
                }
            ]
        }
    }
}