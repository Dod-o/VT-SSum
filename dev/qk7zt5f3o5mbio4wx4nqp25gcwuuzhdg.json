{
    "id": "qk7zt5f3o5mbio4wx4nqp25gcwuuzhdg",
    "title": "Fast Incremental Proximity Search in Large Graphs",
    "info": {
        "author": [
            "Purnamrita Sarkar, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Aug. 1, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Search Engines",
            "Top->Computer Science->Machine Learning->Learning to Rank"
        ]
    },
    "url": "http://videolectures.net/icml08_sarkar_slg/",
    "segmentation": [
        [
            "Content based."
        ],
        [
            "Version database is considered the sightseer data set, which has papers and wards in citations.",
            "You can easily represent that biograf which will have paper nodes which are connected by citations.",
            "Word nodes which are connected to the paper if they appear in the title of the paper, and now the kind of question you want to ask is find the top K papers which match the query SVM say and the main idea is that you somehow want to capture the complex relationships embedding the graph structure.",
            "For example here, even if paper 1 does not have SVM in the title paper, two has assuming the title and paper to sites paper one a very simple kind of relation, and so there's a lot of overlap between paper one and maybe the content of paper one SVM, so.",
            "This again will require ranking all the papers for some query no?"
        ],
        [
            "So all these problems are ranking nodes in a graph and in order to ranking we need proximity measures and in this talk we're going to focus on random walk based proximity measures.",
            "So there are a bunch of well known proximity measures which are based on random walks, personalized page rank.",
            "There is hitting in commute times.",
            "There are also truncated versions of these measures, So what we're going to talk about is present an algorithm which computes nearest neighbors under these truncated measures really quickly.",
            "Without looking at the entire graph, and then we're going to show some results which will show that these measures do really well in real world tasks."
        ],
        [
            "Right, so OK.",
            "So in order to do that I have to go into the background a little bit, talk about all these proximity measures, the standard and truncated versions, and then talk a little bit about previous work before I jump into."
        ],
        [
            "So what is a random walk?",
            "Very simple like you have a graph, you start at node one."
        ],
        [
            "And then you pick a neighbor at random moved."
        ],
        [
            "Move to that neighbor and you keep."
        ],
        [
            "Now the idea is that if I random walk starting at me, it's another lot of times then broadly that node is somehow closed and this main idea behind hitting times are expected.",
            "Path Link to a node so.",
            "Let's be a little more."
        ],
        [
            "Rock specific about the definition of hitting time so the hitting time from I to J in this graph is.",
            "Time random walk starting at node I takes before it hits node J and the way it's defined is."
        ],
        [
            "I will take one step that brings me to my neighbors and so the hitting time from I to J is going to be."
        ],
        [
            "One plus the average of eyes hitting times 2 J."
        ],
        [
            "So here is basically the recursive definition of fitting time from I to J.",
            "Also reading time to yourself is 0.",
            "This is an asymmetric measure, it's not symmetric, so the symmetric version of it is the commute time which is waiting time to to know Jane back from node J.",
            "Great."
        ],
        [
            "So these measures have been used a lot for link prediction and recommender systems and they suffer from several drawbacks.",
            "What are those drawbacks?",
            "One problem is that they often take into account very long paths and that's why they will be very sensitive to parts of the graph which are unrelated to the query node.",
            "That's not property.",
            "And another problem is because of the same reason that it takes into account very long paths, you are more probable to hit a high.",
            "Sometimes So what happens is that you are more prone to pick up popular entities.",
            "And this is really not good for personalization, so let's see how to improve on these measures.",
            "How to get rid of these properties?"
        ],
        [
            "If we don't like, so we're going to use a very simple change of the UN truncated hitting times you're going to truncate all the random walks of path length up to T, so this is the exact same recursive definition from before, but now I'm just going to say that I'll only look at paths of length less than T. So now hitting time from I to J in T steps is just one plus the average of eyes neighbors it in time to Jane minus one steps.",
            "And note that this definition automatically splitting time between I and J2T if I&J are more than the hops away and the commute time is again just the round trip version.",
            "OK, so let's see how these are different from the intro."
        ],
        [
            "This version of hitting times so here is a random graph of 100 nodes.",
            "I give 2 dimensional random coordinates to everyone and then links output between two nodes with high probability.",
            "If they're closing the Euclidean space.",
            "And as you can see, I'm plotting the top 15 nearest neighbors from node 95 here in blue, so.",
            "These notes there's a bunch of nodes which are far away from node 95 in this graph, but they come up in top 15 years neighbors, and essentially what happens is that the random walk gets lost in this very dense part of the graph, and that's again, that's exactly the kind of problem we're talking about.",
            "So let's now look at the neighbors that come up in the truncated hitting time.",
            "These around 10.",
            "And again you have a nicely localized neighborhood, which is something that's.",
            "A nice property."
        ],
        [
            "To sum up, for a small value of T, these measures are not sensitive to very long paths and for the same reason they're not going to necessarily favor very high degree nodes.",
            "So I have some experimental snippets which show this, but unfortunately I don't have time, so please stop by the poster session to hear about it.",
            "More so these things are really good for personalized search.",
            "But let's there is also another reason why these are better than non truncated version and that is these are easier to compute than the exact UN truncated hearing time."
        ],
        [
            "So now let's look at computational complexity.",
            "So I have been talking about computing nearest neighbors from a node or computer nearest neighbor and hitting time two or node.",
            "In order to do that, I want to compute all the nodes hitting time to unload.",
            "So this is a truncated hitting time matrix and one column basically corresponds to hitting time of all nodes to node J and it turns out to be the case that you can compute all these values just using something like a value iteration using the.",
            "Recursive formula that I showed before and that just takes 2 times number of edges time complexity.",
            "OK, great simple dynamic programming, but here is a really counter intuitive fact.",
            "If I want to compute hitting time from Lodi to everyone else, it looks like the only way we can do that is fill up the entire matrix so you will have to basically compute every column in the matrix to get reading time from I to that node.",
            "No.",
            "So.",
            "Keeping these things in mind, we came up before in previous work, we design an algorithm which.",
            "Compute nearest neighbors in much less than their time.",
            "So let's bring so."
        ],
        [
            "Two previous."
        ],
        [
            "And let's see.",
            "So I just said that if you have the setting time matrix, it's easy to compute it in time from everyone else to one node that is 1 column of this hitting time matrix using dynamic programming.",
            "Now in previous work we showed that.",
            "You don't even have to compute all the values in this node.",
            "Essentially, you can compute discovered the potential nearest neighbors of two or node Jane hitting time without looking at the entire column, which basically boils down to filling up the interesting patches in a column.",
            "No.",
            "After, but we really want the heating time from a note to everyone else.",
            "So in previous work what we do."
        ],
        [
            "Was up we filled up.",
            "All the columns.",
            "So we basically discovered the potential nearest neighbors of."
        ],
        [
            "All nodes and after that we looked at hitting time from a node, so took a review of this matrix."
        ],
        [
            "Now, so let's look at pros and cons.",
            "This is really good if you want to look at all pairs of nearest neighbors, so returned nearest neighbors for all nodes, because the amortized time and space per node is very small.",
            "But The thing is that this needs a large pre processing time and also a lot of space because you are basically filling up the entire matrix, even if the clever way.",
            "So what happens if the graph changes between two subsequent queries?",
            "You have to go back and do all of that again, so.",
            "That brings us to our algorithm, which is basically kind of on the fly and much more space efficient."
        ],
        [
            "So so are you going to propose to work and break it up into two parts theoretical justification?"
        ],
        [
            "And algorithm sketch.",
            "So let's just sum up what I said so that we're all on the same page.",
            "I want to return K approximate nearest neighbors in truncated commute time without looking at the entire graph.",
            "Now since I want commute time, let's break it up into hitting time from the query node and hitting time to the query node.",
            "Now, in previous work we showed that you can use dynamic programming to easily compute hitting time to unload, but hitting time from a note to everyone else is hard because you have to fill up the entire matrix.",
            "Now the rest of the talk.",
            "I'll show that you can actually sampling, which makes the exact opposite of the former, which is that.",
            "It's easy to compute it in time from a note to other nodes and hard to computing time two unknown and So what we will do is somehow or other combine these two approaches to use best of both worlds so that we can actually return nearest neighbors and commute time.",
            "OK, so."
        ],
        [
            "So.",
            "So here is a sketch of proposed work.",
            "Again, let's be a little more formal about the query we want to return K approximate nearest neighbors within Twice Tao commute time.",
            "We're going to do some sort of arrange search in commute time, so return all the K nearest neighbors which are reading twice Tao commute time and.",
            "You can actually theoretically justify that for any graph, the number of nodes within twice tell commute time is actually small.",
            "Ann.",
            "How are you going to do that?",
            "You will show we will show that number of nodes within hitting time Tau from a node from the query node is small and the other half is number of nodes within hitting time Tau two unload is small.",
            "And then we will present an algorithm which adaptively finds a neighborhood around the query node which is going to contain all the nodes within twice to commute time with high probability.",
            "So that brings us and then we basically just do ranking over these nodes to get the K nearest."
        ],
        [
            "So that brings."
        ],
        [
            "The theoretical justification.",
            "Let's see.",
            "So this is a graph.",
            "The red node is the query node.",
            "This yellow neighborhood consists of all the nodes which are random walk of T steps starting at the red node is going to hit within time and what we want to see is that how big can this neighborhood be?",
            "So essentially this boils down to looking at this set so all nodes Jay suggested in time from I to J smaller than tell an.",
            "Here is a really interesting result which is that.",
            "For any graph and for any node, this set size can actually be upper bounded by T square 14 -- 12, and that's very interesting, because if Tian Tao are both small with respect to North, then this neighborhood is basically small in size, which is the kind of result we wanted to see.",
            "So."
        ],
        [
            "Now let's since I'm looking at commute time.",
            "Within twice die.",
            "Let's look at the other way.",
            "So let's now look at number of nodes within hitting time Tau to the query node.",
            "So again, this is the graph red query node.",
            "The green neighborhood consists of all the nodes from which party step random walk is going to hit the query node within time.",
            "And.",
            "Boils down to looking at the size of this set so all nodes JS everything time from J2Y is smaller than Tau.",
            "So it turns out to be the case that for directed graphs all you can say is that not too many nodes in a graph are going to have a lot of neighbors within Tao hitting time to it, however."
        ],
        [
            "For undirected graphs, you can use reversibility of random walks to prove a much stronger guarantee.",
            "You can actually show that for any node I, this incoming town neighborhood is going to be upper bounded by T ^2 / T minus Tau times a factor which only depends on kind of degree of that node.",
            "So this is again going to be small if the degree of the node is small.",
            "Antient hour constant or small?"
        ],
        [
            "Respect win.",
            "OK, so let's now look at the algorithm sketch.",
            "Now that we know that there is this small twice down neighborhood around the query node, how are we going to fight?"
        ],
        [
            "OK, so here is a sketch of the algorithm.",
            "First you just do sampling and I'll show how to estimate it in time from a node I and for any node I you kind of maintain a neighborhood envy of I and using the sampled estimates that you got before and dynamic programming we will give high probability bounds on commute time from I to all the nodes within neighborhood of I, and the idea is that as we expand the neighborhoods, these bounds are going to get tighter.",
            "So long.",
            "And after that we are going to do ranking using the bounds on commute time.",
            "Now there are a lot of details which I can't talk about here.",
            "How do you expand your neighborhood and another very interesting detail is how do you stop the neighborhood expansion?",
            "In fact that is going to guarantee that you are not leaving out any node which is within twice now commute time.",
            "OK."
        ],
        [
            "So let's quickly look at sampling.",
            "So simple algorithm.",
            "So I want five truncated hitting time from one to every."
        ],
        [
            "Owners, I'm"
        ],
        [
            "Going to simulate."
        ],
        [
            "Our leg."
        ],
        [
            "For apart from one so and from this sample you can estimate hitting time from one to all the nodes.",
            "So you hit this random walk, hits five in one step, nine in two step, seven in four steps.",
            "So these are the estimates on hitting time from one to all the other nodes, and it does not hit anyone else within five steps.",
            "So waiting time from everyone else is 5.",
            "So that's just the main idea of these truncated measures.",
            "Oh"
        ],
        [
            "So basically what I'm doing is I'm defining xig as the first hitting time from I to J in a random walk, and if the random walk does not hit J within T steps, then I'm going to just set it to T and you can show that expectation of these random variables is the hitting time from I to J and the estimate that we are going to use is just the sample average the moment we see these.",
            "Now the question will ask is how many samples do you need?"
        ],
        [
            "So here are.",
            "Quick look at the theorems.",
            "These are very simple epsilon Delta kind of bounds, but I'll just talk about the interesting bits.",
            "So one thing is that I'm estimating time from I to all nodes.",
            "The number of how many samples do I need to have low error in value with very high probability and you can show that the sample complexity is proportional to the logarithm of N, so N is all nodes in the graph.",
            "So if the graph size grows, it's only going to be a log factor increase in number of samples.",
            "Now what if we don't want to get close in the value?",
            "Using the estimates, but we kind of want to retrieve the top K neighbors correctly and you can basically show that the number of samples required to do that with high probability is small if the gap between the true K10K plus one at nearest neighbor is large."
        ],
        [
            "So.",
            "So we have given the algorithm sketch.",
            "Look at let's."
        ],
        [
            "Look at some experimental results.",
            "We used the sites here, graph the query queries of the form, find me K nearest neighbors in truncating, and commute time.",
            "These graphs had around 600,000 nodes, 3 million edges.",
            "Everything was done in on single CPU machines and so here is a sampling which takes very small time to compute the hitting time from a node.",
            "The exact dynamic programming for power query on average is around 90 seconds.",
            "On the other hand, our algorithm takes around 4 seconds per query and this is without any preprocessing."
        ],
        [
            "OK, so let's look at the graph that we formed.",
            "So this is the keyword author citation graph that we built from sites here.",
            "There is a layer of words, papers and authors and the words are connected to the papers.",
            "If they appeared in the title of the paper, papers are connected to each other via citation and authors are connected to papers.",
            "If they wrote those papers and the weighing schemes we used were very similar to other authors who have worked on these graphs.",
            "As well, so existing work use personalized Pagerank to do content based search in this kind of datasets.",
            "What we're going to do is present some quantifiable link prediction tasks and we will compare our measures with personalized page rank."
        ],
        [
            "So let's look at 1 task and.",
            "We pick a paper.",
            "We pick all the words which appeared in the title of the paper.",
            "This is a typical link prediction task.",
            "Remove all the edges between them."
        ],
        [
            "Around the world, and then we're trying to rank all the papers for those words and see if the paper come.",
            "The original paper comes up in like top 135 ten years neighbors.",
            "I know that this is very interesting because the success of a search engine really depends on how good the top 10 results are."
        ],
        [
            "So and similarly, you can define the task for author nodes where you basically take out links between a paper and its authors.",
            "And do they?"
        ],
        [
            "Back team ranking.",
            "So here is the result here.",
            "The result, the blue line is the hitting time from an old, the red is PPV.",
            "This is commute time and on the X axis we have nearest neighbors like 1, three, 510 etc.",
            "Y axis is accuracy, so higher the better and you can see that in the word task the hitting times do better than PPV and they both beat the commute times by quite a large margin and the hitting times MPP we do.",
            "Similarly, when a little better, that's because they're very similar measures intrinsic."
        ],
        [
            "And let's look at daughter task.",
            "Here you can see that the commute times basically outperforms everyone else by a huge margin and this is.",
            "Kind of interesting because you can see that different link prediction tasks probably require different kind of proximity measures.",
            "And if you can compute the nearest neighbors really really quickly, then you can actually go ahead and adaptively pick the right proximity measure for the right task."
        ],
        [
            "So.",
            "Let's look at conclusion.",
            "We presented an algorithm which is kind of incremental on the fly to compute nearest neighbors.",
            "If the graph changes between two queries, then it's still fast on most quantifiable link prediction tasks it outperforms other well known alternatives, and on a single CPU machine we can actually process queries on really really fast on pretty huge graphs like 600,000 nodes and 3 million edges."
        ],
        [
            "I really like to acknowledge all these people for their input in the paper and thank you.",
            "OK, well crossing over there.",
            "And can we use the microphone chording purposes?"
        ],
        [
            "Thank you if I understood correctly.",
            "The motivation of your work is that you can compute efficiently with dynamic programming.",
            "The heating time from a note, but then you have a hard time to compute it to a node.",
            "Is that correct?",
            "OK, whatever.",
            "It seems that in one sense you are using a forward recurrence basically, and why not using a backward recurrence to computing the opposite direction so it doesn't work that way because so when you look at hitting time two inode it's it's kind of the random walk stops at node, right?",
            "The destination node, right?",
            "You are essentially essentially solving kind of linear system of like voltages or something like that so that the backward recurrence does not work to compute the hitting time from unknown.",
            "It's unclear to me.",
            "I think you can.",
            "You can have the probability of finishing the work in one step.",
            "Immediately and then from there you can compute it recursively in finishing the work in two step in three steps.",
            "Recursively you can.",
            "You can do that easily when I'm fixing the destination node, so I can kind of look at neighborhood, right?",
            "That's exactly the dynamic programming.",
            "Now think about the forward things.",
            "I want to compute my hitting time to everyone else that could not be doable.",
            "Then you start from the node and then you compute from where you go in one step and then recursively for me you're going to the recursive thing because the random walks.",
            "Shops at the destination node, and that's the hitting time 2 from I2J that will keep changing form for all the other nodes that you are looking at that.",
            "But that's why.",
            "But if you combine forward and backward recurrences then you have both results.",
            "You can't do the forward recurrence for the hitting time from a node because of that problem we can.",
            "Well, I think we'll discuss offline.",
            "Thank you.",
            "Who is just wondering when you say personalized page rank.",
            "Were you comparing with exact?",
            "Yeah, we compared everything with exactly so that's why I didn't give any time complexity because I didn't use like the faster ones."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Content based.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Version database is considered the sightseer data set, which has papers and wards in citations.",
                    "label": 0
                },
                {
                    "sent": "You can easily represent that biograf which will have paper nodes which are connected by citations.",
                    "label": 0
                },
                {
                    "sent": "Word nodes which are connected to the paper if they appear in the title of the paper, and now the kind of question you want to ask is find the top K papers which match the query SVM say and the main idea is that you somehow want to capture the complex relationships embedding the graph structure.",
                    "label": 0
                },
                {
                    "sent": "For example here, even if paper 1 does not have SVM in the title paper, two has assuming the title and paper to sites paper one a very simple kind of relation, and so there's a lot of overlap between paper one and maybe the content of paper one SVM, so.",
                    "label": 0
                },
                {
                    "sent": "This again will require ranking all the papers for some query no?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So all these problems are ranking nodes in a graph and in order to ranking we need proximity measures and in this talk we're going to focus on random walk based proximity measures.",
                    "label": 1
                },
                {
                    "sent": "So there are a bunch of well known proximity measures which are based on random walks, personalized page rank.",
                    "label": 1
                },
                {
                    "sent": "There is hitting in commute times.",
                    "label": 0
                },
                {
                    "sent": "There are also truncated versions of these measures, So what we're going to talk about is present an algorithm which computes nearest neighbors under these truncated measures really quickly.",
                    "label": 1
                },
                {
                    "sent": "Without looking at the entire graph, and then we're going to show some results which will show that these measures do really well in real world tasks.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so OK.",
                    "label": 0
                },
                {
                    "sent": "So in order to do that I have to go into the background a little bit, talk about all these proximity measures, the standard and truncated versions, and then talk a little bit about previous work before I jump into.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is a random walk?",
                    "label": 0
                },
                {
                    "sent": "Very simple like you have a graph, you start at node one.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you pick a neighbor at random moved.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move to that neighbor and you keep.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the idea is that if I random walk starting at me, it's another lot of times then broadly that node is somehow closed and this main idea behind hitting times are expected.",
                    "label": 1
                },
                {
                    "sent": "Path Link to a node so.",
                    "label": 0
                },
                {
                    "sent": "Let's be a little more.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rock specific about the definition of hitting time so the hitting time from I to J in this graph is.",
                    "label": 0
                },
                {
                    "sent": "Time random walk starting at node I takes before it hits node J and the way it's defined is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will take one step that brings me to my neighbors and so the hitting time from I to J is going to be.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One plus the average of eyes hitting times 2 J.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is basically the recursive definition of fitting time from I to J.",
                    "label": 0
                },
                {
                    "sent": "Also reading time to yourself is 0.",
                    "label": 0
                },
                {
                    "sent": "This is an asymmetric measure, it's not symmetric, so the symmetric version of it is the commute time which is waiting time to to know Jane back from node J.",
                    "label": 1
                },
                {
                    "sent": "Great.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these measures have been used a lot for link prediction and recommender systems and they suffer from several drawbacks.",
                    "label": 1
                },
                {
                    "sent": "What are those drawbacks?",
                    "label": 0
                },
                {
                    "sent": "One problem is that they often take into account very long paths and that's why they will be very sensitive to parts of the graph which are unrelated to the query node.",
                    "label": 1
                },
                {
                    "sent": "That's not property.",
                    "label": 0
                },
                {
                    "sent": "And another problem is because of the same reason that it takes into account very long paths, you are more probable to hit a high.",
                    "label": 0
                },
                {
                    "sent": "Sometimes So what happens is that you are more prone to pick up popular entities.",
                    "label": 1
                },
                {
                    "sent": "And this is really not good for personalization, so let's see how to improve on these measures.",
                    "label": 0
                },
                {
                    "sent": "How to get rid of these properties?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we don't like, so we're going to use a very simple change of the UN truncated hitting times you're going to truncate all the random walks of path length up to T, so this is the exact same recursive definition from before, but now I'm just going to say that I'll only look at paths of length less than T. So now hitting time from I to J in T steps is just one plus the average of eyes neighbors it in time to Jane minus one steps.",
                    "label": 0
                },
                {
                    "sent": "And note that this definition automatically splitting time between I and J2T if I&J are more than the hops away and the commute time is again just the round trip version.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see how these are different from the intro.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This version of hitting times so here is a random graph of 100 nodes.",
                    "label": 0
                },
                {
                    "sent": "I give 2 dimensional random coordinates to everyone and then links output between two nodes with high probability.",
                    "label": 0
                },
                {
                    "sent": "If they're closing the Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, I'm plotting the top 15 nearest neighbors from node 95 here in blue, so.",
                    "label": 1
                },
                {
                    "sent": "These notes there's a bunch of nodes which are far away from node 95 in this graph, but they come up in top 15 years neighbors, and essentially what happens is that the random walk gets lost in this very dense part of the graph, and that's again, that's exactly the kind of problem we're talking about.",
                    "label": 1
                },
                {
                    "sent": "So let's now look at the neighbors that come up in the truncated hitting time.",
                    "label": 0
                },
                {
                    "sent": "These around 10.",
                    "label": 0
                },
                {
                    "sent": "And again you have a nicely localized neighborhood, which is something that's.",
                    "label": 0
                },
                {
                    "sent": "A nice property.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To sum up, for a small value of T, these measures are not sensitive to very long paths and for the same reason they're not going to necessarily favor very high degree nodes.",
                    "label": 1
                },
                {
                    "sent": "So I have some experimental snippets which show this, but unfortunately I don't have time, so please stop by the poster session to hear about it.",
                    "label": 1
                },
                {
                    "sent": "More so these things are really good for personalized search.",
                    "label": 0
                },
                {
                    "sent": "But let's there is also another reason why these are better than non truncated version and that is these are easier to compute than the exact UN truncated hearing time.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's look at computational complexity.",
                    "label": 1
                },
                {
                    "sent": "So I have been talking about computing nearest neighbors from a node or computer nearest neighbor and hitting time two or node.",
                    "label": 1
                },
                {
                    "sent": "In order to do that, I want to compute all the nodes hitting time to unload.",
                    "label": 0
                },
                {
                    "sent": "So this is a truncated hitting time matrix and one column basically corresponds to hitting time of all nodes to node J and it turns out to be the case that you can compute all these values just using something like a value iteration using the.",
                    "label": 1
                },
                {
                    "sent": "Recursive formula that I showed before and that just takes 2 times number of edges time complexity.",
                    "label": 0
                },
                {
                    "sent": "OK, great simple dynamic programming, but here is a really counter intuitive fact.",
                    "label": 0
                },
                {
                    "sent": "If I want to compute hitting time from Lodi to everyone else, it looks like the only way we can do that is fill up the entire matrix so you will have to basically compute every column in the matrix to get reading time from I to that node.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Keeping these things in mind, we came up before in previous work, we design an algorithm which.",
                    "label": 0
                },
                {
                    "sent": "Compute nearest neighbors in much less than their time.",
                    "label": 0
                },
                {
                    "sent": "So let's bring so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two previous.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let's see.",
                    "label": 0
                },
                {
                    "sent": "So I just said that if you have the setting time matrix, it's easy to compute it in time from everyone else to one node that is 1 column of this hitting time matrix using dynamic programming.",
                    "label": 1
                },
                {
                    "sent": "Now in previous work we showed that.",
                    "label": 0
                },
                {
                    "sent": "You don't even have to compute all the values in this node.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you can compute discovered the potential nearest neighbors of two or node Jane hitting time without looking at the entire column, which basically boils down to filling up the interesting patches in a column.",
                    "label": 1
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "After, but we really want the heating time from a note to everyone else.",
                    "label": 0
                },
                {
                    "sent": "So in previous work what we do.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was up we filled up.",
                    "label": 0
                },
                {
                    "sent": "All the columns.",
                    "label": 0
                },
                {
                    "sent": "So we basically discovered the potential nearest neighbors of.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All nodes and after that we looked at hitting time from a node, so took a review of this matrix.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, so let's look at pros and cons.",
                    "label": 0
                },
                {
                    "sent": "This is really good if you want to look at all pairs of nearest neighbors, so returned nearest neighbors for all nodes, because the amortized time and space per node is very small.",
                    "label": 1
                },
                {
                    "sent": "But The thing is that this needs a large pre processing time and also a lot of space because you are basically filling up the entire matrix, even if the clever way.",
                    "label": 1
                },
                {
                    "sent": "So what happens if the graph changes between two subsequent queries?",
                    "label": 0
                },
                {
                    "sent": "You have to go back and do all of that again, so.",
                    "label": 0
                },
                {
                    "sent": "That brings us to our algorithm, which is basically kind of on the fly and much more space efficient.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so are you going to propose to work and break it up into two parts theoretical justification?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And algorithm sketch.",
                    "label": 0
                },
                {
                    "sent": "So let's just sum up what I said so that we're all on the same page.",
                    "label": 0
                },
                {
                    "sent": "I want to return K approximate nearest neighbors in truncated commute time without looking at the entire graph.",
                    "label": 1
                },
                {
                    "sent": "Now since I want commute time, let's break it up into hitting time from the query node and hitting time to the query node.",
                    "label": 0
                },
                {
                    "sent": "Now, in previous work we showed that you can use dynamic programming to easily compute hitting time to unload, but hitting time from a note to everyone else is hard because you have to fill up the entire matrix.",
                    "label": 0
                },
                {
                    "sent": "Now the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'll show that you can actually sampling, which makes the exact opposite of the former, which is that.",
                    "label": 0
                },
                {
                    "sent": "It's easy to compute it in time from a note to other nodes and hard to computing time two unknown and So what we will do is somehow or other combine these two approaches to use best of both worlds so that we can actually return nearest neighbors and commute time.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here is a sketch of proposed work.",
                    "label": 0
                },
                {
                    "sent": "Again, let's be a little more formal about the query we want to return K approximate nearest neighbors within Twice Tao commute time.",
                    "label": 1
                },
                {
                    "sent": "We're going to do some sort of arrange search in commute time, so return all the K nearest neighbors which are reading twice Tao commute time and.",
                    "label": 0
                },
                {
                    "sent": "You can actually theoretically justify that for any graph, the number of nodes within twice tell commute time is actually small.",
                    "label": 0
                },
                {
                    "sent": "Ann.",
                    "label": 0
                },
                {
                    "sent": "How are you going to do that?",
                    "label": 1
                },
                {
                    "sent": "You will show we will show that number of nodes within hitting time Tau from a node from the query node is small and the other half is number of nodes within hitting time Tau two unload is small.",
                    "label": 0
                },
                {
                    "sent": "And then we will present an algorithm which adaptively finds a neighborhood around the query node which is going to contain all the nodes within twice to commute time with high probability.",
                    "label": 1
                },
                {
                    "sent": "So that brings us and then we basically just do ranking over these nodes to get the K nearest.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that brings.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The theoretical justification.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "So this is a graph.",
                    "label": 0
                },
                {
                    "sent": "The red node is the query node.",
                    "label": 0
                },
                {
                    "sent": "This yellow neighborhood consists of all the nodes which are random walk of T steps starting at the red node is going to hit within time and what we want to see is that how big can this neighborhood be?",
                    "label": 0
                },
                {
                    "sent": "So essentially this boils down to looking at this set so all nodes Jay suggested in time from I to J smaller than tell an.",
                    "label": 0
                },
                {
                    "sent": "Here is a really interesting result which is that.",
                    "label": 0
                },
                {
                    "sent": "For any graph and for any node, this set size can actually be upper bounded by T square 14 -- 12, and that's very interesting, because if Tian Tao are both small with respect to North, then this neighborhood is basically small in size, which is the kind of result we wanted to see.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's since I'm looking at commute time.",
                    "label": 0
                },
                {
                    "sent": "Within twice die.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the other way.",
                    "label": 0
                },
                {
                    "sent": "So let's now look at number of nodes within hitting time Tau to the query node.",
                    "label": 0
                },
                {
                    "sent": "So again, this is the graph red query node.",
                    "label": 0
                },
                {
                    "sent": "The green neighborhood consists of all the nodes from which party step random walk is going to hit the query node within time.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Boils down to looking at the size of this set so all nodes JS everything time from J2Y is smaller than Tau.",
                    "label": 0
                },
                {
                    "sent": "So it turns out to be the case that for directed graphs all you can say is that not too many nodes in a graph are going to have a lot of neighbors within Tao hitting time to it, however.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For undirected graphs, you can use reversibility of random walks to prove a much stronger guarantee.",
                    "label": 0
                },
                {
                    "sent": "You can actually show that for any node I, this incoming town neighborhood is going to be upper bounded by T ^2 / T minus Tau times a factor which only depends on kind of degree of that node.",
                    "label": 0
                },
                {
                    "sent": "So this is again going to be small if the degree of the node is small.",
                    "label": 0
                },
                {
                    "sent": "Antient hour constant or small?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Respect win.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's now look at the algorithm sketch.",
                    "label": 1
                },
                {
                    "sent": "Now that we know that there is this small twice down neighborhood around the query node, how are we going to fight?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here is a sketch of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "First you just do sampling and I'll show how to estimate it in time from a node I and for any node I you kind of maintain a neighborhood envy of I and using the sampled estimates that you got before and dynamic programming we will give high probability bounds on commute time from I to all the nodes within neighborhood of I, and the idea is that as we expand the neighborhoods, these bounds are going to get tighter.",
                    "label": 1
                },
                {
                    "sent": "So long.",
                    "label": 0
                },
                {
                    "sent": "And after that we are going to do ranking using the bounds on commute time.",
                    "label": 0
                },
                {
                    "sent": "Now there are a lot of details which I can't talk about here.",
                    "label": 0
                },
                {
                    "sent": "How do you expand your neighborhood and another very interesting detail is how do you stop the neighborhood expansion?",
                    "label": 0
                },
                {
                    "sent": "In fact that is going to guarantee that you are not leaving out any node which is within twice now commute time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's quickly look at sampling.",
                    "label": 0
                },
                {
                    "sent": "So simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "So I want five truncated hitting time from one to every.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Owners, I'm",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to simulate.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our leg.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For apart from one so and from this sample you can estimate hitting time from one to all the nodes.",
                    "label": 1
                },
                {
                    "sent": "So you hit this random walk, hits five in one step, nine in two step, seven in four steps.",
                    "label": 0
                },
                {
                    "sent": "So these are the estimates on hitting time from one to all the other nodes, and it does not hit anyone else within five steps.",
                    "label": 0
                },
                {
                    "sent": "So waiting time from everyone else is 5.",
                    "label": 0
                },
                {
                    "sent": "So that's just the main idea of these truncated measures.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically what I'm doing is I'm defining xig as the first hitting time from I to J in a random walk, and if the random walk does not hit J within T steps, then I'm going to just set it to T and you can show that expectation of these random variables is the hitting time from I to J and the estimate that we are going to use is just the sample average the moment we see these.",
                    "label": 0
                },
                {
                    "sent": "Now the question will ask is how many samples do you need?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are.",
                    "label": 0
                },
                {
                    "sent": "Quick look at the theorems.",
                    "label": 0
                },
                {
                    "sent": "These are very simple epsilon Delta kind of bounds, but I'll just talk about the interesting bits.",
                    "label": 0
                },
                {
                    "sent": "So one thing is that I'm estimating time from I to all nodes.",
                    "label": 1
                },
                {
                    "sent": "The number of how many samples do I need to have low error in value with very high probability and you can show that the sample complexity is proportional to the logarithm of N, so N is all nodes in the graph.",
                    "label": 1
                },
                {
                    "sent": "So if the graph size grows, it's only going to be a log factor increase in number of samples.",
                    "label": 0
                },
                {
                    "sent": "Now what if we don't want to get close in the value?",
                    "label": 0
                },
                {
                    "sent": "Using the estimates, but we kind of want to retrieve the top K neighbors correctly and you can basically show that the number of samples required to do that with high probability is small if the gap between the true K10K plus one at nearest neighbor is large.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we have given the algorithm sketch.",
                    "label": 0
                },
                {
                    "sent": "Look at let's.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at some experimental results.",
                    "label": 1
                },
                {
                    "sent": "We used the sites here, graph the query queries of the form, find me K nearest neighbors in truncating, and commute time.",
                    "label": 0
                },
                {
                    "sent": "These graphs had around 600,000 nodes, 3 million edges.",
                    "label": 0
                },
                {
                    "sent": "Everything was done in on single CPU machines and so here is a sampling which takes very small time to compute the hitting time from a node.",
                    "label": 0
                },
                {
                    "sent": "The exact dynamic programming for power query on average is around 90 seconds.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, our algorithm takes around 4 seconds per query and this is without any preprocessing.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at the graph that we formed.",
                    "label": 0
                },
                {
                    "sent": "So this is the keyword author citation graph that we built from sites here.",
                    "label": 0
                },
                {
                    "sent": "There is a layer of words, papers and authors and the words are connected to the papers.",
                    "label": 0
                },
                {
                    "sent": "If they appeared in the title of the paper, papers are connected to each other via citation and authors are connected to papers.",
                    "label": 0
                },
                {
                    "sent": "If they wrote those papers and the weighing schemes we used were very similar to other authors who have worked on these graphs.",
                    "label": 0
                },
                {
                    "sent": "As well, so existing work use personalized Pagerank to do content based search in this kind of datasets.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is present some quantifiable link prediction tasks and we will compare our measures with personalized page rank.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at 1 task and.",
                    "label": 0
                },
                {
                    "sent": "We pick a paper.",
                    "label": 0
                },
                {
                    "sent": "We pick all the words which appeared in the title of the paper.",
                    "label": 0
                },
                {
                    "sent": "This is a typical link prediction task.",
                    "label": 0
                },
                {
                    "sent": "Remove all the edges between them.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Around the world, and then we're trying to rank all the papers for those words and see if the paper come.",
                    "label": 0
                },
                {
                    "sent": "The original paper comes up in like top 135 ten years neighbors.",
                    "label": 0
                },
                {
                    "sent": "I know that this is very interesting because the success of a search engine really depends on how good the top 10 results are.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and similarly, you can define the task for author nodes where you basically take out links between a paper and its authors.",
                    "label": 0
                },
                {
                    "sent": "And do they?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back team ranking.",
                    "label": 0
                },
                {
                    "sent": "So here is the result here.",
                    "label": 0
                },
                {
                    "sent": "The result, the blue line is the hitting time from an old, the red is PPV.",
                    "label": 0
                },
                {
                    "sent": "This is commute time and on the X axis we have nearest neighbors like 1, three, 510 etc.",
                    "label": 0
                },
                {
                    "sent": "Y axis is accuracy, so higher the better and you can see that in the word task the hitting times do better than PPV and they both beat the commute times by quite a large margin and the hitting times MPP we do.",
                    "label": 0
                },
                {
                    "sent": "Similarly, when a little better, that's because they're very similar measures intrinsic.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let's look at daughter task.",
                    "label": 0
                },
                {
                    "sent": "Here you can see that the commute times basically outperforms everyone else by a huge margin and this is.",
                    "label": 0
                },
                {
                    "sent": "Kind of interesting because you can see that different link prediction tasks probably require different kind of proximity measures.",
                    "label": 0
                },
                {
                    "sent": "And if you can compute the nearest neighbors really really quickly, then you can actually go ahead and adaptively pick the right proximity measure for the right task.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's look at conclusion.",
                    "label": 0
                },
                {
                    "sent": "We presented an algorithm which is kind of incremental on the fly to compute nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "If the graph changes between two queries, then it's still fast on most quantifiable link prediction tasks it outperforms other well known alternatives, and on a single CPU machine we can actually process queries on really really fast on pretty huge graphs like 600,000 nodes and 3 million edges.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I really like to acknowledge all these people for their input in the paper and thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, well crossing over there.",
                    "label": 0
                },
                {
                    "sent": "And can we use the microphone chording purposes?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you if I understood correctly.",
                    "label": 0
                },
                {
                    "sent": "The motivation of your work is that you can compute efficiently with dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "The heating time from a note, but then you have a hard time to compute it to a node.",
                    "label": 0
                },
                {
                    "sent": "Is that correct?",
                    "label": 0
                },
                {
                    "sent": "OK, whatever.",
                    "label": 0
                },
                {
                    "sent": "It seems that in one sense you are using a forward recurrence basically, and why not using a backward recurrence to computing the opposite direction so it doesn't work that way because so when you look at hitting time two inode it's it's kind of the random walk stops at node, right?",
                    "label": 0
                },
                {
                    "sent": "The destination node, right?",
                    "label": 0
                },
                {
                    "sent": "You are essentially essentially solving kind of linear system of like voltages or something like that so that the backward recurrence does not work to compute the hitting time from unknown.",
                    "label": 0
                },
                {
                    "sent": "It's unclear to me.",
                    "label": 0
                },
                {
                    "sent": "I think you can.",
                    "label": 0
                },
                {
                    "sent": "You can have the probability of finishing the work in one step.",
                    "label": 0
                },
                {
                    "sent": "Immediately and then from there you can compute it recursively in finishing the work in two step in three steps.",
                    "label": 0
                },
                {
                    "sent": "Recursively you can.",
                    "label": 0
                },
                {
                    "sent": "You can do that easily when I'm fixing the destination node, so I can kind of look at neighborhood, right?",
                    "label": 0
                },
                {
                    "sent": "That's exactly the dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "Now think about the forward things.",
                    "label": 0
                },
                {
                    "sent": "I want to compute my hitting time to everyone else that could not be doable.",
                    "label": 0
                },
                {
                    "sent": "Then you start from the node and then you compute from where you go in one step and then recursively for me you're going to the recursive thing because the random walks.",
                    "label": 0
                },
                {
                    "sent": "Shops at the destination node, and that's the hitting time 2 from I2J that will keep changing form for all the other nodes that you are looking at that.",
                    "label": 0
                },
                {
                    "sent": "But that's why.",
                    "label": 0
                },
                {
                    "sent": "But if you combine forward and backward recurrences then you have both results.",
                    "label": 0
                },
                {
                    "sent": "You can't do the forward recurrence for the hitting time from a node because of that problem we can.",
                    "label": 0
                },
                {
                    "sent": "Well, I think we'll discuss offline.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Who is just wondering when you say personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "Were you comparing with exact?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we compared everything with exactly so that's why I didn't give any time complexity because I didn't use like the faster ones.",
                    "label": 0
                }
            ]
        }
    }
}