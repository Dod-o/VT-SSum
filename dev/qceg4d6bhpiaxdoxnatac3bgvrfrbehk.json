{
    "id": "qceg4d6bhpiaxdoxnatac3bgvrfrbehk",
    "title": "Slow subspace learning from stationary processes",
    "info": {
        "author": [
            "Andreas Maurer, Stemmer Imaging"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/oh06_maurer_sslsp/",
    "segmentation": [
        [
            "Nicholas.",
            "So we had all these talks and turning the Clock back a little bit to multitask learning.",
            "And we had this wonderful talks by Andreas and Tongue on it and other many others that gave algorithms.",
            "So I'm going to just talk about bounds and do it very quickly and there will be error balance that apply, in particular to the work of tongue and and Andreas and Massey.",
            "So."
        ],
        [
            "The setup is.",
            "Did you have any tasks which are modeled within random variables?",
            "Maybe I can use?",
            "I don't know how to use this and never learn it, so they are just the usual thing that you have an input space and an output space and its model for prediction tasks.",
            "The press is OK. OK, the input spaces.",
            "I think it is a Hilbert space course.",
            "It can be finite dimensional, but I don't need that, so it's just some inner product space complete and I want the vectors to be normalized to be at most one.",
            "So you can think of and like aucion RBF kernel mapping with the Gaussian RBF kernel that would you could kind of satisfy these requirements.",
            "Then for each of the tasks we have a loss function for each of the EM tasks, which tells me the loss incurred by pairing and an output with the prediction, and I require that it is is Lipschitz, one has Lipschitz constant, one in the second argument.",
            "That's going to be.",
            "My prediction is and that is confined to the interval 01.",
            "So for instance, if you want to apply this to classification then you would.",
            "You would use the kind of double hinge loss.",
            "It's not, you wouldn't use it for for optimization, but just using it for the to get the bounds.",
            "And.",
            "Otherwise, the loss functions can be task specific, so you can actually mix regression with classification and it doesn't really affect the results.",
            "And there is the information on the random variables.",
            "The distributions are unknown as usual, so you get it from training sample, where I'm assuming that I have in examples for every task, so for every L like L is going to be I I'm I'm using the superscripts to identify the tasks, and for every task L every pair I have independent identically distributed.",
            "Variables for my examples and now because I'm talking about linear linear multi task learning, my hypothesis would be a collective hypothesis for all the tasks.",
            "So it will for every task specify a linear predictor which is just going to be a vector on the Hilbert space and so specifying a linear predictors is the same as specifying a linear transformation from the input switch space there.",
            "Space two RM.",
            "So I am.",
            "These will be the hypothesis classes that I'm using.",
            "I'm not making any assumptions on relatedness of the task.",
            "If there are assumptions under the relatedness of the task they are in, contained their implicit in the selection of a particular function class.",
            "For instance, I could require that all the transformations in F have rank one.",
            "That just means that really all the classifiers is just one classifier here, so there would be.",
            "I would say they're related up to the point where they are identical.",
            "But I can I can require other forms of relatedness.",
            "And the objective is to minimize."
        ],
        [
            "It's the.",
            "Minimize some error.",
            "So I'm looking for a transformation in my class which has a small task averaged loss.",
            "So this is this is not as good as in Chinese paper.",
            "Where he's considering the worst loss over the tasks that he wants the worst air over the tasks that he wants to bound.",
            "We are taking an average, so this is the loss function.",
            "He replied to the output and variable and the prediction and I'm taking the expected loss and averaging it over all the tasks.",
            "And this is this is the error that I'm measuring.",
            "That's also what I want to bound.",
            "I'm not.",
            "I won't be able to bound anything else that's better than this.",
            "And so one has to work on the basis of some empirical estimate, which is really looks just the same as this, only that I'm replacing the expectation operator here by an empirical expectation which comes from the empirical distribution that is comes from the sample there.",
            "And so."
        ],
        [
            "The basic tool to work with is is a kind of generic error bound that is first was used and introduced for the single class learning, and then I think it was first in tongues paper that it was.",
            "I've seen it.",
            "Is a version of it for multiclass learning, and it says that if I'm selecting a class of linear transformations, then with high probability uniformly in the transformations in my class, I can bound the error the true error.",
            "With the.",
            "And the empirical estimate of the error plus a complexity measure of the function class that I have an empirical complexity measure which I will define on the next slide and some final term which also contains the confidence parameter in the usual logarithmic dependence.",
            "So the product M&N here would suggest that we already have some benefit of multitask learning, but really it is only an average an effect of the fact that we use an average error there, so it doesn't really, there's no.",
            "No multiclass benefit.",
            "Multitask benefit here implied.",
            "And the.",
            "I think the point of view that I'm going to take a set the selecting a function class defines a notion of class relatedness or define some form of class relatedness.",
            "And I want to show that if if the task relatedness here if the tasks are really related in the way that is.",
            "Encoded in the function class that I get better error bounds then I get for single task learning and actually the error bounds have some of the properties that are compatible with the experimental findings that have been announced and so.",
            "Making to make a very crude definition of task related and snow.",
            "I say that if I select a function class if that my my talks are going to be EF related.",
            "If I am able to achieve an empirical error of 0 here.",
            "Of course I could put in some threshold and then have a second parameter in the in the notion of relatedness.",
            "Then it's also one thing is to connect with regularization.",
            "I can, it's not hard to derive another result that says that if I can.",
            "If I find a regularizer J which is a function of my of my transformations and some data dependent function D such that I can bound these empirical complexity.",
            "But I'll still have to define for every for every class, by the supremum, the maximal value of the regularizer on the class times the data dependent factor.",
            "Then I can stratify really over the values of the regularizer, and I have get a second bound with high probability for all transformations and says that for all transformations can again bound the error with the empirical error, and then the regularizer.",
            "Times some factor which is worse because of the stratification.",
            "Regularizer times a data dependent factor plus the some have some, also some logarithmic penalty in the regularizer in the last term.",
            "And so that connects with the regularization and you can see that if I can bound this if I can get bounds of this kind, then I really justifying regularization with this regularizer.",
            "And so the kind of classes that I'm looking at are.",
            "Going to be classes that define some kind of relatedness between these task specific predictors.",
            "OK, here is the."
        ],
        [
            "The definition of the complexity class.",
            "So I think the class and fix a sample and I'm taking the expectation over these random variables there distributed like they're there for every pair of task and and example index for every example I have on their distributed uniformly minus one and one and take dictation of the supremum of the example average.",
            "Of my classification function or my predictor applied to the example multiplied with the Rademacher variable.",
            "This is very complicated and it's just necessary to get the bounds to work that I had on the previous page and in the linear case I can simplify this and rewrite it as a trace where I have my transformation.",
            "The adjoint of my transformation and and transformation valued random variable, which is just the one.",
            "Who is?",
            "It's like the transformation whose health component is given by this.",
            "This vector here, which is random, is dependent on the on these random variables.",
            "So now the idea of proving bounds for linear linear multi task learning that I propose is to look at this expression and realize that it is an inner product between matrices, so I can use Schwarz inequality quality, or more generally hurt US inequality.",
            "The noncommutative version of it to pull apart the two factors inside.",
            "And then you can see that I separate the supremum under one side from the expectation over the random available on the other side, which is doing most of the work really in these pounds, because that's the most complicated things is having the supremum inside there.",
            "So.",
            "The way I do this is."
        ],
        [
            "I'm using.",
            "Holders inequality which I just restate and also I restate these definition of the shuttle norms.",
            "These norms for transformations are the LP norms of the sequences of singular values.",
            "That you are the singular values of the transformation.",
            "So what you do is you multiply a with it's a joint and then you take the root of it.",
            "That's a positive operator from age to itself and then.",
            "And then you just take the eigen values are going to be positive positive sequence and I guess everybody knows that now the holders inequality says that for contradicts ponents just as with the normal filters inequality I can bound this this inner product with the product of the two norms for the two vectors.",
            "It looks like like the normal holders inequality, it's just more difficult to prove and it's a very nice theorem.",
            "It's not in the book by Horn and Johnson for some reason I don't know, it's very classical.",
            "It's in mathematical physics books.",
            "So what I get is to bound or adima how complexity I use use this equality inequality here.",
            "And so, as I said, I can just separate these terms.",
            "So this is the essential idea really.",
            "And then I have this part.",
            "This is just like the regularizer that I had two pages, but it will be there so I can use this as a regularizer and then I still have to pound this more messy term here and I'm not going to go into any detail how this is done.",
            "It's in the court proceedings this year.",
            "And what I get is this theory."
        ],
        [
            "Him.",
            "Says that for any set of linear transformations or multi classes, multi predictor, linear, multi predictor and.",
            "Any P which may be infinite, but it can shouldn't cannot be smaller than 4 four and said very awkward technical problem that I had because maybe I didn't do it in a very good way.",
            "We can also be true.",
            "Then we actually get a bit abound.",
            "And the empirical complexity is bounded by two over root and we always want to root an.",
            "Remember, the N is the number of examples per task and the M is the number of the tasks.",
            "Then we have this.",
            "The maximal, the maximal Q norm of the transformations in my class.",
            "So that I'm going to use this as a regularizer or in an application, one would justify regularization with the Q norm and this way and times a data dependent term which is the P / 2 norm of the empirical covariance operators, which is just.",
            "You feel like 1 / M N times the P / 2 norm of the total grams of the Crimean of all the data.",
            "Plus it term which is going to go to zero as N goes the number of tasks goes to Infinity.",
            "And I haven't been able to get off the root and get rid of the root there, and it may actually be.",
            "Be necessary at least for the case where P is infinite, which is the most interesting case.",
            "So if you look at it in terms of regularization, as you let P go from 4 to Infinity, this is going to go from 2 to Infinity and this is going to go down from 4 thirds to one.",
            "These norms here as the queue queue is decreasing.",
            "These norms will increase an impose ever stricter constraints on your function class up to the point where it's the one norm, and so it will be more difficult to achieve a low error at the same time you get the reward over here because these norms will decrease.",
            "If you look if you have P equal to four, if you have P would be equal to two, then this would be the one norm.",
            "The trace norm.",
            "And since all the input all the inputs have norm less than or equal to 1, this will be less than one, so it will always be less than one.",
            "This part here.",
            "So in the limit in no matter what I do, in the limit, if I'm going to Infinity, this is always going to be be less than one.",
            "If P is equal to four, that's the first interesting case for multi task learning because we please equal to two, it's just really equivalent to normal single task learning.",
            "If he is equal to four, then this is 2 and it's there, but no more.",
            "That would be like the probe Aeneas norm on the Grammy and divided by.",
            "The B1M N normalized, and so it would be the average inner product between 2 between pairs of example example vectors.",
            "So you can see that if, for instance, if you have like a Gaussian LPF RBF kernel, which is very narrow, then then they are likely to be almost orthogonal or very many of them are going to be new orthogonal.",
            "So you get a large benefit from it because this is going to be very very small already.",
            "If P is infinite, then it'll be yet smaller.",
            "It will be the best.",
            "Can achieve for this term so.",
            "This if you if in the case, pardon.",
            "It will be the small as the largest eigenvalue of the Grammy and divided by 1 / M and so it will be very small.",
            "Out of 1 / N N fit will be.",
            "OK, so that will be something like that.",
            "It's a.",
            "It's the data dependent.",
            "You'd have to P is equal to four, then you can.",
            "You can easily compute it without having to do a decomposition of the Grammy and her, and then you can just you just take the all pairs of inputs and take the inner product.",
            "And since the inputs already.",
            "How does it behave?",
            "For example, behave in terms of M&N?",
            "If you have XI for example to normal body will always decrease if it will decrease in P, it'll decrease in P and its interpretation is easiest in the easiest to compute on a sample if P is equal to four or equal to four and you've got gas in car.",
            "Yeah, then you make let it go, narrow an error.",
            "So what would be the form?",
            "I mean, as as you guess as you take the.",
            "With narrow towards what I mean, the limit of that would be, but it would be nice.",
            "Yeah, so the diagonal here would just.",
            "Face.",
            "No, no, no, you have to.",
            "You have the 1 / 1 over him in there.",
            "In diagonal elements, right?",
            "Yeah, but you get the root of it.",
            "OK, so it's the square root of it's 1 / sqrt N yeah so it'll just go go to 0 but it doesn't really.",
            "That doesn't mean that it then you have good generalization because you will get you will get a pound of close to 0 on the complexity, but you will never be able to get the margin for the.",
            "That's this is separate issue with.",
            "The question is just how this behaves this.",
            "Yeah, So what we're saying is it goes to 1 / sqrt M N as the kernel with tends to 0 for the case because, well, there's there's N squared entries in the in the kernel in the matrix, and the only ones that are left are going to be on are going to be.",
            "The ones on the diagonal, so only M N /, M N squared will be and then.",
            "But then I have to globally the root on it here.",
            "So of course that would be optimal, but it doesn't.",
            "It will not be possible to achieve it.",
            "And if it's an the idea, largest eigenvalue will of course be even even smaller than after matrix, because it will be.",
            "I think it will be 1 / 1 / M. Nii have something with a large stacking value in the next next slide, but I think I should be.",
            "I should be finishing pretty soon.",
            "OK, so you see if this is 1 here then this is Infinity and so I'm saying this corresponds to the regularization scheme that.",
            "That Andreas and Massey are using.",
            "Because that is what it is really.",
            "If it just.",
            "So look at.",
            "I don't think it'll work anymore now.",
            "OK, so if I look at the function class of all those transformations.",
            "Brother is some D positive operator D. Say Sadie Maps the space to itself, D is positive, and trays of D is less than or equal to 1, because that was I don't have the slide and if you slide but this impressed it myself and trace of.",
            "No, this is WD star.",
            "AD inverse W star.",
            "Is less than or equal to some B?",
            "This is like the regularizer that be that has stratify over.",
            "So if I have a W in here then I then I'm seeing that W. Person B squared should say then the one norm of W is less than or equal to be, so it's it's the same as using the one norm.",
            "The reason for this is just to take the one norm of W and.",
            "Multiply it on the right with D to the minus 1/2 D to the plus 1/2, and then I use holders inequality.",
            "The Schwartz.",
            "This words version.",
            "Then I get the one norm of WD to the minus one W star.",
            "To the 1/2 this is just B and the other thing is going to be the one norm of D, which is less, which is going to listen to equal to 1 by this condition, so it'll be.",
            "So what you are doing can be at least it can be bounded in terms you can bound the complexity because the one complex success at this function class is contained in the function plus where this is less than or equal to be so.",
            "So it's really you can use the bound for the one regularization with the one norm and then the last thing."
        ],
        [
            "Applies more to A to the projection operators.",
            "If I look at a function class.",
            "Where I'm saying that the average the average squared norms of my predictors has to be less than or equal to some B squared.",
            "This is I'm doing this to make contact with the normal kind of.",
            "Balance and I have hang the additional requirement that the rank of my transformation is less than or equal to some finite small D. It's the same as saying that all the all the particulars have to lie in a D dimensional subspace.",
            "But when I Start learning, I don't know which subspace that is.",
            "So you can say it's the same as selecting the dimensional projection and then learning on the on the projection.",
            "Then you can.",
            "Yeah, this just has to be worked out.",
            "It happens.",
            "It turns out that I can compound the.",
            "This regularizer in this way, so if if I said Q equal to 1 then.",
            "It'll be there at the pier.",
            "Fees.",
            "He's Infinity, exactly know it's 1/2.",
            "This is 1/2.",
            "What's up here?",
            "Peas, infinitives, so this is going to be good to one 1/2, so it will be.",
            "Fruity so if you assume that you have a homogeneous data distribution like we have lots of irrelevant variables.",
            "I'm just thinking of introducing irrelevant variables.",
            "Now many K irrelevant variables and I'm just to make it simpler.",
            "I assume homogeneous data distribution.",
            "Then I can actually explicitly write down the P / 2 norm of the covariance operator, and I can plug everything into my bound.",
            "There is a corresponding bound for the expected complexity.",
            "Then I get this expression here, which I don't even need to talk about because I'll set set P to Infinity.",
            "So this will be 1/2.",
            "This will be 1/2 here and.",
            "If an outlet let Em go to Infinity, I get this bound and this would be what I get from the Bartlett and Mendelssohn bound on single task learning.",
            "Really the bound and this is what I.",
            "And you can look at it as a ratio of utilized information to available total total available information, number of features that you have.",
            "Those are the features, the relevant features that you've extracted, the deal.",
            "If you found the relevant features and this is the total number of features.",
            "So it's if the number of extracted features is much less than the number of total irrelevant features, then the benefit.",
            "Relative to a single task, learning becomes larger and larger and larger.",
            "But it has to say that for an homogeneous data distribution of the case here, you will never be able to find good good margins.",
            "Because of this, A is a parametric kind of concentration, so it'll be.",
            "It'll be terribly, but what it really says is that the multi task learning will decay a little bit more gracefully than the single task learning in this case.",
            "So what what it really shows is that with multi task learning, if they are related you missed.",
            "When I defined the relatedness that you find, it just is having an empirical error of zero with my class and the function class defines whatever relatedness concept there is.",
            "Yeah, it's an empirical relatedness completely.",
            "I'm not making an approaching, it's like just with a bit making a bit and relatedness by selecting a function class.",
            "And then I find out if it's.",
            "If it, if it works or not.",
            "And so that's one finding that same function.",
            "So for each task you can allow a different function in function.",
            "Yeah, yeah, yeah, it's a function.",
            "Classes like a transformation class which contains vectors, predictors for each one.",
            "Have a.",
            "And so that's one finding is that it gets better.",
            "It gets better as the number of tasks gets larger, which is something that is intuitive.",
            "Then it'll it'll approach some.",
            "This is the performance of single task learning and this here is the error here and there is zero.",
            "This is the number of tasks.",
            "Then the multi task learning performance bound goes down.",
            "It goes to a limiting value.",
            "Which is the limiting value.",
            "In this case it's this.",
            "Value here is how small it is relative to single task learning.",
            "Depends on the ratio of irrelevant information to actually utilized information.",
            "Russian yeah.",
            "So that's it I said, well, I'm already a little bit late because of the technical problems.",
            "Fix.",
            "English.",
            "Chips perfectly.",
            "Be used.",
            "Parameters for each task.",
            "Alright.",
            "Close to each other.",
            "Disappeared.",
            "But that was the version with the JV.",
            "And yeah yeah.",
            "So if I just quickly.",
            "Doesn't want to or there is.",
            "Here."
        ],
        [
            "So it would be this one here that version.",
            "So you just minimizing you minimize this.",
            "This like Regularising and you can use the data dependent bound in here where there is 1 between these minimizing an upper bound.",
            "You know, if it's that good, an idea here, but.",
            "It makes some sense and then you.",
            "Of course you always have this term, but I guess the normal procedure is to just forget about it.",
            "The.",
            "Talk about the basic way you have that respond.",
            "Essential ADP smaller than two lines related.",
            "Is that the one thing about that is PQQI flipped the keys to?",
            "Values one Q smaller than let's say one.",
            "It has to have a decade.",
            "So essentially it is inherently load up.",
            "Yeah, it's like the norms combined combined amplitude normalization with dimensional normalization in a way, and the more the smaller the index of the norm the queue becomes, the more relevant dimension reduction is.",
            "Essentially, it's always indicate language.",
            "Use the low rank and truncated, but if you value of number of tasks is small, it may actually better to regularize with with a larger value of Q1.",
            "That's also one thing that shows up in the bound.",
            "I haven't looked at the details.",
            "This complicated to me, but from my understanding or why it should work if you use the trace now and what you actually gain seems to be this one version of settings or lower more sparsity is you just say two.",
            "It really doesn't say anything.",
            "Exactly PC if Q is equal to two, then you don't get anything benefit.",
            "On that, but if you, as long as you put any penolong is only even smaller than 3/4, but anything smaller than two, you again here.",
            "Other than from that decade you had if you had.",
            "Stopping to make my new relates to say hi if I have.",
            "Basically, yeah it should.",
            "Question.",
            "This is that, which is to say, is there something equivalent to that K root K over?",
            "Do you have a case that you get that sort of some effective dimension that you get in these other cases where you don't actually have a strict low dimensional so you know some measure of decay rate?",
            "It would be a.",
            "So you say you've got this square root?",
            "Yeah, OK, 'cause you've got the rank structure in the previous case where you had to say this one with the Q equals."
        ],
        [
            "One, let's say.",
            "Is there something equivalent in the root?",
            "Yeah, it's the limit in.",
            "The same goes to, well, I don't know what it is really.",
            "You're wondering what it means.",
            "Never knew something.",
            "Yeah, if it could be could be interpreted like this.",
            "Dear.",
            "The the largest eigenvalue of the.",
            "If I have the covariance, it maybe it can be interpreted like this.",
            "I don't really know.",
            "I don't even do it, only intuitively I only see it for P equal to four because then it's like the provenience norm and it makes it makes sense.",
            "Other norms beside this treatment?",
            "I actually don't know this person he sent you, invented it.",
            "No, no shut and and for Neumann in the long long time ago when they were studied quantum mechanics and they were studying operator algebras and the.",
            "They're called shutting norms.",
            "This is norms here, and the player played an important role, but I guess they've come out of fashion a little bit.",
            "That's why in the Horn and Johnson book, they are mentioned in the Horn and Johnson book, but the harder inequality isn't isn't given there for some reason.",
            "So what happened?",
            "Undercover, did you know?",
            "Well, it's really you can put think of it abstractly in terms of a Banach space of transformations, and then then up here.",
            "Head.",
            "You just have the norm."
        ],
        [
            "This would be.",
            "This would be the norm dual to the to the other normal said the P norm that you just the Q norm would be the dual norm to it.",
            "Another normal would have, but I don't know any concrete other norms that has this property on operators or matrices.",
            "And I don't know.",
            "There.",
            "Thank you for.",
            "Thank you for."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nicholas.",
                    "label": 0
                },
                {
                    "sent": "So we had all these talks and turning the Clock back a little bit to multitask learning.",
                    "label": 0
                },
                {
                    "sent": "And we had this wonderful talks by Andreas and Tongue on it and other many others that gave algorithms.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to just talk about bounds and do it very quickly and there will be error balance that apply, in particular to the work of tongue and and Andreas and Massey.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The setup is.",
                    "label": 0
                },
                {
                    "sent": "Did you have any tasks which are modeled within random variables?",
                    "label": 0
                },
                {
                    "sent": "Maybe I can use?",
                    "label": 0
                },
                {
                    "sent": "I don't know how to use this and never learn it, so they are just the usual thing that you have an input space and an output space and its model for prediction tasks.",
                    "label": 0
                },
                {
                    "sent": "The press is OK. OK, the input spaces.",
                    "label": 0
                },
                {
                    "sent": "I think it is a Hilbert space course.",
                    "label": 1
                },
                {
                    "sent": "It can be finite dimensional, but I don't need that, so it's just some inner product space complete and I want the vectors to be normalized to be at most one.",
                    "label": 0
                },
                {
                    "sent": "So you can think of and like aucion RBF kernel mapping with the Gaussian RBF kernel that would you could kind of satisfy these requirements.",
                    "label": 0
                },
                {
                    "sent": "Then for each of the tasks we have a loss function for each of the EM tasks, which tells me the loss incurred by pairing and an output with the prediction, and I require that it is is Lipschitz, one has Lipschitz constant, one in the second argument.",
                    "label": 0
                },
                {
                    "sent": "That's going to be.",
                    "label": 0
                },
                {
                    "sent": "My prediction is and that is confined to the interval 01.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you want to apply this to classification then you would.",
                    "label": 0
                },
                {
                    "sent": "You would use the kind of double hinge loss.",
                    "label": 1
                },
                {
                    "sent": "It's not, you wouldn't use it for for optimization, but just using it for the to get the bounds.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, the loss functions can be task specific, so you can actually mix regression with classification and it doesn't really affect the results.",
                    "label": 0
                },
                {
                    "sent": "And there is the information on the random variables.",
                    "label": 1
                },
                {
                    "sent": "The distributions are unknown as usual, so you get it from training sample, where I'm assuming that I have in examples for every task, so for every L like L is going to be I I'm I'm using the superscripts to identify the tasks, and for every task L every pair I have independent identically distributed.",
                    "label": 0
                },
                {
                    "sent": "Variables for my examples and now because I'm talking about linear linear multi task learning, my hypothesis would be a collective hypothesis for all the tasks.",
                    "label": 1
                },
                {
                    "sent": "So it will for every task specify a linear predictor which is just going to be a vector on the Hilbert space and so specifying a linear predictors is the same as specifying a linear transformation from the input switch space there.",
                    "label": 0
                },
                {
                    "sent": "Space two RM.",
                    "label": 0
                },
                {
                    "sent": "So I am.",
                    "label": 0
                },
                {
                    "sent": "These will be the hypothesis classes that I'm using.",
                    "label": 0
                },
                {
                    "sent": "I'm not making any assumptions on relatedness of the task.",
                    "label": 0
                },
                {
                    "sent": "If there are assumptions under the relatedness of the task they are in, contained their implicit in the selection of a particular function class.",
                    "label": 0
                },
                {
                    "sent": "For instance, I could require that all the transformations in F have rank one.",
                    "label": 0
                },
                {
                    "sent": "That just means that really all the classifiers is just one classifier here, so there would be.",
                    "label": 0
                },
                {
                    "sent": "I would say they're related up to the point where they are identical.",
                    "label": 0
                },
                {
                    "sent": "But I can I can require other forms of relatedness.",
                    "label": 0
                },
                {
                    "sent": "And the objective is to minimize.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's the.",
                    "label": 0
                },
                {
                    "sent": "Minimize some error.",
                    "label": 0
                },
                {
                    "sent": "So I'm looking for a transformation in my class which has a small task averaged loss.",
                    "label": 1
                },
                {
                    "sent": "So this is this is not as good as in Chinese paper.",
                    "label": 0
                },
                {
                    "sent": "Where he's considering the worst loss over the tasks that he wants the worst air over the tasks that he wants to bound.",
                    "label": 0
                },
                {
                    "sent": "We are taking an average, so this is the loss function.",
                    "label": 0
                },
                {
                    "sent": "He replied to the output and variable and the prediction and I'm taking the expected loss and averaging it over all the tasks.",
                    "label": 0
                },
                {
                    "sent": "And this is this is the error that I'm measuring.",
                    "label": 0
                },
                {
                    "sent": "That's also what I want to bound.",
                    "label": 0
                },
                {
                    "sent": "I'm not.",
                    "label": 0
                },
                {
                    "sent": "I won't be able to bound anything else that's better than this.",
                    "label": 0
                },
                {
                    "sent": "And so one has to work on the basis of some empirical estimate, which is really looks just the same as this, only that I'm replacing the expectation operator here by an empirical expectation which comes from the empirical distribution that is comes from the sample there.",
                    "label": 1
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic tool to work with is is a kind of generic error bound that is first was used and introduced for the single class learning, and then I think it was first in tongues paper that it was.",
                    "label": 0
                },
                {
                    "sent": "I've seen it.",
                    "label": 0
                },
                {
                    "sent": "Is a version of it for multiclass learning, and it says that if I'm selecting a class of linear transformations, then with high probability uniformly in the transformations in my class, I can bound the error the true error.",
                    "label": 1
                },
                {
                    "sent": "With the.",
                    "label": 0
                },
                {
                    "sent": "And the empirical estimate of the error plus a complexity measure of the function class that I have an empirical complexity measure which I will define on the next slide and some final term which also contains the confidence parameter in the usual logarithmic dependence.",
                    "label": 0
                },
                {
                    "sent": "So the product M&N here would suggest that we already have some benefit of multitask learning, but really it is only an average an effect of the fact that we use an average error there, so it doesn't really, there's no.",
                    "label": 0
                },
                {
                    "sent": "No multiclass benefit.",
                    "label": 0
                },
                {
                    "sent": "Multitask benefit here implied.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "I think the point of view that I'm going to take a set the selecting a function class defines a notion of class relatedness or define some form of class relatedness.",
                    "label": 0
                },
                {
                    "sent": "And I want to show that if if the task relatedness here if the tasks are really related in the way that is.",
                    "label": 0
                },
                {
                    "sent": "Encoded in the function class that I get better error bounds then I get for single task learning and actually the error bounds have some of the properties that are compatible with the experimental findings that have been announced and so.",
                    "label": 0
                },
                {
                    "sent": "Making to make a very crude definition of task related and snow.",
                    "label": 0
                },
                {
                    "sent": "I say that if I select a function class if that my my talks are going to be EF related.",
                    "label": 0
                },
                {
                    "sent": "If I am able to achieve an empirical error of 0 here.",
                    "label": 0
                },
                {
                    "sent": "Of course I could put in some threshold and then have a second parameter in the in the notion of relatedness.",
                    "label": 0
                },
                {
                    "sent": "Then it's also one thing is to connect with regularization.",
                    "label": 0
                },
                {
                    "sent": "I can, it's not hard to derive another result that says that if I can.",
                    "label": 0
                },
                {
                    "sent": "If I find a regularizer J which is a function of my of my transformations and some data dependent function D such that I can bound these empirical complexity.",
                    "label": 0
                },
                {
                    "sent": "But I'll still have to define for every for every class, by the supremum, the maximal value of the regularizer on the class times the data dependent factor.",
                    "label": 0
                },
                {
                    "sent": "Then I can stratify really over the values of the regularizer, and I have get a second bound with high probability for all transformations and says that for all transformations can again bound the error with the empirical error, and then the regularizer.",
                    "label": 0
                },
                {
                    "sent": "Times some factor which is worse because of the stratification.",
                    "label": 0
                },
                {
                    "sent": "Regularizer times a data dependent factor plus the some have some, also some logarithmic penalty in the regularizer in the last term.",
                    "label": 0
                },
                {
                    "sent": "And so that connects with the regularization and you can see that if I can bound this if I can get bounds of this kind, then I really justifying regularization with this regularizer.",
                    "label": 0
                },
                {
                    "sent": "And so the kind of classes that I'm looking at are.",
                    "label": 0
                },
                {
                    "sent": "Going to be classes that define some kind of relatedness between these task specific predictors.",
                    "label": 0
                },
                {
                    "sent": "OK, here is the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The definition of the complexity class.",
                    "label": 0
                },
                {
                    "sent": "So I think the class and fix a sample and I'm taking the expectation over these random variables there distributed like they're there for every pair of task and and example index for every example I have on their distributed uniformly minus one and one and take dictation of the supremum of the example average.",
                    "label": 0
                },
                {
                    "sent": "Of my classification function or my predictor applied to the example multiplied with the Rademacher variable.",
                    "label": 0
                },
                {
                    "sent": "This is very complicated and it's just necessary to get the bounds to work that I had on the previous page and in the linear case I can simplify this and rewrite it as a trace where I have my transformation.",
                    "label": 0
                },
                {
                    "sent": "The adjoint of my transformation and and transformation valued random variable, which is just the one.",
                    "label": 0
                },
                {
                    "sent": "Who is?",
                    "label": 0
                },
                {
                    "sent": "It's like the transformation whose health component is given by this.",
                    "label": 0
                },
                {
                    "sent": "This vector here, which is random, is dependent on the on these random variables.",
                    "label": 0
                },
                {
                    "sent": "So now the idea of proving bounds for linear linear multi task learning that I propose is to look at this expression and realize that it is an inner product between matrices, so I can use Schwarz inequality quality, or more generally hurt US inequality.",
                    "label": 0
                },
                {
                    "sent": "The noncommutative version of it to pull apart the two factors inside.",
                    "label": 0
                },
                {
                    "sent": "And then you can see that I separate the supremum under one side from the expectation over the random available on the other side, which is doing most of the work really in these pounds, because that's the most complicated things is having the supremum inside there.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The way I do this is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm using.",
                    "label": 0
                },
                {
                    "sent": "Holders inequality which I just restate and also I restate these definition of the shuttle norms.",
                    "label": 0
                },
                {
                    "sent": "These norms for transformations are the LP norms of the sequences of singular values.",
                    "label": 0
                },
                {
                    "sent": "That you are the singular values of the transformation.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you multiply a with it's a joint and then you take the root of it.",
                    "label": 0
                },
                {
                    "sent": "That's a positive operator from age to itself and then.",
                    "label": 0
                },
                {
                    "sent": "And then you just take the eigen values are going to be positive positive sequence and I guess everybody knows that now the holders inequality says that for contradicts ponents just as with the normal filters inequality I can bound this this inner product with the product of the two norms for the two vectors.",
                    "label": 0
                },
                {
                    "sent": "It looks like like the normal holders inequality, it's just more difficult to prove and it's a very nice theorem.",
                    "label": 0
                },
                {
                    "sent": "It's not in the book by Horn and Johnson for some reason I don't know, it's very classical.",
                    "label": 0
                },
                {
                    "sent": "It's in mathematical physics books.",
                    "label": 0
                },
                {
                    "sent": "So what I get is to bound or adima how complexity I use use this equality inequality here.",
                    "label": 0
                },
                {
                    "sent": "And so, as I said, I can just separate these terms.",
                    "label": 0
                },
                {
                    "sent": "So this is the essential idea really.",
                    "label": 0
                },
                {
                    "sent": "And then I have this part.",
                    "label": 0
                },
                {
                    "sent": "This is just like the regularizer that I had two pages, but it will be there so I can use this as a regularizer and then I still have to pound this more messy term here and I'm not going to go into any detail how this is done.",
                    "label": 0
                },
                {
                    "sent": "It's in the court proceedings this year.",
                    "label": 0
                },
                {
                    "sent": "And what I get is this theory.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Him.",
                    "label": 0
                },
                {
                    "sent": "Says that for any set of linear transformations or multi classes, multi predictor, linear, multi predictor and.",
                    "label": 1
                },
                {
                    "sent": "Any P which may be infinite, but it can shouldn't cannot be smaller than 4 four and said very awkward technical problem that I had because maybe I didn't do it in a very good way.",
                    "label": 0
                },
                {
                    "sent": "We can also be true.",
                    "label": 0
                },
                {
                    "sent": "Then we actually get a bit abound.",
                    "label": 0
                },
                {
                    "sent": "And the empirical complexity is bounded by two over root and we always want to root an.",
                    "label": 0
                },
                {
                    "sent": "Remember, the N is the number of examples per task and the M is the number of the tasks.",
                    "label": 0
                },
                {
                    "sent": "Then we have this.",
                    "label": 0
                },
                {
                    "sent": "The maximal, the maximal Q norm of the transformations in my class.",
                    "label": 1
                },
                {
                    "sent": "So that I'm going to use this as a regularizer or in an application, one would justify regularization with the Q norm and this way and times a data dependent term which is the P / 2 norm of the empirical covariance operators, which is just.",
                    "label": 0
                },
                {
                    "sent": "You feel like 1 / M N times the P / 2 norm of the total grams of the Crimean of all the data.",
                    "label": 0
                },
                {
                    "sent": "Plus it term which is going to go to zero as N goes the number of tasks goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "And I haven't been able to get off the root and get rid of the root there, and it may actually be.",
                    "label": 0
                },
                {
                    "sent": "Be necessary at least for the case where P is infinite, which is the most interesting case.",
                    "label": 0
                },
                {
                    "sent": "So if you look at it in terms of regularization, as you let P go from 4 to Infinity, this is going to go from 2 to Infinity and this is going to go down from 4 thirds to one.",
                    "label": 0
                },
                {
                    "sent": "These norms here as the queue queue is decreasing.",
                    "label": 0
                },
                {
                    "sent": "These norms will increase an impose ever stricter constraints on your function class up to the point where it's the one norm, and so it will be more difficult to achieve a low error at the same time you get the reward over here because these norms will decrease.",
                    "label": 0
                },
                {
                    "sent": "If you look if you have P equal to four, if you have P would be equal to two, then this would be the one norm.",
                    "label": 0
                },
                {
                    "sent": "The trace norm.",
                    "label": 0
                },
                {
                    "sent": "And since all the input all the inputs have norm less than or equal to 1, this will be less than one, so it will always be less than one.",
                    "label": 0
                },
                {
                    "sent": "This part here.",
                    "label": 0
                },
                {
                    "sent": "So in the limit in no matter what I do, in the limit, if I'm going to Infinity, this is always going to be be less than one.",
                    "label": 0
                },
                {
                    "sent": "If P is equal to four, that's the first interesting case for multi task learning because we please equal to two, it's just really equivalent to normal single task learning.",
                    "label": 0
                },
                {
                    "sent": "If he is equal to four, then this is 2 and it's there, but no more.",
                    "label": 0
                },
                {
                    "sent": "That would be like the probe Aeneas norm on the Grammy and divided by.",
                    "label": 0
                },
                {
                    "sent": "The B1M N normalized, and so it would be the average inner product between 2 between pairs of example example vectors.",
                    "label": 0
                },
                {
                    "sent": "So you can see that if, for instance, if you have like a Gaussian LPF RBF kernel, which is very narrow, then then they are likely to be almost orthogonal or very many of them are going to be new orthogonal.",
                    "label": 0
                },
                {
                    "sent": "So you get a large benefit from it because this is going to be very very small already.",
                    "label": 0
                },
                {
                    "sent": "If P is infinite, then it'll be yet smaller.",
                    "label": 0
                },
                {
                    "sent": "It will be the best.",
                    "label": 0
                },
                {
                    "sent": "Can achieve for this term so.",
                    "label": 0
                },
                {
                    "sent": "This if you if in the case, pardon.",
                    "label": 0
                },
                {
                    "sent": "It will be the small as the largest eigenvalue of the Grammy and divided by 1 / M and so it will be very small.",
                    "label": 0
                },
                {
                    "sent": "Out of 1 / N N fit will be.",
                    "label": 0
                },
                {
                    "sent": "OK, so that will be something like that.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's the data dependent.",
                    "label": 0
                },
                {
                    "sent": "You'd have to P is equal to four, then you can.",
                    "label": 0
                },
                {
                    "sent": "You can easily compute it without having to do a decomposition of the Grammy and her, and then you can just you just take the all pairs of inputs and take the inner product.",
                    "label": 0
                },
                {
                    "sent": "And since the inputs already.",
                    "label": 0
                },
                {
                    "sent": "How does it behave?",
                    "label": 0
                },
                {
                    "sent": "For example, behave in terms of M&N?",
                    "label": 0
                },
                {
                    "sent": "If you have XI for example to normal body will always decrease if it will decrease in P, it'll decrease in P and its interpretation is easiest in the easiest to compute on a sample if P is equal to four or equal to four and you've got gas in car.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then you make let it go, narrow an error.",
                    "label": 0
                },
                {
                    "sent": "So what would be the form?",
                    "label": 0
                },
                {
                    "sent": "I mean, as as you guess as you take the.",
                    "label": 0
                },
                {
                    "sent": "With narrow towards what I mean, the limit of that would be, but it would be nice.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the diagonal here would just.",
                    "label": 0
                },
                {
                    "sent": "Face.",
                    "label": 0
                },
                {
                    "sent": "No, no, no, you have to.",
                    "label": 0
                },
                {
                    "sent": "You have the 1 / 1 over him in there.",
                    "label": 0
                },
                {
                    "sent": "In diagonal elements, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but you get the root of it.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's the square root of it's 1 / sqrt N yeah so it'll just go go to 0 but it doesn't really.",
                    "label": 0
                },
                {
                    "sent": "That doesn't mean that it then you have good generalization because you will get you will get a pound of close to 0 on the complexity, but you will never be able to get the margin for the.",
                    "label": 0
                },
                {
                    "sent": "That's this is separate issue with.",
                    "label": 0
                },
                {
                    "sent": "The question is just how this behaves this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what we're saying is it goes to 1 / sqrt M N as the kernel with tends to 0 for the case because, well, there's there's N squared entries in the in the kernel in the matrix, and the only ones that are left are going to be on are going to be.",
                    "label": 0
                },
                {
                    "sent": "The ones on the diagonal, so only M N /, M N squared will be and then.",
                    "label": 0
                },
                {
                    "sent": "But then I have to globally the root on it here.",
                    "label": 0
                },
                {
                    "sent": "So of course that would be optimal, but it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It will not be possible to achieve it.",
                    "label": 0
                },
                {
                    "sent": "And if it's an the idea, largest eigenvalue will of course be even even smaller than after matrix, because it will be.",
                    "label": 0
                },
                {
                    "sent": "I think it will be 1 / 1 / M. Nii have something with a large stacking value in the next next slide, but I think I should be.",
                    "label": 0
                },
                {
                    "sent": "I should be finishing pretty soon.",
                    "label": 0
                },
                {
                    "sent": "OK, so you see if this is 1 here then this is Infinity and so I'm saying this corresponds to the regularization scheme that.",
                    "label": 0
                },
                {
                    "sent": "That Andreas and Massey are using.",
                    "label": 0
                },
                {
                    "sent": "Because that is what it is really.",
                    "label": 0
                },
                {
                    "sent": "If it just.",
                    "label": 0
                },
                {
                    "sent": "So look at.",
                    "label": 0
                },
                {
                    "sent": "I don't think it'll work anymore now.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I look at the function class of all those transformations.",
                    "label": 0
                },
                {
                    "sent": "Brother is some D positive operator D. Say Sadie Maps the space to itself, D is positive, and trays of D is less than or equal to 1, because that was I don't have the slide and if you slide but this impressed it myself and trace of.",
                    "label": 0
                },
                {
                    "sent": "No, this is WD star.",
                    "label": 0
                },
                {
                    "sent": "AD inverse W star.",
                    "label": 0
                },
                {
                    "sent": "Is less than or equal to some B?",
                    "label": 0
                },
                {
                    "sent": "This is like the regularizer that be that has stratify over.",
                    "label": 0
                },
                {
                    "sent": "So if I have a W in here then I then I'm seeing that W. Person B squared should say then the one norm of W is less than or equal to be, so it's it's the same as using the one norm.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is just to take the one norm of W and.",
                    "label": 0
                },
                {
                    "sent": "Multiply it on the right with D to the minus 1/2 D to the plus 1/2, and then I use holders inequality.",
                    "label": 0
                },
                {
                    "sent": "The Schwartz.",
                    "label": 0
                },
                {
                    "sent": "This words version.",
                    "label": 0
                },
                {
                    "sent": "Then I get the one norm of WD to the minus one W star.",
                    "label": 0
                },
                {
                    "sent": "To the 1/2 this is just B and the other thing is going to be the one norm of D, which is less, which is going to listen to equal to 1 by this condition, so it'll be.",
                    "label": 0
                },
                {
                    "sent": "So what you are doing can be at least it can be bounded in terms you can bound the complexity because the one complex success at this function class is contained in the function plus where this is less than or equal to be so.",
                    "label": 0
                },
                {
                    "sent": "So it's really you can use the bound for the one regularization with the one norm and then the last thing.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applies more to A to the projection operators.",
                    "label": 0
                },
                {
                    "sent": "If I look at a function class.",
                    "label": 0
                },
                {
                    "sent": "Where I'm saying that the average the average squared norms of my predictors has to be less than or equal to some B squared.",
                    "label": 0
                },
                {
                    "sent": "This is I'm doing this to make contact with the normal kind of.",
                    "label": 0
                },
                {
                    "sent": "Balance and I have hang the additional requirement that the rank of my transformation is less than or equal to some finite small D. It's the same as saying that all the all the particulars have to lie in a D dimensional subspace.",
                    "label": 0
                },
                {
                    "sent": "But when I Start learning, I don't know which subspace that is.",
                    "label": 0
                },
                {
                    "sent": "So you can say it's the same as selecting the dimensional projection and then learning on the on the projection.",
                    "label": 0
                },
                {
                    "sent": "Then you can.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this just has to be worked out.",
                    "label": 0
                },
                {
                    "sent": "It happens.",
                    "label": 0
                },
                {
                    "sent": "It turns out that I can compound the.",
                    "label": 0
                },
                {
                    "sent": "This regularizer in this way, so if if I said Q equal to 1 then.",
                    "label": 0
                },
                {
                    "sent": "It'll be there at the pier.",
                    "label": 0
                },
                {
                    "sent": "Fees.",
                    "label": 0
                },
                {
                    "sent": "He's Infinity, exactly know it's 1/2.",
                    "label": 0
                },
                {
                    "sent": "This is 1/2.",
                    "label": 0
                },
                {
                    "sent": "What's up here?",
                    "label": 0
                },
                {
                    "sent": "Peas, infinitives, so this is going to be good to one 1/2, so it will be.",
                    "label": 0
                },
                {
                    "sent": "Fruity so if you assume that you have a homogeneous data distribution like we have lots of irrelevant variables.",
                    "label": 0
                },
                {
                    "sent": "I'm just thinking of introducing irrelevant variables.",
                    "label": 0
                },
                {
                    "sent": "Now many K irrelevant variables and I'm just to make it simpler.",
                    "label": 0
                },
                {
                    "sent": "I assume homogeneous data distribution.",
                    "label": 0
                },
                {
                    "sent": "Then I can actually explicitly write down the P / 2 norm of the covariance operator, and I can plug everything into my bound.",
                    "label": 0
                },
                {
                    "sent": "There is a corresponding bound for the expected complexity.",
                    "label": 0
                },
                {
                    "sent": "Then I get this expression here, which I don't even need to talk about because I'll set set P to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So this will be 1/2.",
                    "label": 0
                },
                {
                    "sent": "This will be 1/2 here and.",
                    "label": 0
                },
                {
                    "sent": "If an outlet let Em go to Infinity, I get this bound and this would be what I get from the Bartlett and Mendelssohn bound on single task learning.",
                    "label": 0
                },
                {
                    "sent": "Really the bound and this is what I.",
                    "label": 0
                },
                {
                    "sent": "And you can look at it as a ratio of utilized information to available total total available information, number of features that you have.",
                    "label": 0
                },
                {
                    "sent": "Those are the features, the relevant features that you've extracted, the deal.",
                    "label": 0
                },
                {
                    "sent": "If you found the relevant features and this is the total number of features.",
                    "label": 0
                },
                {
                    "sent": "So it's if the number of extracted features is much less than the number of total irrelevant features, then the benefit.",
                    "label": 0
                },
                {
                    "sent": "Relative to a single task, learning becomes larger and larger and larger.",
                    "label": 0
                },
                {
                    "sent": "But it has to say that for an homogeneous data distribution of the case here, you will never be able to find good good margins.",
                    "label": 0
                },
                {
                    "sent": "Because of this, A is a parametric kind of concentration, so it'll be.",
                    "label": 0
                },
                {
                    "sent": "It'll be terribly, but what it really says is that the multi task learning will decay a little bit more gracefully than the single task learning in this case.",
                    "label": 0
                },
                {
                    "sent": "So what what it really shows is that with multi task learning, if they are related you missed.",
                    "label": 0
                },
                {
                    "sent": "When I defined the relatedness that you find, it just is having an empirical error of zero with my class and the function class defines whatever relatedness concept there is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's an empirical relatedness completely.",
                    "label": 0
                },
                {
                    "sent": "I'm not making an approaching, it's like just with a bit making a bit and relatedness by selecting a function class.",
                    "label": 0
                },
                {
                    "sent": "And then I find out if it's.",
                    "label": 0
                },
                {
                    "sent": "If it, if it works or not.",
                    "label": 0
                },
                {
                    "sent": "And so that's one finding that same function.",
                    "label": 0
                },
                {
                    "sent": "So for each task you can allow a different function in function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah, it's a function.",
                    "label": 0
                },
                {
                    "sent": "Classes like a transformation class which contains vectors, predictors for each one.",
                    "label": 0
                },
                {
                    "sent": "Have a.",
                    "label": 0
                },
                {
                    "sent": "And so that's one finding is that it gets better.",
                    "label": 0
                },
                {
                    "sent": "It gets better as the number of tasks gets larger, which is something that is intuitive.",
                    "label": 0
                },
                {
                    "sent": "Then it'll it'll approach some.",
                    "label": 0
                },
                {
                    "sent": "This is the performance of single task learning and this here is the error here and there is zero.",
                    "label": 0
                },
                {
                    "sent": "This is the number of tasks.",
                    "label": 0
                },
                {
                    "sent": "Then the multi task learning performance bound goes down.",
                    "label": 0
                },
                {
                    "sent": "It goes to a limiting value.",
                    "label": 0
                },
                {
                    "sent": "Which is the limiting value.",
                    "label": 0
                },
                {
                    "sent": "In this case it's this.",
                    "label": 0
                },
                {
                    "sent": "Value here is how small it is relative to single task learning.",
                    "label": 0
                },
                {
                    "sent": "Depends on the ratio of irrelevant information to actually utilized information.",
                    "label": 0
                },
                {
                    "sent": "Russian yeah.",
                    "label": 0
                },
                {
                    "sent": "So that's it I said, well, I'm already a little bit late because of the technical problems.",
                    "label": 0
                },
                {
                    "sent": "Fix.",
                    "label": 0
                },
                {
                    "sent": "English.",
                    "label": 0
                },
                {
                    "sent": "Chips perfectly.",
                    "label": 0
                },
                {
                    "sent": "Be used.",
                    "label": 0
                },
                {
                    "sent": "Parameters for each task.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Close to each other.",
                    "label": 0
                },
                {
                    "sent": "Disappeared.",
                    "label": 0
                },
                {
                    "sent": "But that was the version with the JV.",
                    "label": 0
                },
                {
                    "sent": "And yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So if I just quickly.",
                    "label": 0
                },
                {
                    "sent": "Doesn't want to or there is.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it would be this one here that version.",
                    "label": 0
                },
                {
                    "sent": "So you just minimizing you minimize this.",
                    "label": 0
                },
                {
                    "sent": "This like Regularising and you can use the data dependent bound in here where there is 1 between these minimizing an upper bound.",
                    "label": 0
                },
                {
                    "sent": "You know, if it's that good, an idea here, but.",
                    "label": 0
                },
                {
                    "sent": "It makes some sense and then you.",
                    "label": 0
                },
                {
                    "sent": "Of course you always have this term, but I guess the normal procedure is to just forget about it.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Talk about the basic way you have that respond.",
                    "label": 0
                },
                {
                    "sent": "Essential ADP smaller than two lines related.",
                    "label": 0
                },
                {
                    "sent": "Is that the one thing about that is PQQI flipped the keys to?",
                    "label": 0
                },
                {
                    "sent": "Values one Q smaller than let's say one.",
                    "label": 0
                },
                {
                    "sent": "It has to have a decade.",
                    "label": 0
                },
                {
                    "sent": "So essentially it is inherently load up.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's like the norms combined combined amplitude normalization with dimensional normalization in a way, and the more the smaller the index of the norm the queue becomes, the more relevant dimension reduction is.",
                    "label": 0
                },
                {
                    "sent": "Essentially, it's always indicate language.",
                    "label": 0
                },
                {
                    "sent": "Use the low rank and truncated, but if you value of number of tasks is small, it may actually better to regularize with with a larger value of Q1.",
                    "label": 0
                },
                {
                    "sent": "That's also one thing that shows up in the bound.",
                    "label": 0
                },
                {
                    "sent": "I haven't looked at the details.",
                    "label": 0
                },
                {
                    "sent": "This complicated to me, but from my understanding or why it should work if you use the trace now and what you actually gain seems to be this one version of settings or lower more sparsity is you just say two.",
                    "label": 0
                },
                {
                    "sent": "It really doesn't say anything.",
                    "label": 0
                },
                {
                    "sent": "Exactly PC if Q is equal to two, then you don't get anything benefit.",
                    "label": 0
                },
                {
                    "sent": "On that, but if you, as long as you put any penolong is only even smaller than 3/4, but anything smaller than two, you again here.",
                    "label": 0
                },
                {
                    "sent": "Other than from that decade you had if you had.",
                    "label": 0
                },
                {
                    "sent": "Stopping to make my new relates to say hi if I have.",
                    "label": 0
                },
                {
                    "sent": "Basically, yeah it should.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "This is that, which is to say, is there something equivalent to that K root K over?",
                    "label": 0
                },
                {
                    "sent": "Do you have a case that you get that sort of some effective dimension that you get in these other cases where you don't actually have a strict low dimensional so you know some measure of decay rate?",
                    "label": 0
                },
                {
                    "sent": "It would be a.",
                    "label": 0
                },
                {
                    "sent": "So you say you've got this square root?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, 'cause you've got the rank structure in the previous case where you had to say this one with the Q equals.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One, let's say.",
                    "label": 0
                },
                {
                    "sent": "Is there something equivalent in the root?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's the limit in.",
                    "label": 0
                },
                {
                    "sent": "The same goes to, well, I don't know what it is really.",
                    "label": 0
                },
                {
                    "sent": "You're wondering what it means.",
                    "label": 0
                },
                {
                    "sent": "Never knew something.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if it could be could be interpreted like this.",
                    "label": 0
                },
                {
                    "sent": "Dear.",
                    "label": 0
                },
                {
                    "sent": "The the largest eigenvalue of the.",
                    "label": 0
                },
                {
                    "sent": "If I have the covariance, it maybe it can be interpreted like this.",
                    "label": 0
                },
                {
                    "sent": "I don't really know.",
                    "label": 0
                },
                {
                    "sent": "I don't even do it, only intuitively I only see it for P equal to four because then it's like the provenience norm and it makes it makes sense.",
                    "label": 0
                },
                {
                    "sent": "Other norms beside this treatment?",
                    "label": 0
                },
                {
                    "sent": "I actually don't know this person he sent you, invented it.",
                    "label": 0
                },
                {
                    "sent": "No, no shut and and for Neumann in the long long time ago when they were studied quantum mechanics and they were studying operator algebras and the.",
                    "label": 0
                },
                {
                    "sent": "They're called shutting norms.",
                    "label": 0
                },
                {
                    "sent": "This is norms here, and the player played an important role, but I guess they've come out of fashion a little bit.",
                    "label": 0
                },
                {
                    "sent": "That's why in the Horn and Johnson book, they are mentioned in the Horn and Johnson book, but the harder inequality isn't isn't given there for some reason.",
                    "label": 0
                },
                {
                    "sent": "So what happened?",
                    "label": 0
                },
                {
                    "sent": "Undercover, did you know?",
                    "label": 0
                },
                {
                    "sent": "Well, it's really you can put think of it abstractly in terms of a Banach space of transformations, and then then up here.",
                    "label": 0
                },
                {
                    "sent": "Head.",
                    "label": 0
                },
                {
                    "sent": "You just have the norm.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This would be.",
                    "label": 0
                },
                {
                    "sent": "This would be the norm dual to the to the other normal said the P norm that you just the Q norm would be the dual norm to it.",
                    "label": 0
                },
                {
                    "sent": "Another normal would have, but I don't know any concrete other norms that has this property on operators or matrices.",
                    "label": 0
                },
                {
                    "sent": "And I don't know.",
                    "label": 0
                },
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "Thank you for.",
                    "label": 0
                },
                {
                    "sent": "Thank you for.",
                    "label": 0
                }
            ]
        }
    }
}