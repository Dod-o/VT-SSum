{
    "id": "56duchehvrblq3jshv4jwnyeti5q7tvn",
    "title": "Learning Models for Object Recognition from Natural Language Descriptions",
    "info": {
        "author": [
            "Josiah Wang, University of Leeds"
        ],
        "published": "Dec. 1, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Natural Language Processing",
            "Top->Computer Science->Information Extraction"
        ]
    },
    "url": "http://videolectures.net/bmvc09_wang_lmor/",
    "segmentation": [
        [
            "Good morning everyone.",
            "My name is Josiah Wong and my talk today will be about the work I'm doing is."
        ],
        [
            "Part of my PhD, so the main idea behind my work is that given text description describing how an object category looks like, can we learn a model for this object history without any training images?",
            "So this is in contrast with conventional approaches."
        ],
        [
            "Which require many training images for each and every category and current methods deal with 10s to hundreds of object categories and you'll be hard to scale to say thousands of subject categories, especially if you need many trim training images for each and every category.",
            "And there are two recent work in CPR which deals with training with no images, and the similarities between our work and ask is that object categories are defined in terms of name attributes.",
            "For example, we can say that a ship can be described as being wide and has Roulin has some lakes and so that's the similarity to work, and this is quite interesting as you can actually learn something from this attributes.",
            "And the difference is that attributes and values are defined by hand for the categories an in their work for Hardy ET al.",
            "Actually manually defined this lampitt at L. Obtain this from some research from cognitive science, where the researchers there are human subjects too.",
            "Specified associations between attributes."
        ],
        [
            "In the categories an.",
            "In our case what we do is we aim to learn this directly from texture descriptions.",
            "Now I'll text descriptions defined appearance of an object category and for certain object categories such as butterflies, Flowers or sign languages or judo moves, we can actually obtain such information and such description.",
            "For example from nature guides.",
            "In case of butterflies and Flowers, and here we have a description of the butterfly which says that is orange and has black veins and has black margins and sprinkled by white dots, so such information is quite useful.",
            "In identifying the object category.",
            "An no."
        ],
        [
            "The main challenge in our work is to map between texture features and image features.",
            "For example, we will need to know what the word red means in tax and how it relates to image level features.",
            "And we also need to extract some useful information from texture descriptions, so some form passing will be needed.",
            "Some are descriptions are shot in that they do not actually mention important attributes of the text.",
            "The object category, sorry.",
            "And some are properties that I mentioned.",
            "Attacks are actually not visible in images."
        ],
        [
            "Now in our work we deal with 10 butterfly categories as an example, and note that we do not use any training images.",
            "Our training set is a set of texture descriptions we should obtain from online nature guide called nature.com.",
            "And I'll test that is an image data set which we obtained by querying Google images with the Latin name of the butterfly and manually filtering the images.",
            "Our method consist."
        ],
        [
            "Of trimming components.",
            "We have an IP component where we build object models from category.",
            "From texture descriptions we have a visual processing component where we extract visual attributes images and we have finally agenda remodel which we learn from this hex and it relates between texture attributes and."
        ],
        [
            "Image features will first take a look at the energy component or work.",
            "Where here what we?"
        ],
        [
            "Do is to extract name attributes from descriptions and to fill them into our representation, which is known as template, and this template is designed by us and we are particularly interested in colors and patterns.",
            "And we are also interested in their locations, for example, whether they are located at the following or at hiring, and whether they are the margins.",
            "Also, butterflies can look different when you view from the top or from bottom, so in this case we indicated with above and below and note that in our work we actually limit ourselves only above as the images in our data set is actually from this viewpoint.",
            "Now you use this."
        ],
        [
            "Example description to illustrate our NLP process which is made up of four."
        ],
        [
            "Stages the first stage is tokenization, where we divide the text into a sequence of."
        ],
        [
            "What tokens?",
            "The second step is where we assign a part of speech tag to each of these tokens.",
            "For example nouns or adjectives.",
            "And we do this using general purpose tagger."
        ],
        [
            "An next we identify certain kind of phrases from text, for example, adjective phrases which correspond to colors, noun phrases which could potentially be some patterns."
        ],
        [
            "Finally, we perform template filling.",
            "An will use orange example and as you can see in the same sentence we can find the term above an, as the text doesn't mention anything about followings or hind wings, so we assume that means both and we assign orange to two of these slots.",
            "And once we have a template field, we can then use this model to.",
            "Use it As for classification."
        ],
        [
            "So now we look at the visual processing component.",
            "Here."
        ],
        [
            "We are interested in extracting two kind of attributes, dominant color of the butterfly and the colored spots.",
            "Our vision presenting component is made up of several stages.",
            "First, we segment the butterfly to reduce the effect of background clutter.",
            "We then model the color of the butterfly as well as detect the spots on the butterflies."
        ],
        [
            "An firstly segmentation, here we use the method introduced by Vexilla where which is a graph cut method constraint with star shape as shown in this image.",
            "And as you can see it approximates the shape of the butterfly.",
            "So our process is semi automated where user will select a sense of the butterfly and optionally some foreground and background points."
        ],
        [
            "We also need to learn a model of the color name.",
            "For example, orange.",
            "In this case what we do is to learn a person, then state model for each of these color name from example pixels which we mainly select from images of butterflies.",
            "And note that we do not use any class specific information about butterfly.",
            "And here's our examples of orange and red, which we have selected and their distribution in a lab color space."
        ],
        [
            "Now we also detect some spots of the butterflies, so the first step is to detect some candidate spots and we use we do this using a difference of Gaussian detector and as this stage produces many false positives.",
            "What we do is to introduce the second stage where we represent the candidate spots as safe descriptors, and we use a linear classifier to classify this as either sports or non sports.",
            "And we represent the color spots with the Gaussian weighted average of the pixels in the region."
        ],
        [
            "Now we look at a generative model which we learn from templates and this relates between texture features, an image features."
        ],
        [
            "And to specify generative model, we first need to convert templates into some sort of prior distribution.",
            "In the case of spot colors, what we do is to look at the template.",
            "And find spots that are defined template and then assign equal probability to each."
        ],
        [
            "The same goes for wing color.",
            "We take colors in template and model dominant color.",
            "We also model other."
        ],
        [
            "Colors such as patterns and again assign equal probability."
        ],
        [
            "And we model wing color as a mixture of these two components.",
            "The parameter Alpha controls how much of the image which we expect is to be explained by dominant color, and we expect this to be at least 50% and rather than specifying this arbitrarily, what we do is to define a hyper prior over its value using a beta distribution and marginalized."
        ],
        [
            "Now we model butterflies using a general model.",
            "So what we do is to estimate how likely a butterfly category explains a set of images.",
            "An we look at two kinds of components, spots and win colors.",
            "In the case of spots, what we do is to take the LB picks value of each color each spot.",
            "Sorry, and we compute how likely it is to be red, orange, yellow, etc.",
            "And we compare this bunch of spot observations with a set of spot color name priors which you obtain from templates earlier.",
            "And we measure how well this prior explains this set of spots observation.",
            "The same goes for win color.",
            "We take each pixel compute is likelihood over color terms and compare this with win color, name priors and we perform classification by assigning the image to a butterfly category which maximizes the likelihood.",
            "Now."
        ],
        [
            "We performed two experiments.",
            "Firstly is to evaluate how well humans do in learning from textual descriptions.",
            "We then evaluate our proposed method, which also learns from only tech."
        ],
        [
            "The descriptions.",
            "Firstly, the human experiment.",
            "Here we have a screenshot of a webpage we set up, and it displays a random description of this butterfly.",
            "Any butterfly from our 10.",
            "And also random butterfly image for each of our categories is also displayed.",
            "Participants are asked to select the butterfly image which dating best matches the description?",
            "And they are limited to one single trial as we do not want them to learn from images only from text."
        ],
        [
            "Now here's the results of Human experiment chance.",
            "Performance is 10% / 10 categories.",
            "Native speakers achieve an accuracy of 72% non native English speakers 51%.",
            "And notice how this butterfly achieve low accuracy across both classes.",
            "I mean both native and non native speakers.",
            "And here's the example of our problematic butterfly.",
            "As you can see."
        ],
        [
            "The description is quite brief.",
            "Ann this roller spots actually clearly seen an image is not mentioned attacks.",
            "Also the lemon yellow bands described attacks is actually very white in a lot of images.",
            "To further confusion, we have another butterfly which also has lemon yellowish bands of spots.",
            "Right so."
        ],
        [
            "Now we look at the results of our proposed method, which achieve accuracy of about 54% and this is interesting Lee.",
            "Compara bulto performance of non native speaker Ann.",
            "Also notice that in the case of attributes we can see that they are actually complementary in that when combining them both we achieve better performance than using each in isolation."
        ],
        [
            "Now here's the confusion.",
            "Matrix of our method, and as you can see, four categories achieve an accuracy of more than 80%.",
            "And in fact, two of them achieve accuracy of more than 90%.",
            "Also note out."
        ],
        [
            "0 accuracy of our favorite butterfly again, and in this case because the template doesn't mention anything about the spots.",
            "So in this case, if you detect some yellowish spots, you'll be assigned this butterfly an if it's the spot looks more white, will go to this butterfly.",
            "Now these are some new."
        ],
        [
            "Experiments we should have done, which is not mentioned in paper.",
            "Here we compared our method using with two standard image based approaches.",
            "There's spatial color histograms and bag of words using self.",
            "And in both cases we use the segmentation mask, which we."
        ],
        [
            "Say earlier.",
            "And here's the results of our experiments.",
            "We ran our experiments by varying the training number of training images.",
            "And we also ran 50 trials for each of the experiments to gain a best estimate.",
            "Here in blue is the result of our method without training images.",
            "Here in red is the results of the color histogram classifier.",
            "And as you can see our method is comperable to about five training images per category.",
            "And here we have the SIFT descriptor, an bag of words combination which is always amazing, an it achieve better performance with even one training image category.",
            "But of course if you notice the standard deviation is quite large.",
            "So this means that it depends a lot on the training images that is actually use.",
            "So if we have a very bad example image.",
            "The performance would be much lower.",
            "So."
        ],
        [
            "We investigate gated models for linking information in text and images together and we have a very challenging problem of mapping between such information, especially from the ambiguity which arises from how humans describe something and what they actually see.",
            "So, and this is very hard, especially if we want to do this with limited supervision.",
            "And we propose an initial model with."
        ],
        [
            "Achieve a modest accuracy without any training images.",
            "Although state of vision methods do give better results, but they depend on training images actually use, so to further improve our performance, we have some future work which includes extracting more information for tags and using more attributes.",
            "Other than this spots and colors.",
            "And it will also interesting to combine information from more than one description, which we can obtain from more than one source.",
            "And finally, we believe that information that we learn by combining both texts and images we can actually have a better performance than training using images, load and as a final conclusion, this approach of linking between texts and images is actually quite interesting and shows a lot of promise.",
            "And with that I am my presentation.",
            "Thank you.",
            "OK, I've actually tried training colors using generic image by querying Google Image with just red color, but it doesn't retrieve that much good performances.",
            "She's very hard actually, because red could be anything from very saturated plastic red.",
            "So in the case of butterflies is not there.",
            "So I mean it might be a bit biased, but I think it's quite reasonable to just limit so butterflies, since we're actually doing butterfly and we just want the specific butterfly categories.",
            "So.",
            "Are you talking about the butterfly model from template itself?",
            "Yeah, OK, yeah, that's because it is our initial stage.",
            "So in this case we only have one description, so it's hard to actually give some weight to it.",
            "So if we hope that maybe by learning from field."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is Josiah Wong and my talk today will be about the work I'm doing is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Part of my PhD, so the main idea behind my work is that given text description describing how an object category looks like, can we learn a model for this object history without any training images?",
                    "label": 0
                },
                {
                    "sent": "So this is in contrast with conventional approaches.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which require many training images for each and every category and current methods deal with 10s to hundreds of object categories and you'll be hard to scale to say thousands of subject categories, especially if you need many trim training images for each and every category.",
                    "label": 1
                },
                {
                    "sent": "And there are two recent work in CPR which deals with training with no images, and the similarities between our work and ask is that object categories are defined in terms of name attributes.",
                    "label": 0
                },
                {
                    "sent": "For example, we can say that a ship can be described as being wide and has Roulin has some lakes and so that's the similarity to work, and this is quite interesting as you can actually learn something from this attributes.",
                    "label": 0
                },
                {
                    "sent": "And the difference is that attributes and values are defined by hand for the categories an in their work for Hardy ET al.",
                    "label": 1
                },
                {
                    "sent": "Actually manually defined this lampitt at L. Obtain this from some research from cognitive science, where the researchers there are human subjects too.",
                    "label": 0
                },
                {
                    "sent": "Specified associations between attributes.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the categories an.",
                    "label": 0
                },
                {
                    "sent": "In our case what we do is we aim to learn this directly from texture descriptions.",
                    "label": 0
                },
                {
                    "sent": "Now I'll text descriptions defined appearance of an object category and for certain object categories such as butterflies, Flowers or sign languages or judo moves, we can actually obtain such information and such description.",
                    "label": 1
                },
                {
                    "sent": "For example from nature guides.",
                    "label": 1
                },
                {
                    "sent": "In case of butterflies and Flowers, and here we have a description of the butterfly which says that is orange and has black veins and has black margins and sprinkled by white dots, so such information is quite useful.",
                    "label": 0
                },
                {
                    "sent": "In identifying the object category.",
                    "label": 0
                },
                {
                    "sent": "An no.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main challenge in our work is to map between texture features and image features.",
                    "label": 0
                },
                {
                    "sent": "For example, we will need to know what the word red means in tax and how it relates to image level features.",
                    "label": 0
                },
                {
                    "sent": "And we also need to extract some useful information from texture descriptions, so some form passing will be needed.",
                    "label": 0
                },
                {
                    "sent": "Some are descriptions are shot in that they do not actually mention important attributes of the text.",
                    "label": 0
                },
                {
                    "sent": "The object category, sorry.",
                    "label": 0
                },
                {
                    "sent": "And some are properties that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "Attacks are actually not visible in images.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now in our work we deal with 10 butterfly categories as an example, and note that we do not use any training images.",
                    "label": 1
                },
                {
                    "sent": "Our training set is a set of texture descriptions we should obtain from online nature guide called nature.com.",
                    "label": 1
                },
                {
                    "sent": "And I'll test that is an image data set which we obtained by querying Google images with the Latin name of the butterfly and manually filtering the images.",
                    "label": 0
                },
                {
                    "sent": "Our method consist.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of trimming components.",
                    "label": 0
                },
                {
                    "sent": "We have an IP component where we build object models from category.",
                    "label": 0
                },
                {
                    "sent": "From texture descriptions we have a visual processing component where we extract visual attributes images and we have finally agenda remodel which we learn from this hex and it relates between texture attributes and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image features will first take a look at the energy component or work.",
                    "label": 0
                },
                {
                    "sent": "Where here what we?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do is to extract name attributes from descriptions and to fill them into our representation, which is known as template, and this template is designed by us and we are particularly interested in colors and patterns.",
                    "label": 0
                },
                {
                    "sent": "And we are also interested in their locations, for example, whether they are located at the following or at hiring, and whether they are the margins.",
                    "label": 0
                },
                {
                    "sent": "Also, butterflies can look different when you view from the top or from bottom, so in this case we indicated with above and below and note that in our work we actually limit ourselves only above as the images in our data set is actually from this viewpoint.",
                    "label": 0
                },
                {
                    "sent": "Now you use this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example description to illustrate our NLP process which is made up of four.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stages the first stage is tokenization, where we divide the text into a sequence of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What tokens?",
                    "label": 0
                },
                {
                    "sent": "The second step is where we assign a part of speech tag to each of these tokens.",
                    "label": 0
                },
                {
                    "sent": "For example nouns or adjectives.",
                    "label": 0
                },
                {
                    "sent": "And we do this using general purpose tagger.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An next we identify certain kind of phrases from text, for example, adjective phrases which correspond to colors, noun phrases which could potentially be some patterns.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we perform template filling.",
                    "label": 0
                },
                {
                    "sent": "An will use orange example and as you can see in the same sentence we can find the term above an, as the text doesn't mention anything about followings or hind wings, so we assume that means both and we assign orange to two of these slots.",
                    "label": 0
                },
                {
                    "sent": "And once we have a template field, we can then use this model to.",
                    "label": 0
                },
                {
                    "sent": "Use it As for classification.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we look at the visual processing component.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are interested in extracting two kind of attributes, dominant color of the butterfly and the colored spots.",
                    "label": 0
                },
                {
                    "sent": "Our vision presenting component is made up of several stages.",
                    "label": 0
                },
                {
                    "sent": "First, we segment the butterfly to reduce the effect of background clutter.",
                    "label": 0
                },
                {
                    "sent": "We then model the color of the butterfly as well as detect the spots on the butterflies.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An firstly segmentation, here we use the method introduced by Vexilla where which is a graph cut method constraint with star shape as shown in this image.",
                    "label": 1
                },
                {
                    "sent": "And as you can see it approximates the shape of the butterfly.",
                    "label": 1
                },
                {
                    "sent": "So our process is semi automated where user will select a sense of the butterfly and optionally some foreground and background points.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also need to learn a model of the color name.",
                    "label": 0
                },
                {
                    "sent": "For example, orange.",
                    "label": 0
                },
                {
                    "sent": "In this case what we do is to learn a person, then state model for each of these color name from example pixels which we mainly select from images of butterflies.",
                    "label": 0
                },
                {
                    "sent": "And note that we do not use any class specific information about butterfly.",
                    "label": 0
                },
                {
                    "sent": "And here's our examples of orange and red, which we have selected and their distribution in a lab color space.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we also detect some spots of the butterflies, so the first step is to detect some candidate spots and we use we do this using a difference of Gaussian detector and as this stage produces many false positives.",
                    "label": 0
                },
                {
                    "sent": "What we do is to introduce the second stage where we represent the candidate spots as safe descriptors, and we use a linear classifier to classify this as either sports or non sports.",
                    "label": 0
                },
                {
                    "sent": "And we represent the color spots with the Gaussian weighted average of the pixels in the region.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we look at a generative model which we learn from templates and this relates between texture features, an image features.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to specify generative model, we first need to convert templates into some sort of prior distribution.",
                    "label": 0
                },
                {
                    "sent": "In the case of spot colors, what we do is to look at the template.",
                    "label": 0
                },
                {
                    "sent": "And find spots that are defined template and then assign equal probability to each.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same goes for wing color.",
                    "label": 0
                },
                {
                    "sent": "We take colors in template and model dominant color.",
                    "label": 0
                },
                {
                    "sent": "We also model other.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Colors such as patterns and again assign equal probability.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we model wing color as a mixture of these two components.",
                    "label": 0
                },
                {
                    "sent": "The parameter Alpha controls how much of the image which we expect is to be explained by dominant color, and we expect this to be at least 50% and rather than specifying this arbitrarily, what we do is to define a hyper prior over its value using a beta distribution and marginalized.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we model butterflies using a general model.",
                    "label": 0
                },
                {
                    "sent": "So what we do is to estimate how likely a butterfly category explains a set of images.",
                    "label": 0
                },
                {
                    "sent": "An we look at two kinds of components, spots and win colors.",
                    "label": 0
                },
                {
                    "sent": "In the case of spots, what we do is to take the LB picks value of each color each spot.",
                    "label": 0
                },
                {
                    "sent": "Sorry, and we compute how likely it is to be red, orange, yellow, etc.",
                    "label": 0
                },
                {
                    "sent": "And we compare this bunch of spot observations with a set of spot color name priors which you obtain from templates earlier.",
                    "label": 0
                },
                {
                    "sent": "And we measure how well this prior explains this set of spots observation.",
                    "label": 0
                },
                {
                    "sent": "The same goes for win color.",
                    "label": 0
                },
                {
                    "sent": "We take each pixel compute is likelihood over color terms and compare this with win color, name priors and we perform classification by assigning the image to a butterfly category which maximizes the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We performed two experiments.",
                    "label": 0
                },
                {
                    "sent": "Firstly is to evaluate how well humans do in learning from textual descriptions.",
                    "label": 1
                },
                {
                    "sent": "We then evaluate our proposed method, which also learns from only tech.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The descriptions.",
                    "label": 0
                },
                {
                    "sent": "Firstly, the human experiment.",
                    "label": 0
                },
                {
                    "sent": "Here we have a screenshot of a webpage we set up, and it displays a random description of this butterfly.",
                    "label": 0
                },
                {
                    "sent": "Any butterfly from our 10.",
                    "label": 0
                },
                {
                    "sent": "And also random butterfly image for each of our categories is also displayed.",
                    "label": 0
                },
                {
                    "sent": "Participants are asked to select the butterfly image which dating best matches the description?",
                    "label": 0
                },
                {
                    "sent": "And they are limited to one single trial as we do not want them to learn from images only from text.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now here's the results of Human experiment chance.",
                    "label": 0
                },
                {
                    "sent": "Performance is 10% / 10 categories.",
                    "label": 0
                },
                {
                    "sent": "Native speakers achieve an accuracy of 72% non native English speakers 51%.",
                    "label": 0
                },
                {
                    "sent": "And notice how this butterfly achieve low accuracy across both classes.",
                    "label": 0
                },
                {
                    "sent": "I mean both native and non native speakers.",
                    "label": 0
                },
                {
                    "sent": "And here's the example of our problematic butterfly.",
                    "label": 0
                },
                {
                    "sent": "As you can see.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The description is quite brief.",
                    "label": 0
                },
                {
                    "sent": "Ann this roller spots actually clearly seen an image is not mentioned attacks.",
                    "label": 0
                },
                {
                    "sent": "Also the lemon yellow bands described attacks is actually very white in a lot of images.",
                    "label": 0
                },
                {
                    "sent": "To further confusion, we have another butterfly which also has lemon yellowish bands of spots.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we look at the results of our proposed method, which achieve accuracy of about 54% and this is interesting Lee.",
                    "label": 1
                },
                {
                    "sent": "Compara bulto performance of non native speaker Ann.",
                    "label": 0
                },
                {
                    "sent": "Also notice that in the case of attributes we can see that they are actually complementary in that when combining them both we achieve better performance than using each in isolation.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now here's the confusion.",
                    "label": 0
                },
                {
                    "sent": "Matrix of our method, and as you can see, four categories achieve an accuracy of more than 80%.",
                    "label": 0
                },
                {
                    "sent": "And in fact, two of them achieve accuracy of more than 90%.",
                    "label": 0
                },
                {
                    "sent": "Also note out.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "0 accuracy of our favorite butterfly again, and in this case because the template doesn't mention anything about the spots.",
                    "label": 0
                },
                {
                    "sent": "So in this case, if you detect some yellowish spots, you'll be assigned this butterfly an if it's the spot looks more white, will go to this butterfly.",
                    "label": 0
                },
                {
                    "sent": "Now these are some new.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiments we should have done, which is not mentioned in paper.",
                    "label": 0
                },
                {
                    "sent": "Here we compared our method using with two standard image based approaches.",
                    "label": 1
                },
                {
                    "sent": "There's spatial color histograms and bag of words using self.",
                    "label": 1
                },
                {
                    "sent": "And in both cases we use the segmentation mask, which we.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say earlier.",
                    "label": 0
                },
                {
                    "sent": "And here's the results of our experiments.",
                    "label": 0
                },
                {
                    "sent": "We ran our experiments by varying the training number of training images.",
                    "label": 1
                },
                {
                    "sent": "And we also ran 50 trials for each of the experiments to gain a best estimate.",
                    "label": 0
                },
                {
                    "sent": "Here in blue is the result of our method without training images.",
                    "label": 0
                },
                {
                    "sent": "Here in red is the results of the color histogram classifier.",
                    "label": 1
                },
                {
                    "sent": "And as you can see our method is comperable to about five training images per category.",
                    "label": 1
                },
                {
                    "sent": "And here we have the SIFT descriptor, an bag of words combination which is always amazing, an it achieve better performance with even one training image category.",
                    "label": 0
                },
                {
                    "sent": "But of course if you notice the standard deviation is quite large.",
                    "label": 0
                },
                {
                    "sent": "So this means that it depends a lot on the training images that is actually use.",
                    "label": 0
                },
                {
                    "sent": "So if we have a very bad example image.",
                    "label": 0
                },
                {
                    "sent": "The performance would be much lower.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We investigate gated models for linking information in text and images together and we have a very challenging problem of mapping between such information, especially from the ambiguity which arises from how humans describe something and what they actually see.",
                    "label": 1
                },
                {
                    "sent": "So, and this is very hard, especially if we want to do this with limited supervision.",
                    "label": 1
                },
                {
                    "sent": "And we propose an initial model with.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Achieve a modest accuracy without any training images.",
                    "label": 0
                },
                {
                    "sent": "Although state of vision methods do give better results, but they depend on training images actually use, so to further improve our performance, we have some future work which includes extracting more information for tags and using more attributes.",
                    "label": 0
                },
                {
                    "sent": "Other than this spots and colors.",
                    "label": 0
                },
                {
                    "sent": "And it will also interesting to combine information from more than one description, which we can obtain from more than one source.",
                    "label": 0
                },
                {
                    "sent": "And finally, we believe that information that we learn by combining both texts and images we can actually have a better performance than training using images, load and as a final conclusion, this approach of linking between texts and images is actually quite interesting and shows a lot of promise.",
                    "label": 0
                },
                {
                    "sent": "And with that I am my presentation.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, I've actually tried training colors using generic image by querying Google Image with just red color, but it doesn't retrieve that much good performances.",
                    "label": 0
                },
                {
                    "sent": "She's very hard actually, because red could be anything from very saturated plastic red.",
                    "label": 0
                },
                {
                    "sent": "So in the case of butterflies is not there.",
                    "label": 0
                },
                {
                    "sent": "So I mean it might be a bit biased, but I think it's quite reasonable to just limit so butterflies, since we're actually doing butterfly and we just want the specific butterfly categories.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Are you talking about the butterfly model from template itself?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, yeah, that's because it is our initial stage.",
                    "label": 0
                },
                {
                    "sent": "So in this case we only have one description, so it's hard to actually give some weight to it.",
                    "label": 0
                },
                {
                    "sent": "So if we hope that maybe by learning from field.",
                    "label": 0
                }
            ]
        }
    }
}