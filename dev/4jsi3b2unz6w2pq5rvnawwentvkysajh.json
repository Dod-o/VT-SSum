{
    "id": "4jsi3b2unz6w2pq5rvnawwentvkysajh",
    "title": "Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera",
    "info": {
        "author": [
            "Hanme Kim, Department of Computing, Imperial College London"
        ],
        "published": "Oct. 24, 2016",
        "recorded": "October 2016",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2016_kim_event_camera/",
    "segmentation": [
        [
            "Hi, I'm Jamie came from Imperial College London.",
            "Anne I'm going to present real time 3D reconstruction and 6th of tracking with an event camera.",
            "This is our joint work with Doctor Steven Leutenegger, an professor Andrew Davison.",
            "Then what?"
        ],
        [
            "About the event.",
            "Based camera, in contrast to the standard camera that produce a sequence of images frames as shown in the upper graph while it is looking at this kind of very simple scene or spinning disk with black dot.",
            "The event camera works in a totally different way.",
            "Basically it's independent pixel report changes by monitoring it's low intensity value continuously and produce only only produce on output code an event if there is a change which is greater or less than a predefined event threshold.",
            "As a result, the output from the camera is a sequence of events plotted in the lower graph, and each event consists of pixel location telling us where the change was observed, and polychaete indicating whether that's a positive or negative change and a precise timing stamp telling us when exactly the change was observed.",
            "So as we know, the standard camera has very low frame rate between 30 to 60, which is you know for many computer vision applications.",
            "But for example, if you want to track really fast motion, it could be an issue because we're completely blind between two consecutive frames and it could occur motion blur or rolling shutter effect as well.",
            "But the event, provide very high temporal resolution.",
            "That's why we can see very fine detailed of movement of the black dot in the lower graph, and it doesn't have a motion blur effect."
        ],
        [
            "An also, if there is no change in the same meaning that there is no new information while the standard camera keep producing the redundant data that consume power and processing time to event, provide nothing which gave us high potential to consume, much less less power.",
            "In addition to that, it's independent.",
            "Pixel architecture provides very high dynamic range as well, so I hope you now see that we have a lot of benefit.",
            "We get from this special camera, but if we can use it for our problem."
        ],
        [
            "And we are, especially we are interested in SLAM problem, simultaneous localization and mapping.",
            "And we know how to do it with the standard camera by using, for example, feature detection and matching or whole image alignment.",
            "But the problem is we have truly different type output from the camera now or sequence of events, not images.",
            "So we can't directly apply our well defined computer vision algorithm to this output.",
            "Of course we can make us some kind of artificial event frames by accumulating event within some time interval as shown here, but this is the way to reduce a lot of advantages of this camera.",
            "So we in our work we always process event by event.",
            "And use this special virtual frames for visualization only."
        ],
        [
            "So far this has only been successful with restricted motion and two D reconstruction, such as our previous work in 2014.",
            "Which was a simplified slam problem.",
            "We create a map of 3D unknown unstructured scene in a form of panoramic mosaic and we track pure rotating motion and the main idea was realizing on event is related to a dot product of a motion vector with a special gradient vector.",
            "Meaning that if we rotate the event camera like this, we will get a motion vector M from our tracker.",
            "Then we will get a lot of events from where its gradient is not perpendicular to this motion vector.",
            "While we will not get any event where its gradient is perpendicular to that, this is an essential mechanism built into our previous work.",
            "And."
        ],
        [
            "Essentially, we managed to make it real time learning Windows standard laptop or desktop.",
            "As we can see I was holding the camera locating while it is looking at the scene and attract camera rotation is represented by this flying blue box.",
            "While we are reconstructing panoramic mosaic of the scene in real time and we managed to track very fast motion like a shaking hand motion as well."
        ],
        [
            "So in this is to be paper we wanted to move two or the 3D slam so same unknown unstructured 3D scene.",
            "But now we want to create 3D map instead of planar map and we also want to track 6 of 300 motion instead of pure rotating motion.",
            "And the main idea is if we have the same object at a different distance from the camera.",
            "Even though we have the same gradient from those two objects.",
            "The depth difference should tell us something in a form of event rate, meaning that if it translates event camera looking at the scene.",
            "We should have much higher number of events from the object which is close to the camera than the other one, and this is the main idea built into our current work."
        ],
        [
            "So here is the overview of our method, purely based on a stream of events to Beijing.",
            "Setup is 100 camera.",
            "Looking at the scene while we are getting a sequence of events, we process event by event and overall architecture is dividing, tracking and mapping following the current trend in SLAM research and it consists of 3D coupled probabilist filter, each estimating six of compose.",
            "Special gradient of the thing an seen in both depth and we also have the parallel intensity reconstruction block as well.",
            "So before I scribe each component in detail, let me."
        ],
        [
            "Produce a special virtual frame concept car key frame concept we use in our work so as the event camera doesn't have a frame, we create a patrol.",
            "Projective reference frame consists of gradient intensity and depth.",
            "All three estimate components an all three public filters are estimated against to this keyframe and we also can create 3D point cloud from it.",
            "So let's move to the."
        ],
        [
            "Last component, pose estimation.",
            "Here we are showing the keyframe and our 3D model.",
            "And we assume that when we do estimate, pose, we assume that other two current estimates from the other two components are correct in both depth and intensity reconstruction.",
            "And once we have."
        ],
        [
            "New event here.",
            "The true, post could be here or."
        ],
        [
            "Here or here."
        ],
        [
            "Here."
        ],
        [
            "And as we have attracted, trajectory, we know the previous pose when we had a previous event at the same event pixel.",
            "And the design principle of the event camera tells us that the intense."
        ],
        [
            "The difference between those two 3 point should be equal to the camera event camera threshold.",
            "See otherwise there should be an event happened.",
            "So we use it as a measurement for our icaf extended common filter to update our camera pose estimate.",
            "An next component is great."
        ],
        [
            "Estimation, again, we assume that other two components are correct inverse depth and compose.",
            "So once we have a new event, we project 23D point onto our key frame and calculate displacement vector divided by the time interval between two events.",
            "We get motion vector M and we want to update its gradient vector G by using the dot product relationship so that product times timing tomorrow should be equal to the event threshold C."
        ],
        [
            "And then this is a low intensity construction block which is learning in parallel.",
            "So once we have have the estimated gradient shown on the left left.",
            "We want to create on image whose gradient is a similar to the estimated one.",
            "By minimizing this cost function.",
            "And one result is shown on the right."
        ],
        [
            "The last component is the inbox steps as mission.",
            "Again, we assume that other two components are correct, so we know the intensity reconstruction, pose.",
            "So once we have a new event, the true depth could be here or here or here, but the event camera tells us the intensity difference between two points should be equal to the event threshold to see.",
            "So we use it as a measurement for our pixelwise escape based inverse depth estimation."
        ],
        [
            "So let me show you some results.",
            "Our method works in little time on a standard laptop or desktop.",
            "An as you're saying that I'm holding the camera looking at the same damn way being very special movement for depth estimation.",
            "And then the truck, poses represented by this flying RGB axis and beyond that there is a sentence, three 3D point cloud we create from the keyframes and below there is a set of four images starting from left.",
            "First one is a visualization of input events and color coded gradient estimation, intensity reconstruction and the depth keyframe.",
            "And."
        ],
        [
            "We evaluated our method in a different conditions, so here we are showing eight different datasets including indoors and outdoors, and we show that our method works quite well, meaning that we can estimate gradient intensity an estimating a depth."
        ],
        [
            "Also, if the event, has moved little bit far from the current keyframe, we create a new keyframe and propagate all current estimate onto the new keyframe and by keep doing this we can extend the same we can cover using our method so.",
            "As you can see here, kind of a desktop size, since oh all those small axes represent all the keyframe poses that has been created created in this experiment.",
            "And last week."
        ],
        [
            "Using our method, we can actually turn the event, low resolution event based camera into high resolution and high frame rate artificial camera by rendering video using raycasting technique as shown on the right.",
            "So."
        ],
        [
            "So here is the conclusion we propose the 1st six of tracking and 3D reconstruction method purely based on our stream of event with no additional sensing and it learns in real time on a standard laptop or desktop and we hope that this opens up to practical solution to the current limitations of SLAM enabled product which rely on commercial image sensor and below we would actually emphasize that using our method with the event camera.",
            "We can deal with fast motion tracking without motion blur and we can cope with a high time range is seen as well compared to the standard camera.",
            "Thank."
        ],
        [
            "For your attention an please visit our post session.",
            "If you have a question, thanks.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi, I'm Jamie came from Imperial College London.",
                    "label": 0
                },
                {
                    "sent": "Anne I'm going to present real time 3D reconstruction and 6th of tracking with an event camera.",
                    "label": 1
                },
                {
                    "sent": "This is our joint work with Doctor Steven Leutenegger, an professor Andrew Davison.",
                    "label": 0
                },
                {
                    "sent": "Then what?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About the event.",
                    "label": 0
                },
                {
                    "sent": "Based camera, in contrast to the standard camera that produce a sequence of images frames as shown in the upper graph while it is looking at this kind of very simple scene or spinning disk with black dot.",
                    "label": 0
                },
                {
                    "sent": "The event camera works in a totally different way.",
                    "label": 1
                },
                {
                    "sent": "Basically it's independent pixel report changes by monitoring it's low intensity value continuously and produce only only produce on output code an event if there is a change which is greater or less than a predefined event threshold.",
                    "label": 0
                },
                {
                    "sent": "As a result, the output from the camera is a sequence of events plotted in the lower graph, and each event consists of pixel location telling us where the change was observed, and polychaete indicating whether that's a positive or negative change and a precise timing stamp telling us when exactly the change was observed.",
                    "label": 0
                },
                {
                    "sent": "So as we know, the standard camera has very low frame rate between 30 to 60, which is you know for many computer vision applications.",
                    "label": 1
                },
                {
                    "sent": "But for example, if you want to track really fast motion, it could be an issue because we're completely blind between two consecutive frames and it could occur motion blur or rolling shutter effect as well.",
                    "label": 1
                },
                {
                    "sent": "But the event, provide very high temporal resolution.",
                    "label": 0
                },
                {
                    "sent": "That's why we can see very fine detailed of movement of the black dot in the lower graph, and it doesn't have a motion blur effect.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An also, if there is no change in the same meaning that there is no new information while the standard camera keep producing the redundant data that consume power and processing time to event, provide nothing which gave us high potential to consume, much less less power.",
                    "label": 1
                },
                {
                    "sent": "In addition to that, it's independent.",
                    "label": 0
                },
                {
                    "sent": "Pixel architecture provides very high dynamic range as well, so I hope you now see that we have a lot of benefit.",
                    "label": 1
                },
                {
                    "sent": "We get from this special camera, but if we can use it for our problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we are, especially we are interested in SLAM problem, simultaneous localization and mapping.",
                    "label": 0
                },
                {
                    "sent": "And we know how to do it with the standard camera by using, for example, feature detection and matching or whole image alignment.",
                    "label": 1
                },
                {
                    "sent": "But the problem is we have truly different type output from the camera now or sequence of events, not images.",
                    "label": 1
                },
                {
                    "sent": "So we can't directly apply our well defined computer vision algorithm to this output.",
                    "label": 0
                },
                {
                    "sent": "Of course we can make us some kind of artificial event frames by accumulating event within some time interval as shown here, but this is the way to reduce a lot of advantages of this camera.",
                    "label": 0
                },
                {
                    "sent": "So we in our work we always process event by event.",
                    "label": 0
                },
                {
                    "sent": "And use this special virtual frames for visualization only.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far this has only been successful with restricted motion and two D reconstruction, such as our previous work in 2014.",
                    "label": 1
                },
                {
                    "sent": "Which was a simplified slam problem.",
                    "label": 0
                },
                {
                    "sent": "We create a map of 3D unknown unstructured scene in a form of panoramic mosaic and we track pure rotating motion and the main idea was realizing on event is related to a dot product of a motion vector with a special gradient vector.",
                    "label": 0
                },
                {
                    "sent": "Meaning that if we rotate the event camera like this, we will get a motion vector M from our tracker.",
                    "label": 0
                },
                {
                    "sent": "Then we will get a lot of events from where its gradient is not perpendicular to this motion vector.",
                    "label": 0
                },
                {
                    "sent": "While we will not get any event where its gradient is perpendicular to that, this is an essential mechanism built into our previous work.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Essentially, we managed to make it real time learning Windows standard laptop or desktop.",
                    "label": 0
                },
                {
                    "sent": "As we can see I was holding the camera locating while it is looking at the scene and attract camera rotation is represented by this flying blue box.",
                    "label": 0
                },
                {
                    "sent": "While we are reconstructing panoramic mosaic of the scene in real time and we managed to track very fast motion like a shaking hand motion as well.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this is to be paper we wanted to move two or the 3D slam so same unknown unstructured 3D scene.",
                    "label": 1
                },
                {
                    "sent": "But now we want to create 3D map instead of planar map and we also want to track 6 of 300 motion instead of pure rotating motion.",
                    "label": 0
                },
                {
                    "sent": "And the main idea is if we have the same object at a different distance from the camera.",
                    "label": 0
                },
                {
                    "sent": "Even though we have the same gradient from those two objects.",
                    "label": 0
                },
                {
                    "sent": "The depth difference should tell us something in a form of event rate, meaning that if it translates event camera looking at the scene.",
                    "label": 0
                },
                {
                    "sent": "We should have much higher number of events from the object which is close to the camera than the other one, and this is the main idea built into our current work.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the overview of our method, purely based on a stream of events to Beijing.",
                    "label": 1
                },
                {
                    "sent": "Setup is 100 camera.",
                    "label": 0
                },
                {
                    "sent": "Looking at the scene while we are getting a sequence of events, we process event by event and overall architecture is dividing, tracking and mapping following the current trend in SLAM research and it consists of 3D coupled probabilist filter, each estimating six of compose.",
                    "label": 0
                },
                {
                    "sent": "Special gradient of the thing an seen in both depth and we also have the parallel intensity reconstruction block as well.",
                    "label": 0
                },
                {
                    "sent": "So before I scribe each component in detail, let me.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Produce a special virtual frame concept car key frame concept we use in our work so as the event camera doesn't have a frame, we create a patrol.",
                    "label": 0
                },
                {
                    "sent": "Projective reference frame consists of gradient intensity and depth.",
                    "label": 1
                },
                {
                    "sent": "All three estimate components an all three public filters are estimated against to this keyframe and we also can create 3D point cloud from it.",
                    "label": 0
                },
                {
                    "sent": "So let's move to the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last component, pose estimation.",
                    "label": 0
                },
                {
                    "sent": "Here we are showing the keyframe and our 3D model.",
                    "label": 0
                },
                {
                    "sent": "And we assume that when we do estimate, pose, we assume that other two current estimates from the other two components are correct in both depth and intensity reconstruction.",
                    "label": 0
                },
                {
                    "sent": "And once we have.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "New event here.",
                    "label": 0
                },
                {
                    "sent": "The true, post could be here or.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here or here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as we have attracted, trajectory, we know the previous pose when we had a previous event at the same event pixel.",
                    "label": 0
                },
                {
                    "sent": "And the design principle of the event camera tells us that the intense.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The difference between those two 3 point should be equal to the camera event camera threshold.",
                    "label": 1
                },
                {
                    "sent": "See otherwise there should be an event happened.",
                    "label": 1
                },
                {
                    "sent": "So we use it as a measurement for our icaf extended common filter to update our camera pose estimate.",
                    "label": 1
                },
                {
                    "sent": "An next component is great.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimation, again, we assume that other two components are correct inverse depth and compose.",
                    "label": 0
                },
                {
                    "sent": "So once we have a new event, we project 23D point onto our key frame and calculate displacement vector divided by the time interval between two events.",
                    "label": 0
                },
                {
                    "sent": "We get motion vector M and we want to update its gradient vector G by using the dot product relationship so that product times timing tomorrow should be equal to the event threshold C.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this is a low intensity construction block which is learning in parallel.",
                    "label": 0
                },
                {
                    "sent": "So once we have have the estimated gradient shown on the left left.",
                    "label": 0
                },
                {
                    "sent": "We want to create on image whose gradient is a similar to the estimated one.",
                    "label": 0
                },
                {
                    "sent": "By minimizing this cost function.",
                    "label": 0
                },
                {
                    "sent": "And one result is shown on the right.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last component is the inbox steps as mission.",
                    "label": 0
                },
                {
                    "sent": "Again, we assume that other two components are correct, so we know the intensity reconstruction, pose.",
                    "label": 0
                },
                {
                    "sent": "So once we have a new event, the true depth could be here or here or here, but the event camera tells us the intensity difference between two points should be equal to the event threshold to see.",
                    "label": 0
                },
                {
                    "sent": "So we use it as a measurement for our pixelwise escape based inverse depth estimation.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me show you some results.",
                    "label": 0
                },
                {
                    "sent": "Our method works in little time on a standard laptop or desktop.",
                    "label": 0
                },
                {
                    "sent": "An as you're saying that I'm holding the camera looking at the same damn way being very special movement for depth estimation.",
                    "label": 1
                },
                {
                    "sent": "And then the truck, poses represented by this flying RGB axis and beyond that there is a sentence, three 3D point cloud we create from the keyframes and below there is a set of four images starting from left.",
                    "label": 0
                },
                {
                    "sent": "First one is a visualization of input events and color coded gradient estimation, intensity reconstruction and the depth keyframe.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We evaluated our method in a different conditions, so here we are showing eight different datasets including indoors and outdoors, and we show that our method works quite well, meaning that we can estimate gradient intensity an estimating a depth.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, if the event, has moved little bit far from the current keyframe, we create a new keyframe and propagate all current estimate onto the new keyframe and by keep doing this we can extend the same we can cover using our method so.",
                    "label": 0
                },
                {
                    "sent": "As you can see here, kind of a desktop size, since oh all those small axes represent all the keyframe poses that has been created created in this experiment.",
                    "label": 0
                },
                {
                    "sent": "And last week.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using our method, we can actually turn the event, low resolution event based camera into high resolution and high frame rate artificial camera by rendering video using raycasting technique as shown on the right.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the conclusion we propose the 1st six of tracking and 3D reconstruction method purely based on our stream of event with no additional sensing and it learns in real time on a standard laptop or desktop and we hope that this opens up to practical solution to the current limitations of SLAM enabled product which rely on commercial image sensor and below we would actually emphasize that using our method with the event camera.",
                    "label": 0
                },
                {
                    "sent": "We can deal with fast motion tracking without motion blur and we can cope with a high time range is seen as well compared to the standard camera.",
                    "label": 0
                },
                {
                    "sent": "Thank.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For your attention an please visit our post session.",
                    "label": 0
                },
                {
                    "sent": "If you have a question, thanks.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}