{
    "id": "bdlwd4bojiyai7jlxjngazmpmhtjy2xp",
    "title": "Optimization II",
    "info": {
        "author": [
            "Jorge Nocedal, Robert R. McCormick School of Engineering and Applied Science, Northwestern University"
        ],
        "published": "Oct. 11, 2018",
        "recorded": "July 2018",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/DLRLsummerschool2018_nocedal_optimization2/",
    "segmentation": [
        [
            "Thank you Roger, and thanks very much for the invitation.",
            "Coming here is a pleasure to be with all the young people I remember being at a workshop in 2012 and I was telling Roger, you know, some of the people were at that board trip.",
            "I think they are not pretty prominent and he said I was there.",
            "So fun.",
            "I've learned a lot from you in this workshop and I hope I bring a little bit of a different perspective from the point of view of optimization.",
            "I think something really interesting happened today.",
            "You are having three lectures, one by someone who's coming from the theoretical computer science theoretical machine learning community, and it's talking about optimization and machine learning.",
            "Then you have.",
            "Jimmy Ball was really in the middle of the machine learning community, talking about optimization.",
            "And now I'm coming as someone would looks at optimization in a very broadview and trying to assist.",
            "Still some ideas that we have from this bigger experience and how it could be relevant here."
        ],
        [
            "Alright, so first of all, thanks to some of my students who I learn from and they read a lot of papers for me.",
            "Um?"
        ],
        [
            "So there are different perspectives to optimization.",
            "One of them is in the beginning, there is the SGD method.",
            "This is where everything starts, right?",
            "If you do machine learning, you start from there.",
            "And why is it that everything starts with the SGD method?",
            "Because of this graph that I have right there everybody has seen that you run a batch method, run SGD even are logistic regression problem and the SVD went by large margin.",
            "Right, so everything should spectrum is GD the other perspective is no, no every discussion should start from Newton's method.",
            "Becausw if you have an objective function like this and most objective functions look like that, why would you like to go along the gradient direction, which is not very intelligent.",
            "You want to do something a little bit better.",
            "In fact, as Jamie was pointing out, the steepest descent method is going to sag, and.",
            "You often hear about complexity, results about optimization algorithms.",
            "Sometimes these complexity results never happened.",
            "The worst case never happens, so the results are overly pessimistic.",
            "In this case, the worst case result is the average result.",
            "This will always happen.",
            "Grandmother always do like that, so therefore there's an argument of saying every discussion should start with Newton Smith."
        ],
        [
            "Alright, so just to give you a little bit of a history and I know this lecture is being recorded, so some of my colleagues in the Russian School and may be reacting to what I'm going to say here, right?",
            "But there was a time when the world was really divided.",
            "There was not communication between the two blocks.",
            "And the Russian school focused on convexity on 1st order methods.",
            "An gradient projection method.",
            "That's what they were doing, and they were doing fantastic theoretical work there.",
            "Not everything was first order method in theoretical they were playing with ideas that eventually led to interior point methods.",
            "Dick in.",
            "In the beginning Cassian and then at Berkeley.",
            "The final results were done, but just the Western School.",
            "In contrast, during the 60s and 70s was focusing on 2nd derivative methods.",
            "Why do gradient methods?",
            "Convergence rates against complexity.",
            "If I have a nonconvex problem which is what happens here in machine learning, usually get a reasonable starting point and what you want to know is how fast are you going to get to the solution, not what is the worst number of iterations that may happen if you start anywhere.",
            "So I was focusing on rates of convergence, non convexity from the beginning, a lot of the problems were coming from chemical engineering power systems, there were non convex.",
            "You have to do something about that.",
            "You just deal with this and the other aspect that happened in the Western School was not popular in Russia.",
            "Is that the ultimate goal of whatever you're doing?",
            "Should be some software and public domain software in there, everybody could contribute to the ideas.",
            "Now this is a bit of an oversimplification.",
            "In the West they were great theoreticians like Rockefeller doing awesome with organization and so on.",
            "But still what I'm talking about here has relevance today because in my opinion there is a big disbalance that has happened.",
            "In nonlinear optimization community, there's a lot of theoretical work about how to deal with non convexity that deals with algorithms that, in my opinion, nobody would ever use and complexity results that are way too pessimistic, right?",
            "So somehow the Russians cool is taking a great hold on on this community here.",
            "Now, both schools considered stochastic optimization, which is what occurs in machine learning.",
            "As you can see from those very famous names and so therefore I'm going to be focusing a lot in trying to look at the correlations and differences between stochastic optimization problems.",
            "In deterministic optimization you will notice that speakers often cross the line they start talking about is GD, and then they draw a picture of a deterministic method.",
            "Doing something right.",
            "So we do that and it can be productive."
        ],
        [
            "Then it can be dangerous, so large scale nonlinear optimization optimization is of a large field, right?",
            "There's linear programming is integer programming.",
            "There's robust, so many areas where the one area is nonlinear optimization, non convex and large scale, and that's what I've been doing my whole career.",
            "This is a very well research research area where the software is all over the place is being used for optimal trajectory problems, optimal design, etc.",
            "Stochastic optimization in the sense of machine learning in the sense of classification and so on, deals with random variables.",
            "There's randomness in the objective function in this area has developed by borrowing some ideas from deterministic optimization.",
            "For instance, momentum gradient descent itself.",
            "When Robbins came up with that algorithm.",
            "Robinson Monroe analogy to this key percent method.",
            "But momentum and people talk about preconditioning and so on.",
            "His ideas are come from the terministic world and service inspiration.",
            "Now, this exchange of ideas is not straightforward.",
            "You cannot come up with some great algorithm that we have for doing airplane design.",
            "And moving into this domain here, becauses stochastic approximation methods are Markov processes.",
            "They are not the same type of iterations.",
            "There's a difference in nature between this and SGD algorithm.",
            "In a batch gradient method, they're really very different things.",
            "However, there is a continuum between the two worlds.",
            "If you take an algorithm, start increasing the batch size, it becomes at some point it becomes gets into the terministic world, and all these things that work in the deterministic world can be used there.",
            "So you conversate continuum between.",
            "SGD algorithms to the gradient algorithm there and a lot of people are exploring what's happening over there because strange things are happening when you're moving to those domain.",
            "So the interplay between these two worlds, the terministic run the stochastic world is ongoing and it's going to be one of the main subject of my lecture trying to bring some of those ideas here.",
            "Now.",
            "As far as I can see, some of the new algorithms that are being produced for solving.",
            "Training neural networks are coming from the machine learning community.",
            "And it's exciting to see young people doing that, so there is a continuum between the two worlds stochastic in the term."
        ],
        [
            "Mystic alright, So what happens in these two worlds and I have to emphasize, these are pictures about large scale, right large scale nonlinear optimization.",
            "That's neural networks.",
            "That's not support vector machines.",
            "This neural networks and on the other hand.",
            "Climate, the weather forecasting problem, and deterministic problem also within million variables, 5 million variables.",
            "Very difficult neural network, very difficult, both nonconvex.",
            "What are the algorithms that people are using?",
            "Well, as you know, for the stochastic for training neural networks you have is GD yes.",
            "The other dimensionalities are pretty similar.",
            "It depends.",
            "I mean there there optimization problems of all sides, but say weather for the current weather forecasting systems right now probably 5 to 10,000,000 variables and their design problems.",
            "Also millions of variables I have to say that when I was doing my PhD I really wanted to focus on large scale optimization which meant problems with 1000 variables.",
            "And then, but they quickly, we ended up developing algorithms that were linear.",
            "The dimension of the problem.",
            "So we were scaling up the problems in a million variables, and in fact the reason why during the 1990s I was involved in the development of weather forecasting systems is because they needed someone who knew how to solve problems with hundreds of thousands of variables.",
            "And there we struggled for a long time.",
            "And today all the weather forecasts are being run by an optimization arm that loops.",
            "Around very much like backpropagation, the key idea there tools also to get derivatives.",
            "There's no way you're going to do large scale optimization without derivatives, right?",
            "So in that in that domain weather forecasting people have this flow equations navier Stokes equations, how the atmosphere develops, and then you want to know what the derivative is with respect to changes in the initial conditions.",
            "And then you can do by something called the adjoint method, which is very similar to the reverse mode of differentiation.",
            "So you have these two worlds.",
            "Inventing Brett backpropagation.",
            "At the same time also Hessian vector products at the same time don't not talking to each other, I'm standing in the middle and I should have done better.",
            "But we just you know that that happened and it's pretty spectacular.",
            "What is been accomplished in both domains?",
            "OK, so the algorithms then that are trained for machine learning are there and I find it very interesting to hear about the natural gradient method because.",
            "This is this is consistent with the direction that I've been talking about for many years about moving algorithms.",
            "In any case, in the deterministic large scale world.",
            "People use question.",
            "You can methods.",
            "Inexact Newton methods may be nonlinear conjugate gradient methods, and basically the iterations look like what you have there.",
            "They just differ by the fact that there is a matrix in front of the gradient.",
            "It's not a diagonal.",
            "In our world, there's never a diagonal.",
            "Nobody knows how to choose a diagonal.",
            "So that could be a quasi Newton approximation or could be implicitly Hessian matrix in a subspace which would be given to you by Newton conjugate gradient iteration.",
            "There's a difference between the two worlds, so we're saying many first order methods being used in machine learning.",
            "There is always the plane with the step length.",
            "The iterations are expensive in noisy, and maybe one has to deal with concepts like a Fisher information matrix or not, not the his here.",
            "On the other hand.",
            "In the other world, the simple gradient method is never used.",
            "Um, acceleration and momentum is never used that I know those are considered to be theoretical constructs.",
            "Why aren't they use?",
            "Because just by getting great information you can actually use a question.",
            "Newton algorithm only requires great information.",
            "And for quadratic functions those are not optimal.",
            "You can use conjugate gradients.",
            "For convex functions you can use across in your algorithm, and people have discovered over the decades of these algorithms work better, so they're not used.",
            "Something that is used and is really important is align search, and I'm going to talk a lot about that now.",
            "Hessian vector products are units at one deals with and, so that's the way things."
        ],
        [
            "Now to make it appear abit more concrete discussion about the interplay between the two communities, let me talk about momentum."
        ],
        [
            "And let me advocate a proposition here so the heavy ball method people have described it there and my name comment is beware of two dimensional pictures.",
            "Sometimes they're really useful most of the times they are not.",
            "Right, if you have a problem in 100 variables, this may not be very useful, right?",
            "But you've seen this picture momentum.",
            "Why does it work?",
            "Well, you take a gradient step, you take another gradient step and there's a drift.",
            "Therefore, why don't we just move along the drift, right?",
            "It makes a lot of sense until you say, well why the drift of just the last two points?",
            "Why not the last two directions or four directions?",
            "White just one drift, right?",
            "It's the mathematics.",
            "Is there?",
            "The intuition is not there.",
            "In fact, remembering all the drift is what the conjugate training method does.",
            "But so this intuition is false.",
            "This is a 2 dimensional picture that says that intuition because I want to give you another picture where the opposite happens.",
            "Momentum just is the worst.",
            "The wrong thing.",
            "Now if you choose those parameters in the way that I have there, which means that you have some important knowledge about the objective function, the condition, number of the problem, you need those parameters there for the momentum method to actually make sense.",
            "That's why your intuition is not really.",
            "Right, because your intuition would never tell, is the condition number.",
            "If you put numbers are way off that there's no benefit to this.",
            "OK, so momentum is serving inspiration and is used a lot in for training neural networks, but neural networks are not quadratics.",
            "So what's going on right?",
            "In fact, the Gray matter with momentum cannot even be shown to be convey."
        ],
        [
            "For non convex functions.",
            "And here is a picture, just one picture.",
            "I said that don't look at 2D pictures, but at least here.",
            "Here's a 2D picture where X0 is a starting point.",
            "You move to X1, then you take a gradient step.",
            "The two blue steps are radiant steps.",
            "In this case, momentum is something well, the orange dotted line.",
            "Their momentum is actually the first direction that you move, and you see that from X22X2.",
            "Till then you just made things worse, right?",
            "And what I did here is I constructed and non convex function.",
            "Where things just go back, but what we really have to take into consideration, we were doing neural networks.",
            "We're dealing with a very large dimensional space, so these drifts and these things.",
            "How do they work right?"
        ],
        [
            "Nevertheless, so I just said there's a problem.",
            "Momentum is work in practice, and my understanding is used almost routinely.",
            "Fortify the stochastic gradient method.",
            "Maybe this started with this paper here, but my conjecture is that this really does not have anything to do with momentum.",
            "What is momentum?",
            "To me, momentum has to do with the linear dynamical system with friction and the friction is where the momentum comes from.",
            "So momentum is explained in terms of math Nesterov's acceleration has no intuition.",
            "Maybe for Jimmy it does not.",
            "For me.",
            "There's math.",
            "It works there, right?",
            "So I think that my position is that what works works.",
            "Nobody can deny the momentum works, but you have to think about it another way.",
            "It's what is happening is doing iterative averaging or gradient averaging or something like that.",
            "And momentum was just an inspiration.",
            "People are playing with those parameters and as long as you play with those parameters you really moving from the original formulation of the problem.",
            "So therefore Jimmy Jimmy's presentation today was very relevant to this.",
            "How do you?",
            "Average information as you go along, so the practice between our understanding and algorithm in practice is is lacking, and in fact almost every argument you would make for momentum you could do for the conjugate gradient method, which is not very popular in machine learning, but it's a very important algorithm, because this algorithm that I wrote there, the conjugate gradient method, has pretty much the same form as momentum, but the parameters Alpha and beta are given at every iteration they change.",
            "You can compute them very easily via dot product.",
            "You have you have required no knowledge of the condition, number of the problem.",
            "So here you have the momentum method and its knowledge.",
            "Lot of knowledge about the quadratic function.",
            "Here you have a conjugate gradient method knows nothing about the quadratic function and conjugate gradient method is in the worst case just as good as momentum, normally much better than that.",
            "Now it's not used if you take those parameters there and you start fooling around with them is no longer the conjugate gradient method.",
            "It's it's nothing, right?",
            "So if you put that in a noisy regime in so I will mention a little bit later.",
            "Contrary method breaks down.",
            "Yes, Joshua.",
            "Yeah, this is the deterministic the stochastic conjugation method.",
            "I don't know that it exists.",
            "So, but since we're moving from 1, right?",
            "So the other one I'm not going to tell you, I'm now in a I'm now in B.",
            "You have to figure it out right, but I'm going to make a big point about the fact that probably the next like conjugation is going to breakdown with noise if you do it this way.",
            "OK, and I have in my website there is a nonlinear conjugate gradient code.",
            "If anybody wants to play with that, that nonlinear conjugate gradient can be used for nonlinear functions, and Steve Wright was telling me recently getting good results by using nonlinear."
        ],
        [
            "Great, alright, so Nesterov acceleration is another way of doing things.",
            "It's really remarkable result.",
            "It is amazing that one can get a better algorithm so simply, but is it relevant in practice Now there's a famous album called Fiesta for L1 optimization.",
            "There people have used it a lot, whether it's really better than just the original Easter algorithm is not clear, so these methods require some knowledge of the objective function.",
            "There are many complexity papers about.",
            "These algorithms, so we see many, many papers about.",
            "Nesterov acceleration.",
            "In the non convex case, what does he do to try to escape from saddle points?",
            "An combination with a coordinate descent method?",
            "There are many many papers like that and they lead to complexity results and I was saying some complexity results are way too pessimistic.",
            "And so Jeff was talking this morning about, you know, identifying a saddle point.",
            "Or epsilon.",
            "Required you will require thousands thousands of iterations to obtain it or the epsilon result.",
            "This pessimistic theory."
        ],
        [
            "OK, let's skip that.",
            "Alright, so the classical conjugate gradient method, which is to think of beauty because I think it's the best thing for best iterative method for solving a quadratic.",
            "I mean there's an optimal properties is going to breakdown with noise.",
            "And there have been a number of papers looking, and I know here in Toronto, now is the same thing happening.",
            "Revisiting momentum and saying why are we doing this?",
            "And there's this paper that I mentioned there that shows that when there is noise, momentum provides no benefits.",
            "Even for linear regression.",
            "And for Nestor's algorithm, when you inject noise, they empirically find there in that she rates very quickly.",
            "So in any case, I just want to finish our discussion about momentum by saying I'm sure those ideas work, because if something works, it works, but it's it's because there are some statistics behind that.",
            "It's some optimization combined with the statistics.",
            "Think about it this way.",
            "I'm not saying I'm right, I'm just saying this is something like advance, right?"
        ],
        [
            "OK, so this is the summer school you're here for learning understanding is Judy.",
            "Do we understand this GD?",
            "Well, people put these tables right.",
            "Complexity of results.",
            "Do we understand them?"
        ],
        [
            "What is the most basic question?",
            "Do you know why SGD converges?",
            "And for what classes of functions can we say that it converges?",
            "And once you have that, do they include neural networks, deep neural networks?",
            "What happens if you have values and there's non differentiability?",
            "Do those results apply?",
            "OK, so I'm going to have two equations during this lecture.",
            "Two really important equations, one that is coming now and one related to Newton Smith.",
            "So that really important relation is this.",
            "If you aren't deterministic optimization and the easyworld.",
            "You have a proof that the gradient method always converges to a stationary point and the proof is like 5 lines, and immediately you discover the following relation.",
            "The decrease in the objective function, right?",
            "So the new value should be smaller than the new one, so there is a decrease in the objective function is proportional to the grading.",
            "Right, if you're minimizing a function when you're away from that, just get a big decrease when you're close to the solution.",
            "You cannot get a big decrease, so that relation by itself is telling you the gradient method is pushing the objective function in proportion to the gradient and is always going to work unless you do something bad with the stepping right.",
            "So the step link is bad, then then you'll spoil all of this, but we will see that it's easy to know what you do with Stephanie.",
            "Now, what happens when you have stochastic optimization so SGD?",
            "Now the problem no longer looks like this nice optimization problem.",
            "I wrote that the parameters W an random variable sign depends on the choice of the data that you're using.",
            "So now the relationship is this.",
            "The decrease in the objective function has to be measured in expectation, right?",
            "You cannot say for sure because it's a stochastic algorithm.",
            "It has a first term, which is exactly the same as the classical gradient method, and then it has the second term that has to do with this.",
            "Second term is the variance of your stochastic approximation, that is a stochastic gradient that I wrote by that notation there.",
            "So there are two terms that are playing against each other.",
            "If the grading is unbiased, is stochastic gradient method is an unbiased estimator?",
            "You have the same force down.",
            "But because you're not choosing the right grade and there is a noise level and that is preventing you from converging to the solution and early on in 1951 already, Robbins and Monro noted that one term is an Alpha, the other one is in Alpha squared.",
            "Therefore you can converge if you make Alpha go to zero and they have the proof in 1951 and after that you can get rates of convergence results.",
            "So there are two key algorithm components to SGD.",
            "First of all, how do you choose the stochastic direction?",
            "If it's an unbiased estimator, then things are going to go well because that is going to give you the proof.",
            "This is the drift and the step link that is crucial.",
            "If you diminish it to zero, you're going to get a sub linear rate.",
            "If you choose it constant, which is a lot more interesting, what does SGD do?",
            "If you use a constant stepling?",
            "And what happens is it converges to a neighborhood of the solution.",
            "This can be proven and the neighborhood can be care."
        ],
        [
            "Authorized right?",
            "So we have here a run of SGD on a very simple problem with a fixed step length.",
            "You see that it's taking big steps all the time.",
            "With diminishing step links, it looks like it's a really good algorithm, except you don't see how many steps you've been taking there because it's moving really, really slowly towards a solution, right?",
            "But with large step length it says well also late around the neighborhood of the solution.",
            "And what's remarkable is that the neighborhood that you converge that you converge to linearly, not sublinearly you converge at a linear rate of convergence to a neighborhood of the solution.",
            "And once you're there, you know things wander and you can see there is a lucky step that gets close to the solution.",
            "But you would not know.",
            "Maybe you would not know how to identify there was a question there.",
            "Exactly, yes, so the last term in that relation will not go to sere if you're doing SGD.",
            "The only way you can make the last term go to series by choosing Alpha Small, because there is an Alpha squared here.",
            "So in machine learning Alpha sometimes is called the learning rate.",
            "We always called Alpha this steppling how far you go.",
            "I think in SGD it shouldn't be called either one of them because it does two things, it suppresses the noise.",
            "And the other one it controls the size of the step, so it's more than learning rate and stepping.",
            "It doesn't do two things at the same time, and in fact, because it doesn't, two things at the same time.",
            "It's because it's impossible to find a room.",
            "I think there will always work for those problems.",
            "So in any case, I think it's very important to think about the SGD method with fixed step links, because I think that's what people do in practice.",
            "Fixes step link and you get to a neighborhood of the solution.",
            "Really quickly decreases step when you get to another neighborhood of the solution and all that is well understood and that equation that I have there is when I said do you understand why it converges?",
            "Well that that's the explanation that it does Sanjeev.",
            "Yes, yes, I forgot to say about this is all about the convex case and I have a slide about the non convex case because there are a lot of results about the nonconvex case but I have also some complaints about them."
        ],
        [
            "Why is SGD efficient?",
            "You see I don't have much in this slide.",
            "Jimmy talked about that we have a Siam review paper that appeared this year with number 2 and Frank Curtis and we have a few examples that are intuitive.",
            "This is why it's efficient to use as GD.",
            "And then there is a complexity results which are very compelling.",
            "These other ones are really sealed things for certain classes of problems.",
            "It is more efficient to use SGD than a Patch method.",
            "Now non convexity, so that's where you're asking me, right?",
            "There are a number of results have been established for convergence of SGD to stationary points.",
            "And I don't have them here.",
            "And I've worked on some of them because my question is how meaningful are they?",
            "How unrealistic are they?",
            "Well, they they don't seem to match the practice of neural networks, where the problem is seems to be much simpler than a general nonconvex problem, and these results will apply to any non convex function.",
            "So we will say that you know it, you will visit a gradient with zero point where the grain is almost zero.",
            "As you look at the window after so many iterations that are way too many.",
            "How many people are these results?",
            "There's a lot of interesting world coming from the theory community about that, so I defer to them and there was a lot of interest, interesting material incentive, stock.",
            "The very first talk this morning.",
            "Um, yeah so.",
            "Several points how things work with several points is escaping several complete complexity, and so on.",
            "We enter deterministic optimization world, have never worried about saddle points.",
            "We never thought that we really converge.",
            "The saddle points the question about 7 points are going to slow you down is a concern.",
            "But as I mentioned, we never tried to use gradient methods.",
            "We're always trying to use something that has more curvature information about the problem.",
            "So what you hope is that you're not just doing a stupid descent method and then somehow something is going to happen and push you there, or find negative eigenvalue.",
            "But in a Newton type method when you try to get second or the information that we're going to get to.",
            "Contributions of Jimmy, Ann and Roger that we use.",
            "We use secondary information from the beginning that should help you there."
        ],
        [
            "I'm going to get through that.",
            "So what are the weaknesses of this GD?",
            "I mean, in principle as a core one view is it's nothing an in fact SGD, maybe within some statistical variance like the second talk this morning.",
            "That's that's what you want to do.",
            "And my feeling is that.",
            "The Pioneers of the field with neural networks.",
            "My feeling one of them is here have the feeling that you know I don't know what eventually is going to happen, but it's going to look a lot like this GD.",
            "Maybe there's this really powerful type of optimization algorithm here that I look.",
            "I have the feeling they're going to be like a city.",
            "I have.",
            "My impression is is that through their experience, that's what they feel right there.",
            "There are more discipline and more expensive items.",
            "One could you, so that's one possibility.",
            "So look at SGD, look at ideas behind momentum.",
            "There is the answer to the question, and many of you are actually playing with such ideas.",
            "There's an alternate view.",
            "How could it be the optimal algorithms when gradients don't have scale as we showed in the beginning of it, they suffer from ill conditioning.",
            "And this is the first order method.",
            "Why couldn't it be learn something about the curvature of the problem so it never does learn the curvature of the problem?",
            "May do some smoothing.",
            "That's very different when someone asked the question to Jimmy about ill conditioning.",
            "Right now he was talking about even if I have the gradient very good, I'm still suffering from Neil conditioning, right?",
            "So are we happy to just say we cannot do anything about your conditioning?",
            "We cannot do anything about scale.",
            "So and also, if you use a very noisy iteration it makes.",
            "Parallelization limited."
        ],
        [
            "Alright, so there is a picture if we go beyond the stochastic gradient method and that's what the rest of my talk is going to be about.",
            "Speculating about ideas where you go beyond stochastic gradient method.",
            "There is a picture that we have in that review paper that I use all the time.",
            "I think I'm adding a new axis now stochastic gradient method is at the origin is like the, you know the universe started there with stochastic gradient method and you can move on in One Direction by reducing the variance of the gradient.",
            "The last term in that equation is type.",
            "Reducing that to obtain argument that have less noisy methods until you get to a batch method.",
            "The other direction is your start incorporating 2nd order information into the problem, as was done with a K FAQ, and there are other algorithms that I will mention here.",
            "We have these two possibilities of moving, and they're actually complementary.",
            "You can do the two at the same time.",
            "Now remember the key relationship there.",
            "So when I talk about variance reduction, I mean how to how to suppress the blue term.",
            "So there are two ways to suppress the blue term and I have to thank you for your question.",
            "One of them is with a step length and the other one is by decreasing the variance of that term.",
            "If you make that a mini batch, the larger the mini batch the variance will be in.",
            "The smaller that blue term will become.",
            "So there are these two mechanisms at your disposal.",
            "I I don't know, yeah so.",
            "Well, that's why I said the first generations.",
            "That's very sceptical about this, right?",
            "Yeah, so all I'm saying is this is where we are with this GD.",
            "Why don't 11?",
            "We consider it.",
            "Nobody says about removing the noise completely, and in fact, when I'm going to talk about it, more disciplined way is to just reduce it, because why not right?",
            "Once you're getting close to the minimizer, maybe there is no risk of leading that minimizer and just converge quickly to that, right?",
            "So that's just the possibility of moving that way, right?",
            "Where in between is the right spot?",
            "I don't know."
        ],
        [
            "So maybe another axis here?",
            "Is there other ways of improving SGD that have to do with statistics?",
            "That I don't understand well, but that's what I was saying.",
            "Forget about momentum.",
            "Think about statistics."
        ],
        [
            "Now, in terms of incorporating 2nd order information, if we look at that axis that says Stochastic Newton method.",
            "There are three ideas that I think are possible.",
            "One of them is to do in inexact Newton methods.",
            "They scale up.",
            "They can be used using Hessian subsampling.",
            "Second idea would be to start with the natural gradient algorithm built on that, like a fact, and the third one, which we finally picked up there for many years, is to go back and revisit ideas of quasi Newton methods, and we're finding that very promising results.",
            "I think we're doing causing it and methods incorrectly for machine learning and in the stochastic regimes there are these three forms that I see.",
            "And.",
            "I've been talking about second order methods for awhile when nobody in the machine learning community was talking about them, and then with time things are changing and in this today you gotta have a slanted view of optimization for machine learning.",
            "You got two talks to talk about using second order information.",
            "We're in the minority, but so that's just something to keep."
        ],
        [
            "Alright, so now going on till the other axis.",
            "I'm going to move variance reduction.",
            "How can we suppress the noise?",
            "Well, if you use the mini batches suppress the noise everybody have found that using many batches is useful.",
            "Classical complexity theory does not show the benefits of using mini batching.",
            "Complexity analysis does not do that.",
            "Some papers recently challenged that, but in practice it works.",
            "What if it works?",
            "It works.",
            "But why not use a gradient with a much larger batch size?",
            "Why is it that people are using small batch sizes only?",
            "Not take more, you know half of the training set.",
            "Becauses the batch size becomes larger.",
            "People have observed that accuracy deteriorates.",
            "I believe that this has been observed for many years.",
            "Empirically, there are dozens of studies and the theory community, and now looking at to see what is happening there.",
            "But so.",
            "It would be more natural from the very beginning to have used iterations with survey accurate batch that you can paralyze brain computation very easily, and I know that from the beginning people are not getting good results with that.",
            "So there using noisy iterations.",
            "So that's one reason why you don't go jump straight onto.",
            "The non noisy regime."
        ],
        [
            "Alright, so here's a paper by the Facebook team.",
            "What they did is this a paper where they talk about doing Imagenet, bringing down computing time from 29 hours to one hour.",
            "Kindly keyin his group.",
            "And I have this picture here where they run the stochastic well.",
            "Probably Adam.",
            "They're probably running Adam with different batch sizes, and they're plotting their.",
            "This is image net, the floating their accuracy.",
            "As they increase the batch size, the reason why they're brought down computing time so much is because bigger, bigger batch sizes they could paralyze better, but so at some point after 8K.",
            "Accuracy starts getting worse.",
            "It's just another study that showed that now there's another paper paper #2 is from people at Google.",
            "I think probably is one of the authors of that paper.",
            "They go and use batch size up to 65 K. And those two papers, what people say is, you know, in the past we always just played with the learning rate.",
            "This time, instead of decreasing the learning rate at some point, increase the batch size.",
            "So when you get stuck, increase the batch size by decreasing the noise and increase stepling.",
            "So do that and in these two papers they got good result.",
            "So if under very encouraging because that is actually consistent with where I'm going right?"
        ],
        [
            "So where I'm going?",
            "What is this doing here?",
            "On this this fantastic animation is very important.",
            "OK look, look at this.",
            "This is called robust optimization.",
            "Optimization consists of finding the lowest point in the curve, right?",
            "Robust optimization says this.",
            "I give you a disk.",
            "And you want to push it as far down in the objective function.",
            "The disk has to do with the uncertainty that you're willing to tolerate, right?",
            "So here's now the optimization problem.",
            "It has two minimizers.",
            "You're trying to push the disk down, and the disk does not fit into the first regime, so therefore it goes into the second regime.",
            "This is the field of robust optimization, which is a very active area in optimization today.",
            "So one conjecture is SGD converges to minimize that are more robust.",
            "I'm not going to use the term flat.",
            "Even entropy, maybe a little bit.",
            "Misleading because the minimizer degenerate.",
            "I'm going to use robust because the idea of the disk I think is relevant here and one of the conjectures is SGD gets you to more robust minimizers, and therefore, if you're going to improve as GD.",
            "Put in some ingredients that have to do with robust optimization.",
            "Unfortunately, you cannot use the classical robust optimization techniques that teaches semidefinite programming and something that does not scale, so you cannot do it in a very nice way.",
            "So the robust optimization problem is not minimizing objective function.",
            "But minimize the maximum value.",
            "The objective function can have in a certain disk.",
            "I just want to bring this idea.",
            "I don't have any more insights about this problem about SGD doing better minimizes than batch methods than anybody here.",
            "I just want to bring in so this notation robust, robust minimizers might be a good way to look at it.",
            "Alright, so going into this, yes, I said you.",
            "Yeah, so this is the robust optimization problem.",
            "Yeah, lots of people.",
            "This is an active area of research, right?",
            "Adversarial.",
            "Against robust optimization.",
            "OK. Alright, you have to tell me later what, what do you have in mind?",
            "Is she?"
        ],
        [
            "So we're going on to that axis.",
            "Let's find methods that reduce the variance, not just play with the step length.",
            "So instead of manually choosing the mini batch, which is what that Facebook and Google Papers were doing, they were saying we're going to just 8K, or we're going to do it gradually, like this.",
            "Isn't there a way to do this in a systematic form, developing algorithm that gradually increases the batch size, suppresses the noise, starts with a very small value, and doesn't have to go all the way to suppressing the noise, just has to go good enough so that things work well.",
            "The noise is controlled by the sample size, it's beginning.",
            "You start with SGD regime.",
            "And then you stay with the same sample size until you feel that the item cannot make more progress, so the noise is dominating.",
            "You want to have a test that says your noise is dominating your increase.",
            "Decrease in the objective function, improve the quality of the gradient.",
            "So that's that's the idea that the algorithm should do this automatically, and we call this progressive sampling algorithm.",
            "And if you do a progressive sampling, you get a better explanation of the gradient.",
            "So that opens the opportunity for using more optimization algorithms.",
            "So you could try to incorporate 2nd order."
        ],
        [
            "Now, if you're going to use an algorithm that input increase increases the batch size as you go along, the key question is how fast do you do that?",
            "Are we going to end up with some heuristic that is horrible?",
            "Again, they can never be used in practice.",
            "Well, a progressive matching algorithm that increases that sample size at a geometric rate.",
            "Like 1.1 to the K. This theoretical result increase the batch size at a geometric rate.",
            "Complexity wise, it matches the complexity of SGD.",
            "Right, so there are papers, especially by #2 saying SGD gets the best complexity work complexity.",
            "How much function evaluations you do?",
            "And gradient method is much worse.",
            "Well here we have a paper from 2013 where we showed that if you increase the sample size at a geometric rate and you look at how much work you end up doing in the end.",
            "And the complexity is done not on the empirical minimization function, but on the total problem.",
            "The real problem.",
            "The complexity is matches that of SGD.",
            "So at least these two algorithms in terms of complexity should be put at apart.",
            "Batch method bad is to give a good progressive sampling.",
            "In the worst case they are."
        ],
        [
            "Parable like you would never do something like this in practice, so let's go through the heuristics.",
            "Increase the batch size like 1.1 to the K. No good algorithm is to the K, right?",
            "So what we want to do is we want to find automatic criterion that does that.",
            "That's what optimization algorithms that go into textbooks eventually do, that they run by themselves.",
            "Right, So what is that we want to do?",
            "Let GK be your stochastic approximation to the grade.",
            "What do you want to make sure is that you have an acute angle with the true gradient direction.",
            "And how do you measure that?",
            "How do you know that you have the right angle?",
            "Especially you have noisy directions.",
            "Well, in the terministic optimization you would do something else but in machine learning.",
            "I claimed that you would do this.",
            "If you make sure that your gradient estimate GK satisfies that condition.",
            "The difference between it and the true gradient is less than the norm of that rating.",
            "Approximation of true gradient itself.",
            "For fatalism one, then it's a dissent direction and everything is going to work."
        ],
        [
            "Now, this quantity actually is going to.",
            "If you square it is going to look like a variance.",
            "That's why it's something that is useful in case for machine learning, so we go from the deterministic case of the stochastic case.",
            "Now you have your stochastic rain approximation are calling gratify.",
            "You want that in expectation to be less than the norm of the true gradient.",
            "You don't have the true grade, but we're going to do one more step, right?",
            "So if you do this, you can prove convergence.",
            "Your room is going to work right if that expectation is less and you divide by the cardinality of the sample size."
        ],
        [
            "So that's an equation that actually means something that is completely common sense.",
            "You're moving and you can measure the variance of your stochastic gradient.",
            "When the variance is as big as, this is the step itself.",
            "It's time to improve the quality of the great.",
            "That's pretty much what it's saying and how you're going to measure the variance of the gradient.",
            "You can do a sample variance.",
            "You know if the problem is a finite sum problem, you can take a sample of the variance and you can also sample the right hand side."
        ],
        [
            "So, so we have done that.",
            "And if you do this variance estimates you can get pretty good behavior of algorithms.",
            "I'm not going to stay in this graph because I have another graph.",
            "As I mentioned, I'm now much more keen and using Quasi Newton than inexact Newton methods.",
            "But in any any case we have criteria practical criteria to tell you when it's time to increase the batch size.",
            "If you're going to do one of these progressive batching algorithms."
        ],
        [
            "OK, now I want to talk about step links.",
            "If you have any optimization algorithm that is a decent algorithm, you're here like in that picture, and you know what the gradient is.",
            "How far should you go?",
            "Well, it depends.",
            "There are two objective functions there.",
            "If there is high curvature, you cannot go very far.",
            "If there's more curvature, you can go quite far.",
            "That's pretty simple.",
            "So that's how it has to be in the rule there, right?",
            "OK, so that's what leads you to these Lipschitz constant."
        ],
        [
            "Um?",
            "But different directions should have different scales.",
            "So I have a quadratic function and there are two lines there.",
            "There is a solid line and there is a dotted line.",
            "Along the dotted line, you cannot move very far because the curvature prevents you from doing that, but along this solid line you can move very long."
        ],
        [
            "Now something that is done in the theory community is they use a universal Lipschitz constant.",
            "They use a bound for the curvature everywhere right?",
            "And they apply that to the algorithm.",
            "So the steepest descent algorithm is going to take a step that is going to guarantee to give you the sense in any direction that you move.",
            "And that means that for the for the solid line, you're going to make much less progress than you could do because you are going to restrict yourself to that.",
            "That's why line searches have proved to be so useful in the deterministic setting.",
            "You take a search direction and use it well.",
            "Explore the objective function and see how far you can get there.",
            "When the function is very noisy, this doesn't make any sense, but once you get more accurate estimates, that's one way of exploiting the fact that there is scale in the problem."
        ],
        [
            "Now, I mentioned that in SGD stepling this light up with noise suppression too, so it's not such a simple thing.",
            "I was giving you a picture there that is just for the deterministic setting.",
            "Alright, so how do you scale the gradient direction?",
            "So I was mentioning that in theory an A lot of papers say.",
            "Well, not in the machine learning practice, but in the theory says take Alpha to be one over the Lipschitz constant and everything is going to be peachy.",
            "And so my my message again to you is.",
            "That's great in theory.",
            "You don't want to do that in practice.",
            "You're taking the most conservative step length, and so some people say, well, I'm going to do an adaptive Lipschitz estimation and adaptive estimation of what's the highest curvature of the problem.",
            "But that's not a good concept because you're learning behind move.",
            "You learn a certain curvature.",
            "You move another curvature, and with all of that you want to get a universal curvature.",
            "Well, the line search idea tells you one time you're going to take a big step you want.",
            "I'm going to take a small step that is proved to be quite useful.",
            "Alright, so now since we want to scale the gradient differently.",
            "Then something that comes natural is.",
            "In addition to the line search, or perhaps to learn to help the line, Sir, why don't you do a diagonal scaling?",
            "And so that idea is very popular and has proved to be quite useful in machine learning and assumes that you have knowledge.",
            "Of the objective function along the coordinate direction.",
            "Now what are the core in the directions?",
            "Well, if you think that they are related to features, then you would say or maybe I have knowledge about what happens among different feet."
        ],
        [
            "But in general, when we solve all sorts of nonlinear optimization problems, we never know how to choose.",
            "Diagonal matrices because we don't know what the objective function is doing along corinthe directions.",
            "Coordinate directions are totally arbitrary.",
            "Our algorithm may be moving here, maybe moving there, but it's only never going to move along with this coordinate directions, so we cannot learn goes well, but Adam and Adigrat are.",
            "Most likely doing something else, gathering some statistics I don't understand.",
            "I don't understand them well, but you have experts here.",
            "So instead of finding sophisticated, let's tabling strategies, find a method that produces waves well scaled direction.",
            "So so if we can do this.",
            "And Jim is talking by putting a matrix in front is very important if you can put a matrix.",
            "A choice of this step link is going to be much less important, and that's definitely the case in Newton and Quasi Newton meth."
        ],
        [
            "Alright, so I want to say something about Newton's method because of the ideal and I think it's also motivate things.",
            "So.",
            "Some classes are not any optimization.",
            "Start with Newton's method because they say this is the.",
            "This is the scale invariant the optimal algorithm.",
            "It doesn't matter how badly scaled the problem is, it immediately learns the hidden information contains a lot of information.",
            "So Newton's method is the thing to do if you're an engineer and you have a problem with 100 variables.",
            "And lots of problems in engineering are ill condition.",
            "Use Newton's method right?",
            "But what happens?",
            "You have a problem in a million variables.",
            "You cannot invert the matrix, so this is out of the question.",
            "There is too much information.",
            "How do you compute and you can spit?",
            "So the idea is, well, computing inexact mutants that.",
            "What does that mean?",
            "Um?",
            "Or use a natural gradient method.",
            "And choose an inexact or computer.",
            "In exact step there, or use causing you."
        ],
        [
            "An idea?",
            "OK, so this is the 2nd.",
            "Equation I have two equations for this talk.",
            "What is Newtons method?",
            "Let's take an X Ray of Newton's method.",
            "Again, in the strongly convex case.",
            "Because maybe I'll talk to.",
            "When there's a question and answer, I'll talk about the nonconvex case with Sanjeev and we can discuss why convex and nonconvex, why the emphasis shifts, any case industry convex case just to make our life simple when the Hessian is positive definite.",
            "Take an eigenvalue decomposition of the hisi.",
            "So Lambda I vvi transport the eyes are the eigenvectors and so on.",
            "Every symmetric positive definite matrix has eigenvalue decomposition and the nice thing is that the inverse of the matrix is the same form, except that the eigenvalues become the reciprocal.",
            "OK, great, so if I multiply that inverse Hessian times the gradient, what happens?",
            "Well, the direction becomes when I multiply that summation with VI VI, transpose.",
            "Well, forget about the charming parenthesis.",
            "Just look at what's outside.",
            "It says one over Lambda I times VI.",
            "What is the term that is really going to matter if you have a tiny eigenvalue, Lambda is 10 to the minus 6.",
            "Right and one over Lambda is 10 to the 6th right?",
            "So the smallest eigenvalue is the one that is going to determine by far where the Newton direction is going to go.",
            "Or the smallest eigenvalues are the ones that are really going to matter.",
            "So one way of doing an inexact Newton method is it look at that expression which says the Newton step consists of looking not a coordinate direction consists of looking at the eigenvectors of the problem.",
            "And moving along those eigenvectors at distance one over Lambda I.",
            "Right and the more information you have about the product closer you get to the new contract.",
            "Now unfortunately the most difficult thing to compute.",
            "Is the smallest eigenvalue of a matrix the largest eigenvalues?",
            "Are the easy ones.",
            "So every inexact Newton method where they're going to do is they're going to start approximating the easy directions, and the more work you do, the closer is going to get to the Newton method.",
            "That's one way of trying to do an inexact Newton method has proved to be very successful is these are the Hessian Free Newton method."
        ],
        [
            "So an inexact Newton method is going to do the following.",
            "Instead of inverting the matrix, you solve the linear system of equations by our favorite algorithm, the conjugate gradient method.",
            "You apply the conjugate gradient method to that.",
            "That's called the Newton exact Newton CG algorithm.",
            "It exists, and tons of software.",
            "There is a great way of doing.",
            "Inexact Newton method.",
            "Now the talk this morning, Sanjay for Aurora hadn't had a method.",
            "An approximation of the Newton method.",
            "There was an expansion of the inverse of the matrix.",
            "I -- A inverse.",
            "It turns out that if you look at that method.",
            "The way it was put in the paper by Hassan and Bullington.",
            "So when you write it down, it consists of applying SGD.",
            "To solve that system there instead of, instead of using conjugate gradients to solve SGD.",
            "To that problem, there's a small change that the Hessian is sampled right, but all of that it seems like this is a new.",
            "This is a new Ave actually.",
            "If within that family of methods that is doing an inexact Newton method in that way, and by the way, using SGD to solve the Newton equations is not the best thing to do.",
            "We have analysis and we have experience about doing that.",
            "Now all what I gave you was an intuition about the convex case.",
            "I said that if the Hessian is positive definite, this is what happens.",
            "What is it that people in the optimization community do a lot?",
            "They for they say I don't care the nature is positive definite or not.",
            "I'm just going to type conjugate gradients to that.",
            "And see what happens.",
            "You start applying conjugate gradient method, and if it's some point, you find the direction of negative curvature.",
            "Is it easy for you to find the dictionary coverage?",
            "It's really easy because the conjugate gradient method has to compute PAP transpose.",
            "So if you detect that you stop the conjugate gradient process, you follow the direction of negative curvature.",
            "Until about Zero trust region and then you move on.",
            "That is something that if we doing for many years and there are many alternatives have been proposed, but that works fairly well.",
            "What it does is it tries to work as much as possible in the convex regime.",
            "So remember when I was saying we don't worry so much about saddles.",
            "So if you have the opportunity of learning metric learning curvature, you learn that as much as possible and then when you fold down to negative curvature you follow the negative curvature direction.",
            "But you don't just pick gradient somewhere and then finding negative curvature direction.",
            "So what it actually does, this inexact Newton method is.",
            "It will take a Newton step in a subspace.",
            "And the subspace is related by the directions of the conjugate gradient method shows.",
            "And those directions happen to be the kernel subspace and they are closely aligned with the eigenvalues of the problem.",
            "Is it's reminiscent of finding the most relevant eigenvalues, little by little, so this is an idea that should not be this may."
        ],
        [
            "No, how do you do this?",
            "More practical?",
            "Write the conjugate gradient method requires all these Hessian vector products in a machine learning problem.",
            "We know how to mini batch gradient.",
            "We can mini batch the Hessian without forming it.",
            "You say, let me assume that the true has entered.",
            "The problem is not the whole sum, it's part of the sun.",
            "In fact, a very small part of the sun, less and less than the mini batch take a very small percentage of that.",
            "So let's do that.",
            "And then I have the equation there.",
            "There is no longer newtons method and inexact Newton method is an inexact subsample Newton method where the Hessian has a sample.",
            "The greatness in mini batch, you can coordinate these two things, and of course a special cases.",
            "If there is no Hessian, you get just mini batch gradient, so you can do a light version of this algorithm, or you can put quite a lot more curvature information into the problem.",
            "Subsample Newton method.",
            "You can solve the equations by conjugate gradients or stochastic gradient, but it's less efficient to solve this by stochastic random.",
            "That's quite interesting question whether.",
            "These two methods compete, but I'm not going to get into that because if I start speaking, it gets complicated.",
            "I think it's time for me.",
            "It's this year 30.",
            "That means that I've spoken.",
            "For one hour.",
            "What did I say in one hour?"
        ],
        [
            "Um?",
            "So I'm going to take a pulse here.",
            "I just show you a pictures that we took.",
            "Unfortunately these are these are Spectra.",
            "Offer for a few problems.",
            "This is just a problem and Nathan so 11 speculation is this.",
            "Accelerated gradient methods and so on are optimal.",
            "When the eigenvalue distribution is bad, the first graph that I have very synthetic problem where the eigenvalues are sort of evenly distributed.",
            "The conjugate gradient method is very bad for that is going to give you the worst case behavior, but when when when there's a drop in the eigenvalues?",
            "The conjugate training method really shines an for these problems.",
            "We see those drops in the eigenvalues and I don't know if anybody has these plots for machine learning, but I'm going to take a break here in order to get.",
            "Questions and complaints.",
            "Yes, go ahead.",
            "There's no microphone.",
            "I'll repeat your question.",
            "Yeah, so so that disciplined way of solving it in exactly guarantees that it's a descent direction.",
            "So every single step, the conjugate gradient method produces.",
            "Is it just interaction?",
            "So when you solve the Newton equations?",
            "By the conjugate gradient method, the first step is the gradient direction.",
            "The second one is said the same direction and so on.",
            "You can prove it very easily and when you get a negative curvature direction then you have to choose a sign.",
            "But it's a nice property of conjugate gradient.",
            "It gives you the same direction.",
            "You don't have to do any of this flipping stuff and so on.",
            "Yeah, nice property of kasigi.",
            "Oh talk about step like yeah.",
            "Yeah.",
            "Yes, the microphone.",
            "I think they didn't hear your question, so alright, so you talked about.",
            "Step length an linesearch has something important to have in order to to know to not overshoot.",
            "The update, but maybe it's OK to reverse with the update if you consider the robust SGD scenario where you say I'm going to only visit the region of solutions corresponding to wide enough valleys.",
            "So, so why care about so much about the very accurate step length and line search?",
            "Fair enough.",
            "OK, so the answer is I don't know because I don't understand the regime that you're talking about, right?",
            "But I want to emphasize one thing.",
            "I'm advocating the line search in order to take two small steps, actually.",
            "Voicemail steps not to take steps that are too small.",
            "To make sure that you.",
            "If you explore enough, the direction that you have some directions, you cannot move very much.",
            "If you have a direction where you can move a lot.",
            "Google not right and there I was referring to the theory community where this stuff is so simple.",
            "In machine learning the step length is.",
            "Who knows what it is?",
            "Because people are just playing with it right?",
            "It's noise suppression.",
            "It's doing lots of things.",
            "Yeah, OK, so related question has to do with all those directions where the eigenvalues are very small.",
            "In fact, if you take the mini batch approximation of the Hessian then most of the directions of zero eigenvalue.",
            "So this idea of using something like Newton which blows up of the low eigenvalue directions seems weird.",
            "No, yes, except that we're not inverting the matrix, but we're applying conjugate gradient to them.",
            "In conjugate gradient starts by moving along the largest eigenvalues first.",
            "So it's a real regularizer.",
            "Applying conjugate gradient solver linear system is very simple, similar to putting A plus gamma.",
            "I you can show that after you have explored that space you have something of that form.",
            "Now with conjugate grains you will know when this step becomes very large right?",
            "But yeah, that's one of the nice properties of that procedure.",
            "Aren't in more trouble?",
            "Thanks just piggyback on your side.",
            "So for your subsampling causing you to method, do you?",
            "Do you guys have a like a damping factor for your when you solving the CG?",
            "No, no, no, we never put it ourselves.",
            "If the problem comes like that with the regularizer, yes.",
            "But we never added.",
            "I see and then the second question is so in those two casieri Jim, do you find you have to turn the other step size and things like that more carefully then the deterministic setup?",
            "Yes.",
            "So for us, what has been a was giving us a lot of trouble?",
            "Is the generalization issue.",
            "Um?",
            "So I mean, at some point you were saying it's not an issue or someone, so we work logistic regression problems and we can try some ideas really nicely there so you can see things up efficiency and so on.",
            "There's no non convexity, but you can train this and then when we go to neural networks that.",
            "Things get mixed up.",
            "So the algorithm is now going really well too along to a not so good solution.",
            "So let's make sure that the increasing the sample size because we do both is slow enough.",
            "Slower now, so it's going to get to the right solution, but it's not going to take all the benefits that we have, right?",
            "So it is very annoying this situation at present, the fact that we don't understand if I understand the issue, we will know how to start correcting and Joshua was saying one of the themes.",
            "I guess of this today is highlighting that question that needs to be answered.",
            "Why different optimization algorithms end up giving you different generalization properties?",
            "If one of you writes a paper.",
            "That has math like a simple intuitive equation doing that.",
            "That's going to be a great paper.",
            "And So what does this paper look like?",
            "It has an equation.",
            "That has a variable amount of batching, right?",
            "And in there and in there you have a generalization.",
            "You're going to testing error.",
            "And maybe you're not going to do a tremendously complicated neural network.",
            "Find the simplest example that is going to convince us about what's going on right, and show that right.",
            "If you move things at a certain rate, generally the final result of generalization is going to be poor.",
            "So I've been asking some of the theoreticians 'cause they they they say, oh, we have an intuition about this.",
            "Give me their math.",
            "There would be a great paper.",
            "His.",
            "He's going to say he wrote the paper already.",
            "So maybe an orthogonal suggestion for people here.",
            "Regarding 2nd order methods and increasing the batch size and all these kinds of explorations.",
            "And in the language I was using before looking for these sharp minima.",
            "Maybe instead of trying to look at.",
            "Classification image classification problems image net where?",
            "It looks like optimization is maybe easier and maybe we don't need these things.",
            "I would suggest people here who are interested in exploring these kinds of methods you talked about.",
            "To look at papers where.",
            "People are only able to solve the optimization problem using fancy tricks like curriculum learning where you.",
            "You go through a series of stages of different tasks that are gradually more difficult, gradually more difficult.",
            "And if you don't do that, you find really bad solutions.",
            "It happens in things like when you have memory, networks and things like that, and it happens a lot in reinforcement learning.",
            "But there are this variance issues and not just bias issues.",
            "I mean optimization issues, but I think it might be.",
            "Worthwhile to focus the empirical exploration of these methods where we already know that.",
            "The optimization problem is hard that that people have trouble even just training to get to fit the data and not not even.",
            "Jonesy yeah.",
            "Very good, I mean another question that I would like to know is why is it that it's turning out to be so simple to train neural networks?",
            "You don't say so simple compared to what so simple compared to general nonlinear objective nonconvex functions where problem in a million variables you couldn't solve.",
            "Now I mentioned that the weather forecast that produced every day is solving optimization problem in 5 million variables.",
            "How can they do that?",
            "A lot of those nonlinear optimization problems are solved with a good initial starting point, so every weather forecast starts with the solution of the previous weather forecast and there are being done all the time.",
            "When you do optimal design of the shape of an airplane wing, don't start with a break right?",
            "You start with a really good solution and what you do is not linear the flow.",
            "And the drag that happens there is not linear right?",
            "And and this is something that is actually so interesting because.",
            "I have called it in the convex optimization community that say people really should only lock the convex problems if you don't have a convex problem, you're doing something wrong.",
            "Model it that way.",
            "Right there.",
            "The engineers will say you know I'm sorry, but the world is not like this.",
            "I'm working on power flow equations.",
            "Then there a scene or I'm actually doing my glasses.",
            "I'm doing progressive glasses and I'm looking at the quality of the glasses.",
            "I change that and it happens to be not convex, right, nonlinear?",
            "But again, what you do is you start with one solution and you want to get.",
            "A better solution.",
            "That seems like a very easy problem, but when the dimensionality goes up it becomes very, very difficult, right?",
            "So you have a million variables.",
            "There are 990,000 ways of going in and not very intelligent way right?",
            "So that's why these curvatures inline search is approved to be useful.",
            "But here as I mean we're reminding us where in the stochastic regime we're doing dealing with stochastic objective functions, so nothing is translating so simply.",
            "But I wanted to convey some of the ideas that we have.",
            "Here, without my pretending that they're going to be necessarily useful or so on, and by the way, I really.",
            "Would welcome collaboration.",
            "Or any post doctoral student who wants to was really interested in the optimization issue, right?",
            "I would really welcome collaboration because we are not experts in.",
            "On your networks or so on.",
            "What is the quota here?",
            "Alright.",
            "White males.",
            "Um?",
            "So there's this interesting hypothesis related to that we discussed over lunch during the lunch break.",
            "About overpressurization lots of papers about parameterisation neural Nets.",
            "There's for example this paper by Jason Uscinski and others.",
            "At the last I clear suggesting that for many of these deep deep Nets.",
            "There's a low low dimensional linear manifold in parameter space in which you can find a solution.",
            "So if this is true of many deep learning problems, how does that you know the things you've been talking about intersect with that potential observation?",
            "Yeah, so my feeling is that it must be the case that there's a meaningful low dimensional manifold, because otherwise you couldn't solve the problems in such a small amount of time.",
            "What is the nature of that manifold?",
            "And so on varies from problem to problem.",
            "Now Roger mentioned at the beginning of the talk limited memory BFGS, which is something that learns information about the problem, but it doesn't do it perfectly, it just looked.",
            "It looks at.",
            "It works because in a lot of the problems there are some manifold that is the really relevant manifold, and if you talk to a climate scientist, he's going to tell you a lot of what this is.",
            "It has to do with fundamental modes of how the atmosphere goes, blah, blah, blah, and other areas there like that they're not.",
            "The problems are not such that.",
            "Every variable matters, right?",
            "So the only thing I can say to answer your questions in must exist.",
            "Now I know that in some conference you had French guy talking about manifold.",
            "Optimization and so on.",
            "But there's a huge there was a huge gap between that theory and and our reality.",
            "I believe that the K fact people here listening to their explanation.",
            "There are as close as they are about thinking about this problem.",
            "I also want to make it another compliment to Jenny's talk.",
            "A large dimensional problem the other way you can solve it through the great.",
            "You don't have a grain, you cannot solve it.",
            "But in almost everything that I've been talking about here, I'm talking about generic problems.",
            "And he's talking about the structure of a neural network.",
            "He's looking for me for most problems, the computational graph the backpropagation finishes at the note at the leaves you propagate the variables you finish, and then you've done.",
            "It's a black box model.",
            "So what I really like is start looking at a neural network in a more structured way and I think this shampoo paper does that too.",
            "But this neural natural gradient idea seems to be a more fundamental way of.",
            "Attacking things.",
            "So because a lot of your questions come to the specifics right?",
            "I'm talking about sort of generic ideas of optimization that hopefully I'm sure that some of the obvious we're developing here, by the way, are going to be useful by.",
            "Somewhere, somewhere, subsampling you can progressive budget, I'm sure they're going to be useful, But then in this specific setting, I think what you guys are doing is the right thing.",
            "Looking at what happens to specific structures and then you look at the Chronicle structure an oh that's that's interesting, right?",
            "The inverse of the of the Chronicle product is the inverse of the product.",
            "Everything is nice except for that expectation term that that bothers me.",
            "It doesn't.",
            "Yes.",
            "So my question is so not ignoring the power of these methods, how about the trust region methods?",
            "How about the convex approximation methods?",
            "Because I've seen some power about them, but I don't see anybody is using them.",
            "In the Northern Machine learning community.",
            "Yeah, trust region methods are.",
            "It's a very important class of algorithms, right?",
            "So you create a quadratic model of the objective function.",
            "And you minimize your objective in depth term.",
            "If the function is convex.",
            "You don't need a trust region.",
            "You want to do a line search instead.",
            "But if the function is not convex saddle or so on, then you say I have a quadratic model of the objective function.",
            "Based on information I gathered, where should I go right?",
            "Where should I move?",
            "So go to the minimum of this quadratic model in a region that somehow you developed is the Taylor.",
            "At Taylor region that you trust, right?",
            "So you minimize them all in that domain.",
            "Is getting more popular in the theory community to look at trust Vision argues, especially because they are related to nesters.",
            "Cubic regularization algorithm is very similar to this Nestor of cubic regularization method.",
            "The one thing the one weakness and I was a big fan of.",
            "Trust region algorithms.",
            "And by the way, we have software for constrained optimization that called Nitro that is used by many many people and they may not use the trust region technique because a lot of constraint problems are not convex.",
            "The problem with the trust region is twofold.",
            "One of them is is variable.",
            "So is this the same problem about giving the same importance to all the directions and the other one is that it's scale dependent?",
            "If you change the variables of the problem, you should change the trust region, but we don't know how to do that.",
            "It's done out of phase.",
            "So those are good algorithms, but I found that there are two depends on the choice of the trust region.",
            "So that you can see, G algorithm is usually implemented with the trust region.",
            "But you could do it without a trust region if you want.",
            "But yeah, those are.",
            "And in our book we have a long chapter, but.",
            "That idea of trust region, which is quite important, Lee.",
            "It's all about non convexity.",
            "Somebody here?",
            "It looks like Georgia could interrogate me for another 24 hours.",
            "I take just take the opportunity.",
            "I don't know if it's interrogating or saying well, it's not.",
            "It's not like this, it's like that.",
            "Yes, yeah, you mentioned in the beginning of the talk that it's better to increase the batch sized rather than diminishing the learning rate.",
            "And then you talked about the methods that have adaptive batch sizes and adaptive learning rates.",
            "Is there like any relationship between these two types of methods given the.",
            "The fact that.",
            "Increasing batch size is apparently better than diminishing learning rate.",
            "I don't know if my I made myself clear.",
            "Yeah, well there are a couple of groups.",
            "There may be more who say that it is beneficial to increase the batch size.",
            "And so when you went in the past, you thought it was time to decrease the.",
            "Steppling it's time to increase the batch size.",
            "And increases Stephanie.",
            "You know, with an increasing Bachelor you get better, so that's what they did in those couple of papers.",
            "But in a limited way.",
            "And they call it the warm up phase or something like that.",
            "And they're claiming hey paper, they have a face where they do that and mini batches being used for awhile.",
            "And then it's increased in the back and the step length is increased and they do that again and they do that in a step way.",
            "If that answers your questions, the two of them are tight together.",
            "And of course, this is all this is not about how far to go.",
            "It's all about noise suppression, right?",
            "And their motivation?",
            "From what kind man told me is to do better optimization to do optimization faster.",
            "So.",
            "Yo she was asking about generalization issues, right?",
            "I was talking because I'm not worried about that.",
            "I want I'm worried about optimization.",
            "I want to do the optimization faster.",
            "That's why he's doing larger mini batches, right?",
            "Different groups.",
            "You started your talk with these different axes, and one of them was.",
            "Moving in the Newton direction so stochastic Newton like methods and I wasn't sure by the end if you had really.",
            "Fill that spot with something.",
            "Mean this so there's the key fact type methods.",
            "Is there some?",
            "Is there like really you successful stochastic Newton version that we can use from your net?",
            "That stochastic Newton method was very noisy gradients that we tried to do does not work.",
            "But what is not there?",
            "There is more slides that you don't want to see because I have like a million slides.",
            "I realize this is such a rich topic.",
            "Is.",
            "You can do quasi Newton.",
            "As long as the batch is increased and much less than what we expected, don't try to do quasi Newton in the very noisy regime.",
            "But once the batch size hasn't depending on the problem is 800 or something like that.",
            "Not only can we do question Newton, but were actually employing line searches to our benefit, so that's a paper that we published in acnl progressive batching question is an algorithm that I find very exciting because the numbers look very good numbers that I've never seen before.",
            "But for your question there on that axis of 2nd order methods.",
            "There's a K FAQ.",
            "Inexact Newton method with subsampling.",
            "So the general algorithm and the third daughter and they would put their is causing human algorithms.",
            "But only within the progressive approach, where the gradients of good quality and by the way we have to do some robust without a paper about how to do robust optimization given that the sample size is changed and your brother has recently or last year paper about synchronous optimization where they are worrying about fault tolerance, it turns out it was pretty much the same ideas that if we use those we can do stable question.",
            "You can updating so there are three methods along those lines.",
            "And by the way, along the other line of suppressing noise.",
            "There's a whole bunch of algorithms, class of items that I did not mention.",
            "Those are the aggregated gradient methods Sag's, V, RG saga.",
            "This is a beautiful algorithm to sign for the some finite some problem.",
            "And they at the expense of storage.",
            "In the case of sag, they can get a faster rate of convergence.",
            "But I don't know that these arguments will ever be useful in machine learning.",
            "The reason why I did not consider them here is because I think even though they are among the most original ideas that people have come up with Mark Schmidt, Canadian.",
            "What's that happens in Canada?",
            "March mid will initiator of those ideas.",
            "I really designed for finite sum problem or that they will do well if you look at generally testing error.",
            "They're not so good, but those noise suppressing.",
            "They also variance.",
            "SV RG is stochastic variance reduction method.",
            "Very good way to do things.",
            "So the amount of solid theoretical work around tensors is still very limited, and.",
            "Do you think the advancement in that field will help develop modern optimization techniques that can?",
            "Like fat with faster convergence rates and with for example, currently we haven't thought much beyond second order derivatives because that's perhaps the most we can deal with, but do you think the advancements in that field can result in faster optimization techniques?",
            "OK, I think your question has two branches in the machine learning in the neural network context, tensors occur naturally, so exploiting them is a natural thing to do.",
            "Now there's a whole field called tensor tensor linear algebra that has to do with multi attributes.",
            "That's another interesting area on how you know how you represent tensors.",
            "How you work with them.",
            "Now you asked me if to predict if you think it's good.",
            "It's a good area to explore.",
            "Since I'm not in machine learning, I will not make any predictions.",
            "All my opinions are related to what is interesting to study, not whether they're going to be successful or not.",
            "I think both versions of tensors are worth further study.",
            "Thank you.",
            "4 minutes and 11 seconds."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you Roger, and thanks very much for the invitation.",
                    "label": 0
                },
                {
                    "sent": "Coming here is a pleasure to be with all the young people I remember being at a workshop in 2012 and I was telling Roger, you know, some of the people were at that board trip.",
                    "label": 0
                },
                {
                    "sent": "I think they are not pretty prominent and he said I was there.",
                    "label": 0
                },
                {
                    "sent": "So fun.",
                    "label": 0
                },
                {
                    "sent": "I've learned a lot from you in this workshop and I hope I bring a little bit of a different perspective from the point of view of optimization.",
                    "label": 0
                },
                {
                    "sent": "I think something really interesting happened today.",
                    "label": 0
                },
                {
                    "sent": "You are having three lectures, one by someone who's coming from the theoretical computer science theoretical machine learning community, and it's talking about optimization and machine learning.",
                    "label": 0
                },
                {
                    "sent": "Then you have.",
                    "label": 0
                },
                {
                    "sent": "Jimmy Ball was really in the middle of the machine learning community, talking about optimization.",
                    "label": 0
                },
                {
                    "sent": "And now I'm coming as someone would looks at optimization in a very broadview and trying to assist.",
                    "label": 0
                },
                {
                    "sent": "Still some ideas that we have from this bigger experience and how it could be relevant here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so first of all, thanks to some of my students who I learn from and they read a lot of papers for me.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are different perspectives to optimization.",
                    "label": 1
                },
                {
                    "sent": "One of them is in the beginning, there is the SGD method.",
                    "label": 1
                },
                {
                    "sent": "This is where everything starts, right?",
                    "label": 0
                },
                {
                    "sent": "If you do machine learning, you start from there.",
                    "label": 0
                },
                {
                    "sent": "And why is it that everything starts with the SGD method?",
                    "label": 0
                },
                {
                    "sent": "Because of this graph that I have right there everybody has seen that you run a batch method, run SGD even are logistic regression problem and the SVD went by large margin.",
                    "label": 0
                },
                {
                    "sent": "Right, so everything should spectrum is GD the other perspective is no, no every discussion should start from Newton's method.",
                    "label": 0
                },
                {
                    "sent": "Becausw if you have an objective function like this and most objective functions look like that, why would you like to go along the gradient direction, which is not very intelligent.",
                    "label": 0
                },
                {
                    "sent": "You want to do something a little bit better.",
                    "label": 0
                },
                {
                    "sent": "In fact, as Jamie was pointing out, the steepest descent method is going to sag, and.",
                    "label": 0
                },
                {
                    "sent": "You often hear about complexity, results about optimization algorithms.",
                    "label": 0
                },
                {
                    "sent": "Sometimes these complexity results never happened.",
                    "label": 1
                },
                {
                    "sent": "The worst case never happens, so the results are overly pessimistic.",
                    "label": 0
                },
                {
                    "sent": "In this case, the worst case result is the average result.",
                    "label": 0
                },
                {
                    "sent": "This will always happen.",
                    "label": 0
                },
                {
                    "sent": "Grandmother always do like that, so therefore there's an argument of saying every discussion should start with Newton Smith.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so just to give you a little bit of a history and I know this lecture is being recorded, so some of my colleagues in the Russian School and may be reacting to what I'm going to say here, right?",
                    "label": 0
                },
                {
                    "sent": "But there was a time when the world was really divided.",
                    "label": 0
                },
                {
                    "sent": "There was not communication between the two blocks.",
                    "label": 0
                },
                {
                    "sent": "And the Russian school focused on convexity on 1st order methods.",
                    "label": 1
                },
                {
                    "sent": "An gradient projection method.",
                    "label": 0
                },
                {
                    "sent": "That's what they were doing, and they were doing fantastic theoretical work there.",
                    "label": 1
                },
                {
                    "sent": "Not everything was first order method in theoretical they were playing with ideas that eventually led to interior point methods.",
                    "label": 0
                },
                {
                    "sent": "Dick in.",
                    "label": 1
                },
                {
                    "sent": "In the beginning Cassian and then at Berkeley.",
                    "label": 0
                },
                {
                    "sent": "The final results were done, but just the Western School.",
                    "label": 0
                },
                {
                    "sent": "In contrast, during the 60s and 70s was focusing on 2nd derivative methods.",
                    "label": 0
                },
                {
                    "sent": "Why do gradient methods?",
                    "label": 0
                },
                {
                    "sent": "Convergence rates against complexity.",
                    "label": 0
                },
                {
                    "sent": "If I have a nonconvex problem which is what happens here in machine learning, usually get a reasonable starting point and what you want to know is how fast are you going to get to the solution, not what is the worst number of iterations that may happen if you start anywhere.",
                    "label": 0
                },
                {
                    "sent": "So I was focusing on rates of convergence, non convexity from the beginning, a lot of the problems were coming from chemical engineering power systems, there were non convex.",
                    "label": 0
                },
                {
                    "sent": "You have to do something about that.",
                    "label": 0
                },
                {
                    "sent": "You just deal with this and the other aspect that happened in the Western School was not popular in Russia.",
                    "label": 0
                },
                {
                    "sent": "Is that the ultimate goal of whatever you're doing?",
                    "label": 1
                },
                {
                    "sent": "Should be some software and public domain software in there, everybody could contribute to the ideas.",
                    "label": 0
                },
                {
                    "sent": "Now this is a bit of an oversimplification.",
                    "label": 0
                },
                {
                    "sent": "In the West they were great theoreticians like Rockefeller doing awesome with organization and so on.",
                    "label": 0
                },
                {
                    "sent": "But still what I'm talking about here has relevance today because in my opinion there is a big disbalance that has happened.",
                    "label": 0
                },
                {
                    "sent": "In nonlinear optimization community, there's a lot of theoretical work about how to deal with non convexity that deals with algorithms that, in my opinion, nobody would ever use and complexity results that are way too pessimistic, right?",
                    "label": 0
                },
                {
                    "sent": "So somehow the Russians cool is taking a great hold on on this community here.",
                    "label": 1
                },
                {
                    "sent": "Now, both schools considered stochastic optimization, which is what occurs in machine learning.",
                    "label": 0
                },
                {
                    "sent": "As you can see from those very famous names and so therefore I'm going to be focusing a lot in trying to look at the correlations and differences between stochastic optimization problems.",
                    "label": 0
                },
                {
                    "sent": "In deterministic optimization you will notice that speakers often cross the line they start talking about is GD, and then they draw a picture of a deterministic method.",
                    "label": 0
                },
                {
                    "sent": "Doing something right.",
                    "label": 0
                },
                {
                    "sent": "So we do that and it can be productive.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then it can be dangerous, so large scale nonlinear optimization optimization is of a large field, right?",
                    "label": 0
                },
                {
                    "sent": "There's linear programming is integer programming.",
                    "label": 0
                },
                {
                    "sent": "There's robust, so many areas where the one area is nonlinear optimization, non convex and large scale, and that's what I've been doing my whole career.",
                    "label": 0
                },
                {
                    "sent": "This is a very well research research area where the software is all over the place is being used for optimal trajectory problems, optimal design, etc.",
                    "label": 0
                },
                {
                    "sent": "Stochastic optimization in the sense of machine learning in the sense of classification and so on, deals with random variables.",
                    "label": 0
                },
                {
                    "sent": "There's randomness in the objective function in this area has developed by borrowing some ideas from deterministic optimization.",
                    "label": 0
                },
                {
                    "sent": "For instance, momentum gradient descent itself.",
                    "label": 1
                },
                {
                    "sent": "When Robbins came up with that algorithm.",
                    "label": 0
                },
                {
                    "sent": "Robinson Monroe analogy to this key percent method.",
                    "label": 0
                },
                {
                    "sent": "But momentum and people talk about preconditioning and so on.",
                    "label": 0
                },
                {
                    "sent": "His ideas are come from the terministic world and service inspiration.",
                    "label": 0
                },
                {
                    "sent": "Now, this exchange of ideas is not straightforward.",
                    "label": 1
                },
                {
                    "sent": "You cannot come up with some great algorithm that we have for doing airplane design.",
                    "label": 1
                },
                {
                    "sent": "And moving into this domain here, becauses stochastic approximation methods are Markov processes.",
                    "label": 0
                },
                {
                    "sent": "They are not the same type of iterations.",
                    "label": 0
                },
                {
                    "sent": "There's a difference in nature between this and SGD algorithm.",
                    "label": 0
                },
                {
                    "sent": "In a batch gradient method, they're really very different things.",
                    "label": 1
                },
                {
                    "sent": "However, there is a continuum between the two worlds.",
                    "label": 0
                },
                {
                    "sent": "If you take an algorithm, start increasing the batch size, it becomes at some point it becomes gets into the terministic world, and all these things that work in the deterministic world can be used there.",
                    "label": 0
                },
                {
                    "sent": "So you conversate continuum between.",
                    "label": 0
                },
                {
                    "sent": "SGD algorithms to the gradient algorithm there and a lot of people are exploring what's happening over there because strange things are happening when you're moving to those domain.",
                    "label": 0
                },
                {
                    "sent": "So the interplay between these two worlds, the terministic run the stochastic world is ongoing and it's going to be one of the main subject of my lecture trying to bring some of those ideas here.",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "As far as I can see, some of the new algorithms that are being produced for solving.",
                    "label": 0
                },
                {
                    "sent": "Training neural networks are coming from the machine learning community.",
                    "label": 0
                },
                {
                    "sent": "And it's exciting to see young people doing that, so there is a continuum between the two worlds stochastic in the term.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mystic alright, So what happens in these two worlds and I have to emphasize, these are pictures about large scale, right large scale nonlinear optimization.",
                    "label": 0
                },
                {
                    "sent": "That's neural networks.",
                    "label": 0
                },
                {
                    "sent": "That's not support vector machines.",
                    "label": 0
                },
                {
                    "sent": "This neural networks and on the other hand.",
                    "label": 0
                },
                {
                    "sent": "Climate, the weather forecasting problem, and deterministic problem also within million variables, 5 million variables.",
                    "label": 0
                },
                {
                    "sent": "Very difficult neural network, very difficult, both nonconvex.",
                    "label": 0
                },
                {
                    "sent": "What are the algorithms that people are using?",
                    "label": 0
                },
                {
                    "sent": "Well, as you know, for the stochastic for training neural networks you have is GD yes.",
                    "label": 0
                },
                {
                    "sent": "The other dimensionalities are pretty similar.",
                    "label": 0
                },
                {
                    "sent": "It depends.",
                    "label": 0
                },
                {
                    "sent": "I mean there there optimization problems of all sides, but say weather for the current weather forecasting systems right now probably 5 to 10,000,000 variables and their design problems.",
                    "label": 0
                },
                {
                    "sent": "Also millions of variables I have to say that when I was doing my PhD I really wanted to focus on large scale optimization which meant problems with 1000 variables.",
                    "label": 0
                },
                {
                    "sent": "And then, but they quickly, we ended up developing algorithms that were linear.",
                    "label": 0
                },
                {
                    "sent": "The dimension of the problem.",
                    "label": 0
                },
                {
                    "sent": "So we were scaling up the problems in a million variables, and in fact the reason why during the 1990s I was involved in the development of weather forecasting systems is because they needed someone who knew how to solve problems with hundreds of thousands of variables.",
                    "label": 0
                },
                {
                    "sent": "And there we struggled for a long time.",
                    "label": 0
                },
                {
                    "sent": "And today all the weather forecasts are being run by an optimization arm that loops.",
                    "label": 0
                },
                {
                    "sent": "Around very much like backpropagation, the key idea there tools also to get derivatives.",
                    "label": 0
                },
                {
                    "sent": "There's no way you're going to do large scale optimization without derivatives, right?",
                    "label": 0
                },
                {
                    "sent": "So in that in that domain weather forecasting people have this flow equations navier Stokes equations, how the atmosphere develops, and then you want to know what the derivative is with respect to changes in the initial conditions.",
                    "label": 0
                },
                {
                    "sent": "And then you can do by something called the adjoint method, which is very similar to the reverse mode of differentiation.",
                    "label": 0
                },
                {
                    "sent": "So you have these two worlds.",
                    "label": 0
                },
                {
                    "sent": "Inventing Brett backpropagation.",
                    "label": 0
                },
                {
                    "sent": "At the same time also Hessian vector products at the same time don't not talking to each other, I'm standing in the middle and I should have done better.",
                    "label": 0
                },
                {
                    "sent": "But we just you know that that happened and it's pretty spectacular.",
                    "label": 0
                },
                {
                    "sent": "What is been accomplished in both domains?",
                    "label": 0
                },
                {
                    "sent": "OK, so the algorithms then that are trained for machine learning are there and I find it very interesting to hear about the natural gradient method because.",
                    "label": 0
                },
                {
                    "sent": "This is this is consistent with the direction that I've been talking about for many years about moving algorithms.",
                    "label": 0
                },
                {
                    "sent": "In any case, in the deterministic large scale world.",
                    "label": 0
                },
                {
                    "sent": "People use question.",
                    "label": 0
                },
                {
                    "sent": "You can methods.",
                    "label": 0
                },
                {
                    "sent": "Inexact Newton methods may be nonlinear conjugate gradient methods, and basically the iterations look like what you have there.",
                    "label": 1
                },
                {
                    "sent": "They just differ by the fact that there is a matrix in front of the gradient.",
                    "label": 0
                },
                {
                    "sent": "It's not a diagonal.",
                    "label": 0
                },
                {
                    "sent": "In our world, there's never a diagonal.",
                    "label": 0
                },
                {
                    "sent": "Nobody knows how to choose a diagonal.",
                    "label": 0
                },
                {
                    "sent": "So that could be a quasi Newton approximation or could be implicitly Hessian matrix in a subspace which would be given to you by Newton conjugate gradient iteration.",
                    "label": 0
                },
                {
                    "sent": "There's a difference between the two worlds, so we're saying many first order methods being used in machine learning.",
                    "label": 1
                },
                {
                    "sent": "There is always the plane with the step length.",
                    "label": 0
                },
                {
                    "sent": "The iterations are expensive in noisy, and maybe one has to deal with concepts like a Fisher information matrix or not, not the his here.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                },
                {
                    "sent": "In the other world, the simple gradient method is never used.",
                    "label": 0
                },
                {
                    "sent": "Um, acceleration and momentum is never used that I know those are considered to be theoretical constructs.",
                    "label": 0
                },
                {
                    "sent": "Why aren't they use?",
                    "label": 0
                },
                {
                    "sent": "Because just by getting great information you can actually use a question.",
                    "label": 0
                },
                {
                    "sent": "Newton algorithm only requires great information.",
                    "label": 0
                },
                {
                    "sent": "And for quadratic functions those are not optimal.",
                    "label": 0
                },
                {
                    "sent": "You can use conjugate gradients.",
                    "label": 0
                },
                {
                    "sent": "For convex functions you can use across in your algorithm, and people have discovered over the decades of these algorithms work better, so they're not used.",
                    "label": 0
                },
                {
                    "sent": "Something that is used and is really important is align search, and I'm going to talk a lot about that now.",
                    "label": 0
                },
                {
                    "sent": "Hessian vector products are units at one deals with and, so that's the way things.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to make it appear abit more concrete discussion about the interplay between the two communities, let me talk about momentum.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let me advocate a proposition here so the heavy ball method people have described it there and my name comment is beware of two dimensional pictures.",
                    "label": 1
                },
                {
                    "sent": "Sometimes they're really useful most of the times they are not.",
                    "label": 0
                },
                {
                    "sent": "Right, if you have a problem in 100 variables, this may not be very useful, right?",
                    "label": 0
                },
                {
                    "sent": "But you've seen this picture momentum.",
                    "label": 0
                },
                {
                    "sent": "Why does it work?",
                    "label": 0
                },
                {
                    "sent": "Well, you take a gradient step, you take another gradient step and there's a drift.",
                    "label": 0
                },
                {
                    "sent": "Therefore, why don't we just move along the drift, right?",
                    "label": 0
                },
                {
                    "sent": "It makes a lot of sense until you say, well why the drift of just the last two points?",
                    "label": 0
                },
                {
                    "sent": "Why not the last two directions or four directions?",
                    "label": 0
                },
                {
                    "sent": "White just one drift, right?",
                    "label": 0
                },
                {
                    "sent": "It's the mathematics.",
                    "label": 0
                },
                {
                    "sent": "Is there?",
                    "label": 0
                },
                {
                    "sent": "The intuition is not there.",
                    "label": 1
                },
                {
                    "sent": "In fact, remembering all the drift is what the conjugate training method does.",
                    "label": 0
                },
                {
                    "sent": "But so this intuition is false.",
                    "label": 0
                },
                {
                    "sent": "This is a 2 dimensional picture that says that intuition because I want to give you another picture where the opposite happens.",
                    "label": 0
                },
                {
                    "sent": "Momentum just is the worst.",
                    "label": 0
                },
                {
                    "sent": "The wrong thing.",
                    "label": 0
                },
                {
                    "sent": "Now if you choose those parameters in the way that I have there, which means that you have some important knowledge about the objective function, the condition, number of the problem, you need those parameters there for the momentum method to actually make sense.",
                    "label": 1
                },
                {
                    "sent": "That's why your intuition is not really.",
                    "label": 1
                },
                {
                    "sent": "Right, because your intuition would never tell, is the condition number.",
                    "label": 0
                },
                {
                    "sent": "If you put numbers are way off that there's no benefit to this.",
                    "label": 1
                },
                {
                    "sent": "OK, so momentum is serving inspiration and is used a lot in for training neural networks, but neural networks are not quadratics.",
                    "label": 0
                },
                {
                    "sent": "So what's going on right?",
                    "label": 0
                },
                {
                    "sent": "In fact, the Gray matter with momentum cannot even be shown to be convey.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For non convex functions.",
                    "label": 0
                },
                {
                    "sent": "And here is a picture, just one picture.",
                    "label": 0
                },
                {
                    "sent": "I said that don't look at 2D pictures, but at least here.",
                    "label": 0
                },
                {
                    "sent": "Here's a 2D picture where X0 is a starting point.",
                    "label": 0
                },
                {
                    "sent": "You move to X1, then you take a gradient step.",
                    "label": 0
                },
                {
                    "sent": "The two blue steps are radiant steps.",
                    "label": 0
                },
                {
                    "sent": "In this case, momentum is something well, the orange dotted line.",
                    "label": 0
                },
                {
                    "sent": "Their momentum is actually the first direction that you move, and you see that from X22X2.",
                    "label": 0
                },
                {
                    "sent": "Till then you just made things worse, right?",
                    "label": 0
                },
                {
                    "sent": "And what I did here is I constructed and non convex function.",
                    "label": 0
                },
                {
                    "sent": "Where things just go back, but what we really have to take into consideration, we were doing neural networks.",
                    "label": 0
                },
                {
                    "sent": "We're dealing with a very large dimensional space, so these drifts and these things.",
                    "label": 0
                },
                {
                    "sent": "How do they work right?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nevertheless, so I just said there's a problem.",
                    "label": 0
                },
                {
                    "sent": "Momentum is work in practice, and my understanding is used almost routinely.",
                    "label": 1
                },
                {
                    "sent": "Fortify the stochastic gradient method.",
                    "label": 0
                },
                {
                    "sent": "Maybe this started with this paper here, but my conjecture is that this really does not have anything to do with momentum.",
                    "label": 0
                },
                {
                    "sent": "What is momentum?",
                    "label": 0
                },
                {
                    "sent": "To me, momentum has to do with the linear dynamical system with friction and the friction is where the momentum comes from.",
                    "label": 1
                },
                {
                    "sent": "So momentum is explained in terms of math Nesterov's acceleration has no intuition.",
                    "label": 0
                },
                {
                    "sent": "Maybe for Jimmy it does not.",
                    "label": 0
                },
                {
                    "sent": "For me.",
                    "label": 0
                },
                {
                    "sent": "There's math.",
                    "label": 0
                },
                {
                    "sent": "It works there, right?",
                    "label": 0
                },
                {
                    "sent": "So I think that my position is that what works works.",
                    "label": 0
                },
                {
                    "sent": "Nobody can deny the momentum works, but you have to think about it another way.",
                    "label": 0
                },
                {
                    "sent": "It's what is happening is doing iterative averaging or gradient averaging or something like that.",
                    "label": 0
                },
                {
                    "sent": "And momentum was just an inspiration.",
                    "label": 0
                },
                {
                    "sent": "People are playing with those parameters and as long as you play with those parameters you really moving from the original formulation of the problem.",
                    "label": 0
                },
                {
                    "sent": "So therefore Jimmy Jimmy's presentation today was very relevant to this.",
                    "label": 0
                },
                {
                    "sent": "How do you?",
                    "label": 0
                },
                {
                    "sent": "Average information as you go along, so the practice between our understanding and algorithm in practice is is lacking, and in fact almost every argument you would make for momentum you could do for the conjugate gradient method, which is not very popular in machine learning, but it's a very important algorithm, because this algorithm that I wrote there, the conjugate gradient method, has pretty much the same form as momentum, but the parameters Alpha and beta are given at every iteration they change.",
                    "label": 1
                },
                {
                    "sent": "You can compute them very easily via dot product.",
                    "label": 0
                },
                {
                    "sent": "You have you have required no knowledge of the condition, number of the problem.",
                    "label": 0
                },
                {
                    "sent": "So here you have the momentum method and its knowledge.",
                    "label": 0
                },
                {
                    "sent": "Lot of knowledge about the quadratic function.",
                    "label": 0
                },
                {
                    "sent": "Here you have a conjugate gradient method knows nothing about the quadratic function and conjugate gradient method is in the worst case just as good as momentum, normally much better than that.",
                    "label": 0
                },
                {
                    "sent": "Now it's not used if you take those parameters there and you start fooling around with them is no longer the conjugate gradient method.",
                    "label": 0
                },
                {
                    "sent": "It's it's nothing, right?",
                    "label": 0
                },
                {
                    "sent": "So if you put that in a noisy regime in so I will mention a little bit later.",
                    "label": 0
                },
                {
                    "sent": "Contrary method breaks down.",
                    "label": 0
                },
                {
                    "sent": "Yes, Joshua.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is the deterministic the stochastic conjugation method.",
                    "label": 0
                },
                {
                    "sent": "I don't know that it exists.",
                    "label": 0
                },
                {
                    "sent": "So, but since we're moving from 1, right?",
                    "label": 0
                },
                {
                    "sent": "So the other one I'm not going to tell you, I'm now in a I'm now in B.",
                    "label": 0
                },
                {
                    "sent": "You have to figure it out right, but I'm going to make a big point about the fact that probably the next like conjugation is going to breakdown with noise if you do it this way.",
                    "label": 0
                },
                {
                    "sent": "OK, and I have in my website there is a nonlinear conjugate gradient code.",
                    "label": 0
                },
                {
                    "sent": "If anybody wants to play with that, that nonlinear conjugate gradient can be used for nonlinear functions, and Steve Wright was telling me recently getting good results by using nonlinear.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great, alright, so Nesterov acceleration is another way of doing things.",
                    "label": 1
                },
                {
                    "sent": "It's really remarkable result.",
                    "label": 0
                },
                {
                    "sent": "It is amazing that one can get a better algorithm so simply, but is it relevant in practice Now there's a famous album called Fiesta for L1 optimization.",
                    "label": 1
                },
                {
                    "sent": "There people have used it a lot, whether it's really better than just the original Easter algorithm is not clear, so these methods require some knowledge of the objective function.",
                    "label": 1
                },
                {
                    "sent": "There are many complexity papers about.",
                    "label": 0
                },
                {
                    "sent": "These algorithms, so we see many, many papers about.",
                    "label": 0
                },
                {
                    "sent": "Nesterov acceleration.",
                    "label": 0
                },
                {
                    "sent": "In the non convex case, what does he do to try to escape from saddle points?",
                    "label": 0
                },
                {
                    "sent": "An combination with a coordinate descent method?",
                    "label": 0
                },
                {
                    "sent": "There are many many papers like that and they lead to complexity results and I was saying some complexity results are way too pessimistic.",
                    "label": 0
                },
                {
                    "sent": "And so Jeff was talking this morning about, you know, identifying a saddle point.",
                    "label": 0
                },
                {
                    "sent": "Or epsilon.",
                    "label": 0
                },
                {
                    "sent": "Required you will require thousands thousands of iterations to obtain it or the epsilon result.",
                    "label": 0
                },
                {
                    "sent": "This pessimistic theory.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's skip that.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the classical conjugate gradient method, which is to think of beauty because I think it's the best thing for best iterative method for solving a quadratic.",
                    "label": 0
                },
                {
                    "sent": "I mean there's an optimal properties is going to breakdown with noise.",
                    "label": 0
                },
                {
                    "sent": "And there have been a number of papers looking, and I know here in Toronto, now is the same thing happening.",
                    "label": 0
                },
                {
                    "sent": "Revisiting momentum and saying why are we doing this?",
                    "label": 0
                },
                {
                    "sent": "And there's this paper that I mentioned there that shows that when there is noise, momentum provides no benefits.",
                    "label": 1
                },
                {
                    "sent": "Even for linear regression.",
                    "label": 0
                },
                {
                    "sent": "And for Nestor's algorithm, when you inject noise, they empirically find there in that she rates very quickly.",
                    "label": 0
                },
                {
                    "sent": "So in any case, I just want to finish our discussion about momentum by saying I'm sure those ideas work, because if something works, it works, but it's it's because there are some statistics behind that.",
                    "label": 0
                },
                {
                    "sent": "It's some optimization combined with the statistics.",
                    "label": 0
                },
                {
                    "sent": "Think about it this way.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying I'm right, I'm just saying this is something like advance, right?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the summer school you're here for learning understanding is Judy.",
                    "label": 0
                },
                {
                    "sent": "Do we understand this GD?",
                    "label": 0
                },
                {
                    "sent": "Well, people put these tables right.",
                    "label": 0
                },
                {
                    "sent": "Complexity of results.",
                    "label": 0
                },
                {
                    "sent": "Do we understand them?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is the most basic question?",
                    "label": 0
                },
                {
                    "sent": "Do you know why SGD converges?",
                    "label": 0
                },
                {
                    "sent": "And for what classes of functions can we say that it converges?",
                    "label": 1
                },
                {
                    "sent": "And once you have that, do they include neural networks, deep neural networks?",
                    "label": 0
                },
                {
                    "sent": "What happens if you have values and there's non differentiability?",
                    "label": 0
                },
                {
                    "sent": "Do those results apply?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to have two equations during this lecture.",
                    "label": 0
                },
                {
                    "sent": "Two really important equations, one that is coming now and one related to Newton Smith.",
                    "label": 0
                },
                {
                    "sent": "So that really important relation is this.",
                    "label": 0
                },
                {
                    "sent": "If you aren't deterministic optimization and the easyworld.",
                    "label": 0
                },
                {
                    "sent": "You have a proof that the gradient method always converges to a stationary point and the proof is like 5 lines, and immediately you discover the following relation.",
                    "label": 0
                },
                {
                    "sent": "The decrease in the objective function, right?",
                    "label": 0
                },
                {
                    "sent": "So the new value should be smaller than the new one, so there is a decrease in the objective function is proportional to the grading.",
                    "label": 0
                },
                {
                    "sent": "Right, if you're minimizing a function when you're away from that, just get a big decrease when you're close to the solution.",
                    "label": 0
                },
                {
                    "sent": "You cannot get a big decrease, so that relation by itself is telling you the gradient method is pushing the objective function in proportion to the gradient and is always going to work unless you do something bad with the stepping right.",
                    "label": 0
                },
                {
                    "sent": "So the step link is bad, then then you'll spoil all of this, but we will see that it's easy to know what you do with Stephanie.",
                    "label": 0
                },
                {
                    "sent": "Now, what happens when you have stochastic optimization so SGD?",
                    "label": 0
                },
                {
                    "sent": "Now the problem no longer looks like this nice optimization problem.",
                    "label": 0
                },
                {
                    "sent": "I wrote that the parameters W an random variable sign depends on the choice of the data that you're using.",
                    "label": 0
                },
                {
                    "sent": "So now the relationship is this.",
                    "label": 0
                },
                {
                    "sent": "The decrease in the objective function has to be measured in expectation, right?",
                    "label": 0
                },
                {
                    "sent": "You cannot say for sure because it's a stochastic algorithm.",
                    "label": 0
                },
                {
                    "sent": "It has a first term, which is exactly the same as the classical gradient method, and then it has the second term that has to do with this.",
                    "label": 0
                },
                {
                    "sent": "Second term is the variance of your stochastic approximation, that is a stochastic gradient that I wrote by that notation there.",
                    "label": 0
                },
                {
                    "sent": "So there are two terms that are playing against each other.",
                    "label": 1
                },
                {
                    "sent": "If the grading is unbiased, is stochastic gradient method is an unbiased estimator?",
                    "label": 0
                },
                {
                    "sent": "You have the same force down.",
                    "label": 0
                },
                {
                    "sent": "But because you're not choosing the right grade and there is a noise level and that is preventing you from converging to the solution and early on in 1951 already, Robbins and Monro noted that one term is an Alpha, the other one is in Alpha squared.",
                    "label": 0
                },
                {
                    "sent": "Therefore you can converge if you make Alpha go to zero and they have the proof in 1951 and after that you can get rates of convergence results.",
                    "label": 0
                },
                {
                    "sent": "So there are two key algorithm components to SGD.",
                    "label": 0
                },
                {
                    "sent": "First of all, how do you choose the stochastic direction?",
                    "label": 0
                },
                {
                    "sent": "If it's an unbiased estimator, then things are going to go well because that is going to give you the proof.",
                    "label": 0
                },
                {
                    "sent": "This is the drift and the step link that is crucial.",
                    "label": 0
                },
                {
                    "sent": "If you diminish it to zero, you're going to get a sub linear rate.",
                    "label": 0
                },
                {
                    "sent": "If you choose it constant, which is a lot more interesting, what does SGD do?",
                    "label": 1
                },
                {
                    "sent": "If you use a constant stepling?",
                    "label": 0
                },
                {
                    "sent": "And what happens is it converges to a neighborhood of the solution.",
                    "label": 0
                },
                {
                    "sent": "This can be proven and the neighborhood can be care.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Authorized right?",
                    "label": 0
                },
                {
                    "sent": "So we have here a run of SGD on a very simple problem with a fixed step length.",
                    "label": 0
                },
                {
                    "sent": "You see that it's taking big steps all the time.",
                    "label": 0
                },
                {
                    "sent": "With diminishing step links, it looks like it's a really good algorithm, except you don't see how many steps you've been taking there because it's moving really, really slowly towards a solution, right?",
                    "label": 0
                },
                {
                    "sent": "But with large step length it says well also late around the neighborhood of the solution.",
                    "label": 0
                },
                {
                    "sent": "And what's remarkable is that the neighborhood that you converge that you converge to linearly, not sublinearly you converge at a linear rate of convergence to a neighborhood of the solution.",
                    "label": 0
                },
                {
                    "sent": "And once you're there, you know things wander and you can see there is a lucky step that gets close to the solution.",
                    "label": 0
                },
                {
                    "sent": "But you would not know.",
                    "label": 0
                },
                {
                    "sent": "Maybe you would not know how to identify there was a question there.",
                    "label": 0
                },
                {
                    "sent": "Exactly, yes, so the last term in that relation will not go to sere if you're doing SGD.",
                    "label": 0
                },
                {
                    "sent": "The only way you can make the last term go to series by choosing Alpha Small, because there is an Alpha squared here.",
                    "label": 0
                },
                {
                    "sent": "So in machine learning Alpha sometimes is called the learning rate.",
                    "label": 0
                },
                {
                    "sent": "We always called Alpha this steppling how far you go.",
                    "label": 0
                },
                {
                    "sent": "I think in SGD it shouldn't be called either one of them because it does two things, it suppresses the noise.",
                    "label": 0
                },
                {
                    "sent": "And the other one it controls the size of the step, so it's more than learning rate and stepping.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do two things at the same time, and in fact, because it doesn't, two things at the same time.",
                    "label": 0
                },
                {
                    "sent": "It's because it's impossible to find a room.",
                    "label": 0
                },
                {
                    "sent": "I think there will always work for those problems.",
                    "label": 0
                },
                {
                    "sent": "So in any case, I think it's very important to think about the SGD method with fixed step links, because I think that's what people do in practice.",
                    "label": 0
                },
                {
                    "sent": "Fixes step link and you get to a neighborhood of the solution.",
                    "label": 1
                },
                {
                    "sent": "Really quickly decreases step when you get to another neighborhood of the solution and all that is well understood and that equation that I have there is when I said do you understand why it converges?",
                    "label": 0
                },
                {
                    "sent": "Well that that's the explanation that it does Sanjeev.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, I forgot to say about this is all about the convex case and I have a slide about the non convex case because there are a lot of results about the nonconvex case but I have also some complaints about them.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why is SGD efficient?",
                    "label": 0
                },
                {
                    "sent": "You see I don't have much in this slide.",
                    "label": 0
                },
                {
                    "sent": "Jimmy talked about that we have a Siam review paper that appeared this year with number 2 and Frank Curtis and we have a few examples that are intuitive.",
                    "label": 1
                },
                {
                    "sent": "This is why it's efficient to use as GD.",
                    "label": 0
                },
                {
                    "sent": "And then there is a complexity results which are very compelling.",
                    "label": 0
                },
                {
                    "sent": "These other ones are really sealed things for certain classes of problems.",
                    "label": 0
                },
                {
                    "sent": "It is more efficient to use SGD than a Patch method.",
                    "label": 0
                },
                {
                    "sent": "Now non convexity, so that's where you're asking me, right?",
                    "label": 0
                },
                {
                    "sent": "There are a number of results have been established for convergence of SGD to stationary points.",
                    "label": 0
                },
                {
                    "sent": "And I don't have them here.",
                    "label": 0
                },
                {
                    "sent": "And I've worked on some of them because my question is how meaningful are they?",
                    "label": 0
                },
                {
                    "sent": "How unrealistic are they?",
                    "label": 0
                },
                {
                    "sent": "Well, they they don't seem to match the practice of neural networks, where the problem is seems to be much simpler than a general nonconvex problem, and these results will apply to any non convex function.",
                    "label": 0
                },
                {
                    "sent": "So we will say that you know it, you will visit a gradient with zero point where the grain is almost zero.",
                    "label": 0
                },
                {
                    "sent": "As you look at the window after so many iterations that are way too many.",
                    "label": 0
                },
                {
                    "sent": "How many people are these results?",
                    "label": 0
                },
                {
                    "sent": "There's a lot of interesting world coming from the theory community about that, so I defer to them and there was a lot of interest, interesting material incentive, stock.",
                    "label": 0
                },
                {
                    "sent": "The very first talk this morning.",
                    "label": 0
                },
                {
                    "sent": "Um, yeah so.",
                    "label": 0
                },
                {
                    "sent": "Several points how things work with several points is escaping several complete complexity, and so on.",
                    "label": 0
                },
                {
                    "sent": "We enter deterministic optimization world, have never worried about saddle points.",
                    "label": 0
                },
                {
                    "sent": "We never thought that we really converge.",
                    "label": 0
                },
                {
                    "sent": "The saddle points the question about 7 points are going to slow you down is a concern.",
                    "label": 0
                },
                {
                    "sent": "But as I mentioned, we never tried to use gradient methods.",
                    "label": 0
                },
                {
                    "sent": "We're always trying to use something that has more curvature information about the problem.",
                    "label": 0
                },
                {
                    "sent": "So what you hope is that you're not just doing a stupid descent method and then somehow something is going to happen and push you there, or find negative eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "But in a Newton type method when you try to get second or the information that we're going to get to.",
                    "label": 0
                },
                {
                    "sent": "Contributions of Jimmy, Ann and Roger that we use.",
                    "label": 0
                },
                {
                    "sent": "We use secondary information from the beginning that should help you there.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to get through that.",
                    "label": 0
                },
                {
                    "sent": "So what are the weaknesses of this GD?",
                    "label": 0
                },
                {
                    "sent": "I mean, in principle as a core one view is it's nothing an in fact SGD, maybe within some statistical variance like the second talk this morning.",
                    "label": 0
                },
                {
                    "sent": "That's that's what you want to do.",
                    "label": 0
                },
                {
                    "sent": "And my feeling is that.",
                    "label": 0
                },
                {
                    "sent": "The Pioneers of the field with neural networks.",
                    "label": 1
                },
                {
                    "sent": "My feeling one of them is here have the feeling that you know I don't know what eventually is going to happen, but it's going to look a lot like this GD.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's this really powerful type of optimization algorithm here that I look.",
                    "label": 0
                },
                {
                    "sent": "I have the feeling they're going to be like a city.",
                    "label": 0
                },
                {
                    "sent": "I have.",
                    "label": 0
                },
                {
                    "sent": "My impression is is that through their experience, that's what they feel right there.",
                    "label": 0
                },
                {
                    "sent": "There are more discipline and more expensive items.",
                    "label": 0
                },
                {
                    "sent": "One could you, so that's one possibility.",
                    "label": 0
                },
                {
                    "sent": "So look at SGD, look at ideas behind momentum.",
                    "label": 0
                },
                {
                    "sent": "There is the answer to the question, and many of you are actually playing with such ideas.",
                    "label": 0
                },
                {
                    "sent": "There's an alternate view.",
                    "label": 0
                },
                {
                    "sent": "How could it be the optimal algorithms when gradients don't have scale as we showed in the beginning of it, they suffer from ill conditioning.",
                    "label": 0
                },
                {
                    "sent": "And this is the first order method.",
                    "label": 1
                },
                {
                    "sent": "Why couldn't it be learn something about the curvature of the problem so it never does learn the curvature of the problem?",
                    "label": 0
                },
                {
                    "sent": "May do some smoothing.",
                    "label": 0
                },
                {
                    "sent": "That's very different when someone asked the question to Jimmy about ill conditioning.",
                    "label": 0
                },
                {
                    "sent": "Right now he was talking about even if I have the gradient very good, I'm still suffering from Neil conditioning, right?",
                    "label": 0
                },
                {
                    "sent": "So are we happy to just say we cannot do anything about your conditioning?",
                    "label": 0
                },
                {
                    "sent": "We cannot do anything about scale.",
                    "label": 0
                },
                {
                    "sent": "So and also, if you use a very noisy iteration it makes.",
                    "label": 0
                },
                {
                    "sent": "Parallelization limited.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so there is a picture if we go beyond the stochastic gradient method and that's what the rest of my talk is going to be about.",
                    "label": 0
                },
                {
                    "sent": "Speculating about ideas where you go beyond stochastic gradient method.",
                    "label": 0
                },
                {
                    "sent": "There is a picture that we have in that review paper that I use all the time.",
                    "label": 0
                },
                {
                    "sent": "I think I'm adding a new axis now stochastic gradient method is at the origin is like the, you know the universe started there with stochastic gradient method and you can move on in One Direction by reducing the variance of the gradient.",
                    "label": 0
                },
                {
                    "sent": "The last term in that equation is type.",
                    "label": 0
                },
                {
                    "sent": "Reducing that to obtain argument that have less noisy methods until you get to a batch method.",
                    "label": 0
                },
                {
                    "sent": "The other direction is your start incorporating 2nd order information into the problem, as was done with a K FAQ, and there are other algorithms that I will mention here.",
                    "label": 0
                },
                {
                    "sent": "We have these two possibilities of moving, and they're actually complementary.",
                    "label": 0
                },
                {
                    "sent": "You can do the two at the same time.",
                    "label": 0
                },
                {
                    "sent": "Now remember the key relationship there.",
                    "label": 0
                },
                {
                    "sent": "So when I talk about variance reduction, I mean how to how to suppress the blue term.",
                    "label": 0
                },
                {
                    "sent": "So there are two ways to suppress the blue term and I have to thank you for your question.",
                    "label": 0
                },
                {
                    "sent": "One of them is with a step length and the other one is by decreasing the variance of that term.",
                    "label": 0
                },
                {
                    "sent": "If you make that a mini batch, the larger the mini batch the variance will be in.",
                    "label": 0
                },
                {
                    "sent": "The smaller that blue term will become.",
                    "label": 0
                },
                {
                    "sent": "So there are these two mechanisms at your disposal.",
                    "label": 0
                },
                {
                    "sent": "I I don't know, yeah so.",
                    "label": 0
                },
                {
                    "sent": "Well, that's why I said the first generations.",
                    "label": 0
                },
                {
                    "sent": "That's very sceptical about this, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so all I'm saying is this is where we are with this GD.",
                    "label": 0
                },
                {
                    "sent": "Why don't 11?",
                    "label": 0
                },
                {
                    "sent": "We consider it.",
                    "label": 0
                },
                {
                    "sent": "Nobody says about removing the noise completely, and in fact, when I'm going to talk about it, more disciplined way is to just reduce it, because why not right?",
                    "label": 0
                },
                {
                    "sent": "Once you're getting close to the minimizer, maybe there is no risk of leading that minimizer and just converge quickly to that, right?",
                    "label": 0
                },
                {
                    "sent": "So that's just the possibility of moving that way, right?",
                    "label": 0
                },
                {
                    "sent": "Where in between is the right spot?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So maybe another axis here?",
                    "label": 0
                },
                {
                    "sent": "Is there other ways of improving SGD that have to do with statistics?",
                    "label": 0
                },
                {
                    "sent": "That I don't understand well, but that's what I was saying.",
                    "label": 0
                },
                {
                    "sent": "Forget about momentum.",
                    "label": 0
                },
                {
                    "sent": "Think about statistics.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, in terms of incorporating 2nd order information, if we look at that axis that says Stochastic Newton method.",
                    "label": 1
                },
                {
                    "sent": "There are three ideas that I think are possible.",
                    "label": 0
                },
                {
                    "sent": "One of them is to do in inexact Newton methods.",
                    "label": 0
                },
                {
                    "sent": "They scale up.",
                    "label": 0
                },
                {
                    "sent": "They can be used using Hessian subsampling.",
                    "label": 0
                },
                {
                    "sent": "Second idea would be to start with the natural gradient algorithm built on that, like a fact, and the third one, which we finally picked up there for many years, is to go back and revisit ideas of quasi Newton methods, and we're finding that very promising results.",
                    "label": 0
                },
                {
                    "sent": "I think we're doing causing it and methods incorrectly for machine learning and in the stochastic regimes there are these three forms that I see.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I've been talking about second order methods for awhile when nobody in the machine learning community was talking about them, and then with time things are changing and in this today you gotta have a slanted view of optimization for machine learning.",
                    "label": 0
                },
                {
                    "sent": "You got two talks to talk about using second order information.",
                    "label": 0
                },
                {
                    "sent": "We're in the minority, but so that's just something to keep.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now going on till the other axis.",
                    "label": 0
                },
                {
                    "sent": "I'm going to move variance reduction.",
                    "label": 0
                },
                {
                    "sent": "How can we suppress the noise?",
                    "label": 0
                },
                {
                    "sent": "Well, if you use the mini batches suppress the noise everybody have found that using many batches is useful.",
                    "label": 0
                },
                {
                    "sent": "Classical complexity theory does not show the benefits of using mini batching.",
                    "label": 0
                },
                {
                    "sent": "Complexity analysis does not do that.",
                    "label": 0
                },
                {
                    "sent": "Some papers recently challenged that, but in practice it works.",
                    "label": 0
                },
                {
                    "sent": "What if it works?",
                    "label": 0
                },
                {
                    "sent": "It works.",
                    "label": 0
                },
                {
                    "sent": "But why not use a gradient with a much larger batch size?",
                    "label": 0
                },
                {
                    "sent": "Why is it that people are using small batch sizes only?",
                    "label": 0
                },
                {
                    "sent": "Not take more, you know half of the training set.",
                    "label": 0
                },
                {
                    "sent": "Becauses the batch size becomes larger.",
                    "label": 0
                },
                {
                    "sent": "People have observed that accuracy deteriorates.",
                    "label": 0
                },
                {
                    "sent": "I believe that this has been observed for many years.",
                    "label": 0
                },
                {
                    "sent": "Empirically, there are dozens of studies and the theory community, and now looking at to see what is happening there.",
                    "label": 0
                },
                {
                    "sent": "But so.",
                    "label": 0
                },
                {
                    "sent": "It would be more natural from the very beginning to have used iterations with survey accurate batch that you can paralyze brain computation very easily, and I know that from the beginning people are not getting good results with that.",
                    "label": 0
                },
                {
                    "sent": "So there using noisy iterations.",
                    "label": 0
                },
                {
                    "sent": "So that's one reason why you don't go jump straight onto.",
                    "label": 0
                },
                {
                    "sent": "The non noisy regime.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so here's a paper by the Facebook team.",
                    "label": 0
                },
                {
                    "sent": "What they did is this a paper where they talk about doing Imagenet, bringing down computing time from 29 hours to one hour.",
                    "label": 0
                },
                {
                    "sent": "Kindly keyin his group.",
                    "label": 0
                },
                {
                    "sent": "And I have this picture here where they run the stochastic well.",
                    "label": 0
                },
                {
                    "sent": "Probably Adam.",
                    "label": 0
                },
                {
                    "sent": "They're probably running Adam with different batch sizes, and they're plotting their.",
                    "label": 0
                },
                {
                    "sent": "This is image net, the floating their accuracy.",
                    "label": 0
                },
                {
                    "sent": "As they increase the batch size, the reason why they're brought down computing time so much is because bigger, bigger batch sizes they could paralyze better, but so at some point after 8K.",
                    "label": 0
                },
                {
                    "sent": "Accuracy starts getting worse.",
                    "label": 0
                },
                {
                    "sent": "It's just another study that showed that now there's another paper paper #2 is from people at Google.",
                    "label": 0
                },
                {
                    "sent": "I think probably is one of the authors of that paper.",
                    "label": 0
                },
                {
                    "sent": "They go and use batch size up to 65 K. And those two papers, what people say is, you know, in the past we always just played with the learning rate.",
                    "label": 0
                },
                {
                    "sent": "This time, instead of decreasing the learning rate at some point, increase the batch size.",
                    "label": 0
                },
                {
                    "sent": "So when you get stuck, increase the batch size by decreasing the noise and increase stepling.",
                    "label": 0
                },
                {
                    "sent": "So do that and in these two papers they got good result.",
                    "label": 0
                },
                {
                    "sent": "So if under very encouraging because that is actually consistent with where I'm going right?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So where I'm going?",
                    "label": 0
                },
                {
                    "sent": "What is this doing here?",
                    "label": 0
                },
                {
                    "sent": "On this this fantastic animation is very important.",
                    "label": 0
                },
                {
                    "sent": "OK look, look at this.",
                    "label": 0
                },
                {
                    "sent": "This is called robust optimization.",
                    "label": 0
                },
                {
                    "sent": "Optimization consists of finding the lowest point in the curve, right?",
                    "label": 0
                },
                {
                    "sent": "Robust optimization says this.",
                    "label": 0
                },
                {
                    "sent": "I give you a disk.",
                    "label": 0
                },
                {
                    "sent": "And you want to push it as far down in the objective function.",
                    "label": 0
                },
                {
                    "sent": "The disk has to do with the uncertainty that you're willing to tolerate, right?",
                    "label": 0
                },
                {
                    "sent": "So here's now the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It has two minimizers.",
                    "label": 0
                },
                {
                    "sent": "You're trying to push the disk down, and the disk does not fit into the first regime, so therefore it goes into the second regime.",
                    "label": 0
                },
                {
                    "sent": "This is the field of robust optimization, which is a very active area in optimization today.",
                    "label": 0
                },
                {
                    "sent": "So one conjecture is SGD converges to minimize that are more robust.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to use the term flat.",
                    "label": 0
                },
                {
                    "sent": "Even entropy, maybe a little bit.",
                    "label": 0
                },
                {
                    "sent": "Misleading because the minimizer degenerate.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use robust because the idea of the disk I think is relevant here and one of the conjectures is SGD gets you to more robust minimizers, and therefore, if you're going to improve as GD.",
                    "label": 0
                },
                {
                    "sent": "Put in some ingredients that have to do with robust optimization.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, you cannot use the classical robust optimization techniques that teaches semidefinite programming and something that does not scale, so you cannot do it in a very nice way.",
                    "label": 0
                },
                {
                    "sent": "So the robust optimization problem is not minimizing objective function.",
                    "label": 0
                },
                {
                    "sent": "But minimize the maximum value.",
                    "label": 0
                },
                {
                    "sent": "The objective function can have in a certain disk.",
                    "label": 0
                },
                {
                    "sent": "I just want to bring this idea.",
                    "label": 0
                },
                {
                    "sent": "I don't have any more insights about this problem about SGD doing better minimizes than batch methods than anybody here.",
                    "label": 0
                },
                {
                    "sent": "I just want to bring in so this notation robust, robust minimizers might be a good way to look at it.",
                    "label": 0
                },
                {
                    "sent": "Alright, so going into this, yes, I said you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is the robust optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, lots of people.",
                    "label": 0
                },
                {
                    "sent": "This is an active area of research, right?",
                    "label": 0
                },
                {
                    "sent": "Adversarial.",
                    "label": 0
                },
                {
                    "sent": "Against robust optimization.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, you have to tell me later what, what do you have in mind?",
                    "label": 0
                },
                {
                    "sent": "Is she?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going on to that axis.",
                    "label": 0
                },
                {
                    "sent": "Let's find methods that reduce the variance, not just play with the step length.",
                    "label": 0
                },
                {
                    "sent": "So instead of manually choosing the mini batch, which is what that Facebook and Google Papers were doing, they were saying we're going to just 8K, or we're going to do it gradually, like this.",
                    "label": 0
                },
                {
                    "sent": "Isn't there a way to do this in a systematic form, developing algorithm that gradually increases the batch size, suppresses the noise, starts with a very small value, and doesn't have to go all the way to suppressing the noise, just has to go good enough so that things work well.",
                    "label": 0
                },
                {
                    "sent": "The noise is controlled by the sample size, it's beginning.",
                    "label": 0
                },
                {
                    "sent": "You start with SGD regime.",
                    "label": 0
                },
                {
                    "sent": "And then you stay with the same sample size until you feel that the item cannot make more progress, so the noise is dominating.",
                    "label": 0
                },
                {
                    "sent": "You want to have a test that says your noise is dominating your increase.",
                    "label": 0
                },
                {
                    "sent": "Decrease in the objective function, improve the quality of the gradient.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the idea that the algorithm should do this automatically, and we call this progressive sampling algorithm.",
                    "label": 0
                },
                {
                    "sent": "And if you do a progressive sampling, you get a better explanation of the gradient.",
                    "label": 0
                },
                {
                    "sent": "So that opens the opportunity for using more optimization algorithms.",
                    "label": 0
                },
                {
                    "sent": "So you could try to incorporate 2nd order.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, if you're going to use an algorithm that input increase increases the batch size as you go along, the key question is how fast do you do that?",
                    "label": 0
                },
                {
                    "sent": "Are we going to end up with some heuristic that is horrible?",
                    "label": 0
                },
                {
                    "sent": "Again, they can never be used in practice.",
                    "label": 0
                },
                {
                    "sent": "Well, a progressive matching algorithm that increases that sample size at a geometric rate.",
                    "label": 1
                },
                {
                    "sent": "Like 1.1 to the K. This theoretical result increase the batch size at a geometric rate.",
                    "label": 0
                },
                {
                    "sent": "Complexity wise, it matches the complexity of SGD.",
                    "label": 0
                },
                {
                    "sent": "Right, so there are papers, especially by #2 saying SGD gets the best complexity work complexity.",
                    "label": 0
                },
                {
                    "sent": "How much function evaluations you do?",
                    "label": 1
                },
                {
                    "sent": "And gradient method is much worse.",
                    "label": 0
                },
                {
                    "sent": "Well here we have a paper from 2013 where we showed that if you increase the sample size at a geometric rate and you look at how much work you end up doing in the end.",
                    "label": 0
                },
                {
                    "sent": "And the complexity is done not on the empirical minimization function, but on the total problem.",
                    "label": 0
                },
                {
                    "sent": "The real problem.",
                    "label": 0
                },
                {
                    "sent": "The complexity is matches that of SGD.",
                    "label": 1
                },
                {
                    "sent": "So at least these two algorithms in terms of complexity should be put at apart.",
                    "label": 0
                },
                {
                    "sent": "Batch method bad is to give a good progressive sampling.",
                    "label": 0
                },
                {
                    "sent": "In the worst case they are.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parable like you would never do something like this in practice, so let's go through the heuristics.",
                    "label": 0
                },
                {
                    "sent": "Increase the batch size like 1.1 to the K. No good algorithm is to the K, right?",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is we want to find automatic criterion that does that.",
                    "label": 0
                },
                {
                    "sent": "That's what optimization algorithms that go into textbooks eventually do, that they run by themselves.",
                    "label": 0
                },
                {
                    "sent": "Right, So what is that we want to do?",
                    "label": 0
                },
                {
                    "sent": "Let GK be your stochastic approximation to the grade.",
                    "label": 0
                },
                {
                    "sent": "What do you want to make sure is that you have an acute angle with the true gradient direction.",
                    "label": 0
                },
                {
                    "sent": "And how do you measure that?",
                    "label": 0
                },
                {
                    "sent": "How do you know that you have the right angle?",
                    "label": 0
                },
                {
                    "sent": "Especially you have noisy directions.",
                    "label": 0
                },
                {
                    "sent": "Well, in the terministic optimization you would do something else but in machine learning.",
                    "label": 0
                },
                {
                    "sent": "I claimed that you would do this.",
                    "label": 0
                },
                {
                    "sent": "If you make sure that your gradient estimate GK satisfies that condition.",
                    "label": 0
                },
                {
                    "sent": "The difference between it and the true gradient is less than the norm of that rating.",
                    "label": 0
                },
                {
                    "sent": "Approximation of true gradient itself.",
                    "label": 0
                },
                {
                    "sent": "For fatalism one, then it's a dissent direction and everything is going to work.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, this quantity actually is going to.",
                    "label": 0
                },
                {
                    "sent": "If you square it is going to look like a variance.",
                    "label": 0
                },
                {
                    "sent": "That's why it's something that is useful in case for machine learning, so we go from the deterministic case of the stochastic case.",
                    "label": 0
                },
                {
                    "sent": "Now you have your stochastic rain approximation are calling gratify.",
                    "label": 0
                },
                {
                    "sent": "You want that in expectation to be less than the norm of the true gradient.",
                    "label": 0
                },
                {
                    "sent": "You don't have the true grade, but we're going to do one more step, right?",
                    "label": 0
                },
                {
                    "sent": "So if you do this, you can prove convergence.",
                    "label": 0
                },
                {
                    "sent": "Your room is going to work right if that expectation is less and you divide by the cardinality of the sample size.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's an equation that actually means something that is completely common sense.",
                    "label": 0
                },
                {
                    "sent": "You're moving and you can measure the variance of your stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "When the variance is as big as, this is the step itself.",
                    "label": 0
                },
                {
                    "sent": "It's time to improve the quality of the great.",
                    "label": 0
                },
                {
                    "sent": "That's pretty much what it's saying and how you're going to measure the variance of the gradient.",
                    "label": 0
                },
                {
                    "sent": "You can do a sample variance.",
                    "label": 0
                },
                {
                    "sent": "You know if the problem is a finite sum problem, you can take a sample of the variance and you can also sample the right hand side.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so we have done that.",
                    "label": 0
                },
                {
                    "sent": "And if you do this variance estimates you can get pretty good behavior of algorithms.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to stay in this graph because I have another graph.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, I'm now much more keen and using Quasi Newton than inexact Newton methods.",
                    "label": 0
                },
                {
                    "sent": "But in any any case we have criteria practical criteria to tell you when it's time to increase the batch size.",
                    "label": 0
                },
                {
                    "sent": "If you're going to do one of these progressive batching algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now I want to talk about step links.",
                    "label": 0
                },
                {
                    "sent": "If you have any optimization algorithm that is a decent algorithm, you're here like in that picture, and you know what the gradient is.",
                    "label": 0
                },
                {
                    "sent": "How far should you go?",
                    "label": 0
                },
                {
                    "sent": "Well, it depends.",
                    "label": 0
                },
                {
                    "sent": "There are two objective functions there.",
                    "label": 0
                },
                {
                    "sent": "If there is high curvature, you cannot go very far.",
                    "label": 0
                },
                {
                    "sent": "If there's more curvature, you can go quite far.",
                    "label": 0
                },
                {
                    "sent": "That's pretty simple.",
                    "label": 0
                },
                {
                    "sent": "So that's how it has to be in the rule there, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what leads you to these Lipschitz constant.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But different directions should have different scales.",
                    "label": 0
                },
                {
                    "sent": "So I have a quadratic function and there are two lines there.",
                    "label": 0
                },
                {
                    "sent": "There is a solid line and there is a dotted line.",
                    "label": 0
                },
                {
                    "sent": "Along the dotted line, you cannot move very far because the curvature prevents you from doing that, but along this solid line you can move very long.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now something that is done in the theory community is they use a universal Lipschitz constant.",
                    "label": 1
                },
                {
                    "sent": "They use a bound for the curvature everywhere right?",
                    "label": 1
                },
                {
                    "sent": "And they apply that to the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the steepest descent algorithm is going to take a step that is going to guarantee to give you the sense in any direction that you move.",
                    "label": 0
                },
                {
                    "sent": "And that means that for the for the solid line, you're going to make much less progress than you could do because you are going to restrict yourself to that.",
                    "label": 1
                },
                {
                    "sent": "That's why line searches have proved to be so useful in the deterministic setting.",
                    "label": 1
                },
                {
                    "sent": "You take a search direction and use it well.",
                    "label": 0
                },
                {
                    "sent": "Explore the objective function and see how far you can get there.",
                    "label": 0
                },
                {
                    "sent": "When the function is very noisy, this doesn't make any sense, but once you get more accurate estimates, that's one way of exploiting the fact that there is scale in the problem.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, I mentioned that in SGD stepling this light up with noise suppression too, so it's not such a simple thing.",
                    "label": 0
                },
                {
                    "sent": "I was giving you a picture there that is just for the deterministic setting.",
                    "label": 0
                },
                {
                    "sent": "Alright, so how do you scale the gradient direction?",
                    "label": 1
                },
                {
                    "sent": "So I was mentioning that in theory an A lot of papers say.",
                    "label": 0
                },
                {
                    "sent": "Well, not in the machine learning practice, but in the theory says take Alpha to be one over the Lipschitz constant and everything is going to be peachy.",
                    "label": 0
                },
                {
                    "sent": "And so my my message again to you is.",
                    "label": 0
                },
                {
                    "sent": "That's great in theory.",
                    "label": 0
                },
                {
                    "sent": "You don't want to do that in practice.",
                    "label": 1
                },
                {
                    "sent": "You're taking the most conservative step length, and so some people say, well, I'm going to do an adaptive Lipschitz estimation and adaptive estimation of what's the highest curvature of the problem.",
                    "label": 0
                },
                {
                    "sent": "But that's not a good concept because you're learning behind move.",
                    "label": 0
                },
                {
                    "sent": "You learn a certain curvature.",
                    "label": 0
                },
                {
                    "sent": "You move another curvature, and with all of that you want to get a universal curvature.",
                    "label": 0
                },
                {
                    "sent": "Well, the line search idea tells you one time you're going to take a big step you want.",
                    "label": 0
                },
                {
                    "sent": "I'm going to take a small step that is proved to be quite useful.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now since we want to scale the gradient differently.",
                    "label": 0
                },
                {
                    "sent": "Then something that comes natural is.",
                    "label": 0
                },
                {
                    "sent": "In addition to the line search, or perhaps to learn to help the line, Sir, why don't you do a diagonal scaling?",
                    "label": 0
                },
                {
                    "sent": "And so that idea is very popular and has proved to be quite useful in machine learning and assumes that you have knowledge.",
                    "label": 0
                },
                {
                    "sent": "Of the objective function along the coordinate direction.",
                    "label": 0
                },
                {
                    "sent": "Now what are the core in the directions?",
                    "label": 0
                },
                {
                    "sent": "Well, if you think that they are related to features, then you would say or maybe I have knowledge about what happens among different feet.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in general, when we solve all sorts of nonlinear optimization problems, we never know how to choose.",
                    "label": 0
                },
                {
                    "sent": "Diagonal matrices because we don't know what the objective function is doing along corinthe directions.",
                    "label": 0
                },
                {
                    "sent": "Coordinate directions are totally arbitrary.",
                    "label": 1
                },
                {
                    "sent": "Our algorithm may be moving here, maybe moving there, but it's only never going to move along with this coordinate directions, so we cannot learn goes well, but Adam and Adigrat are.",
                    "label": 0
                },
                {
                    "sent": "Most likely doing something else, gathering some statistics I don't understand.",
                    "label": 0
                },
                {
                    "sent": "I don't understand them well, but you have experts here.",
                    "label": 0
                },
                {
                    "sent": "So instead of finding sophisticated, let's tabling strategies, find a method that produces waves well scaled direction.",
                    "label": 1
                },
                {
                    "sent": "So so if we can do this.",
                    "label": 0
                },
                {
                    "sent": "And Jim is talking by putting a matrix in front is very important if you can put a matrix.",
                    "label": 0
                },
                {
                    "sent": "A choice of this step link is going to be much less important, and that's definitely the case in Newton and Quasi Newton meth.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I want to say something about Newton's method because of the ideal and I think it's also motivate things.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Some classes are not any optimization.",
                    "label": 0
                },
                {
                    "sent": "Start with Newton's method because they say this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the scale invariant the optimal algorithm.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter how badly scaled the problem is, it immediately learns the hidden information contains a lot of information.",
                    "label": 0
                },
                {
                    "sent": "So Newton's method is the thing to do if you're an engineer and you have a problem with 100 variables.",
                    "label": 0
                },
                {
                    "sent": "And lots of problems in engineering are ill condition.",
                    "label": 0
                },
                {
                    "sent": "Use Newton's method right?",
                    "label": 0
                },
                {
                    "sent": "But what happens?",
                    "label": 0
                },
                {
                    "sent": "You have a problem in a million variables.",
                    "label": 0
                },
                {
                    "sent": "You cannot invert the matrix, so this is out of the question.",
                    "label": 0
                },
                {
                    "sent": "There is too much information.",
                    "label": 0
                },
                {
                    "sent": "How do you compute and you can spit?",
                    "label": 0
                },
                {
                    "sent": "So the idea is, well, computing inexact mutants that.",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Or use a natural gradient method.",
                    "label": 0
                },
                {
                    "sent": "And choose an inexact or computer.",
                    "label": 0
                },
                {
                    "sent": "In exact step there, or use causing you.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An idea?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the 2nd.",
                    "label": 0
                },
                {
                    "sent": "Equation I have two equations for this talk.",
                    "label": 0
                },
                {
                    "sent": "What is Newtons method?",
                    "label": 0
                },
                {
                    "sent": "Let's take an X Ray of Newton's method.",
                    "label": 0
                },
                {
                    "sent": "Again, in the strongly convex case.",
                    "label": 0
                },
                {
                    "sent": "Because maybe I'll talk to.",
                    "label": 0
                },
                {
                    "sent": "When there's a question and answer, I'll talk about the nonconvex case with Sanjeev and we can discuss why convex and nonconvex, why the emphasis shifts, any case industry convex case just to make our life simple when the Hessian is positive definite.",
                    "label": 0
                },
                {
                    "sent": "Take an eigenvalue decomposition of the hisi.",
                    "label": 0
                },
                {
                    "sent": "So Lambda I vvi transport the eyes are the eigenvectors and so on.",
                    "label": 0
                },
                {
                    "sent": "Every symmetric positive definite matrix has eigenvalue decomposition and the nice thing is that the inverse of the matrix is the same form, except that the eigenvalues become the reciprocal.",
                    "label": 0
                },
                {
                    "sent": "OK, great, so if I multiply that inverse Hessian times the gradient, what happens?",
                    "label": 0
                },
                {
                    "sent": "Well, the direction becomes when I multiply that summation with VI VI, transpose.",
                    "label": 0
                },
                {
                    "sent": "Well, forget about the charming parenthesis.",
                    "label": 0
                },
                {
                    "sent": "Just look at what's outside.",
                    "label": 0
                },
                {
                    "sent": "It says one over Lambda I times VI.",
                    "label": 0
                },
                {
                    "sent": "What is the term that is really going to matter if you have a tiny eigenvalue, Lambda is 10 to the minus 6.",
                    "label": 0
                },
                {
                    "sent": "Right and one over Lambda is 10 to the 6th right?",
                    "label": 0
                },
                {
                    "sent": "So the smallest eigenvalue is the one that is going to determine by far where the Newton direction is going to go.",
                    "label": 0
                },
                {
                    "sent": "Or the smallest eigenvalues are the ones that are really going to matter.",
                    "label": 0
                },
                {
                    "sent": "So one way of doing an inexact Newton method is it look at that expression which says the Newton step consists of looking not a coordinate direction consists of looking at the eigenvectors of the problem.",
                    "label": 0
                },
                {
                    "sent": "And moving along those eigenvectors at distance one over Lambda I.",
                    "label": 0
                },
                {
                    "sent": "Right and the more information you have about the product closer you get to the new contract.",
                    "label": 0
                },
                {
                    "sent": "Now unfortunately the most difficult thing to compute.",
                    "label": 0
                },
                {
                    "sent": "Is the smallest eigenvalue of a matrix the largest eigenvalues?",
                    "label": 0
                },
                {
                    "sent": "Are the easy ones.",
                    "label": 0
                },
                {
                    "sent": "So every inexact Newton method where they're going to do is they're going to start approximating the easy directions, and the more work you do, the closer is going to get to the Newton method.",
                    "label": 0
                },
                {
                    "sent": "That's one way of trying to do an inexact Newton method has proved to be very successful is these are the Hessian Free Newton method.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So an inexact Newton method is going to do the following.",
                    "label": 0
                },
                {
                    "sent": "Instead of inverting the matrix, you solve the linear system of equations by our favorite algorithm, the conjugate gradient method.",
                    "label": 0
                },
                {
                    "sent": "You apply the conjugate gradient method to that.",
                    "label": 0
                },
                {
                    "sent": "That's called the Newton exact Newton CG algorithm.",
                    "label": 0
                },
                {
                    "sent": "It exists, and tons of software.",
                    "label": 0
                },
                {
                    "sent": "There is a great way of doing.",
                    "label": 0
                },
                {
                    "sent": "Inexact Newton method.",
                    "label": 0
                },
                {
                    "sent": "Now the talk this morning, Sanjay for Aurora hadn't had a method.",
                    "label": 0
                },
                {
                    "sent": "An approximation of the Newton method.",
                    "label": 0
                },
                {
                    "sent": "There was an expansion of the inverse of the matrix.",
                    "label": 0
                },
                {
                    "sent": "I -- A inverse.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if you look at that method.",
                    "label": 0
                },
                {
                    "sent": "The way it was put in the paper by Hassan and Bullington.",
                    "label": 0
                },
                {
                    "sent": "So when you write it down, it consists of applying SGD.",
                    "label": 0
                },
                {
                    "sent": "To solve that system there instead of, instead of using conjugate gradients to solve SGD.",
                    "label": 0
                },
                {
                    "sent": "To that problem, there's a small change that the Hessian is sampled right, but all of that it seems like this is a new.",
                    "label": 0
                },
                {
                    "sent": "This is a new Ave actually.",
                    "label": 0
                },
                {
                    "sent": "If within that family of methods that is doing an inexact Newton method in that way, and by the way, using SGD to solve the Newton equations is not the best thing to do.",
                    "label": 0
                },
                {
                    "sent": "We have analysis and we have experience about doing that.",
                    "label": 0
                },
                {
                    "sent": "Now all what I gave you was an intuition about the convex case.",
                    "label": 0
                },
                {
                    "sent": "I said that if the Hessian is positive definite, this is what happens.",
                    "label": 1
                },
                {
                    "sent": "What is it that people in the optimization community do a lot?",
                    "label": 0
                },
                {
                    "sent": "They for they say I don't care the nature is positive definite or not.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to type conjugate gradients to that.",
                    "label": 0
                },
                {
                    "sent": "And see what happens.",
                    "label": 0
                },
                {
                    "sent": "You start applying conjugate gradient method, and if it's some point, you find the direction of negative curvature.",
                    "label": 0
                },
                {
                    "sent": "Is it easy for you to find the dictionary coverage?",
                    "label": 0
                },
                {
                    "sent": "It's really easy because the conjugate gradient method has to compute PAP transpose.",
                    "label": 0
                },
                {
                    "sent": "So if you detect that you stop the conjugate gradient process, you follow the direction of negative curvature.",
                    "label": 0
                },
                {
                    "sent": "Until about Zero trust region and then you move on.",
                    "label": 0
                },
                {
                    "sent": "That is something that if we doing for many years and there are many alternatives have been proposed, but that works fairly well.",
                    "label": 0
                },
                {
                    "sent": "What it does is it tries to work as much as possible in the convex regime.",
                    "label": 0
                },
                {
                    "sent": "So remember when I was saying we don't worry so much about saddles.",
                    "label": 0
                },
                {
                    "sent": "So if you have the opportunity of learning metric learning curvature, you learn that as much as possible and then when you fold down to negative curvature you follow the negative curvature direction.",
                    "label": 0
                },
                {
                    "sent": "But you don't just pick gradient somewhere and then finding negative curvature direction.",
                    "label": 0
                },
                {
                    "sent": "So what it actually does, this inexact Newton method is.",
                    "label": 0
                },
                {
                    "sent": "It will take a Newton step in a subspace.",
                    "label": 0
                },
                {
                    "sent": "And the subspace is related by the directions of the conjugate gradient method shows.",
                    "label": 0
                },
                {
                    "sent": "And those directions happen to be the kernel subspace and they are closely aligned with the eigenvalues of the problem.",
                    "label": 0
                },
                {
                    "sent": "Is it's reminiscent of finding the most relevant eigenvalues, little by little, so this is an idea that should not be this may.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, how do you do this?",
                    "label": 0
                },
                {
                    "sent": "More practical?",
                    "label": 0
                },
                {
                    "sent": "Write the conjugate gradient method requires all these Hessian vector products in a machine learning problem.",
                    "label": 0
                },
                {
                    "sent": "We know how to mini batch gradient.",
                    "label": 0
                },
                {
                    "sent": "We can mini batch the Hessian without forming it.",
                    "label": 0
                },
                {
                    "sent": "You say, let me assume that the true has entered.",
                    "label": 0
                },
                {
                    "sent": "The problem is not the whole sum, it's part of the sun.",
                    "label": 0
                },
                {
                    "sent": "In fact, a very small part of the sun, less and less than the mini batch take a very small percentage of that.",
                    "label": 0
                },
                {
                    "sent": "So let's do that.",
                    "label": 0
                },
                {
                    "sent": "And then I have the equation there.",
                    "label": 0
                },
                {
                    "sent": "There is no longer newtons method and inexact Newton method is an inexact subsample Newton method where the Hessian has a sample.",
                    "label": 1
                },
                {
                    "sent": "The greatness in mini batch, you can coordinate these two things, and of course a special cases.",
                    "label": 0
                },
                {
                    "sent": "If there is no Hessian, you get just mini batch gradient, so you can do a light version of this algorithm, or you can put quite a lot more curvature information into the problem.",
                    "label": 0
                },
                {
                    "sent": "Subsample Newton method.",
                    "label": 0
                },
                {
                    "sent": "You can solve the equations by conjugate gradients or stochastic gradient, but it's less efficient to solve this by stochastic random.",
                    "label": 1
                },
                {
                    "sent": "That's quite interesting question whether.",
                    "label": 0
                },
                {
                    "sent": "These two methods compete, but I'm not going to get into that because if I start speaking, it gets complicated.",
                    "label": 0
                },
                {
                    "sent": "I think it's time for me.",
                    "label": 0
                },
                {
                    "sent": "It's this year 30.",
                    "label": 0
                },
                {
                    "sent": "That means that I've spoken.",
                    "label": 0
                },
                {
                    "sent": "For one hour.",
                    "label": 0
                },
                {
                    "sent": "What did I say in one hour?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to take a pulse here.",
                    "label": 0
                },
                {
                    "sent": "I just show you a pictures that we took.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately these are these are Spectra.",
                    "label": 0
                },
                {
                    "sent": "Offer for a few problems.",
                    "label": 0
                },
                {
                    "sent": "This is just a problem and Nathan so 11 speculation is this.",
                    "label": 0
                },
                {
                    "sent": "Accelerated gradient methods and so on are optimal.",
                    "label": 0
                },
                {
                    "sent": "When the eigenvalue distribution is bad, the first graph that I have very synthetic problem where the eigenvalues are sort of evenly distributed.",
                    "label": 0
                },
                {
                    "sent": "The conjugate gradient method is very bad for that is going to give you the worst case behavior, but when when when there's a drop in the eigenvalues?",
                    "label": 0
                },
                {
                    "sent": "The conjugate training method really shines an for these problems.",
                    "label": 0
                },
                {
                    "sent": "We see those drops in the eigenvalues and I don't know if anybody has these plots for machine learning, but I'm going to take a break here in order to get.",
                    "label": 0
                },
                {
                    "sent": "Questions and complaints.",
                    "label": 0
                },
                {
                    "sent": "Yes, go ahead.",
                    "label": 0
                },
                {
                    "sent": "There's no microphone.",
                    "label": 0
                },
                {
                    "sent": "I'll repeat your question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so that disciplined way of solving it in exactly guarantees that it's a descent direction.",
                    "label": 0
                },
                {
                    "sent": "So every single step, the conjugate gradient method produces.",
                    "label": 0
                },
                {
                    "sent": "Is it just interaction?",
                    "label": 0
                },
                {
                    "sent": "So when you solve the Newton equations?",
                    "label": 0
                },
                {
                    "sent": "By the conjugate gradient method, the first step is the gradient direction.",
                    "label": 0
                },
                {
                    "sent": "The second one is said the same direction and so on.",
                    "label": 0
                },
                {
                    "sent": "You can prove it very easily and when you get a negative curvature direction then you have to choose a sign.",
                    "label": 0
                },
                {
                    "sent": "But it's a nice property of conjugate gradient.",
                    "label": 1
                },
                {
                    "sent": "It gives you the same direction.",
                    "label": 0
                },
                {
                    "sent": "You don't have to do any of this flipping stuff and so on.",
                    "label": 0
                },
                {
                    "sent": "Yeah, nice property of kasigi.",
                    "label": 0
                },
                {
                    "sent": "Oh talk about step like yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, the microphone.",
                    "label": 0
                },
                {
                    "sent": "I think they didn't hear your question, so alright, so you talked about.",
                    "label": 0
                },
                {
                    "sent": "Step length an linesearch has something important to have in order to to know to not overshoot.",
                    "label": 0
                },
                {
                    "sent": "The update, but maybe it's OK to reverse with the update if you consider the robust SGD scenario where you say I'm going to only visit the region of solutions corresponding to wide enough valleys.",
                    "label": 0
                },
                {
                    "sent": "So, so why care about so much about the very accurate step length and line search?",
                    "label": 0
                },
                {
                    "sent": "Fair enough.",
                    "label": 0
                },
                {
                    "sent": "OK, so the answer is I don't know because I don't understand the regime that you're talking about, right?",
                    "label": 0
                },
                {
                    "sent": "But I want to emphasize one thing.",
                    "label": 0
                },
                {
                    "sent": "I'm advocating the line search in order to take two small steps, actually.",
                    "label": 0
                },
                {
                    "sent": "Voicemail steps not to take steps that are too small.",
                    "label": 0
                },
                {
                    "sent": "To make sure that you.",
                    "label": 0
                },
                {
                    "sent": "If you explore enough, the direction that you have some directions, you cannot move very much.",
                    "label": 0
                },
                {
                    "sent": "If you have a direction where you can move a lot.",
                    "label": 0
                },
                {
                    "sent": "Google not right and there I was referring to the theory community where this stuff is so simple.",
                    "label": 0
                },
                {
                    "sent": "In machine learning the step length is.",
                    "label": 0
                },
                {
                    "sent": "Who knows what it is?",
                    "label": 0
                },
                {
                    "sent": "Because people are just playing with it right?",
                    "label": 0
                },
                {
                    "sent": "It's noise suppression.",
                    "label": 0
                },
                {
                    "sent": "It's doing lots of things.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so related question has to do with all those directions where the eigenvalues are very small.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you take the mini batch approximation of the Hessian then most of the directions of zero eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "So this idea of using something like Newton which blows up of the low eigenvalue directions seems weird.",
                    "label": 0
                },
                {
                    "sent": "No, yes, except that we're not inverting the matrix, but we're applying conjugate gradient to them.",
                    "label": 0
                },
                {
                    "sent": "In conjugate gradient starts by moving along the largest eigenvalues first.",
                    "label": 0
                },
                {
                    "sent": "So it's a real regularizer.",
                    "label": 0
                },
                {
                    "sent": "Applying conjugate gradient solver linear system is very simple, similar to putting A plus gamma.",
                    "label": 0
                },
                {
                    "sent": "I you can show that after you have explored that space you have something of that form.",
                    "label": 0
                },
                {
                    "sent": "Now with conjugate grains you will know when this step becomes very large right?",
                    "label": 0
                },
                {
                    "sent": "But yeah, that's one of the nice properties of that procedure.",
                    "label": 0
                },
                {
                    "sent": "Aren't in more trouble?",
                    "label": 0
                },
                {
                    "sent": "Thanks just piggyback on your side.",
                    "label": 0
                },
                {
                    "sent": "So for your subsampling causing you to method, do you?",
                    "label": 0
                },
                {
                    "sent": "Do you guys have a like a damping factor for your when you solving the CG?",
                    "label": 0
                },
                {
                    "sent": "No, no, no, we never put it ourselves.",
                    "label": 0
                },
                {
                    "sent": "If the problem comes like that with the regularizer, yes.",
                    "label": 0
                },
                {
                    "sent": "But we never added.",
                    "label": 0
                },
                {
                    "sent": "I see and then the second question is so in those two casieri Jim, do you find you have to turn the other step size and things like that more carefully then the deterministic setup?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So for us, what has been a was giving us a lot of trouble?",
                    "label": 0
                },
                {
                    "sent": "Is the generalization issue.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I mean, at some point you were saying it's not an issue or someone, so we work logistic regression problems and we can try some ideas really nicely there so you can see things up efficiency and so on.",
                    "label": 0
                },
                {
                    "sent": "There's no non convexity, but you can train this and then when we go to neural networks that.",
                    "label": 0
                },
                {
                    "sent": "Things get mixed up.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm is now going really well too along to a not so good solution.",
                    "label": 0
                },
                {
                    "sent": "So let's make sure that the increasing the sample size because we do both is slow enough.",
                    "label": 0
                },
                {
                    "sent": "Slower now, so it's going to get to the right solution, but it's not going to take all the benefits that we have, right?",
                    "label": 0
                },
                {
                    "sent": "So it is very annoying this situation at present, the fact that we don't understand if I understand the issue, we will know how to start correcting and Joshua was saying one of the themes.",
                    "label": 0
                },
                {
                    "sent": "I guess of this today is highlighting that question that needs to be answered.",
                    "label": 0
                },
                {
                    "sent": "Why different optimization algorithms end up giving you different generalization properties?",
                    "label": 0
                },
                {
                    "sent": "If one of you writes a paper.",
                    "label": 0
                },
                {
                    "sent": "That has math like a simple intuitive equation doing that.",
                    "label": 0
                },
                {
                    "sent": "That's going to be a great paper.",
                    "label": 0
                },
                {
                    "sent": "And So what does this paper look like?",
                    "label": 0
                },
                {
                    "sent": "It has an equation.",
                    "label": 0
                },
                {
                    "sent": "That has a variable amount of batching, right?",
                    "label": 0
                },
                {
                    "sent": "And in there and in there you have a generalization.",
                    "label": 0
                },
                {
                    "sent": "You're going to testing error.",
                    "label": 0
                },
                {
                    "sent": "And maybe you're not going to do a tremendously complicated neural network.",
                    "label": 0
                },
                {
                    "sent": "Find the simplest example that is going to convince us about what's going on right, and show that right.",
                    "label": 0
                },
                {
                    "sent": "If you move things at a certain rate, generally the final result of generalization is going to be poor.",
                    "label": 0
                },
                {
                    "sent": "So I've been asking some of the theoreticians 'cause they they they say, oh, we have an intuition about this.",
                    "label": 0
                },
                {
                    "sent": "Give me their math.",
                    "label": 0
                },
                {
                    "sent": "There would be a great paper.",
                    "label": 0
                },
                {
                    "sent": "His.",
                    "label": 0
                },
                {
                    "sent": "He's going to say he wrote the paper already.",
                    "label": 0
                },
                {
                    "sent": "So maybe an orthogonal suggestion for people here.",
                    "label": 0
                },
                {
                    "sent": "Regarding 2nd order methods and increasing the batch size and all these kinds of explorations.",
                    "label": 0
                },
                {
                    "sent": "And in the language I was using before looking for these sharp minima.",
                    "label": 0
                },
                {
                    "sent": "Maybe instead of trying to look at.",
                    "label": 0
                },
                {
                    "sent": "Classification image classification problems image net where?",
                    "label": 0
                },
                {
                    "sent": "It looks like optimization is maybe easier and maybe we don't need these things.",
                    "label": 0
                },
                {
                    "sent": "I would suggest people here who are interested in exploring these kinds of methods you talked about.",
                    "label": 0
                },
                {
                    "sent": "To look at papers where.",
                    "label": 0
                },
                {
                    "sent": "People are only able to solve the optimization problem using fancy tricks like curriculum learning where you.",
                    "label": 0
                },
                {
                    "sent": "You go through a series of stages of different tasks that are gradually more difficult, gradually more difficult.",
                    "label": 0
                },
                {
                    "sent": "And if you don't do that, you find really bad solutions.",
                    "label": 0
                },
                {
                    "sent": "It happens in things like when you have memory, networks and things like that, and it happens a lot in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "But there are this variance issues and not just bias issues.",
                    "label": 0
                },
                {
                    "sent": "I mean optimization issues, but I think it might be.",
                    "label": 0
                },
                {
                    "sent": "Worthwhile to focus the empirical exploration of these methods where we already know that.",
                    "label": 0
                },
                {
                    "sent": "The optimization problem is hard that that people have trouble even just training to get to fit the data and not not even.",
                    "label": 0
                },
                {
                    "sent": "Jonesy yeah.",
                    "label": 0
                },
                {
                    "sent": "Very good, I mean another question that I would like to know is why is it that it's turning out to be so simple to train neural networks?",
                    "label": 0
                },
                {
                    "sent": "You don't say so simple compared to what so simple compared to general nonlinear objective nonconvex functions where problem in a million variables you couldn't solve.",
                    "label": 0
                },
                {
                    "sent": "Now I mentioned that the weather forecast that produced every day is solving optimization problem in 5 million variables.",
                    "label": 0
                },
                {
                    "sent": "How can they do that?",
                    "label": 0
                },
                {
                    "sent": "A lot of those nonlinear optimization problems are solved with a good initial starting point, so every weather forecast starts with the solution of the previous weather forecast and there are being done all the time.",
                    "label": 0
                },
                {
                    "sent": "When you do optimal design of the shape of an airplane wing, don't start with a break right?",
                    "label": 0
                },
                {
                    "sent": "You start with a really good solution and what you do is not linear the flow.",
                    "label": 0
                },
                {
                    "sent": "And the drag that happens there is not linear right?",
                    "label": 0
                },
                {
                    "sent": "And and this is something that is actually so interesting because.",
                    "label": 0
                },
                {
                    "sent": "I have called it in the convex optimization community that say people really should only lock the convex problems if you don't have a convex problem, you're doing something wrong.",
                    "label": 0
                },
                {
                    "sent": "Model it that way.",
                    "label": 0
                },
                {
                    "sent": "Right there.",
                    "label": 0
                },
                {
                    "sent": "The engineers will say you know I'm sorry, but the world is not like this.",
                    "label": 0
                },
                {
                    "sent": "I'm working on power flow equations.",
                    "label": 0
                },
                {
                    "sent": "Then there a scene or I'm actually doing my glasses.",
                    "label": 0
                },
                {
                    "sent": "I'm doing progressive glasses and I'm looking at the quality of the glasses.",
                    "label": 0
                },
                {
                    "sent": "I change that and it happens to be not convex, right, nonlinear?",
                    "label": 0
                },
                {
                    "sent": "But again, what you do is you start with one solution and you want to get.",
                    "label": 0
                },
                {
                    "sent": "A better solution.",
                    "label": 0
                },
                {
                    "sent": "That seems like a very easy problem, but when the dimensionality goes up it becomes very, very difficult, right?",
                    "label": 0
                },
                {
                    "sent": "So you have a million variables.",
                    "label": 0
                },
                {
                    "sent": "There are 990,000 ways of going in and not very intelligent way right?",
                    "label": 0
                },
                {
                    "sent": "So that's why these curvatures inline search is approved to be useful.",
                    "label": 0
                },
                {
                    "sent": "But here as I mean we're reminding us where in the stochastic regime we're doing dealing with stochastic objective functions, so nothing is translating so simply.",
                    "label": 0
                },
                {
                    "sent": "But I wanted to convey some of the ideas that we have.",
                    "label": 0
                },
                {
                    "sent": "Here, without my pretending that they're going to be necessarily useful or so on, and by the way, I really.",
                    "label": 0
                },
                {
                    "sent": "Would welcome collaboration.",
                    "label": 0
                },
                {
                    "sent": "Or any post doctoral student who wants to was really interested in the optimization issue, right?",
                    "label": 0
                },
                {
                    "sent": "I would really welcome collaboration because we are not experts in.",
                    "label": 0
                },
                {
                    "sent": "On your networks or so on.",
                    "label": 0
                },
                {
                    "sent": "What is the quota here?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "White males.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So there's this interesting hypothesis related to that we discussed over lunch during the lunch break.",
                    "label": 0
                },
                {
                    "sent": "About overpressurization lots of papers about parameterisation neural Nets.",
                    "label": 0
                },
                {
                    "sent": "There's for example this paper by Jason Uscinski and others.",
                    "label": 0
                },
                {
                    "sent": "At the last I clear suggesting that for many of these deep deep Nets.",
                    "label": 0
                },
                {
                    "sent": "There's a low low dimensional linear manifold in parameter space in which you can find a solution.",
                    "label": 0
                },
                {
                    "sent": "So if this is true of many deep learning problems, how does that you know the things you've been talking about intersect with that potential observation?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so my feeling is that it must be the case that there's a meaningful low dimensional manifold, because otherwise you couldn't solve the problems in such a small amount of time.",
                    "label": 0
                },
                {
                    "sent": "What is the nature of that manifold?",
                    "label": 1
                },
                {
                    "sent": "And so on varies from problem to problem.",
                    "label": 0
                },
                {
                    "sent": "Now Roger mentioned at the beginning of the talk limited memory BFGS, which is something that learns information about the problem, but it doesn't do it perfectly, it just looked.",
                    "label": 0
                },
                {
                    "sent": "It looks at.",
                    "label": 0
                },
                {
                    "sent": "It works because in a lot of the problems there are some manifold that is the really relevant manifold, and if you talk to a climate scientist, he's going to tell you a lot of what this is.",
                    "label": 0
                },
                {
                    "sent": "It has to do with fundamental modes of how the atmosphere goes, blah, blah, blah, and other areas there like that they're not.",
                    "label": 0
                },
                {
                    "sent": "The problems are not such that.",
                    "label": 0
                },
                {
                    "sent": "Every variable matters, right?",
                    "label": 0
                },
                {
                    "sent": "So the only thing I can say to answer your questions in must exist.",
                    "label": 0
                },
                {
                    "sent": "Now I know that in some conference you had French guy talking about manifold.",
                    "label": 0
                },
                {
                    "sent": "Optimization and so on.",
                    "label": 0
                },
                {
                    "sent": "But there's a huge there was a huge gap between that theory and and our reality.",
                    "label": 0
                },
                {
                    "sent": "I believe that the K fact people here listening to their explanation.",
                    "label": 0
                },
                {
                    "sent": "There are as close as they are about thinking about this problem.",
                    "label": 0
                },
                {
                    "sent": "I also want to make it another compliment to Jenny's talk.",
                    "label": 0
                },
                {
                    "sent": "A large dimensional problem the other way you can solve it through the great.",
                    "label": 0
                },
                {
                    "sent": "You don't have a grain, you cannot solve it.",
                    "label": 0
                },
                {
                    "sent": "But in almost everything that I've been talking about here, I'm talking about generic problems.",
                    "label": 0
                },
                {
                    "sent": "And he's talking about the structure of a neural network.",
                    "label": 0
                },
                {
                    "sent": "He's looking for me for most problems, the computational graph the backpropagation finishes at the note at the leaves you propagate the variables you finish, and then you've done.",
                    "label": 0
                },
                {
                    "sent": "It's a black box model.",
                    "label": 0
                },
                {
                    "sent": "So what I really like is start looking at a neural network in a more structured way and I think this shampoo paper does that too.",
                    "label": 0
                },
                {
                    "sent": "But this neural natural gradient idea seems to be a more fundamental way of.",
                    "label": 0
                },
                {
                    "sent": "Attacking things.",
                    "label": 0
                },
                {
                    "sent": "So because a lot of your questions come to the specifics right?",
                    "label": 0
                },
                {
                    "sent": "I'm talking about sort of generic ideas of optimization that hopefully I'm sure that some of the obvious we're developing here, by the way, are going to be useful by.",
                    "label": 0
                },
                {
                    "sent": "Somewhere, somewhere, subsampling you can progressive budget, I'm sure they're going to be useful, But then in this specific setting, I think what you guys are doing is the right thing.",
                    "label": 0
                },
                {
                    "sent": "Looking at what happens to specific structures and then you look at the Chronicle structure an oh that's that's interesting, right?",
                    "label": 0
                },
                {
                    "sent": "The inverse of the of the Chronicle product is the inverse of the product.",
                    "label": 0
                },
                {
                    "sent": "Everything is nice except for that expectation term that that bothers me.",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So my question is so not ignoring the power of these methods, how about the trust region methods?",
                    "label": 0
                },
                {
                    "sent": "How about the convex approximation methods?",
                    "label": 0
                },
                {
                    "sent": "Because I've seen some power about them, but I don't see anybody is using them.",
                    "label": 0
                },
                {
                    "sent": "In the Northern Machine learning community.",
                    "label": 0
                },
                {
                    "sent": "Yeah, trust region methods are.",
                    "label": 0
                },
                {
                    "sent": "It's a very important class of algorithms, right?",
                    "label": 0
                },
                {
                    "sent": "So you create a quadratic model of the objective function.",
                    "label": 1
                },
                {
                    "sent": "And you minimize your objective in depth term.",
                    "label": 0
                },
                {
                    "sent": "If the function is convex.",
                    "label": 0
                },
                {
                    "sent": "You don't need a trust region.",
                    "label": 0
                },
                {
                    "sent": "You want to do a line search instead.",
                    "label": 0
                },
                {
                    "sent": "But if the function is not convex saddle or so on, then you say I have a quadratic model of the objective function.",
                    "label": 0
                },
                {
                    "sent": "Based on information I gathered, where should I go right?",
                    "label": 0
                },
                {
                    "sent": "Where should I move?",
                    "label": 0
                },
                {
                    "sent": "So go to the minimum of this quadratic model in a region that somehow you developed is the Taylor.",
                    "label": 0
                },
                {
                    "sent": "At Taylor region that you trust, right?",
                    "label": 0
                },
                {
                    "sent": "So you minimize them all in that domain.",
                    "label": 0
                },
                {
                    "sent": "Is getting more popular in the theory community to look at trust Vision argues, especially because they are related to nesters.",
                    "label": 0
                },
                {
                    "sent": "Cubic regularization algorithm is very similar to this Nestor of cubic regularization method.",
                    "label": 0
                },
                {
                    "sent": "The one thing the one weakness and I was a big fan of.",
                    "label": 0
                },
                {
                    "sent": "Trust region algorithms.",
                    "label": 0
                },
                {
                    "sent": "And by the way, we have software for constrained optimization that called Nitro that is used by many many people and they may not use the trust region technique because a lot of constraint problems are not convex.",
                    "label": 0
                },
                {
                    "sent": "The problem with the trust region is twofold.",
                    "label": 0
                },
                {
                    "sent": "One of them is is variable.",
                    "label": 0
                },
                {
                    "sent": "So is this the same problem about giving the same importance to all the directions and the other one is that it's scale dependent?",
                    "label": 0
                },
                {
                    "sent": "If you change the variables of the problem, you should change the trust region, but we don't know how to do that.",
                    "label": 0
                },
                {
                    "sent": "It's done out of phase.",
                    "label": 0
                },
                {
                    "sent": "So those are good algorithms, but I found that there are two depends on the choice of the trust region.",
                    "label": 0
                },
                {
                    "sent": "So that you can see, G algorithm is usually implemented with the trust region.",
                    "label": 0
                },
                {
                    "sent": "But you could do it without a trust region if you want.",
                    "label": 0
                },
                {
                    "sent": "But yeah, those are.",
                    "label": 0
                },
                {
                    "sent": "And in our book we have a long chapter, but.",
                    "label": 0
                },
                {
                    "sent": "That idea of trust region, which is quite important, Lee.",
                    "label": 0
                },
                {
                    "sent": "It's all about non convexity.",
                    "label": 0
                },
                {
                    "sent": "Somebody here?",
                    "label": 0
                },
                {
                    "sent": "It looks like Georgia could interrogate me for another 24 hours.",
                    "label": 0
                },
                {
                    "sent": "I take just take the opportunity.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it's interrogating or saying well, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not like this, it's like that.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah, you mentioned in the beginning of the talk that it's better to increase the batch sized rather than diminishing the learning rate.",
                    "label": 0
                },
                {
                    "sent": "And then you talked about the methods that have adaptive batch sizes and adaptive learning rates.",
                    "label": 0
                },
                {
                    "sent": "Is there like any relationship between these two types of methods given the.",
                    "label": 0
                },
                {
                    "sent": "The fact that.",
                    "label": 0
                },
                {
                    "sent": "Increasing batch size is apparently better than diminishing learning rate.",
                    "label": 0
                },
                {
                    "sent": "I don't know if my I made myself clear.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well there are a couple of groups.",
                    "label": 0
                },
                {
                    "sent": "There may be more who say that it is beneficial to increase the batch size.",
                    "label": 0
                },
                {
                    "sent": "And so when you went in the past, you thought it was time to decrease the.",
                    "label": 0
                },
                {
                    "sent": "Steppling it's time to increase the batch size.",
                    "label": 0
                },
                {
                    "sent": "And increases Stephanie.",
                    "label": 0
                },
                {
                    "sent": "You know, with an increasing Bachelor you get better, so that's what they did in those couple of papers.",
                    "label": 0
                },
                {
                    "sent": "But in a limited way.",
                    "label": 0
                },
                {
                    "sent": "And they call it the warm up phase or something like that.",
                    "label": 0
                },
                {
                    "sent": "And they're claiming hey paper, they have a face where they do that and mini batches being used for awhile.",
                    "label": 0
                },
                {
                    "sent": "And then it's increased in the back and the step length is increased and they do that again and they do that in a step way.",
                    "label": 0
                },
                {
                    "sent": "If that answers your questions, the two of them are tight together.",
                    "label": 0
                },
                {
                    "sent": "And of course, this is all this is not about how far to go.",
                    "label": 0
                },
                {
                    "sent": "It's all about noise suppression, right?",
                    "label": 0
                },
                {
                    "sent": "And their motivation?",
                    "label": 0
                },
                {
                    "sent": "From what kind man told me is to do better optimization to do optimization faster.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yo she was asking about generalization issues, right?",
                    "label": 0
                },
                {
                    "sent": "I was talking because I'm not worried about that.",
                    "label": 0
                },
                {
                    "sent": "I want I'm worried about optimization.",
                    "label": 0
                },
                {
                    "sent": "I want to do the optimization faster.",
                    "label": 0
                },
                {
                    "sent": "That's why he's doing larger mini batches, right?",
                    "label": 0
                },
                {
                    "sent": "Different groups.",
                    "label": 0
                },
                {
                    "sent": "You started your talk with these different axes, and one of them was.",
                    "label": 0
                },
                {
                    "sent": "Moving in the Newton direction so stochastic Newton like methods and I wasn't sure by the end if you had really.",
                    "label": 0
                },
                {
                    "sent": "Fill that spot with something.",
                    "label": 0
                },
                {
                    "sent": "Mean this so there's the key fact type methods.",
                    "label": 0
                },
                {
                    "sent": "Is there some?",
                    "label": 0
                },
                {
                    "sent": "Is there like really you successful stochastic Newton version that we can use from your net?",
                    "label": 0
                },
                {
                    "sent": "That stochastic Newton method was very noisy gradients that we tried to do does not work.",
                    "label": 0
                },
                {
                    "sent": "But what is not there?",
                    "label": 0
                },
                {
                    "sent": "There is more slides that you don't want to see because I have like a million slides.",
                    "label": 0
                },
                {
                    "sent": "I realize this is such a rich topic.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "You can do quasi Newton.",
                    "label": 0
                },
                {
                    "sent": "As long as the batch is increased and much less than what we expected, don't try to do quasi Newton in the very noisy regime.",
                    "label": 0
                },
                {
                    "sent": "But once the batch size hasn't depending on the problem is 800 or something like that.",
                    "label": 0
                },
                {
                    "sent": "Not only can we do question Newton, but were actually employing line searches to our benefit, so that's a paper that we published in acnl progressive batching question is an algorithm that I find very exciting because the numbers look very good numbers that I've never seen before.",
                    "label": 0
                },
                {
                    "sent": "But for your question there on that axis of 2nd order methods.",
                    "label": 0
                },
                {
                    "sent": "There's a K FAQ.",
                    "label": 0
                },
                {
                    "sent": "Inexact Newton method with subsampling.",
                    "label": 0
                },
                {
                    "sent": "So the general algorithm and the third daughter and they would put their is causing human algorithms.",
                    "label": 0
                },
                {
                    "sent": "But only within the progressive approach, where the gradients of good quality and by the way we have to do some robust without a paper about how to do robust optimization given that the sample size is changed and your brother has recently or last year paper about synchronous optimization where they are worrying about fault tolerance, it turns out it was pretty much the same ideas that if we use those we can do stable question.",
                    "label": 0
                },
                {
                    "sent": "You can updating so there are three methods along those lines.",
                    "label": 0
                },
                {
                    "sent": "And by the way, along the other line of suppressing noise.",
                    "label": 0
                },
                {
                    "sent": "There's a whole bunch of algorithms, class of items that I did not mention.",
                    "label": 1
                },
                {
                    "sent": "Those are the aggregated gradient methods Sag's, V, RG saga.",
                    "label": 0
                },
                {
                    "sent": "This is a beautiful algorithm to sign for the some finite some problem.",
                    "label": 0
                },
                {
                    "sent": "And they at the expense of storage.",
                    "label": 0
                },
                {
                    "sent": "In the case of sag, they can get a faster rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "But I don't know that these arguments will ever be useful in machine learning.",
                    "label": 0
                },
                {
                    "sent": "The reason why I did not consider them here is because I think even though they are among the most original ideas that people have come up with Mark Schmidt, Canadian.",
                    "label": 0
                },
                {
                    "sent": "What's that happens in Canada?",
                    "label": 0
                },
                {
                    "sent": "March mid will initiator of those ideas.",
                    "label": 0
                },
                {
                    "sent": "I really designed for finite sum problem or that they will do well if you look at generally testing error.",
                    "label": 0
                },
                {
                    "sent": "They're not so good, but those noise suppressing.",
                    "label": 0
                },
                {
                    "sent": "They also variance.",
                    "label": 0
                },
                {
                    "sent": "SV RG is stochastic variance reduction method.",
                    "label": 0
                },
                {
                    "sent": "Very good way to do things.",
                    "label": 0
                },
                {
                    "sent": "So the amount of solid theoretical work around tensors is still very limited, and.",
                    "label": 0
                },
                {
                    "sent": "Do you think the advancement in that field will help develop modern optimization techniques that can?",
                    "label": 0
                },
                {
                    "sent": "Like fat with faster convergence rates and with for example, currently we haven't thought much beyond second order derivatives because that's perhaps the most we can deal with, but do you think the advancements in that field can result in faster optimization techniques?",
                    "label": 0
                },
                {
                    "sent": "OK, I think your question has two branches in the machine learning in the neural network context, tensors occur naturally, so exploiting them is a natural thing to do.",
                    "label": 0
                },
                {
                    "sent": "Now there's a whole field called tensor tensor linear algebra that has to do with multi attributes.",
                    "label": 0
                },
                {
                    "sent": "That's another interesting area on how you know how you represent tensors.",
                    "label": 0
                },
                {
                    "sent": "How you work with them.",
                    "label": 0
                },
                {
                    "sent": "Now you asked me if to predict if you think it's good.",
                    "label": 0
                },
                {
                    "sent": "It's a good area to explore.",
                    "label": 0
                },
                {
                    "sent": "Since I'm not in machine learning, I will not make any predictions.",
                    "label": 0
                },
                {
                    "sent": "All my opinions are related to what is interesting to study, not whether they're going to be successful or not.",
                    "label": 0
                },
                {
                    "sent": "I think both versions of tensors are worth further study.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "4 minutes and 11 seconds.",
                    "label": 0
                }
            ]
        }
    }
}