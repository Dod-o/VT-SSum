{
    "id": "cavydyzzd2mqx4gdcmtsiaa22da7aikb",
    "title": "Learning Good Edit Similarities with Generalization Guarantees",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Aur\u00e9lien Bellet, University of Saint-Etienne"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_bellet_guarantees/",
    "segmentation": [
        [
            "Hi everyone.",
            "So.",
            "My name is already over there and I'm going to present the joint work with.",
            "And maximum.",
            "I should say maybe I'm a bit of an outsider decision.",
            "My talk is going to be more about machine learning then the mining, but I hope you enjoy it anyway.",
            "The title is learning good.",
            "Edit Similarities with generalization guarantees and what we will be doing is trying to learn string similarity for classification, for which we have general generalization guarantees that it that it performs well on unseen examples, but also that we have a theoretical guarantee that it will lead to low error classifier."
        ],
        [
            "So I'm going to talk a little bit about their similarity."
        ],
        [
            "Learning a very common approach in supervised classification is to learn to classify objects using a pairwise similarity or distance function, and you have many successful examples among them K nearest neighbors supported on machines.",
            "And as a matter of fact, the best way to get a good appropriate similarity function for your task is to learn it from data."
        ],
        [
            "So we know the case we want to learn a similarity function K between pairs of objects that imply a new instance space in which the performance of a given classification algorithm is improved.",
            "So it's going here if you start with this instance space, you run an appropriate OG function K and you end up with in you instance space in which obviously for example K nearest neighbors is going to perform better than here.",
            "And when you're talking about numerical data, feature vectors, very popular approach is to learn the transformation matrix of a manager.",
            "This distance of this form.",
            "Is huge to extensively for improving results in classification."
        ],
        [
            "But in our case we talking about strings, so we don't want to.",
            "We don't want to numerical seniority want to string similarity for stringification, and as I said in the beginning, we want it to be guaranteed to generalize well to new examples.",
            "And we also want I want it to be too probably induce low error classifiers for the task.",
            "Looking at theoretical problems and in order to do that, we're going to make use of the social theory of learning, with epsilon gamma Tau similarity function from that can."
        ],
        [
            "And that's"
        ],
        [
            "I'm gonna explain right now.",
            "So, but generally they wanted a definite definition of what what makes a good similarity function, and you want it.",
            "In terms of natural and direct properties, that's the first requirement.",
            "They wanted something that include the usual notion of a good kernel and decision theory, but without requiring the positive semidefinite Ness.",
            "And third, they of course wanted that you can derive learning guarantees from this definition."
        ],
        [
            "So here's the definition.",
            "I should say we're talking maybe theater.",
            "We talking about binary classification here.",
            "So we have supervised binary classification.",
            "So we have examples and their labels minus one over this one.",
            "And this definition.",
            "It looks nasty, but it is.",
            "It is the intuition behind it is quite simple.",
            "So we say that assimilate similarity function K is an epsilon gamma.",
            "Tell good similarity function for a given problem.",
            "If we're going to assume we are given a set of what we're going to reasonable points, I come back to that later and explain how we can have this set.",
            "But for now, let's assume we are given a set of so called reasonable points, right?",
            "And what the first condition says here is that most of the examples, so minus one minus epsilon probability mass examples.",
            "Should satisfy this, which is essentially saying that your example should be on average more similar to reasonable points of the same class with the same label.",
            "Then two reasonable points of the other class and they should be should be gamma.",
            "More similar, at least, so this is this can be seen as some kind of margin.",
            "And the second condition is just to say that our proportion of the examples or."
        ],
        [
            "Code reasonable.",
            "So I'm going to give just a very quick and simple examples of.",
            "What is behind the definition?",
            "So here we have like a really simple problem with green dots and red dots.",
            "We could see them as Apple and oranges I guess, and so we're given the reasonable points here.",
            "It's a CNG once again that come back later to hold can find them.",
            "And if we take very simple similarity function which would be minus the Euclidean distance, then we can we can compute.",
            "These power meters of the definition, so first thing is that we have to equal 3 / 8 because we have three reasonable points, I would have eight points in total an if we choose to fix epsilon to 0.",
            "Here that means that all points should respect the gamma margin that we have, and then we can compute actually this margin.",
            "In here it's .03."
        ],
        [
            "But of course you could choose to ignore a point, so just say that.",
            "747 points respects the proposed margin, so ignore this point.",
            "For example, and you can get a better margin."
        ],
        [
            "The idea I hope you.",
            "You gotta be intuition Ann.",
            "So this is all nice, but what what can we do with that?",
            "What we're going to do next if we have such a seniority function that we space definition is to use the similarity function, the similarity scores to the reasonable points as features.",
            "So we're going to build that new instance space.",
            "I was talking about earlier.",
            "So here you have GC and a that were all reasonable points, and so you're just going to start this new instance space where the features are missing, right discord.",
            "And what I can tell us about this is that so if we respect the definition or K is absolutely metal good, then we have in this space linear separator Alfa.",
            "So a hyperplane basically that has error close to epsilon and margin gamma.",
            "Fill the definition.",
            "We know we there's a.",
            "There's a error classifier in there."
        ],
        [
            "Anne.",
            "The good news is that we can also learn efficiently using pretty simple linear program, where on the left hand side we're trying to minimize the margin violations.",
            "And here we're just winterizing with Norm.",
            "This looks very much like a why not miss VM.",
            "Anne told you would come back to the reasonable points and actually thanks to this L1 regularization, we're going to be able to automatically select among the training examples the points that are going to serve that it doesn't work synergies."
        ],
        [
            "I don't know if you're familiar with this will be most of you are, but.",
            "About the effects of at one norm constraint or regularization and sparsity.",
            "But essentially this is more geometric interpretation.",
            "Where you're trying to optimize that blue function here while being satisfying the constraint here is Delta can straighten one constraint.",
            "And while we're there to constraint, you're going to pretty much hit devolve at any place and the other one case you're going to be attracted by the edges.",
            "You will end up zero out around 4 minutes and you're anywhere vector, and that that's how that's how we compute the original point.",
            "So in the end, after running non zero coordinates in the in, the Alpha separator are going to be the reasonable boat."
        ],
        [
            "OK, I hope it's all clear and now we're going to move up to or actual contribution.",
            "The idea is to actually somewhat optimized that definition so learner similarity function so that it fulfills this death."
        ],
        [
            "And of course there's too much additional motivations for this.",
            "The first one is that the definition gives us a natural objective that we can optimize, which is that of the definition, and we know that if we satisfy this, then we will be able to have a lower class value.",
            "So that's why the nice thing to have and 2nd motivation is that most similarity function for structured data.",
            "So if we're talking about strings, trees, graphs, but other than more not PSD positive semi definite.",
            "And therefore they're not believe kernels, and you can use.",
            "You can use them directly in SVM.",
            "I know districts around there to do it anyway, but well, it's it just seems to us that this is well suited framework for for those are non PSTN."
        ],
        [
            "So in this particular work, we're going to focus on distributed distance.",
            "That's what we're going to try to optimize.",
            "So the string distance for Levenshtein distance between two strings.",
            "X&Y is the minimum number of operations to journey to turn X into Y and the allowable operations are insertion, deletion, and substitution of symbols, so you have a quick example here.",
            "So the distance between ABBAA is 2 because you need at least two operations to go from one to another, such as substituting.",
            "Open a Dan deleting.",
            "And you can also define a more general version.",
            "He see when will you use a coast for a forest operation?",
            "So you have basically a cost matrix here that defines the cost for each operation and now we can.",
            "You can use this class to find the cheapest sequence of operation to get from one string to another and of course it might change the operation you use from this case with this one depending on the actual cost."
        ],
        [
            "OK, so we're going to try to optimize discussed that would be trying to do and then exist decent amount of literature on learning it.",
            "It cost or it is probabilities in some cases.",
            "But they all share."
        ],
        [
            "The same drawbacks, essentially an the first one is that they usually are each routine procedures.",
            "And I think that that that can be costly, especially because computing edit distance between long strings is costly.",
            "So if you have to do it several times because it's intuitive."
        ],
        [
            "Use time.",
            "Second one is that they usually make user friendly positive pairs, that is pairs of the same label.",
            "Trying to move them closer together and they don't really use negative pairs.",
            "So we would like to."
        ],
        [
            "Be able to use negative peers as well.",
            "An evolve older, not learn to be epsilon, gamma.",
            "To do it, of course.",
            "So usually you optimize some quantities that could be maximum likelihood or something like that, which isn't theoretically theoretically linked to your performance of your correct classifier.",
            "So essentially you know similarity function and you hope that it's going to improve the classifier, But you don't know it for sure.",
            "We want some theoretical guarantee that order CLT functions."
        ],
        [
            "Perform well.",
            "So we're going to or method is going to address these three issues, so the first one is that iterative approach problem.",
            "And this is because what I explained a bit earlier that the optimal script, the best sequence of operation depends on the custom so.",
            "That's why you have to recompute the distance when you change it."
        ],
        [
            "When you change the glass, and in order case we're going to just define a simpler edit similarity function.",
            "E.g where will take the sequence of operation of the standard edit distance, the one where the cost our one and only after that will apply specific costs to those operations.",
            "So now or it is script, it's not going to change, we just tweak their costs."
        ],
        [
            "And we'll optimize this cagey.",
            "This is just a bit different to make sure it's in minus one one."
        ],
        [
            "As required by the definition.",
            "Now the next problem is that to optimize the goodness of seniority, presents 22 difficulties and the first one is that if you optimize directly, you get a nonconvex optimization problem.",
            "That's a good and also we do.",
            "We do not notice have reasonable points, because here we we get it when we learn the pacifier, but here we are.",
            "Before that, we're trying to learn this narrative.",
            "So the solution to the first issue is that we just gotta optimize criterion that bounds the one of the definition.",
            "And essentially, while in the definition you're trying to average over the similarities too with respect to the reasonable points here, we just require it to be true with respect to each reasonable points individually, so that's a stricter condition.",
            "But if we know that if we specify this."
        ],
        [
            "We satisfy the definition and the second issue is just.",
            "So did the fact that we don't have the reasonable point effects and we don't have taking all points as visible responding, not good idea 'cause we're going to have another constraint problem.",
            "And if we think about it, we can see reasonable points as being some good representatives of subset of the data.",
            "So keeping that idea in mind, we model this by introducing an indicator matching function FN that associate's to each training example.",
            "Examples from the training sample and how did we do this and experiments?",
            "We just matched each example 2X P nearest neighbors of the same class into its P phonics neighbors of the opposite class using the."
        ],
        [
            "Standard edit distance.",
            "OK, so.",
            "We have to keep in mind that so we're trying to move closer to pairs of the same class and further away to those of opposite class, and this is the convex formulation we could come up with, and it's essentially as I said, optimizing the definition more or less where we're trying to minimize here the margin violations which we separate into cases when the pairs of examples are different class.",
            "We want it.",
            "We want this to be greater than some variable V1, and when they are of the same class, you want them to be.",
            "Smaller than some other value will be 2 and then you have to be to be one baby two, which is your margin.",
            "Pretty much that you can set as a parameter like the desired margin you want in your particular problem and the other parameter is the regularization edit list.",
            "So we optimize this and we get some edit costs that hopefully you end up with this humidity that respect."
        ],
        [
            "Definition, and so we as I say we just don't want to hope we want guarantees.",
            "And So what we want to do is to bound the true error on or for a bit muddled.",
            "By essentially so bounding the expected loss over the whole distribution.",
            "And we could do this using uniform stability, uniform stability.",
            "The idea behind it is fairly simple.",
            "It's about studying the impact of a small change in the training sample and see if the accumulated loss of your model changes.",
            "Order another one and the fact is if you can, if you can bound that change in the in the in the loss, then you can drive it a generalization down."
        ],
        [
            "So I'm not going.",
            "I'm not going to go into the details of the proofs, but essentially we could prove that the algorithm is uniform stability.",
            "In Kappa over NT&T, being the number of training examples.",
            "And from that we could derive a generalization balance of bounding the true loss by the empirical loss over showing example and some term that depends on empty.",
            "And what we like about that build as well is that it is that it's independent from the sense of the alphabet, but doesn't appear in the Dom.",
            "So it should be quite robust to probably big alphabets.",
            "That's what we expect."
        ],
        [
            "And.",
            "At last a few experiments we just experimented over a very simple task which was classifying words as either French or English English, and we we studied three different similarities in red.",
            "Here you have the standard edit distance with the coastal Sector 1.",
            "Um?",
            "Before you say that, here is the number of training examples used for under cost, and this is the the accuracy.",
            "So this is the baseline.",
            "Then we have in blue.",
            "Method that is based on probabilistic models and it's running edit probabilities using an iterative approach based on expectation maximization and optimizes if it's based on maximum likelihood essentially.",
            "And after a while we see that this message outperforms the baseline.",
            "OK, and or messages in green here and we can see that both our personal performs the other two similarities, but also that it converges really fast.",
            "We can outperform the baseline with really few examples, you know 50.",
            "And and we converge really fast as well."
        ],
        [
            "And I don't know advantage of this that we notice is that when you later on build your classifiers from the similarity function you have.",
            "It turns out that learning Assirati to make it fit the definition better also.",
            "Results in sparser models.",
            "So we have a model that performs better, but it is also sparse."
        ],
        [
            "So as a quick recap, we made use of the framework craft by the can to create a novel and efficient way to learn string similarities.",
            "And we showed that the synergies probably generalize well and that they need to lower our classifiers for the task.",
            "Future work could include adapting a framework for treat it cost running, which is kind of straightforward.",
            "We can keep the same approach the same, and just.",
            "Change the string edits script in the message.",
            "Then take a treated script instead.",
            "An would like also to learn maybe other types of similarities, including numerical simulations.",
            "Thank you.",
            "So when you did the comparisons with the EM algorithm, is that also using an L1 regularizer so that we're normalizing for the difference in the algorithm and the difference in the smoothing?",
            "But you have to differentiate two things.",
            "There is the edit cost learning procedures.",
            "So how you run the task and in our case?"
        ],
        [
            "Remember, in convex program it's not as one regularised.",
            "What is it?",
            "1 regularised is how you learn the separator after that.",
            "And this is the same method for all the similarities, so it's.",
            "So nice.",
            "I'm a little bit puzzled on the task.",
            "In task is classifying if a word it's English or French word right?",
            "You can apply these on.",
            "More complex task, like for example detective noun phrases are qualifying or not, so we will do the task.",
            "Dancing seems a little bit simple, so instead of this kind of East answer is positive.",
            "Any task and then you can really understand the value of your approach.",
            "Yeah, I told you we we, we just had this small experiments going on for this paper, which we are definitely looking into more expensive things or indifferent.",
            "Data sets and we're looking into that.",
            "You're right that we need probably more experiments to be sure.",
            "OK, since we have the 5th talk as well."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi everyone.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "My name is already over there and I'm going to present the joint work with.",
                    "label": 0
                },
                {
                    "sent": "And maximum.",
                    "label": 0
                },
                {
                    "sent": "I should say maybe I'm a bit of an outsider decision.",
                    "label": 0
                },
                {
                    "sent": "My talk is going to be more about machine learning then the mining, but I hope you enjoy it anyway.",
                    "label": 0
                },
                {
                    "sent": "The title is learning good.",
                    "label": 0
                },
                {
                    "sent": "Edit Similarities with generalization guarantees and what we will be doing is trying to learn string similarity for classification, for which we have general generalization guarantees that it that it performs well on unseen examples, but also that we have a theoretical guarantee that it will lead to low error classifier.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk a little bit about their similarity.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning a very common approach in supervised classification is to learn to classify objects using a pairwise similarity or distance function, and you have many successful examples among them K nearest neighbors supported on machines.",
                    "label": 0
                },
                {
                    "sent": "And as a matter of fact, the best way to get a good appropriate similarity function for your task is to learn it from data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we know the case we want to learn a similarity function K between pairs of objects that imply a new instance space in which the performance of a given classification algorithm is improved.",
                    "label": 1
                },
                {
                    "sent": "So it's going here if you start with this instance space, you run an appropriate OG function K and you end up with in you instance space in which obviously for example K nearest neighbors is going to perform better than here.",
                    "label": 1
                },
                {
                    "sent": "And when you're talking about numerical data, feature vectors, very popular approach is to learn the transformation matrix of a manager.",
                    "label": 0
                },
                {
                    "sent": "This distance of this form.",
                    "label": 0
                },
                {
                    "sent": "Is huge to extensively for improving results in classification.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in our case we talking about strings, so we don't want to.",
                    "label": 0
                },
                {
                    "sent": "We don't want to numerical seniority want to string similarity for stringification, and as I said in the beginning, we want it to be guaranteed to generalize well to new examples.",
                    "label": 1
                },
                {
                    "sent": "And we also want I want it to be too probably induce low error classifiers for the task.",
                    "label": 0
                },
                {
                    "sent": "Looking at theoretical problems and in order to do that, we're going to make use of the social theory of learning, with epsilon gamma Tau similarity function from that can.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm gonna explain right now.",
                    "label": 0
                },
                {
                    "sent": "So, but generally they wanted a definite definition of what what makes a good similarity function, and you want it.",
                    "label": 1
                },
                {
                    "sent": "In terms of natural and direct properties, that's the first requirement.",
                    "label": 1
                },
                {
                    "sent": "They wanted something that include the usual notion of a good kernel and decision theory, but without requiring the positive semidefinite Ness.",
                    "label": 0
                },
                {
                    "sent": "And third, they of course wanted that you can derive learning guarantees from this definition.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the definition.",
                    "label": 0
                },
                {
                    "sent": "I should say we're talking maybe theater.",
                    "label": 0
                },
                {
                    "sent": "We talking about binary classification here.",
                    "label": 0
                },
                {
                    "sent": "So we have supervised binary classification.",
                    "label": 0
                },
                {
                    "sent": "So we have examples and their labels minus one over this one.",
                    "label": 0
                },
                {
                    "sent": "And this definition.",
                    "label": 0
                },
                {
                    "sent": "It looks nasty, but it is.",
                    "label": 0
                },
                {
                    "sent": "It is the intuition behind it is quite simple.",
                    "label": 0
                },
                {
                    "sent": "So we say that assimilate similarity function K is an epsilon gamma.",
                    "label": 1
                },
                {
                    "sent": "Tell good similarity function for a given problem.",
                    "label": 1
                },
                {
                    "sent": "If we're going to assume we are given a set of what we're going to reasonable points, I come back to that later and explain how we can have this set.",
                    "label": 0
                },
                {
                    "sent": "But for now, let's assume we are given a set of so called reasonable points, right?",
                    "label": 0
                },
                {
                    "sent": "And what the first condition says here is that most of the examples, so minus one minus epsilon probability mass examples.",
                    "label": 0
                },
                {
                    "sent": "Should satisfy this, which is essentially saying that your example should be on average more similar to reasonable points of the same class with the same label.",
                    "label": 0
                },
                {
                    "sent": "Then two reasonable points of the other class and they should be should be gamma.",
                    "label": 0
                },
                {
                    "sent": "More similar, at least, so this is this can be seen as some kind of margin.",
                    "label": 0
                },
                {
                    "sent": "And the second condition is just to say that our proportion of the examples or.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Code reasonable.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give just a very quick and simple examples of.",
                    "label": 0
                },
                {
                    "sent": "What is behind the definition?",
                    "label": 1
                },
                {
                    "sent": "So here we have like a really simple problem with green dots and red dots.",
                    "label": 0
                },
                {
                    "sent": "We could see them as Apple and oranges I guess, and so we're given the reasonable points here.",
                    "label": 0
                },
                {
                    "sent": "It's a CNG once again that come back later to hold can find them.",
                    "label": 0
                },
                {
                    "sent": "And if we take very simple similarity function which would be minus the Euclidean distance, then we can we can compute.",
                    "label": 0
                },
                {
                    "sent": "These power meters of the definition, so first thing is that we have to equal 3 / 8 because we have three reasonable points, I would have eight points in total an if we choose to fix epsilon to 0.",
                    "label": 0
                },
                {
                    "sent": "Here that means that all points should respect the gamma margin that we have, and then we can compute actually this margin.",
                    "label": 0
                },
                {
                    "sent": "In here it's .03.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But of course you could choose to ignore a point, so just say that.",
                    "label": 0
                },
                {
                    "sent": "747 points respects the proposed margin, so ignore this point.",
                    "label": 0
                },
                {
                    "sent": "For example, and you can get a better margin.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The idea I hope you.",
                    "label": 0
                },
                {
                    "sent": "You gotta be intuition Ann.",
                    "label": 0
                },
                {
                    "sent": "So this is all nice, but what what can we do with that?",
                    "label": 0
                },
                {
                    "sent": "What we're going to do next if we have such a seniority function that we space definition is to use the similarity function, the similarity scores to the reasonable points as features.",
                    "label": 1
                },
                {
                    "sent": "So we're going to build that new instance space.",
                    "label": 0
                },
                {
                    "sent": "I was talking about earlier.",
                    "label": 0
                },
                {
                    "sent": "So here you have GC and a that were all reasonable points, and so you're just going to start this new instance space where the features are missing, right discord.",
                    "label": 0
                },
                {
                    "sent": "And what I can tell us about this is that so if we respect the definition or K is absolutely metal good, then we have in this space linear separator Alfa.",
                    "label": 0
                },
                {
                    "sent": "So a hyperplane basically that has error close to epsilon and margin gamma.",
                    "label": 0
                },
                {
                    "sent": "Fill the definition.",
                    "label": 0
                },
                {
                    "sent": "We know we there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a error classifier in there.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The good news is that we can also learn efficiently using pretty simple linear program, where on the left hand side we're trying to minimize the margin violations.",
                    "label": 1
                },
                {
                    "sent": "And here we're just winterizing with Norm.",
                    "label": 0
                },
                {
                    "sent": "This looks very much like a why not miss VM.",
                    "label": 0
                },
                {
                    "sent": "Anne told you would come back to the reasonable points and actually thanks to this L1 regularization, we're going to be able to automatically select among the training examples the points that are going to serve that it doesn't work synergies.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I don't know if you're familiar with this will be most of you are, but.",
                    "label": 0
                },
                {
                    "sent": "About the effects of at one norm constraint or regularization and sparsity.",
                    "label": 1
                },
                {
                    "sent": "But essentially this is more geometric interpretation.",
                    "label": 1
                },
                {
                    "sent": "Where you're trying to optimize that blue function here while being satisfying the constraint here is Delta can straighten one constraint.",
                    "label": 0
                },
                {
                    "sent": "And while we're there to constraint, you're going to pretty much hit devolve at any place and the other one case you're going to be attracted by the edges.",
                    "label": 0
                },
                {
                    "sent": "You will end up zero out around 4 minutes and you're anywhere vector, and that that's how that's how we compute the original point.",
                    "label": 0
                },
                {
                    "sent": "So in the end, after running non zero coordinates in the in, the Alpha separator are going to be the reasonable boat.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I hope it's all clear and now we're going to move up to or actual contribution.",
                    "label": 0
                },
                {
                    "sent": "The idea is to actually somewhat optimized that definition so learner similarity function so that it fulfills this death.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course there's too much additional motivations for this.",
                    "label": 0
                },
                {
                    "sent": "The first one is that the definition gives us a natural objective that we can optimize, which is that of the definition, and we know that if we satisfy this, then we will be able to have a lower class value.",
                    "label": 1
                },
                {
                    "sent": "So that's why the nice thing to have and 2nd motivation is that most similarity function for structured data.",
                    "label": 0
                },
                {
                    "sent": "So if we're talking about strings, trees, graphs, but other than more not PSD positive semi definite.",
                    "label": 1
                },
                {
                    "sent": "And therefore they're not believe kernels, and you can use.",
                    "label": 0
                },
                {
                    "sent": "You can use them directly in SVM.",
                    "label": 0
                },
                {
                    "sent": "I know districts around there to do it anyway, but well, it's it just seems to us that this is well suited framework for for those are non PSTN.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this particular work, we're going to focus on distributed distance.",
                    "label": 0
                },
                {
                    "sent": "That's what we're going to try to optimize.",
                    "label": 0
                },
                {
                    "sent": "So the string distance for Levenshtein distance between two strings.",
                    "label": 0
                },
                {
                    "sent": "X&Y is the minimum number of operations to journey to turn X into Y and the allowable operations are insertion, deletion, and substitution of symbols, so you have a quick example here.",
                    "label": 1
                },
                {
                    "sent": "So the distance between ABBAA is 2 because you need at least two operations to go from one to another, such as substituting.",
                    "label": 0
                },
                {
                    "sent": "Open a Dan deleting.",
                    "label": 0
                },
                {
                    "sent": "And you can also define a more general version.",
                    "label": 0
                },
                {
                    "sent": "He see when will you use a coast for a forest operation?",
                    "label": 0
                },
                {
                    "sent": "So you have basically a cost matrix here that defines the cost for each operation and now we can.",
                    "label": 0
                },
                {
                    "sent": "You can use this class to find the cheapest sequence of operation to get from one string to another and of course it might change the operation you use from this case with this one depending on the actual cost.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we're going to try to optimize discussed that would be trying to do and then exist decent amount of literature on learning it.",
                    "label": 1
                },
                {
                    "sent": "It cost or it is probabilities in some cases.",
                    "label": 0
                },
                {
                    "sent": "But they all share.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same drawbacks, essentially an the first one is that they usually are each routine procedures.",
                    "label": 0
                },
                {
                    "sent": "And I think that that that can be costly, especially because computing edit distance between long strings is costly.",
                    "label": 0
                },
                {
                    "sent": "So if you have to do it several times because it's intuitive.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use time.",
                    "label": 0
                },
                {
                    "sent": "Second one is that they usually make user friendly positive pairs, that is pairs of the same label.",
                    "label": 1
                },
                {
                    "sent": "Trying to move them closer together and they don't really use negative pairs.",
                    "label": 0
                },
                {
                    "sent": "So we would like to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be able to use negative peers as well.",
                    "label": 0
                },
                {
                    "sent": "An evolve older, not learn to be epsilon, gamma.",
                    "label": 0
                },
                {
                    "sent": "To do it, of course.",
                    "label": 0
                },
                {
                    "sent": "So usually you optimize some quantities that could be maximum likelihood or something like that, which isn't theoretically theoretically linked to your performance of your correct classifier.",
                    "label": 0
                },
                {
                    "sent": "So essentially you know similarity function and you hope that it's going to improve the classifier, But you don't know it for sure.",
                    "label": 0
                },
                {
                    "sent": "We want some theoretical guarantee that order CLT functions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Perform well.",
                    "label": 0
                },
                {
                    "sent": "So we're going to or method is going to address these three issues, so the first one is that iterative approach problem.",
                    "label": 1
                },
                {
                    "sent": "And this is because what I explained a bit earlier that the optimal script, the best sequence of operation depends on the custom so.",
                    "label": 1
                },
                {
                    "sent": "That's why you have to recompute the distance when you change it.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When you change the glass, and in order case we're going to just define a simpler edit similarity function.",
                    "label": 1
                },
                {
                    "sent": "E.g where will take the sequence of operation of the standard edit distance, the one where the cost our one and only after that will apply specific costs to those operations.",
                    "label": 0
                },
                {
                    "sent": "So now or it is script, it's not going to change, we just tweak their costs.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we'll optimize this cagey.",
                    "label": 0
                },
                {
                    "sent": "This is just a bit different to make sure it's in minus one one.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As required by the definition.",
                    "label": 0
                },
                {
                    "sent": "Now the next problem is that to optimize the goodness of seniority, presents 22 difficulties and the first one is that if you optimize directly, you get a nonconvex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "That's a good and also we do.",
                    "label": 1
                },
                {
                    "sent": "We do not notice have reasonable points, because here we we get it when we learn the pacifier, but here we are.",
                    "label": 0
                },
                {
                    "sent": "Before that, we're trying to learn this narrative.",
                    "label": 0
                },
                {
                    "sent": "So the solution to the first issue is that we just gotta optimize criterion that bounds the one of the definition.",
                    "label": 1
                },
                {
                    "sent": "And essentially, while in the definition you're trying to average over the similarities too with respect to the reasonable points here, we just require it to be true with respect to each reasonable points individually, so that's a stricter condition.",
                    "label": 0
                },
                {
                    "sent": "But if we know that if we specify this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We satisfy the definition and the second issue is just.",
                    "label": 1
                },
                {
                    "sent": "So did the fact that we don't have the reasonable point effects and we don't have taking all points as visible responding, not good idea 'cause we're going to have another constraint problem.",
                    "label": 0
                },
                {
                    "sent": "And if we think about it, we can see reasonable points as being some good representatives of subset of the data.",
                    "label": 1
                },
                {
                    "sent": "So keeping that idea in mind, we model this by introducing an indicator matching function FN that associate's to each training example.",
                    "label": 1
                },
                {
                    "sent": "Examples from the training sample and how did we do this and experiments?",
                    "label": 0
                },
                {
                    "sent": "We just matched each example 2X P nearest neighbors of the same class into its P phonics neighbors of the opposite class using the.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Standard edit distance.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We have to keep in mind that so we're trying to move closer to pairs of the same class and further away to those of opposite class, and this is the convex formulation we could come up with, and it's essentially as I said, optimizing the definition more or less where we're trying to minimize here the margin violations which we separate into cases when the pairs of examples are different class.",
                    "label": 1
                },
                {
                    "sent": "We want it.",
                    "label": 0
                },
                {
                    "sent": "We want this to be greater than some variable V1, and when they are of the same class, you want them to be.",
                    "label": 0
                },
                {
                    "sent": "Smaller than some other value will be 2 and then you have to be to be one baby two, which is your margin.",
                    "label": 1
                },
                {
                    "sent": "Pretty much that you can set as a parameter like the desired margin you want in your particular problem and the other parameter is the regularization edit list.",
                    "label": 0
                },
                {
                    "sent": "So we optimize this and we get some edit costs that hopefully you end up with this humidity that respect.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Definition, and so we as I say we just don't want to hope we want guarantees.",
                    "label": 0
                },
                {
                    "sent": "And So what we want to do is to bound the true error on or for a bit muddled.",
                    "label": 1
                },
                {
                    "sent": "By essentially so bounding the expected loss over the whole distribution.",
                    "label": 1
                },
                {
                    "sent": "And we could do this using uniform stability, uniform stability.",
                    "label": 0
                },
                {
                    "sent": "The idea behind it is fairly simple.",
                    "label": 0
                },
                {
                    "sent": "It's about studying the impact of a small change in the training sample and see if the accumulated loss of your model changes.",
                    "label": 1
                },
                {
                    "sent": "Order another one and the fact is if you can, if you can bound that change in the in the in the loss, then you can drive it a generalization down.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm not going.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into the details of the proofs, but essentially we could prove that the algorithm is uniform stability.",
                    "label": 1
                },
                {
                    "sent": "In Kappa over NT&T, being the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "And from that we could derive a generalization balance of bounding the true loss by the empirical loss over showing example and some term that depends on empty.",
                    "label": 0
                },
                {
                    "sent": "And what we like about that build as well is that it is that it's independent from the sense of the alphabet, but doesn't appear in the Dom.",
                    "label": 1
                },
                {
                    "sent": "So it should be quite robust to probably big alphabets.",
                    "label": 0
                },
                {
                    "sent": "That's what we expect.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "At last a few experiments we just experimented over a very simple task which was classifying words as either French or English English, and we we studied three different similarities in red.",
                    "label": 1
                },
                {
                    "sent": "Here you have the standard edit distance with the coastal Sector 1.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Before you say that, here is the number of training examples used for under cost, and this is the the accuracy.",
                    "label": 0
                },
                {
                    "sent": "So this is the baseline.",
                    "label": 0
                },
                {
                    "sent": "Then we have in blue.",
                    "label": 0
                },
                {
                    "sent": "Method that is based on probabilistic models and it's running edit probabilities using an iterative approach based on expectation maximization and optimizes if it's based on maximum likelihood essentially.",
                    "label": 0
                },
                {
                    "sent": "And after a while we see that this message outperforms the baseline.",
                    "label": 0
                },
                {
                    "sent": "OK, and or messages in green here and we can see that both our personal performs the other two similarities, but also that it converges really fast.",
                    "label": 0
                },
                {
                    "sent": "We can outperform the baseline with really few examples, you know 50.",
                    "label": 0
                },
                {
                    "sent": "And and we converge really fast as well.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I don't know advantage of this that we notice is that when you later on build your classifiers from the similarity function you have.",
                    "label": 0
                },
                {
                    "sent": "It turns out that learning Assirati to make it fit the definition better also.",
                    "label": 0
                },
                {
                    "sent": "Results in sparser models.",
                    "label": 0
                },
                {
                    "sent": "So we have a model that performs better, but it is also sparse.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as a quick recap, we made use of the framework craft by the can to create a novel and efficient way to learn string similarities.",
                    "label": 1
                },
                {
                    "sent": "And we showed that the synergies probably generalize well and that they need to lower our classifiers for the task.",
                    "label": 0
                },
                {
                    "sent": "Future work could include adapting a framework for treat it cost running, which is kind of straightforward.",
                    "label": 0
                },
                {
                    "sent": "We can keep the same approach the same, and just.",
                    "label": 0
                },
                {
                    "sent": "Change the string edits script in the message.",
                    "label": 1
                },
                {
                    "sent": "Then take a treated script instead.",
                    "label": 0
                },
                {
                    "sent": "An would like also to learn maybe other types of similarities, including numerical simulations.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So when you did the comparisons with the EM algorithm, is that also using an L1 regularizer so that we're normalizing for the difference in the algorithm and the difference in the smoothing?",
                    "label": 1
                },
                {
                    "sent": "But you have to differentiate two things.",
                    "label": 0
                },
                {
                    "sent": "There is the edit cost learning procedures.",
                    "label": 0
                },
                {
                    "sent": "So how you run the task and in our case?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remember, in convex program it's not as one regularised.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "1 regularised is how you learn the separator after that.",
                    "label": 0
                },
                {
                    "sent": "And this is the same method for all the similarities, so it's.",
                    "label": 0
                },
                {
                    "sent": "So nice.",
                    "label": 0
                },
                {
                    "sent": "I'm a little bit puzzled on the task.",
                    "label": 0
                },
                {
                    "sent": "In task is classifying if a word it's English or French word right?",
                    "label": 0
                },
                {
                    "sent": "You can apply these on.",
                    "label": 0
                },
                {
                    "sent": "More complex task, like for example detective noun phrases are qualifying or not, so we will do the task.",
                    "label": 0
                },
                {
                    "sent": "Dancing seems a little bit simple, so instead of this kind of East answer is positive.",
                    "label": 0
                },
                {
                    "sent": "Any task and then you can really understand the value of your approach.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I told you we we, we just had this small experiments going on for this paper, which we are definitely looking into more expensive things or indifferent.",
                    "label": 0
                },
                {
                    "sent": "Data sets and we're looking into that.",
                    "label": 0
                },
                {
                    "sent": "You're right that we need probably more experiments to be sure.",
                    "label": 0
                },
                {
                    "sent": "OK, since we have the 5th talk as well.",
                    "label": 0
                }
            ]
        }
    }
}