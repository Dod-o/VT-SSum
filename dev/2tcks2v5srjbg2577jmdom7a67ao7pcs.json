{
    "id": "2tcks2v5srjbg2577jmdom7a67ao7pcs",
    "title": "Lagrange Dual Decomposition for Finite Horizon Markov Decision Processes",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Thomas Furmston, Department of Computer Science, University College London"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_furmston_lagrange/",
    "segmentation": [
        [
            "This is joint work between myself and David Barber.",
            "You still?",
            "So what we're focusing on is Georgie composition of finite device number, position based."
        ],
        [
            "So we've outlined talk.",
            "I consider.",
            "State.",
            "Get my jewelry composition.",
            "Then some excitement."
        ],
        [
            "So this is basic stuff, plus we're interested in general dynamic control problems, so examples are."
        ],
        [
            "Sticks."
        ],
        [
            "Optimization and like network management."
        ],
        [
            "So basic definitions, we have an action state space, so we have actions which are committed by a state certified S initial state distribution, which determines what stage and starting and then the policy which determines allocation Act.",
            "So we can have a non stationary policy which depends on S&T which is the time.",
            "Or we can have station policy which reactions depends on this statement.",
            "We have a reward function.",
            "Transition dynamics.",
            "Which machine online registry planning without doing all that and then planning horizon, which is generally defined?"
        ],
        [
            "OK, and the objective we want to maximize just said to expect towards services over the planet visor and then expectation.",
            "At that time point, I would say action switches."
        ],
        [
            "OK, so in this paper we focus on a particular case which is analyzing this fine line and the policy is stationary.",
            "I'm particularly interested in obtaining a dynamic programming solution to this problem class.",
            "One of the reasons why I would like to do this is because other planning items such as am have slight versions and policy gradients.",
            "Those particular susceptible to look like this one.",
            "This is a difficult problem because the stationality policy experiments."
        ],
        [
            "So we can see this in the influence diagrams of the table and say.",
            "On the event we have no searching policies and we can see that it's chain structure that's actually easy to optimize.",
            "So very conditional S4.",
            "Then the rest of the influence diagram is broken off from a foreign policy.",
            "IPhone.",
            "And so on and so forth, because right back with another program so.",
            "However, when you have a stationary policy, you have a large policy can actually possible for conditional access for.",
            "Highest point independent.",
            "So yeah."
        ],
        [
            "OK, so general idea of our paper is to use the equal Joule decomposition to use a theoretical ease of optimizing the finite horizon.",
            "MDP with station policies.",
            "So the first thing is to rewrite the original projective function, which is here.",
            "And this can be rewritten as maximization of higher then probably want to page with the constraint that cars that we could.",
            "Piper and the.",
            "Distributions only.",
            "Policies.",
            "So ordinarily to add this constraint.",
            "2."
        ],
        [
            "Optimized simply had some of this form where the land is.",
            "The multiplies their forces.",
            "Is a quote here.",
            "Unfortunately, we shouldn't take the.",
            "This doesn't lead to a dynamic programming solution, so we consider equivalent set of constraints.",
            "So we just have the stretcher distribution opportunity with stabilizer."
        ],
        [
            "So.",
            "We got these terms is a bunch and we get.",
            "I know this phone so.",
            "We have a say action, marginal with the award, which is a traditional objective.",
            "But the policies nonstationary?",
            "We also have the lunch time here which.",
            "And then we have a second divorce over here."
        ],
        [
            "So the first thing to know is that we can do the optimization of the station policy straight away, so This is why it's a visual album.",
            "So if you perform the authorization of Pi, we just got this constraints over the multiplies."
        ],
        [
            "And we obtain the following dual objective function which is of this form.",
            "And this is a standard technique in Julie composition.",
            "Literature reiterates this.",
            "Radioactivity for process of sleep problems and master problems."
        ],
        [
            "OK, so in slave problem we consider the Lambda to be fixed and we optimize every pirate ones are H. When we look at this objective function, this is just an ordinary MDP with North station policies, but with the extra Lambda term here so we can see that we have another station reward function now.",
            "Decomposition.",
            "So it's just the normal people who.",
            "The punishment applies later.",
            "Nonstationary would function and we can solve this using."
        ],
        [
            "Start programming.",
            "OK, the master problem you consider their pie station policies.",
            "We fix an interview minimization of the LaGrange multipliers.",
            "We just do a projected subgradient routine.",
            "So first take a step in the great interest in memorization.",
            "Is the gradient step size, and then you just project down into your constraints over lambdas.",
            "The distribution of the H. I don't get any details here, but it's in the paper.",
            "So we have a simple update, the lambdas and simple update."
        ],
        [
            "So to summarize visually.",
            "The composition is right translatable.",
            "Highs and then you use that to Debbie Landers and you're saying this and so some conversions criteria.",
            "So there's no problem is just a stationary MVP without station rewards or the normal rewards plus that other terms and then allowed to sit this updated projections.",
            "Projection separating stuff."
        ],
        [
            "OK, so we've seen that the rewards.",
            "Judy competition leads to.",
            "And then you pay with one station one so.",
            "Is made to impose on social policies.",
            "Question is, how do they?",
            "Our land is updated and how design courage stationarity.",
            "So the first relation we obtain, we actually have an exact update for this, but I just wanted to show that if an action.",
            "Give US data Connection A was optimal at time T. Then they LaGrange multiplier for that path and time point would actually get down.",
            "And twice it wasn't optimal.",
            "It will go up.",
            "Additionally, we have.",
            "Patient re the magnitude of the change between the advanced multipliers for this order equation.",
            "An ISA is just the number of time points at Action Aid was optimal.",
            "Yes.",
            "So.",
            "What this is saying basically is that if it was optimal in a large amount of time points and your small change or decrease or increase worker, and if it was up for an interview, will be a large increase or decrease."
        ],
        [
            "So it's probably best if I just give an example.",
            "So consider an SDK of two actions.",
            "Given State S previously problem found that action one was optimal for knowledge of time points, an action two is only optimal for few time points.",
            "Then Lambda TA1S would decrease only slightly louder TA2S would increase increases in various levels, so you would expect action.",
            "They would still be optimal in extra slow problems.",
            "And then four time points where a two is optimal.",
            "Lambda TA1S will increase more domestically than I would have in this case.",
            "And one day to ask, will decrease more domestically.",
            "There's not more likely that they won't reboot to be optimal next level, and that's likely that a two week.",
            "So this is a typical feature of duty, competition, algorithms, folder resource allocation.",
            "Majority vote."
        ],
        [
            "So we compare it against PM, then policy gradients, fix step size and align search.",
            "And then we also did a switching algorithm which starts an expectation maximization and then goes to gradients."
        ],
        [
            "OK, so we considered free.",
            "It's just a.",
            "No toy one chain problems at this pace.",
            "It starts in state S1 for this given time advising it needs to go down to the end."
        ],
        [
            "We considered mountain car.",
            "I was only for discrete at the moment.",
            "We waiting on extended continuous so we have to discretize it.",
            "So we had to change in 31 states free actions and again apologizing for 25 and those who don't know technologies to get the agent up to the right motivation."
        ],
        [
            "And then we consider it also other world over certain agents or initial state here.",
            "I have to go state.",
            "Well, avoiding these areas which pose."
        ],
        [
            "OK, so for the chain problem we can see that.",
            "Jewel decomposition diagram algorithm is optimal, and so is the MPG and the X Ray close.",
            "Position it takes for iterations.",
            "Yeah, even after 100 iterations, it's still not actually optimal place for it.",
            "So for fixed policy gradients.",
            "So bottom line, searches any few iterations, but it's even worse and we can see this again in the mountain car.",
            "So he happened MPG, but similar levels to decomposition common programming again, except installations compared 200 actually optimal.",
            "I kinda like such disappointing radiance.",
            "And then in the polar world we couldn't get reasonable results for the these two algorithms.",
            "So yeah, but now?",
            "So that you could decomposition takes her situations.",
            "Yeah, after 1000 situations up to 39 it would.",
            "I think it would be optimal.",
            "Different time there's there's a huge amount of data in this problem.",
            "And the long search get stuck again."
        ],
        [
            "OK, so we visited a few decomposition algorithm for finite horizon MTPS with stationary policies.",
            "One extension that's been working on is to go to continue stacking domains and we also like to extend it more complex.",
            "So when we're tackling rising from one of the approaches is simply to do.",
            "Let's see by iteration for certain number of time steps, take the policy that last time step and assume that it's we're just going to use it for every timestamp.",
            "So have you compared to using this heuristic I have had I compared it to view is."
        ],
        [
            "Mentioned that is rising.",
            "Against chain problem and it was always in position.",
            "What's the car?",
            "What is the capital?",
            "Find almost.",
            "Depends on the.",
            "A lot of problems.",
            "Order I think it's important depending.",
            "So I mentioned.",
            "More.",
            "So what are your ideas to make it?",
            "Well, to make it work for continuous?",
            "The problem.",
            "Chinese workout.",
            "It's very easy to do the constraints for.",
            "Discreet"
        ],
        [
            "Something working on.",
            "OK anymore."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work between myself and David Barber.",
                    "label": 0
                },
                {
                    "sent": "You still?",
                    "label": 0
                },
                {
                    "sent": "So what we're focusing on is Georgie composition of finite device number, position based.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've outlined talk.",
                    "label": 0
                },
                {
                    "sent": "I consider.",
                    "label": 0
                },
                {
                    "sent": "State.",
                    "label": 0
                },
                {
                    "sent": "Get my jewelry composition.",
                    "label": 0
                },
                {
                    "sent": "Then some excitement.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is basic stuff, plus we're interested in general dynamic control problems, so examples are.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sticks.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimization and like network management.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basic definitions, we have an action state space, so we have actions which are committed by a state certified S initial state distribution, which determines what stage and starting and then the policy which determines allocation Act.",
                    "label": 0
                },
                {
                    "sent": "So we can have a non stationary policy which depends on S&T which is the time.",
                    "label": 0
                },
                {
                    "sent": "Or we can have station policy which reactions depends on this statement.",
                    "label": 0
                },
                {
                    "sent": "We have a reward function.",
                    "label": 0
                },
                {
                    "sent": "Transition dynamics.",
                    "label": 0
                },
                {
                    "sent": "Which machine online registry planning without doing all that and then planning horizon, which is generally defined?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and the objective we want to maximize just said to expect towards services over the planet visor and then expectation.",
                    "label": 0
                },
                {
                    "sent": "At that time point, I would say action switches.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in this paper we focus on a particular case which is analyzing this fine line and the policy is stationary.",
                    "label": 0
                },
                {
                    "sent": "I'm particularly interested in obtaining a dynamic programming solution to this problem class.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons why I would like to do this is because other planning items such as am have slight versions and policy gradients.",
                    "label": 0
                },
                {
                    "sent": "Those particular susceptible to look like this one.",
                    "label": 0
                },
                {
                    "sent": "This is a difficult problem because the stationality policy experiments.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can see this in the influence diagrams of the table and say.",
                    "label": 0
                },
                {
                    "sent": "On the event we have no searching policies and we can see that it's chain structure that's actually easy to optimize.",
                    "label": 0
                },
                {
                    "sent": "So very conditional S4.",
                    "label": 0
                },
                {
                    "sent": "Then the rest of the influence diagram is broken off from a foreign policy.",
                    "label": 0
                },
                {
                    "sent": "IPhone.",
                    "label": 0
                },
                {
                    "sent": "And so on and so forth, because right back with another program so.",
                    "label": 0
                },
                {
                    "sent": "However, when you have a stationary policy, you have a large policy can actually possible for conditional access for.",
                    "label": 0
                },
                {
                    "sent": "Highest point independent.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so general idea of our paper is to use the equal Joule decomposition to use a theoretical ease of optimizing the finite horizon.",
                    "label": 1
                },
                {
                    "sent": "MDP with station policies.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is to rewrite the original projective function, which is here.",
                    "label": 0
                },
                {
                    "sent": "And this can be rewritten as maximization of higher then probably want to page with the constraint that cars that we could.",
                    "label": 0
                },
                {
                    "sent": "Piper and the.",
                    "label": 0
                },
                {
                    "sent": "Distributions only.",
                    "label": 0
                },
                {
                    "sent": "Policies.",
                    "label": 0
                },
                {
                    "sent": "So ordinarily to add this constraint.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimized simply had some of this form where the land is.",
                    "label": 0
                },
                {
                    "sent": "The multiplies their forces.",
                    "label": 0
                },
                {
                    "sent": "Is a quote here.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, we shouldn't take the.",
                    "label": 0
                },
                {
                    "sent": "This doesn't lead to a dynamic programming solution, so we consider equivalent set of constraints.",
                    "label": 0
                },
                {
                    "sent": "So we just have the stretcher distribution opportunity with stabilizer.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We got these terms is a bunch and we get.",
                    "label": 0
                },
                {
                    "sent": "I know this phone so.",
                    "label": 0
                },
                {
                    "sent": "We have a say action, marginal with the award, which is a traditional objective.",
                    "label": 0
                },
                {
                    "sent": "But the policies nonstationary?",
                    "label": 0
                },
                {
                    "sent": "We also have the lunch time here which.",
                    "label": 0
                },
                {
                    "sent": "And then we have a second divorce over here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first thing to know is that we can do the optimization of the station policy straight away, so This is why it's a visual album.",
                    "label": 0
                },
                {
                    "sent": "So if you perform the authorization of Pi, we just got this constraints over the multiplies.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we obtain the following dual objective function which is of this form.",
                    "label": 0
                },
                {
                    "sent": "And this is a standard technique in Julie composition.",
                    "label": 0
                },
                {
                    "sent": "Literature reiterates this.",
                    "label": 0
                },
                {
                    "sent": "Radioactivity for process of sleep problems and master problems.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in slave problem we consider the Lambda to be fixed and we optimize every pirate ones are H. When we look at this objective function, this is just an ordinary MDP with North station policies, but with the extra Lambda term here so we can see that we have another station reward function now.",
                    "label": 0
                },
                {
                    "sent": "Decomposition.",
                    "label": 0
                },
                {
                    "sent": "So it's just the normal people who.",
                    "label": 0
                },
                {
                    "sent": "The punishment applies later.",
                    "label": 0
                },
                {
                    "sent": "Nonstationary would function and we can solve this using.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start programming.",
                    "label": 0
                },
                {
                    "sent": "OK, the master problem you consider their pie station policies.",
                    "label": 0
                },
                {
                    "sent": "We fix an interview minimization of the LaGrange multipliers.",
                    "label": 0
                },
                {
                    "sent": "We just do a projected subgradient routine.",
                    "label": 0
                },
                {
                    "sent": "So first take a step in the great interest in memorization.",
                    "label": 0
                },
                {
                    "sent": "Is the gradient step size, and then you just project down into your constraints over lambdas.",
                    "label": 0
                },
                {
                    "sent": "The distribution of the H. I don't get any details here, but it's in the paper.",
                    "label": 0
                },
                {
                    "sent": "So we have a simple update, the lambdas and simple update.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to summarize visually.",
                    "label": 0
                },
                {
                    "sent": "The composition is right translatable.",
                    "label": 0
                },
                {
                    "sent": "Highs and then you use that to Debbie Landers and you're saying this and so some conversions criteria.",
                    "label": 0
                },
                {
                    "sent": "So there's no problem is just a stationary MVP without station rewards or the normal rewards plus that other terms and then allowed to sit this updated projections.",
                    "label": 0
                },
                {
                    "sent": "Projection separating stuff.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we've seen that the rewards.",
                    "label": 0
                },
                {
                    "sent": "Judy competition leads to.",
                    "label": 0
                },
                {
                    "sent": "And then you pay with one station one so.",
                    "label": 0
                },
                {
                    "sent": "Is made to impose on social policies.",
                    "label": 0
                },
                {
                    "sent": "Question is, how do they?",
                    "label": 0
                },
                {
                    "sent": "Our land is updated and how design courage stationarity.",
                    "label": 0
                },
                {
                    "sent": "So the first relation we obtain, we actually have an exact update for this, but I just wanted to show that if an action.",
                    "label": 0
                },
                {
                    "sent": "Give US data Connection A was optimal at time T. Then they LaGrange multiplier for that path and time point would actually get down.",
                    "label": 0
                },
                {
                    "sent": "And twice it wasn't optimal.",
                    "label": 0
                },
                {
                    "sent": "It will go up.",
                    "label": 0
                },
                {
                    "sent": "Additionally, we have.",
                    "label": 0
                },
                {
                    "sent": "Patient re the magnitude of the change between the advanced multipliers for this order equation.",
                    "label": 0
                },
                {
                    "sent": "An ISA is just the number of time points at Action Aid was optimal.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What this is saying basically is that if it was optimal in a large amount of time points and your small change or decrease or increase worker, and if it was up for an interview, will be a large increase or decrease.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's probably best if I just give an example.",
                    "label": 0
                },
                {
                    "sent": "So consider an SDK of two actions.",
                    "label": 0
                },
                {
                    "sent": "Given State S previously problem found that action one was optimal for knowledge of time points, an action two is only optimal for few time points.",
                    "label": 0
                },
                {
                    "sent": "Then Lambda TA1S would decrease only slightly louder TA2S would increase increases in various levels, so you would expect action.",
                    "label": 0
                },
                {
                    "sent": "They would still be optimal in extra slow problems.",
                    "label": 0
                },
                {
                    "sent": "And then four time points where a two is optimal.",
                    "label": 0
                },
                {
                    "sent": "Lambda TA1S will increase more domestically than I would have in this case.",
                    "label": 0
                },
                {
                    "sent": "And one day to ask, will decrease more domestically.",
                    "label": 0
                },
                {
                    "sent": "There's not more likely that they won't reboot to be optimal next level, and that's likely that a two week.",
                    "label": 0
                },
                {
                    "sent": "So this is a typical feature of duty, competition, algorithms, folder resource allocation.",
                    "label": 0
                },
                {
                    "sent": "Majority vote.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we compare it against PM, then policy gradients, fix step size and align search.",
                    "label": 0
                },
                {
                    "sent": "And then we also did a switching algorithm which starts an expectation maximization and then goes to gradients.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we considered free.",
                    "label": 0
                },
                {
                    "sent": "It's just a.",
                    "label": 0
                },
                {
                    "sent": "No toy one chain problems at this pace.",
                    "label": 0
                },
                {
                    "sent": "It starts in state S1 for this given time advising it needs to go down to the end.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We considered mountain car.",
                    "label": 0
                },
                {
                    "sent": "I was only for discrete at the moment.",
                    "label": 0
                },
                {
                    "sent": "We waiting on extended continuous so we have to discretize it.",
                    "label": 0
                },
                {
                    "sent": "So we had to change in 31 states free actions and again apologizing for 25 and those who don't know technologies to get the agent up to the right motivation.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we consider it also other world over certain agents or initial state here.",
                    "label": 0
                },
                {
                    "sent": "I have to go state.",
                    "label": 0
                },
                {
                    "sent": "Well, avoiding these areas which pose.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for the chain problem we can see that.",
                    "label": 0
                },
                {
                    "sent": "Jewel decomposition diagram algorithm is optimal, and so is the MPG and the X Ray close.",
                    "label": 0
                },
                {
                    "sent": "Position it takes for iterations.",
                    "label": 0
                },
                {
                    "sent": "Yeah, even after 100 iterations, it's still not actually optimal place for it.",
                    "label": 0
                },
                {
                    "sent": "So for fixed policy gradients.",
                    "label": 0
                },
                {
                    "sent": "So bottom line, searches any few iterations, but it's even worse and we can see this again in the mountain car.",
                    "label": 0
                },
                {
                    "sent": "So he happened MPG, but similar levels to decomposition common programming again, except installations compared 200 actually optimal.",
                    "label": 0
                },
                {
                    "sent": "I kinda like such disappointing radiance.",
                    "label": 0
                },
                {
                    "sent": "And then in the polar world we couldn't get reasonable results for the these two algorithms.",
                    "label": 0
                },
                {
                    "sent": "So yeah, but now?",
                    "label": 0
                },
                {
                    "sent": "So that you could decomposition takes her situations.",
                    "label": 0
                },
                {
                    "sent": "Yeah, after 1000 situations up to 39 it would.",
                    "label": 0
                },
                {
                    "sent": "I think it would be optimal.",
                    "label": 0
                },
                {
                    "sent": "Different time there's there's a huge amount of data in this problem.",
                    "label": 0
                },
                {
                    "sent": "And the long search get stuck again.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we visited a few decomposition algorithm for finite horizon MTPS with stationary policies.",
                    "label": 1
                },
                {
                    "sent": "One extension that's been working on is to go to continue stacking domains and we also like to extend it more complex.",
                    "label": 0
                },
                {
                    "sent": "So when we're tackling rising from one of the approaches is simply to do.",
                    "label": 0
                },
                {
                    "sent": "Let's see by iteration for certain number of time steps, take the policy that last time step and assume that it's we're just going to use it for every timestamp.",
                    "label": 0
                },
                {
                    "sent": "So have you compared to using this heuristic I have had I compared it to view is.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mentioned that is rising.",
                    "label": 0
                },
                {
                    "sent": "Against chain problem and it was always in position.",
                    "label": 0
                },
                {
                    "sent": "What's the car?",
                    "label": 0
                },
                {
                    "sent": "What is the capital?",
                    "label": 0
                },
                {
                    "sent": "Find almost.",
                    "label": 0
                },
                {
                    "sent": "Depends on the.",
                    "label": 0
                },
                {
                    "sent": "A lot of problems.",
                    "label": 0
                },
                {
                    "sent": "Order I think it's important depending.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned.",
                    "label": 0
                },
                {
                    "sent": "More.",
                    "label": 0
                },
                {
                    "sent": "So what are your ideas to make it?",
                    "label": 0
                },
                {
                    "sent": "Well, to make it work for continuous?",
                    "label": 0
                },
                {
                    "sent": "The problem.",
                    "label": 0
                },
                {
                    "sent": "Chinese workout.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to do the constraints for.",
                    "label": 0
                },
                {
                    "sent": "Discreet",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something working on.",
                    "label": 0
                },
                {
                    "sent": "OK anymore.",
                    "label": 0
                }
            ]
        }
    }
}