{
    "id": "l3o3iiym3zbfqu37vz4hupbwa4ycasnc",
    "title": "Efficiency of Quasi-Newton Methods on Strictly Positive Functions",
    "info": {
        "author": [
            "Yurii Nesterov, Universit\u00e9 catholique de Louvain"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_nesterov_eqn/",
    "segmentation": [
        [
            "OK, this is the topic of my talk and I would like to thank the organizers for inviting me at this beautiful place and also they really put my talk very nicely.",
            "So after two talks about which I can speak a little bit and compare my results with results of previous people.",
            "Actually my main motivation in this topic was more or less natural.",
            "So you know last years so we managed.",
            "Several situation to improve low complexity bounds of.",
            "I can try to speak louder anyway, so let us take like that because of two microphones.",
            "This is maybe too much OK.",
            "So in several situations we managed to improve the low complexity bounds which come from complexity theory.",
            "Black black box complexity theory.",
            "So if you look at the results of the past, so this was the case for polynomial time interior point methods after that.",
            "So there was smoothing technique where we managed also to get algorithms which are much faster and actually in optimization the standard optimism.",
            "Classical optimization.",
            "Of course there is during the keys.",
            "The well known technique for improving the rate of convergence, practical convergence of gradient methods, and this is the quasi Newton method.",
            "So variable metric methods.",
            "This technique is known for a long time, but unfortunately and you have seen that from the first talk today, so not too many results, almost no results.",
            "We know about global performance of these methods, so all results are related to.",
            "Local complexity analysis.",
            "And actually this is a pretty big cause.",
            "Now we're interested in solving high dimensional problems and local complexity analysis.",
            "Local rate of convergence in many applications.",
            "It is already too much, so in many applications.",
            "So the problem is so big and difficult that first we don't need very high accuracy because for big problems the accuracy of the data and of the problem.",
            "So he is never very very high and 2nd, so the most of computational time is spent.",
            "The first stage of optimization when we come to the hopefully to the neighborhood of the optimal point.",
            "And if you have after that quadratic or super linear rate of convergence.",
            "Of course this is good, but it is not very important because anyway, for fast convergence you do several iterations and that's it.",
            "But all computational time is spent.",
            "At the first stage, so my intention was to look somehow another ways of accelerating.",
            "The methods and as a result, so I'll show you something related to the quasi Newton methods.",
            "So in order to."
        ],
        [
            "Who?",
            "Oh she, what's going on here?",
            "So let us discuss a little bit about the theoretical estimates which we are able to prove for numerical methods in general, so we'll speak about absolute and relative accuracy after that.",
            "So we will speak about in you class of convex functions, which appears to be quite interesting for optimization schemes, and it appears that for for this function, so we can propose natural some version of Quasi Newton.",
            "Method or variable matrix methods for which were able already to get some nontrivial and quite.",
            "Interesting.",
            "Results from the rate of converge."
        ],
        [
            "So let us look, what do we have?",
            "I just recall you actually is what the definitions of."
        ],
        [
            "The cure issues, so the absolute accuracy we have already seen this definition.",
            "The second talk, so again, so we are not interested in very complicated structure of optimization problem because all difficulties are already here.",
            "So when we minimize function F with convex function F over the closed."
        ],
        [
            "Set OK so reality for curacy is just the standard definition.",
            "We want to find the point X bar which is fishable and which function value is good up to accuracy."
        ],
        [
            "Epsilon, and in order to get some reasonable results, so we need to assume something.",
            "Of course about the properties of our problem, so we."
        ],
        [
            "Need to speak about the problem classes and there are basically two bounds.",
            "Which two properties which were interested in and both of them are bounds on the growth of the function.",
            "The first is the bound on the growth of the function with respect to the linear approximation, so this is strong convexity.",
            "Of course we can.",
            "Very often we can say that mu is equal to 0 and then that is just convexity.",
            "So assume this and all."
        ],
        [
            "So always assume some upper bounds on the growth of function, and they usually formed in terms of bounds on the first day we have a second derivative.",
            "In this talk we will be mainly interested in.",
            "Non small septum isation, so this will be our bound."
        ],
        [
            "OK, and note that even this problem classes they designed for definition of.",
            "Absolute accuracy.",
            "This means that complexity of the problem is not changing if we add to our function an absolute constant, whatever is this constant, big, small and so on.",
            "So we do not change complexity of the problem and the number of iterations which we need in order to get the epsilon accuracy remains the same.",
            "So this is a confirmation that all we see is done for absolute accuracy, but in application of."
        ],
        [
            "Of course, very often we want to get.",
            "Really, what you're curious you good deal with curacy of the solution and this direction of the research, actually is not studied well very well in optimization even from the viewpoint of lower complexity bounds and so."
        ],
        [
            "It is real that you're curious here.",
            "Again, we have the same problem.",
            "But in order to define relative accuracy we need.",
            "The optimal value of our objective function to be positive."
        ],
        [
            "Without this video which you have no sense, and then we introduce parameter accuracy parameter Delta between zero and one and put it said that Xbox has relative accuracy Delta if this inequality is satisfied, OK, there are different.",
            "Another variant of.",
            "Definition of relative curious even instead of 1 minus Delta here we put one plus Delta here, but there are more or less equivalent for us.",
            "It is important.",
            "It is convenient to have this definition now for our problem class, so this condition that optimal value is positive must be satisfied somehow before even we start to apply methods which I designed for computing the solution in relative accuracy."
        ],
        [
            "And how to do that so?"
        ],
        [
            "Of course we the first thing we.",
            "Think about is to consider special problem classes.",
            "So there are different problem classes which fits this relative accuracy.",
            "One of them is just as follows we have.",
            "Oh cool, convex functions, just homogeneous convex function which contains 0 insufficiency.",
            "Also it is positive on the whole place and our feasible set Q it doesn't contain zero.",
            "Then clearly from these two assumptions.",
            "OK, we know that the option value of the problem is positive and we can try to solve to solve it.",
            "So our main assumption is that we know here the lower and upper bounds on the growth of.",
            "The objective function and the important parameter is the condition number.",
            "If you want of this set was Felicity.",
            "Which is just grammar 0 divided by gamma one, which is the thing which is smaller than one less or equal than one."
        ],
        [
            "So then it is possible to prove that we can apply in this situation the smoothing technique and get the gradient methods which have this.",
            "This complexity, actually complexity is not too bad the tools because for symmetric set for example, this Alpha can be one over square root of dimension.",
            "So the square root of N divided by data, but still so we have here kind of very explicit structure of the problem which is not always the case so for."
        ],
        [
            "Another model of our data.",
            "For example, when we have polyhedral B, which is for example symmetric with respect to region.",
            "So we can achieve similar results."
        ],
        [
            "But using a special norm which we compute by preprocessing.",
            "So it is just a kind of Phillips with very efficient input type procedure which run in order to find a good approximation of this set, but."
        ],
        [
            "All of that.",
            "Actually, indeed requires very special structure of our problem, so.",
            "Which is not always the case.",
            "So the question is can we address this question so the relative accuracy?",
            "Think if we have a usual black box framework, OK?"
        ],
        [
            "And for that, of course, so we need to develop some new problem classes.",
            "The main feature of this problem, classes that they're not invariant anymore with respect to additional constant, but the invariant with respect to multiplication by a constant.",
            "This is already makes some difference difference."
        ],
        [
            "So the partial answer partial suggestion to this question was done recently in the barrier subgradient method."
        ],
        [
            "Which works also with quite special class of problems.",
            "It does not minimization.",
            "It is maximization over closed convex set of.",
            "Can key function.",
            "And this is important again, since we are speaking about reality for curacy, so that this function is non negative, non negative on this set.",
            "Q and for this problem.",
            "So we need a special descriptor description of the convex set.",
            "Our convex set queue.",
            "So we assume that we know for this self concordant barrier function doesn't matter.",
            "What is it?"
        ],
        [
            "So for us is important.",
            "So what is what we can do actually?",
            "So this barrier subgradient method?",
            "Actually it is just the gradient method primal dual gradient prime dual subgradient method which is applied to.",
            "But the initial F which we compute by taking the logarithm of the objective and it looks like that.",
            "So for this potential function we update.",
            "The model of our problem.",
            "OK, so before the model of our problem, we subtract the barrier function for our fishable set with certain confusion, and we maximize.",
            "So our main assumption is that this separation at each step is feasible, and this is very simple."
        ],
        [
            "Think So what we can prove about that?",
            "So this is the complexity of finding the.",
            "The Delta, the.",
            "Approximate solution of our problem with relative accuracy dealt so you see, so it is proportional to parameter of the barrier divided by Delta Square and.",
            "Of course, well this thing is good, I would say because this parameter very often can be much less than the dimension of the space.",
            "But this thing of course is not very encouraging.",
            "OK, so the question is, can we finally get a more general framework for addressing the functions for minimizing the functions in relative accuracy?",
            "This was.",
            "The question."
        ],
        [
            "And actually the answer came.",
            "Well possible answer.",
            "Comes from some may be strange observation which is described now in."
        ],
        [
            "This definition of strictly positive function, so look what is it?",
            "It is.",
            "Just let us look at this object so we don't know anything about it except strange definition function F is called strictly positive on Q if for all X&Y.",
            "We have this inequality.",
            "OK, no definition looks very similar to the convexity, except that for convexity here and here we have minus OK, so we have this, which is this thing.",
            "And of course if we assume convexity."
        ],
        [
            "The top, so the equivalent thing that F of Y is greater or equal than the absolute value of linear approximation.",
            "OK, so.",
            "For sure already have nonnegative function and this would be true for all X of Y.",
            "Very strange guy, but let us look.",
            "What is it?",
            "So what we can do with these functions, of course, is a subclass of convex functions.",
            "There's no negative."
        ],
        [
            "Very good, so let us."
        ],
        [
            "Look what it could be first generation.",
            "Of course that the constant and in this in this class is OK.",
            "So if we take the constant then our definition is valid."
        ],
        [
            "Now.",
            "This tender thing.",
            "OK, now strict positive ITI is an affine invariant property.",
            "So what does it mean?",
            "So if we form from function F Another function but substitute an extent instead of XA linear function of other variables we still have, we still have this inequality because actually so this is definition that is written in multi dimensional form but the same as the convexity.",
            "1 dimensional property so."
        ],
        [
            "So the restriction of our variables on some subsets doesn't change anything now again.",
            "So in terms of F, OK, this is strictly positive.",
            "Functions form a convex column, so we can multiply them.",
            "And because this this inequality is convex in F, so we can multiply it by a constant ad and so on.",
            "So this."
        ],
        [
            "News and we already can look."
        ],
        [
            "It's simple examples so.",
            "But quite important for applications.",
            "So let F use a support function of.",
            "Convex, bounded and centrally symmetric set convex set OK then."
        ],
        [
            "It is strictly positive the way this over again that the answer is very simple source since we have homogeneous function is.",
            "Use the scalar product of the gradient by the point.",
            "OK, this is the state of fact, but since.",
            "The subgradient is symmetric.",
            "So minus F prime belongs to B and therefore."
        ],
        [
            "Or so we have this F of Y is greater than that because this guy is inside and this is exactly that.",
            "So it is just just identity.",
            "So this is inequality which we need to establish so."
        ],
        [
            "So the most simple and natural examples of strictly positive functions are norms.",
            "Usual norms in finite dimensional."
        ],
        [
            "Space no.",
            "Another good news is that the maximum of two strictly positive functions is strictly positive function."
        ],
        [
            "White this or maybe it is not really evident.",
            "Let us look at the proof again.",
            "It is one line, so let us take the value of our function at point Y.",
            "So and then this value is greater than a particular component of one of our function is now if at the point X, so this component is active in the maximum, then we can use this inequality.",
            "Since F1 is positive function and this guy equal to the value of our function and this can be considered as the derivative of our function of primer fix.",
            "So you see.",
            "So simple observation tells us that maximum is also forms the positive functions."
        ],
        [
            "Now we can."
        ],
        [
            "Really write down different examples which are quite important for applications functions which are strictly positive.",
            "OK, this is maximum of the norm.",
            "Sum of norms doesn't matter.",
            "What are the norms?",
            "So the same with spectral norms.",
            "Whatever you want many functions like this.",
            "So what did you already have?",
            "Quite."
        ],
        [
            "Right class of applications.",
            "But the question is what happens with the general convex functions?"
        ],
        [
            "No, just look.",
            "Assume that our function be a general convex function with bounded uniformly bounded subgradients.",
            "So it is exactly the class or function which is considered in optimization at least well derived for them the complexity bound."
        ],
        [
            "Then it appears that the maximum of this function and the norm is strictly positive function.",
            "So this server is something."
        ],
        [
            "Let this look.",
            "So in the proof, so of course the.",
            "Gradients of this guy, since the gradients of this is smaller than the gradient of this, is smaller than there.",
            "So for F also also upgrade."
        ],
        [
            "Abound, but now we need to prove that this guy is positive.",
            "Yes, and then it is just trivial so.",
            "Since this isn't boxing, most of these guys greater than or norm.",
            "Why this is created at a normal X?",
            "So we have this."
        ],
        [
            "Inequality.",
            "And of course, this is true because Subgradients appositive.",
            "Observation is very important because it allows us to form indeed many.",
            "Strictly positive functions from the arbitrary arbitrary convex."
        ],
        [
            "And let us look what we can do is assume we have the minimization problem.",
            "Of this type were five has bounded subgradients and extra is optimal solution.",
            "So can we transform it in minimization problems with strictly positive function?"
        ],
        [
            "The answer is yes for that, let us form.",
            "This is the function of this.",
            "Useful.",
            "OK, so we shift the value of our functions to have it at 0 equal to zero.",
            "We add a constant and we take the maximum.",
            "So here we get.",
            "General function OK with bounded subgradients, but here is norm with."
        ],
        [
            "And so we can guarantee by the previous result that it is strictly positive.",
            "Now let us."
        ],
        [
            "Look so it is also clear that.",
            "If.",
            "OK. X -- 6 zero is less than R then.",
            "This part of the maximum is active, so inside this bowl of the shape of our function is defined by the shape of our initial objective function OK."
        ],
        [
            "So if the optimal value is not far from zero, then the minimization problem of that type will have the same solution at the minimization problem of that type.",
            "And Moreover, we can even say so that it's the new value of the objective function here, so belongs to certain intervals between R and two."
        ],
        [
            "So we already."
        ],
        [
            "Have seen."
        ],
        [
            "Everything except except this last fact.",
            "OK, which tells us that the option will do this.",
            "The new optimization problems cannot be too small and this is more or less clear.",
            "So if we put X0 here, we see that the optimal value of the objective is smaller than two R. On the other hand, this part of the maximum can be estimated from below.",
            "OK, since the gradient Sofia bounded by that and this maximum have uniformly is uniformly greater than L / R, it is also models clear, so the conclusion.",
            "Is that actually any?",
            "Optimization problems can be written with small efforts in form of minimization of positive function over some set.",
            "OK. Now let us think how we can how we."
        ],
        [
            "And.",
            "Solve this problem and here actually you will see the reason why we are interested in this.",
            "This strange class."
        ],
        [
            "So functions.",
            "So let us look.",
            "And the optimization problem with this strictly positive objective function."
        ],
        [
            "So then let us form."
        ],
        [
            "Another optimization problems where we do the thing which actually is completely forbidden in the complexity theory because it destroys all bounds on the derivative and so.",
            "But here we do that, so we pass to the problem with squared objective function.",
            "OK.",
            "Since our function is positive, it is clear that these problems are equivalent and for for this problem we have this nice formula for the generative now."
        ],
        [
            "Why it is interesting for us?",
            "So this."
        ],
        [
            "Trivial lemma is as follows.",
            "It appears that.",
            "For our new function.",
            "On the top of usual.",
            "Inequality constraints lower bound.",
            "We can add something.",
            "Else impositive quadratic term in the right hand side of this inequality, and this is true for all X&Y."
        ],
        [
            "OK, well it is true.",
            "It is just the trivial observation.",
            "So we have our squared objective function.",
            "OK, which is this for this guy just."
        ],
        [
            "From the definition of strictly positive function, they have this lower bound, but note that it is already the bound which is quadratic in Y. OK, which is already good, but let us look.",
            "What is it?"
        ],
        [
            "We open the brackets.",
            "And the objective.",
            "So it is our new objective.",
            "This guy is arnu gradient and on the top we have.",
            "This is some additional time, so just for."
        ],
        [
            "Nothing.",
            "We have normal linear support function which actually should help our methods.",
            "OK because we introduce some additional curvature and it should."
        ],
        [
            "So let us look there for the most natural scheme for minimizing these functions, and it is."
        ],
        [
            "Looks like follows.",
            "Initial.",
            "Matrix GO so just identity matrix if you want doesn't end the relative accuracy Delta which we want to have."
        ],
        [
            "OK then, so we will work with the estimate function so called estimates functions say which we will update and the initial estimate function is just the squared distance between X and X0 computed in this metric Jo."
        ],
        [
            "So at each iteration our next point will be the minimum of this estimate function."
        ],
        [
            "But we will update for the next iteration.",
            "The estimate function in the following form.",
            "Exactly what we often use in optimization.",
            "We updated, so using some positive coefficient and the lower bound lower bound for the objective function usually so it was always the case except strongly convex function.",
            "But for general convex function we have here only linear term.",
            "But here we have.",
            "Additional quadratic term, so is it result.",
            "At each iteration we will modify the hash and we will modify the creation of our estimate functions and it would be just quadratic function.",
            "Of course it will see.",
            "Just trying to understand what is the effect."
        ],
        [
            "What is clear just from the form of this update and actually this is the same as."
        ],
        [
            "The things which we use in so-called estimate sequences is that our function is a lower bound for our objective function multiplied by coefficient achy, achy, just the summation of all this guy plus the initial initial guy.",
            "So the trivial consequences of our inequality that this is the lower support for our objective function.",
            "Now for calculations a key.",
            "So now we just fix the formula.",
            "So it will be.",
            "We have time for that, so it will be clear later.",
            "So why it is interesting to have that but.",
            "In some sense, the method is already is already defined OK, and note that here we have almost no parameters, everything is already is already fix, so let us look what is the rate of convergence of this guy.",
            "So for that actually we will need we need to relate the minimal value of this estimate functions with the values of our objective function.",
            "If."
        ],
        [
            "OK."
        ],
        [
            "Before that, let us look at the valuation of the Hessians OK Fashions at each iterations is updated by rank 14 molar, from which we have.",
            "Because we have a formula for a key.",
            "So this simple expression.",
            "So therefore the inverse Hessian is updated in this form.",
            "But the most important thing is what happens with the determinant of this guy.",
            "Determinant of Jake grows linearly of course with coefficients 1D.",
            "To the power K plus one.",
            "OK, so we get the quadratic function which becomes more and more bigger.",
            "So."
        ],
        [
            "Now the important thing in our analysis will be the norm of a K squared multiplied by this guy with respect to the new metric.",
            "OK, what is so?",
            "Let me just show."
        ],
        [
            "So you what is it?",
            "It is the squared norm of the linear part of this addition.",
            "With respect to the new creation, and you kacian is the old fashion plus, but this this quadratic term.",
            "So for this guy I just showed you."
        ],
        [
            "The formula this is the result.",
            "Actually.",
            "This is the reason."
        ],
        [
            "Or our."
        ],
        [
            "Result, it appears that this just because of our choice of coefficients this.",
            "Expression is just equal to Delta multiplied by a key multiplied by the value of our squared objective.",
            "It is just the identity straightforward computation, so, but this is important.",
            "While it is important because this term."
        ],
        [
            "So this guy."
        ],
        [
            "Help us to prove the full and lower bound for the minimal value of the estimate functions OK."
        ],
        [
            "We will look at it a little bit later and the reason is.",
            "This follows so when we modify.",
            "Our objective function by.",
            "By this objective estimate function by this objective by this additional quadratic function.",
            "Then we can write down the lower bound for knew value of psychic.",
            "Easter is the old.",
            "Bound plus constant term minus squared norm of this guy of this gradient in you metric and for this squared norm of the gradient in you metric.",
            "So this guy exactly."
        ],
        [
            "So we have exact formula.",
            "We have exact form.",
            "So finally what we get.",
            "We get this inequality and.",
            "As usual in the estimate sequences, it remains to understand what is the rate of growth of the sum of the."
        ],
        [
            "Sky fusions, AI."
        ],
        [
            "OK."
        ],
        [
            "So again, so I don't have to."
        ],
        [
            "Much time."
        ],
        [
            "So finally what we prove what we're able to prove is the full.",
            "So after key iteration.",
            "So this average point satisfies the following inequality.",
            "It is less than the optimal value of our function.",
            "This will multiply by 1 minus Delta is less than the option value of our functions, plus some interesting turn where you see we have exponent.",
            "So we have some linear rate of convergence which depends on Delta of K of North.",
            "OK, no, I don't think I have time to discuss all of that, but."
        ],
        [
            "This is the complete proof.",
            "It is not very complicated anyway, and the main element here is the inequality between the arithmetic and geometric mean.",
            "OK, so we get this growth of the photograph of this coefficients AK.",
            "So what do we get finally?",
            "So in order to understand."
        ],
        [
            "He's so."
        ],
        [
            "Let us introduce the following definition.",
            "Because the characteristic of the quality of our solution is not standard, so we will say that point X bar is an approximate solution of our problem in mixed accuracy.",
            "Epsilon Delta accuracy if it satisfies this inequality.",
            "There are two parts here, so this part is responsible for relative accuracy and epsilon is responsible for absolute accuracy.",
            "Now what is the complexity?"
        ],
        [
            "Of our.",
            "Finding such a point so it is this.",
            "This is very interesting expression.",
            "Actually.",
            "If you look at that.",
            "So."
        ],
        [
            "We see that high absolute appeal accuracy here is easy to achieve, so it is inside the logarithm."
        ],
        [
            "High relatif accuracy.",
            "Using difficult.",
            "But again, in many applications we don't need high relative accuracy because for engineers, actually I don't know one person to accuracy in many many situations in more than enough if it is relative accuracy.",
            "The reason when people want to have high absolute accuracy in the standard approaches is just they don't have no idea about them.",
            "Constance, but the parameters of the class which we have now, I mean so this LR when we speak about relative absolute accuracy.",
            "So these guys are really very important and they can make the problem very very difficult here.",
            "So all these parameters, so they're inside the logarithm.",
            "OK, so we shouldn't care too much about that, but there is one more.",
            "Interesting feature of this complexity estimate.",
            "So look how the dimension enters this bound.",
            "OK, it is here and it is here so people which."
        ],
        [
            "Already know, yeah."
        ],
        [
            "This is something in analysis, so this is that when N goes to Infinity, so the right hand side is still bounded and it is bounded more or less by the thing which is typical more or less 4.",
            "Gradient method.",
            "So when we have one over epsilon squared, so we have in the denominator.",
            "Product of the accuracies here OK.",
            "Yes and yes, and but if we don't do that and dimension is not very high that, then the number of iterations is him is proportional to N over Delta, so as.",
            "Far is you compare this with existing approaches like the barrier subgradient method where we had here Delta Square there.",
            "Here we have clear improvement.",
            "Looking now."
        ],
        [
            "Lettuce"
        ],
        [
            "Let us look what we can do now for the standard definition of the accuracy.",
            "Assume we know we want.",
            "We don't want mix accuracy, but we want to compute the point X bar in relative accuracy.",
            "What we can get from our approach."
        ],
        [
            "So again, some.",
            "So some simple reasoning tells us that in order to get Delta usual accuracy of the data, we need this.",
            "This number of iterations and divided by data plus something which is inside the logarithm.",
            "So and here you see this factor.",
            "So the main factor which defines the complexity of the problem so is not dependent on the data at all.",
            "So in fact.",
            "So this means that we get.",
            "And I think this is more or less the first time the first general fully polynomial time approximation scheme.",
            "OK, where all everything which is related to the problem is inside the logarithm.",
            "OK, and the main factor doesn't depend on anything except North Delta.",
            "The algorithm of this type, they are quite popular in computer science and but usually they are developed for very particular problems like I don't know maximum.",
            "Maximum, minimum.",
            "Maximum concurrent problem flow problems and so on.",
            "So for that they use special technique reformulations but here we have completely general scheme with this type of."
        ],
        [
            "No, the dependence in the end OK in the dimension of the space is the same is for optimal methods.",
            "So for optimal methods that lower complexity bounds in finite dimension must be proportional to dimension and the logarithm of the accuracy.",
            "Here of course we have Delta which is outside the logarithm, but again, so for many situations when the accuracy is not very high, it is still quite good."
        ],
        [
            "Tomates.",
            "And each iteration of this method is very simple, so the same as with ellipsoid method where we have square here.",
            "OK, so therefore definitely four big North and not too small Delta.",
            "So this scheme is much better."
        ],
        [
            "Yes, and also what is important but do not lose anything.",
            "So if N goes to Infinity then we still have upper bound proportional to one over Delta squared."
        ],
        [
            "Now with absolute accuracy, what do we get in the standard formulation?",
            "So we want to have this."
        ],
        [
            "So, and we assume that we know the size of the region."
        ],
        [
            "And the dip should constant for the gradient."
        ],
        [
            "So again.",
            "Just show you the result.",
            "So from this question you estimate for the question you 10 methods, so we have.",
            "We can derive this bound on the residual in the objective functions, which depends of course on Delta, because in our method, so the only parameter is this Delta which describes the relative accuracy.",
            "But now we want absolute accuracy.",
            "Therefore let us choose the deal to have this guy smaller than half of epsilon.",
            "And this guy also have epsilon epsilon divided by our OK then."
        ],
        [
            "If we do this so we get we get."
        ],
        [
            "This amount of iterations, so let's look at that.",
            "OK, again the main term is N divided by epsilon.",
            "And all other things are inside the location.",
            "So."
        ],
        [
            "We still have.",
            "The bound uniform bound when North goes to Infty."
        ],
        [
            "Each.",
            "Which is proportional to one or epsilon squared.",
            "And on the other hand, also you can see that this upper bound is.",
            "Increasing in.",
            "Is increasing and we get this one over epsilon square or only one or equal to Infinity?",
            "So in any keys in all wars, if North is fine, it will get a bound which is better than the bound of the standard subgradient method.",
            "Now I think it is.",
            "Interesting to compare this result with the results of the previous talk because it was mentioned there that there is a lower complexity bound for the black box schemes of the order.",
            "Well, we don't speak about sparse sparse solutions, but dense solutions, yes, so which is proportional by North squared divided by epsilon squared.",
            "With some logarithms.",
            "So here we get something much better, so we improve the lower bound.",
            "Which should be impossible without violation some assumptions, and the only assumptions you have seen that in the presentation of the result is that.",
            "They considered the pure black box scheme, so they were playing with the Oracle as they want, and this internal work of the resisting Oracle.",
            "If you want were not visible for the method here, what we do, we do not do not.",
            "So change the problem too much.",
            "So in order to get this complexity results, we just shift the value of the objective function at the maximum square.",
            "The objective function, and only after we apply the algorithm.",
            "But it appears that even this very simple operations they already can destroy the assumptions of which we need to establish the bounds of the lower bounds of the complexity theory.",
            "Therefore their bounds.",
            "She seems to low complexity bounds.",
            "We need to be very careful because it appears that it is really easy to destroy that this is the example."
        ],
        [
            "OK, now.",
            "Let's just discuss a little bit.",
            "What do we have?"
        ],
        [
            "Couple of minutes.",
            "So this scheme which we get, I call it quasi Newton still.",
            "But maybe it is better to call them variable metric anyway, so it looks very natural.",
            "So this is the one iteration of this scheme.",
            "We minimize the initial quadratic function plus the accumulated quadratic function.",
            "So this is in if you want to minimize the relative scale.",
            "If we want to minimize in absolute scale, there are some parameters.",
            "Here epsilon and the."
        ],
        [
            "Size of the problems in class.",
            "So if we compare this."
        ],
        [
            "With the existing approaches, so we had something like this."
        ],
        [
            "It already, of course, so this so called dual subgradient method or gradient or subgradient method which has more or less the same form similar form.",
            "Again the idea is that all these guys are the lower support for our objective functions, but there there were some explicit rules for choosing the coefficients here, here if the function is Lipschitz continuous gradient then here we use L. If it is just non smooth function, so this guy should grow square root of K. But again so here we have a standard results which are typical for black box scheme and this is different cause.",
            "Here the update of the estimate function is linear, and here it is quadratic.",
            "So at each iteration of this method, so we improve more and more the our knowledge about the geometry of the feasible set."
        ],
        [
            "OK and well, maybe I should mention in the end that this results the also raise again."
        ],
        [
            "The old questions.",
            "So what is there all of the parameters of our problem class?",
            "The dimension in the queue?"
        ],
        [
            "I've seen the complexity analysis and know that actually now it becomes more and more important the complexity of the Oracle, which we typically do not discuss."
        ],
        [
            "So if we look at the existing result.",
            "So they available typical available methods, so black books available available methods in the company exited.",
            "So everywhere you have the total complexity equal to the number of iterations multiplied by the complexity of each iteration of the methods.",
            "OK, so if you look at them with different values of epsilon and T."
        ],
        [
            "You see that, at least in one situation when.",
            "One over epsilon is smaller than N log of one or epsilon and complexity of the Oracle is quadratic.",
            "Then this new method is the best.",
            "OK, it outperforms all other existing oigs."
        ],
        [
            "And techniques, so therefore, of course it is interesting to understand.",
            "So what?",
            "For all values of epsilon Tau, what is the best method?",
            "And actually can we say something here more or less internal in the spirit of low complexity bounds and so on?",
            "That is not clear at all there."
        ],
        [
            "Is also one question which actually May maybe destroy all this nice.",
            "In theory, is that it seems that very often these three parameters of the class, they are not really independent.",
            "So when we form our minimization problems very often have a choice and we can trade off between the dimension, the complexity of the Oracle and all of that actually defines the final accuracy.",
            "We want to have cause the accuracy of the solution shouldn't be.",
            "Much better, well it is.",
            "This is useless then the accuracy of the model."
        ],
        [
            "And in many cities."
        ],
        [
            "Creations like just give you several examples.",
            "There are much more finite elements, optimal control piggies and so on, so we indeed.",
            "These things are independent and how to construct the right descriptions, because now we look more and more in the particular problem structure.",
            "So this question is of course open and but very interest."
        ],
        [
            "And also of course this situation which we have with quite a Newton method so is really very strange."
        ],
        [
            "Three call you.",
            "The history so this was the main approach in optimization.",
            "In 60s and 70s.",
            "So you have seen that, for example, from the references in the 5th."
        ],
        [
            "Stock today, so and this method, they indeed were very good and people were very good."
        ],
        [
            "With them so and some algorithm now are almost forgotten's.",
            "For example our algorithm offshore by nonsmooth optimization was really the best technique for solving nonsmooth mediation problem that it had all features on practice.",
            "People observe what we want, linear rate of convergence, local, very often, quadratic rate of convergence, and so on.",
            "But it was impossible to prove anything about."
        ],
        [
            "This guy the only.",
            "Variable metric methods for which we know more or less some complexity result is the ellipsoid method.",
            "It was the excellent theoretical tool with which we proved many.",
            "Villages of many prob."
        ],
        [
            "Times, but in practice they didn't work very well.",
            "So but maybe again so we should look at again at this guy.",
            "And on the way to approach it to accelerate it.",
            "And maybe to think more about applications.",
            "Maybe it was just."
        ],
        [
            "Applied to different things and in the last 25 years so we didn't have any progress in this field at all, so I don't know."
        ],
        [
            "It seems that maybe now it is a good time to think again about this all technique and to provide them with some global complexity analysis.",
            "Becausw this is exactly what the difference is, I don't know.",
            "The theoretical results from the practical results.",
            "Now nobody is interested in the performance.",
            "Not sure algorithm at the test problems which were in use 25 years ago because it's just problems now.",
            "They're just too small to simple and so on.",
            "But if there would be something reasonable proved about their complexity, this is exactly which this for years, and which remains usable for many, many years in the future.",
            "So this is the final question.",
            "I hope that we will have some progress.",
            "Soon in this direction.",
            "Thank you.",
            "Thanks.",
            "Convergence.",
            "Everything here is mysterious and special reason to use the average here at the state.",
            "Greater convergence average.",
            "You see, in which terms?",
            "Yes.",
            "In terms of the everything.",
            "Yeah.",
            "No, no, this is just a convenient way to choose the answer, so otherwise you can say that about the best points because you observe here you can observe here the optimal value, so it is so this way of generating the answer is useful when you cannot compute the objective function or it is difficult to compute like and stuck astic formulation.",
            "So on so.",
            "So you can use the best value of course.",
            "Actually, it does not utilized the average here in the process, right?",
            "Useless stuff for convergence or so.",
            "Average error.",
            "Every Chapter 8 no, no, this is not necessary in the algorithm.",
            "The only thing we need is this quadratic law quadratic model of the function and it is updated.",
            "And of course the usual thing to store this model is to update the matrix of the quadratic function and the linear term.",
            "So it's only this.",
            "Yeah, I think there's any hope for.",
            "I think we should sort of order an iteration complexity so that we don't have to solve these quadratics.",
            "It depends on the structure, again, of course.",
            "Well, there are some some formulations in which even.",
            "Generations, but iteration is too much, so for that you should apply special methods using.",
            "I'm sorry, coordinate descent and all things like that.",
            "For them it is also possible to develop some theory and see how they work.",
            "Of course this this technique, at least in this form, so it is clear that it is useful for moderate size problems where you can update the store and update.",
            "The the question, but again, so if you have particular problems where you have sparse gradients because you see the discussions is updated by rank 1 updates, and now we assume that the vectors are full, but very often you have only a few elements, so you should store the same technique as linear programming.",
            "You should store it in multiplicative form, an update.",
            "I don't node cheskey factorization and all things like that so.",
            "So some situation it is feasible.",
            "So this is just the general framework for analyzing its convergence, but how you implement this separation?",
            "So it depends on your particular problem."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this is the topic of my talk and I would like to thank the organizers for inviting me at this beautiful place and also they really put my talk very nicely.",
                    "label": 0
                },
                {
                    "sent": "So after two talks about which I can speak a little bit and compare my results with results of previous people.",
                    "label": 0
                },
                {
                    "sent": "Actually my main motivation in this topic was more or less natural.",
                    "label": 0
                },
                {
                    "sent": "So you know last years so we managed.",
                    "label": 0
                },
                {
                    "sent": "Several situation to improve low complexity bounds of.",
                    "label": 0
                },
                {
                    "sent": "I can try to speak louder anyway, so let us take like that because of two microphones.",
                    "label": 0
                },
                {
                    "sent": "This is maybe too much OK.",
                    "label": 0
                },
                {
                    "sent": "So in several situations we managed to improve the low complexity bounds which come from complexity theory.",
                    "label": 0
                },
                {
                    "sent": "Black black box complexity theory.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the results of the past, so this was the case for polynomial time interior point methods after that.",
                    "label": 0
                },
                {
                    "sent": "So there was smoothing technique where we managed also to get algorithms which are much faster and actually in optimization the standard optimism.",
                    "label": 0
                },
                {
                    "sent": "Classical optimization.",
                    "label": 0
                },
                {
                    "sent": "Of course there is during the keys.",
                    "label": 0
                },
                {
                    "sent": "The well known technique for improving the rate of convergence, practical convergence of gradient methods, and this is the quasi Newton method.",
                    "label": 0
                },
                {
                    "sent": "So variable metric methods.",
                    "label": 0
                },
                {
                    "sent": "This technique is known for a long time, but unfortunately and you have seen that from the first talk today, so not too many results, almost no results.",
                    "label": 0
                },
                {
                    "sent": "We know about global performance of these methods, so all results are related to.",
                    "label": 0
                },
                {
                    "sent": "Local complexity analysis.",
                    "label": 0
                },
                {
                    "sent": "And actually this is a pretty big cause.",
                    "label": 0
                },
                {
                    "sent": "Now we're interested in solving high dimensional problems and local complexity analysis.",
                    "label": 0
                },
                {
                    "sent": "Local rate of convergence in many applications.",
                    "label": 0
                },
                {
                    "sent": "It is already too much, so in many applications.",
                    "label": 0
                },
                {
                    "sent": "So the problem is so big and difficult that first we don't need very high accuracy because for big problems the accuracy of the data and of the problem.",
                    "label": 0
                },
                {
                    "sent": "So he is never very very high and 2nd, so the most of computational time is spent.",
                    "label": 0
                },
                {
                    "sent": "The first stage of optimization when we come to the hopefully to the neighborhood of the optimal point.",
                    "label": 0
                },
                {
                    "sent": "And if you have after that quadratic or super linear rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "Of course this is good, but it is not very important because anyway, for fast convergence you do several iterations and that's it.",
                    "label": 0
                },
                {
                    "sent": "But all computational time is spent.",
                    "label": 0
                },
                {
                    "sent": "At the first stage, so my intention was to look somehow another ways of accelerating.",
                    "label": 0
                },
                {
                    "sent": "The methods and as a result, so I'll show you something related to the quasi Newton methods.",
                    "label": 0
                },
                {
                    "sent": "So in order to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who?",
                    "label": 0
                },
                {
                    "sent": "Oh she, what's going on here?",
                    "label": 0
                },
                {
                    "sent": "So let us discuss a little bit about the theoretical estimates which we are able to prove for numerical methods in general, so we'll speak about absolute and relative accuracy after that.",
                    "label": 0
                },
                {
                    "sent": "So we will speak about in you class of convex functions, which appears to be quite interesting for optimization schemes, and it appears that for for this function, so we can propose natural some version of Quasi Newton.",
                    "label": 0
                },
                {
                    "sent": "Method or variable matrix methods for which were able already to get some nontrivial and quite.",
                    "label": 0
                },
                {
                    "sent": "Interesting.",
                    "label": 0
                },
                {
                    "sent": "Results from the rate of converge.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let us look, what do we have?",
                    "label": 0
                },
                {
                    "sent": "I just recall you actually is what the definitions of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The cure issues, so the absolute accuracy we have already seen this definition.",
                    "label": 0
                },
                {
                    "sent": "The second talk, so again, so we are not interested in very complicated structure of optimization problem because all difficulties are already here.",
                    "label": 0
                },
                {
                    "sent": "So when we minimize function F with convex function F over the closed.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set OK so reality for curacy is just the standard definition.",
                    "label": 0
                },
                {
                    "sent": "We want to find the point X bar which is fishable and which function value is good up to accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Epsilon, and in order to get some reasonable results, so we need to assume something.",
                    "label": 0
                },
                {
                    "sent": "Of course about the properties of our problem, so we.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Need to speak about the problem classes and there are basically two bounds.",
                    "label": 1
                },
                {
                    "sent": "Which two properties which were interested in and both of them are bounds on the growth of the function.",
                    "label": 1
                },
                {
                    "sent": "The first is the bound on the growth of the function with respect to the linear approximation, so this is strong convexity.",
                    "label": 0
                },
                {
                    "sent": "Of course we can.",
                    "label": 0
                },
                {
                    "sent": "Very often we can say that mu is equal to 0 and then that is just convexity.",
                    "label": 0
                },
                {
                    "sent": "So assume this and all.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So always assume some upper bounds on the growth of function, and they usually formed in terms of bounds on the first day we have a second derivative.",
                    "label": 1
                },
                {
                    "sent": "In this talk we will be mainly interested in.",
                    "label": 0
                },
                {
                    "sent": "Non small septum isation, so this will be our bound.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and note that even this problem classes they designed for definition of.",
                    "label": 1
                },
                {
                    "sent": "Absolute accuracy.",
                    "label": 0
                },
                {
                    "sent": "This means that complexity of the problem is not changing if we add to our function an absolute constant, whatever is this constant, big, small and so on.",
                    "label": 0
                },
                {
                    "sent": "So we do not change complexity of the problem and the number of iterations which we need in order to get the epsilon accuracy remains the same.",
                    "label": 0
                },
                {
                    "sent": "So this is a confirmation that all we see is done for absolute accuracy, but in application of.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, very often we want to get.",
                    "label": 0
                },
                {
                    "sent": "Really, what you're curious you good deal with curacy of the solution and this direction of the research, actually is not studied well very well in optimization even from the viewpoint of lower complexity bounds and so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is real that you're curious here.",
                    "label": 0
                },
                {
                    "sent": "Again, we have the same problem.",
                    "label": 0
                },
                {
                    "sent": "But in order to define relative accuracy we need.",
                    "label": 1
                },
                {
                    "sent": "The optimal value of our objective function to be positive.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Without this video which you have no sense, and then we introduce parameter accuracy parameter Delta between zero and one and put it said that Xbox has relative accuracy Delta if this inequality is satisfied, OK, there are different.",
                    "label": 0
                },
                {
                    "sent": "Another variant of.",
                    "label": 0
                },
                {
                    "sent": "Definition of relative curious even instead of 1 minus Delta here we put one plus Delta here, but there are more or less equivalent for us.",
                    "label": 0
                },
                {
                    "sent": "It is important.",
                    "label": 0
                },
                {
                    "sent": "It is convenient to have this definition now for our problem class, so this condition that optimal value is positive must be satisfied somehow before even we start to apply methods which I designed for computing the solution in relative accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And how to do that so?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course we the first thing we.",
                    "label": 0
                },
                {
                    "sent": "Think about is to consider special problem classes.",
                    "label": 0
                },
                {
                    "sent": "So there are different problem classes which fits this relative accuracy.",
                    "label": 0
                },
                {
                    "sent": "One of them is just as follows we have.",
                    "label": 0
                },
                {
                    "sent": "Oh cool, convex functions, just homogeneous convex function which contains 0 insufficiency.",
                    "label": 0
                },
                {
                    "sent": "Also it is positive on the whole place and our feasible set Q it doesn't contain zero.",
                    "label": 0
                },
                {
                    "sent": "Then clearly from these two assumptions.",
                    "label": 0
                },
                {
                    "sent": "OK, we know that the option value of the problem is positive and we can try to solve to solve it.",
                    "label": 0
                },
                {
                    "sent": "So our main assumption is that we know here the lower and upper bounds on the growth of.",
                    "label": 0
                },
                {
                    "sent": "The objective function and the important parameter is the condition number.",
                    "label": 0
                },
                {
                    "sent": "If you want of this set was Felicity.",
                    "label": 0
                },
                {
                    "sent": "Which is just grammar 0 divided by gamma one, which is the thing which is smaller than one less or equal than one.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then it is possible to prove that we can apply in this situation the smoothing technique and get the gradient methods which have this.",
                    "label": 0
                },
                {
                    "sent": "This complexity, actually complexity is not too bad the tools because for symmetric set for example, this Alpha can be one over square root of dimension.",
                    "label": 0
                },
                {
                    "sent": "So the square root of N divided by data, but still so we have here kind of very explicit structure of the problem which is not always the case so for.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another model of our data.",
                    "label": 0
                },
                {
                    "sent": "For example, when we have polyhedral B, which is for example symmetric with respect to region.",
                    "label": 0
                },
                {
                    "sent": "So we can achieve similar results.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But using a special norm which we compute by preprocessing.",
                    "label": 0
                },
                {
                    "sent": "So it is just a kind of Phillips with very efficient input type procedure which run in order to find a good approximation of this set, but.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All of that.",
                    "label": 0
                },
                {
                    "sent": "Actually, indeed requires very special structure of our problem, so.",
                    "label": 0
                },
                {
                    "sent": "Which is not always the case.",
                    "label": 0
                },
                {
                    "sent": "So the question is can we address this question so the relative accuracy?",
                    "label": 0
                },
                {
                    "sent": "Think if we have a usual black box framework, OK?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for that, of course, so we need to develop some new problem classes.",
                    "label": 1
                },
                {
                    "sent": "The main feature of this problem, classes that they're not invariant anymore with respect to additional constant, but the invariant with respect to multiplication by a constant.",
                    "label": 1
                },
                {
                    "sent": "This is already makes some difference difference.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the partial answer partial suggestion to this question was done recently in the barrier subgradient method.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which works also with quite special class of problems.",
                    "label": 0
                },
                {
                    "sent": "It does not minimization.",
                    "label": 0
                },
                {
                    "sent": "It is maximization over closed convex set of.",
                    "label": 1
                },
                {
                    "sent": "Can key function.",
                    "label": 0
                },
                {
                    "sent": "And this is important again, since we are speaking about reality for curacy, so that this function is non negative, non negative on this set.",
                    "label": 0
                },
                {
                    "sent": "Q and for this problem.",
                    "label": 0
                },
                {
                    "sent": "So we need a special descriptor description of the convex set.",
                    "label": 0
                },
                {
                    "sent": "Our convex set queue.",
                    "label": 0
                },
                {
                    "sent": "So we assume that we know for this self concordant barrier function doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for us is important.",
                    "label": 0
                },
                {
                    "sent": "So what is what we can do actually?",
                    "label": 0
                },
                {
                    "sent": "So this barrier subgradient method?",
                    "label": 1
                },
                {
                    "sent": "Actually it is just the gradient method primal dual gradient prime dual subgradient method which is applied to.",
                    "label": 0
                },
                {
                    "sent": "But the initial F which we compute by taking the logarithm of the objective and it looks like that.",
                    "label": 0
                },
                {
                    "sent": "So for this potential function we update.",
                    "label": 0
                },
                {
                    "sent": "The model of our problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so before the model of our problem, we subtract the barrier function for our fishable set with certain confusion, and we maximize.",
                    "label": 0
                },
                {
                    "sent": "So our main assumption is that this separation at each step is feasible, and this is very simple.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think So what we can prove about that?",
                    "label": 0
                },
                {
                    "sent": "So this is the complexity of finding the.",
                    "label": 0
                },
                {
                    "sent": "The Delta, the.",
                    "label": 0
                },
                {
                    "sent": "Approximate solution of our problem with relative accuracy dealt so you see, so it is proportional to parameter of the barrier divided by Delta Square and.",
                    "label": 0
                },
                {
                    "sent": "Of course, well this thing is good, I would say because this parameter very often can be much less than the dimension of the space.",
                    "label": 0
                },
                {
                    "sent": "But this thing of course is not very encouraging.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is, can we finally get a more general framework for addressing the functions for minimizing the functions in relative accuracy?",
                    "label": 0
                },
                {
                    "sent": "This was.",
                    "label": 0
                },
                {
                    "sent": "The question.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually the answer came.",
                    "label": 0
                },
                {
                    "sent": "Well possible answer.",
                    "label": 0
                },
                {
                    "sent": "Comes from some may be strange observation which is described now in.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This definition of strictly positive function, so look what is it?",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "Just let us look at this object so we don't know anything about it except strange definition function F is called strictly positive on Q if for all X&Y.",
                    "label": 1
                },
                {
                    "sent": "We have this inequality.",
                    "label": 0
                },
                {
                    "sent": "OK, no definition looks very similar to the convexity, except that for convexity here and here we have minus OK, so we have this, which is this thing.",
                    "label": 0
                },
                {
                    "sent": "And of course if we assume convexity.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The top, so the equivalent thing that F of Y is greater or equal than the absolute value of linear approximation.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "For sure already have nonnegative function and this would be true for all X of Y.",
                    "label": 0
                },
                {
                    "sent": "Very strange guy, but let us look.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "So what we can do with these functions, of course, is a subclass of convex functions.",
                    "label": 0
                },
                {
                    "sent": "There's no negative.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very good, so let us.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look what it could be first generation.",
                    "label": 0
                },
                {
                    "sent": "Of course that the constant and in this in this class is OK.",
                    "label": 0
                },
                {
                    "sent": "So if we take the constant then our definition is valid.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This tender thing.",
                    "label": 0
                },
                {
                    "sent": "OK, now strict positive ITI is an affine invariant property.",
                    "label": 0
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "So if we form from function F Another function but substitute an extent instead of XA linear function of other variables we still have, we still have this inequality because actually so this is definition that is written in multi dimensional form but the same as the convexity.",
                    "label": 0
                },
                {
                    "sent": "1 dimensional property so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the restriction of our variables on some subsets doesn't change anything now again.",
                    "label": 0
                },
                {
                    "sent": "So in terms of F, OK, this is strictly positive.",
                    "label": 1
                },
                {
                    "sent": "Functions form a convex column, so we can multiply them.",
                    "label": 0
                },
                {
                    "sent": "And because this this inequality is convex in F, so we can multiply it by a constant ad and so on.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "News and we already can look.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's simple examples so.",
                    "label": 0
                },
                {
                    "sent": "But quite important for applications.",
                    "label": 0
                },
                {
                    "sent": "So let F use a support function of.",
                    "label": 0
                },
                {
                    "sent": "Convex, bounded and centrally symmetric set convex set OK then.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is strictly positive the way this over again that the answer is very simple source since we have homogeneous function is.",
                    "label": 1
                },
                {
                    "sent": "Use the scalar product of the gradient by the point.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the state of fact, but since.",
                    "label": 0
                },
                {
                    "sent": "The subgradient is symmetric.",
                    "label": 0
                },
                {
                    "sent": "So minus F prime belongs to B and therefore.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or so we have this F of Y is greater than that because this guy is inside and this is exactly that.",
                    "label": 0
                },
                {
                    "sent": "So it is just just identity.",
                    "label": 0
                },
                {
                    "sent": "So this is inequality which we need to establish so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the most simple and natural examples of strictly positive functions are norms.",
                    "label": 0
                },
                {
                    "sent": "Usual norms in finite dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space no.",
                    "label": 0
                },
                {
                    "sent": "Another good news is that the maximum of two strictly positive functions is strictly positive function.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "White this or maybe it is not really evident.",
                    "label": 0
                },
                {
                    "sent": "Let us look at the proof again.",
                    "label": 0
                },
                {
                    "sent": "It is one line, so let us take the value of our function at point Y.",
                    "label": 0
                },
                {
                    "sent": "So and then this value is greater than a particular component of one of our function is now if at the point X, so this component is active in the maximum, then we can use this inequality.",
                    "label": 0
                },
                {
                    "sent": "Since F1 is positive function and this guy equal to the value of our function and this can be considered as the derivative of our function of primer fix.",
                    "label": 0
                },
                {
                    "sent": "So you see.",
                    "label": 0
                },
                {
                    "sent": "So simple observation tells us that maximum is also forms the positive functions.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really write down different examples which are quite important for applications functions which are strictly positive.",
                    "label": 1
                },
                {
                    "sent": "OK, this is maximum of the norm.",
                    "label": 0
                },
                {
                    "sent": "Sum of norms doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "What are the norms?",
                    "label": 0
                },
                {
                    "sent": "So the same with spectral norms.",
                    "label": 0
                },
                {
                    "sent": "Whatever you want many functions like this.",
                    "label": 0
                },
                {
                    "sent": "So what did you already have?",
                    "label": 0
                },
                {
                    "sent": "Quite.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right class of applications.",
                    "label": 0
                },
                {
                    "sent": "But the question is what happens with the general convex functions?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, just look.",
                    "label": 0
                },
                {
                    "sent": "Assume that our function be a general convex function with bounded uniformly bounded subgradients.",
                    "label": 1
                },
                {
                    "sent": "So it is exactly the class or function which is considered in optimization at least well derived for them the complexity bound.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then it appears that the maximum of this function and the norm is strictly positive function.",
                    "label": 0
                },
                {
                    "sent": "So this server is something.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let this look.",
                    "label": 0
                },
                {
                    "sent": "So in the proof, so of course the.",
                    "label": 0
                },
                {
                    "sent": "Gradients of this guy, since the gradients of this is smaller than the gradient of this, is smaller than there.",
                    "label": 0
                },
                {
                    "sent": "So for F also also upgrade.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Abound, but now we need to prove that this guy is positive.",
                    "label": 0
                },
                {
                    "sent": "Yes, and then it is just trivial so.",
                    "label": 0
                },
                {
                    "sent": "Since this isn't boxing, most of these guys greater than or norm.",
                    "label": 0
                },
                {
                    "sent": "Why this is created at a normal X?",
                    "label": 0
                },
                {
                    "sent": "So we have this.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inequality.",
                    "label": 0
                },
                {
                    "sent": "And of course, this is true because Subgradients appositive.",
                    "label": 0
                },
                {
                    "sent": "Observation is very important because it allows us to form indeed many.",
                    "label": 0
                },
                {
                    "sent": "Strictly positive functions from the arbitrary arbitrary convex.",
                    "label": 1
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let us look what we can do is assume we have the minimization problem.",
                    "label": 0
                },
                {
                    "sent": "Of this type were five has bounded subgradients and extra is optimal solution.",
                    "label": 1
                },
                {
                    "sent": "So can we transform it in minimization problems with strictly positive function?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The answer is yes for that, let us form.",
                    "label": 0
                },
                {
                    "sent": "This is the function of this.",
                    "label": 0
                },
                {
                    "sent": "Useful.",
                    "label": 0
                },
                {
                    "sent": "OK, so we shift the value of our functions to have it at 0 equal to zero.",
                    "label": 0
                },
                {
                    "sent": "We add a constant and we take the maximum.",
                    "label": 0
                },
                {
                    "sent": "So here we get.",
                    "label": 0
                },
                {
                    "sent": "General function OK with bounded subgradients, but here is norm with.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we can guarantee by the previous result that it is strictly positive.",
                    "label": 0
                },
                {
                    "sent": "Now let us.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look so it is also clear that.",
                    "label": 1
                },
                {
                    "sent": "If.",
                    "label": 1
                },
                {
                    "sent": "OK. X -- 6 zero is less than R then.",
                    "label": 0
                },
                {
                    "sent": "This part of the maximum is active, so inside this bowl of the shape of our function is defined by the shape of our initial objective function OK.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if the optimal value is not far from zero, then the minimization problem of that type will have the same solution at the minimization problem of that type.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, we can even say so that it's the new value of the objective function here, so belongs to certain intervals between R and two.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we already.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have seen.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Everything except except this last fact.",
                    "label": 0
                },
                {
                    "sent": "OK, which tells us that the option will do this.",
                    "label": 0
                },
                {
                    "sent": "The new optimization problems cannot be too small and this is more or less clear.",
                    "label": 0
                },
                {
                    "sent": "So if we put X0 here, we see that the optimal value of the objective is smaller than two R. On the other hand, this part of the maximum can be estimated from below.",
                    "label": 1
                },
                {
                    "sent": "OK, since the gradient Sofia bounded by that and this maximum have uniformly is uniformly greater than L / R, it is also models clear, so the conclusion.",
                    "label": 0
                },
                {
                    "sent": "Is that actually any?",
                    "label": 0
                },
                {
                    "sent": "Optimization problems can be written with small efforts in form of minimization of positive function over some set.",
                    "label": 0
                },
                {
                    "sent": "OK. Now let us think how we can how we.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Solve this problem and here actually you will see the reason why we are interested in this.",
                    "label": 0
                },
                {
                    "sent": "This strange class.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So functions.",
                    "label": 0
                },
                {
                    "sent": "So let us look.",
                    "label": 0
                },
                {
                    "sent": "And the optimization problem with this strictly positive objective function.",
                    "label": 1
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then let us form.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another optimization problems where we do the thing which actually is completely forbidden in the complexity theory because it destroys all bounds on the derivative and so.",
                    "label": 0
                },
                {
                    "sent": "But here we do that, so we pass to the problem with squared objective function.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Since our function is positive, it is clear that these problems are equivalent and for for this problem we have this nice formula for the generative now.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why it is interesting for us?",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trivial lemma is as follows.",
                    "label": 0
                },
                {
                    "sent": "It appears that.",
                    "label": 0
                },
                {
                    "sent": "For our new function.",
                    "label": 0
                },
                {
                    "sent": "On the top of usual.",
                    "label": 0
                },
                {
                    "sent": "Inequality constraints lower bound.",
                    "label": 0
                },
                {
                    "sent": "We can add something.",
                    "label": 0
                },
                {
                    "sent": "Else impositive quadratic term in the right hand side of this inequality, and this is true for all X&Y.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, well it is true.",
                    "label": 0
                },
                {
                    "sent": "It is just the trivial observation.",
                    "label": 0
                },
                {
                    "sent": "So we have our squared objective function.",
                    "label": 0
                },
                {
                    "sent": "OK, which is this for this guy just.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the definition of strictly positive function, they have this lower bound, but note that it is already the bound which is quadratic in Y. OK, which is already good, but let us look.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We open the brackets.",
                    "label": 0
                },
                {
                    "sent": "And the objective.",
                    "label": 0
                },
                {
                    "sent": "So it is our new objective.",
                    "label": 0
                },
                {
                    "sent": "This guy is arnu gradient and on the top we have.",
                    "label": 0
                },
                {
                    "sent": "This is some additional time, so just for.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nothing.",
                    "label": 0
                },
                {
                    "sent": "We have normal linear support function which actually should help our methods.",
                    "label": 0
                },
                {
                    "sent": "OK because we introduce some additional curvature and it should.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let us look there for the most natural scheme for minimizing these functions, and it is.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looks like follows.",
                    "label": 0
                },
                {
                    "sent": "Initial.",
                    "label": 0
                },
                {
                    "sent": "Matrix GO so just identity matrix if you want doesn't end the relative accuracy Delta which we want to have.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK then, so we will work with the estimate function so called estimates functions say which we will update and the initial estimate function is just the squared distance between X and X0 computed in this metric Jo.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at each iteration our next point will be the minimum of this estimate function.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we will update for the next iteration.",
                    "label": 0
                },
                {
                    "sent": "The estimate function in the following form.",
                    "label": 0
                },
                {
                    "sent": "Exactly what we often use in optimization.",
                    "label": 0
                },
                {
                    "sent": "We updated, so using some positive coefficient and the lower bound lower bound for the objective function usually so it was always the case except strongly convex function.",
                    "label": 0
                },
                {
                    "sent": "But for general convex function we have here only linear term.",
                    "label": 0
                },
                {
                    "sent": "But here we have.",
                    "label": 0
                },
                {
                    "sent": "Additional quadratic term, so is it result.",
                    "label": 0
                },
                {
                    "sent": "At each iteration we will modify the hash and we will modify the creation of our estimate functions and it would be just quadratic function.",
                    "label": 0
                },
                {
                    "sent": "Of course it will see.",
                    "label": 0
                },
                {
                    "sent": "Just trying to understand what is the effect.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is clear just from the form of this update and actually this is the same as.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The things which we use in so-called estimate sequences is that our function is a lower bound for our objective function multiplied by coefficient achy, achy, just the summation of all this guy plus the initial initial guy.",
                    "label": 0
                },
                {
                    "sent": "So the trivial consequences of our inequality that this is the lower support for our objective function.",
                    "label": 0
                },
                {
                    "sent": "Now for calculations a key.",
                    "label": 0
                },
                {
                    "sent": "So now we just fix the formula.",
                    "label": 0
                },
                {
                    "sent": "So it will be.",
                    "label": 0
                },
                {
                    "sent": "We have time for that, so it will be clear later.",
                    "label": 0
                },
                {
                    "sent": "So why it is interesting to have that but.",
                    "label": 0
                },
                {
                    "sent": "In some sense, the method is already is already defined OK, and note that here we have almost no parameters, everything is already is already fix, so let us look what is the rate of convergence of this guy.",
                    "label": 0
                },
                {
                    "sent": "So for that actually we will need we need to relate the minimal value of this estimate functions with the values of our objective function.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before that, let us look at the valuation of the Hessians OK Fashions at each iterations is updated by rank 14 molar, from which we have.",
                    "label": 0
                },
                {
                    "sent": "Because we have a formula for a key.",
                    "label": 0
                },
                {
                    "sent": "So this simple expression.",
                    "label": 0
                },
                {
                    "sent": "So therefore the inverse Hessian is updated in this form.",
                    "label": 0
                },
                {
                    "sent": "But the most important thing is what happens with the determinant of this guy.",
                    "label": 0
                },
                {
                    "sent": "Determinant of Jake grows linearly of course with coefficients 1D.",
                    "label": 0
                },
                {
                    "sent": "To the power K plus one.",
                    "label": 0
                },
                {
                    "sent": "OK, so we get the quadratic function which becomes more and more bigger.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the important thing in our analysis will be the norm of a K squared multiplied by this guy with respect to the new metric.",
                    "label": 0
                },
                {
                    "sent": "OK, what is so?",
                    "label": 0
                },
                {
                    "sent": "Let me just show.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you what is it?",
                    "label": 0
                },
                {
                    "sent": "It is the squared norm of the linear part of this addition.",
                    "label": 0
                },
                {
                    "sent": "With respect to the new creation, and you kacian is the old fashion plus, but this this quadratic term.",
                    "label": 0
                },
                {
                    "sent": "So for this guy I just showed you.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The formula this is the result.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "This is the reason.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or our.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Result, it appears that this just because of our choice of coefficients this.",
                    "label": 0
                },
                {
                    "sent": "Expression is just equal to Delta multiplied by a key multiplied by the value of our squared objective.",
                    "label": 0
                },
                {
                    "sent": "It is just the identity straightforward computation, so, but this is important.",
                    "label": 0
                },
                {
                    "sent": "While it is important because this term.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this guy.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Help us to prove the full and lower bound for the minimal value of the estimate functions OK.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will look at it a little bit later and the reason is.",
                    "label": 0
                },
                {
                    "sent": "This follows so when we modify.",
                    "label": 0
                },
                {
                    "sent": "Our objective function by.",
                    "label": 0
                },
                {
                    "sent": "By this objective estimate function by this objective by this additional quadratic function.",
                    "label": 0
                },
                {
                    "sent": "Then we can write down the lower bound for knew value of psychic.",
                    "label": 0
                },
                {
                    "sent": "Easter is the old.",
                    "label": 0
                },
                {
                    "sent": "Bound plus constant term minus squared norm of this guy of this gradient in you metric and for this squared norm of the gradient in you metric.",
                    "label": 0
                },
                {
                    "sent": "So this guy exactly.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have exact formula.",
                    "label": 0
                },
                {
                    "sent": "We have exact form.",
                    "label": 0
                },
                {
                    "sent": "So finally what we get.",
                    "label": 0
                },
                {
                    "sent": "We get this inequality and.",
                    "label": 0
                },
                {
                    "sent": "As usual in the estimate sequences, it remains to understand what is the rate of growth of the sum of the.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sky fusions, AI.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, so I don't have to.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much time.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally what we prove what we're able to prove is the full.",
                    "label": 0
                },
                {
                    "sent": "So after key iteration.",
                    "label": 0
                },
                {
                    "sent": "So this average point satisfies the following inequality.",
                    "label": 0
                },
                {
                    "sent": "It is less than the optimal value of our function.",
                    "label": 0
                },
                {
                    "sent": "This will multiply by 1 minus Delta is less than the option value of our functions, plus some interesting turn where you see we have exponent.",
                    "label": 0
                },
                {
                    "sent": "So we have some linear rate of convergence which depends on Delta of K of North.",
                    "label": 1
                },
                {
                    "sent": "OK, no, I don't think I have time to discuss all of that, but.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the complete proof.",
                    "label": 0
                },
                {
                    "sent": "It is not very complicated anyway, and the main element here is the inequality between the arithmetic and geometric mean.",
                    "label": 0
                },
                {
                    "sent": "OK, so we get this growth of the photograph of this coefficients AK.",
                    "label": 0
                },
                {
                    "sent": "So what do we get finally?",
                    "label": 0
                },
                {
                    "sent": "So in order to understand.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He's so.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let us introduce the following definition.",
                    "label": 0
                },
                {
                    "sent": "Because the characteristic of the quality of our solution is not standard, so we will say that point X bar is an approximate solution of our problem in mixed accuracy.",
                    "label": 1
                },
                {
                    "sent": "Epsilon Delta accuracy if it satisfies this inequality.",
                    "label": 0
                },
                {
                    "sent": "There are two parts here, so this part is responsible for relative accuracy and epsilon is responsible for absolute accuracy.",
                    "label": 0
                },
                {
                    "sent": "Now what is the complexity?",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of our.",
                    "label": 0
                },
                {
                    "sent": "Finding such a point so it is this.",
                    "label": 0
                },
                {
                    "sent": "This is very interesting expression.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "If you look at that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see that high absolute appeal accuracy here is easy to achieve, so it is inside the logarithm.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "High relatif accuracy.",
                    "label": 0
                },
                {
                    "sent": "Using difficult.",
                    "label": 0
                },
                {
                    "sent": "But again, in many applications we don't need high relative accuracy because for engineers, actually I don't know one person to accuracy in many many situations in more than enough if it is relative accuracy.",
                    "label": 1
                },
                {
                    "sent": "The reason when people want to have high absolute accuracy in the standard approaches is just they don't have no idea about them.",
                    "label": 0
                },
                {
                    "sent": "Constance, but the parameters of the class which we have now, I mean so this LR when we speak about relative absolute accuracy.",
                    "label": 0
                },
                {
                    "sent": "So these guys are really very important and they can make the problem very very difficult here.",
                    "label": 0
                },
                {
                    "sent": "So all these parameters, so they're inside the logarithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so we shouldn't care too much about that, but there is one more.",
                    "label": 0
                },
                {
                    "sent": "Interesting feature of this complexity estimate.",
                    "label": 0
                },
                {
                    "sent": "So look how the dimension enters this bound.",
                    "label": 0
                },
                {
                    "sent": "OK, it is here and it is here so people which.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Already know, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is something in analysis, so this is that when N goes to Infinity, so the right hand side is still bounded and it is bounded more or less by the thing which is typical more or less 4.",
                    "label": 0
                },
                {
                    "sent": "Gradient method.",
                    "label": 0
                },
                {
                    "sent": "So when we have one over epsilon squared, so we have in the denominator.",
                    "label": 0
                },
                {
                    "sent": "Product of the accuracies here OK.",
                    "label": 0
                },
                {
                    "sent": "Yes and yes, and but if we don't do that and dimension is not very high that, then the number of iterations is him is proportional to N over Delta, so as.",
                    "label": 1
                },
                {
                    "sent": "Far is you compare this with existing approaches like the barrier subgradient method where we had here Delta Square there.",
                    "label": 0
                },
                {
                    "sent": "Here we have clear improvement.",
                    "label": 0
                },
                {
                    "sent": "Looking now.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lettuce",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let us look what we can do now for the standard definition of the accuracy.",
                    "label": 0
                },
                {
                    "sent": "Assume we know we want.",
                    "label": 0
                },
                {
                    "sent": "We don't want mix accuracy, but we want to compute the point X bar in relative accuracy.",
                    "label": 1
                },
                {
                    "sent": "What we can get from our approach.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, some.",
                    "label": 0
                },
                {
                    "sent": "So some simple reasoning tells us that in order to get Delta usual accuracy of the data, we need this.",
                    "label": 0
                },
                {
                    "sent": "This number of iterations and divided by data plus something which is inside the logarithm.",
                    "label": 0
                },
                {
                    "sent": "So and here you see this factor.",
                    "label": 0
                },
                {
                    "sent": "So the main factor which defines the complexity of the problem so is not dependent on the data at all.",
                    "label": 1
                },
                {
                    "sent": "So in fact.",
                    "label": 0
                },
                {
                    "sent": "So this means that we get.",
                    "label": 1
                },
                {
                    "sent": "And I think this is more or less the first time the first general fully polynomial time approximation scheme.",
                    "label": 1
                },
                {
                    "sent": "OK, where all everything which is related to the problem is inside the logarithm.",
                    "label": 0
                },
                {
                    "sent": "OK, and the main factor doesn't depend on anything except North Delta.",
                    "label": 0
                },
                {
                    "sent": "The algorithm of this type, they are quite popular in computer science and but usually they are developed for very particular problems like I don't know maximum.",
                    "label": 0
                },
                {
                    "sent": "Maximum, minimum.",
                    "label": 0
                },
                {
                    "sent": "Maximum concurrent problem flow problems and so on.",
                    "label": 0
                },
                {
                    "sent": "So for that they use special technique reformulations but here we have completely general scheme with this type of.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, the dependence in the end OK in the dimension of the space is the same is for optimal methods.",
                    "label": 1
                },
                {
                    "sent": "So for optimal methods that lower complexity bounds in finite dimension must be proportional to dimension and the logarithm of the accuracy.",
                    "label": 0
                },
                {
                    "sent": "Here of course we have Delta which is outside the logarithm, but again, so for many situations when the accuracy is not very high, it is still quite good.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tomates.",
                    "label": 0
                },
                {
                    "sent": "And each iteration of this method is very simple, so the same as with ellipsoid method where we have square here.",
                    "label": 1
                },
                {
                    "sent": "OK, so therefore definitely four big North and not too small Delta.",
                    "label": 0
                },
                {
                    "sent": "So this scheme is much better.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, and also what is important but do not lose anything.",
                    "label": 0
                },
                {
                    "sent": "So if N goes to Infinity then we still have upper bound proportional to one over Delta squared.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now with absolute accuracy, what do we get in the standard formulation?",
                    "label": 0
                },
                {
                    "sent": "So we want to have this.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, and we assume that we know the size of the region.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the dip should constant for the gradient.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again.",
                    "label": 0
                },
                {
                    "sent": "Just show you the result.",
                    "label": 0
                },
                {
                    "sent": "So from this question you estimate for the question you 10 methods, so we have.",
                    "label": 0
                },
                {
                    "sent": "We can derive this bound on the residual in the objective functions, which depends of course on Delta, because in our method, so the only parameter is this Delta which describes the relative accuracy.",
                    "label": 0
                },
                {
                    "sent": "But now we want absolute accuracy.",
                    "label": 0
                },
                {
                    "sent": "Therefore let us choose the deal to have this guy smaller than half of epsilon.",
                    "label": 0
                },
                {
                    "sent": "And this guy also have epsilon epsilon divided by our OK then.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we do this so we get we get.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This amount of iterations, so let's look at that.",
                    "label": 0
                },
                {
                    "sent": "OK, again the main term is N divided by epsilon.",
                    "label": 0
                },
                {
                    "sent": "And all other things are inside the location.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We still have.",
                    "label": 0
                },
                {
                    "sent": "The bound uniform bound when North goes to Infty.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each.",
                    "label": 0
                },
                {
                    "sent": "Which is proportional to one or epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand, also you can see that this upper bound is.",
                    "label": 0
                },
                {
                    "sent": "Increasing in.",
                    "label": 0
                },
                {
                    "sent": "Is increasing and we get this one over epsilon square or only one or equal to Infinity?",
                    "label": 1
                },
                {
                    "sent": "So in any keys in all wars, if North is fine, it will get a bound which is better than the bound of the standard subgradient method.",
                    "label": 1
                },
                {
                    "sent": "Now I think it is.",
                    "label": 0
                },
                {
                    "sent": "Interesting to compare this result with the results of the previous talk because it was mentioned there that there is a lower complexity bound for the black box schemes of the order.",
                    "label": 0
                },
                {
                    "sent": "Well, we don't speak about sparse sparse solutions, but dense solutions, yes, so which is proportional by North squared divided by epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "With some logarithms.",
                    "label": 0
                },
                {
                    "sent": "So here we get something much better, so we improve the lower bound.",
                    "label": 0
                },
                {
                    "sent": "Which should be impossible without violation some assumptions, and the only assumptions you have seen that in the presentation of the result is that.",
                    "label": 0
                },
                {
                    "sent": "They considered the pure black box scheme, so they were playing with the Oracle as they want, and this internal work of the resisting Oracle.",
                    "label": 0
                },
                {
                    "sent": "If you want were not visible for the method here, what we do, we do not do not.",
                    "label": 0
                },
                {
                    "sent": "So change the problem too much.",
                    "label": 0
                },
                {
                    "sent": "So in order to get this complexity results, we just shift the value of the objective function at the maximum square.",
                    "label": 0
                },
                {
                    "sent": "The objective function, and only after we apply the algorithm.",
                    "label": 0
                },
                {
                    "sent": "But it appears that even this very simple operations they already can destroy the assumptions of which we need to establish the bounds of the lower bounds of the complexity theory.",
                    "label": 0
                },
                {
                    "sent": "Therefore their bounds.",
                    "label": 0
                },
                {
                    "sent": "She seems to low complexity bounds.",
                    "label": 0
                },
                {
                    "sent": "We need to be very careful because it appears that it is really easy to destroy that this is the example.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "Let's just discuss a little bit.",
                    "label": 0
                },
                {
                    "sent": "What do we have?",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Couple of minutes.",
                    "label": 0
                },
                {
                    "sent": "So this scheme which we get, I call it quasi Newton still.",
                    "label": 0
                },
                {
                    "sent": "But maybe it is better to call them variable metric anyway, so it looks very natural.",
                    "label": 1
                },
                {
                    "sent": "So this is the one iteration of this scheme.",
                    "label": 0
                },
                {
                    "sent": "We minimize the initial quadratic function plus the accumulated quadratic function.",
                    "label": 0
                },
                {
                    "sent": "So this is in if you want to minimize the relative scale.",
                    "label": 1
                },
                {
                    "sent": "If we want to minimize in absolute scale, there are some parameters.",
                    "label": 1
                },
                {
                    "sent": "Here epsilon and the.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Size of the problems in class.",
                    "label": 0
                },
                {
                    "sent": "So if we compare this.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the existing approaches, so we had something like this.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It already, of course, so this so called dual subgradient method or gradient or subgradient method which has more or less the same form similar form.",
                    "label": 0
                },
                {
                    "sent": "Again the idea is that all these guys are the lower support for our objective functions, but there there were some explicit rules for choosing the coefficients here, here if the function is Lipschitz continuous gradient then here we use L. If it is just non smooth function, so this guy should grow square root of K. But again so here we have a standard results which are typical for black box scheme and this is different cause.",
                    "label": 0
                },
                {
                    "sent": "Here the update of the estimate function is linear, and here it is quadratic.",
                    "label": 0
                },
                {
                    "sent": "So at each iteration of this method, so we improve more and more the our knowledge about the geometry of the feasible set.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK and well, maybe I should mention in the end that this results the also raise again.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The old questions.",
                    "label": 0
                },
                {
                    "sent": "So what is there all of the parameters of our problem class?",
                    "label": 0
                },
                {
                    "sent": "The dimension in the queue?",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've seen the complexity analysis and know that actually now it becomes more and more important the complexity of the Oracle, which we typically do not discuss.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we look at the existing result.",
                    "label": 0
                },
                {
                    "sent": "So they available typical available methods, so black books available available methods in the company exited.",
                    "label": 1
                },
                {
                    "sent": "So everywhere you have the total complexity equal to the number of iterations multiplied by the complexity of each iteration of the methods.",
                    "label": 1
                },
                {
                    "sent": "OK, so if you look at them with different values of epsilon and T.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You see that, at least in one situation when.",
                    "label": 0
                },
                {
                    "sent": "One over epsilon is smaller than N log of one or epsilon and complexity of the Oracle is quadratic.",
                    "label": 1
                },
                {
                    "sent": "Then this new method is the best.",
                    "label": 0
                },
                {
                    "sent": "OK, it outperforms all other existing oigs.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And techniques, so therefore, of course it is interesting to understand.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "For all values of epsilon Tau, what is the best method?",
                    "label": 0
                },
                {
                    "sent": "And actually can we say something here more or less internal in the spirit of low complexity bounds and so on?",
                    "label": 0
                },
                {
                    "sent": "That is not clear at all there.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is also one question which actually May maybe destroy all this nice.",
                    "label": 0
                },
                {
                    "sent": "In theory, is that it seems that very often these three parameters of the class, they are not really independent.",
                    "label": 1
                },
                {
                    "sent": "So when we form our minimization problems very often have a choice and we can trade off between the dimension, the complexity of the Oracle and all of that actually defines the final accuracy.",
                    "label": 1
                },
                {
                    "sent": "We want to have cause the accuracy of the solution shouldn't be.",
                    "label": 0
                },
                {
                    "sent": "Much better, well it is.",
                    "label": 0
                },
                {
                    "sent": "This is useless then the accuracy of the model.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in many cities.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Creations like just give you several examples.",
                    "label": 0
                },
                {
                    "sent": "There are much more finite elements, optimal control piggies and so on, so we indeed.",
                    "label": 0
                },
                {
                    "sent": "These things are independent and how to construct the right descriptions, because now we look more and more in the particular problem structure.",
                    "label": 0
                },
                {
                    "sent": "So this question is of course open and but very interest.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also of course this situation which we have with quite a Newton method so is really very strange.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three call you.",
                    "label": 0
                },
                {
                    "sent": "The history so this was the main approach in optimization.",
                    "label": 0
                },
                {
                    "sent": "In 60s and 70s.",
                    "label": 0
                },
                {
                    "sent": "So you have seen that, for example, from the references in the 5th.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stock today, so and this method, they indeed were very good and people were very good.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With them so and some algorithm now are almost forgotten's.",
                    "label": 0
                },
                {
                    "sent": "For example our algorithm offshore by nonsmooth optimization was really the best technique for solving nonsmooth mediation problem that it had all features on practice.",
                    "label": 0
                },
                {
                    "sent": "People observe what we want, linear rate of convergence, local, very often, quadratic rate of convergence, and so on.",
                    "label": 0
                },
                {
                    "sent": "But it was impossible to prove anything about.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This guy the only.",
                    "label": 0
                },
                {
                    "sent": "Variable metric methods for which we know more or less some complexity result is the ellipsoid method.",
                    "label": 1
                },
                {
                    "sent": "It was the excellent theoretical tool with which we proved many.",
                    "label": 0
                },
                {
                    "sent": "Villages of many prob.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Times, but in practice they didn't work very well.",
                    "label": 0
                },
                {
                    "sent": "So but maybe again so we should look at again at this guy.",
                    "label": 0
                },
                {
                    "sent": "And on the way to approach it to accelerate it.",
                    "label": 0
                },
                {
                    "sent": "And maybe to think more about applications.",
                    "label": 0
                },
                {
                    "sent": "Maybe it was just.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applied to different things and in the last 25 years so we didn't have any progress in this field at all, so I don't know.",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It seems that maybe now it is a good time to think again about this all technique and to provide them with some global complexity analysis.",
                    "label": 1
                },
                {
                    "sent": "Becausw this is exactly what the difference is, I don't know.",
                    "label": 0
                },
                {
                    "sent": "The theoretical results from the practical results.",
                    "label": 0
                },
                {
                    "sent": "Now nobody is interested in the performance.",
                    "label": 0
                },
                {
                    "sent": "Not sure algorithm at the test problems which were in use 25 years ago because it's just problems now.",
                    "label": 0
                },
                {
                    "sent": "They're just too small to simple and so on.",
                    "label": 0
                },
                {
                    "sent": "But if there would be something reasonable proved about their complexity, this is exactly which this for years, and which remains usable for many, many years in the future.",
                    "label": 0
                },
                {
                    "sent": "So this is the final question.",
                    "label": 0
                },
                {
                    "sent": "I hope that we will have some progress.",
                    "label": 0
                },
                {
                    "sent": "Soon in this direction.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Convergence.",
                    "label": 0
                },
                {
                    "sent": "Everything here is mysterious and special reason to use the average here at the state.",
                    "label": 0
                },
                {
                    "sent": "Greater convergence average.",
                    "label": 0
                },
                {
                    "sent": "You see, in which terms?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "In terms of the everything.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, no, this is just a convenient way to choose the answer, so otherwise you can say that about the best points because you observe here you can observe here the optimal value, so it is so this way of generating the answer is useful when you cannot compute the objective function or it is difficult to compute like and stuck astic formulation.",
                    "label": 0
                },
                {
                    "sent": "So on so.",
                    "label": 0
                },
                {
                    "sent": "So you can use the best value of course.",
                    "label": 0
                },
                {
                    "sent": "Actually, it does not utilized the average here in the process, right?",
                    "label": 0
                },
                {
                    "sent": "Useless stuff for convergence or so.",
                    "label": 0
                },
                {
                    "sent": "Average error.",
                    "label": 0
                },
                {
                    "sent": "Every Chapter 8 no, no, this is not necessary in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The only thing we need is this quadratic law quadratic model of the function and it is updated.",
                    "label": 0
                },
                {
                    "sent": "And of course the usual thing to store this model is to update the matrix of the quadratic function and the linear term.",
                    "label": 0
                },
                {
                    "sent": "So it's only this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think there's any hope for.",
                    "label": 0
                },
                {
                    "sent": "I think we should sort of order an iteration complexity so that we don't have to solve these quadratics.",
                    "label": 0
                },
                {
                    "sent": "It depends on the structure, again, of course.",
                    "label": 0
                },
                {
                    "sent": "Well, there are some some formulations in which even.",
                    "label": 0
                },
                {
                    "sent": "Generations, but iteration is too much, so for that you should apply special methods using.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, coordinate descent and all things like that.",
                    "label": 1
                },
                {
                    "sent": "For them it is also possible to develop some theory and see how they work.",
                    "label": 0
                },
                {
                    "sent": "Of course this this technique, at least in this form, so it is clear that it is useful for moderate size problems where you can update the store and update.",
                    "label": 0
                },
                {
                    "sent": "The the question, but again, so if you have particular problems where you have sparse gradients because you see the discussions is updated by rank 1 updates, and now we assume that the vectors are full, but very often you have only a few elements, so you should store the same technique as linear programming.",
                    "label": 0
                },
                {
                    "sent": "You should store it in multiplicative form, an update.",
                    "label": 0
                },
                {
                    "sent": "I don't node cheskey factorization and all things like that so.",
                    "label": 0
                },
                {
                    "sent": "So some situation it is feasible.",
                    "label": 0
                },
                {
                    "sent": "So this is just the general framework for analyzing its convergence, but how you implement this separation?",
                    "label": 0
                },
                {
                    "sent": "So it depends on your particular problem.",
                    "label": 0
                }
            ]
        }
    }
}