{
    "id": "ifwyvp4mxd2uubfpmw67754bbtqbtzi2",
    "title": "Second-order Quantile Methods for Experts and Combinatorial Games",
    "info": {
        "author": [
            "Wouter M. Koolen, Centrum Wiskunde & Informatica (CWI)"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_koolen_quantile_methods/",
    "segmentation": [
        [
            "Hi so I'm very excited to tell you about my latest results which are with Tim.",
            "OK."
        ],
        [
            "Oh you improved the draft.",
            "That I sent you.",
            "Let's see if the pictures come out OK, so I want to talk about prediction with expert advice just to keep things simple.",
            "And I just want to say this is not a toy, it's actually a core instance of many of the advanced online learning tasks.",
            "But let's just look at sort of a simple case."
        ],
        [
            "And see how it goes.",
            "So I want to talk about like 2 popular ways to measure how this problem can be.",
            "In fact simpler than the worst case, right?",
            "It's sort of motivated by practical considerations, so the 1st."
        ],
        [
            "Type of.",
            "Data.",
            "Is sort of yeah, I called the data complexity and the prime example of this is the stochastic case, so it's known that if you have stochastic case with a gap, you can achieve fast rates.",
            "So this is some important special case and there are sort of adverse aerial variations of this like low noise condition, low variance condition.",
            "So in this talk I will."
        ],
        [
            "Talk about these things as second order because you can show that sort of this is the weakest assumption that applies all of them.",
            "OK, and then on the other side."
        ],
        [
            "You have another notion to measure the complexity, which is sort of very strongly related to model selection.",
            "And here the intuition is that if you if you did a good job modeling, so you have for example, a simple good model or experts, or you have multiple good models, then this is an easier case, right?"
        ],
        [
            "And let's talk about, let's call this quantile patterns, because again, the second one sort of implies the first one.",
            "If you do it right.",
            "OK, and then you can think about sort."
        ],
        [
            "Interactions of these so that means we have second order quantile data.",
            "So we want our algorithms to do predict especially well if we are faced with data of this type.",
            "Alright, so is."
        ],
        [
            "At heart so it turns out if you look at the algorithms that we have.",
            "Let's say the exponential or multiplicative weights family edge brought things like this.",
            "If you tune the learning rate with knowledge of all your data in hindsight and stick that in an run with that, then you do in fact exploit all these nice properties.",
            "OK, so.",
            "But that's not really fair.",
            "So can we exploit this online for real without doing this?",
            "Cheating with the learning rate?",
            "So you think OK, it can't be that hard?",
            "Well.",
            "It turns out it's not so easy."
        ],
        [
            "Everyone seems to be struggling with his learning rate.",
            "And I just want to give you sort of a flavor of that.",
            "It's not such a sort of simple parameter, so here I plot the regret of the hedge algorithm on some data that I made up as a function of the learning rate.",
            "And as you can clearly see, there are these two local minima right, and as time goes on, what can what can happen is that the optimal learning rate jumps from this Valley to this Valley, sort of bypassing this mountain of loss in the middle.",
            "So this tells us that we can't.",
            "Have a monotonically decreasing learning rate, but it can't even very slowly.",
            "Overtime it may have to sort of jump.",
            "Right internal.",
            "So.",
            "As a result of this, we don't have second order quantile adaptivity.",
            "We have either or, so we have a bunch of nice specialized algorithms that adapt to 2nd order properties and we have a different set that give us quantile localness.",
            "OK, so.",
            "We tried to get."
        ],
        [
            "Most of that.",
            "And to do so, we had to sort of.",
            "We had to come up with a new way of making such algorithms.",
            "And so sort of.",
            "In essence, what we did is massage, massage, massage the algorithms until they were in such a form that we can do what we sort of like to do.",
            "If you want to learn a parameter, namely the Bayesian thing we put a prior on it and integrate it out right.",
            "So in the standard way of writing heads, you can't do it, but it turns out if you do it like this, it does work.",
            "So what you see here is that you should put the weights proportional to a prior.",
            "And then something that looks like exponential weights with a second order correction term integrated over learning rates with some prior.",
            "With a sort of mysterious eater appearing in some spot that you wouldn't guess if you sort of started thinking from scratch about this, so I just wanted to show you these weights because they're really simple and they can be implemented in closed form this integral.",
            "So it's the same efficiency SH.",
            "You don't lose anything running this.",
            "OK, So what do you get as a result?",
            "Well, this is the first second order quantile bounds, so it says you can take any subset of your experts and what you get is that your regret is bounded by the square root of the variance among these guys.",
            "Times the minus log prior cost of all these guys combined.",
            "So that's the what you expect.",
            "And then there is a small overhead for learning the learning rate.",
            "Just log log T. Right, so the algorithm is fast.",
            "It doesn't have much overhead, so you should use it.",
            "And then at the poster we can tell you about extensions to more complicated settings.",
            "So the combinatorial games, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi so I'm very excited to tell you about my latest results which are with Tim.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh you improved the draft.",
                    "label": 0
                },
                {
                    "sent": "That I sent you.",
                    "label": 0
                },
                {
                    "sent": "Let's see if the pictures come out OK, so I want to talk about prediction with expert advice just to keep things simple.",
                    "label": 1
                },
                {
                    "sent": "And I just want to say this is not a toy, it's actually a core instance of many of the advanced online learning tasks.",
                    "label": 1
                },
                {
                    "sent": "But let's just look at sort of a simple case.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And see how it goes.",
                    "label": 0
                },
                {
                    "sent": "So I want to talk about like 2 popular ways to measure how this problem can be.",
                    "label": 0
                },
                {
                    "sent": "In fact simpler than the worst case, right?",
                    "label": 1
                },
                {
                    "sent": "It's sort of motivated by practical considerations, so the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Type of.",
                    "label": 0
                },
                {
                    "sent": "Data.",
                    "label": 0
                },
                {
                    "sent": "Is sort of yeah, I called the data complexity and the prime example of this is the stochastic case, so it's known that if you have stochastic case with a gap, you can achieve fast rates.",
                    "label": 0
                },
                {
                    "sent": "So this is some important special case and there are sort of adverse aerial variations of this like low noise condition, low variance condition.",
                    "label": 1
                },
                {
                    "sent": "So in this talk I will.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about these things as second order because you can show that sort of this is the weakest assumption that applies all of them.",
                    "label": 0
                },
                {
                    "sent": "OK, and then on the other side.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have another notion to measure the complexity, which is sort of very strongly related to model selection.",
                    "label": 0
                },
                {
                    "sent": "And here the intuition is that if you if you did a good job modeling, so you have for example, a simple good model or experts, or you have multiple good models, then this is an easier case, right?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let's talk about, let's call this quantile patterns, because again, the second one sort of implies the first one.",
                    "label": 0
                },
                {
                    "sent": "If you do it right.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you can think about sort.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interactions of these so that means we have second order quantile data.",
                    "label": 0
                },
                {
                    "sent": "So we want our algorithms to do predict especially well if we are faced with data of this type.",
                    "label": 0
                },
                {
                    "sent": "Alright, so is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At heart so it turns out if you look at the algorithms that we have.",
                    "label": 0
                },
                {
                    "sent": "Let's say the exponential or multiplicative weights family edge brought things like this.",
                    "label": 0
                },
                {
                    "sent": "If you tune the learning rate with knowledge of all your data in hindsight and stick that in an run with that, then you do in fact exploit all these nice properties.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "But that's not really fair.",
                    "label": 0
                },
                {
                    "sent": "So can we exploit this online for real without doing this?",
                    "label": 1
                },
                {
                    "sent": "Cheating with the learning rate?",
                    "label": 0
                },
                {
                    "sent": "So you think OK, it can't be that hard?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "It turns out it's not so easy.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Everyone seems to be struggling with his learning rate.",
                    "label": 1
                },
                {
                    "sent": "And I just want to give you sort of a flavor of that.",
                    "label": 0
                },
                {
                    "sent": "It's not such a sort of simple parameter, so here I plot the regret of the hedge algorithm on some data that I made up as a function of the learning rate.",
                    "label": 1
                },
                {
                    "sent": "And as you can clearly see, there are these two local minima right, and as time goes on, what can what can happen is that the optimal learning rate jumps from this Valley to this Valley, sort of bypassing this mountain of loss in the middle.",
                    "label": 0
                },
                {
                    "sent": "So this tells us that we can't.",
                    "label": 0
                },
                {
                    "sent": "Have a monotonically decreasing learning rate, but it can't even very slowly.",
                    "label": 0
                },
                {
                    "sent": "Overtime it may have to sort of jump.",
                    "label": 0
                },
                {
                    "sent": "Right internal.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As a result of this, we don't have second order quantile adaptivity.",
                    "label": 0
                },
                {
                    "sent": "We have either or, so we have a bunch of nice specialized algorithms that adapt to 2nd order properties and we have a different set that give us quantile localness.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We tried to get.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Most of that.",
                    "label": 0
                },
                {
                    "sent": "And to do so, we had to sort of.",
                    "label": 0
                },
                {
                    "sent": "We had to come up with a new way of making such algorithms.",
                    "label": 0
                },
                {
                    "sent": "And so sort of.",
                    "label": 0
                },
                {
                    "sent": "In essence, what we did is massage, massage, massage the algorithms until they were in such a form that we can do what we sort of like to do.",
                    "label": 0
                },
                {
                    "sent": "If you want to learn a parameter, namely the Bayesian thing we put a prior on it and integrate it out right.",
                    "label": 1
                },
                {
                    "sent": "So in the standard way of writing heads, you can't do it, but it turns out if you do it like this, it does work.",
                    "label": 0
                },
                {
                    "sent": "So what you see here is that you should put the weights proportional to a prior.",
                    "label": 0
                },
                {
                    "sent": "And then something that looks like exponential weights with a second order correction term integrated over learning rates with some prior.",
                    "label": 0
                },
                {
                    "sent": "With a sort of mysterious eater appearing in some spot that you wouldn't guess if you sort of started thinking from scratch about this, so I just wanted to show you these weights because they're really simple and they can be implemented in closed form this integral.",
                    "label": 0
                },
                {
                    "sent": "So it's the same efficiency SH.",
                    "label": 0
                },
                {
                    "sent": "You don't lose anything running this.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do you get as a result?",
                    "label": 0
                },
                {
                    "sent": "Well, this is the first second order quantile bounds, so it says you can take any subset of your experts and what you get is that your regret is bounded by the square root of the variance among these guys.",
                    "label": 0
                },
                {
                    "sent": "Times the minus log prior cost of all these guys combined.",
                    "label": 0
                },
                {
                    "sent": "So that's the what you expect.",
                    "label": 1
                },
                {
                    "sent": "And then there is a small overhead for learning the learning rate.",
                    "label": 1
                },
                {
                    "sent": "Just log log T. Right, so the algorithm is fast.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have much overhead, so you should use it.",
                    "label": 1
                },
                {
                    "sent": "And then at the poster we can tell you about extensions to more complicated settings.",
                    "label": 0
                },
                {
                    "sent": "So the combinatorial games, thanks.",
                    "label": 0
                }
            ]
        }
    }
}