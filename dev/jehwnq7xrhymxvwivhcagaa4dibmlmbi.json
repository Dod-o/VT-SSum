{
    "id": "jehwnq7xrhymxvwivhcagaa4dibmlmbi",
    "title": "Learning Dictionaries for Image Analysis and Sensing",
    "info": {
        "author": [
            "Guillermo Sapiro, Department of Electrical and Computer Engineering, University of Minnesota"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Image Analysis"
        ]
    },
    "url": "http://videolectures.net/mlss09us_sapiro_ldias/",
    "segmentation": [
        [
            "In addition to restore, classify and sense images and videos.",
            "Thank you, the title is clearly there an so there is all the titles in this talk are red.",
            "And there is a lot of red drawings, and as you're matching images, this is a talk about image processing.",
            "Has also lot of red but we will use our imagination and I appreciate the effort to try to solve this.",
            "So the basic idea of my talk, an actually there were some good introduction to some of the topics I'm going to present in the previous talk, but actually I'm going to show something slightly different and maybe will give an explanation to some of the things that the problems that we saw happening in the previous case."
        ],
        [
            "So there is.",
            "This is a international collaboration.",
            "That's what we are here.",
            "But I'm at the University of Minnesota.",
            "I don't know if that was on red or not there, but it's a collaboration with a lot of people, some of them in my Group A PhD students, some of them in France and in the UK and in Israel.",
            "So all this.",
            "Blaster grad students, which are important components of this work."
        ],
        [
            "And this is what's going to be the overview.",
            "So first I'm going to introduce the problem of what do we mean by learning dictionaries that are efficient for sparse representations?",
            "And I'm going to use the problem of image denoising, which interesting enough I'm going to be using the Castle image a lot by chance.",
            "Doesn't have a lot of red, which is very good, so we're going to talk about denoising.",
            "We're gonna learn dictionaries which are multi scale.",
            "And those are dictionaries that are going to be very efficient for representation.",
            "I'm going to make just two kind of small announcements.",
            "Couple of slides only on new results that we're basically combining the model of sparsity with the model of self similarity.",
            "And I'm also going to talk.",
            "Just one slide about this that there's going to be a poster at 5:30 and about, and this actually work is very related to the previous work as I'm going to explain that slide because it presents a model which is in contrast with L1 law, so it is consistent and it comes from universal calling on MDL.",
            "So it has a lot of relationship with the previous talk with parts of the previous talk.",
            "Then we are going to learn we're going to see how we compute efficient representations that are efficient for.",
            "Different tasks and that's gonna be a major message of this talk that we should be very careful with the task and not just take off the shelf representations and the tasks are going to be classified and it has are going to be to sense.",
            "And I'm taking this talk very much as an overview.",
            "As a summer school, so I'm going to give you the highlights and if you want more detail just just please let me know.",
            "But I'm going to try to give you the key idea on what we mean by dictionary."
        ],
        [
            "Really.",
            "So let's start from.",
            "What is a sparse?",
            "An offer?",
            "Redundant representation, so so according to the Webster Dictionary as sparse representation is that we use very few and scatter element."
        ],
        [
            "So.",
            "Titles are not relevant, so there is a lot of titles, but just let's leave them aside, the basic idea and this is a model very similar to what was present before formulation is that we have a certain observation Y that we want to approximate with a model with a representation X, but we need to put some prior information or on X or otherwise this problem is still pose.",
            "So why are our measurements X?",
            "Are the unknowns what we want to try to infer?",
            "So these are the relations to the measurements.",
            "Of course, we want our model our representation to be close to the measurements, otherwise there it's a very easy task and this is the prior.",
            "This is the classical based approach.",
            "If we want to look from that perspective, we can actually give it a different names and everyone will give it the name that he or she prefers.",
            "But it's a classical base estimation with the prior and likelihood.",
            "Anne.",
            "Red is missing, so I'm going to just say here all my highlights are in red, so I'm going to have to say them instead of writing them.",
            "The image processing community and the signal processing community in general has been occupied for the last 50 years or so in understanding what supplier that's one of the most important things in image processing.",
            "What's a good prior for natural images, for example?"
        ],
        [
            "So there we are, sparsity.",
            "An already is gone, so I'm going to tell you what's happening here.",
            "All the area sparsity assumes the following prior assumes that we have a dictionary.",
            "And the dictionary.",
            "So I have a signal in an N dimensional signal, for example, and a Patch of an image an 8 by 8 page.",
            "So thinks about think about N being 64.",
            "I have a Dictionary of atoms.",
            "Normally, this dictionary software complete not always, but very often it is over complete.",
            "So let's think that case 256 four times over complete an I have a vector Alpha and the basic idea is I'm going to represent eggs with elements from this dictionary, but only a few.",
            "In this case 3.",
            "OK, and that's actually kind of the order of magnitude for a 64 dimensional signal, we have 256 and we normally use five or six different atoms at a time, so we don't use all the time.",
            "The same atoms, every single use is different, but we only use a field, so that's the concept of sparsity that this vector is mostly 0.",
            "OK."
        ],
        [
            "So the big problem for us?",
            "Is what should be the dictionary dict if we have the given, then the basic idea is that we are going to try to approximate our signal.",
            "Why the measurements as the Alpha as I show in the previous slide in a way that the LC rapsodo norm?",
            "So this just counts the number of coefficients.",
            "So it's bounded, let's say by L. Either I bound this or I bound the error here.",
            "So we find the Alpha that minimizes this and we reconstruct the image.",
            "The image as the Alpha.",
            "OK Anne, I'm not going to talk too much about the techniques to solve this problem.",
            "I'm going to briefly mention so if we want to talk about L 0, then we use matching pursuit or orthogonal matching pursuit or variations like that.",
            "Normally this is of course an NP complete problem, so those are approximations and either you do a greedy algorithm as matching pursuit, orthogonal matching pursuit.",
            "Are you regularize the problem and that was part of what was mentioned in the previous talk you put here on L1 norm and then the problem becomes convex and then you can optimize for it and you can prove that under certain conditions the problems are equivalent.",
            "OK. Now the big question that we're going to try to address it's what's this, so this should be chosen so that is sparse affies the signal, so not every day will be correct, and that's going to be a lot of what this talk is about about learning the dictionary.",
            "OK, and this is a big difference with the previous talk, because for image processing and for a lot of signal processing problems we can actually learn the we can learn the not only to specify the signal, but we can learn it D to do other things that are two hour.",
            "Convenience.",
            "OK, now I'm going to prove you a very trivial theorem.",
            "Every signal in the world is sparse, OK, and the proof is very simple.",
            "Just put the signal in the dictionary and then sparsity is 1.",
            "OK, but that's that that trivial theorem shows that taking off the shelf dictionaries which has been.",
            "The tradition for many years in image processing one get me to the sparsifying level that I want.",
            "I just gave you an example that you can give the most crazy signal in the world.",
            "Random noise, Gaussian IID and it can be sparse if Gaussian IID is part of the dictionary.",
            "So that is a trivial way of motivating that we should be learning the.",
            "The idea is that we are going to learn this.",
            "And.",
            "We are going to learn it for a number of things here.",
            "By the way, I'm going to put that on the web so you can complete the fill in the blanks that you're forced to do them now, but we are going to learn the two sparse if I signals we are going to learn the for the task, so we're going to have different dictionaries.",
            "If you want to denoise, or if you want to classify, we're also going to learn D for sensing different sensing devices.",
            "OK, so that's a whole goal that I have a freedom of learning.",
            "The of course I have to show you how we do that, but that's going to be the.",
            "Men can't."
        ],
        [
            "Yep here.",
            "So, dictionary learning is the main message here.",
            "OK, and."
        ],
        [
            "There are number of ways of doing that, but the basic idea is the following.",
            "We're going to have how are we going to learn D?",
            "I'm going to have a lot of images.",
            "OK, so I'm going to put all my images as columns here.",
            "OK, so this is N that was my dimension, but this is a very large number 10,000.",
            "OK, I have a lot of images.",
            "Images are not problems today.",
            "OK, the problem that we have too many images is the problem.",
            "OK so if you go to Flickr there is a 6000 images loaded a minute more or less.",
            "So, so that's not the big problem.",
            "Today we're going to learn a dictionary in such a way that every single one of these images.",
            "Is sparsely represented with this dictionary?",
            "That's a whole God.",
            "Our optimization problem looks like this.",
            "I'm going to sum over all the images.",
            "I want a good approximation of every single image by a fixed dictionary and Alpha J, Ann.",
            "I want all my Alpha Jays for all the images for all the images.",
            "I want them to be sparse and you're welcome to put here on L1 norm and make this a loss of problem instead of L0 problem.",
            "OK, but the optimization is in contrast to what it was before that it was only over Alpha.",
            "The optimization is over both the dictionary.",
            "Under representation, both at the same time, OK.",
            "Even if I put L1 here, this is a non convex problem.",
            "OK bye is convex.",
            "If I fix one and optimize for the other.",
            "If I put an L1 here.",
            "So the basic idea of that is to do alternate optimization.",
            "I'm going to mention that next.",
            "So once again each example is a linear combination of atoms from D and each one has to be sparse.",
            "And here is a partial list of a lot of work that has been done in this area.",
            "Just a partial list.",
            "Of that"
        ],
        [
            "So how do we learn the dictionary?",
            "I'm going to tell you just one way.",
            "There are other ways of learning the dictionary, but I just want to give you from the pedagogic POV.",
            "One way of doing that is not the only way, and sometimes we don't.",
            "We used to use this a lot.",
            "We use it a bit less today in our group, but it's a very good way of learning the dictionary.",
            "An unfortunate there is a lot of red, but we can still work here.",
            "So the idea is that we are going to initialize the dictionary, let's say with with cosine plus 48 plus.",
            "Heart functions remember normally this is over complete.",
            "So we're going to initialize the dictionary and we are going to basically.",
            "Door matching pursuit or L1 so with this dictionary.",
            "So let's just look at this.",
            "This is Slider was prepared by by Mickey Laden.",
            "It's a very nice slide, so the basic idea is that I go every every single one of my images and I represent Lee sparsely with the given dictionary.",
            "OK so I have a sparse code I mentioned.",
            "If you use the L1 this is a convex problem.",
            "If you use the L 0 then you do matching pursuit, orthogonal matching pursuit in general.",
            "Now I have for every signal I have it's sparse representation and I'm going to update the dictionary.",
            "In the particular case of the K. SVD, the basic idea is that you fix what's called the active set, and that's done basically by almost all the algorithms that are out there, so I'm not going to.",
            "I'm going to change the atoms of my dictionary, but I'm not going to.",
            "I'm not going to change that this iteration.",
            "The atoms that are being used by the signal and idea is very simple.",
            "So you go to the first Atom of the dictionary.",
            "You say OK, who use me.",
            "And you see that this image you see you and this image you see?",
            "And then you say, OK, I'm going to change myself.",
            "So you two that you like me, you're going to like me even more.",
            "OK, and that's just an SVD problem.",
            "OK. Then I go to the next item and say who used me.",
            "I might find 70 images, they use me.",
            "I'm going to say I'm going to find myself.",
            "I'm going to change myself that you 7 like me more.",
            "It's like I weighted average of all you guys which is an SVD.",
            "OK, and then you keep going so you got a new dictionary.",
            "One column at a time.",
            "And then you iterate these guys.",
            "Once you have a new dictionary, you do new sparse coding, update, dictionary and gold.",
            "OK.",
            "The basic idea in every dictionary learning out there is that you fix the active set fixing the active set makes the problem of dictionary update a convex problem.",
            "OK, so so you fix the active set by doing an L1 or or oh MP and then you optimize the and then you iterate.",
            "OK, so that's a basic idea, yeah?",
            "Yeah, all the images are or.",
            "That means I'm amazed idea we have now online learning that we're learning as images combat, but just in the simplest form, the set of all the images.",
            "But that's a good question.",
            "Just to give you an idea, we train with a few 1000 images for purchase a few thousand patches 8 by 8.",
            "So we're talking 64 we take 1000 or 5000 or 10,000, but you can do that online as as they can.",
            "Yeah, thanks.",
            "By the way, I'm happy to entertain questions during the talk if needed.",
            "Not too many, but just a few yes.",
            "We covered by either president.",
            "Also.",
            "I mean that because as far as I understood you had to keep your head space of all possible cherries that we might have and maybe the problem we would like to actually speak for incentive to cherry that we get is somebody we somehow related to the nature of the problem that you're I'm gonna show you dictionaries and we're going to see that yeah, I have a lot of fun seeing dictionaries, so let's see how they show here, but.",
            "Yeah."
        ],
        [
            "OK.",
            "This this slide says show me the pictures.",
            "OK so I'm going to show you the pictures."
        ],
        [
            "Dictionaries, so these are dictionaries.",
            "These are dictionaries have been learned for color images with exactly the algorithm I mentioned before we do OMP we changed the metric on the OMP, so all in peace.",
            "Looking for the best.",
            "The best Atom in a greedy fashion.",
            "But the metric now is not an inclusion is awaited Oakley and that's to take into account some color, some color metrics.",
            "This is a 7 by 7 dictionary.",
            "This is a 5 by 5 dictionary, OK?",
            "He's knowledgeable.",
            "OK, it's not double.",
            "OK, and that's very important because there's kind of a.",
            "Tradition that when you learn dictionaries, you get Gabor and we can explain offline why sometimes you get and one sometimes you don't.",
            "But these are not gobble type dictionary.",
            "OK, so we're going to see much many more diction is late."
        ],
        [
            "Run.",
            "Just a comment.",
            "This was our optimization.",
            "You can actually put weights so you don't have to do L2 norms for your fitting term.",
            "You can put weighted L2 norms even with missing information.",
            "The only problem there is your SVD becomes A1 rank approximation with missing information, so it's a bit of computation.",
            "More expensive, but the framework works."
        ],
        [
            "So when you have that, you can deal with nonuniform noise.",
            "And this is the noise and this is not so basically how we do this.",
            "We learn a dictionary offline from a data set.",
            "OK, and that's"
        ],
        [
            "This type of dictionary.",
            "Now when."
        ],
        [
            "A new image comes.",
            "We run the image with the noise through another couple of rounds of dictionary learning with very low sparsity so we don't fit the noise.",
            "That dictionary is that when we use, we use to sparsely project the image and we get this nice looking image."
        ],
        [
            "We can do much better.",
            "This is an image where basically every pixel has two colors missing.",
            "OK, so we only have 30% of the data and we can go back here and this is a good point to explain the intuition about this.",
            "So how do I get from this this?",
            "This is based estimation, I have a prior OK.",
            "In other words I have seen enough skies in my life that you just hint that you have Sky and I can get Sky back.",
            "OK, that's becausw.",
            "Actually I should have a diagram that I go from here to here with this plus a dictionary.",
            "I go to this, but the dictionary I learn offline so it's not a lot of work OK but but that's important of base estimation of base type of approach."
        ],
        [
            "Let me just."
        ],
        [
            "Keep the mosaicing.",
            "We can do also in painting and other examples but we did a lot of examples."
        ],
        [
            "This."
        ],
        [
            "You can extend this to multiscale dictionaries.",
            "The way we approach multi scale is in a quadtree formulation.",
            "So basically we're going to learn, let's say a 20 by 20 dictionary.",
            "We're going to also learn 10 by 10 dictionaries, but we're only going to position them in the corners and also 5 by 5.",
            "So what happens an?",
            "I think I forgot to mention.",
            "Will you spy?"
        ],
        [
            "Patches all overlapping patches, so this is not blocks.",
            "These are overlapping patches, so every pixel here is an 8 by 8 Patch, and while we are asking is that every single Patch in his image is in.",
            "This image is partially represented.",
            "OK."
        ],
        [
            "So for example, a Patch 20 by 20 might be represented by 220 by 20 elements of the dictionary for 10 by 10, San 55 by 5.",
            "The learning is the same, it's just more add more elements and the recent tricks to be done there.",
            "OK, but we end up with a multi scale dictionary and it's important concept that also appears in a lot of the wavelets.",
            "Literature is not that this Patch belongs to certain scale and then another Patch to a different scale, not every Patch.",
            "Is.",
            "Represent at the same time with multiple scales."
        ],
        [
            "OK, this is a dictionary that has been learned.",
            "A multi scale, just two scales.",
            "Just to have an idea."
        ],
        [
            "This is a color dictionary and within it imposed that.",
            "But it happened that the the core skills is almost black and white.",
            "OK, the colors remain in the large scales."
        ],
        [
            "You're not going to be able to show the differences between reconstructing with singles get or more diverse scale, but as expected, multiple scales."
        ],
        [
            "Work much better."
        ],
        [
            "You can do this for video, but let me just let me just skip that.",
            "So the basic idea now is that I don't take for granted the dictionary, so I don't go on the side.",
            "My images are good with this city or with wavelets with Haar or with some other.",
            "I force my dictionary to be good for me.",
            "OK, so I want to just make."
        ],
        [
            "Kind of a quote unquote advertising of two recent models that we have been doing.",
            "One is, the following is an is based on universal calling and developing incoherent dictionaries.",
            "So the first time is the same.",
            "The second the sparsifying term is not an LO or an L1 is a log of the coefficients.",
            "This cans from theories in universal coding on MD El OK, so it's not that we put the lock there.",
            "You could actually put it there because Robert Statistics tells you that you should be doing this and not an L1, but of course this is a nonconvex problem.",
            "In contrast with that one.",
            "But if you go to the poster in about half an hour.",
            "Lecumberri Anne Anne Ramirez will tell you why this log shows natural when you do MDF.",
            "OK, this is a.",
            "This is just a normalization term which it always happens.",
            "My dictionaries are normalized, otherwise it doesn't make a lot of sense.",
            "This is an incoherence term that is asking for the atoms in my dictionary to be as incoherent as possible, and this was briefly mentioned before this is needed for the sparse coding techniques to actually work.",
            "OK, when your dictionaries are like this, all the conditions for sparse coding, OMP soft thresholding and the most popular sparse coding techniques are actually based on looking at the maximal eigenvalue of the gram matrix.",
            "So this actually makes your algorithms better for those algorithms.",
            "For those optimization techniques.",
            "And again, so this gives a consistent model.",
            "In contrast, as was mentioned in the previous talk, the last one is not.",
            "It gives much better generalization properties because of this inconsistent.",
            "So basically new images that have not been seen by the training set actually get much better represented.",
            "The active set itself is much more stable.",
            "The coding speed is improved because of this term.",
            "That's actually what soft thresholding likes to have this this as orthogonal as possible.",
            "And the reconstruction is better.",
            "And here comes the missing red.",
            "And I'm going to really see poster by Ramirez and recovery OK today in the afternoon or next week.",
            "If you're too tired after waiting for the projector base in right here, OK?",
            "So this is just kind of funny."
        ],
        [
            "Model the other new model is that there are two models in image processing that are being very successful.",
            "In the last few years.",
            "One is the sparsity that I just mentioned and the other is the self similarity, meaning things repeating images.",
            "It turns out that they are not contradictory and they don't compete.",
            "You can put them together and this is recent work with this team an if you want to know more about that just let me know, but you get significant improvement.",
            "This is a demo seeking example.",
            "All results and new results so.",
            "The most popular, the two of the most popular image processing models today, you can merge together with this sparsity framework."
        ],
        [
            "OK, this was all about representation, but I promised to you before that we can actually design the dictionary to do other things.",
            "So the title here is learn.",
            "Let's learn to classify.",
            "Let's learn dictionaries that are good for something else.",
            "Not for representation.",
            "I don't care to represent so.",
            "If you go to a radiologist that is doing MRI, they really like to see the MRI, but if somebody without them showing them the MRI tells them there is a tumor, there is not a tumor, they still will be very happy for a number of applications, so that's what we're going to try to do now.",
            "We're not going to try to show you back the image we're going to tell you what the image is OK without showing it without."
        ],
        [
            "In an efficient representation.",
            "So let's just give intuition.",
            "These are global dictionaries meaning dictionary that will learn this is a multi scale, 3 scales of a dictionary that has been learned from a global database.",
            "We used the Berkeley database that has been used a lot for segmentation.",
            "We use it to train here, so it's a standard database."
        ],
        [
            "Now we learn a dictionary only with patches of Barbara.",
            "And the dictionary is very different.",
            "OK so it has a lot of texture that wasn't present before."
        ],
        [
            "Then we go and learn the dictionary for another image boat.",
            "And yet we get a different dictionary.",
            "OK, because it has a lot of these mass steals and a lot of the handmade objects.",
            "OK, so this is telling us that somehow the dictionary that we're learning for certain images might be able to help us to classic."
        ],
        [
            "By those images."
        ],
        [
            "And that's exactly what we want to exploit.",
            "So.",
            "We could take classes of images, so let's say I have two classes.",
            "I have bikes and no bikes, so I could learn a dictionary for bikes and I could learn a dictionary for non bikes and then when you give me a new image that you don't know if it's a bike or not, you try to represent it with each one.",
            "If it's a bike it will be much better represented with the first one.",
            "OK and this was done by by Perea couple of years ago in a very very nice work.",
            "You can actually learn a dictionary for all the classes.",
            "Combine and hope that you're going to be able to distinguish.",
            "So you put some hope in your technique, and you say you know, hopefully Class A will pick atoms 3, seven and 20, and Class B will pick atoms 525 and 13, and therefore I'm going to be able to differentiate between them.",
            "But that's a hope.",
            "There is no indication that that will happen.",
            "So we actually in red.",
            "Save that we are not going to hope for this.",
            "We're going to enforce it.",
            "We're going to enforce these two cases.",
            "We're going to enforce that if you're learning two addictions for two classes, they are good for one.",
            "And but for the other.",
            "OK, because it might happen that the dictionary for bikes is also good for non bikes.",
            "You don't know that OK, and we're going to impose that different classes use different coefficients.",
            "Different atoms were going to impose that into the variational formulation and this is related to some works in the in the.",
            "Probabilistic approaches of generative and discriminative.",
            "So this is a big controversy in the learning community between generic."
        ],
        [
            "And this community.",
            "I don't want to go into that.",
            "So let me just give you one example.",
            "OK, we're going to learn multiple dictionaries.",
            "Dictionary one is going to be good for class one, bad for Class B.",
            "For Class 2 and dictionary two is going to be bad for class one and good for Class 2.",
            "So this is what we have before we are going to try to fix the image X approximate as good as we can with the dictionary.",
            "So this is what we have before nothing else than what we had before.",
            "But we're going to be more sophisticated.",
            "This is what we had before and we're going to put a term that comes from regressions from SBM from whatever classification you like.",
            "And the basic idea and look at this minus and this comes with a plus and discounts with a minus.",
            "So this is encouraging a dictionary to have small error for the right class and large error for the other class.",
            "Because one comes with a plus, they are with a minus, so it's just classic logistic regression or techniques like that.",
            "So at the same time I want a dictionary to good to be good for bikes, but for everybody else.",
            "OK so I'm not leaving this to chance."
        ],
        [
            "OK, so just some examples.",
            "This is the standard Broads texture database.",
            "This is what you get.",
            "This is the error about 10% what you get if you learn one dictionary per class, but they don't know that they're going to be competing with the other dictionaries.",
            "If you tell them that they're going to be competing, you have the error.",
            "OK, so you're telling the dictionary you be good for your class, but be bad for the rest.",
            "At the same time.",
            "OK, and these are basically is competing with eleven other classes."
        ],
        [
            "Air OK, this is a read all.",
            "Read this image, but actually comes better here.",
            "This is this is with the grass database that we are actually trying to learn to detect bikes against non bikes and these are the results.",
            "Basically every Patch will tell me if it's a bike or not.",
            "Bike going through that Patch and we basically draw this as hot pixels.",
            "So here it identify regions that have very high chance of being bikes.",
            "What do I mean by that?",
            "These patches are very well represented with the bikes dictionary very badly, represented within an byte.",
            "Bikes dictionary and similar for this other images.",
            "OK, once again I can show you we show in the paper that if we don't impose that."
        ],
        [
            "The results are not as good.",
            "Let me give you a different example.",
            "In the previous case, I say let's learn dictionaries that compete with each other.",
            "You can instead of learning the dictionary, you can actually impose that the coefficients used for class one are different than the other coefficients used by Class 2.",
            "OK, so basically this is.",
            "This is just one metric that we use, but the basic idea is I want the Alpha vector for Class 1 to be orthogonal to the Alpha vector for Class 2.",
            "OK, just as one example, you can put other things here.",
            "Everything makes the optimization of this even more complicated.",
            "I'm skipping that part, but the basic idea is that you can put the task.",
            "In your optimization, the reconstruction won't be as good because the dictionary has to sacrifice OK, but their classification will be much better than if you want offline.",
            "I can show you reconstructions with the pure reconstructive dictionaries or reconstructive plus classification.",
            "They're not as good."
        ],
        [
            "OK. Let me just skip the results.",
            "It works OK, but by 1."
        ],
        [
            "Just give you the concepts and you can combine both concepts of learning dictionaries that are separating and learning alphas that are separating.",
            "And this is related to a paper that we had last year at NIPS with Julian and other colleagues.",
            "And once again I think the important part is to see if you put the discrimination.",
            "So this is for the for this data set for the digits data set, if you don't put the discrimination you get a 4% error the moment you tell.",
            "I'm going to be learning dictionaries for discrimination.",
            "You go down to 1%.",
            "OK, and it's more fair because I'm telling is unfair to use this because my optimization didn't include my task.",
            "Now my optimization is including my task so it's a more fair request."
        ],
        [
            "K. So that was the 2nd part.",
            "We are going to learn.",
            "To classify or to other task.",
            "So the last part of the talk is to learn to sense and that's related to compress."
        ],
        [
            "Dancing.",
            "So compressed sensing say the following, just in very rough words, saying the amount of sensing the amount of samples you need from a signal is proportional to the sparsity level of that signal.",
            "OK, not to Nyquist.",
            "But this proportional to the sparsity level of that signal, OK?",
            "That means that the the more sparse your signal is, the less samples you need so that we have been addressing already.",
            "Because I've been creating dictionaries to get signals very sparse.",
            "OK, so we have been doing a service to this already without doing it on purpose, but it came out.",
            "So that's a request from the dictionary.",
            "Now how are we going to sense part of the beauty of the theory is that most of the compressed sensing literature is about sensing with random matrices.",
            "You get your image, you multiply by a random image by random matrix, and that's a way to sense.",
            "There are a couple of reasons for random sampling.",
            "One is universality.",
            "It works for every for every signal, and the other is because the inverse problem is very stable.",
            "But we like to learn things we don't like to take off the shelf dictionaries or off the shelf sensing matrices.",
            "So the first question is, shall we adapt the senses to the data type, and every engineer will tell you yes, every engineer will tell you that a sensor for voice should not be the same as a sensor for images.",
            "OK, that's why I'm using this as a microphone and the lady is using that to fill me.",
            "OK, not the other way around.",
            "OK, so this issue of universality is kind of mathematically very interesting, but in practice is very clear to everybody that we should be able to do better.",
            "OK, and this has been addressed of optimizing the sensing.",
            "Not taking random, this has been addressed by me, KDB, pay rate, Vice address, that an hour ago and his previous talk, and there are more so upper bounded, some very nice work about designing the sensing that is actually optimized computationally it does as good as as the random, but computationally is much more efficient, OK?",
            "We are gonna try to optimize the sensing and the dictionary, both of them at the same time.",
            "That's what we're going to try to do."
        ],
        [
            "So here is the formulation and I changed notation to be in agreement more with annotation using the compressed sensing, but I'm going to explain it to you.",
            "X is all my data.",
            "Vectorform OK.",
            "This is now my dictionary.",
            "This site is what was did.",
            "OK. And these are my coefficients that I'm going to request to be sparse, so this is what I had before.",
            "And this is what I had before these two terms.",
            "Is that OK?",
            "So I trying to represent my image all my images in a sparse fashion with elements of my dictionary.",
            "But I have to make two more requirements.",
            "One is I'm not going to see the image again in my life because I'm going to sense the image with the matrix multiplication, so this is what I'm going to actually see.",
            "And it's going to be multiplied by this sensing matrix.",
            "OK, during the training I do have access to both, but during the reconstruction this term will disappear.",
            "OK, so this is my sensing matrix.",
            "Forget about this for one second.",
            "OK look only at this.",
            "And I'm going to optimize for my dictionary.",
            "For my coefficients as before and for the sensing matrix all at the same time.",
            "In kind of a coordinate descent type of approach.",
            "OK, if I decide not to optimize for my sensing.",
            "I'm in the previous case.",
            "But and better that the previous case, I'm going to show you because I'm taking into account the sensing.",
            "If for some reason you fixed the dictionary.",
            "I'm still going to go and optimize the sensing, which is related to a large work and an your vice work.",
            "But I'm going to optimize for everything.",
            "OK, there is one more condition that appears here which is related to the uncertainty problem.",
            "Before he's at that condition comes from the theory of compressed sensing.",
            "It says that these two guys cannot be arbitrary, they should be uncorrelated among themselves.",
            "OK, and that's pretty into it.",
            "If you can not sense in the same directions of your dictionary, you're not getting that.",
            "You're not getting nothing.",
            "And that's expressed as they are IP restricted, isometric property and that's related to making the gram matrix corresponding to this product as close as possible to the identity matrix.",
            "So that term also shows up here.",
            "So we have two new terms.",
            "And we're going to optimize for them.",
            "K."
        ],
        [
            "Before I show you the pictures, this is what we get.",
            "Forget about this.",
            "This is just a."
        ],
        [
            "Parameters this Alpha pyramid."
        ],
        [
            "This is classical compressed sensing mean squared error.",
            "What do I mean by classical compressed sensing?",
            "Do do a random matrix?",
            "And one off the shelf dictionary that you somehow learn.",
            "In this case, we learn a dictionary with K SVD.",
            "This is where we are.",
            "This is where we are.",
            "If we optimize for the dictionary under sensing matrix.",
            "About four times improvement in our mean square error, and these are intermediate cases.",
            "For example, you fix the sensing matrix by optimize the dictionary."
        ],
        [
            "Knowing that you're not going to see if we're going to see why so it's a different diction."
        ],
        [
            "Then you end up in the middle or you fix the dictionary and you optimize for the sensing matrix and I forgot which one is the two because they are very similar."
        ],
        [
            "Yeah.",
            "For the learning stage, I know it.",
            "Why I simulated?",
            "I multiply by fee?",
            "Say that again, sorry.",
            "Whi fi X function wise Phoenix wise equal Phoenix Now at the at this I know because I can simulate.",
            "I know it's an I know what the sensing will see because I'm simulating the sensor in the reconstruction.",
            "You don't know this.",
            "So all the images I show you are after we have learned and you only do this with this.",
            "That's a classical compressed sensing reconstruction.",
            "OK, but in the learning I assume for this case that I know both.",
            "There will be applications that people say.",
            "Oh no, you never have access to the original image then then I won't have this, but I will have this.",
            "I will still tell you to optimize for the dictionary.",
            "Yeah, sorry.",
            "So either you're learning path you in their values of these places I can get up, but my question is that now that you even exist example he started dancing when she exacted.",
            "Degenerative nature is all the guarantee that these learning examples in East Framework as a source vision because I'm learn from images.",
            "And I'm going to reconstruct images are not there.",
            "My images are not in the database I use for learning, but it's not.",
            "I'm going to learn images and reconstruct speech.",
            "I'm going to learn images to reconstruct images.",
            "That's that's part of the point.",
            "The point here is University is fine, but I'm trying to design a camera.",
            "OK, so let's use images to design my camera.",
            "Now if tomorrow you wanna use that camera for speech, classical compressed sensing will do much better and we will do.",
            "Random matrices will do much better and we will do, but they will do much worse than if you told me in advance that you were going to use speech OK.",
            "If you are cheating me then.",
            "I follow you OK."
        ],
        [
            "Thanks.",
            "So."
        ],
        [
            "Just believe the images."
        ],
        [
            "So I'm going to show you only these two extremes.",
            "OK."
        ],
        [
            "Great classical compressed sensing."
        ],
        [
            "We are sensing."
        ],
        [
            "Twice the sparsity level, which is about what the theory tells you to do.",
            "Our result with optimized dictionary, an optimized sensing matrix."
        ],
        [
            "So this.",
            "Israel is not just numbers, so if it was like a .1 DB we might not see it or half .0."
        ],
        [
            "One TV, so we actually see it."
        ],
        [
            "Here is another example."
        ],
        [
            "I'm here is another example."
        ],
        [
            "OK.",
            "So let me conclude.",
            "First, I show you recites for chest denoising Ann.",
            "I skip that.",
            "But together with a paper by VCA.",
            "That uses self similarity used to be the best results.",
            "Now that we combine self similarity with sparsity we get better results than either one of two is pretty general in the sense that we can do multiscale vectorial data.",
            "We can learn addictions with internal structure and that's the goal of that poster.",
            "At 5:30, one of the two goals.",
            "And we can learn the dictionary for classification or for other goals that you might have.",
            "And I want to refer you to.",
            "So Arthur Slam has a very nice paper at ICM L in two weeks related to this.",
            "Also about learning.",
            "Basically there is an extension of K means which is called Q flats that it's tough learning centers.",
            "You learn hyperplanes and you can extend that to do classification.",
            "And I also strongly recommend you to look at papers by.",
            "Korean by Larry Correia and his group at ICL.",
            "I'm not a coauthor, so I'm free to advertise that work.",
            "Very nice work also using completely different techniques that are now being merged with these techniques.",
            "But if you're going to ICL, I strongly recommend you to look at those papers.",
            "We can learn the dictionary for adapting to your sensing device.",
            "I have a red coming here that says that a lot still needs to be done.",
            "This is just a warm up."
        ],
        [
            "So one of the day home messages please do not use the wrong dictionaries.",
            "OK, and please be a bit critical when people report results with the wrong dictionaries or with dictionaries that are not the best that you know, so you will see a lot of very good papers in the literature that report results with random dictionaries is very nice to do the math, but the results are not going to be as good if you optimize and this I mentioned before is another word by Julian at ICML where we do online learning.",
            "We don't wait for the data to come, we just do it.",
            "And this is an example where we are actually learning 7 million patches in about 8 minutes.",
            "That's a learning.",
            "Today is a very good programmer, so that's part of the 8 minutes but but it's done online, so it's Lars that was mentioned before for this party.",
            "Calling an online learning for the dictionary.",
            "OK, and you get very good results very fast and without a lot of memory consumption because it's done.",
            "Air."
        ],
        [
            "Offline and this is my last message, sparsity should be learned and I'll take him for granted.",
            "Because then the question is real, how do I know my signal is gonna be sparse on this basis?",
            "I'm not hoping for that and trying to enforce that sparsity should have a goal.",
            "I'm going to learn to do something so you should not learn to do A and expect it to do be OK, cars gone, roads, planes go on there because they were designed for that.",
            "So if we are going to design A dictionary for denoising, we shouldn't be expecting that dictionary to be good for classification.",
            "Of course, if we had infinite amounts of data and the problem were convex, yes.",
            "But we don't have infinite amount.",
            "And the problem is non convex so why not to help the optimization by hinting to the optimization?",
            "What will I be doing with that dictionary White to hide that from the optimization?",
            "Let's just put it there and that's why we should learn to do a by dictionary.",
            "And I this beeps that I'm on time and I apologize for the red, but I hope that we could use our imagination and once again thanks for trying to accommodate to that last minute technical problem.",
            "Thank you.",
            "We have time.",
            "You're right, I didn't tell you is I entertain questions during or after.",
            "No, that's go ahead.",
            "Steep learning, so fast matching is the matching atoms under a known learn and then using the hyper I will refer you to a paper by Stephen Assault on that.",
            "A recent paper on that they have tried.",
            "I will refer you to a paper by them and to paper by us a year from now, hopefully, but but you should look at Steven also auto, under and rubber to see have a paper on that on their website on deblurring by learning dictionaries.",
            "Yes.",
            "Test.",
            "Close said that images are used to make the dictionary price metric closes when you get outside of that.",
            "Close means we learn a dictionary with the Berkeley database, and then we went with the Kodak camera and took images.",
            "And within I step.",
            "So that's how close.",
            "I hope them to be.",
            "So our natural images if you want to work on ultrasound, I will use an ultrasound.",
            "I don't think this will be good to do Ultra Sun and learn from natural images, but we don't have a qualitative measure and with that I'm going to give you another answer that maybe not what you wanted but we don't have a qualitative measure besides using the optimization itself to compare dictionaries.",
            "If I just give you a Dictionary of the street with another, I said what's the distance between them?",
            "One learn for this type of images, one for the other.",
            "I know how to measure the distance through my optimization.",
            "OK, but we don't measure closeness.",
            "We just use that they're all natural images.",
            "Be careful, this dictionary is 256 atoms to represent all the images that are in Flickr.",
            "OK, the reason we can do that is because we work on patches.",
            "OK, we couldn't do that if we were working on the full image computation, it will be impossible, but also you can represent all Flickr, which is, which is a few items.",
            "But by the way, JPEG works.",
            "OK, JPEG works represents every single image in the war with 64 coefficients.",
            "Bear bear Patch.",
            "OK, that's pretty good, and that's why JPEG is a great technique.",
            "Yes, yes.",
            "There's a lot of sort of right, even back in the fray or back.",
            "It's tough.",
            "It's like, wait, let's talk about why that's not happening.",
            "Here it is.",
            "K There are issues of scale and there are issues of overlapping patches.",
            "Those two and this is kind of a level of intuition here is here, but we had a long discussions with you hear about that, but if you go back to the authors of the original papers that show geboren wavelets, they will give you the same reason.",
            "That means if you go for example to the original paper by Bruno, there was no overlapping.",
            "On Dodge paper.",
            "OK, and there was kind of a filtering.",
            "The paper is a super paper, you just the framework was slightly different OK?",
            "Maybe your mom?",
            "I was looking for adapting it.",
            "Better.",
            "No, I'm not.",
            "My sparse answer is no.",
            "Sparsity people claim that it's very important for the visual system, but I will let the experts to try to infer more on that.",
            "Yeah.",
            "Classification performance compared to techniques like convolutional neural Nets, degross.",
            "Everybody compared the grass I show briefly with the graph is the best results with the.",
            "With this I went very fast over that, but is.",
            "So for the grass data."
        ],
        [
            "So.",
            "Here we are.",
            "So slightly better than what was published in the literature."
        ],
        [
            "This weird state of the art."
        ],
        [
            "This we are at.",
            "One sorry, 1.5.",
            "As you know there is less than that, but with a lot of years and work and this is, this is part of the universality.",
            "This was not design.",
            "The technique was not designed for this data set.",
            "The data set was used to learn the same parameters.",
            "The same techniques are used for every classification algorithm that we are using, so there's kind of so I think this is a pretty good result.",
            "Most people in the community say that if you're below 1.5 to that data set without putting a lot of tuning of parameters, then and this is kind of a fair shot type of results.",
            "So for some datasets are compatible for none of the datasets that we have tried.",
            "It makes a disaster OK, but I don't think is expected to make a disaster because I'm optimizing for my goal, so so it shouldn't be a disaster.",
            "And for some of them is state of the art.",
            "We haven't been working on tuning parameters to get to the .6 that I think is the best published.",
            "We just wanted to present the framework when we got this result we were happy with it.",
            "Anymore questions.",
            "Not just angle speaker one, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In addition to restore, classify and sense images and videos.",
                    "label": 1
                },
                {
                    "sent": "Thank you, the title is clearly there an so there is all the titles in this talk are red.",
                    "label": 0
                },
                {
                    "sent": "And there is a lot of red drawings, and as you're matching images, this is a talk about image processing.",
                    "label": 0
                },
                {
                    "sent": "Has also lot of red but we will use our imagination and I appreciate the effort to try to solve this.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea of my talk, an actually there were some good introduction to some of the topics I'm going to present in the previous talk, but actually I'm going to show something slightly different and maybe will give an explanation to some of the things that the problems that we saw happening in the previous case.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is.",
                    "label": 0
                },
                {
                    "sent": "This is a international collaboration.",
                    "label": 0
                },
                {
                    "sent": "That's what we are here.",
                    "label": 0
                },
                {
                    "sent": "But I'm at the University of Minnesota.",
                    "label": 0
                },
                {
                    "sent": "I don't know if that was on red or not there, but it's a collaboration with a lot of people, some of them in my Group A PhD students, some of them in France and in the UK and in Israel.",
                    "label": 0
                },
                {
                    "sent": "So all this.",
                    "label": 0
                },
                {
                    "sent": "Blaster grad students, which are important components of this work.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what's going to be the overview.",
                    "label": 0
                },
                {
                    "sent": "So first I'm going to introduce the problem of what do we mean by learning dictionaries that are efficient for sparse representations?",
                    "label": 0
                },
                {
                    "sent": "And I'm going to use the problem of image denoising, which interesting enough I'm going to be using the Castle image a lot by chance.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have a lot of red, which is very good, so we're going to talk about denoising.",
                    "label": 0
                },
                {
                    "sent": "We're gonna learn dictionaries which are multi scale.",
                    "label": 0
                },
                {
                    "sent": "And those are dictionaries that are going to be very efficient for representation.",
                    "label": 0
                },
                {
                    "sent": "I'm going to make just two kind of small announcements.",
                    "label": 0
                },
                {
                    "sent": "Couple of slides only on new results that we're basically combining the model of sparsity with the model of self similarity.",
                    "label": 0
                },
                {
                    "sent": "And I'm also going to talk.",
                    "label": 0
                },
                {
                    "sent": "Just one slide about this that there's going to be a poster at 5:30 and about, and this actually work is very related to the previous work as I'm going to explain that slide because it presents a model which is in contrast with L1 law, so it is consistent and it comes from universal calling on MDL.",
                    "label": 0
                },
                {
                    "sent": "So it has a lot of relationship with the previous talk with parts of the previous talk.",
                    "label": 0
                },
                {
                    "sent": "Then we are going to learn we're going to see how we compute efficient representations that are efficient for.",
                    "label": 0
                },
                {
                    "sent": "Different tasks and that's gonna be a major message of this talk that we should be very careful with the task and not just take off the shelf representations and the tasks are going to be classified and it has are going to be to sense.",
                    "label": 0
                },
                {
                    "sent": "And I'm taking this talk very much as an overview.",
                    "label": 0
                },
                {
                    "sent": "As a summer school, so I'm going to give you the highlights and if you want more detail just just please let me know.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to try to give you the key idea on what we mean by dictionary.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "So let's start from.",
                    "label": 0
                },
                {
                    "sent": "What is a sparse?",
                    "label": 0
                },
                {
                    "sent": "An offer?",
                    "label": 0
                },
                {
                    "sent": "Redundant representation, so so according to the Webster Dictionary as sparse representation is that we use very few and scatter element.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Titles are not relevant, so there is a lot of titles, but just let's leave them aside, the basic idea and this is a model very similar to what was present before formulation is that we have a certain observation Y that we want to approximate with a model with a representation X, but we need to put some prior information or on X or otherwise this problem is still pose.",
                    "label": 0
                },
                {
                    "sent": "So why are our measurements X?",
                    "label": 1
                },
                {
                    "sent": "Are the unknowns what we want to try to infer?",
                    "label": 0
                },
                {
                    "sent": "So these are the relations to the measurements.",
                    "label": 0
                },
                {
                    "sent": "Of course, we want our model our representation to be close to the measurements, otherwise there it's a very easy task and this is the prior.",
                    "label": 1
                },
                {
                    "sent": "This is the classical based approach.",
                    "label": 0
                },
                {
                    "sent": "If we want to look from that perspective, we can actually give it a different names and everyone will give it the name that he or she prefers.",
                    "label": 0
                },
                {
                    "sent": "But it's a classical base estimation with the prior and likelihood.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Red is missing, so I'm going to just say here all my highlights are in red, so I'm going to have to say them instead of writing them.",
                    "label": 0
                },
                {
                    "sent": "The image processing community and the signal processing community in general has been occupied for the last 50 years or so in understanding what supplier that's one of the most important things in image processing.",
                    "label": 0
                },
                {
                    "sent": "What's a good prior for natural images, for example?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there we are, sparsity.",
                    "label": 0
                },
                {
                    "sent": "An already is gone, so I'm going to tell you what's happening here.",
                    "label": 0
                },
                {
                    "sent": "All the area sparsity assumes the following prior assumes that we have a dictionary.",
                    "label": 0
                },
                {
                    "sent": "And the dictionary.",
                    "label": 0
                },
                {
                    "sent": "So I have a signal in an N dimensional signal, for example, and a Patch of an image an 8 by 8 page.",
                    "label": 0
                },
                {
                    "sent": "So thinks about think about N being 64.",
                    "label": 0
                },
                {
                    "sent": "I have a Dictionary of atoms.",
                    "label": 0
                },
                {
                    "sent": "Normally, this dictionary software complete not always, but very often it is over complete.",
                    "label": 0
                },
                {
                    "sent": "So let's think that case 256 four times over complete an I have a vector Alpha and the basic idea is I'm going to represent eggs with elements from this dictionary, but only a few.",
                    "label": 0
                },
                {
                    "sent": "In this case 3.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's actually kind of the order of magnitude for a 64 dimensional signal, we have 256 and we normally use five or six different atoms at a time, so we don't use all the time.",
                    "label": 0
                },
                {
                    "sent": "The same atoms, every single use is different, but we only use a field, so that's the concept of sparsity that this vector is mostly 0.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the big problem for us?",
                    "label": 0
                },
                {
                    "sent": "Is what should be the dictionary dict if we have the given, then the basic idea is that we are going to try to approximate our signal.",
                    "label": 1
                },
                {
                    "sent": "Why the measurements as the Alpha as I show in the previous slide in a way that the LC rapsodo norm?",
                    "label": 0
                },
                {
                    "sent": "So this just counts the number of coefficients.",
                    "label": 0
                },
                {
                    "sent": "So it's bounded, let's say by L. Either I bound this or I bound the error here.",
                    "label": 0
                },
                {
                    "sent": "So we find the Alpha that minimizes this and we reconstruct the image.",
                    "label": 0
                },
                {
                    "sent": "The image as the Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK Anne, I'm not going to talk too much about the techniques to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "I'm going to briefly mention so if we want to talk about L 0, then we use matching pursuit or orthogonal matching pursuit or variations like that.",
                    "label": 0
                },
                {
                    "sent": "Normally this is of course an NP complete problem, so those are approximations and either you do a greedy algorithm as matching pursuit, orthogonal matching pursuit.",
                    "label": 0
                },
                {
                    "sent": "Are you regularize the problem and that was part of what was mentioned in the previous talk you put here on L1 norm and then the problem becomes convex and then you can optimize for it and you can prove that under certain conditions the problems are equivalent.",
                    "label": 0
                },
                {
                    "sent": "OK. Now the big question that we're going to try to address it's what's this, so this should be chosen so that is sparse affies the signal, so not every day will be correct, and that's going to be a lot of what this talk is about about learning the dictionary.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is a big difference with the previous talk, because for image processing and for a lot of signal processing problems we can actually learn the we can learn the not only to specify the signal, but we can learn it D to do other things that are two hour.",
                    "label": 0
                },
                {
                    "sent": "Convenience.",
                    "label": 0
                },
                {
                    "sent": "OK, now I'm going to prove you a very trivial theorem.",
                    "label": 0
                },
                {
                    "sent": "Every signal in the world is sparse, OK, and the proof is very simple.",
                    "label": 0
                },
                {
                    "sent": "Just put the signal in the dictionary and then sparsity is 1.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's that that trivial theorem shows that taking off the shelf dictionaries which has been.",
                    "label": 0
                },
                {
                    "sent": "The tradition for many years in image processing one get me to the sparsifying level that I want.",
                    "label": 0
                },
                {
                    "sent": "I just gave you an example that you can give the most crazy signal in the world.",
                    "label": 0
                },
                {
                    "sent": "Random noise, Gaussian IID and it can be sparse if Gaussian IID is part of the dictionary.",
                    "label": 0
                },
                {
                    "sent": "So that is a trivial way of motivating that we should be learning the.",
                    "label": 0
                },
                {
                    "sent": "The idea is that we are going to learn this.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "We are going to learn it for a number of things here.",
                    "label": 1
                },
                {
                    "sent": "By the way, I'm going to put that on the web so you can complete the fill in the blanks that you're forced to do them now, but we are going to learn the two sparse if I signals we are going to learn the for the task, so we're going to have different dictionaries.",
                    "label": 0
                },
                {
                    "sent": "If you want to denoise, or if you want to classify, we're also going to learn D for sensing different sensing devices.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a whole goal that I have a freedom of learning.",
                    "label": 0
                },
                {
                    "sent": "The of course I have to show you how we do that, but that's going to be the.",
                    "label": 0
                },
                {
                    "sent": "Men can't.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yep here.",
                    "label": 0
                },
                {
                    "sent": "So, dictionary learning is the main message here.",
                    "label": 1
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are number of ways of doing that, but the basic idea is the following.",
                    "label": 0
                },
                {
                    "sent": "We're going to have how are we going to learn D?",
                    "label": 0
                },
                {
                    "sent": "I'm going to have a lot of images.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to put all my images as columns here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is N that was my dimension, but this is a very large number 10,000.",
                    "label": 0
                },
                {
                    "sent": "OK, I have a lot of images.",
                    "label": 0
                },
                {
                    "sent": "Images are not problems today.",
                    "label": 0
                },
                {
                    "sent": "OK, the problem that we have too many images is the problem.",
                    "label": 0
                },
                {
                    "sent": "OK so if you go to Flickr there is a 6000 images loaded a minute more or less.",
                    "label": 0
                },
                {
                    "sent": "So, so that's not the big problem.",
                    "label": 0
                },
                {
                    "sent": "Today we're going to learn a dictionary in such a way that every single one of these images.",
                    "label": 0
                },
                {
                    "sent": "Is sparsely represented with this dictionary?",
                    "label": 0
                },
                {
                    "sent": "That's a whole God.",
                    "label": 0
                },
                {
                    "sent": "Our optimization problem looks like this.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sum over all the images.",
                    "label": 0
                },
                {
                    "sent": "I want a good approximation of every single image by a fixed dictionary and Alpha J, Ann.",
                    "label": 0
                },
                {
                    "sent": "I want all my Alpha Jays for all the images for all the images.",
                    "label": 0
                },
                {
                    "sent": "I want them to be sparse and you're welcome to put here on L1 norm and make this a loss of problem instead of L0 problem.",
                    "label": 0
                },
                {
                    "sent": "OK, but the optimization is in contrast to what it was before that it was only over Alpha.",
                    "label": 0
                },
                {
                    "sent": "The optimization is over both the dictionary.",
                    "label": 0
                },
                {
                    "sent": "Under representation, both at the same time, OK.",
                    "label": 0
                },
                {
                    "sent": "Even if I put L1 here, this is a non convex problem.",
                    "label": 0
                },
                {
                    "sent": "OK bye is convex.",
                    "label": 0
                },
                {
                    "sent": "If I fix one and optimize for the other.",
                    "label": 0
                },
                {
                    "sent": "If I put an L1 here.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea of that is to do alternate optimization.",
                    "label": 0
                },
                {
                    "sent": "I'm going to mention that next.",
                    "label": 0
                },
                {
                    "sent": "So once again each example is a linear combination of atoms from D and each one has to be sparse.",
                    "label": 1
                },
                {
                    "sent": "And here is a partial list of a lot of work that has been done in this area.",
                    "label": 0
                },
                {
                    "sent": "Just a partial list.",
                    "label": 0
                },
                {
                    "sent": "Of that",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we learn the dictionary?",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you just one way.",
                    "label": 0
                },
                {
                    "sent": "There are other ways of learning the dictionary, but I just want to give you from the pedagogic POV.",
                    "label": 0
                },
                {
                    "sent": "One way of doing that is not the only way, and sometimes we don't.",
                    "label": 0
                },
                {
                    "sent": "We used to use this a lot.",
                    "label": 0
                },
                {
                    "sent": "We use it a bit less today in our group, but it's a very good way of learning the dictionary.",
                    "label": 0
                },
                {
                    "sent": "An unfortunate there is a lot of red, but we can still work here.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we are going to initialize the dictionary, let's say with with cosine plus 48 plus.",
                    "label": 0
                },
                {
                    "sent": "Heart functions remember normally this is over complete.",
                    "label": 0
                },
                {
                    "sent": "So we're going to initialize the dictionary and we are going to basically.",
                    "label": 0
                },
                {
                    "sent": "Door matching pursuit or L1 so with this dictionary.",
                    "label": 1
                },
                {
                    "sent": "So let's just look at this.",
                    "label": 0
                },
                {
                    "sent": "This is Slider was prepared by by Mickey Laden.",
                    "label": 0
                },
                {
                    "sent": "It's a very nice slide, so the basic idea is that I go every every single one of my images and I represent Lee sparsely with the given dictionary.",
                    "label": 0
                },
                {
                    "sent": "OK so I have a sparse code I mentioned.",
                    "label": 0
                },
                {
                    "sent": "If you use the L1 this is a convex problem.",
                    "label": 0
                },
                {
                    "sent": "If you use the L 0 then you do matching pursuit, orthogonal matching pursuit in general.",
                    "label": 0
                },
                {
                    "sent": "Now I have for every signal I have it's sparse representation and I'm going to update the dictionary.",
                    "label": 0
                },
                {
                    "sent": "In the particular case of the K. SVD, the basic idea is that you fix what's called the active set, and that's done basically by almost all the algorithms that are out there, so I'm not going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to change the atoms of my dictionary, but I'm not going to.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to change that this iteration.",
                    "label": 0
                },
                {
                    "sent": "The atoms that are being used by the signal and idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "So you go to the first Atom of the dictionary.",
                    "label": 0
                },
                {
                    "sent": "You say OK, who use me.",
                    "label": 0
                },
                {
                    "sent": "And you see that this image you see you and this image you see?",
                    "label": 0
                },
                {
                    "sent": "And then you say, OK, I'm going to change myself.",
                    "label": 0
                },
                {
                    "sent": "So you two that you like me, you're going to like me even more.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's just an SVD problem.",
                    "label": 0
                },
                {
                    "sent": "OK. Then I go to the next item and say who used me.",
                    "label": 0
                },
                {
                    "sent": "I might find 70 images, they use me.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say I'm going to find myself.",
                    "label": 0
                },
                {
                    "sent": "I'm going to change myself that you 7 like me more.",
                    "label": 0
                },
                {
                    "sent": "It's like I weighted average of all you guys which is an SVD.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you keep going so you got a new dictionary.",
                    "label": 0
                },
                {
                    "sent": "One column at a time.",
                    "label": 0
                },
                {
                    "sent": "And then you iterate these guys.",
                    "label": 0
                },
                {
                    "sent": "Once you have a new dictionary, you do new sparse coding, update, dictionary and gold.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The basic idea in every dictionary learning out there is that you fix the active set fixing the active set makes the problem of dictionary update a convex problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so so you fix the active set by doing an L1 or or oh MP and then you optimize the and then you iterate.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a basic idea, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, all the images are or.",
                    "label": 0
                },
                {
                    "sent": "That means I'm amazed idea we have now online learning that we're learning as images combat, but just in the simplest form, the set of all the images.",
                    "label": 0
                },
                {
                    "sent": "But that's a good question.",
                    "label": 0
                },
                {
                    "sent": "Just to give you an idea, we train with a few 1000 images for purchase a few thousand patches 8 by 8.",
                    "label": 0
                },
                {
                    "sent": "So we're talking 64 we take 1000 or 5000 or 10,000, but you can do that online as as they can.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks.",
                    "label": 0
                },
                {
                    "sent": "By the way, I'm happy to entertain questions during the talk if needed.",
                    "label": 0
                },
                {
                    "sent": "Not too many, but just a few yes.",
                    "label": 0
                },
                {
                    "sent": "We covered by either president.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "I mean that because as far as I understood you had to keep your head space of all possible cherries that we might have and maybe the problem we would like to actually speak for incentive to cherry that we get is somebody we somehow related to the nature of the problem that you're I'm gonna show you dictionaries and we're going to see that yeah, I have a lot of fun seeing dictionaries, so let's see how they show here, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This this slide says show me the pictures.",
                    "label": 1
                },
                {
                    "sent": "OK so I'm going to show you the pictures.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dictionaries, so these are dictionaries.",
                    "label": 0
                },
                {
                    "sent": "These are dictionaries have been learned for color images with exactly the algorithm I mentioned before we do OMP we changed the metric on the OMP, so all in peace.",
                    "label": 1
                },
                {
                    "sent": "Looking for the best.",
                    "label": 0
                },
                {
                    "sent": "The best Atom in a greedy fashion.",
                    "label": 0
                },
                {
                    "sent": "But the metric now is not an inclusion is awaited Oakley and that's to take into account some color, some color metrics.",
                    "label": 0
                },
                {
                    "sent": "This is a 7 by 7 dictionary.",
                    "label": 0
                },
                {
                    "sent": "This is a 5 by 5 dictionary, OK?",
                    "label": 0
                },
                {
                    "sent": "He's knowledgeable.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not double.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's very important because there's kind of a.",
                    "label": 0
                },
                {
                    "sent": "Tradition that when you learn dictionaries, you get Gabor and we can explain offline why sometimes you get and one sometimes you don't.",
                    "label": 0
                },
                {
                    "sent": "But these are not gobble type dictionary.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to see much many more diction is late.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Run.",
                    "label": 0
                },
                {
                    "sent": "Just a comment.",
                    "label": 0
                },
                {
                    "sent": "This was our optimization.",
                    "label": 0
                },
                {
                    "sent": "You can actually put weights so you don't have to do L2 norms for your fitting term.",
                    "label": 0
                },
                {
                    "sent": "You can put weighted L2 norms even with missing information.",
                    "label": 0
                },
                {
                    "sent": "The only problem there is your SVD becomes A1 rank approximation with missing information, so it's a bit of computation.",
                    "label": 0
                },
                {
                    "sent": "More expensive, but the framework works.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when you have that, you can deal with nonuniform noise.",
                    "label": 1
                },
                {
                    "sent": "And this is the noise and this is not so basically how we do this.",
                    "label": 0
                },
                {
                    "sent": "We learn a dictionary offline from a data set.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This type of dictionary.",
                    "label": 0
                },
                {
                    "sent": "Now when.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A new image comes.",
                    "label": 0
                },
                {
                    "sent": "We run the image with the noise through another couple of rounds of dictionary learning with very low sparsity so we don't fit the noise.",
                    "label": 0
                },
                {
                    "sent": "That dictionary is that when we use, we use to sparsely project the image and we get this nice looking image.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can do much better.",
                    "label": 0
                },
                {
                    "sent": "This is an image where basically every pixel has two colors missing.",
                    "label": 0
                },
                {
                    "sent": "OK, so we only have 30% of the data and we can go back here and this is a good point to explain the intuition about this.",
                    "label": 0
                },
                {
                    "sent": "So how do I get from this this?",
                    "label": 0
                },
                {
                    "sent": "This is based estimation, I have a prior OK.",
                    "label": 0
                },
                {
                    "sent": "In other words I have seen enough skies in my life that you just hint that you have Sky and I can get Sky back.",
                    "label": 0
                },
                {
                    "sent": "OK, that's becausw.",
                    "label": 0
                },
                {
                    "sent": "Actually I should have a diagram that I go from here to here with this plus a dictionary.",
                    "label": 0
                },
                {
                    "sent": "I go to this, but the dictionary I learn offline so it's not a lot of work OK but but that's important of base estimation of base type of approach.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Keep the mosaicing.",
                    "label": 0
                },
                {
                    "sent": "We can do also in painting and other examples but we did a lot of examples.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can extend this to multiscale dictionaries.",
                    "label": 1
                },
                {
                    "sent": "The way we approach multi scale is in a quadtree formulation.",
                    "label": 0
                },
                {
                    "sent": "So basically we're going to learn, let's say a 20 by 20 dictionary.",
                    "label": 0
                },
                {
                    "sent": "We're going to also learn 10 by 10 dictionaries, but we're only going to position them in the corners and also 5 by 5.",
                    "label": 0
                },
                {
                    "sent": "So what happens an?",
                    "label": 0
                },
                {
                    "sent": "I think I forgot to mention.",
                    "label": 0
                },
                {
                    "sent": "Will you spy?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patches all overlapping patches, so this is not blocks.",
                    "label": 0
                },
                {
                    "sent": "These are overlapping patches, so every pixel here is an 8 by 8 Patch, and while we are asking is that every single Patch in his image is in.",
                    "label": 0
                },
                {
                    "sent": "This image is partially represented.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, a Patch 20 by 20 might be represented by 220 by 20 elements of the dictionary for 10 by 10, San 55 by 5.",
                    "label": 0
                },
                {
                    "sent": "The learning is the same, it's just more add more elements and the recent tricks to be done there.",
                    "label": 0
                },
                {
                    "sent": "OK, but we end up with a multi scale dictionary and it's important concept that also appears in a lot of the wavelets.",
                    "label": 0
                },
                {
                    "sent": "Literature is not that this Patch belongs to certain scale and then another Patch to a different scale, not every Patch.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Represent at the same time with multiple scales.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this is a dictionary that has been learned.",
                    "label": 0
                },
                {
                    "sent": "A multi scale, just two scales.",
                    "label": 0
                },
                {
                    "sent": "Just to have an idea.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a color dictionary and within it imposed that.",
                    "label": 0
                },
                {
                    "sent": "But it happened that the the core skills is almost black and white.",
                    "label": 0
                },
                {
                    "sent": "OK, the colors remain in the large scales.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're not going to be able to show the differences between reconstructing with singles get or more diverse scale, but as expected, multiple scales.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work much better.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do this for video, but let me just let me just skip that.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea now is that I don't take for granted the dictionary, so I don't go on the side.",
                    "label": 0
                },
                {
                    "sent": "My images are good with this city or with wavelets with Haar or with some other.",
                    "label": 0
                },
                {
                    "sent": "I force my dictionary to be good for me.",
                    "label": 0
                },
                {
                    "sent": "OK, so I want to just make.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of a quote unquote advertising of two recent models that we have been doing.",
                    "label": 0
                },
                {
                    "sent": "One is, the following is an is based on universal calling and developing incoherent dictionaries.",
                    "label": 0
                },
                {
                    "sent": "So the first time is the same.",
                    "label": 0
                },
                {
                    "sent": "The second the sparsifying term is not an LO or an L1 is a log of the coefficients.",
                    "label": 0
                },
                {
                    "sent": "This cans from theories in universal coding on MD El OK, so it's not that we put the lock there.",
                    "label": 0
                },
                {
                    "sent": "You could actually put it there because Robert Statistics tells you that you should be doing this and not an L1, but of course this is a nonconvex problem.",
                    "label": 0
                },
                {
                    "sent": "In contrast with that one.",
                    "label": 0
                },
                {
                    "sent": "But if you go to the poster in about half an hour.",
                    "label": 0
                },
                {
                    "sent": "Lecumberri Anne Anne Ramirez will tell you why this log shows natural when you do MDF.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a.",
                    "label": 0
                },
                {
                    "sent": "This is just a normalization term which it always happens.",
                    "label": 0
                },
                {
                    "sent": "My dictionaries are normalized, otherwise it doesn't make a lot of sense.",
                    "label": 0
                },
                {
                    "sent": "This is an incoherence term that is asking for the atoms in my dictionary to be as incoherent as possible, and this was briefly mentioned before this is needed for the sparse coding techniques to actually work.",
                    "label": 0
                },
                {
                    "sent": "OK, when your dictionaries are like this, all the conditions for sparse coding, OMP soft thresholding and the most popular sparse coding techniques are actually based on looking at the maximal eigenvalue of the gram matrix.",
                    "label": 0
                },
                {
                    "sent": "So this actually makes your algorithms better for those algorithms.",
                    "label": 0
                },
                {
                    "sent": "For those optimization techniques.",
                    "label": 0
                },
                {
                    "sent": "And again, so this gives a consistent model.",
                    "label": 0
                },
                {
                    "sent": "In contrast, as was mentioned in the previous talk, the last one is not.",
                    "label": 0
                },
                {
                    "sent": "It gives much better generalization properties because of this inconsistent.",
                    "label": 0
                },
                {
                    "sent": "So basically new images that have not been seen by the training set actually get much better represented.",
                    "label": 0
                },
                {
                    "sent": "The active set itself is much more stable.",
                    "label": 0
                },
                {
                    "sent": "The coding speed is improved because of this term.",
                    "label": 0
                },
                {
                    "sent": "That's actually what soft thresholding likes to have this this as orthogonal as possible.",
                    "label": 0
                },
                {
                    "sent": "And the reconstruction is better.",
                    "label": 0
                },
                {
                    "sent": "And here comes the missing red.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to really see poster by Ramirez and recovery OK today in the afternoon or next week.",
                    "label": 0
                },
                {
                    "sent": "If you're too tired after waiting for the projector base in right here, OK?",
                    "label": 0
                },
                {
                    "sent": "So this is just kind of funny.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model the other new model is that there are two models in image processing that are being very successful.",
                    "label": 0
                },
                {
                    "sent": "In the last few years.",
                    "label": 0
                },
                {
                    "sent": "One is the sparsity that I just mentioned and the other is the self similarity, meaning things repeating images.",
                    "label": 0
                },
                {
                    "sent": "It turns out that they are not contradictory and they don't compete.",
                    "label": 0
                },
                {
                    "sent": "You can put them together and this is recent work with this team an if you want to know more about that just let me know, but you get significant improvement.",
                    "label": 0
                },
                {
                    "sent": "This is a demo seeking example.",
                    "label": 0
                },
                {
                    "sent": "All results and new results so.",
                    "label": 0
                },
                {
                    "sent": "The most popular, the two of the most popular image processing models today, you can merge together with this sparsity framework.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this was all about representation, but I promised to you before that we can actually design the dictionary to do other things.",
                    "label": 0
                },
                {
                    "sent": "So the title here is learn.",
                    "label": 0
                },
                {
                    "sent": "Let's learn to classify.",
                    "label": 0
                },
                {
                    "sent": "Let's learn dictionaries that are good for something else.",
                    "label": 0
                },
                {
                    "sent": "Not for representation.",
                    "label": 0
                },
                {
                    "sent": "I don't care to represent so.",
                    "label": 0
                },
                {
                    "sent": "If you go to a radiologist that is doing MRI, they really like to see the MRI, but if somebody without them showing them the MRI tells them there is a tumor, there is not a tumor, they still will be very happy for a number of applications, so that's what we're going to try to do now.",
                    "label": 0
                },
                {
                    "sent": "We're not going to try to show you back the image we're going to tell you what the image is OK without showing it without.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In an efficient representation.",
                    "label": 0
                },
                {
                    "sent": "So let's just give intuition.",
                    "label": 0
                },
                {
                    "sent": "These are global dictionaries meaning dictionary that will learn this is a multi scale, 3 scales of a dictionary that has been learned from a global database.",
                    "label": 0
                },
                {
                    "sent": "We used the Berkeley database that has been used a lot for segmentation.",
                    "label": 0
                },
                {
                    "sent": "We use it to train here, so it's a standard database.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we learn a dictionary only with patches of Barbara.",
                    "label": 0
                },
                {
                    "sent": "And the dictionary is very different.",
                    "label": 0
                },
                {
                    "sent": "OK so it has a lot of texture that wasn't present before.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we go and learn the dictionary for another image boat.",
                    "label": 0
                },
                {
                    "sent": "And yet we get a different dictionary.",
                    "label": 0
                },
                {
                    "sent": "OK, because it has a lot of these mass steals and a lot of the handmade objects.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is telling us that somehow the dictionary that we're learning for certain images might be able to help us to classic.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By those images.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's exactly what we want to exploit.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We could take classes of images, so let's say I have two classes.",
                    "label": 0
                },
                {
                    "sent": "I have bikes and no bikes, so I could learn a dictionary for bikes and I could learn a dictionary for non bikes and then when you give me a new image that you don't know if it's a bike or not, you try to represent it with each one.",
                    "label": 0
                },
                {
                    "sent": "If it's a bike it will be much better represented with the first one.",
                    "label": 0
                },
                {
                    "sent": "OK and this was done by by Perea couple of years ago in a very very nice work.",
                    "label": 0
                },
                {
                    "sent": "You can actually learn a dictionary for all the classes.",
                    "label": 0
                },
                {
                    "sent": "Combine and hope that you're going to be able to distinguish.",
                    "label": 0
                },
                {
                    "sent": "So you put some hope in your technique, and you say you know, hopefully Class A will pick atoms 3, seven and 20, and Class B will pick atoms 525 and 13, and therefore I'm going to be able to differentiate between them.",
                    "label": 0
                },
                {
                    "sent": "But that's a hope.",
                    "label": 0
                },
                {
                    "sent": "There is no indication that that will happen.",
                    "label": 0
                },
                {
                    "sent": "So we actually in red.",
                    "label": 0
                },
                {
                    "sent": "Save that we are not going to hope for this.",
                    "label": 0
                },
                {
                    "sent": "We're going to enforce it.",
                    "label": 0
                },
                {
                    "sent": "We're going to enforce these two cases.",
                    "label": 0
                },
                {
                    "sent": "We're going to enforce that if you're learning two addictions for two classes, they are good for one.",
                    "label": 0
                },
                {
                    "sent": "And but for the other.",
                    "label": 0
                },
                {
                    "sent": "OK, because it might happen that the dictionary for bikes is also good for non bikes.",
                    "label": 0
                },
                {
                    "sent": "You don't know that OK, and we're going to impose that different classes use different coefficients.",
                    "label": 0
                },
                {
                    "sent": "Different atoms were going to impose that into the variational formulation and this is related to some works in the in the.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic approaches of generative and discriminative.",
                    "label": 0
                },
                {
                    "sent": "So this is a big controversy in the learning community between generic.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this community.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go into that.",
                    "label": 0
                },
                {
                    "sent": "So let me just give you one example.",
                    "label": 0
                },
                {
                    "sent": "OK, we're going to learn multiple dictionaries.",
                    "label": 1
                },
                {
                    "sent": "Dictionary one is going to be good for class one, bad for Class B.",
                    "label": 1
                },
                {
                    "sent": "For Class 2 and dictionary two is going to be bad for class one and good for Class 2.",
                    "label": 0
                },
                {
                    "sent": "So this is what we have before we are going to try to fix the image X approximate as good as we can with the dictionary.",
                    "label": 0
                },
                {
                    "sent": "So this is what we have before nothing else than what we had before.",
                    "label": 0
                },
                {
                    "sent": "But we're going to be more sophisticated.",
                    "label": 0
                },
                {
                    "sent": "This is what we had before and we're going to put a term that comes from regressions from SBM from whatever classification you like.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea and look at this minus and this comes with a plus and discounts with a minus.",
                    "label": 0
                },
                {
                    "sent": "So this is encouraging a dictionary to have small error for the right class and large error for the other class.",
                    "label": 0
                },
                {
                    "sent": "Because one comes with a plus, they are with a minus, so it's just classic logistic regression or techniques like that.",
                    "label": 0
                },
                {
                    "sent": "So at the same time I want a dictionary to good to be good for bikes, but for everybody else.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm not leaving this to chance.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just some examples.",
                    "label": 0
                },
                {
                    "sent": "This is the standard Broads texture database.",
                    "label": 0
                },
                {
                    "sent": "This is what you get.",
                    "label": 0
                },
                {
                    "sent": "This is the error about 10% what you get if you learn one dictionary per class, but they don't know that they're going to be competing with the other dictionaries.",
                    "label": 0
                },
                {
                    "sent": "If you tell them that they're going to be competing, you have the error.",
                    "label": 0
                },
                {
                    "sent": "OK, so you're telling the dictionary you be good for your class, but be bad for the rest.",
                    "label": 0
                },
                {
                    "sent": "At the same time.",
                    "label": 0
                },
                {
                    "sent": "OK, and these are basically is competing with eleven other classes.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Air OK, this is a read all.",
                    "label": 0
                },
                {
                    "sent": "Read this image, but actually comes better here.",
                    "label": 0
                },
                {
                    "sent": "This is this is with the grass database that we are actually trying to learn to detect bikes against non bikes and these are the results.",
                    "label": 0
                },
                {
                    "sent": "Basically every Patch will tell me if it's a bike or not.",
                    "label": 0
                },
                {
                    "sent": "Bike going through that Patch and we basically draw this as hot pixels.",
                    "label": 0
                },
                {
                    "sent": "So here it identify regions that have very high chance of being bikes.",
                    "label": 0
                },
                {
                    "sent": "What do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "These patches are very well represented with the bikes dictionary very badly, represented within an byte.",
                    "label": 0
                },
                {
                    "sent": "Bikes dictionary and similar for this other images.",
                    "label": 0
                },
                {
                    "sent": "OK, once again I can show you we show in the paper that if we don't impose that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results are not as good.",
                    "label": 0
                },
                {
                    "sent": "Let me give you a different example.",
                    "label": 0
                },
                {
                    "sent": "In the previous case, I say let's learn dictionaries that compete with each other.",
                    "label": 0
                },
                {
                    "sent": "You can instead of learning the dictionary, you can actually impose that the coefficients used for class one are different than the other coefficients used by Class 2.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically this is.",
                    "label": 0
                },
                {
                    "sent": "This is just one metric that we use, but the basic idea is I want the Alpha vector for Class 1 to be orthogonal to the Alpha vector for Class 2.",
                    "label": 0
                },
                {
                    "sent": "OK, just as one example, you can put other things here.",
                    "label": 0
                },
                {
                    "sent": "Everything makes the optimization of this even more complicated.",
                    "label": 0
                },
                {
                    "sent": "I'm skipping that part, but the basic idea is that you can put the task.",
                    "label": 0
                },
                {
                    "sent": "In your optimization, the reconstruction won't be as good because the dictionary has to sacrifice OK, but their classification will be much better than if you want offline.",
                    "label": 0
                },
                {
                    "sent": "I can show you reconstructions with the pure reconstructive dictionaries or reconstructive plus classification.",
                    "label": 0
                },
                {
                    "sent": "They're not as good.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Let me just skip the results.",
                    "label": 0
                },
                {
                    "sent": "It works OK, but by 1.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just give you the concepts and you can combine both concepts of learning dictionaries that are separating and learning alphas that are separating.",
                    "label": 0
                },
                {
                    "sent": "And this is related to a paper that we had last year at NIPS with Julian and other colleagues.",
                    "label": 0
                },
                {
                    "sent": "And once again I think the important part is to see if you put the discrimination.",
                    "label": 0
                },
                {
                    "sent": "So this is for the for this data set for the digits data set, if you don't put the discrimination you get a 4% error the moment you tell.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be learning dictionaries for discrimination.",
                    "label": 0
                },
                {
                    "sent": "You go down to 1%.",
                    "label": 0
                },
                {
                    "sent": "OK, and it's more fair because I'm telling is unfair to use this because my optimization didn't include my task.",
                    "label": 0
                },
                {
                    "sent": "Now my optimization is including my task so it's a more fair request.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K. So that was the 2nd part.",
                    "label": 0
                },
                {
                    "sent": "We are going to learn.",
                    "label": 0
                },
                {
                    "sent": "To classify or to other task.",
                    "label": 0
                },
                {
                    "sent": "So the last part of the talk is to learn to sense and that's related to compress.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dancing.",
                    "label": 0
                },
                {
                    "sent": "So compressed sensing say the following, just in very rough words, saying the amount of sensing the amount of samples you need from a signal is proportional to the sparsity level of that signal.",
                    "label": 0
                },
                {
                    "sent": "OK, not to Nyquist.",
                    "label": 0
                },
                {
                    "sent": "But this proportional to the sparsity level of that signal, OK?",
                    "label": 0
                },
                {
                    "sent": "That means that the the more sparse your signal is, the less samples you need so that we have been addressing already.",
                    "label": 0
                },
                {
                    "sent": "Because I've been creating dictionaries to get signals very sparse.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have been doing a service to this already without doing it on purpose, but it came out.",
                    "label": 0
                },
                {
                    "sent": "So that's a request from the dictionary.",
                    "label": 0
                },
                {
                    "sent": "Now how are we going to sense part of the beauty of the theory is that most of the compressed sensing literature is about sensing with random matrices.",
                    "label": 0
                },
                {
                    "sent": "You get your image, you multiply by a random image by random matrix, and that's a way to sense.",
                    "label": 1
                },
                {
                    "sent": "There are a couple of reasons for random sampling.",
                    "label": 0
                },
                {
                    "sent": "One is universality.",
                    "label": 0
                },
                {
                    "sent": "It works for every for every signal, and the other is because the inverse problem is very stable.",
                    "label": 0
                },
                {
                    "sent": "But we like to learn things we don't like to take off the shelf dictionaries or off the shelf sensing matrices.",
                    "label": 0
                },
                {
                    "sent": "So the first question is, shall we adapt the senses to the data type, and every engineer will tell you yes, every engineer will tell you that a sensor for voice should not be the same as a sensor for images.",
                    "label": 0
                },
                {
                    "sent": "OK, that's why I'm using this as a microphone and the lady is using that to fill me.",
                    "label": 0
                },
                {
                    "sent": "OK, not the other way around.",
                    "label": 0
                },
                {
                    "sent": "OK, so this issue of universality is kind of mathematically very interesting, but in practice is very clear to everybody that we should be able to do better.",
                    "label": 0
                },
                {
                    "sent": "OK, and this has been addressed of optimizing the sensing.",
                    "label": 0
                },
                {
                    "sent": "Not taking random, this has been addressed by me, KDB, pay rate, Vice address, that an hour ago and his previous talk, and there are more so upper bounded, some very nice work about designing the sensing that is actually optimized computationally it does as good as as the random, but computationally is much more efficient, OK?",
                    "label": 0
                },
                {
                    "sent": "We are gonna try to optimize the sensing and the dictionary, both of them at the same time.",
                    "label": 0
                },
                {
                    "sent": "That's what we're going to try to do.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the formulation and I changed notation to be in agreement more with annotation using the compressed sensing, but I'm going to explain it to you.",
                    "label": 0
                },
                {
                    "sent": "X is all my data.",
                    "label": 0
                },
                {
                    "sent": "Vectorform OK.",
                    "label": 0
                },
                {
                    "sent": "This is now my dictionary.",
                    "label": 0
                },
                {
                    "sent": "This site is what was did.",
                    "label": 0
                },
                {
                    "sent": "OK. And these are my coefficients that I'm going to request to be sparse, so this is what I had before.",
                    "label": 0
                },
                {
                    "sent": "And this is what I had before these two terms.",
                    "label": 0
                },
                {
                    "sent": "Is that OK?",
                    "label": 0
                },
                {
                    "sent": "So I trying to represent my image all my images in a sparse fashion with elements of my dictionary.",
                    "label": 0
                },
                {
                    "sent": "But I have to make two more requirements.",
                    "label": 0
                },
                {
                    "sent": "One is I'm not going to see the image again in my life because I'm going to sense the image with the matrix multiplication, so this is what I'm going to actually see.",
                    "label": 0
                },
                {
                    "sent": "And it's going to be multiplied by this sensing matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, during the training I do have access to both, but during the reconstruction this term will disappear.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is my sensing matrix.",
                    "label": 0
                },
                {
                    "sent": "Forget about this for one second.",
                    "label": 0
                },
                {
                    "sent": "OK look only at this.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to optimize for my dictionary.",
                    "label": 0
                },
                {
                    "sent": "For my coefficients as before and for the sensing matrix all at the same time.",
                    "label": 0
                },
                {
                    "sent": "In kind of a coordinate descent type of approach.",
                    "label": 0
                },
                {
                    "sent": "OK, if I decide not to optimize for my sensing.",
                    "label": 0
                },
                {
                    "sent": "I'm in the previous case.",
                    "label": 0
                },
                {
                    "sent": "But and better that the previous case, I'm going to show you because I'm taking into account the sensing.",
                    "label": 0
                },
                {
                    "sent": "If for some reason you fixed the dictionary.",
                    "label": 0
                },
                {
                    "sent": "I'm still going to go and optimize the sensing, which is related to a large work and an your vice work.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to optimize for everything.",
                    "label": 0
                },
                {
                    "sent": "OK, there is one more condition that appears here which is related to the uncertainty problem.",
                    "label": 0
                },
                {
                    "sent": "Before he's at that condition comes from the theory of compressed sensing.",
                    "label": 1
                },
                {
                    "sent": "It says that these two guys cannot be arbitrary, they should be uncorrelated among themselves.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's pretty into it.",
                    "label": 0
                },
                {
                    "sent": "If you can not sense in the same directions of your dictionary, you're not getting that.",
                    "label": 0
                },
                {
                    "sent": "You're not getting nothing.",
                    "label": 1
                },
                {
                    "sent": "And that's expressed as they are IP restricted, isometric property and that's related to making the gram matrix corresponding to this product as close as possible to the identity matrix.",
                    "label": 0
                },
                {
                    "sent": "So that term also shows up here.",
                    "label": 0
                },
                {
                    "sent": "So we have two new terms.",
                    "label": 0
                },
                {
                    "sent": "And we're going to optimize for them.",
                    "label": 0
                },
                {
                    "sent": "K.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before I show you the pictures, this is what we get.",
                    "label": 0
                },
                {
                    "sent": "Forget about this.",
                    "label": 0
                },
                {
                    "sent": "This is just a.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameters this Alpha pyramid.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is classical compressed sensing mean squared error.",
                    "label": 0
                },
                {
                    "sent": "What do I mean by classical compressed sensing?",
                    "label": 0
                },
                {
                    "sent": "Do do a random matrix?",
                    "label": 0
                },
                {
                    "sent": "And one off the shelf dictionary that you somehow learn.",
                    "label": 0
                },
                {
                    "sent": "In this case, we learn a dictionary with K SVD.",
                    "label": 0
                },
                {
                    "sent": "This is where we are.",
                    "label": 0
                },
                {
                    "sent": "This is where we are.",
                    "label": 0
                },
                {
                    "sent": "If we optimize for the dictionary under sensing matrix.",
                    "label": 0
                },
                {
                    "sent": "About four times improvement in our mean square error, and these are intermediate cases.",
                    "label": 0
                },
                {
                    "sent": "For example, you fix the sensing matrix by optimize the dictionary.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Knowing that you're not going to see if we're going to see why so it's a different diction.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you end up in the middle or you fix the dictionary and you optimize for the sensing matrix and I forgot which one is the two because they are very similar.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "For the learning stage, I know it.",
                    "label": 0
                },
                {
                    "sent": "Why I simulated?",
                    "label": 0
                },
                {
                    "sent": "I multiply by fee?",
                    "label": 0
                },
                {
                    "sent": "Say that again, sorry.",
                    "label": 0
                },
                {
                    "sent": "Whi fi X function wise Phoenix wise equal Phoenix Now at the at this I know because I can simulate.",
                    "label": 0
                },
                {
                    "sent": "I know it's an I know what the sensing will see because I'm simulating the sensor in the reconstruction.",
                    "label": 1
                },
                {
                    "sent": "You don't know this.",
                    "label": 0
                },
                {
                    "sent": "So all the images I show you are after we have learned and you only do this with this.",
                    "label": 0
                },
                {
                    "sent": "That's a classical compressed sensing reconstruction.",
                    "label": 1
                },
                {
                    "sent": "OK, but in the learning I assume for this case that I know both.",
                    "label": 1
                },
                {
                    "sent": "There will be applications that people say.",
                    "label": 0
                },
                {
                    "sent": "Oh no, you never have access to the original image then then I won't have this, but I will have this.",
                    "label": 0
                },
                {
                    "sent": "I will still tell you to optimize for the dictionary.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry.",
                    "label": 0
                },
                {
                    "sent": "So either you're learning path you in their values of these places I can get up, but my question is that now that you even exist example he started dancing when she exacted.",
                    "label": 0
                },
                {
                    "sent": "Degenerative nature is all the guarantee that these learning examples in East Framework as a source vision because I'm learn from images.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to reconstruct images are not there.",
                    "label": 0
                },
                {
                    "sent": "My images are not in the database I use for learning, but it's not.",
                    "label": 0
                },
                {
                    "sent": "I'm going to learn images and reconstruct speech.",
                    "label": 0
                },
                {
                    "sent": "I'm going to learn images to reconstruct images.",
                    "label": 0
                },
                {
                    "sent": "That's that's part of the point.",
                    "label": 0
                },
                {
                    "sent": "The point here is University is fine, but I'm trying to design a camera.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's use images to design my camera.",
                    "label": 0
                },
                {
                    "sent": "Now if tomorrow you wanna use that camera for speech, classical compressed sensing will do much better and we will do.",
                    "label": 0
                },
                {
                    "sent": "Random matrices will do much better and we will do, but they will do much worse than if you told me in advance that you were going to use speech OK.",
                    "label": 0
                },
                {
                    "sent": "If you are cheating me then.",
                    "label": 0
                },
                {
                    "sent": "I follow you OK.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just believe the images.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to show you only these two extremes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great classical compressed sensing.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are sensing.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Twice the sparsity level, which is about what the theory tells you to do.",
                    "label": 0
                },
                {
                    "sent": "Our result with optimized dictionary, an optimized sensing matrix.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Israel is not just numbers, so if it was like a .1 DB we might not see it or half .0.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One TV, so we actually see it.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is another example.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm here is another example.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let me conclude.",
                    "label": 0
                },
                {
                    "sent": "First, I show you recites for chest denoising Ann.",
                    "label": 0
                },
                {
                    "sent": "I skip that.",
                    "label": 0
                },
                {
                    "sent": "But together with a paper by VCA.",
                    "label": 0
                },
                {
                    "sent": "That uses self similarity used to be the best results.",
                    "label": 0
                },
                {
                    "sent": "Now that we combine self similarity with sparsity we get better results than either one of two is pretty general in the sense that we can do multiscale vectorial data.",
                    "label": 0
                },
                {
                    "sent": "We can learn addictions with internal structure and that's the goal of that poster.",
                    "label": 0
                },
                {
                    "sent": "At 5:30, one of the two goals.",
                    "label": 0
                },
                {
                    "sent": "And we can learn the dictionary for classification or for other goals that you might have.",
                    "label": 0
                },
                {
                    "sent": "And I want to refer you to.",
                    "label": 0
                },
                {
                    "sent": "So Arthur Slam has a very nice paper at ICM L in two weeks related to this.",
                    "label": 0
                },
                {
                    "sent": "Also about learning.",
                    "label": 0
                },
                {
                    "sent": "Basically there is an extension of K means which is called Q flats that it's tough learning centers.",
                    "label": 0
                },
                {
                    "sent": "You learn hyperplanes and you can extend that to do classification.",
                    "label": 0
                },
                {
                    "sent": "And I also strongly recommend you to look at papers by.",
                    "label": 0
                },
                {
                    "sent": "Korean by Larry Correia and his group at ICL.",
                    "label": 0
                },
                {
                    "sent": "I'm not a coauthor, so I'm free to advertise that work.",
                    "label": 0
                },
                {
                    "sent": "Very nice work also using completely different techniques that are now being merged with these techniques.",
                    "label": 0
                },
                {
                    "sent": "But if you're going to ICL, I strongly recommend you to look at those papers.",
                    "label": 0
                },
                {
                    "sent": "We can learn the dictionary for adapting to your sensing device.",
                    "label": 0
                },
                {
                    "sent": "I have a red coming here that says that a lot still needs to be done.",
                    "label": 0
                },
                {
                    "sent": "This is just a warm up.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the day home messages please do not use the wrong dictionaries.",
                    "label": 0
                },
                {
                    "sent": "OK, and please be a bit critical when people report results with the wrong dictionaries or with dictionaries that are not the best that you know, so you will see a lot of very good papers in the literature that report results with random dictionaries is very nice to do the math, but the results are not going to be as good if you optimize and this I mentioned before is another word by Julian at ICML where we do online learning.",
                    "label": 0
                },
                {
                    "sent": "We don't wait for the data to come, we just do it.",
                    "label": 0
                },
                {
                    "sent": "And this is an example where we are actually learning 7 million patches in about 8 minutes.",
                    "label": 0
                },
                {
                    "sent": "That's a learning.",
                    "label": 0
                },
                {
                    "sent": "Today is a very good programmer, so that's part of the 8 minutes but but it's done online, so it's Lars that was mentioned before for this party.",
                    "label": 0
                },
                {
                    "sent": "Calling an online learning for the dictionary.",
                    "label": 1
                },
                {
                    "sent": "OK, and you get very good results very fast and without a lot of memory consumption because it's done.",
                    "label": 1
                },
                {
                    "sent": "Air.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Offline and this is my last message, sparsity should be learned and I'll take him for granted.",
                    "label": 0
                },
                {
                    "sent": "Because then the question is real, how do I know my signal is gonna be sparse on this basis?",
                    "label": 0
                },
                {
                    "sent": "I'm not hoping for that and trying to enforce that sparsity should have a goal.",
                    "label": 0
                },
                {
                    "sent": "I'm going to learn to do something so you should not learn to do A and expect it to do be OK, cars gone, roads, planes go on there because they were designed for that.",
                    "label": 0
                },
                {
                    "sent": "So if we are going to design A dictionary for denoising, we shouldn't be expecting that dictionary to be good for classification.",
                    "label": 0
                },
                {
                    "sent": "Of course, if we had infinite amounts of data and the problem were convex, yes.",
                    "label": 0
                },
                {
                    "sent": "But we don't have infinite amount.",
                    "label": 0
                },
                {
                    "sent": "And the problem is non convex so why not to help the optimization by hinting to the optimization?",
                    "label": 0
                },
                {
                    "sent": "What will I be doing with that dictionary White to hide that from the optimization?",
                    "label": 0
                },
                {
                    "sent": "Let's just put it there and that's why we should learn to do a by dictionary.",
                    "label": 0
                },
                {
                    "sent": "And I this beeps that I'm on time and I apologize for the red, but I hope that we could use our imagination and once again thanks for trying to accommodate to that last minute technical problem.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "We have time.",
                    "label": 0
                },
                {
                    "sent": "You're right, I didn't tell you is I entertain questions during or after.",
                    "label": 0
                },
                {
                    "sent": "No, that's go ahead.",
                    "label": 0
                },
                {
                    "sent": "Steep learning, so fast matching is the matching atoms under a known learn and then using the hyper I will refer you to a paper by Stephen Assault on that.",
                    "label": 0
                },
                {
                    "sent": "A recent paper on that they have tried.",
                    "label": 0
                },
                {
                    "sent": "I will refer you to a paper by them and to paper by us a year from now, hopefully, but but you should look at Steven also auto, under and rubber to see have a paper on that on their website on deblurring by learning dictionaries.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Test.",
                    "label": 0
                },
                {
                    "sent": "Close said that images are used to make the dictionary price metric closes when you get outside of that.",
                    "label": 0
                },
                {
                    "sent": "Close means we learn a dictionary with the Berkeley database, and then we went with the Kodak camera and took images.",
                    "label": 0
                },
                {
                    "sent": "And within I step.",
                    "label": 0
                },
                {
                    "sent": "So that's how close.",
                    "label": 0
                },
                {
                    "sent": "I hope them to be.",
                    "label": 0
                },
                {
                    "sent": "So our natural images if you want to work on ultrasound, I will use an ultrasound.",
                    "label": 0
                },
                {
                    "sent": "I don't think this will be good to do Ultra Sun and learn from natural images, but we don't have a qualitative measure and with that I'm going to give you another answer that maybe not what you wanted but we don't have a qualitative measure besides using the optimization itself to compare dictionaries.",
                    "label": 0
                },
                {
                    "sent": "If I just give you a Dictionary of the street with another, I said what's the distance between them?",
                    "label": 0
                },
                {
                    "sent": "One learn for this type of images, one for the other.",
                    "label": 0
                },
                {
                    "sent": "I know how to measure the distance through my optimization.",
                    "label": 0
                },
                {
                    "sent": "OK, but we don't measure closeness.",
                    "label": 0
                },
                {
                    "sent": "We just use that they're all natural images.",
                    "label": 0
                },
                {
                    "sent": "Be careful, this dictionary is 256 atoms to represent all the images that are in Flickr.",
                    "label": 0
                },
                {
                    "sent": "OK, the reason we can do that is because we work on patches.",
                    "label": 0
                },
                {
                    "sent": "OK, we couldn't do that if we were working on the full image computation, it will be impossible, but also you can represent all Flickr, which is, which is a few items.",
                    "label": 0
                },
                {
                    "sent": "But by the way, JPEG works.",
                    "label": 0
                },
                {
                    "sent": "OK, JPEG works represents every single image in the war with 64 coefficients.",
                    "label": 0
                },
                {
                    "sent": "Bear bear Patch.",
                    "label": 0
                },
                {
                    "sent": "OK, that's pretty good, and that's why JPEG is a great technique.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of sort of right, even back in the fray or back.",
                    "label": 0
                },
                {
                    "sent": "It's tough.",
                    "label": 0
                },
                {
                    "sent": "It's like, wait, let's talk about why that's not happening.",
                    "label": 0
                },
                {
                    "sent": "Here it is.",
                    "label": 0
                },
                {
                    "sent": "K There are issues of scale and there are issues of overlapping patches.",
                    "label": 0
                },
                {
                    "sent": "Those two and this is kind of a level of intuition here is here, but we had a long discussions with you hear about that, but if you go back to the authors of the original papers that show geboren wavelets, they will give you the same reason.",
                    "label": 0
                },
                {
                    "sent": "That means if you go for example to the original paper by Bruno, there was no overlapping.",
                    "label": 0
                },
                {
                    "sent": "On Dodge paper.",
                    "label": 0
                },
                {
                    "sent": "OK, and there was kind of a filtering.",
                    "label": 0
                },
                {
                    "sent": "The paper is a super paper, you just the framework was slightly different OK?",
                    "label": 0
                },
                {
                    "sent": "Maybe your mom?",
                    "label": 0
                },
                {
                    "sent": "I was looking for adapting it.",
                    "label": 0
                },
                {
                    "sent": "Better.",
                    "label": 0
                },
                {
                    "sent": "No, I'm not.",
                    "label": 0
                },
                {
                    "sent": "My sparse answer is no.",
                    "label": 0
                },
                {
                    "sent": "Sparsity people claim that it's very important for the visual system, but I will let the experts to try to infer more on that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Classification performance compared to techniques like convolutional neural Nets, degross.",
                    "label": 0
                },
                {
                    "sent": "Everybody compared the grass I show briefly with the graph is the best results with the.",
                    "label": 0
                },
                {
                    "sent": "With this I went very fast over that, but is.",
                    "label": 0
                },
                {
                    "sent": "So for the grass data.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here we are.",
                    "label": 0
                },
                {
                    "sent": "So slightly better than what was published in the literature.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This weird state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This we are at.",
                    "label": 0
                },
                {
                    "sent": "One sorry, 1.5.",
                    "label": 0
                },
                {
                    "sent": "As you know there is less than that, but with a lot of years and work and this is, this is part of the universality.",
                    "label": 0
                },
                {
                    "sent": "This was not design.",
                    "label": 0
                },
                {
                    "sent": "The technique was not designed for this data set.",
                    "label": 0
                },
                {
                    "sent": "The data set was used to learn the same parameters.",
                    "label": 0
                },
                {
                    "sent": "The same techniques are used for every classification algorithm that we are using, so there's kind of so I think this is a pretty good result.",
                    "label": 0
                },
                {
                    "sent": "Most people in the community say that if you're below 1.5 to that data set without putting a lot of tuning of parameters, then and this is kind of a fair shot type of results.",
                    "label": 0
                },
                {
                    "sent": "So for some datasets are compatible for none of the datasets that we have tried.",
                    "label": 0
                },
                {
                    "sent": "It makes a disaster OK, but I don't think is expected to make a disaster because I'm optimizing for my goal, so so it shouldn't be a disaster.",
                    "label": 0
                },
                {
                    "sent": "And for some of them is state of the art.",
                    "label": 0
                },
                {
                    "sent": "We haven't been working on tuning parameters to get to the .6 that I think is the best published.",
                    "label": 0
                },
                {
                    "sent": "We just wanted to present the framework when we got this result we were happy with it.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Not just angle speaker one, thank you.",
                    "label": 0
                }
            ]
        }
    }
}