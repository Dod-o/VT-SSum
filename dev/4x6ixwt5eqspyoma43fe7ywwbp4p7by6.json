{
    "id": "4x6ixwt5eqspyoma43fe7ywwbp4p7by6",
    "title": "Laplacian Matrices of Graphs: Algorithms and Applications",
    "info": {
        "author": [
            "Daniel A. Spielman, Department of Computer Science, Yale University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_spielman_laplacian_matrices/",
    "segmentation": [
        [
            "So first I'm going to hold this mic because they're recording, but I'm going to try not to use it because I tend to be loud.",
            "If for any reason I fail to be loud and someone in the back can't hear me or just do this and an I'll notice.",
            "OK, so today I want to tell you a lot about Laplacian matrices of graphs and algorithms for solving systems of linear equations in them."
        ],
        [
            "So I'm going to try to do a few different things, so of course I will tell you what laplacian's are in a little about them.",
            "That will come surprisingly late in the talk though.",
            "1st I'm going to try to tell you what we use them for, so I'll briefly mention problems of interpolation on graphs.",
            "Computing eigenvalues.",
            "OK, that will be in Laplacian so around.",
            "Then I gotta tell you what they really are, where they come up in optimization.",
            "When you're solving linear programming problems on graphs.",
            "I'll then talk about one of my favorite tools for studying these, which is sparsification, and then I'll tell you how we're going to use it.",
            "So in particular I will show you 2 pretty new algorithms.",
            "Well, OK, one.",
            "The paper got an archive last week so they can call very new algorithms for solving systems of linear equations and laplacian's that just use sparsification as the tool to drive the algorithms."
        ],
        [
            "OK, so first the problem of interpolation on graph.",
            "So this is what we call it.",
            "Some people call it learning on graphs or inference on graphs coming from this paper of Drew Ghahramani and Lafferty.",
            "So they suggest this problem where you have a graph an you know the values of a function at some nodes like here I know it's zero there.",
            "I know it's one.",
            "And you would like to guess the values at the other nodes of the graph.",
            "I think of this as a problem of interpolation.",
            "You're really trying to fill in the other values under the assumption that nodes that are connected by edges are similar.",
            "So sort of like there's some almost continuous space there, and the way they suggest doing that is they try to find a function that doesn't vary too much over the edges, so they try to minimize the sum over all edges.",
            "Is this going to work?",
            "Yes, the sum over all edges of the square of the differences of the function across the edges.",
            "This is what they suggest.",
            "Minimizing subject to the given values.",
            "So this sum is something that's going to appear a lot in this talk, and this will give us our first definition of the Laplace."
        ],
        [
            "Lawson.",
            "The Laplacian of this graph is the matrix so that when you take the vector X, so X should be indexed by vertices of the graph.",
            "The rows and columns of El are indexed by vertices of graph.",
            "If you take X transpose LX, you get exactly this quadratic form here.",
            "So that is one way of defining the Laplacian for now, and so in case you wanted."
        ],
        [
            "Here is the solution to this system.",
            "And how would you actually compute this where you're trying to minimize this quadratic subject to some given conditions.",
            "Basically you take a derivative an when you take the derivative of the quadratic, you get something linear and you discover what you need to do is solve a system of linear equations in the Laplacian matrix, or properly speaking, in this case, it's in a submatrix of the Laplacian, but it turns out to be an essentially equivalent computational problem."
        ],
        [
            "OK, so the thing that will appear a most in this talk is the Laplacian quadratic form.",
            "So just remind you given a graph, it's the sum over edges.",
            "Of the OK look quadratic form takes a function X from vertices to the reals.",
            "You take the sum of the squares of the differences of that function across edges.",
            "That is, the Laplacian quadratic form in the graph.",
            "And I remind you again that the Matrix is the matrix.",
            "The symmetric matrix that when you put here, you get that quadratic form again.",
            "I'll tell you exactly what the entries in that matrix are in a little bit.",
            "Before we do that, let's make some observations about the spectrum of the Laplacian matrix.",
            "So.",
            "The first thing we want to observe is that it is a positive semidefinite matrix for people who are used to thinking about positive semidefinite matrices.",
            "This just follows from the fact that that quadratic form is never negative.",
            "If you're not used to thinking about these things well, think about any eigenvector of the matrix that you put here.",
            "V If the Laplacian is equal to Lambda V well multiplied by V transpose on the other side.",
            "Now you're going to get Lambda V transpose, VV transpose.",
            "V has got to be positive.",
            "And because we know this is always possible, that means lambdas positive.",
            "OK, so if you're not used to thinking about the variational characterization of eigenvalues, here's another proof.",
            "All the eigenvalues, Laplacian or at least non negative.",
            "I should be careful that does have an eigenvalue of 0.",
            "If you put in the all ones vector, you're going to get 0 here.",
            "Or any constant vector, you'll get 0.",
            "And if the graph is connected, it's pretty easy to see that that's the only eigenvalue of only eigenvector wagon value 0.",
            "So if the graph is connected the eigenvalue zeros multiplicity one.",
            "So I will let Lambda one the smallest eigenvalue be 0 and the other eigenvalues will be larger.",
            "OK, just one warning.",
            "During this talk I will sometimes write or talk about the inverse of the Laplacian.",
            "Why am I doing that?",
            "Because I want to talk about solving systems of linear equations, but I just told you that the matrix is singular, so you know it has no inverse.",
            "I really mean is the pseudo inverse that is the inverse orthogonal to the null space and since we know you can just take a look at a graph and check if it's connected and if it is, you know the nullspace is the constant vectors and you can easily just work orthogonal to that.",
            "So if you catch me saying the inverse, don't worry, I just mean restrict your attention to the space orthogonal to null space and there you have an inverse.",
            "Also, this is usually called the pseudoinverse.",
            "OK, so why are?"
        ],
        [
            "Be interested in eigenvalues of Laplacian matrices.",
            "It's not our priority obvious, but once you know about it, you start to see that there's this whole field called spectral graph theory.",
            "That's all about what do eigenvalues and eigenvectors of matrices like the Laplacian tell you about a graph?",
            "So I teach an entire course on this.",
            "I'll be doing it in the fall, but for those who haven't seen it, let me give you just one motivation which comes from the problem trying to find clusters in graphs or partitioning graphs.",
            "So.",
            "Usually when you're trying to find a cluster of nodes in a graph, you're trying to find a set of nodes, like the set S here, which has a relatively small boundary where by the boundary I mean just the edges."
        ],
        [
            "Leaving the set.",
            "So the Laplacian quadratic form allows us to measure the boundary of a set of nodes.",
            "The way you do that is you take the vector X, that's the."
        ],
        [
            "Characteristic."
        ],
        [
            "After that set of nodes, so one inside and zero outside an.",
            "If you then apply the Laplacian quadratic form to that, you will get the sum of the squares of differences across all edges, which will just pick up one for each edge in the boundary and will be 0 elsewhere.",
            "So when you apply the Laplacian quadratic form to the characteristic vector of a set, you discover the size of the boundary.",
            "Now often if you are given a graph, let's say you're trying to identify communities or something like that, you want to find the set S for which this is relatively small.",
            "Well.",
            "Spectral methods give you a heuristic for doing this.",
            "What you do is you find you want to find a vector X for which the Laplacian quadratic form is small.",
            "You do it by, say, finding the second eigenvector.",
            "The first eigenvector is just constant.",
            "That's useless.",
            "Find the second eigenvector and round it to a 01 vector.",
            "An empirically this is a good way to start finding clusters in graphs or start partitioning them, and we heard a talk or two about this earlier.",
            "You can do much fancier things."
        ],
        [
            "But this is just."
        ],
        [
            "OK, so how are we going to compute an eigenvector of the Laplacian?",
            "So let's start by looking at the power method.",
            "Can I get a show of hands for how many of you have seen the power method?",
            "Hallelujah, OK, so I'll just remind you.",
            "What the power method does in the analysis.",
            "OK, so remember you just take of like a random vector X, ideally a Gaussian random vector X.",
            "You multiply it by the Laplacian, you just want to keep multiplying it by the Laplacian, but you know that if you just keep doing that, you get something that tends to go to Infinity or zero way too quickly.",
            "You get numerical errors, so you re scale at every step.",
            "You tend to divide by the norm, and this converges very quickly to an eigenvector of the largest eigenvalue of the Laplacian.",
            "And I will remind you of the analysis in a moment.",
            "But we want the eigenvalue of this one eigenvector of the smallest eigenvalue to do things like partitioning or graph drawing those use small eigenvalues.",
            "But if you're trying to do graph coloring, then you want the large eigenvalues.",
            "But partitioning you want the small ones.",
            "OK, well you say we have a way of dealing with that.",
            "Let's just flip the spectrum around."
        ],
        [
            "I have some upper bound on the spec."
        ],
        [
            "Let's call it T if all the eigenvalues are less than T, by the way, twice the maximum degree in a graph would suffice.",
            "Then I can just take T times the identity minus the Laplacian.",
            "Now if I look at that matrix so it was the smallest eigenvalue before, now becomes the largest eigenvalue.",
            "And you can imagine applying the Laplace the power method on this.",
            "And you know that it will eventually approach the eigenvalue eigenvector largest eigenvalue of this matrix and that gets us the smallest eigenvalue of the original.",
            "But I put a question Mark next to the quickly because the convergence rate of this.",
            "Can be really unsatisfying.",
            "And to remind you why."
        ],
        [
            "Just reminder of the analysis of the power method.",
            "So since L is a symmetric matrix, its eigenvectors form an orthonormal basis, so I can take my initial random vector X and let's expand it in that orthonormal basis and if it were a Gaussian random vector, then the coefficients of those vectors are IID.",
            "So there now we have.",
            "So we get a bunch of coefficients that are basically independent random times eigenvectors.",
            "Let's see what happens when we keep multiplying by the Laplacian.",
            "Well, every time you multiply by the Laplacian, the coefficient of the eigenvector gets multiplied by Lambda I.",
            "So after K iterations we get Lambda I to the K here, which means it really only the biggest ones are going to count.",
            "And you can make that precise if you want to basically wipe out the contribution of all eigenvectors whose eigenvalues are smaller than one minus epsilon times the largest.",
            "You basically need Logn over epsilon iterations and they're gone.",
            "So that's very fast, and what's nice is this convergence rate actually doesn't depend on Lambda N, it's just something absolute."
        ],
        [
            "Give this breaks down.",
            "However, for what I just showed you for how we get the smallest eigenvalues.",
            "Because if you do the exact same analysis with this other matrix, suddenly we don't have Lambda eyes.",
            "And here we have this T minus Lambda I and there's this problem that the smallest eigenvalues tend to be small.",
            "And if I have T, even if it's more than one, you know 1 minus something small isn't very different from 1 minus twice something small.",
            "Or here's another way of putting that.",
            "If I just want to make all of the eigenvalues less than twice the smallest disappear.",
            "Which I certainly want.",
            "We need many iterations.",
            "The number of iterations is not something like log in, and there's no epsilon here.",
            "It depends on one over Lambda 2.",
            "So the smaller Lambda two is, the longer it takes to get rid of like the smallest.",
            "Eigen varies like this.",
            "OK if you're using Matlab and you type my guys to get it the smallest eigenvalues.",
            "This is basically what it's doing, but it's doing land shows which is a little faster.",
            "That's one of the reasons for a lot of natural matrices.",
            "You get the largest eigenvalues much faster than you get the smallest.",
            "So what do we do?",
            "You know this convergent trade I don't like well."
        ],
        [
            "Is one reason we want to solve systems of linear equations in Laplacian?",
            "It's so much better if when we use the power method we apply it to the inverse of the Laplacian?",
            "That is it every single iteration.",
            "Let's solve a system of linear equations in AX.",
            "Then what happens?",
            "Well then, after K iterations we have multiples of Lambda I to the minus K. And now our analysis is back just like it was for finding the largest eigenvalues.",
            "If you want to get rid of all the eigenvalues that are more than epsilon times the second smallest that happens after Logn over epsilon iterations.",
            "The only problem is the operation that we are doing at every single step is more expensive.",
            "Before I said you do the power method to multiply by matrix, here we have to solve a system of linear equations in the matrix.",
            "On the other hand, it turns out we can solve."
        ],
        [
            "Those systems of linear equations very quickly, and some of you have seen me give talks about how to do this.",
            "I promised today will be a talk about a completely different way of doing it.",
            "OK, so but I may as well remind you about the results, so we got nearly linear time algorithms for this about a decade ago.",
            "Shanghai Tang and I did this.",
            "We got algorithms which should get us an epsilon.",
            "Approximate solution of these linear equations, but others see is some very large constant that was on the log in initially about 100 or really big breakthrough was when Kudis, Miller and Pang got this algorithm down to time.",
            "Basically order EM Logn log one over epsilon and what I want to point out about these two results as they use.",
            "Two techniques.",
            "These two graph theory primitives, low stretch spanning trees, and sparse ifers.",
            "I'll talk about sparse ifers later.",
            "The Curtis Miller and Pang improve their results by really emphasizing low start spanning trees and using very showing that instead of sparsification, they could basically is very crude random sampling and that got us a great result.",
            "And I got today.",
            "At that point I thought we were done.",
            "'cause you know, M login is incredible.",
            "But then Cohen King Pachucki Pangen Row actually got this down to M root login.",
            "Which is pretty amazing.",
            "'cause if you think about it, that's less time than it would take you to sort the non zero entries in the matrix.",
            "Although I gave a talk on mention that at some point someone yes but actually to solve linear equations you don't need full accuracy on the entries so.",
            "OK, it's a little bit of a cheat.",
            "But that's an absurdly fast algorithm.",
            "Now I see."
        ],
        [
            "To tell you, for those who haven't seen me say before, what do I mean by an approximate solution to a system of linear equations?",
            "So what we should do to think about this is if I'm trying to solve LG X = B, then the solution is say alright is the inverse of the Laplacian, Times B we want a vector X that's close to that, so you want X to be close to the true solution in Norm.",
            "How close do you want it to be for an epsilon approximate solution it should be at most epsilon times the true solution.",
            "And some of you are wondering why do I have this funny norm here, so I'm not using the standard Euclidean norm.",
            "I'm measuring the error in the matrix norm.",
            "Which is just written this way and I'm not doing this to be complicated.",
            "It turns out that for most purposes this is the norm in which you want to measure the error.",
            "Meaning if you go back and look at our analysis of the inverse power method, you should measure the error in this norm.",
            "If you want the right analysis of the behavior.",
            "Later I'm going to show you what happens if you plug these approximate solvers into things like interior point methods.",
            "Again, you don't get a decent analysis.",
            "In less, you measure the error in this norm.",
            "This is the right norm in which to measure the error.",
            "It's also very conveniently the norm in which our algorithms give the right error.",
            "So I mean, actually, most of the algorithms you see in numerical linear algebra naturally give you error that should be measured in this norm.",
            "This is how we'll do it.",
            "But the key point if you're not used to think about this is just an approximate solution is a vector X that is sort of epsilon close to the right solution.",
            "And I just mentioned if you actually want to do this, there are two.",
            "Piece of code that are pretty good out there right now.",
            "Giannis Koutras has some nice code called the Combinatorial Multigrid.",
            "There's also a lean algebraic multigrid.",
            "Neither of these are algorithms.",
            "Anyone is really proved theorems about, but they both work pretty well, and at least with solving systems of linear equations, you can check if you get the right answer.",
            "So in the end, we don't worry as much about whether or not we can prove the algorithm is correct, and less like there is some really time critical thing that's going to blow up if you don't give the answer in the right amount of time and.",
            "To the best of my knowledge, that is not the case for the policy and solvers yet."
        ],
        [
            "OK, so given that these algorithms are so fast, you might wonder why do I care about having others?",
            "Well, there are few reasons.",
            "One, we're interested in finding other algorithms.",
            "Because there are other systems of linear equations we might want to solve other than laplacians and sort of the more algorithms that you know to handle.",
            "One thing it's more likely that you can handle something else, so I was very excited when counter or reckless Infernet L'enjeu came up with a very very clean combinatorial algorithm for solving systems of linear equations in laplacian's that took time order, M log, square, dance that was even slower than Curtis Miller.",
            "Pang, but.",
            "It really is.",
            "The algorithm is very clean, it just uses low stretch spanning trees.",
            "And some data structures.",
            "And it's it's a very paper.",
            "Sort of long, but the meat of it's contained in about 5 pages, and it's a very, very clean, simple algorithm.",
            "I think a lot of people have studied to try to understand what we can do better and how we might solve other systems.",
            "On the other hand, I'm going to take a completely different route.",
            "Today.",
            "I'm going to not talk about low stretch spanning trees.",
            "Note I won't even define them and show you just how to solve systems of linear equations using graphs.",
            "Parsa fires, which I'll tell you about and.",
            "There are two advantages of these.",
            "OK, so Richard Pang and I developed these algorithms and now with the intently is our most recent paper because we were trying to come up with algorithms we could execute in parallel or on distributed systems because, well, one of the main places people solve these systems in scientific computing where for them billions of equations is really not impressive.",
            "That's their small cases.",
            "So you know.",
            "So I can't on a single processor solve the problems they care bout.",
            "We need something we can do in parallel.",
            "It turned out this will also give us at least once you accept sparsa fires.",
            "Incredibly simple ways of understanding these nearly linear time algorithms an will also get us eventually.",
            "The fastest algorithms that we know after some precomputation.",
            "So just the same way with Gaussian elimination you compute the inverse of a matrix, and after that it's trivial to solve a system of linear equations.",
            "By the inverse, it depends how many non zeros are in the inverse.",
            "What will end up proving here is we have something that's essentially an approximate of the inverse with only a linear number of nonzeros.",
            "So it will actually get us flat out linear time solvers after the precomputation.",
            "And the pre computation will be nearly linear too."
        ],
        [
            "OK, that's where we're going eventually.",
            "Time for me to tell you about Laplacian matrices.",
            "Finally tell you what the entries are, so we'll just get them by thinking about this Laplacian quadratic form.",
            "So what we have in the Laplacian quadratic form is a sum over edges of the square of a difference across an edge.",
            "OK, so if I want to write XI minus XJ and matrix notation.",
            "I'll make a vector out of XI and XJ and multiply it by vector.",
            "That's one and minus one.",
            "OK, now if I want to take the square, you could just put."
        ],
        [
            "Parentheses around that and take the square, but let's not do that.",
            "We multiply this by its transpose, so write the row vector with X on the left.",
            "Put the 1 -- 1 in a column, multiply it by the row input XIXJ there.",
            "And if you then multiply these two together, this is the direction that it gives you a matrix, not a real number.",
            "And this is the matrix that you get.",
            "OK, so if I had a graph where it only had one edge between vertex I and vertex J, it would basically look like this little two by two matrices would look like this little two by two matrix, but you need to put in a bunch of zeros for all the other vertices.",
            "So shift I and J, you know, shift the one and minus one to positions inj.",
            "You can build up a Laplacian by adding one such matrix for every edge.",
            "You just have to pad with zeros appropriately.",
            "So for example here."
        ],
        [
            "Is a graph I'll use in many examples, and here is the Laplacian matrix corresponding to it and the edge between vertex two and six here corresponds to this circled minus one here.",
            "Between that's column 6 and row two to represent vertex two and six.",
            "So again, This is why I usually deal with the quadratic form more than the matrix.",
            "The Matrix is a little less informative to me.",
            "But the main things to observe here is the matrix is symmetric.",
            "The off diagonals are non positive and it is something we call diagonally dominant.",
            "I will mention once in awhile that means the diagonal entries are at least as big as the sum of the absolute values of the entries in the row and column.",
            "So if you go across any row, said 3 minus ones here, the diagonal is 3 that compensates.",
            "We can also handle and it will sometimes come up matrices like this where the diagonals are strictly bigger, so as long as the diagonals are strictly bigger than the sum of the entries in the row and columns that we call symmetric, diagonally dominant and solving those is systems and those equivalent to solving systems in laplacians."
        ],
        [
            "OK so I need another way of writing the Laplacian for one of the results I'm going to present so one way again, I said you can think about the Laplacian.",
            "In this case, talk about a weighted graph.",
            "My weights will always be non negative.",
            "I will not have positive weights there.",
            "WIJ, because I want my matrices deposit be positive semidefinite.",
            "In this case, I can take the Laplacian and I write it as a sum of elementary laplacian's, one for each edge again.",
            "So for me, LIJ is just the matrix that gets me NJJ times await.",
            "And I'm going to use the important fact that Elijay can be written as an outer product of two vectors in which I drew earlier.",
            "I'll call it BIJ for me BIJ is the vector corresponding to edge IJ, and it has zeros everywhere.",
            "But as a one in the coordinate corresponding to Vertex I Anna minus one in the coordinate corresponding to vertex J.",
            "So you could write it as EI minus CJ, where those are the elementary unit factors.",
            "For me it will be important that I can write the Laplacian is the sum of outer products of these."
        ],
        [
            "That eventually gives us actually this form for the applausi, and that we often right.",
            "In the middle here I have this.",
            "I've written it out as BWB transpose so W should be a diagonal matrix with the edge weights.",
            "B is what we call the signed edge vertex adjacency matrix.",
            "So the rows.",
            "If I'm doing this right.",
            "Oh darn, did I do it the wrong way?",
            "The columns.",
            "Oh OK, I wrote rose.",
            "I might have written the transpose on the wrong side.",
            "I hope you'll excuse me.",
            "It.",
            "If I did it by rows and yet every single row should have an entry for each edge.",
            "But I might have done columns.",
            "I'm sorry I can.",
            "I can't even check in my head whether I got the rows or columns edit actually have to draw it and do the outer product.",
            "Maybe someone will tell me which it should have been OK, but the key point is those B vectors appear in this matrix B.",
            "You'll see in a moment why I want this.",
            "One thing that will seem very odd to you perhaps is that I had a graph and when I formed this signed edge vertex adjacency matrix.",
            "You know I had to assign one end of an edge, one endpoint of an edge of minus one and one endpoint to one.",
            "And it seems like an arbitrary choice.",
            "And the reason I didn't care on the previous slide."
        ],
        [
            "This is always multiplying by the transpose, so assign didn't matter.",
            "On the other hand, in a moment we're going to look at some results for directed graphs, and in that case I guess it's just useful to remember that you can put directions in by making the right choice of signs.",
            "OK."
        ],
        [
            "So now I want to tell you about one more applications of laplacian's before I start telling you about algorithms for solving linear equations in them.",
            "So it turns out.",
            "That if you're trying to solve linear programming problems on graphs.",
            "And then you're trying to solve the linear programming problem by an interior point method.",
            "The work the interior point methods are doing are usually just solving systems of linear equations in Laplacian matrices of graphs.",
            "Or you can reduce it to that.",
            "It's not always immediately obvious.",
            "So Sam Daitch and I wrote a paper gosh, more than a couple years back about doing this for maximum flow problems and minimum cost flow problems.",
            "Ann, I didn't realize it was a really general phenomenon until recently, so Sushant Sachdeva will give you give a talk on Monday in which you will briefly mentioned where we use this for doing regularization in something that we call Lipschitz learning.",
            "But what I would like to explain to you is the cleanest example of this that I know by Rasmus King.",
            "Anoop Rowan Sushan such Davidson paper that just appeared on archive.",
            "I trust a night or two ago now for the problem called isotonic regression.",
            "OK, now I didn't know about isotonic regression until these guys started working on it, but can I get no answer?",
            "How many of you encountered isotonic regression before?",
            "OK, a fair number?",
            "OK, that's good, but for the rest I will give an explanation.",
            "OK, it comes up in it."
        ],
        [
            "Couple of situations.",
            "Here's one that I like.",
            "1st to define it I should tell you that it has a directed graph.",
            "It should be a directed acyclic graph defines the problem.",
            "And I show you that a function we say on the vertices is isotonic with respect to the directed acyclic graph.",
            "If the function is increasing on edges.",
            "So when you follow a directed edge, the function only increases, so actually quite accidentally at first the labels, the numbers that I chose for these vertices are an isotonic function with respect to this directed acyclic graph.",
            "However, if I take that vertex that I labeled three, and I say now label it."
        ],
        [
            "Seven and I have a function that's not isotonic with respect to the graph, because those two edges are now decreasing.",
            "They should have been increasing.",
            "OK, so the problem in isotonic regression is I give you some function that's not necessarily isotonic with respect to the graph, and you want to find the closest isotonic function.",
            "In some norm, we'll worry about the norm later different."
        ],
        [
            "Forms give you different problems.",
            "But again, the idea is I give you some function on the vertices of the graph.",
            "It is not isotonic, you want to find the closest function that is OK."
        ],
        [
            "So why might you want to do that?",
            "Here's an example that I like.",
            "There are many others.",
            "I'm sure many of you will know.",
            "Imagine you're trying to reconstruct some events, maybe for some fun crime TV show and you have some estimates of among when many of them have happened, but you know your estimates are noisy.",
            "But you also know that certain things had to happen before others.",
            "So if you know that certain things had to happen for others, those give you precedence constraints which gives you directed edges.",
            "So think of every node is one of these events.",
            "It's labeled with an estimate of when it happened and given the labels of when you think they happened and the president's events, and then the precedence constraints, try to find the most likely reasonable timeline.",
            "That's one sort of example.",
            "We're actually where I think we're fairly general graph is reasonable.",
            "Alot of the other examples actually come from low dimensional datasets where you know some relation between your input output function that you're trying to.",
            "Recover from some data.",
            "OK, so let's."
        ],
        [
            "Turn this into a linear programming problem and then develop a very fast.",
            "A very fast interior point method for it so it King round Sachdeva do well.",
            "OK, first I'm going to consider just doing this in the L1 norm.",
            "'cause that's where we're going to get a linear programming problem.",
            "I should mention in their paper they have the first fast algorithm.",
            "You have the fastest algorithm for doing this in the L1 norm, and they can do it in any other P norm for P bigger than one, and the running comes about order M to the three halves, which is much faster than anything that was known before.",
            "OK so today."
        ],
        [
            "But let's write.",
            "This is a linear program.",
            "We'll do it using the signed edge vertex adjacency matrix.",
            "So here is the signed edge vertex adjacency matrix for this graph.",
            "Yeah, I definitely had the transposes in the wrong place before.",
            "Sorry, OK, so.",
            "The way to understand this is for every single edge we have one row in this matrix, for example, the edge from node one.",
            "Node 4 is that circled row and here which node gets the one in which node gets the minus one is very important.",
            "But we have one row for every single edge in the graph and now function is isotonic precisely if B * X is less than or equal to 0 componentwise.",
            "So isotonic is just you multiply the vector by the signed edge vertex adjacency matrix.",
            "Every entry is less than zero or less than or equal to 0, so this is what are."
        ],
        [
            "Linear program would look like now.",
            "For isotonic regression, we want to minimize the one norm of X -- Y.",
            "Subject to BX being less than or equal to 0.",
            "OK, that looks a lot like a linear program, but it's not quite standard form because one norms aren't things you usually put into a linear program.",
            "So to fix that, what we do is we introduce some extra variables, one for every single chord."
        ],
        [
            "So I'll introduce Max variables I call RI, and now we're trying to do is minimize the sum of the R eyes.",
            "Subject to the absolute value of each XI minus Yi being less than or equal to RI.",
            "OK, that's closer to standard form.",
            "We have to introduce a bunch of new variables, but still standard form for linear programs doesn't let you put in absolute values.",
            "So we just turned that into two inequality's.",
            "One is that X -- y is it."
        ],
        [
            "After assessing equal to R and the opposite of status lesson equal to and now this is a linear programming problem in standard form.",
            "So we can write it with matrices.",
            "It really is minimized with some of our eyes subject to this matrix.",
            "Here times RX being less than or equal to that vector.",
            "If you take a look at what an interior point method does when given a linear programming problem like this.",
            "You see that it solves a series of systems of linear equations of this form.",
            "So the key thing to note here is."
        ],
        [
            "The matrix I drew is there.",
            "Its transpose is here and in the middle is a diagonal matrix.",
            "All the entries on the diagonal are non negative.",
            "Actually they should be positive really.",
            "And I've written them in blocks, S0S1 and S2.",
            "This is what interior point methods do.",
            "I don't care if you're looking at afine.",
            "Potential prime will do or whatever.",
            "They're all basically solving systems of linear equations like this, with a little decoration around it, which can actually significantly affect the runtime.",
            "Well, we're going to be able to solve systems of linear equations like this by solving systems of linear equations in the Laplacian.",
            "To see why, multiply it out.",
            "So if you multiply this out, what we get is, well, OK, it's a two by two block matrix.",
            "One block contains the Laplacian.",
            "Because it is B transpose.",
            "These signed at Vertex adjacency matrix and S 0.",
            "That diagonal is now giving you the weights on the edges times be again, but the key point is that is a Laplacian in the weights come from this SO.",
            "OK, we're adding to it some diagonal matrices, but they are non negative diagonals you add to it.",
            "This S1 and S2.",
            "Those are positive.",
            "I told you that we can solve symmetric diagonally dominant systems so that lower block is symmetric, diagonally dominant.",
            "That's good.",
            "But it's a bigger matrix than just the Laplacian.",
            "There's also a diagonal matrix up here and some diagonals there, but you think you have a block matrix.",
            "There are a few diagonals, and the rest is Laplacian.",
            "You should be able to solve that, and you're right.",
            "You can reduce this to solving Laplacian.",
            "There are many ways of thinking about it.",
            "The 1st way of thinking about it just say, well, the first block of variables I can eliminate if I eliminate which correspond to where I've got some diagonal matrices up there and there, and after I eliminate them, I turn this into something like a Laplacian and that's true.",
            "What, how?"
        ],
        [
            "Happens when you're doing elimination.",
            "I want to remind you of.",
            "What you are really doing is you are really rewriting your matrix.",
            "In this form.",
            "You're rewriting it as a block diagonal matrix where you multiplied by an upper triangular matrix on the right and a lower triangular matrix on the left, and they're the same.",
            "So there's a U transpose into you on either side.",
            "And what is that?",
            "You look like?",
            "It looks like identities and then actually in this case a diagonal matrix.",
            "So if you want to solve systems of linear equations in this whole thing, what do you need to do?",
            "You solve a system of linear equations and you.",
            "A system in this and a system in you transpose and as you can guess you this is diagonal that's dying last diagonal.",
            "That's trivial to solve systems of linear equations in.",
            "That's a simple linear time operation.",
            "This matrix, well, that's it's a diagonal you can solve in that this is symmetric, diagonally dominant.",
            "You can solve in that because I told you have algorithms for that that are very fast.",
            "And then you just end up on doing a solving you.",
            "OK, So what you learn is that you can solve these systems of linear equations very very quickly by reducing to laplacian's, and so Sushant will only get to mention briefly a similar idea in his talk on Monday.",
            "But where you can also do things like this.",
            "So a lot of these things with a little bit of work you can turn into solving systems, equations and laplacian's.",
            "And but there is 1."
        ],
        [
            "And you need to keep track of K. What is the accuracy in the solves that you need?",
            "Because we're using our Laplacian solvers, we're usually doing approximate solves.",
            "Well, actually these guys proved in this paper that in this case they only need a constant accuracy in every solve even doesn't depend on the demand, doesn't depend on the dimension, doesn't depend on the problem.",
            "Constant absolute accuracy.",
            "When you're measuring error in the matrix norm is sufficient.",
            "My work with Sam Dyson was much, much cruder.",
            "It was polynomial accuracy, but we weren't.",
            "We weren't nearly as ambitious back then.",
            "OK, so that at least tells you where some idea of how Laplacian's come up, implying interior Point methods to solve linear programming problems.",
            "I'd now like to talk about something complete."
        ],
        [
            "Different so if you were asleep in jet lag you can come back now and you probably don't worry bout what you missed.",
            "I want to talk about graph sparsification and how we will use it to quickly solve systems of linear equations in Laplacian matrices.",
            "So sparsification.",
            "Is the problem of approximating an arbitrary graph by a sparse graph?",
            "We want to graph a few edges.",
            "Let me begin by defining what this approximation is.",
            "I will say that a graph H is an epsilon approximation of a graph G if their Laplacian quadratic forms are always approximately the same.",
            "So the ratio of the quadratic form should always be between one plus epsilon and 1 / 1 plus epsilon for all vectors X.",
            "So this is equivalent to saying things like that the Laplacian of H is less than or equal to 1 plus epsilon times Laplacian of G in the partial order in sort of the loevner partial order.",
            "That means that this matrix minus that matrix is positive semidefinite.",
            "It's the same thing as saying that of course anymore the other direction is that."
        ],
        [
            "The 1 / 1 plus epsilon times LG is less than or equal to LH.",
            "So you can also talk about this in terms of positive semidefinite matrices.",
            "It's a very strong notion of approximation.",
            "So remember when we were looking at the boundaries of sets?",
            "They were given by evaluating the Laplacian quadratic form.",
            "So this tells you that if one graph approximates another, then for every single set of vertices, the sizes of the boundaries are approximately the same in both graphs.",
            "It also tells you that spectral properties of the two laplacian's are approximately the same.",
            "It also tells you that the inverse is."
        ],
        [
            "So in this case, pseudo inverses of the Laplacian matrices satisfy the same relation.",
            "This follows pretty immediately from the previous line pretty neatly.",
            "Just you can multiply through by matrices, and this doesn't change.",
            "So this means that the solutions to linear equations in one matrix are approximately the same if the other matrix approximates it.",
            "So you can use this to come up with ways of solving systems of linear."
        ],
        [
            "Asians?",
            "So in particular, if H is an epsilon approximation of G. If you get and if you actually exactly solve a system of linear equations in age, so let X be the solution to the Laplacian.",
            "H * X is equal to B.",
            "Then it's an epsilon approximate solution to the same equations in G. Where we have to be very careful when about how we define an approximate solution to a system of linear equations.",
            "Again, This is why I defined it in terms of the matrix norm.",
            "It means that when I approximate the matrices, I get an approximate solution to the system of linear equations.",
            "And if you were actually trying to understand the error that you would get if instead of solving the system and H exactly, I solved the system in H approximately again, you would measure in these norms and adjust composes.",
            "You just add the two errors.",
            "OK, so."
        ],
        [
            "This actually I told you now that if you can solve systems in H then you can solve systems in G. But you can do better than that, so I told you in the previous slide that is solution to system and H gives you an epsilon accurate solution to system in G. You can drive that precision to be as high as you want by doing it again and again and again.",
            "So this is the algorithm people often called iterative refinement.",
            "You know what you do is you first get your initial guess at a solution.",
            "Say why to solve the system for why.",
            "Where are initially set to be.",
            "And then what you do is you update X.",
            "You sort of computer residual.",
            "You take a look at how much error you're making and you re solve again.",
            "And if you keep doing this iteratively after the case iteration, you drive the error in your solution down to epsilon to the K. So once you can get approximate solutions, you can get highly accurate solutions.",
            "If I had another 5 minutes or if I was assigning homework, I'd point out that this is really the same as the analysis of the power method we did earlier, or a translation of it if you want."
        ],
        [
            "OK, so if we can spar sofy then we can solve systems of equations, assuming you can solve systems of linear equations approximation.",
            "So here are two theorems that I want to refer to.",
            "About sparse approximations of graphs, the first is sort of the best existence result that we know right now.",
            "It was joint work with Josh Batson and Nikhil Shrivastav.",
            "So we proved that every graph G has an epsilon approximation with relatively few edges.",
            "If you want an epsilon approximation, the number of edges is number of vertices times.",
            "What did I write there?",
            "Yes, two plus epsilon squared divided by epsilon squared.",
            "So you know there's nothing hidden here.",
            "This is the best that we know exists.",
            "And by the way, I'm just demonstrating this by the complete graph on 10 vertices is pretty well approximated by the Petersen graph.",
            "Is the right way of thinking of it, but where I've blown up the weights on the edges 'cause what you do is you get rid of some of these edges.",
            "You have to increase the weights on the others, otherwise it wouldn't be an approximation."
        ],
        [
            "Now that's an existence result.",
            "It's a polynomial time algorithm.",
            "If you want to do it really quickly right now, the algorithms we know work by random sampling, and the simplest of these one that Nikhil Srivastava and I came up with where will get about N log N over epsilon squared edges.",
            "So we're going to lose a log in.",
            "I don't want to lose the log in just to be clear, I want to do this on graphs with a billion nodes where login is sort of painful even, but we can do it quickly and the way you do it is you solve system, just randomly sample edges and if you include them then you increase their weight.",
            "An you sample them with probability.",
            "Well, it's equal to their effective resistance in the graph.",
            "I'm not going to define that, but basically I'll mention you can get it.",
            "You can compute the sampling probability for an edge by solving a system of linear equations in Laplacian, but it turns out you can do this all pretty quickly.",
            "So we had a nearly linear time algorithm.",
            "Did I leave?"
        ],
        [
            "Slide is going to say kudis.",
            "Levin and Pang got this down to time like N log squared N. Maybe that's on a later slide I've suddenly forgotten.",
            "I'm sorry, but why don't I tell you that there's been some new breakthroughs on this?",
            "Where it sort of combine the two so this top paper was just presented at stock last week where they sort of combine that they come up with a nice way of basically combining those two results to get a sampling based algorithm.",
            "Which is actually pretty efficient, so it gets you a linear in N edges, which is the right order of magnitude where there's a number Q there, which shows you how many edges you get in the bigger it is the faster algorithm.",
            "So they got it down to time like MN times the end of the 1 / Q, which you should think of his small.",
            "This was the fastest algorithm we knew for getting that kind of sparse a fire at the time.",
            "And then in Tetley and his son just have a paper that I don't know if it's public eppers just announced it was accepted to Fox Isop repent.",
            "It looks very nice where they actually get this down to a nearly linear time algorithm building on the techniques of the previous paper.",
            "So this is actually now sparsa fires that are in the linear size regime in time.",
            "Basically, order M * N to something pretty small.",
            "Now, OK, there's a bit of a tradeoff between the exponent on end and the number of edges.",
            "But it's huge progress on this.",
            "Both of these require solving Laplacian linear equations, which can pose a problem because what I want to do is I want to use parser fires to solve Laplacian linear equations, and it's not clear I can do that if I need to solve Laplacian linear equations to build my sparsa fires in less, we get some sort of fortunate induction."
        ],
        [
            "Alright, here's where I mentioned the results of Curtis Miller and thanks so they actually showed again using the best Laplacian solvers, but essentially solving smaller systems that they could get the sparse ifers like Nyquil and I built.",
            "But in time order M log squared in I want to mention there is a very interesting result which I don't think yet has found its killer app by this sort of.",
            "I called the dream team from MIT Co only Musco, Musco, pangan sidford.",
            "There a few other people there even working on such things where they came up with some very cool ways to do some.",
            "Very crude random sampling on smaller systems.",
            "To get effective sparsification this is interesting to me because it looks like the sort of thing you could doesn't require laplacian's at all.",
            "So you can apply it on very general linear systems, so it's worth looking at.",
            "I won't say too much more about it.",
            "One result that I want to use because what I was very interested in getting into this was how can we?",
            "Build solve systems of linear equations in parallel is a result of Giannis Kutis, which build sparse ifers in nearly linear time, logarithmically polylogarithmic parallel time.",
            "By using graph spanners, is a primitive and that's something we can use."
        ],
        [
            "OK, so let me not rattle off results about sparse ifers anymore.",
            "Let me tell you how you can use them to solve systems of linear equations.",
            "I'll give you 2 algorithms.",
            "The first will be the simplest, so let's remember.",
            "A fundamental result we all learned a long time ago.",
            "The Power series expansion for 1 / 1 -- X.",
            "1 / 1 -- X is equal to 1 + X + X ^2 + X ^3 and so on.",
            "Well the same thing is true for matrices, at least in the symmetric case.",
            "If I have a matrix X an, all the eigenvalues are less than one.",
            "Then the inverse of the identity minus X is equal to I + X + X ^2 + X ^3, and so on.",
            "You can check just by multiplying by I -- X that this is converging to the right thing.",
            "OK, so you can use that power series now.",
            "How am I going to apply that to laplacian's?",
            "One way of thinking about it is a Laplacian matrix.",
            "The way we wrote it, it had sort of a minus an adjacency matrix and a diagonal term.",
            "And if you re scale things, you can basically write it as I minus something and the norm of this thing will be less than one so.",
            "At least orthogonal to the nullspace, so you can apply this formula to laplacian's after a slight rescaling.",
            "This is the Power series expansion.",
            "We're going to use to get a very fast algorithm for solving systems of linear equations in laplacians.",
            "By the way, this is also what you're doing.",
            "You're doing iterative refinement to improve solutions of systems of linear equations.",
            "If you look at it the right way."
        ],
        [
            "OK, now you can ask yourself how many terms in this power series do you need before you get a good approximate solution.",
            "And the answer is, it depends on the condition, number of the matrix X.",
            "Basically, for our purposes, it's the ratio of the largest eigenvalue to the smallest eigenvalue.",
            "After you take that many terms, the others start to go to 0.",
            "And they have a negligible effect.",
            "Now, unfortunately, that's more terms than I want, so I'm going to rewrite this power series in."
        ],
        [
            "End of writing it is this some let's write it as a product.",
            "So what we'll do is rewrite it is the identity minus X.",
            "Times identity minus X squared times identity minus X to the 4th put in X to every power of two here.",
            "And if you do that, you will see when you write that product you actually get all of those terms.",
            "So every single term comes up once and you know the terms in here that contribute.",
            "You get just by taking the you know the binary expansion of the exponent.",
            "Now before where I said I needed to some Kappa terms to get a good approximation.",
            "I'm in better shape, I only need to take a product of LGE Kappa terms to get the same thing.",
            "And that's sort of the right regime.",
            "I want to be in to be fast.",
            "I want to be logarithmically the condition number, not linear in the condition number.",
            "OK, so here's our."
        ],
        [
            "Idea for how we would solve a system of linear equations in the Laplacian.",
            "Take a look at this product.",
            "A vlog Kappa terms.",
            "To solve a system of linear equations Opossum, we just want to multiply by each of those matrices.",
            "OK, even if my initial matrix is sparse, these matrices very quickly get dense, but So what will sparsifying them?",
            "That's my answer to everything.",
            "You know?",
            "Probably one day my kids will be eating sparsa fires for dinner or something.",
            "I don't know.",
            "This is how we this is what we're going to do with everything.",
            "OK, so you just want to sparsified those matrices.",
            "And that should get you a fast algorithm.",
            "There's only one caveat here, and this is where some of the work went."
        ],
        [
            "Which is that sparsifying the terms in a product doesn't tell me anything about the product.",
            "So sparsification the way I defined it using like the semidefinite partial order.",
            "That's a notion for symmetric matrices.",
            "And OK, all these matrices here are simultaneously diagonalizable.",
            "But as soon as I approximate them and I take a product, I get things that are not symmetric and not simultaneously diagonalizable.",
            "An it unfortunately turns out that if you give Me 2 matrices, A&B and arbitrarily good approximations of the Manhattan behat, you actually really learn nothing.",
            "There's nothing nice I can say about the relationship between a B and a happy hat.",
            "So while this was a really nice idea, it needs another idea to make it work.",
            "Because again, sparsifying the terms in the product doesn't give me any useful guarantees about the product.",
            "And what we need to do to fix this is symmetrise it somehow."
        ],
        [
            "OK, so to do that, let me give you another way of deriving this expression.",
            "So one way of seeing that the inverse of I -- X is equal to that is you can first check that the inverse of I -- X is equal to y + X times the inverse of I -- X ^2.",
            "OK, and then if you apply that again you get I + X ^2 times the inverse of I -- X to the 4th and so on.",
            "So if you use this formula I wrote up here, it's a way of re deriving this.",
            "What we do is we just use a slightly different formula that's symmetric.",
            "So it turns out you can also write I -- X inverse is equal to this thing.",
            "OK, don't try to figure out why this minute just the important note.",
            "Is it the I -- X ^2 term appears in the middle and it's symmetric and multiplied by and I + X on either side and when you have this formula then sparsification goes."
        ],
        [
            "So here's what we do to solve a system of linear equations.",
            "You need to multiply by I + X once in the beginning and once in the end 'cause you have them on left and right.",
            "The term in the middle this I -- X ^2 the inverse of that we're going to sparsified that matrix the I -- X ^2.",
            "Oh, sparse matrix.",
            "That call I -- X sub two and 5 -- X and two is approximately equal to minus X squared.",
            "Then I can substitute it into this formula.",
            "And when I submitted in not now we have symmetry.",
            "And now you can prove that then you have actually an approximation.",
            "So in this formula, when you approximate X2 X squared by X2, everything goes through.",
            "And I'm going to gloss over one detail about how you efficiently compute X2.",
            "This parser fire because that is not the purpose of this talk today.",
            "OK, so the first purpose now is, once you actually verify this identity.",
            "You now know a very fast way of saying if sparse ifers exist, then very fast ways of solving systems of linear equations in laplacians exist.",
            "I'm going to show you a different way of building things quickly.",
            "First, let me some."
        ],
        [
            "Guys, what does this get you?",
            "So this gets you a nice parallel or distributed algorithm for solving equations in laplacians.",
            "The depth of some exponent, some power of log N times the dimension in the condition number and then the work is nearly linear.",
            "OK, this was a good first attempt.",
            "There is one thing I want to criticize about this result.",
            "OK, the power on log in that see I don't know.",
            "It's like 6.",
            "It's not horrendous.",
            "It's not anything you'd use actually.",
            "Depending on the condition number is not always OK.",
            "So if you just take an unweighted graph, its condition number cannot be too big.",
            "If you take an unweighted graph and I take it to the policy and the condition number most end squared so you don't worry about it.",
            "But if you're actually solving linear programming problems by an interior point method.",
            "The matrices that you get at the end become very poorly conditioned.",
            "So as you solve systems of linear equation, so if you solve he's running to your point methods.",
            "The closer you get to convergence, the more poorly conditioned your matrices become.",
            "As a matter of fact, for many, many years Matlab had a special version of a Cholesky factorization built into it just for the purpose of interior point methods that you could use that would handle the sort of errors that came up in this.",
            "Unfortunately, they removed it.",
            "Now some code doesn't work anymore is a couple of years ago, but.",
            "The condition number eventually matter, so I'll give you something that won't dip."
        ],
        [
            "On the condition number, this is an algorithm we got under archive last week.",
            "Now if something we're calling Sparsified Cholesky factorizations.",
            "So what it will do is this.",
            "You give me an arbitrary Laplacian or non negative symmetric backing dominatrix.",
            "It will compute an upper triangular matrix you that's very sparse, order N nonzeros so that your original matrix M is approximated by U transpose EO.",
            "OK, how many I get enough answer?",
            "How many of you use to salesky factorization?",
            "Again, many so those of you know, perhaps why people who do that usually write this, not you transpose you is LL transpose, but I'm not because elwer laplacian's all this talk.",
            "And that's why I'm going to call this thing M Instead of L, just so the people who know toleski don't get confused.",
            "OK, so again, after you're done the factorization, we'll see how you do that.",
            "You get this matrix you and once you have that you can solve systems in your Laplacian in time order M. Log one over epsilon.",
            "At least, and the depth is basically log M log epsilon inverse because it turns out you has a very nice structure as a block structure, so you can actually apply it quickly.",
            "And we can form the factorization in nearly linear work in polylog depth.",
            "I won't go into that as much, so let me just tell you."
        ],
        [
            "This works.",
            "So I should 1st remind you a few facts about what Cholesky factorization does.",
            "The ordinary one, especially for those who aren't familiar with it.",
            "First, you all learn this under the name Gaussian elimination.",
            "Teleski is the version of Gaussian elimination you apply to symmetric matrices so that all the matrices you get are symmetric.",
            "Along the way, and I'll show you an example, but the main thing that goes in your applying Cholesky factorization to Laplacian matrix.",
            "The smaller matrices you get are also Laplacian matrices.",
            "So for example, if I take this matrix here and say eliminate graph, sorry eliminate node one.",
            "It corresponds to putting a clique on the neighbors of node one, so I'll get a Laplacian of this graph where these new edges show up with some weights depending on the original weights."
        ],
        [
            "And we can actually see what they were.",
            "I'll do that in a moment.",
            "The thing knows when you're done and you get this factorization into you transpose you where you as an upper triangular matrix.",
            "And then you can solve systems in your matrix by doing 2 solves in a triangular matrix and those are fast.",
            "They take time proportional to the number of nonzeros in them."
        ],
        [
            "Tricks?",
            "OK, so let me remind you what salesky does.",
            "It's like Gaussian elimination.",
            "So what do you do?",
            "You take your initial matrix here.",
            "I didn't put in, I just put in that part of the matrix nodes 123 and four in this graph.",
            "You apply some row operations to take multiples of the first row and subtract them off from the other rows that correspond to multiplying by this matrix.",
            "It's a lower triangular matrix and you get something that looks like this.",
            "And we've cleared the first row and column and put in some new entries in the other rows.",
            "But this isn't symmetric, So what you're really doing is you're really multiplying by this matrix on either side.",
            "Where did that go here?"
        ],
        [
            "I multiply by it there and there, and then I get something symmetric again.",
            "And this is a Laplacian of a smaller matrix where you can see I've got the edge weights suggested.",
            "Now these are not my matrices.",
            "You I should say these are actually the inverse of U in U transpose."
        ],
        [
            "The actual matrices you show up like this, really what you're doing."
        ],
        [
            "Taking your initial matrix."
        ],
        [
            "In your writing it as you transpose times this smaller Laplacian times you.",
            "Unfortunately for this matrix it's very easy to get the inverse.",
            "Here's you.",
            "It looked a lot like you inverse, just with different signs."
        ],
        [
            "OK, so the important things to know here are that the number of non zero entries in you.",
            "Is equals to the degree of the vertices when they were illuminated.",
            "So when you eliminate a vertex, you take its degree.",
            "That is what you get in you, and that happens for every single vertex, not in you in person every vertex.",
            "OK, so clearly the way to get a you that sufficient and to make this fast is to try to get his few non zeros as possible and one of the main heuristics people have used for a very long time in the field is what's called an incomplete Cholesky factorization, where merging can vanderhorst basically suggested.",
            "Throw away the small entries you get in you.",
            "And throw away everything else we can.",
            "We get some sort of approximation of the original.",
            "What I'm going to suggest now is very similar, but we're going to sparse if I so it's going to throw away entries carefully and increase their weights.",
            "I think I have about a minute left."
        ],
        [
            "Or two, I can tell you how this works.",
            "OK, so here's the idea.",
            "We need to pick a bunch of vertices.",
            "Let me call them S to eliminate.",
            "So the way we're going to do that is OK. Once I do that, rewrite your matrix in blocks, so the vertices we eliminate go first.",
            "T is the compliment.",
            "When you eliminate them, what you are doing is you're writing your matrix as something that's lower triangular times a block diagonal times something that's upper triangular.",
            "If the initial one was Laplacian, this thing is also Laplacian.",
            "What we can do I should say, is if the initial thing is Laplacian, this new one is Laplacian, but denser so we sparsifying it.",
            "And that is going to be the key of our algorithm.",
            "Choose a block of vertices to eliminate, get a new Laplacian sparsified."
        ],
        [
            "And there's one catch.",
            "You'll notice that MSS comes up often here.",
            "And we need to multiply by its inverse there and there, and to solve a system of equations.",
            "Here we need to multiply by its inverse.",
            "So we need our set S to give us something where MSS is really easy to solve linear equations in.",
            "The ideal would be an independent set of vertices in a graph.",
            "If I don't get an independent set of vertices, MSS would be diagonal.",
            "And then solving equations and it would be easy.",
            "Can you find a large independent set of vertices in this graph?",
            "Yes you can, because we sparsified, so there aren't too many edges, so there is a large independent set.",
            "But not one big enough for the induction to work and for the error analysis to work.",
            "So we need something a little bigger than independent set.",
            "So we look for something we call 4 strongly diagonally dominant.",
            "This means not the diagonals exceed the off diagonals in the row and column.",
            "We need them to be four times the sum of the off diagonals in their row and column.",
            "You can find that just by random sampling.",
            "You pick a random subset of vertices, throw away the ones that don't satisfy the condition.",
            "You've got it.",
            "OK, so that is our sparsified Cholesky factorization.",
            "We basically do some random sampling.",
            "If you use the ideal sparsa fires the best.",
            "We know this gets you a factorization with order N non zeros that you can solve systems in linear time.",
            "If we're not using those best factorizations, then we need to."
        ],
        [
            "Go to some other sparsa fires and you get sort of two different results."
        ],
        [
            "If you want the best parallel time right now we use the sparse fires of cudas and we lose some log factors every."
        ],
        [
            "Where?",
            "If we use the ones of the paper I mentioned earlier from many people, they have an idea and we use also some techniques from the earlier paper I mentioned with Richard Peng.",
            "Then we get things which are only off by a log factor of the optimal that we know the depth doesn't quite go down to log in, but it's still pretty good.",
            "OK, this will all improve clearly, and I think this is a big motivation for building parallel sparse ifers."
        ],
        [
            "So let me say what I think is going to happen next.",
            "One thing is these techniques that only need sparse ifers.",
            "And especially with some of the new work on building sparse fires, make it look like we should now be able to solve many other types of systems of linear equations.",
            "For example, it's sort of a one line observation to realize that you can handle laplacian's with complex weight sort of things that occur in analyzing some sort of electrical circuits.",
            "Actually, gotta be careful where the complex number goes, we put it in the right place, ones on the diagonal, complex off diagonal, so you can handle.",
            "I think we'll find others, so take your favorite linear programming problem on a graph and look at the systems of equations you get when you apply the interior point method might be able to solve those really quickly now.",
            "Um?",
            "One of the techniques, if you look at our paper that we always need that.",
            "I didn't really talk about was we need to know how to spar.",
            "Suffice certain graphs that appear during the course of the algorithm we call product demand graphs, sort of like cliques but with edge weights determined by some values at vertices.",
            "Improving that is a cool problem for a graduate student, I think because it's probably solvable to just do something better and it would clearly improve the algorithms.",
            "Um?",
            "We want faster factorizations for this.",
            "It is very clear if you look at our analysis of this, we're throwing away a lot of factors.",
            "And not just there, a few logarithmically factors.",
            "Also some huge constants that I'm lying about and there are many points of Slack in the analysis, so this is sort of thing where I think someone some people could really clean this up.",
            "We held onto this paper for year cleaning it up.",
            "We saved many, many logs were done.",
            "Time for someone with a fresh perspective.",
            "I mean literally the first draft of it was a year ago.",
            "OK, so if you want to."
        ],
        [
            "Earn a lot more.",
            "I've a link on my web page where I try to keep up with everything related to solving Laplacian linear equations and sparse ifers.",
            "I haven't modified it in three weeks, so I got some new papers to put in, but you can look at that.",
            "I have course notes that I have right for all the grad courses I teach, so spectral graph theory and graphs and networks online that contain a lot of this material and the fall version of my spectral graph theory course will contain a lot of what I said today an I always recommend the monograph L X = B by Nasheed fish noise.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first I'm going to hold this mic because they're recording, but I'm going to try not to use it because I tend to be loud.",
                    "label": 0
                },
                {
                    "sent": "If for any reason I fail to be loud and someone in the back can't hear me or just do this and an I'll notice.",
                    "label": 0
                },
                {
                    "sent": "OK, so today I want to tell you a lot about Laplacian matrices of graphs and algorithms for solving systems of linear equations in them.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to try to do a few different things, so of course I will tell you what laplacian's are in a little about them.",
                    "label": 0
                },
                {
                    "sent": "That will come surprisingly late in the talk though.",
                    "label": 0
                },
                {
                    "sent": "1st I'm going to try to tell you what we use them for, so I'll briefly mention problems of interpolation on graphs.",
                    "label": 1
                },
                {
                    "sent": "Computing eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "OK, that will be in Laplacian so around.",
                    "label": 0
                },
                {
                    "sent": "Then I gotta tell you what they really are, where they come up in optimization.",
                    "label": 1
                },
                {
                    "sent": "When you're solving linear programming problems on graphs.",
                    "label": 0
                },
                {
                    "sent": "I'll then talk about one of my favorite tools for studying these, which is sparsification, and then I'll tell you how we're going to use it.",
                    "label": 0
                },
                {
                    "sent": "So in particular I will show you 2 pretty new algorithms.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, one.",
                    "label": 0
                },
                {
                    "sent": "The paper got an archive last week so they can call very new algorithms for solving systems of linear equations and laplacian's that just use sparsification as the tool to drive the algorithms.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so first the problem of interpolation on graph.",
                    "label": 1
                },
                {
                    "sent": "So this is what we call it.",
                    "label": 0
                },
                {
                    "sent": "Some people call it learning on graphs or inference on graphs coming from this paper of Drew Ghahramani and Lafferty.",
                    "label": 1
                },
                {
                    "sent": "So they suggest this problem where you have a graph an you know the values of a function at some nodes like here I know it's zero there.",
                    "label": 0
                },
                {
                    "sent": "I know it's one.",
                    "label": 0
                },
                {
                    "sent": "And you would like to guess the values at the other nodes of the graph.",
                    "label": 0
                },
                {
                    "sent": "I think of this as a problem of interpolation.",
                    "label": 0
                },
                {
                    "sent": "You're really trying to fill in the other values under the assumption that nodes that are connected by edges are similar.",
                    "label": 0
                },
                {
                    "sent": "So sort of like there's some almost continuous space there, and the way they suggest doing that is they try to find a function that doesn't vary too much over the edges, so they try to minimize the sum over all edges.",
                    "label": 0
                },
                {
                    "sent": "Is this going to work?",
                    "label": 0
                },
                {
                    "sent": "Yes, the sum over all edges of the square of the differences of the function across the edges.",
                    "label": 0
                },
                {
                    "sent": "This is what they suggest.",
                    "label": 0
                },
                {
                    "sent": "Minimizing subject to the given values.",
                    "label": 1
                },
                {
                    "sent": "So this sum is something that's going to appear a lot in this talk, and this will give us our first definition of the Laplace.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lawson.",
                    "label": 0
                },
                {
                    "sent": "The Laplacian of this graph is the matrix so that when you take the vector X, so X should be indexed by vertices of the graph.",
                    "label": 0
                },
                {
                    "sent": "The rows and columns of El are indexed by vertices of graph.",
                    "label": 0
                },
                {
                    "sent": "If you take X transpose LX, you get exactly this quadratic form here.",
                    "label": 0
                },
                {
                    "sent": "So that is one way of defining the Laplacian for now, and so in case you wanted.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the solution to this system.",
                    "label": 0
                },
                {
                    "sent": "And how would you actually compute this where you're trying to minimize this quadratic subject to some given conditions.",
                    "label": 0
                },
                {
                    "sent": "Basically you take a derivative an when you take the derivative of the quadratic, you get something linear and you discover what you need to do is solve a system of linear equations in the Laplacian matrix, or properly speaking, in this case, it's in a submatrix of the Laplacian, but it turns out to be an essentially equivalent computational problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the thing that will appear a most in this talk is the Laplacian quadratic form.",
                    "label": 0
                },
                {
                    "sent": "So just remind you given a graph, it's the sum over edges.",
                    "label": 0
                },
                {
                    "sent": "Of the OK look quadratic form takes a function X from vertices to the reals.",
                    "label": 0
                },
                {
                    "sent": "You take the sum of the squares of the differences of that function across edges.",
                    "label": 0
                },
                {
                    "sent": "That is, the Laplacian quadratic form in the graph.",
                    "label": 0
                },
                {
                    "sent": "And I remind you again that the Matrix is the matrix.",
                    "label": 0
                },
                {
                    "sent": "The symmetric matrix that when you put here, you get that quadratic form again.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you exactly what the entries in that matrix are in a little bit.",
                    "label": 0
                },
                {
                    "sent": "Before we do that, let's make some observations about the spectrum of the Laplacian matrix.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The first thing we want to observe is that it is a positive semidefinite matrix for people who are used to thinking about positive semidefinite matrices.",
                    "label": 0
                },
                {
                    "sent": "This just follows from the fact that that quadratic form is never negative.",
                    "label": 0
                },
                {
                    "sent": "If you're not used to thinking about these things well, think about any eigenvector of the matrix that you put here.",
                    "label": 0
                },
                {
                    "sent": "V If the Laplacian is equal to Lambda V well multiplied by V transpose on the other side.",
                    "label": 0
                },
                {
                    "sent": "Now you're going to get Lambda V transpose, VV transpose.",
                    "label": 0
                },
                {
                    "sent": "V has got to be positive.",
                    "label": 0
                },
                {
                    "sent": "And because we know this is always possible, that means lambdas positive.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you're not used to thinking about the variational characterization of eigenvalues, here's another proof.",
                    "label": 0
                },
                {
                    "sent": "All the eigenvalues, Laplacian or at least non negative.",
                    "label": 0
                },
                {
                    "sent": "I should be careful that does have an eigenvalue of 0.",
                    "label": 0
                },
                {
                    "sent": "If you put in the all ones vector, you're going to get 0 here.",
                    "label": 0
                },
                {
                    "sent": "Or any constant vector, you'll get 0.",
                    "label": 0
                },
                {
                    "sent": "And if the graph is connected, it's pretty easy to see that that's the only eigenvalue of only eigenvector wagon value 0.",
                    "label": 0
                },
                {
                    "sent": "So if the graph is connected the eigenvalue zeros multiplicity one.",
                    "label": 0
                },
                {
                    "sent": "So I will let Lambda one the smallest eigenvalue be 0 and the other eigenvalues will be larger.",
                    "label": 0
                },
                {
                    "sent": "OK, just one warning.",
                    "label": 0
                },
                {
                    "sent": "During this talk I will sometimes write or talk about the inverse of the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Why am I doing that?",
                    "label": 0
                },
                {
                    "sent": "Because I want to talk about solving systems of linear equations, but I just told you that the matrix is singular, so you know it has no inverse.",
                    "label": 0
                },
                {
                    "sent": "I really mean is the pseudo inverse that is the inverse orthogonal to the null space and since we know you can just take a look at a graph and check if it's connected and if it is, you know the nullspace is the constant vectors and you can easily just work orthogonal to that.",
                    "label": 0
                },
                {
                    "sent": "So if you catch me saying the inverse, don't worry, I just mean restrict your attention to the space orthogonal to null space and there you have an inverse.",
                    "label": 0
                },
                {
                    "sent": "Also, this is usually called the pseudoinverse.",
                    "label": 0
                },
                {
                    "sent": "OK, so why are?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be interested in eigenvalues of Laplacian matrices.",
                    "label": 0
                },
                {
                    "sent": "It's not our priority obvious, but once you know about it, you start to see that there's this whole field called spectral graph theory.",
                    "label": 0
                },
                {
                    "sent": "That's all about what do eigenvalues and eigenvectors of matrices like the Laplacian tell you about a graph?",
                    "label": 0
                },
                {
                    "sent": "So I teach an entire course on this.",
                    "label": 0
                },
                {
                    "sent": "I'll be doing it in the fall, but for those who haven't seen it, let me give you just one motivation which comes from the problem trying to find clusters in graphs or partitioning graphs.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Usually when you're trying to find a cluster of nodes in a graph, you're trying to find a set of nodes, like the set S here, which has a relatively small boundary where by the boundary I mean just the edges.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Leaving the set.",
                    "label": 0
                },
                {
                    "sent": "So the Laplacian quadratic form allows us to measure the boundary of a set of nodes.",
                    "label": 1
                },
                {
                    "sent": "The way you do that is you take the vector X, that's the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Characteristic.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After that set of nodes, so one inside and zero outside an.",
                    "label": 0
                },
                {
                    "sent": "If you then apply the Laplacian quadratic form to that, you will get the sum of the squares of differences across all edges, which will just pick up one for each edge in the boundary and will be 0 elsewhere.",
                    "label": 0
                },
                {
                    "sent": "So when you apply the Laplacian quadratic form to the characteristic vector of a set, you discover the size of the boundary.",
                    "label": 1
                },
                {
                    "sent": "Now often if you are given a graph, let's say you're trying to identify communities or something like that, you want to find the set S for which this is relatively small.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Spectral methods give you a heuristic for doing this.",
                    "label": 0
                },
                {
                    "sent": "What you do is you find you want to find a vector X for which the Laplacian quadratic form is small.",
                    "label": 0
                },
                {
                    "sent": "You do it by, say, finding the second eigenvector.",
                    "label": 0
                },
                {
                    "sent": "The first eigenvector is just constant.",
                    "label": 0
                },
                {
                    "sent": "That's useless.",
                    "label": 0
                },
                {
                    "sent": "Find the second eigenvector and round it to a 01 vector.",
                    "label": 0
                },
                {
                    "sent": "An empirically this is a good way to start finding clusters in graphs or start partitioning them, and we heard a talk or two about this earlier.",
                    "label": 0
                },
                {
                    "sent": "You can do much fancier things.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this is just.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how are we going to compute an eigenvector of the Laplacian?",
                    "label": 0
                },
                {
                    "sent": "So let's start by looking at the power method.",
                    "label": 0
                },
                {
                    "sent": "Can I get a show of hands for how many of you have seen the power method?",
                    "label": 0
                },
                {
                    "sent": "Hallelujah, OK, so I'll just remind you.",
                    "label": 0
                },
                {
                    "sent": "What the power method does in the analysis.",
                    "label": 1
                },
                {
                    "sent": "OK, so remember you just take of like a random vector X, ideally a Gaussian random vector X.",
                    "label": 0
                },
                {
                    "sent": "You multiply it by the Laplacian, you just want to keep multiplying it by the Laplacian, but you know that if you just keep doing that, you get something that tends to go to Infinity or zero way too quickly.",
                    "label": 0
                },
                {
                    "sent": "You get numerical errors, so you re scale at every step.",
                    "label": 0
                },
                {
                    "sent": "You tend to divide by the norm, and this converges very quickly to an eigenvector of the largest eigenvalue of the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And I will remind you of the analysis in a moment.",
                    "label": 0
                },
                {
                    "sent": "But we want the eigenvalue of this one eigenvector of the smallest eigenvalue to do things like partitioning or graph drawing those use small eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "But if you're trying to do graph coloring, then you want the large eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "But partitioning you want the small ones.",
                    "label": 0
                },
                {
                    "sent": "OK, well you say we have a way of dealing with that.",
                    "label": 0
                },
                {
                    "sent": "Let's just flip the spectrum around.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have some upper bound on the spec.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's call it T if all the eigenvalues are less than T, by the way, twice the maximum degree in a graph would suffice.",
                    "label": 0
                },
                {
                    "sent": "Then I can just take T times the identity minus the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Now if I look at that matrix so it was the smallest eigenvalue before, now becomes the largest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "And you can imagine applying the Laplace the power method on this.",
                    "label": 1
                },
                {
                    "sent": "And you know that it will eventually approach the eigenvalue eigenvector largest eigenvalue of this matrix and that gets us the smallest eigenvalue of the original.",
                    "label": 0
                },
                {
                    "sent": "But I put a question Mark next to the quickly because the convergence rate of this.",
                    "label": 0
                },
                {
                    "sent": "Can be really unsatisfying.",
                    "label": 0
                },
                {
                    "sent": "And to remind you why.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just reminder of the analysis of the power method.",
                    "label": 1
                },
                {
                    "sent": "So since L is a symmetric matrix, its eigenvectors form an orthonormal basis, so I can take my initial random vector X and let's expand it in that orthonormal basis and if it were a Gaussian random vector, then the coefficients of those vectors are IID.",
                    "label": 0
                },
                {
                    "sent": "So there now we have.",
                    "label": 0
                },
                {
                    "sent": "So we get a bunch of coefficients that are basically independent random times eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Let's see what happens when we keep multiplying by the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Well, every time you multiply by the Laplacian, the coefficient of the eigenvector gets multiplied by Lambda I.",
                    "label": 0
                },
                {
                    "sent": "So after K iterations we get Lambda I to the K here, which means it really only the biggest ones are going to count.",
                    "label": 0
                },
                {
                    "sent": "And you can make that precise if you want to basically wipe out the contribution of all eigenvectors whose eigenvalues are smaller than one minus epsilon times the largest.",
                    "label": 0
                },
                {
                    "sent": "You basically need Logn over epsilon iterations and they're gone.",
                    "label": 0
                },
                {
                    "sent": "So that's very fast, and what's nice is this convergence rate actually doesn't depend on Lambda N, it's just something absolute.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give this breaks down.",
                    "label": 0
                },
                {
                    "sent": "However, for what I just showed you for how we get the smallest eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Because if you do the exact same analysis with this other matrix, suddenly we don't have Lambda eyes.",
                    "label": 0
                },
                {
                    "sent": "And here we have this T minus Lambda I and there's this problem that the smallest eigenvalues tend to be small.",
                    "label": 0
                },
                {
                    "sent": "And if I have T, even if it's more than one, you know 1 minus something small isn't very different from 1 minus twice something small.",
                    "label": 0
                },
                {
                    "sent": "Or here's another way of putting that.",
                    "label": 0
                },
                {
                    "sent": "If I just want to make all of the eigenvalues less than twice the smallest disappear.",
                    "label": 0
                },
                {
                    "sent": "Which I certainly want.",
                    "label": 0
                },
                {
                    "sent": "We need many iterations.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations is not something like log in, and there's no epsilon here.",
                    "label": 0
                },
                {
                    "sent": "It depends on one over Lambda 2.",
                    "label": 0
                },
                {
                    "sent": "So the smaller Lambda two is, the longer it takes to get rid of like the smallest.",
                    "label": 0
                },
                {
                    "sent": "Eigen varies like this.",
                    "label": 0
                },
                {
                    "sent": "OK if you're using Matlab and you type my guys to get it the smallest eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "This is basically what it's doing, but it's doing land shows which is a little faster.",
                    "label": 0
                },
                {
                    "sent": "That's one of the reasons for a lot of natural matrices.",
                    "label": 0
                },
                {
                    "sent": "You get the largest eigenvalues much faster than you get the smallest.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "You know this convergent trade I don't like well.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is one reason we want to solve systems of linear equations in Laplacian?",
                    "label": 0
                },
                {
                    "sent": "It's so much better if when we use the power method we apply it to the inverse of the Laplacian?",
                    "label": 1
                },
                {
                    "sent": "That is it every single iteration.",
                    "label": 0
                },
                {
                    "sent": "Let's solve a system of linear equations in AX.",
                    "label": 0
                },
                {
                    "sent": "Then what happens?",
                    "label": 1
                },
                {
                    "sent": "Well then, after K iterations we have multiples of Lambda I to the minus K. And now our analysis is back just like it was for finding the largest eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "If you want to get rid of all the eigenvalues that are more than epsilon times the second smallest that happens after Logn over epsilon iterations.",
                    "label": 0
                },
                {
                    "sent": "The only problem is the operation that we are doing at every single step is more expensive.",
                    "label": 0
                },
                {
                    "sent": "Before I said you do the power method to multiply by matrix, here we have to solve a system of linear equations in the matrix.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, it turns out we can solve.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those systems of linear equations very quickly, and some of you have seen me give talks about how to do this.",
                    "label": 0
                },
                {
                    "sent": "I promised today will be a talk about a completely different way of doing it.",
                    "label": 0
                },
                {
                    "sent": "OK, so but I may as well remind you about the results, so we got nearly linear time algorithms for this about a decade ago.",
                    "label": 0
                },
                {
                    "sent": "Shanghai Tang and I did this.",
                    "label": 0
                },
                {
                    "sent": "We got algorithms which should get us an epsilon.",
                    "label": 0
                },
                {
                    "sent": "Approximate solution of these linear equations, but others see is some very large constant that was on the log in initially about 100 or really big breakthrough was when Kudis, Miller and Pang got this algorithm down to time.",
                    "label": 0
                },
                {
                    "sent": "Basically order EM Logn log one over epsilon and what I want to point out about these two results as they use.",
                    "label": 0
                },
                {
                    "sent": "Two techniques.",
                    "label": 0
                },
                {
                    "sent": "These two graph theory primitives, low stretch spanning trees, and sparse ifers.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about sparse ifers later.",
                    "label": 0
                },
                {
                    "sent": "The Curtis Miller and Pang improve their results by really emphasizing low start spanning trees and using very showing that instead of sparsification, they could basically is very crude random sampling and that got us a great result.",
                    "label": 0
                },
                {
                    "sent": "And I got today.",
                    "label": 0
                },
                {
                    "sent": "At that point I thought we were done.",
                    "label": 0
                },
                {
                    "sent": "'cause you know, M login is incredible.",
                    "label": 0
                },
                {
                    "sent": "But then Cohen King Pachucki Pangen Row actually got this down to M root login.",
                    "label": 0
                },
                {
                    "sent": "Which is pretty amazing.",
                    "label": 0
                },
                {
                    "sent": "'cause if you think about it, that's less time than it would take you to sort the non zero entries in the matrix.",
                    "label": 0
                },
                {
                    "sent": "Although I gave a talk on mention that at some point someone yes but actually to solve linear equations you don't need full accuracy on the entries so.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a little bit of a cheat.",
                    "label": 0
                },
                {
                    "sent": "But that's an absurdly fast algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now I see.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To tell you, for those who haven't seen me say before, what do I mean by an approximate solution to a system of linear equations?",
                    "label": 0
                },
                {
                    "sent": "So what we should do to think about this is if I'm trying to solve LG X = B, then the solution is say alright is the inverse of the Laplacian, Times B we want a vector X that's close to that, so you want X to be close to the true solution in Norm.",
                    "label": 0
                },
                {
                    "sent": "How close do you want it to be for an epsilon approximate solution it should be at most epsilon times the true solution.",
                    "label": 0
                },
                {
                    "sent": "And some of you are wondering why do I have this funny norm here, so I'm not using the standard Euclidean norm.",
                    "label": 0
                },
                {
                    "sent": "I'm measuring the error in the matrix norm.",
                    "label": 0
                },
                {
                    "sent": "Which is just written this way and I'm not doing this to be complicated.",
                    "label": 0
                },
                {
                    "sent": "It turns out that for most purposes this is the norm in which you want to measure the error.",
                    "label": 0
                },
                {
                    "sent": "Meaning if you go back and look at our analysis of the inverse power method, you should measure the error in this norm.",
                    "label": 0
                },
                {
                    "sent": "If you want the right analysis of the behavior.",
                    "label": 0
                },
                {
                    "sent": "Later I'm going to show you what happens if you plug these approximate solvers into things like interior point methods.",
                    "label": 0
                },
                {
                    "sent": "Again, you don't get a decent analysis.",
                    "label": 0
                },
                {
                    "sent": "In less, you measure the error in this norm.",
                    "label": 0
                },
                {
                    "sent": "This is the right norm in which to measure the error.",
                    "label": 0
                },
                {
                    "sent": "It's also very conveniently the norm in which our algorithms give the right error.",
                    "label": 0
                },
                {
                    "sent": "So I mean, actually, most of the algorithms you see in numerical linear algebra naturally give you error that should be measured in this norm.",
                    "label": 0
                },
                {
                    "sent": "This is how we'll do it.",
                    "label": 0
                },
                {
                    "sent": "But the key point if you're not used to think about this is just an approximate solution is a vector X that is sort of epsilon close to the right solution.",
                    "label": 0
                },
                {
                    "sent": "And I just mentioned if you actually want to do this, there are two.",
                    "label": 0
                },
                {
                    "sent": "Piece of code that are pretty good out there right now.",
                    "label": 0
                },
                {
                    "sent": "Giannis Koutras has some nice code called the Combinatorial Multigrid.",
                    "label": 0
                },
                {
                    "sent": "There's also a lean algebraic multigrid.",
                    "label": 0
                },
                {
                    "sent": "Neither of these are algorithms.",
                    "label": 0
                },
                {
                    "sent": "Anyone is really proved theorems about, but they both work pretty well, and at least with solving systems of linear equations, you can check if you get the right answer.",
                    "label": 0
                },
                {
                    "sent": "So in the end, we don't worry as much about whether or not we can prove the algorithm is correct, and less like there is some really time critical thing that's going to blow up if you don't give the answer in the right amount of time and.",
                    "label": 0
                },
                {
                    "sent": "To the best of my knowledge, that is not the case for the policy and solvers yet.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so given that these algorithms are so fast, you might wonder why do I care about having others?",
                    "label": 0
                },
                {
                    "sent": "Well, there are few reasons.",
                    "label": 0
                },
                {
                    "sent": "One, we're interested in finding other algorithms.",
                    "label": 0
                },
                {
                    "sent": "Because there are other systems of linear equations we might want to solve other than laplacians and sort of the more algorithms that you know to handle.",
                    "label": 0
                },
                {
                    "sent": "One thing it's more likely that you can handle something else, so I was very excited when counter or reckless Infernet L'enjeu came up with a very very clean combinatorial algorithm for solving systems of linear equations in laplacian's that took time order, M log, square, dance that was even slower than Curtis Miller.",
                    "label": 0
                },
                {
                    "sent": "Pang, but.",
                    "label": 0
                },
                {
                    "sent": "It really is.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is very clean, it just uses low stretch spanning trees.",
                    "label": 0
                },
                {
                    "sent": "And some data structures.",
                    "label": 0
                },
                {
                    "sent": "And it's it's a very paper.",
                    "label": 0
                },
                {
                    "sent": "Sort of long, but the meat of it's contained in about 5 pages, and it's a very, very clean, simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "I think a lot of people have studied to try to understand what we can do better and how we might solve other systems.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, I'm going to take a completely different route.",
                    "label": 0
                },
                {
                    "sent": "Today.",
                    "label": 0
                },
                {
                    "sent": "I'm going to not talk about low stretch spanning trees.",
                    "label": 0
                },
                {
                    "sent": "Note I won't even define them and show you just how to solve systems of linear equations using graphs.",
                    "label": 0
                },
                {
                    "sent": "Parsa fires, which I'll tell you about and.",
                    "label": 0
                },
                {
                    "sent": "There are two advantages of these.",
                    "label": 0
                },
                {
                    "sent": "OK, so Richard Pang and I developed these algorithms and now with the intently is our most recent paper because we were trying to come up with algorithms we could execute in parallel or on distributed systems because, well, one of the main places people solve these systems in scientific computing where for them billions of equations is really not impressive.",
                    "label": 0
                },
                {
                    "sent": "That's their small cases.",
                    "label": 0
                },
                {
                    "sent": "So you know.",
                    "label": 0
                },
                {
                    "sent": "So I can't on a single processor solve the problems they care bout.",
                    "label": 0
                },
                {
                    "sent": "We need something we can do in parallel.",
                    "label": 0
                },
                {
                    "sent": "It turned out this will also give us at least once you accept sparsa fires.",
                    "label": 0
                },
                {
                    "sent": "Incredibly simple ways of understanding these nearly linear time algorithms an will also get us eventually.",
                    "label": 0
                },
                {
                    "sent": "The fastest algorithms that we know after some precomputation.",
                    "label": 0
                },
                {
                    "sent": "So just the same way with Gaussian elimination you compute the inverse of a matrix, and after that it's trivial to solve a system of linear equations.",
                    "label": 0
                },
                {
                    "sent": "By the inverse, it depends how many non zeros are in the inverse.",
                    "label": 0
                },
                {
                    "sent": "What will end up proving here is we have something that's essentially an approximate of the inverse with only a linear number of nonzeros.",
                    "label": 0
                },
                {
                    "sent": "So it will actually get us flat out linear time solvers after the precomputation.",
                    "label": 0
                },
                {
                    "sent": "And the pre computation will be nearly linear too.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's where we're going eventually.",
                    "label": 0
                },
                {
                    "sent": "Time for me to tell you about Laplacian matrices.",
                    "label": 0
                },
                {
                    "sent": "Finally tell you what the entries are, so we'll just get them by thinking about this Laplacian quadratic form.",
                    "label": 0
                },
                {
                    "sent": "So what we have in the Laplacian quadratic form is a sum over edges of the square of a difference across an edge.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I want to write XI minus XJ and matrix notation.",
                    "label": 0
                },
                {
                    "sent": "I'll make a vector out of XI and XJ and multiply it by vector.",
                    "label": 0
                },
                {
                    "sent": "That's one and minus one.",
                    "label": 0
                },
                {
                    "sent": "OK, now if I want to take the square, you could just put.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parentheses around that and take the square, but let's not do that.",
                    "label": 0
                },
                {
                    "sent": "We multiply this by its transpose, so write the row vector with X on the left.",
                    "label": 0
                },
                {
                    "sent": "Put the 1 -- 1 in a column, multiply it by the row input XIXJ there.",
                    "label": 0
                },
                {
                    "sent": "And if you then multiply these two together, this is the direction that it gives you a matrix, not a real number.",
                    "label": 0
                },
                {
                    "sent": "And this is the matrix that you get.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I had a graph where it only had one edge between vertex I and vertex J, it would basically look like this little two by two matrices would look like this little two by two matrix, but you need to put in a bunch of zeros for all the other vertices.",
                    "label": 0
                },
                {
                    "sent": "So shift I and J, you know, shift the one and minus one to positions inj.",
                    "label": 0
                },
                {
                    "sent": "You can build up a Laplacian by adding one such matrix for every edge.",
                    "label": 0
                },
                {
                    "sent": "You just have to pad with zeros appropriately.",
                    "label": 0
                },
                {
                    "sent": "So for example here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a graph I'll use in many examples, and here is the Laplacian matrix corresponding to it and the edge between vertex two and six here corresponds to this circled minus one here.",
                    "label": 1
                },
                {
                    "sent": "Between that's column 6 and row two to represent vertex two and six.",
                    "label": 0
                },
                {
                    "sent": "So again, This is why I usually deal with the quadratic form more than the matrix.",
                    "label": 0
                },
                {
                    "sent": "The Matrix is a little less informative to me.",
                    "label": 0
                },
                {
                    "sent": "But the main things to observe here is the matrix is symmetric.",
                    "label": 0
                },
                {
                    "sent": "The off diagonals are non positive and it is something we call diagonally dominant.",
                    "label": 0
                },
                {
                    "sent": "I will mention once in awhile that means the diagonal entries are at least as big as the sum of the absolute values of the entries in the row and column.",
                    "label": 0
                },
                {
                    "sent": "So if you go across any row, said 3 minus ones here, the diagonal is 3 that compensates.",
                    "label": 0
                },
                {
                    "sent": "We can also handle and it will sometimes come up matrices like this where the diagonals are strictly bigger, so as long as the diagonals are strictly bigger than the sum of the entries in the row and columns that we call symmetric, diagonally dominant and solving those is systems and those equivalent to solving systems in laplacians.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I need another way of writing the Laplacian for one of the results I'm going to present so one way again, I said you can think about the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "In this case, talk about a weighted graph.",
                    "label": 0
                },
                {
                    "sent": "My weights will always be non negative.",
                    "label": 0
                },
                {
                    "sent": "I will not have positive weights there.",
                    "label": 0
                },
                {
                    "sent": "WIJ, because I want my matrices deposit be positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "In this case, I can take the Laplacian and I write it as a sum of elementary laplacian's, one for each edge again.",
                    "label": 0
                },
                {
                    "sent": "So for me, LIJ is just the matrix that gets me NJJ times await.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to use the important fact that Elijay can be written as an outer product of two vectors in which I drew earlier.",
                    "label": 0
                },
                {
                    "sent": "I'll call it BIJ for me BIJ is the vector corresponding to edge IJ, and it has zeros everywhere.",
                    "label": 0
                },
                {
                    "sent": "But as a one in the coordinate corresponding to Vertex I Anna minus one in the coordinate corresponding to vertex J.",
                    "label": 0
                },
                {
                    "sent": "So you could write it as EI minus CJ, where those are the elementary unit factors.",
                    "label": 0
                },
                {
                    "sent": "For me it will be important that I can write the Laplacian is the sum of outer products of these.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That eventually gives us actually this form for the applausi, and that we often right.",
                    "label": 0
                },
                {
                    "sent": "In the middle here I have this.",
                    "label": 0
                },
                {
                    "sent": "I've written it out as BWB transpose so W should be a diagonal matrix with the edge weights.",
                    "label": 0
                },
                {
                    "sent": "B is what we call the signed edge vertex adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "So the rows.",
                    "label": 0
                },
                {
                    "sent": "If I'm doing this right.",
                    "label": 0
                },
                {
                    "sent": "Oh darn, did I do it the wrong way?",
                    "label": 0
                },
                {
                    "sent": "The columns.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, I wrote rose.",
                    "label": 0
                },
                {
                    "sent": "I might have written the transpose on the wrong side.",
                    "label": 0
                },
                {
                    "sent": "I hope you'll excuse me.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "If I did it by rows and yet every single row should have an entry for each edge.",
                    "label": 0
                },
                {
                    "sent": "But I might have done columns.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I can.",
                    "label": 0
                },
                {
                    "sent": "I can't even check in my head whether I got the rows or columns edit actually have to draw it and do the outer product.",
                    "label": 0
                },
                {
                    "sent": "Maybe someone will tell me which it should have been OK, but the key point is those B vectors appear in this matrix B.",
                    "label": 0
                },
                {
                    "sent": "You'll see in a moment why I want this.",
                    "label": 0
                },
                {
                    "sent": "One thing that will seem very odd to you perhaps is that I had a graph and when I formed this signed edge vertex adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "You know I had to assign one end of an edge, one endpoint of an edge of minus one and one endpoint to one.",
                    "label": 0
                },
                {
                    "sent": "And it seems like an arbitrary choice.",
                    "label": 0
                },
                {
                    "sent": "And the reason I didn't care on the previous slide.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is always multiplying by the transpose, so assign didn't matter.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, in a moment we're going to look at some results for directed graphs, and in that case I guess it's just useful to remember that you can put directions in by making the right choice of signs.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I want to tell you about one more applications of laplacian's before I start telling you about algorithms for solving linear equations in them.",
                    "label": 0
                },
                {
                    "sent": "So it turns out.",
                    "label": 0
                },
                {
                    "sent": "That if you're trying to solve linear programming problems on graphs.",
                    "label": 0
                },
                {
                    "sent": "And then you're trying to solve the linear programming problem by an interior point method.",
                    "label": 0
                },
                {
                    "sent": "The work the interior point methods are doing are usually just solving systems of linear equations in Laplacian matrices of graphs.",
                    "label": 1
                },
                {
                    "sent": "Or you can reduce it to that.",
                    "label": 0
                },
                {
                    "sent": "It's not always immediately obvious.",
                    "label": 0
                },
                {
                    "sent": "So Sam Daitch and I wrote a paper gosh, more than a couple years back about doing this for maximum flow problems and minimum cost flow problems.",
                    "label": 0
                },
                {
                    "sent": "Ann, I didn't realize it was a really general phenomenon until recently, so Sushant Sachdeva will give you give a talk on Monday in which you will briefly mentioned where we use this for doing regularization in something that we call Lipschitz learning.",
                    "label": 0
                },
                {
                    "sent": "But what I would like to explain to you is the cleanest example of this that I know by Rasmus King.",
                    "label": 0
                },
                {
                    "sent": "Anoop Rowan Sushan such Davidson paper that just appeared on archive.",
                    "label": 0
                },
                {
                    "sent": "I trust a night or two ago now for the problem called isotonic regression.",
                    "label": 0
                },
                {
                    "sent": "OK, now I didn't know about isotonic regression until these guys started working on it, but can I get no answer?",
                    "label": 0
                },
                {
                    "sent": "How many of you encountered isotonic regression before?",
                    "label": 0
                },
                {
                    "sent": "OK, a fair number?",
                    "label": 0
                },
                {
                    "sent": "OK, that's good, but for the rest I will give an explanation.",
                    "label": 0
                },
                {
                    "sent": "OK, it comes up in it.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Couple of situations.",
                    "label": 0
                },
                {
                    "sent": "Here's one that I like.",
                    "label": 0
                },
                {
                    "sent": "1st to define it I should tell you that it has a directed graph.",
                    "label": 0
                },
                {
                    "sent": "It should be a directed acyclic graph defines the problem.",
                    "label": 0
                },
                {
                    "sent": "And I show you that a function we say on the vertices is isotonic with respect to the directed acyclic graph.",
                    "label": 0
                },
                {
                    "sent": "If the function is increasing on edges.",
                    "label": 0
                },
                {
                    "sent": "So when you follow a directed edge, the function only increases, so actually quite accidentally at first the labels, the numbers that I chose for these vertices are an isotonic function with respect to this directed acyclic graph.",
                    "label": 0
                },
                {
                    "sent": "However, if I take that vertex that I labeled three, and I say now label it.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seven and I have a function that's not isotonic with respect to the graph, because those two edges are now decreasing.",
                    "label": 0
                },
                {
                    "sent": "They should have been increasing.",
                    "label": 0
                },
                {
                    "sent": "OK, so the problem in isotonic regression is I give you some function that's not necessarily isotonic with respect to the graph, and you want to find the closest isotonic function.",
                    "label": 0
                },
                {
                    "sent": "In some norm, we'll worry about the norm later different.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Forms give you different problems.",
                    "label": 0
                },
                {
                    "sent": "But again, the idea is I give you some function on the vertices of the graph.",
                    "label": 0
                },
                {
                    "sent": "It is not isotonic, you want to find the closest function that is OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why might you want to do that?",
                    "label": 0
                },
                {
                    "sent": "Here's an example that I like.",
                    "label": 0
                },
                {
                    "sent": "There are many others.",
                    "label": 0
                },
                {
                    "sent": "I'm sure many of you will know.",
                    "label": 0
                },
                {
                    "sent": "Imagine you're trying to reconstruct some events, maybe for some fun crime TV show and you have some estimates of among when many of them have happened, but you know your estimates are noisy.",
                    "label": 0
                },
                {
                    "sent": "But you also know that certain things had to happen before others.",
                    "label": 0
                },
                {
                    "sent": "So if you know that certain things had to happen for others, those give you precedence constraints which gives you directed edges.",
                    "label": 0
                },
                {
                    "sent": "So think of every node is one of these events.",
                    "label": 0
                },
                {
                    "sent": "It's labeled with an estimate of when it happened and given the labels of when you think they happened and the president's events, and then the precedence constraints, try to find the most likely reasonable timeline.",
                    "label": 0
                },
                {
                    "sent": "That's one sort of example.",
                    "label": 0
                },
                {
                    "sent": "We're actually where I think we're fairly general graph is reasonable.",
                    "label": 0
                },
                {
                    "sent": "Alot of the other examples actually come from low dimensional datasets where you know some relation between your input output function that you're trying to.",
                    "label": 0
                },
                {
                    "sent": "Recover from some data.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turn this into a linear programming problem and then develop a very fast.",
                    "label": 0
                },
                {
                    "sent": "A very fast interior point method for it so it King round Sachdeva do well.",
                    "label": 0
                },
                {
                    "sent": "OK, first I'm going to consider just doing this in the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "'cause that's where we're going to get a linear programming problem.",
                    "label": 0
                },
                {
                    "sent": "I should mention in their paper they have the first fast algorithm.",
                    "label": 0
                },
                {
                    "sent": "You have the fastest algorithm for doing this in the L1 norm, and they can do it in any other P norm for P bigger than one, and the running comes about order M to the three halves, which is much faster than anything that was known before.",
                    "label": 0
                },
                {
                    "sent": "OK so today.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But let's write.",
                    "label": 0
                },
                {
                    "sent": "This is a linear program.",
                    "label": 0
                },
                {
                    "sent": "We'll do it using the signed edge vertex adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "So here is the signed edge vertex adjacency matrix for this graph.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I definitely had the transposes in the wrong place before.",
                    "label": 0
                },
                {
                    "sent": "Sorry, OK, so.",
                    "label": 0
                },
                {
                    "sent": "The way to understand this is for every single edge we have one row in this matrix, for example, the edge from node one.",
                    "label": 0
                },
                {
                    "sent": "Node 4 is that circled row and here which node gets the one in which node gets the minus one is very important.",
                    "label": 0
                },
                {
                    "sent": "But we have one row for every single edge in the graph and now function is isotonic precisely if B * X is less than or equal to 0 componentwise.",
                    "label": 0
                },
                {
                    "sent": "So isotonic is just you multiply the vector by the signed edge vertex adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "Every entry is less than zero or less than or equal to 0, so this is what are.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Linear program would look like now.",
                    "label": 0
                },
                {
                    "sent": "For isotonic regression, we want to minimize the one norm of X -- Y.",
                    "label": 1
                },
                {
                    "sent": "Subject to BX being less than or equal to 0.",
                    "label": 0
                },
                {
                    "sent": "OK, that looks a lot like a linear program, but it's not quite standard form because one norms aren't things you usually put into a linear program.",
                    "label": 0
                },
                {
                    "sent": "So to fix that, what we do is we introduce some extra variables, one for every single chord.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll introduce Max variables I call RI, and now we're trying to do is minimize the sum of the R eyes.",
                    "label": 0
                },
                {
                    "sent": "Subject to the absolute value of each XI minus Yi being less than or equal to RI.",
                    "label": 0
                },
                {
                    "sent": "OK, that's closer to standard form.",
                    "label": 0
                },
                {
                    "sent": "We have to introduce a bunch of new variables, but still standard form for linear programs doesn't let you put in absolute values.",
                    "label": 0
                },
                {
                    "sent": "So we just turned that into two inequality's.",
                    "label": 0
                },
                {
                    "sent": "One is that X -- y is it.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After assessing equal to R and the opposite of status lesson equal to and now this is a linear programming problem in standard form.",
                    "label": 0
                },
                {
                    "sent": "So we can write it with matrices.",
                    "label": 0
                },
                {
                    "sent": "It really is minimized with some of our eyes subject to this matrix.",
                    "label": 0
                },
                {
                    "sent": "Here times RX being less than or equal to that vector.",
                    "label": 0
                },
                {
                    "sent": "If you take a look at what an interior point method does when given a linear programming problem like this.",
                    "label": 0
                },
                {
                    "sent": "You see that it solves a series of systems of linear equations of this form.",
                    "label": 0
                },
                {
                    "sent": "So the key thing to note here is.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The matrix I drew is there.",
                    "label": 0
                },
                {
                    "sent": "Its transpose is here and in the middle is a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "All the entries on the diagonal are non negative.",
                    "label": 0
                },
                {
                    "sent": "Actually they should be positive really.",
                    "label": 0
                },
                {
                    "sent": "And I've written them in blocks, S0S1 and S2.",
                    "label": 0
                },
                {
                    "sent": "This is what interior point methods do.",
                    "label": 0
                },
                {
                    "sent": "I don't care if you're looking at afine.",
                    "label": 0
                },
                {
                    "sent": "Potential prime will do or whatever.",
                    "label": 0
                },
                {
                    "sent": "They're all basically solving systems of linear equations like this, with a little decoration around it, which can actually significantly affect the runtime.",
                    "label": 0
                },
                {
                    "sent": "Well, we're going to be able to solve systems of linear equations like this by solving systems of linear equations in the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "To see why, multiply it out.",
                    "label": 0
                },
                {
                    "sent": "So if you multiply this out, what we get is, well, OK, it's a two by two block matrix.",
                    "label": 0
                },
                {
                    "sent": "One block contains the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Because it is B transpose.",
                    "label": 0
                },
                {
                    "sent": "These signed at Vertex adjacency matrix and S 0.",
                    "label": 0
                },
                {
                    "sent": "That diagonal is now giving you the weights on the edges times be again, but the key point is that is a Laplacian in the weights come from this SO.",
                    "label": 0
                },
                {
                    "sent": "OK, we're adding to it some diagonal matrices, but they are non negative diagonals you add to it.",
                    "label": 0
                },
                {
                    "sent": "This S1 and S2.",
                    "label": 0
                },
                {
                    "sent": "Those are positive.",
                    "label": 0
                },
                {
                    "sent": "I told you that we can solve symmetric diagonally dominant systems so that lower block is symmetric, diagonally dominant.",
                    "label": 0
                },
                {
                    "sent": "That's good.",
                    "label": 0
                },
                {
                    "sent": "But it's a bigger matrix than just the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "There's also a diagonal matrix up here and some diagonals there, but you think you have a block matrix.",
                    "label": 0
                },
                {
                    "sent": "There are a few diagonals, and the rest is Laplacian.",
                    "label": 0
                },
                {
                    "sent": "You should be able to solve that, and you're right.",
                    "label": 0
                },
                {
                    "sent": "You can reduce this to solving Laplacian.",
                    "label": 0
                },
                {
                    "sent": "There are many ways of thinking about it.",
                    "label": 0
                },
                {
                    "sent": "The 1st way of thinking about it just say, well, the first block of variables I can eliminate if I eliminate which correspond to where I've got some diagonal matrices up there and there, and after I eliminate them, I turn this into something like a Laplacian and that's true.",
                    "label": 0
                },
                {
                    "sent": "What, how?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Happens when you're doing elimination.",
                    "label": 0
                },
                {
                    "sent": "I want to remind you of.",
                    "label": 0
                },
                {
                    "sent": "What you are really doing is you are really rewriting your matrix.",
                    "label": 0
                },
                {
                    "sent": "In this form.",
                    "label": 0
                },
                {
                    "sent": "You're rewriting it as a block diagonal matrix where you multiplied by an upper triangular matrix on the right and a lower triangular matrix on the left, and they're the same.",
                    "label": 0
                },
                {
                    "sent": "So there's a U transpose into you on either side.",
                    "label": 0
                },
                {
                    "sent": "And what is that?",
                    "label": 0
                },
                {
                    "sent": "You look like?",
                    "label": 0
                },
                {
                    "sent": "It looks like identities and then actually in this case a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "So if you want to solve systems of linear equations in this whole thing, what do you need to do?",
                    "label": 0
                },
                {
                    "sent": "You solve a system of linear equations and you.",
                    "label": 0
                },
                {
                    "sent": "A system in this and a system in you transpose and as you can guess you this is diagonal that's dying last diagonal.",
                    "label": 0
                },
                {
                    "sent": "That's trivial to solve systems of linear equations in.",
                    "label": 0
                },
                {
                    "sent": "That's a simple linear time operation.",
                    "label": 0
                },
                {
                    "sent": "This matrix, well, that's it's a diagonal you can solve in that this is symmetric, diagonally dominant.",
                    "label": 0
                },
                {
                    "sent": "You can solve in that because I told you have algorithms for that that are very fast.",
                    "label": 0
                },
                {
                    "sent": "And then you just end up on doing a solving you.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you learn is that you can solve these systems of linear equations very very quickly by reducing to laplacian's, and so Sushant will only get to mention briefly a similar idea in his talk on Monday.",
                    "label": 0
                },
                {
                    "sent": "But where you can also do things like this.",
                    "label": 0
                },
                {
                    "sent": "So a lot of these things with a little bit of work you can turn into solving systems, equations and laplacian's.",
                    "label": 0
                },
                {
                    "sent": "And but there is 1.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you need to keep track of K. What is the accuracy in the solves that you need?",
                    "label": 0
                },
                {
                    "sent": "Because we're using our Laplacian solvers, we're usually doing approximate solves.",
                    "label": 0
                },
                {
                    "sent": "Well, actually these guys proved in this paper that in this case they only need a constant accuracy in every solve even doesn't depend on the demand, doesn't depend on the dimension, doesn't depend on the problem.",
                    "label": 0
                },
                {
                    "sent": "Constant absolute accuracy.",
                    "label": 0
                },
                {
                    "sent": "When you're measuring error in the matrix norm is sufficient.",
                    "label": 0
                },
                {
                    "sent": "My work with Sam Dyson was much, much cruder.",
                    "label": 0
                },
                {
                    "sent": "It was polynomial accuracy, but we weren't.",
                    "label": 0
                },
                {
                    "sent": "We weren't nearly as ambitious back then.",
                    "label": 0
                },
                {
                    "sent": "OK, so that at least tells you where some idea of how Laplacian's come up, implying interior Point methods to solve linear programming problems.",
                    "label": 0
                },
                {
                    "sent": "I'd now like to talk about something complete.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different so if you were asleep in jet lag you can come back now and you probably don't worry bout what you missed.",
                    "label": 0
                },
                {
                    "sent": "I want to talk about graph sparsification and how we will use it to quickly solve systems of linear equations in Laplacian matrices.",
                    "label": 0
                },
                {
                    "sent": "So sparsification.",
                    "label": 0
                },
                {
                    "sent": "Is the problem of approximating an arbitrary graph by a sparse graph?",
                    "label": 0
                },
                {
                    "sent": "We want to graph a few edges.",
                    "label": 0
                },
                {
                    "sent": "Let me begin by defining what this approximation is.",
                    "label": 0
                },
                {
                    "sent": "I will say that a graph H is an epsilon approximation of a graph G if their Laplacian quadratic forms are always approximately the same.",
                    "label": 0
                },
                {
                    "sent": "So the ratio of the quadratic form should always be between one plus epsilon and 1 / 1 plus epsilon for all vectors X.",
                    "label": 0
                },
                {
                    "sent": "So this is equivalent to saying things like that the Laplacian of H is less than or equal to 1 plus epsilon times Laplacian of G in the partial order in sort of the loevner partial order.",
                    "label": 0
                },
                {
                    "sent": "That means that this matrix minus that matrix is positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "It's the same thing as saying that of course anymore the other direction is that.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The 1 / 1 plus epsilon times LG is less than or equal to LH.",
                    "label": 0
                },
                {
                    "sent": "So you can also talk about this in terms of positive semidefinite matrices.",
                    "label": 0
                },
                {
                    "sent": "It's a very strong notion of approximation.",
                    "label": 0
                },
                {
                    "sent": "So remember when we were looking at the boundaries of sets?",
                    "label": 0
                },
                {
                    "sent": "They were given by evaluating the Laplacian quadratic form.",
                    "label": 0
                },
                {
                    "sent": "So this tells you that if one graph approximates another, then for every single set of vertices, the sizes of the boundaries are approximately the same in both graphs.",
                    "label": 0
                },
                {
                    "sent": "It also tells you that spectral properties of the two laplacian's are approximately the same.",
                    "label": 0
                },
                {
                    "sent": "It also tells you that the inverse is.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case, pseudo inverses of the Laplacian matrices satisfy the same relation.",
                    "label": 0
                },
                {
                    "sent": "This follows pretty immediately from the previous line pretty neatly.",
                    "label": 0
                },
                {
                    "sent": "Just you can multiply through by matrices, and this doesn't change.",
                    "label": 0
                },
                {
                    "sent": "So this means that the solutions to linear equations in one matrix are approximately the same if the other matrix approximates it.",
                    "label": 0
                },
                {
                    "sent": "So you can use this to come up with ways of solving systems of linear.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asians?",
                    "label": 0
                },
                {
                    "sent": "So in particular, if H is an epsilon approximation of G. If you get and if you actually exactly solve a system of linear equations in age, so let X be the solution to the Laplacian.",
                    "label": 1
                },
                {
                    "sent": "H * X is equal to B.",
                    "label": 0
                },
                {
                    "sent": "Then it's an epsilon approximate solution to the same equations in G. Where we have to be very careful when about how we define an approximate solution to a system of linear equations.",
                    "label": 0
                },
                {
                    "sent": "Again, This is why I defined it in terms of the matrix norm.",
                    "label": 0
                },
                {
                    "sent": "It means that when I approximate the matrices, I get an approximate solution to the system of linear equations.",
                    "label": 0
                },
                {
                    "sent": "And if you were actually trying to understand the error that you would get if instead of solving the system and H exactly, I solved the system in H approximately again, you would measure in these norms and adjust composes.",
                    "label": 0
                },
                {
                    "sent": "You just add the two errors.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This actually I told you now that if you can solve systems in H then you can solve systems in G. But you can do better than that, so I told you in the previous slide that is solution to system and H gives you an epsilon accurate solution to system in G. You can drive that precision to be as high as you want by doing it again and again and again.",
                    "label": 0
                },
                {
                    "sent": "So this is the algorithm people often called iterative refinement.",
                    "label": 0
                },
                {
                    "sent": "You know what you do is you first get your initial guess at a solution.",
                    "label": 0
                },
                {
                    "sent": "Say why to solve the system for why.",
                    "label": 0
                },
                {
                    "sent": "Where are initially set to be.",
                    "label": 0
                },
                {
                    "sent": "And then what you do is you update X.",
                    "label": 0
                },
                {
                    "sent": "You sort of computer residual.",
                    "label": 0
                },
                {
                    "sent": "You take a look at how much error you're making and you re solve again.",
                    "label": 0
                },
                {
                    "sent": "And if you keep doing this iteratively after the case iteration, you drive the error in your solution down to epsilon to the K. So once you can get approximate solutions, you can get highly accurate solutions.",
                    "label": 0
                },
                {
                    "sent": "If I had another 5 minutes or if I was assigning homework, I'd point out that this is really the same as the analysis of the power method we did earlier, or a translation of it if you want.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if we can spar sofy then we can solve systems of equations, assuming you can solve systems of linear equations approximation.",
                    "label": 0
                },
                {
                    "sent": "So here are two theorems that I want to refer to.",
                    "label": 0
                },
                {
                    "sent": "About sparse approximations of graphs, the first is sort of the best existence result that we know right now.",
                    "label": 0
                },
                {
                    "sent": "It was joint work with Josh Batson and Nikhil Shrivastav.",
                    "label": 0
                },
                {
                    "sent": "So we proved that every graph G has an epsilon approximation with relatively few edges.",
                    "label": 0
                },
                {
                    "sent": "If you want an epsilon approximation, the number of edges is number of vertices times.",
                    "label": 0
                },
                {
                    "sent": "What did I write there?",
                    "label": 0
                },
                {
                    "sent": "Yes, two plus epsilon squared divided by epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "So you know there's nothing hidden here.",
                    "label": 0
                },
                {
                    "sent": "This is the best that we know exists.",
                    "label": 0
                },
                {
                    "sent": "And by the way, I'm just demonstrating this by the complete graph on 10 vertices is pretty well approximated by the Petersen graph.",
                    "label": 0
                },
                {
                    "sent": "Is the right way of thinking of it, but where I've blown up the weights on the edges 'cause what you do is you get rid of some of these edges.",
                    "label": 0
                },
                {
                    "sent": "You have to increase the weights on the others, otherwise it wouldn't be an approximation.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that's an existence result.",
                    "label": 0
                },
                {
                    "sent": "It's a polynomial time algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you want to do it really quickly right now, the algorithms we know work by random sampling, and the simplest of these one that Nikhil Srivastava and I came up with where will get about N log N over epsilon squared edges.",
                    "label": 0
                },
                {
                    "sent": "So we're going to lose a log in.",
                    "label": 0
                },
                {
                    "sent": "I don't want to lose the log in just to be clear, I want to do this on graphs with a billion nodes where login is sort of painful even, but we can do it quickly and the way you do it is you solve system, just randomly sample edges and if you include them then you increase their weight.",
                    "label": 0
                },
                {
                    "sent": "An you sample them with probability.",
                    "label": 0
                },
                {
                    "sent": "Well, it's equal to their effective resistance in the graph.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to define that, but basically I'll mention you can get it.",
                    "label": 0
                },
                {
                    "sent": "You can compute the sampling probability for an edge by solving a system of linear equations in Laplacian, but it turns out you can do this all pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "So we had a nearly linear time algorithm.",
                    "label": 0
                },
                {
                    "sent": "Did I leave?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slide is going to say kudis.",
                    "label": 0
                },
                {
                    "sent": "Levin and Pang got this down to time like N log squared N. Maybe that's on a later slide I've suddenly forgotten.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, but why don't I tell you that there's been some new breakthroughs on this?",
                    "label": 0
                },
                {
                    "sent": "Where it sort of combine the two so this top paper was just presented at stock last week where they sort of combine that they come up with a nice way of basically combining those two results to get a sampling based algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which is actually pretty efficient, so it gets you a linear in N edges, which is the right order of magnitude where there's a number Q there, which shows you how many edges you get in the bigger it is the faster algorithm.",
                    "label": 0
                },
                {
                    "sent": "So they got it down to time like MN times the end of the 1 / Q, which you should think of his small.",
                    "label": 0
                },
                {
                    "sent": "This was the fastest algorithm we knew for getting that kind of sparse a fire at the time.",
                    "label": 0
                },
                {
                    "sent": "And then in Tetley and his son just have a paper that I don't know if it's public eppers just announced it was accepted to Fox Isop repent.",
                    "label": 0
                },
                {
                    "sent": "It looks very nice where they actually get this down to a nearly linear time algorithm building on the techniques of the previous paper.",
                    "label": 0
                },
                {
                    "sent": "So this is actually now sparsa fires that are in the linear size regime in time.",
                    "label": 0
                },
                {
                    "sent": "Basically, order M * N to something pretty small.",
                    "label": 0
                },
                {
                    "sent": "Now, OK, there's a bit of a tradeoff between the exponent on end and the number of edges.",
                    "label": 0
                },
                {
                    "sent": "But it's huge progress on this.",
                    "label": 0
                },
                {
                    "sent": "Both of these require solving Laplacian linear equations, which can pose a problem because what I want to do is I want to use parser fires to solve Laplacian linear equations, and it's not clear I can do that if I need to solve Laplacian linear equations to build my sparsa fires in less, we get some sort of fortunate induction.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, here's where I mentioned the results of Curtis Miller and thanks so they actually showed again using the best Laplacian solvers, but essentially solving smaller systems that they could get the sparse ifers like Nyquil and I built.",
                    "label": 0
                },
                {
                    "sent": "But in time order M log squared in I want to mention there is a very interesting result which I don't think yet has found its killer app by this sort of.",
                    "label": 0
                },
                {
                    "sent": "I called the dream team from MIT Co only Musco, Musco, pangan sidford.",
                    "label": 0
                },
                {
                    "sent": "There a few other people there even working on such things where they came up with some very cool ways to do some.",
                    "label": 0
                },
                {
                    "sent": "Very crude random sampling on smaller systems.",
                    "label": 0
                },
                {
                    "sent": "To get effective sparsification this is interesting to me because it looks like the sort of thing you could doesn't require laplacian's at all.",
                    "label": 0
                },
                {
                    "sent": "So you can apply it on very general linear systems, so it's worth looking at.",
                    "label": 0
                },
                {
                    "sent": "I won't say too much more about it.",
                    "label": 0
                },
                {
                    "sent": "One result that I want to use because what I was very interested in getting into this was how can we?",
                    "label": 0
                },
                {
                    "sent": "Build solve systems of linear equations in parallel is a result of Giannis Kutis, which build sparse ifers in nearly linear time, logarithmically polylogarithmic parallel time.",
                    "label": 0
                },
                {
                    "sent": "By using graph spanners, is a primitive and that's something we can use.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me not rattle off results about sparse ifers anymore.",
                    "label": 0
                },
                {
                    "sent": "Let me tell you how you can use them to solve systems of linear equations.",
                    "label": 0
                },
                {
                    "sent": "I'll give you 2 algorithms.",
                    "label": 0
                },
                {
                    "sent": "The first will be the simplest, so let's remember.",
                    "label": 0
                },
                {
                    "sent": "A fundamental result we all learned a long time ago.",
                    "label": 0
                },
                {
                    "sent": "The Power series expansion for 1 / 1 -- X.",
                    "label": 0
                },
                {
                    "sent": "1 / 1 -- X is equal to 1 + X + X ^2 + X ^3 and so on.",
                    "label": 0
                },
                {
                    "sent": "Well the same thing is true for matrices, at least in the symmetric case.",
                    "label": 0
                },
                {
                    "sent": "If I have a matrix X an, all the eigenvalues are less than one.",
                    "label": 0
                },
                {
                    "sent": "Then the inverse of the identity minus X is equal to I + X + X ^2 + X ^3, and so on.",
                    "label": 0
                },
                {
                    "sent": "You can check just by multiplying by I -- X that this is converging to the right thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can use that power series now.",
                    "label": 0
                },
                {
                    "sent": "How am I going to apply that to laplacian's?",
                    "label": 0
                },
                {
                    "sent": "One way of thinking about it is a Laplacian matrix.",
                    "label": 0
                },
                {
                    "sent": "The way we wrote it, it had sort of a minus an adjacency matrix and a diagonal term.",
                    "label": 0
                },
                {
                    "sent": "And if you re scale things, you can basically write it as I minus something and the norm of this thing will be less than one so.",
                    "label": 0
                },
                {
                    "sent": "At least orthogonal to the nullspace, so you can apply this formula to laplacian's after a slight rescaling.",
                    "label": 0
                },
                {
                    "sent": "This is the Power series expansion.",
                    "label": 0
                },
                {
                    "sent": "We're going to use to get a very fast algorithm for solving systems of linear equations in laplacians.",
                    "label": 0
                },
                {
                    "sent": "By the way, this is also what you're doing.",
                    "label": 0
                },
                {
                    "sent": "You're doing iterative refinement to improve solutions of systems of linear equations.",
                    "label": 0
                },
                {
                    "sent": "If you look at it the right way.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now you can ask yourself how many terms in this power series do you need before you get a good approximate solution.",
                    "label": 0
                },
                {
                    "sent": "And the answer is, it depends on the condition, number of the matrix X.",
                    "label": 0
                },
                {
                    "sent": "Basically, for our purposes, it's the ratio of the largest eigenvalue to the smallest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "After you take that many terms, the others start to go to 0.",
                    "label": 0
                },
                {
                    "sent": "And they have a negligible effect.",
                    "label": 0
                },
                {
                    "sent": "Now, unfortunately, that's more terms than I want, so I'm going to rewrite this power series in.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "End of writing it is this some let's write it as a product.",
                    "label": 0
                },
                {
                    "sent": "So what we'll do is rewrite it is the identity minus X.",
                    "label": 0
                },
                {
                    "sent": "Times identity minus X squared times identity minus X to the 4th put in X to every power of two here.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, you will see when you write that product you actually get all of those terms.",
                    "label": 0
                },
                {
                    "sent": "So every single term comes up once and you know the terms in here that contribute.",
                    "label": 0
                },
                {
                    "sent": "You get just by taking the you know the binary expansion of the exponent.",
                    "label": 0
                },
                {
                    "sent": "Now before where I said I needed to some Kappa terms to get a good approximation.",
                    "label": 0
                },
                {
                    "sent": "I'm in better shape, I only need to take a product of LGE Kappa terms to get the same thing.",
                    "label": 0
                },
                {
                    "sent": "And that's sort of the right regime.",
                    "label": 0
                },
                {
                    "sent": "I want to be in to be fast.",
                    "label": 0
                },
                {
                    "sent": "I want to be logarithmically the condition number, not linear in the condition number.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's our.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea for how we would solve a system of linear equations in the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Take a look at this product.",
                    "label": 0
                },
                {
                    "sent": "A vlog Kappa terms.",
                    "label": 0
                },
                {
                    "sent": "To solve a system of linear equations Opossum, we just want to multiply by each of those matrices.",
                    "label": 0
                },
                {
                    "sent": "OK, even if my initial matrix is sparse, these matrices very quickly get dense, but So what will sparsifying them?",
                    "label": 0
                },
                {
                    "sent": "That's my answer to everything.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Probably one day my kids will be eating sparsa fires for dinner or something.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "This is how we this is what we're going to do with everything.",
                    "label": 0
                },
                {
                    "sent": "OK, so you just want to sparsified those matrices.",
                    "label": 0
                },
                {
                    "sent": "And that should get you a fast algorithm.",
                    "label": 0
                },
                {
                    "sent": "There's only one caveat here, and this is where some of the work went.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is that sparsifying the terms in a product doesn't tell me anything about the product.",
                    "label": 0
                },
                {
                    "sent": "So sparsification the way I defined it using like the semidefinite partial order.",
                    "label": 0
                },
                {
                    "sent": "That's a notion for symmetric matrices.",
                    "label": 0
                },
                {
                    "sent": "And OK, all these matrices here are simultaneously diagonalizable.",
                    "label": 0
                },
                {
                    "sent": "But as soon as I approximate them and I take a product, I get things that are not symmetric and not simultaneously diagonalizable.",
                    "label": 0
                },
                {
                    "sent": "An it unfortunately turns out that if you give Me 2 matrices, A&B and arbitrarily good approximations of the Manhattan behat, you actually really learn nothing.",
                    "label": 0
                },
                {
                    "sent": "There's nothing nice I can say about the relationship between a B and a happy hat.",
                    "label": 0
                },
                {
                    "sent": "So while this was a really nice idea, it needs another idea to make it work.",
                    "label": 0
                },
                {
                    "sent": "Because again, sparsifying the terms in the product doesn't give me any useful guarantees about the product.",
                    "label": 0
                },
                {
                    "sent": "And what we need to do to fix this is symmetrise it somehow.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to do that, let me give you another way of deriving this expression.",
                    "label": 0
                },
                {
                    "sent": "So one way of seeing that the inverse of I -- X is equal to that is you can first check that the inverse of I -- X is equal to y + X times the inverse of I -- X ^2.",
                    "label": 0
                },
                {
                    "sent": "OK, and then if you apply that again you get I + X ^2 times the inverse of I -- X to the 4th and so on.",
                    "label": 0
                },
                {
                    "sent": "So if you use this formula I wrote up here, it's a way of re deriving this.",
                    "label": 0
                },
                {
                    "sent": "What we do is we just use a slightly different formula that's symmetric.",
                    "label": 0
                },
                {
                    "sent": "So it turns out you can also write I -- X inverse is equal to this thing.",
                    "label": 0
                },
                {
                    "sent": "OK, don't try to figure out why this minute just the important note.",
                    "label": 0
                },
                {
                    "sent": "Is it the I -- X ^2 term appears in the middle and it's symmetric and multiplied by and I + X on either side and when you have this formula then sparsification goes.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's what we do to solve a system of linear equations.",
                    "label": 0
                },
                {
                    "sent": "You need to multiply by I + X once in the beginning and once in the end 'cause you have them on left and right.",
                    "label": 0
                },
                {
                    "sent": "The term in the middle this I -- X ^2 the inverse of that we're going to sparsified that matrix the I -- X ^2.",
                    "label": 0
                },
                {
                    "sent": "Oh, sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "That call I -- X sub two and 5 -- X and two is approximately equal to minus X squared.",
                    "label": 0
                },
                {
                    "sent": "Then I can substitute it into this formula.",
                    "label": 0
                },
                {
                    "sent": "And when I submitted in not now we have symmetry.",
                    "label": 0
                },
                {
                    "sent": "And now you can prove that then you have actually an approximation.",
                    "label": 0
                },
                {
                    "sent": "So in this formula, when you approximate X2 X squared by X2, everything goes through.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to gloss over one detail about how you efficiently compute X2.",
                    "label": 0
                },
                {
                    "sent": "This parser fire because that is not the purpose of this talk today.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first purpose now is, once you actually verify this identity.",
                    "label": 0
                },
                {
                    "sent": "You now know a very fast way of saying if sparse ifers exist, then very fast ways of solving systems of linear equations in laplacians exist.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you a different way of building things quickly.",
                    "label": 0
                },
                {
                    "sent": "First, let me some.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guys, what does this get you?",
                    "label": 0
                },
                {
                    "sent": "So this gets you a nice parallel or distributed algorithm for solving equations in laplacians.",
                    "label": 0
                },
                {
                    "sent": "The depth of some exponent, some power of log N times the dimension in the condition number and then the work is nearly linear.",
                    "label": 0
                },
                {
                    "sent": "OK, this was a good first attempt.",
                    "label": 0
                },
                {
                    "sent": "There is one thing I want to criticize about this result.",
                    "label": 0
                },
                {
                    "sent": "OK, the power on log in that see I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's like 6.",
                    "label": 0
                },
                {
                    "sent": "It's not horrendous.",
                    "label": 0
                },
                {
                    "sent": "It's not anything you'd use actually.",
                    "label": 0
                },
                {
                    "sent": "Depending on the condition number is not always OK.",
                    "label": 0
                },
                {
                    "sent": "So if you just take an unweighted graph, its condition number cannot be too big.",
                    "label": 0
                },
                {
                    "sent": "If you take an unweighted graph and I take it to the policy and the condition number most end squared so you don't worry about it.",
                    "label": 0
                },
                {
                    "sent": "But if you're actually solving linear programming problems by an interior point method.",
                    "label": 0
                },
                {
                    "sent": "The matrices that you get at the end become very poorly conditioned.",
                    "label": 0
                },
                {
                    "sent": "So as you solve systems of linear equation, so if you solve he's running to your point methods.",
                    "label": 0
                },
                {
                    "sent": "The closer you get to convergence, the more poorly conditioned your matrices become.",
                    "label": 0
                },
                {
                    "sent": "As a matter of fact, for many, many years Matlab had a special version of a Cholesky factorization built into it just for the purpose of interior point methods that you could use that would handle the sort of errors that came up in this.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, they removed it.",
                    "label": 0
                },
                {
                    "sent": "Now some code doesn't work anymore is a couple of years ago, but.",
                    "label": 0
                },
                {
                    "sent": "The condition number eventually matter, so I'll give you something that won't dip.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the condition number, this is an algorithm we got under archive last week.",
                    "label": 0
                },
                {
                    "sent": "Now if something we're calling Sparsified Cholesky factorizations.",
                    "label": 0
                },
                {
                    "sent": "So what it will do is this.",
                    "label": 0
                },
                {
                    "sent": "You give me an arbitrary Laplacian or non negative symmetric backing dominatrix.",
                    "label": 0
                },
                {
                    "sent": "It will compute an upper triangular matrix you that's very sparse, order N nonzeros so that your original matrix M is approximated by U transpose EO.",
                    "label": 0
                },
                {
                    "sent": "OK, how many I get enough answer?",
                    "label": 0
                },
                {
                    "sent": "How many of you use to salesky factorization?",
                    "label": 0
                },
                {
                    "sent": "Again, many so those of you know, perhaps why people who do that usually write this, not you transpose you is LL transpose, but I'm not because elwer laplacian's all this talk.",
                    "label": 0
                },
                {
                    "sent": "And that's why I'm going to call this thing M Instead of L, just so the people who know toleski don't get confused.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, after you're done the factorization, we'll see how you do that.",
                    "label": 0
                },
                {
                    "sent": "You get this matrix you and once you have that you can solve systems in your Laplacian in time order M. Log one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "At least, and the depth is basically log M log epsilon inverse because it turns out you has a very nice structure as a block structure, so you can actually apply it quickly.",
                    "label": 0
                },
                {
                    "sent": "And we can form the factorization in nearly linear work in polylog depth.",
                    "label": 0
                },
                {
                    "sent": "I won't go into that as much, so let me just tell you.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This works.",
                    "label": 0
                },
                {
                    "sent": "So I should 1st remind you a few facts about what Cholesky factorization does.",
                    "label": 0
                },
                {
                    "sent": "The ordinary one, especially for those who aren't familiar with it.",
                    "label": 0
                },
                {
                    "sent": "First, you all learn this under the name Gaussian elimination.",
                    "label": 0
                },
                {
                    "sent": "Teleski is the version of Gaussian elimination you apply to symmetric matrices so that all the matrices you get are symmetric.",
                    "label": 0
                },
                {
                    "sent": "Along the way, and I'll show you an example, but the main thing that goes in your applying Cholesky factorization to Laplacian matrix.",
                    "label": 0
                },
                {
                    "sent": "The smaller matrices you get are also Laplacian matrices.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I take this matrix here and say eliminate graph, sorry eliminate node one.",
                    "label": 0
                },
                {
                    "sent": "It corresponds to putting a clique on the neighbors of node one, so I'll get a Laplacian of this graph where these new edges show up with some weights depending on the original weights.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can actually see what they were.",
                    "label": 0
                },
                {
                    "sent": "I'll do that in a moment.",
                    "label": 0
                },
                {
                    "sent": "The thing knows when you're done and you get this factorization into you transpose you where you as an upper triangular matrix.",
                    "label": 0
                },
                {
                    "sent": "And then you can solve systems in your matrix by doing 2 solves in a triangular matrix and those are fast.",
                    "label": 0
                },
                {
                    "sent": "They take time proportional to the number of nonzeros in them.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tricks?",
                    "label": 0
                },
                {
                    "sent": "OK, so let me remind you what salesky does.",
                    "label": 0
                },
                {
                    "sent": "It's like Gaussian elimination.",
                    "label": 0
                },
                {
                    "sent": "So what do you do?",
                    "label": 0
                },
                {
                    "sent": "You take your initial matrix here.",
                    "label": 0
                },
                {
                    "sent": "I didn't put in, I just put in that part of the matrix nodes 123 and four in this graph.",
                    "label": 0
                },
                {
                    "sent": "You apply some row operations to take multiples of the first row and subtract them off from the other rows that correspond to multiplying by this matrix.",
                    "label": 0
                },
                {
                    "sent": "It's a lower triangular matrix and you get something that looks like this.",
                    "label": 0
                },
                {
                    "sent": "And we've cleared the first row and column and put in some new entries in the other rows.",
                    "label": 0
                },
                {
                    "sent": "But this isn't symmetric, So what you're really doing is you're really multiplying by this matrix on either side.",
                    "label": 0
                },
                {
                    "sent": "Where did that go here?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I multiply by it there and there, and then I get something symmetric again.",
                    "label": 0
                },
                {
                    "sent": "And this is a Laplacian of a smaller matrix where you can see I've got the edge weights suggested.",
                    "label": 0
                },
                {
                    "sent": "Now these are not my matrices.",
                    "label": 0
                },
                {
                    "sent": "You I should say these are actually the inverse of U in U transpose.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The actual matrices you show up like this, really what you're doing.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Taking your initial matrix.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In your writing it as you transpose times this smaller Laplacian times you.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately for this matrix it's very easy to get the inverse.",
                    "label": 0
                },
                {
                    "sent": "Here's you.",
                    "label": 0
                },
                {
                    "sent": "It looked a lot like you inverse, just with different signs.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the important things to know here are that the number of non zero entries in you.",
                    "label": 0
                },
                {
                    "sent": "Is equals to the degree of the vertices when they were illuminated.",
                    "label": 0
                },
                {
                    "sent": "So when you eliminate a vertex, you take its degree.",
                    "label": 0
                },
                {
                    "sent": "That is what you get in you, and that happens for every single vertex, not in you in person every vertex.",
                    "label": 0
                },
                {
                    "sent": "OK, so clearly the way to get a you that sufficient and to make this fast is to try to get his few non zeros as possible and one of the main heuristics people have used for a very long time in the field is what's called an incomplete Cholesky factorization, where merging can vanderhorst basically suggested.",
                    "label": 0
                },
                {
                    "sent": "Throw away the small entries you get in you.",
                    "label": 0
                },
                {
                    "sent": "And throw away everything else we can.",
                    "label": 0
                },
                {
                    "sent": "We get some sort of approximation of the original.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to suggest now is very similar, but we're going to sparse if I so it's going to throw away entries carefully and increase their weights.",
                    "label": 0
                },
                {
                    "sent": "I think I have about a minute left.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or two, I can tell you how this works.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the idea.",
                    "label": 0
                },
                {
                    "sent": "We need to pick a bunch of vertices.",
                    "label": 0
                },
                {
                    "sent": "Let me call them S to eliminate.",
                    "label": 0
                },
                {
                    "sent": "So the way we're going to do that is OK. Once I do that, rewrite your matrix in blocks, so the vertices we eliminate go first.",
                    "label": 0
                },
                {
                    "sent": "T is the compliment.",
                    "label": 0
                },
                {
                    "sent": "When you eliminate them, what you are doing is you're writing your matrix as something that's lower triangular times a block diagonal times something that's upper triangular.",
                    "label": 0
                },
                {
                    "sent": "If the initial one was Laplacian, this thing is also Laplacian.",
                    "label": 0
                },
                {
                    "sent": "What we can do I should say, is if the initial thing is Laplacian, this new one is Laplacian, but denser so we sparsifying it.",
                    "label": 0
                },
                {
                    "sent": "And that is going to be the key of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "Choose a block of vertices to eliminate, get a new Laplacian sparsified.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's one catch.",
                    "label": 0
                },
                {
                    "sent": "You'll notice that MSS comes up often here.",
                    "label": 0
                },
                {
                    "sent": "And we need to multiply by its inverse there and there, and to solve a system of equations.",
                    "label": 0
                },
                {
                    "sent": "Here we need to multiply by its inverse.",
                    "label": 0
                },
                {
                    "sent": "So we need our set S to give us something where MSS is really easy to solve linear equations in.",
                    "label": 0
                },
                {
                    "sent": "The ideal would be an independent set of vertices in a graph.",
                    "label": 0
                },
                {
                    "sent": "If I don't get an independent set of vertices, MSS would be diagonal.",
                    "label": 0
                },
                {
                    "sent": "And then solving equations and it would be easy.",
                    "label": 0
                },
                {
                    "sent": "Can you find a large independent set of vertices in this graph?",
                    "label": 0
                },
                {
                    "sent": "Yes you can, because we sparsified, so there aren't too many edges, so there is a large independent set.",
                    "label": 0
                },
                {
                    "sent": "But not one big enough for the induction to work and for the error analysis to work.",
                    "label": 0
                },
                {
                    "sent": "So we need something a little bigger than independent set.",
                    "label": 0
                },
                {
                    "sent": "So we look for something we call 4 strongly diagonally dominant.",
                    "label": 0
                },
                {
                    "sent": "This means not the diagonals exceed the off diagonals in the row and column.",
                    "label": 0
                },
                {
                    "sent": "We need them to be four times the sum of the off diagonals in their row and column.",
                    "label": 0
                },
                {
                    "sent": "You can find that just by random sampling.",
                    "label": 0
                },
                {
                    "sent": "You pick a random subset of vertices, throw away the ones that don't satisfy the condition.",
                    "label": 0
                },
                {
                    "sent": "You've got it.",
                    "label": 0
                },
                {
                    "sent": "OK, so that is our sparsified Cholesky factorization.",
                    "label": 0
                },
                {
                    "sent": "We basically do some random sampling.",
                    "label": 0
                },
                {
                    "sent": "If you use the ideal sparsa fires the best.",
                    "label": 0
                },
                {
                    "sent": "We know this gets you a factorization with order N non zeros that you can solve systems in linear time.",
                    "label": 0
                },
                {
                    "sent": "If we're not using those best factorizations, then we need to.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go to some other sparsa fires and you get sort of two different results.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you want the best parallel time right now we use the sparse fires of cudas and we lose some log factors every.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "If we use the ones of the paper I mentioned earlier from many people, they have an idea and we use also some techniques from the earlier paper I mentioned with Richard Peng.",
                    "label": 0
                },
                {
                    "sent": "Then we get things which are only off by a log factor of the optimal that we know the depth doesn't quite go down to log in, but it's still pretty good.",
                    "label": 0
                },
                {
                    "sent": "OK, this will all improve clearly, and I think this is a big motivation for building parallel sparse ifers.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me say what I think is going to happen next.",
                    "label": 0
                },
                {
                    "sent": "One thing is these techniques that only need sparse ifers.",
                    "label": 0
                },
                {
                    "sent": "And especially with some of the new work on building sparse fires, make it look like we should now be able to solve many other types of systems of linear equations.",
                    "label": 0
                },
                {
                    "sent": "For example, it's sort of a one line observation to realize that you can handle laplacian's with complex weight sort of things that occur in analyzing some sort of electrical circuits.",
                    "label": 0
                },
                {
                    "sent": "Actually, gotta be careful where the complex number goes, we put it in the right place, ones on the diagonal, complex off diagonal, so you can handle.",
                    "label": 0
                },
                {
                    "sent": "I think we'll find others, so take your favorite linear programming problem on a graph and look at the systems of equations you get when you apply the interior point method might be able to solve those really quickly now.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "One of the techniques, if you look at our paper that we always need that.",
                    "label": 0
                },
                {
                    "sent": "I didn't really talk about was we need to know how to spar.",
                    "label": 0
                },
                {
                    "sent": "Suffice certain graphs that appear during the course of the algorithm we call product demand graphs, sort of like cliques but with edge weights determined by some values at vertices.",
                    "label": 0
                },
                {
                    "sent": "Improving that is a cool problem for a graduate student, I think because it's probably solvable to just do something better and it would clearly improve the algorithms.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We want faster factorizations for this.",
                    "label": 0
                },
                {
                    "sent": "It is very clear if you look at our analysis of this, we're throwing away a lot of factors.",
                    "label": 0
                },
                {
                    "sent": "And not just there, a few logarithmically factors.",
                    "label": 0
                },
                {
                    "sent": "Also some huge constants that I'm lying about and there are many points of Slack in the analysis, so this is sort of thing where I think someone some people could really clean this up.",
                    "label": 0
                },
                {
                    "sent": "We held onto this paper for year cleaning it up.",
                    "label": 0
                },
                {
                    "sent": "We saved many, many logs were done.",
                    "label": 0
                },
                {
                    "sent": "Time for someone with a fresh perspective.",
                    "label": 0
                },
                {
                    "sent": "I mean literally the first draft of it was a year ago.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you want to.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Earn a lot more.",
                    "label": 0
                },
                {
                    "sent": "I've a link on my web page where I try to keep up with everything related to solving Laplacian linear equations and sparse ifers.",
                    "label": 0
                },
                {
                    "sent": "I haven't modified it in three weeks, so I got some new papers to put in, but you can look at that.",
                    "label": 0
                },
                {
                    "sent": "I have course notes that I have right for all the grad courses I teach, so spectral graph theory and graphs and networks online that contain a lot of this material and the fall version of my spectral graph theory course will contain a lot of what I said today an I always recommend the monograph L X = B by Nasheed fish noise.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}