{
    "id": "s2p2nszqfyvrxszuigs67eti3tiy2i2j",
    "title": "Adaptive Feature Selection in Image Segmentation",
    "info": {
        "author": [
            "Volker Roth, ETH Zurich"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2004",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning->Preprocessing"
        ]
    },
    "url": "http://videolectures.net/lmcv04_roth_afsis/",
    "segmentation": [
        [
            "Young Women's Group is going to continue talking about segmentation and this is more a scientific talk, but we put it here 'cause of matches.",
            "Should I take the microphone?",
            "Smoke.",
            "Is stupid.",
            "I'm not sure what the sound.",
            "I I see.",
            "Oops.",
            "What does the book?",
            "What is happened here?",
            "The title of this talk is adaptive feature selection in image segmentation and what I'm presenting now is joint work with Tim on longer."
        ],
        [
            "So let me start with a short outline of the talk.",
            "I will briefly formalize the image segmentation class problem.",
            "That's a clustering problem, so we've heard the general principle in your consumers talk before.",
            "Then I will focus on the issue of feature extraction and data Fusion and there is an Eve way of using different kinds of features by simply stacking all features into a high dimensional vector and call this the stacked feature vector approach.",
            "And given these tech feature vectors, I will then focus on feature selection.",
            "This is necessary because most of the dimensions most of the components of these tech features will be irrelevant for the actual segmentation task.",
            "Then in the fourth point, I will address the model selection issue.",
            "I will talk about choosing the correct number of segments and the correct number of features and this will be a resampling based stability analysis concept here and I will present some segmentation results."
        ],
        [
            "Well yeah, let me start with the problem of image segmentation as we have heard before.",
            "The goal is to divide an image into connected regions which are somehow associated with semantic equivalence classes.",
            "However, in most approaches the semantics is not modeled directly, but rather it is modeled indirectly by defining some mathematical similarity criterion based on low level features.",
            "So I'm following this.",
            "This line of building segmentation algorithms.",
            "We now extract some low level information from the feature and now we face the problem that the informative information is often spread over different types of features.",
            "For example, it might be spread over texture, features and color features and there arises the question of how to combine these different types of features and the native solution.",
            "It's simply build a very large, very high dimensional feature vector by simply stacking all these different types of features in one big vector.",
            "However."
        ],
        [
            "Um, so this here explains how.",
            "The features are extracted here.",
            "We see a real word image of a while this is a shell and this is a coral and some black shadow areas or whatever.",
            "And so we extract locally on different sides of the image.",
            "We extract different kinds of features.",
            "So for example we extract color frequency so we have a histogram of color features and we extract the texture of features.",
            "This is so the results of local Gabor filters.",
            "On different orientations and different scales and we could put everything together.",
            "All the histogram bins and all the answers of the texture filters in a very high dimensional stacked feature vector, and the goal is now we have OK. Now we have a description of each side of the image in terms of a high dimensional vector and the goal is to find the clustering solution to assign clustering label to each of these sites and this gives us a segmentation of the image."
        ],
        [
            "OK, um, there is however, a problem.",
            "When we build such high dimensional feature vectors, we usually have the problem that when it comes to clustering, our clustering algorithms become increasingly instable.",
            "With increasing input dimension and the problem is that for most of the relevant cost functions we don't have global optimal optimization algorithms, but we only have local optimal algorithms, and there's a problem that we have an increasing number of local minima in high dimensional spaces.",
            "Um, so we could solve that by introducing some kind of regularization into our clustering model.",
            "However, there is.",
            "We don't simply have higher dimensions, but our vectors are feature vectors have a special structure which comes from stacking together all these different types of features and what we can see is that for the actual segmentation tasks, for example, for for segmenting, one special pattern from the from the unstructured background or something like that.",
            "Most of the entries of this high dimensional vector will be irrelevant.",
            "They will simply contain noise because they extract features which are on the wrong scale or whatever, and only a few of these components will be relevant.",
            "So it is natural to introduce some some complexity complexity control by focusing on some kind of relevance determination by focusing on on selecting features out of this high.",
            "Dimensional vectors.",
            "OK."
        ],
        [
            "So now the goal is to simultaneously discover groups of.",
            "Of image sites which then gives us the segmentation of the image and at the same time extract relevant features from our set of features.",
            "And well, we can.",
            "In principle we can say that feature selection has been well studied in supervised problems, but segmentation is an unsupervised problem and feature selection is complicated due to the absence of labels that would guide the search for suitable clustering solutions.",
            "And in particular, there are some common problems when we try to combine feature selection and clustering.",
            "The first problem is that usually most approaches like a clear model selection strategy and this leads to some ambiguities.",
            "So after running the algorithms, we usually end up with a set of contradict contradictory hypothesis, so we will find different segmentations and different feature sets and we.",
            "Don't know how to decide which one is the best solution.",
            "A second problem is that most common approaches to this problem they employ some kind of stepwise approach so they iterate clustering and feature selection.",
            "So in the first step a clustering solution is found and in the second step the relevance determination step features are scored for relevance and the problem is that in these approaches clustering and feature selection optimize different objective function objective functions and it's it's.",
            "Unclear what the fixed points of these iterations will be, because two different goals are could possibly be contradicting each other.",
            "So."
        ],
        [
            "These problems bringing us to A wish list and some possible solutions well on our wish list.",
            "The first point is to overcome these.",
            "These ambiguities and one possible solution to that problem is to use sophisticated way of model selection and our approach is to observe the stability of the clustering of the cluster solutions.",
            "The second problem is more.",
            "It's more of a theoretical nature.",
            "We would like to find the model where the clustering and the feature selection step both optimized.",
            "The same objective function.",
            "So we want to get rid of this contradictory or possibly contradictory objective functions in clustering and feature selection, and this can be solved by incorporating what is called an automatic relevance determination principle into a Gaussian mixture model for clustering.",
            "And what we finally end up with a model where the both the feature selection and the clustering maximized the same likelihood function."
        ],
        [
            "So let me come to some some details here.",
            "Well, our data is a collection of an image site, so local patches of the image and our goal is to assign cluster labels to all these sites.",
            "So we have to formalize a clustering model.",
            "What we use here is a simple Gaussian mixture model with K components.",
            "So we have K Gaussian modes and we have to restriction that all the covariances are identical.",
            "So this is a very simple.",
            "Gaussian mixture model.",
            "Come in the standard setting.",
            "Gaussian mixture models can be optimized by the classical EM algorithm in the SERP.",
            "We estimate the cluster membership probabilities given the para meters and then the answer.",
            "Please find the optimal para meters given the current cluster membership probabilities.",
            "So on those two steps are iterated.",
            "So this is the classical approach.",
            "Now we will plug in a feature selection principle into this iteration of E&M step.",
            "This will be done by modifying the maximization step in such a way that we could.",
            "If we could plug in the feature selection method."
        ],
        [
            "And the key ingredient here is that the maximization step can be restated, reformulated as a linear regression problem.",
            "This goes back to a paper by Trevor Hastie, who has shown this equivalence of the M step in special sort of Gaussian mixture models and linear regression.",
            "So we can formulate the answer as the problem of minimizing a linear least squares functional.",
            "So this is our data matrix.",
            "This is the vector of currently estimated class membership probabilities from the preceding esep and we are inferring these weights.",
            "These regression coefficients here.",
            "So we have some kind of reparametrization we have re parameterized our model.",
            "We started with the model parameters, the means of the covariance, and now we have.",
            "This vector of regression coefficients so, but it's simply a mathematical reparametrization, reparametrization of the problem.",
            "OK. Now, taking a base in perspective, we are given this this Gaussian likelihood here, which leads to the least squares problem and now we define prior distributions over the the regression coefficients and very popular kind of prior distribution is are the so called automatic relevance determination priors.",
            "So these are simply.",
            "1 dimensional Gaussians with inverse variance theater I.",
            "So it's just a very simple collection of 1 dimensional Gaussians and the role of these are hyper para meters theater I hear is that they have some sense or in some sense that can be interpreted as a relevant parameter.",
            "Because if you look at the asymptotics if.",
            "So Theta I approaches Infinity, then the prior variance shrinks to zero and the posterior will be infinitely peaked at zero and the corresponding feature will be removed from the model.",
            "So in some sense these these new hyperparameters played role of relevance parameters for special for certain features.",
            "So we have a relaxation of a binary binary selection variable.",
            "So if you want to select features we want to.",
            "Estimate a binary variable.",
            "The feature is present or it is absent and here we have a relaxation of this binary variable to a real valued variance over prior distribution."
        ],
        [
            "And taking a basin view instead of selecting now this variance relevance variables, which is of course a hard combinatorial problem, you have to optimize over all possible subsets of relevant features.",
            "We choose hyper priors, so which in this case have an exponential form and we simply average of all possible parameter values by marginalizing over marginalizing.",
            "So we.",
            "We simply average over all possible values of this relevance variables, so we this integral can be solved analytically and this is the result here.",
            "And if you combine it with the likelihood we end up with an L1 constraint regression in lock space.",
            "So we minimize the likelihood term.",
            "This squared error functional plus under a constraint on the L1 norm of this of this coefficient vector.",
            "Um?",
            "The idea here is that we started with inch by introducing some relevant variables, but we have overcome the problem of selecting the these variables, but we have simply averaged in the basic sense of all possible values.",
            "So this is the feature selection part in that."
        ],
        [
            "OK so I will not go into further technical details, but simply sum up here with the overview of the model.",
            "So we have an EM iteration.",
            "The Eastep remains unchanged.",
            "We simply estimate the probabilities that object XI belongs to Class A cluster new and then the M step.",
            "We have done some some reformulations.",
            "The first reformulation is purely mathematical.",
            "Reparametrization of the problem.",
            "The episode was formulated as a linear regression problem.",
            "Then we have introduced priors over the regression coefficients.",
            "We have introduced hyper priors and we integrated out all the relevant variables and the final functional is L1 constraint.",
            "This creates functional is statistical literature.",
            "They are known as the Lasal model introduced by think Trevor Hastie.",
            "OK and together by iterating in answer we have a combined clustering and feature selection method.",
            "And both both inference step optimize the same objective function with, which is simply the constraint likelihood of the data.",
            "So we have a unique objective function which leads to which both leads to clustering and feature selection.",
            "OK."
        ],
        [
            "Um?",
            "Some words on model selection.",
            "The idea is to analyze the stability of the of the clustering solution on the re samples from the data.",
            "So we in the ideal case we have two random samples coming from the same data source, and then there's some inference step.",
            "We learn the mixture models from both samples and that counts the prediction step.",
            "We use the first model to predict.",
            "The cluster membership of the data in the second sample and there is a comparison step for the second sample, we compare the cluster labels inferred from the second model.",
            "That is the.",
            "This is still the clustering solution on the second data set.",
            "We compare these clustering solutions with the predicted labels from the other sample and this gives us some some kind of stability index from which we can estimate how many clusters there should be in the in the set and how many features there should be.",
            "Basically our free para meter in terms of feature selections is this L1 constraint which has the role of selecting the number of features.",
            "OK."
        ],
        [
            "This is just a graphical model.",
            "Here we have a re samples and we will learn our mixture models and we compare one solution with the predicted solution.",
            "In this case or some kind of dissimilarity matrix and then we do some some stability analysis.",
            "So this is a stability of partitions.",
            "I won't go into detail here now let."
        ],
        [
            "Let me show you some examples here.",
            "This is a toy example.",
            "Here we see an artificial image which has been constructed in the following way.",
            "These two segments here have the same texture, so they cannot be distinguished from each other by great value.",
            "So by by Gray value statistics, these two features, these two segments here have no texture at all, but they have different.",
            "Great value statistics.",
            "So what we need is a combination of texture and color features here and what you see here in that curve is the overall stability versus the number of mixture components in the model and there is a clear peak here.",
            "But here we have the model with the high stability and it finds 5 mixture components and while there are some errors with respect to the gradual true solution but these errors are due to the high noise in these segments here.",
            "So we have some some clear principle of estimating a suitable number of segments in our in.",
            "In this approach here."
        ],
        [
            "If we focus on the on the feature selection issue here for the most stable number of segments, you can see a plot of the stability index.",
            "The dark curve versus the percentage of features which have been selected, and you see a clear maximum at about or about 40% of all the features have been selected, and this can be interpreted in the in the following sense.",
            "If you say like too many features, then we are in.",
            "In a much too high dimensional space and we will have the problem of of getting stuck in local minima.",
            "So the solution will be unstable if you have too few features then we will find many contradictory hypothesis.",
            "So one feature votes for this partition.",
            "The next feature was for the Second Partition.",
            "But in between those two extremes we can expect to find stable solutions and this is what the model tells us here."
        ],
        [
            "So this was a toy example.",
            "Here is the real world example of the shell.",
            "And here you see the stability curves.",
            "The most stable model favors two components.",
            "So and there are two, there are two possible interpretations.",
            "In one of the interpretation and one of the results only texture features have been extracted.",
            "So here the textured part is segmented from the untextured background and the other possibility, which is also quite stable.",
            "Only how selected color features where basically the the shadow has been segmented from the bright coral anschell.",
            "Here there is a second rather stable solution which uses 3 mixture components and you can see here that.",
            "These curves are the so you can see the percentage of features versus disability.",
            "So in these two cases here we have only one or two features selected, and here we have.",
            "I think it was seven or eight features which have been selected because I mean it's clear if you want to find 3 segments here we need both texture and color features and what you can see also again here if you select too many features then again all the solutions become instable, so it's.",
            "Really important to have this feature selection mechanism in plugged into our clustering approach."
        ],
        [
            "There are some further results here.",
            "We have applied it to quite quite many images from the coral database, so there are.",
            "Text, So what you can see here are the.",
            "The most stable solution, so the most stable number of components found for the most stable number of features and in all of these cases you have a combination of color and texture features, and so here you have a nice operations of these two zebra, and while you see this.",
            "Leopard or whatever it is and I don't have time to go into details here.",
            "Let me."
        ],
        [
            "Sum up.",
            "So when it comes to.",
            "Image segmentation the informative information is often spread over different types of features, which leads us to the problem of data Fusion and the naive solution to data Fusion is the use of stacked features like feature vectors.",
            "Simply putting all the features together in higher into one high dimensional feature vector, but due to this feature the inherent nature of this of this tax feature vectors.",
            "Most of the components will be irrelevant for the actual task.",
            "So in order to restrict the model complexity, we should focus on feature selection on selecting features which are relevant for the actual tasks and.",
            "Well, in general, feature selection is difficult in these types of problems because we have no labeling information and one goal is to overcome instabilities and ambiguities and possible approach.",
            "To overcome this problem is to use of stability based model selection criterion.",
            "Out on the more theoretical side, we would like to have a consistent model where both the clustering and the feature selection optimize the same objective function, and this goal is achieved by incorporating an automatic relevance determination mechanism into a Gaussian mixture model.",
            "Thank you for your attention.",
            "We have time for maybe one or two quick questions.",
            "So when you talk about the the idea that too many features mean that you get local minima, I'm just wondering if that's really what's going on, or whether it's really a case of more case of overfitting that would happen.",
            "Yeah, so it can be both.",
            "You have both sides have the algorithmically instability which comes home to many local minima.",
            "And on the other hand you have some kind of generalization problem and basically stability measures to some of both.",
            "So both methods, the inherent algorithmic instability and the overfitting problem and you cannot really decide which part you are measuring.",
            "So you measure this sum of both terms.",
            "True, is it that you find more local minima when you have these extra you know were relevant features?",
            "Yeah, I mean it's well, yeah, you will find more local minima if the dimension of the input space increases here, OK, Thanks.",
            "Thanks.",
            "Cool, some work from a few years ago by Byron Dahmen collaborator, who had an idea of incorporating irrelevance by having.",
            "Basically, if you had a dimension for which save, so you just got two classes, then the distribution on a particular dimension was the same for both classes.",
            "Then that was sort of irrelevant, and if it was specific to the class, that would be, you know to be relevant would be allowed to vary it for these different things, do you?",
            "I'm just wondering whether you see in the kinds of have you looked at the kinds of distribution you get for the the features that you find to be relevant and irrelevant, and whether that.",
            "Is a characterization of the distinction or weather is different to that now we have not looked into that into the distributions, but I think the problem might be that if you look at the single particular features than you treat the features as independent and for this special kind of features this is definitely not the case.",
            "If you have features on different scales and different rotations, there they are not independent at all, and so maybe it's very dangerous to look at independent.",
            "You could generalize that to have groups of relevant, irrelevant, and have distributions for those.",
            "Probably that would be a good idea to look at the distributions, yeah?",
            "OK, so the session there will break for 15 minutes, so back at 20 past."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Young Women's Group is going to continue talking about segmentation and this is more a scientific talk, but we put it here 'cause of matches.",
                    "label": 0
                },
                {
                    "sent": "Should I take the microphone?",
                    "label": 0
                },
                {
                    "sent": "Smoke.",
                    "label": 0
                },
                {
                    "sent": "Is stupid.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure what the sound.",
                    "label": 0
                },
                {
                    "sent": "I I see.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "What does the book?",
                    "label": 0
                },
                {
                    "sent": "What is happened here?",
                    "label": 0
                },
                {
                    "sent": "The title of this talk is adaptive feature selection in image segmentation and what I'm presenting now is joint work with Tim on longer.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me start with a short outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "I will briefly formalize the image segmentation class problem.",
                    "label": 1
                },
                {
                    "sent": "That's a clustering problem, so we've heard the general principle in your consumers talk before.",
                    "label": 1
                },
                {
                    "sent": "Then I will focus on the issue of feature extraction and data Fusion and there is an Eve way of using different kinds of features by simply stacking all features into a high dimensional vector and call this the stacked feature vector approach.",
                    "label": 1
                },
                {
                    "sent": "And given these tech feature vectors, I will then focus on feature selection.",
                    "label": 0
                },
                {
                    "sent": "This is necessary because most of the dimensions most of the components of these tech features will be irrelevant for the actual segmentation task.",
                    "label": 0
                },
                {
                    "sent": "Then in the fourth point, I will address the model selection issue.",
                    "label": 0
                },
                {
                    "sent": "I will talk about choosing the correct number of segments and the correct number of features and this will be a resampling based stability analysis concept here and I will present some segmentation results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well yeah, let me start with the problem of image segmentation as we have heard before.",
                    "label": 0
                },
                {
                    "sent": "The goal is to divide an image into connected regions which are somehow associated with semantic equivalence classes.",
                    "label": 1
                },
                {
                    "sent": "However, in most approaches the semantics is not modeled directly, but rather it is modeled indirectly by defining some mathematical similarity criterion based on low level features.",
                    "label": 0
                },
                {
                    "sent": "So I'm following this.",
                    "label": 0
                },
                {
                    "sent": "This line of building segmentation algorithms.",
                    "label": 1
                },
                {
                    "sent": "We now extract some low level information from the feature and now we face the problem that the informative information is often spread over different types of features.",
                    "label": 0
                },
                {
                    "sent": "For example, it might be spread over texture, features and color features and there arises the question of how to combine these different types of features and the native solution.",
                    "label": 0
                },
                {
                    "sent": "It's simply build a very large, very high dimensional feature vector by simply stacking all these different types of features in one big vector.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, so this here explains how.",
                    "label": 0
                },
                {
                    "sent": "The features are extracted here.",
                    "label": 0
                },
                {
                    "sent": "We see a real word image of a while this is a shell and this is a coral and some black shadow areas or whatever.",
                    "label": 0
                },
                {
                    "sent": "And so we extract locally on different sides of the image.",
                    "label": 0
                },
                {
                    "sent": "We extract different kinds of features.",
                    "label": 0
                },
                {
                    "sent": "So for example we extract color frequency so we have a histogram of color features and we extract the texture of features.",
                    "label": 0
                },
                {
                    "sent": "This is so the results of local Gabor filters.",
                    "label": 0
                },
                {
                    "sent": "On different orientations and different scales and we could put everything together.",
                    "label": 0
                },
                {
                    "sent": "All the histogram bins and all the answers of the texture filters in a very high dimensional stacked feature vector, and the goal is now we have OK. Now we have a description of each side of the image in terms of a high dimensional vector and the goal is to find the clustering solution to assign clustering label to each of these sites and this gives us a segmentation of the image.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, um, there is however, a problem.",
                    "label": 0
                },
                {
                    "sent": "When we build such high dimensional feature vectors, we usually have the problem that when it comes to clustering, our clustering algorithms become increasingly instable.",
                    "label": 1
                },
                {
                    "sent": "With increasing input dimension and the problem is that for most of the relevant cost functions we don't have global optimal optimization algorithms, but we only have local optimal algorithms, and there's a problem that we have an increasing number of local minima in high dimensional spaces.",
                    "label": 1
                },
                {
                    "sent": "Um, so we could solve that by introducing some kind of regularization into our clustering model.",
                    "label": 0
                },
                {
                    "sent": "However, there is.",
                    "label": 0
                },
                {
                    "sent": "We don't simply have higher dimensions, but our vectors are feature vectors have a special structure which comes from stacking together all these different types of features and what we can see is that for the actual segmentation tasks, for example, for for segmenting, one special pattern from the from the unstructured background or something like that.",
                    "label": 0
                },
                {
                    "sent": "Most of the entries of this high dimensional vector will be irrelevant.",
                    "label": 0
                },
                {
                    "sent": "They will simply contain noise because they extract features which are on the wrong scale or whatever, and only a few of these components will be relevant.",
                    "label": 0
                },
                {
                    "sent": "So it is natural to introduce some some complexity complexity control by focusing on some kind of relevance determination by focusing on on selecting features out of this high.",
                    "label": 0
                },
                {
                    "sent": "Dimensional vectors.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the goal is to simultaneously discover groups of.",
                    "label": 1
                },
                {
                    "sent": "Of image sites which then gives us the segmentation of the image and at the same time extract relevant features from our set of features.",
                    "label": 0
                },
                {
                    "sent": "And well, we can.",
                    "label": 0
                },
                {
                    "sent": "In principle we can say that feature selection has been well studied in supervised problems, but segmentation is an unsupervised problem and feature selection is complicated due to the absence of labels that would guide the search for suitable clustering solutions.",
                    "label": 1
                },
                {
                    "sent": "And in particular, there are some common problems when we try to combine feature selection and clustering.",
                    "label": 0
                },
                {
                    "sent": "The first problem is that usually most approaches like a clear model selection strategy and this leads to some ambiguities.",
                    "label": 0
                },
                {
                    "sent": "So after running the algorithms, we usually end up with a set of contradict contradictory hypothesis, so we will find different segmentations and different feature sets and we.",
                    "label": 0
                },
                {
                    "sent": "Don't know how to decide which one is the best solution.",
                    "label": 0
                },
                {
                    "sent": "A second problem is that most common approaches to this problem they employ some kind of stepwise approach so they iterate clustering and feature selection.",
                    "label": 0
                },
                {
                    "sent": "So in the first step a clustering solution is found and in the second step the relevance determination step features are scored for relevance and the problem is that in these approaches clustering and feature selection optimize different objective function objective functions and it's it's.",
                    "label": 0
                },
                {
                    "sent": "Unclear what the fixed points of these iterations will be, because two different goals are could possibly be contradicting each other.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These problems bringing us to A wish list and some possible solutions well on our wish list.",
                    "label": 0
                },
                {
                    "sent": "The first point is to overcome these.",
                    "label": 0
                },
                {
                    "sent": "These ambiguities and one possible solution to that problem is to use sophisticated way of model selection and our approach is to observe the stability of the clustering of the cluster solutions.",
                    "label": 0
                },
                {
                    "sent": "The second problem is more.",
                    "label": 0
                },
                {
                    "sent": "It's more of a theoretical nature.",
                    "label": 0
                },
                {
                    "sent": "We would like to find the model where the clustering and the feature selection step both optimized.",
                    "label": 0
                },
                {
                    "sent": "The same objective function.",
                    "label": 0
                },
                {
                    "sent": "So we want to get rid of this contradictory or possibly contradictory objective functions in clustering and feature selection, and this can be solved by incorporating what is called an automatic relevance determination principle into a Gaussian mixture model for clustering.",
                    "label": 1
                },
                {
                    "sent": "And what we finally end up with a model where the both the feature selection and the clustering maximized the same likelihood function.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me come to some some details here.",
                    "label": 0
                },
                {
                    "sent": "Well, our data is a collection of an image site, so local patches of the image and our goal is to assign cluster labels to all these sites.",
                    "label": 0
                },
                {
                    "sent": "So we have to formalize a clustering model.",
                    "label": 0
                },
                {
                    "sent": "What we use here is a simple Gaussian mixture model with K components.",
                    "label": 1
                },
                {
                    "sent": "So we have K Gaussian modes and we have to restriction that all the covariances are identical.",
                    "label": 0
                },
                {
                    "sent": "So this is a very simple.",
                    "label": 0
                },
                {
                    "sent": "Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "Come in the standard setting.",
                    "label": 0
                },
                {
                    "sent": "Gaussian mixture models can be optimized by the classical EM algorithm in the SERP.",
                    "label": 0
                },
                {
                    "sent": "We estimate the cluster membership probabilities given the para meters and then the answer.",
                    "label": 0
                },
                {
                    "sent": "Please find the optimal para meters given the current cluster membership probabilities.",
                    "label": 0
                },
                {
                    "sent": "So on those two steps are iterated.",
                    "label": 0
                },
                {
                    "sent": "So this is the classical approach.",
                    "label": 0
                },
                {
                    "sent": "Now we will plug in a feature selection principle into this iteration of E&M step.",
                    "label": 0
                },
                {
                    "sent": "This will be done by modifying the maximization step in such a way that we could.",
                    "label": 0
                },
                {
                    "sent": "If we could plug in the feature selection method.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the key ingredient here is that the maximization step can be restated, reformulated as a linear regression problem.",
                    "label": 1
                },
                {
                    "sent": "This goes back to a paper by Trevor Hastie, who has shown this equivalence of the M step in special sort of Gaussian mixture models and linear regression.",
                    "label": 0
                },
                {
                    "sent": "So we can formulate the answer as the problem of minimizing a linear least squares functional.",
                    "label": 0
                },
                {
                    "sent": "So this is our data matrix.",
                    "label": 0
                },
                {
                    "sent": "This is the vector of currently estimated class membership probabilities from the preceding esep and we are inferring these weights.",
                    "label": 0
                },
                {
                    "sent": "These regression coefficients here.",
                    "label": 0
                },
                {
                    "sent": "So we have some kind of reparametrization we have re parameterized our model.",
                    "label": 0
                },
                {
                    "sent": "We started with the model parameters, the means of the covariance, and now we have.",
                    "label": 0
                },
                {
                    "sent": "This vector of regression coefficients so, but it's simply a mathematical reparametrization, reparametrization of the problem.",
                    "label": 0
                },
                {
                    "sent": "OK. Now, taking a base in perspective, we are given this this Gaussian likelihood here, which leads to the least squares problem and now we define prior distributions over the the regression coefficients and very popular kind of prior distribution is are the so called automatic relevance determination priors.",
                    "label": 1
                },
                {
                    "sent": "So these are simply.",
                    "label": 0
                },
                {
                    "sent": "1 dimensional Gaussians with inverse variance theater I.",
                    "label": 0
                },
                {
                    "sent": "So it's just a very simple collection of 1 dimensional Gaussians and the role of these are hyper para meters theater I hear is that they have some sense or in some sense that can be interpreted as a relevant parameter.",
                    "label": 0
                },
                {
                    "sent": "Because if you look at the asymptotics if.",
                    "label": 0
                },
                {
                    "sent": "So Theta I approaches Infinity, then the prior variance shrinks to zero and the posterior will be infinitely peaked at zero and the corresponding feature will be removed from the model.",
                    "label": 0
                },
                {
                    "sent": "So in some sense these these new hyperparameters played role of relevance parameters for special for certain features.",
                    "label": 0
                },
                {
                    "sent": "So we have a relaxation of a binary binary selection variable.",
                    "label": 0
                },
                {
                    "sent": "So if you want to select features we want to.",
                    "label": 0
                },
                {
                    "sent": "Estimate a binary variable.",
                    "label": 0
                },
                {
                    "sent": "The feature is present or it is absent and here we have a relaxation of this binary variable to a real valued variance over prior distribution.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And taking a basin view instead of selecting now this variance relevance variables, which is of course a hard combinatorial problem, you have to optimize over all possible subsets of relevant features.",
                    "label": 1
                },
                {
                    "sent": "We choose hyper priors, so which in this case have an exponential form and we simply average of all possible parameter values by marginalizing over marginalizing.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "We simply average over all possible values of this relevance variables, so we this integral can be solved analytically and this is the result here.",
                    "label": 0
                },
                {
                    "sent": "And if you combine it with the likelihood we end up with an L1 constraint regression in lock space.",
                    "label": 0
                },
                {
                    "sent": "So we minimize the likelihood term.",
                    "label": 0
                },
                {
                    "sent": "This squared error functional plus under a constraint on the L1 norm of this of this coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The idea here is that we started with inch by introducing some relevant variables, but we have overcome the problem of selecting the these variables, but we have simply averaged in the basic sense of all possible values.",
                    "label": 0
                },
                {
                    "sent": "So this is the feature selection part in that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I will not go into further technical details, but simply sum up here with the overview of the model.",
                    "label": 0
                },
                {
                    "sent": "So we have an EM iteration.",
                    "label": 0
                },
                {
                    "sent": "The Eastep remains unchanged.",
                    "label": 0
                },
                {
                    "sent": "We simply estimate the probabilities that object XI belongs to Class A cluster new and then the M step.",
                    "label": 0
                },
                {
                    "sent": "We have done some some reformulations.",
                    "label": 0
                },
                {
                    "sent": "The first reformulation is purely mathematical.",
                    "label": 0
                },
                {
                    "sent": "Reparametrization of the problem.",
                    "label": 0
                },
                {
                    "sent": "The episode was formulated as a linear regression problem.",
                    "label": 1
                },
                {
                    "sent": "Then we have introduced priors over the regression coefficients.",
                    "label": 0
                },
                {
                    "sent": "We have introduced hyper priors and we integrated out all the relevant variables and the final functional is L1 constraint.",
                    "label": 0
                },
                {
                    "sent": "This creates functional is statistical literature.",
                    "label": 0
                },
                {
                    "sent": "They are known as the Lasal model introduced by think Trevor Hastie.",
                    "label": 0
                },
                {
                    "sent": "OK and together by iterating in answer we have a combined clustering and feature selection method.",
                    "label": 0
                },
                {
                    "sent": "And both both inference step optimize the same objective function with, which is simply the constraint likelihood of the data.",
                    "label": 1
                },
                {
                    "sent": "So we have a unique objective function which leads to which both leads to clustering and feature selection.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Some words on model selection.",
                    "label": 1
                },
                {
                    "sent": "The idea is to analyze the stability of the of the clustering solution on the re samples from the data.",
                    "label": 0
                },
                {
                    "sent": "So we in the ideal case we have two random samples coming from the same data source, and then there's some inference step.",
                    "label": 1
                },
                {
                    "sent": "We learn the mixture models from both samples and that counts the prediction step.",
                    "label": 1
                },
                {
                    "sent": "We use the first model to predict.",
                    "label": 0
                },
                {
                    "sent": "The cluster membership of the data in the second sample and there is a comparison step for the second sample, we compare the cluster labels inferred from the second model.",
                    "label": 1
                },
                {
                    "sent": "That is the.",
                    "label": 0
                },
                {
                    "sent": "This is still the clustering solution on the second data set.",
                    "label": 0
                },
                {
                    "sent": "We compare these clustering solutions with the predicted labels from the other sample and this gives us some some kind of stability index from which we can estimate how many clusters there should be in the in the set and how many features there should be.",
                    "label": 0
                },
                {
                    "sent": "Basically our free para meter in terms of feature selections is this L1 constraint which has the role of selecting the number of features.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just a graphical model.",
                    "label": 0
                },
                {
                    "sent": "Here we have a re samples and we will learn our mixture models and we compare one solution with the predicted solution.",
                    "label": 0
                },
                {
                    "sent": "In this case or some kind of dissimilarity matrix and then we do some some stability analysis.",
                    "label": 0
                },
                {
                    "sent": "So this is a stability of partitions.",
                    "label": 0
                },
                {
                    "sent": "I won't go into detail here now let.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me show you some examples here.",
                    "label": 0
                },
                {
                    "sent": "This is a toy example.",
                    "label": 0
                },
                {
                    "sent": "Here we see an artificial image which has been constructed in the following way.",
                    "label": 0
                },
                {
                    "sent": "These two segments here have the same texture, so they cannot be distinguished from each other by great value.",
                    "label": 0
                },
                {
                    "sent": "So by by Gray value statistics, these two features, these two segments here have no texture at all, but they have different.",
                    "label": 0
                },
                {
                    "sent": "Great value statistics.",
                    "label": 0
                },
                {
                    "sent": "So what we need is a combination of texture and color features here and what you see here in that curve is the overall stability versus the number of mixture components in the model and there is a clear peak here.",
                    "label": 1
                },
                {
                    "sent": "But here we have the model with the high stability and it finds 5 mixture components and while there are some errors with respect to the gradual true solution but these errors are due to the high noise in these segments here.",
                    "label": 0
                },
                {
                    "sent": "So we have some some clear principle of estimating a suitable number of segments in our in.",
                    "label": 0
                },
                {
                    "sent": "In this approach here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we focus on the on the feature selection issue here for the most stable number of segments, you can see a plot of the stability index.",
                    "label": 0
                },
                {
                    "sent": "The dark curve versus the percentage of features which have been selected, and you see a clear maximum at about or about 40% of all the features have been selected, and this can be interpreted in the in the following sense.",
                    "label": 0
                },
                {
                    "sent": "If you say like too many features, then we are in.",
                    "label": 0
                },
                {
                    "sent": "In a much too high dimensional space and we will have the problem of of getting stuck in local minima.",
                    "label": 0
                },
                {
                    "sent": "So the solution will be unstable if you have too few features then we will find many contradictory hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So one feature votes for this partition.",
                    "label": 0
                },
                {
                    "sent": "The next feature was for the Second Partition.",
                    "label": 0
                },
                {
                    "sent": "But in between those two extremes we can expect to find stable solutions and this is what the model tells us here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was a toy example.",
                    "label": 0
                },
                {
                    "sent": "Here is the real world example of the shell.",
                    "label": 0
                },
                {
                    "sent": "And here you see the stability curves.",
                    "label": 0
                },
                {
                    "sent": "The most stable model favors two components.",
                    "label": 0
                },
                {
                    "sent": "So and there are two, there are two possible interpretations.",
                    "label": 0
                },
                {
                    "sent": "In one of the interpretation and one of the results only texture features have been extracted.",
                    "label": 0
                },
                {
                    "sent": "So here the textured part is segmented from the untextured background and the other possibility, which is also quite stable.",
                    "label": 0
                },
                {
                    "sent": "Only how selected color features where basically the the shadow has been segmented from the bright coral anschell.",
                    "label": 0
                },
                {
                    "sent": "Here there is a second rather stable solution which uses 3 mixture components and you can see here that.",
                    "label": 0
                },
                {
                    "sent": "These curves are the so you can see the percentage of features versus disability.",
                    "label": 0
                },
                {
                    "sent": "So in these two cases here we have only one or two features selected, and here we have.",
                    "label": 0
                },
                {
                    "sent": "I think it was seven or eight features which have been selected because I mean it's clear if you want to find 3 segments here we need both texture and color features and what you can see also again here if you select too many features then again all the solutions become instable, so it's.",
                    "label": 0
                },
                {
                    "sent": "Really important to have this feature selection mechanism in plugged into our clustering approach.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are some further results here.",
                    "label": 0
                },
                {
                    "sent": "We have applied it to quite quite many images from the coral database, so there are.",
                    "label": 0
                },
                {
                    "sent": "Text, So what you can see here are the.",
                    "label": 0
                },
                {
                    "sent": "The most stable solution, so the most stable number of components found for the most stable number of features and in all of these cases you have a combination of color and texture features, and so here you have a nice operations of these two zebra, and while you see this.",
                    "label": 0
                },
                {
                    "sent": "Leopard or whatever it is and I don't have time to go into details here.",
                    "label": 0
                },
                {
                    "sent": "Let me.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sum up.",
                    "label": 0
                },
                {
                    "sent": "So when it comes to.",
                    "label": 0
                },
                {
                    "sent": "Image segmentation the informative information is often spread over different types of features, which leads us to the problem of data Fusion and the naive solution to data Fusion is the use of stacked features like feature vectors.",
                    "label": 1
                },
                {
                    "sent": "Simply putting all the features together in higher into one high dimensional feature vector, but due to this feature the inherent nature of this of this tax feature vectors.",
                    "label": 1
                },
                {
                    "sent": "Most of the components will be irrelevant for the actual task.",
                    "label": 0
                },
                {
                    "sent": "So in order to restrict the model complexity, we should focus on feature selection on selecting features which are relevant for the actual tasks and.",
                    "label": 1
                },
                {
                    "sent": "Well, in general, feature selection is difficult in these types of problems because we have no labeling information and one goal is to overcome instabilities and ambiguities and possible approach.",
                    "label": 0
                },
                {
                    "sent": "To overcome this problem is to use of stability based model selection criterion.",
                    "label": 0
                },
                {
                    "sent": "Out on the more theoretical side, we would like to have a consistent model where both the clustering and the feature selection optimize the same objective function, and this goal is achieved by incorporating an automatic relevance determination mechanism into a Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "We have time for maybe one or two quick questions.",
                    "label": 0
                },
                {
                    "sent": "So when you talk about the the idea that too many features mean that you get local minima, I'm just wondering if that's really what's going on, or whether it's really a case of more case of overfitting that would happen.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it can be both.",
                    "label": 0
                },
                {
                    "sent": "You have both sides have the algorithmically instability which comes home to many local minima.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand you have some kind of generalization problem and basically stability measures to some of both.",
                    "label": 0
                },
                {
                    "sent": "So both methods, the inherent algorithmic instability and the overfitting problem and you cannot really decide which part you are measuring.",
                    "label": 0
                },
                {
                    "sent": "So you measure this sum of both terms.",
                    "label": 0
                },
                {
                    "sent": "True, is it that you find more local minima when you have these extra you know were relevant features?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean it's well, yeah, you will find more local minima if the dimension of the input space increases here, OK, Thanks.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Cool, some work from a few years ago by Byron Dahmen collaborator, who had an idea of incorporating irrelevance by having.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you had a dimension for which save, so you just got two classes, then the distribution on a particular dimension was the same for both classes.",
                    "label": 0
                },
                {
                    "sent": "Then that was sort of irrelevant, and if it was specific to the class, that would be, you know to be relevant would be allowed to vary it for these different things, do you?",
                    "label": 0
                },
                {
                    "sent": "I'm just wondering whether you see in the kinds of have you looked at the kinds of distribution you get for the the features that you find to be relevant and irrelevant, and whether that.",
                    "label": 0
                },
                {
                    "sent": "Is a characterization of the distinction or weather is different to that now we have not looked into that into the distributions, but I think the problem might be that if you look at the single particular features than you treat the features as independent and for this special kind of features this is definitely not the case.",
                    "label": 0
                },
                {
                    "sent": "If you have features on different scales and different rotations, there they are not independent at all, and so maybe it's very dangerous to look at independent.",
                    "label": 0
                },
                {
                    "sent": "You could generalize that to have groups of relevant, irrelevant, and have distributions for those.",
                    "label": 0
                },
                {
                    "sent": "Probably that would be a good idea to look at the distributions, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, so the session there will break for 15 minutes, so back at 20 past.",
                    "label": 0
                }
            ]
        }
    }
}