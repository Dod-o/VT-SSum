{
    "id": "lvpb4jmasvf7sfuj3weiuurosnyu34hn",
    "title": "DBpedia FlexiFusion \u2013 Best of Wikipedia > Wikidata > Your Data",
    "info": {
        "author": [
            "Johannes Frey, Agile Knowledge Engineering and Semantic Web (AKSW), University of Leipzig"
        ],
        "published": "Dec. 10, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_unknown_your_data/",
    "segmentation": [
        [
            "So first of all, you have to deal with data source selection and you have to think about conflict resolution.",
            "But most importantly the individual property and entity selection or reduction for your domain or use case.",
            "Yeah, it's highly highly needs to be highly customizable and the idea or goal of flex effusion is to make it easier to mass produce such custom knowledge graphs or custom DPS how icon."
        ],
        [
            "And then discovered the paper we propose to measure things on.",
            "On the one hand, we proposed the profusion data set, which basically is a precomputed and merged global view of all triples from all the input sources that will statement level provenance and on the other hand we propose to flex effusion workflow which can be applied on this pre Fusion data set with two simple methods in order to derive your custom knowledge graph.",
            "This is the big picture which I'm going to use to run you through the workflow itself.",
            "Let's just start right away with."
        ],
        [
            "Source selection and our workflow.",
            "We make use of the novel DPT later pass.",
            "Technology, which is in a nutshell, DPS Maven plus GitHub for data.",
            "So you register your data by uploading structured Meteor data.",
            "More specific the data and vocabulary and the data is organized using the concept hierarchy publisher Group, Artifact version file and this allows us to define a flexible input source selection.",
            "And also to store and persist intermediate and final results of Flex effusion on the database."
        ],
        [
            "Before we can actually start with the Fusion, we have to think about identifiers in the different sources.",
            "Do not show redeveloped global ID management as a solution for this.",
            "We start with collecting the data from the data of us with our custom sparkle query.",
            "Collect all same as links and I rise in this data set computer clustering based on connected components algorithm and finally assign a global ID for every single cluster.",
            "This is then persisted on a dataverse."
        ],
        [
            "And the next step we actually produce this pre Fusion data set."
        ],
        [
            "So we start fetching again the data from the data bus and we also take the global ID assignment, which I previously introduced and now we're just a straight forward replace every IRRI with its corresponding global ID.",
            "Moreover, we keep track of the provenance from the individual files."
        ],
        [
            "In the second step we derive so-called pre fused entities which is just simply a grouping all tribals first by their now global ID subject and then by their predicate values.",
            "So you can see over here these are the two different values resources of the Eiffel Tower from the French and English chapter, and then they're basically merged into one."
        ],
        [
            "The result of this are pre Fusion is stored in a Jason LD and a custom Jason LD format.",
            "We store 1 record one chasing record which looks like this for every subject prepare predicate pair.",
            "So in this case you can see again the example the Eiffel Tower with the flow count properties carrying the two different values for and three reported in three different sources."
        ],
        [
            "In the next one final step, we can."
        ],
        [
            "Actually perform diffusion.",
            "In order to make it really flexible, we came up with two functions.",
            "The first one is to reduce function, which is a customizable function which is applied for every subject predicate pair to reduce or filter the amount of information for the entities.",
            "So the idea is to remove irrelevant or bad data or to reduce Fusion decisions in the next step.",
            "One example could be to fill the owner for the DBO Birthplace properties or entities of type person."
        ],
        [
            "And the next step we performed the resolve function which picks a number of objects from the list of each reduced subject predicate pair, which is then finally handed over to the result data set.",
            "The purpose is quite simple.",
            "We want to resolve conflicts in order to improve the data quality.",
            "One example is to just select all values for labels, but another one could be to perform majority written for functional."
        ],
        [
            "It is.",
            "In order to evaluate our data set in the workflow, we came up with two usage scenarios based on the use cases mentioned before 1st is diffused pedia.",
            "So we configured flex effusion in the following way.",
            "We used to reduce function to reduce to a six sources which is we data English, German, French, Dutch and Swedish and resolved.",
            "We are very simple preference list.",
            "So big data values are preferred over English over the German DB Pedia values and so on and so forth.",
            "We use the simple heuristic which is quality P Mott.",
            "That's the predicate median out degree for the property, which is calculated overall input sources for this property.",
            "And if it's one, then we consider as functional property and we only select one value.",
            "Otherwise we take all values for the enrichment scenario.",
            "We picked the Catalan chapter and used a very similar configuration, but we reduce only two SP pairs, whereas actually subject from the Catalan DB pedia and the resolve works as above.",
            "But we add cattle on as the highly prioritized source in this preference list.",
            "As a result.",
            "We studied or we significantly?"
        ],
        [
            "Improved improved the coverage.",
            "There is some numbers or actually a lot of numbers in here, so we compare diffuse data set with the individual source datasets.",
            "Ann just want to highlight some so one is that the coverage of the actual ontology is increased so you can see this is the highest vocabulary usage in the English chapter and after the Fusion we have like almost cover the entire DB Peter Ontology with close to.",
            "2300 properties and also quite important information density per entity was significantly increased, so in the the biggest source which is wiki data is close to four or in the initials for that five, and we improved to 7 and in Dutch is also close to 7.",
            "But it has yeah much less information compared to the other sources."
        ],
        [
            "We also had to look on type coverage so we studied the three major classes, person companies, location and animals, and we gained a type.",
            "Gain for these types, ranging from 10% to 33%.",
            "We observed also some important thing which is highlighted in the second column of in the 2nd row of each type, which is only in source.",
            "So we basically had a look at the attached PDF and the Dutch TV pedia was able to contribute around one 100,000 knew locations which did exist at least in one other source, but it was not.",
            "Type in this resource so there were, for example, existed in English Wikipedia or the Pedia.",
            "But we didn't know that it was an actual location."
        ],
        [
            "We also did this for functional properties and observed similar effects."
        ],
        [
            "Yeah, when it comes to data quality, that's quite challenging because there's no gold standard.",
            "Process your data set.",
            "So we came up with a release in a simple idea.",
            "We use the RDF unit framework which automatically generates unit tests for data quality based on the schema knowledge.",
            "So the ontology and basically generated around thousand tests including domains, domain and range tests, or if persons carry birthdays and so on, and we define.",
            "Similar quality indicator.",
            "A lower number of failures implies better data quality."
        ],
        [
            "And by using this simple quality indicator, we just compared the Fusion data set with the individual sources and if they fail rates in the Fusion were better than in the actual source, then we consider this as an improvement.",
            "And if we just summed up and if the improvements outperformed the decreased amount of quality, then we consider this as a tendency of quality improvement."
        ],
        [
            "We also studied for the enriched use case, how well or how it actually affected the density of the knowledge graph.",
            "So just want to highlight here we studied the indegree of the resources which shows that the intra connection of the Knowledge Graph was gained or boosted by 11 and the extra linking was boosted by around 23."
        ],
        [
            "We also had to look for this enrichment use case, which source actually contributed data, and as you can see.",
            "The highest or the biggest amount of data is contributed from big data and from the English chapter, which is not really surprising, but we also classified the kind of information which was contributed.",
            "I don't have time to go into the details, but the blue box for example indicates well synchronized data.",
            "So that means the data is aligned with other sources, or at least not challenged and green, for example, indicates that this data is not available in other sources, so this novel information or unique information, but we don't know whether it's outlier.",
            "Or whether it's actual complementary data."
        ],
        [
            "There's a GFS data browser where you can browse through the data.",
            "It's at globalthatpedia.org, or you can also download the dumps on the VPN."
        ],
        [
            "Databus I skip related work through through time can."
        ],
        [
            "Strains to sum up.",
            "What are the contributions we came up with this pre Fusion data set which is one of the largest open general domain purpose knowledge graphs both statement level provenance and I think it's a good playground for research questions, but it also with the help of the scalable workflow we proposed allows a mass production of custom knowledge graphs or DPS and we have shown the usefulness of the data set in the workflow with a qualitative and quantitative evaluation given these two.",
            "Use it."
        ],
        [
            "Scenarios.",
            "There's a lot of future work.",
            "First of all we want to integrate other datasets in the future.",
            "Currently working on Musicbrainz and YFI want to carry out the Silver standard evaluation for specific domain that we get more insight into the data quality aspect we need more sophisticated reduce or resolve functions.",
            "I also want to have mapping management which works similar like the idea management but for properties.",
            "This is already developed at the moment with all equivalent class and equivalent properties.",
            "We can perform clustering validation on the ID management.",
            "And I dream of iterative flexor Fusion, which enables feedback loops between these single steps or between mapping linking infusion of their menu errors in the Fusion.",
            "Then we proposed the spec to the mapping."
        ],
        [
            "Thank you.",
            "Thank you, thank you for the presentation and the work.",
            "Are there any questions?",
            "Do you encounter differences in modeling decisions there as well?",
            "Like that?",
            "You might have a car model that is.",
            "Like the specific model has one infobox in one language and all related models have one infobox in the other language?",
            "Yes, so with the help of the DPM mapping language we try to take care of this a bit, but there's this kernel arity here.",
            "Yeah, this match, let's say so.",
            "This is exactly as you mentioned, so there might be for a specific model there might be.",
            "An article carrying only information for the single model.",
            "But then there's maybe like a special addition of this model available for two years and another source there's also.",
            "Wikipedia article for this dedicated for this dedicated entity.",
            "So we basically ended in this prototype rely on the big data, same as links, so we basically trust that the editors investigator already took care of this kernel arity mismatch, and in case it doesn't make sense to them that they did not link it to each other so that they are really considered as two separate entities.",
            "So you would use and you also assume that the mappings that you have the DB pedia mappings that you have, and the wiki data that you have does not clash.",
            "Yeah, I basically assume that that's the best effort they provided for us, so this is a starting point, right?",
            "Of course, there potentially Arizona.",
            "I'm actually quite sure there are errors in it, so This is why I say it's also play count for research.",
            "So one could basically now use it to identify to perform his algorithm on top of identifying misaligned properties and then notices the semantics is not correct.",
            "This should be two different properties.",
            "For example.",
            "OK, very nice, thank you.",
            "Are there more questions?",
            "I think we can take one more quick question.",
            "So I had one question, so there was this floor example right where the object of the floor was four in one language and three in the other language.",
            "So will you be resolving those sorts of conflicts as well?",
            "In your car, do just gather these different triples.",
            "Yeah, I so I. I specifically took this example because we are we having a semantic gap in here, right?",
            "So you probably know that some countries start counting at one and some start counting at 0.",
            "So This is why I took this by.",
            "In this case it was really an error and it's now fixed.",
            "So we have some project with this GFS data browser in collaboration with Wikimedia and some.",
            "Editor already stumbled open and fixed the error in the source, but indeed we need some global schema for this and this clonal schema and schema requires some official semantic specification which states OK, we start to count at zero or we start to count as one and then the input sources need to be aligned in order to yeah match or correspond with this definition.",
            "So you need to normalize and add one or subtract 1 value in order to.",
            "Normalized values beforehand so fast this answer your question, yes, OK, thank you.",
            "Let us thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, you have to deal with data source selection and you have to think about conflict resolution.",
                    "label": 0
                },
                {
                    "sent": "But most importantly the individual property and entity selection or reduction for your domain or use case.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's highly highly needs to be highly customizable and the idea or goal of flex effusion is to make it easier to mass produce such custom knowledge graphs or custom DPS how icon.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then discovered the paper we propose to measure things on.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, we proposed the profusion data set, which basically is a precomputed and merged global view of all triples from all the input sources that will statement level provenance and on the other hand we propose to flex effusion workflow which can be applied on this pre Fusion data set with two simple methods in order to derive your custom knowledge graph.",
                    "label": 1
                },
                {
                    "sent": "This is the big picture which I'm going to use to run you through the workflow itself.",
                    "label": 0
                },
                {
                    "sent": "Let's just start right away with.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Source selection and our workflow.",
                    "label": 0
                },
                {
                    "sent": "We make use of the novel DPT later pass.",
                    "label": 0
                },
                {
                    "sent": "Technology, which is in a nutshell, DPS Maven plus GitHub for data.",
                    "label": 0
                },
                {
                    "sent": "So you register your data by uploading structured Meteor data.",
                    "label": 0
                },
                {
                    "sent": "More specific the data and vocabulary and the data is organized using the concept hierarchy publisher Group, Artifact version file and this allows us to define a flexible input source selection.",
                    "label": 0
                },
                {
                    "sent": "And also to store and persist intermediate and final results of Flex effusion on the database.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before we can actually start with the Fusion, we have to think about identifiers in the different sources.",
                    "label": 0
                },
                {
                    "sent": "Do not show redeveloped global ID management as a solution for this.",
                    "label": 0
                },
                {
                    "sent": "We start with collecting the data from the data of us with our custom sparkle query.",
                    "label": 0
                },
                {
                    "sent": "Collect all same as links and I rise in this data set computer clustering based on connected components algorithm and finally assign a global ID for every single cluster.",
                    "label": 0
                },
                {
                    "sent": "This is then persisted on a dataverse.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the next step we actually produce this pre Fusion data set.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we start fetching again the data from the data bus and we also take the global ID assignment, which I previously introduced and now we're just a straight forward replace every IRRI with its corresponding global ID.",
                    "label": 0
                },
                {
                    "sent": "Moreover, we keep track of the provenance from the individual files.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the second step we derive so-called pre fused entities which is just simply a grouping all tribals first by their now global ID subject and then by their predicate values.",
                    "label": 0
                },
                {
                    "sent": "So you can see over here these are the two different values resources of the Eiffel Tower from the French and English chapter, and then they're basically merged into one.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The result of this are pre Fusion is stored in a Jason LD and a custom Jason LD format.",
                    "label": 0
                },
                {
                    "sent": "We store 1 record one chasing record which looks like this for every subject prepare predicate pair.",
                    "label": 0
                },
                {
                    "sent": "So in this case you can see again the example the Eiffel Tower with the flow count properties carrying the two different values for and three reported in three different sources.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the next one final step, we can.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually perform diffusion.",
                    "label": 0
                },
                {
                    "sent": "In order to make it really flexible, we came up with two functions.",
                    "label": 0
                },
                {
                    "sent": "The first one is to reduce function, which is a customizable function which is applied for every subject predicate pair to reduce or filter the amount of information for the entities.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to remove irrelevant or bad data or to reduce Fusion decisions in the next step.",
                    "label": 0
                },
                {
                    "sent": "One example could be to fill the owner for the DBO Birthplace properties or entities of type person.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the next step we performed the resolve function which picks a number of objects from the list of each reduced subject predicate pair, which is then finally handed over to the result data set.",
                    "label": 0
                },
                {
                    "sent": "The purpose is quite simple.",
                    "label": 0
                },
                {
                    "sent": "We want to resolve conflicts in order to improve the data quality.",
                    "label": 0
                },
                {
                    "sent": "One example is to just select all values for labels, but another one could be to perform majority written for functional.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "In order to evaluate our data set in the workflow, we came up with two usage scenarios based on the use cases mentioned before 1st is diffused pedia.",
                    "label": 0
                },
                {
                    "sent": "So we configured flex effusion in the following way.",
                    "label": 0
                },
                {
                    "sent": "We used to reduce function to reduce to a six sources which is we data English, German, French, Dutch and Swedish and resolved.",
                    "label": 0
                },
                {
                    "sent": "We are very simple preference list.",
                    "label": 0
                },
                {
                    "sent": "So big data values are preferred over English over the German DB Pedia values and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "We use the simple heuristic which is quality P Mott.",
                    "label": 0
                },
                {
                    "sent": "That's the predicate median out degree for the property, which is calculated overall input sources for this property.",
                    "label": 0
                },
                {
                    "sent": "And if it's one, then we consider as functional property and we only select one value.",
                    "label": 1
                },
                {
                    "sent": "Otherwise we take all values for the enrichment scenario.",
                    "label": 1
                },
                {
                    "sent": "We picked the Catalan chapter and used a very similar configuration, but we reduce only two SP pairs, whereas actually subject from the Catalan DB pedia and the resolve works as above.",
                    "label": 0
                },
                {
                    "sent": "But we add cattle on as the highly prioritized source in this preference list.",
                    "label": 0
                },
                {
                    "sent": "As a result.",
                    "label": 0
                },
                {
                    "sent": "We studied or we significantly?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Improved improved the coverage.",
                    "label": 0
                },
                {
                    "sent": "There is some numbers or actually a lot of numbers in here, so we compare diffuse data set with the individual source datasets.",
                    "label": 0
                },
                {
                    "sent": "Ann just want to highlight some so one is that the coverage of the actual ontology is increased so you can see this is the highest vocabulary usage in the English chapter and after the Fusion we have like almost cover the entire DB Peter Ontology with close to.",
                    "label": 0
                },
                {
                    "sent": "2300 properties and also quite important information density per entity was significantly increased, so in the the biggest source which is wiki data is close to four or in the initials for that five, and we improved to 7 and in Dutch is also close to 7.",
                    "label": 0
                },
                {
                    "sent": "But it has yeah much less information compared to the other sources.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also had to look on type coverage so we studied the three major classes, person companies, location and animals, and we gained a type.",
                    "label": 0
                },
                {
                    "sent": "Gain for these types, ranging from 10% to 33%.",
                    "label": 0
                },
                {
                    "sent": "We observed also some important thing which is highlighted in the second column of in the 2nd row of each type, which is only in source.",
                    "label": 0
                },
                {
                    "sent": "So we basically had a look at the attached PDF and the Dutch TV pedia was able to contribute around one 100,000 knew locations which did exist at least in one other source, but it was not.",
                    "label": 0
                },
                {
                    "sent": "Type in this resource so there were, for example, existed in English Wikipedia or the Pedia.",
                    "label": 0
                },
                {
                    "sent": "But we didn't know that it was an actual location.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also did this for functional properties and observed similar effects.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, when it comes to data quality, that's quite challenging because there's no gold standard.",
                    "label": 0
                },
                {
                    "sent": "Process your data set.",
                    "label": 0
                },
                {
                    "sent": "So we came up with a release in a simple idea.",
                    "label": 0
                },
                {
                    "sent": "We use the RDF unit framework which automatically generates unit tests for data quality based on the schema knowledge.",
                    "label": 0
                },
                {
                    "sent": "So the ontology and basically generated around thousand tests including domains, domain and range tests, or if persons carry birthdays and so on, and we define.",
                    "label": 0
                },
                {
                    "sent": "Similar quality indicator.",
                    "label": 0
                },
                {
                    "sent": "A lower number of failures implies better data quality.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by using this simple quality indicator, we just compared the Fusion data set with the individual sources and if they fail rates in the Fusion were better than in the actual source, then we consider this as an improvement.",
                    "label": 0
                },
                {
                    "sent": "And if we just summed up and if the improvements outperformed the decreased amount of quality, then we consider this as a tendency of quality improvement.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also studied for the enriched use case, how well or how it actually affected the density of the knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "So just want to highlight here we studied the indegree of the resources which shows that the intra connection of the Knowledge Graph was gained or boosted by 11 and the extra linking was boosted by around 23.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also had to look for this enrichment use case, which source actually contributed data, and as you can see.",
                    "label": 0
                },
                {
                    "sent": "The highest or the biggest amount of data is contributed from big data and from the English chapter, which is not really surprising, but we also classified the kind of information which was contributed.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to go into the details, but the blue box for example indicates well synchronized data.",
                    "label": 0
                },
                {
                    "sent": "So that means the data is aligned with other sources, or at least not challenged and green, for example, indicates that this data is not available in other sources, so this novel information or unique information, but we don't know whether it's outlier.",
                    "label": 0
                },
                {
                    "sent": "Or whether it's actual complementary data.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a GFS data browser where you can browse through the data.",
                    "label": 0
                },
                {
                    "sent": "It's at globalthatpedia.org, or you can also download the dumps on the VPN.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Databus I skip related work through through time can.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strains to sum up.",
                    "label": 0
                },
                {
                    "sent": "What are the contributions we came up with this pre Fusion data set which is one of the largest open general domain purpose knowledge graphs both statement level provenance and I think it's a good playground for research questions, but it also with the help of the scalable workflow we proposed allows a mass production of custom knowledge graphs or DPS and we have shown the usefulness of the data set in the workflow with a qualitative and quantitative evaluation given these two.",
                    "label": 0
                },
                {
                    "sent": "Use it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scenarios.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of future work.",
                    "label": 0
                },
                {
                    "sent": "First of all we want to integrate other datasets in the future.",
                    "label": 0
                },
                {
                    "sent": "Currently working on Musicbrainz and YFI want to carry out the Silver standard evaluation for specific domain that we get more insight into the data quality aspect we need more sophisticated reduce or resolve functions.",
                    "label": 0
                },
                {
                    "sent": "I also want to have mapping management which works similar like the idea management but for properties.",
                    "label": 0
                },
                {
                    "sent": "This is already developed at the moment with all equivalent class and equivalent properties.",
                    "label": 0
                },
                {
                    "sent": "We can perform clustering validation on the ID management.",
                    "label": 0
                },
                {
                    "sent": "And I dream of iterative flexor Fusion, which enables feedback loops between these single steps or between mapping linking infusion of their menu errors in the Fusion.",
                    "label": 0
                },
                {
                    "sent": "Then we proposed the spec to the mapping.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you for the presentation and the work.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "Do you encounter differences in modeling decisions there as well?",
                    "label": 0
                },
                {
                    "sent": "Like that?",
                    "label": 0
                },
                {
                    "sent": "You might have a car model that is.",
                    "label": 0
                },
                {
                    "sent": "Like the specific model has one infobox in one language and all related models have one infobox in the other language?",
                    "label": 0
                },
                {
                    "sent": "Yes, so with the help of the DPM mapping language we try to take care of this a bit, but there's this kernel arity here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this match, let's say so.",
                    "label": 0
                },
                {
                    "sent": "This is exactly as you mentioned, so there might be for a specific model there might be.",
                    "label": 0
                },
                {
                    "sent": "An article carrying only information for the single model.",
                    "label": 0
                },
                {
                    "sent": "But then there's maybe like a special addition of this model available for two years and another source there's also.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia article for this dedicated for this dedicated entity.",
                    "label": 0
                },
                {
                    "sent": "So we basically ended in this prototype rely on the big data, same as links, so we basically trust that the editors investigator already took care of this kernel arity mismatch, and in case it doesn't make sense to them that they did not link it to each other so that they are really considered as two separate entities.",
                    "label": 0
                },
                {
                    "sent": "So you would use and you also assume that the mappings that you have the DB pedia mappings that you have, and the wiki data that you have does not clash.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I basically assume that that's the best effort they provided for us, so this is a starting point, right?",
                    "label": 0
                },
                {
                    "sent": "Of course, there potentially Arizona.",
                    "label": 0
                },
                {
                    "sent": "I'm actually quite sure there are errors in it, so This is why I say it's also play count for research.",
                    "label": 0
                },
                {
                    "sent": "So one could basically now use it to identify to perform his algorithm on top of identifying misaligned properties and then notices the semantics is not correct.",
                    "label": 0
                },
                {
                    "sent": "This should be two different properties.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "OK, very nice, thank you.",
                    "label": 0
                },
                {
                    "sent": "Are there more questions?",
                    "label": 0
                },
                {
                    "sent": "I think we can take one more quick question.",
                    "label": 0
                },
                {
                    "sent": "So I had one question, so there was this floor example right where the object of the floor was four in one language and three in the other language.",
                    "label": 0
                },
                {
                    "sent": "So will you be resolving those sorts of conflicts as well?",
                    "label": 0
                },
                {
                    "sent": "In your car, do just gather these different triples.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I so I. I specifically took this example because we are we having a semantic gap in here, right?",
                    "label": 0
                },
                {
                    "sent": "So you probably know that some countries start counting at one and some start counting at 0.",
                    "label": 0
                },
                {
                    "sent": "So This is why I took this by.",
                    "label": 0
                },
                {
                    "sent": "In this case it was really an error and it's now fixed.",
                    "label": 0
                },
                {
                    "sent": "So we have some project with this GFS data browser in collaboration with Wikimedia and some.",
                    "label": 0
                },
                {
                    "sent": "Editor already stumbled open and fixed the error in the source, but indeed we need some global schema for this and this clonal schema and schema requires some official semantic specification which states OK, we start to count at zero or we start to count as one and then the input sources need to be aligned in order to yeah match or correspond with this definition.",
                    "label": 0
                },
                {
                    "sent": "So you need to normalize and add one or subtract 1 value in order to.",
                    "label": 0
                },
                {
                    "sent": "Normalized values beforehand so fast this answer your question, yes, OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Let us thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}