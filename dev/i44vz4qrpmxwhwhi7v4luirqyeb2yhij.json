{
    "id": "i44vz4qrpmxwhwhi7v4luirqyeb2yhij",
    "title": "Text Information Extraction",
    "info": {
        "author": [
            "Kamal Nigam, Google, Inc."
        ],
        "published": "Feb. 25, 2007",
        "recorded": "September 2006",
        "category": [
            "Top->Computer Science->Information Extraction"
        ]
    },
    "url": "http://videolectures.net/mlas06_nigam_tie/",
    "segmentation": [
        [
            "OK good alright so.",
            "William gave the grand overview of text classification.",
            "I'm going to try to do the same thing in the same amount of time for information extraction.",
            "Both."
        ],
        [
            "So it seems like impossible challenges, so we'll just see if we can get through in an hour.",
            "So I first want to start with a little bit of motivation for why information extraction is really interesting problem with lots of potential applications, and then I'll tell you a little bit about sort of how I think about what information extraction is, and then we'll jump into some of the techniques.",
            "So here's just a typical Google search.",
            "And you know, don't tell anyone outside this room that I'm saying things that Google that don't look beautiful.",
            "But you know, imagine that you are looking for a job as a Baker, right?",
            "Sort of obvious thing that you would do if you were typing in a Google query is you would do something like Baker job opening.",
            "Does your magic little red pen things that part of.",
            "Yes.",
            "They shoot anyone in the eye, please.",
            "Yeah, I got it wrong, but Luckily it was on my hand.",
            "Yeah OK, good right?",
            "So that you know the top top search results you should be slightly disappointed by right and the main intuition of what's going on is that Google doesn't really understand that when you say Baker in that context, we're really meaning Baker.",
            "The title of some job that's actually out there, right?",
            "You get the school district, you get a company and then finally down here.",
            "Search result #5 you actually do get a job opening for Baker and Apprentice Baker actually right and so.",
            "How could information extraction sort of try to make life better for you so?"
        ],
        [
            "About 6 five years ago, long and long time ago there was a company here in Pittsburgh called Whizbang Labs that lots of CME people.",
            "William myself and Tom Mitchell were all pretty involved with that.",
            "Was trying to take text classification and machine learning technologies and really apply them to real applications.",
            "And so there was a site not unlike monster.com called Flip dog.com that had the largest number of job postings on the web at that time.",
            "In your monsters business model is that people.",
            "Pay to list jobs in monster right?"
        ],
        [
            "Doug's approach is very different.",
            "It was that while still providing this kind of structured search interface where you could say I want to search in the category of food services and the title, I want to be Baker in the location.",
            "I want to be somewhere in the United States and then you get this nice listing.",
            "The data wasn't being provided to us by merchants."
        ],
        [
            "Or something in some kind of nice XML feed or something, but we actually went out into the web, found instances of people listing jobs for sale and sort of deconstructed what was actually going on here and understood it at this level of semantics, which is that we were able to say.",
            "Here's a job title which is ice cream grew.",
            "Here's the email address of the person you actually want to contact if you're interested in applying, here's the description.",
            "You might also be able to extract things like the location.",
            "Upper Midwest and you might using something like text classification.",
            "You might be able to look at the title and description and put it into some hierarchy of all the jobs from the web.",
            "So travel hospitality or something like that.",
            "And this in general having this kind of structured data is great for enabling various types of fast."
        ],
        [
            "Did search right?",
            "So here's another example of faceted search that I personally really enjoy using.",
            "Highly recommend it.",
            "It's called eBay Express, which is not unlike something like shopping.com, which you know if you're looking for something to buy, you know you can just give it lots of different attributes and values, and it's sort of nicely organized and that it knows that if you're in the sort of jewelry world that has things like type of material.",
            "If I choose gold to be the metal that I'm interested in, it knows automatically that they're white, gold, yellow gold.",
            "Rose gold and I can drill down on all of those types of things, right?",
            "You can even do things like the shape you know you want it to have sort of letters and initials in it.",
            "Right, and this type of where is this information coming from?",
            "What this I believe without knowing and."
        ],
        [
            "Tells that it's provided by merchants, but merchants.",
            "Often have descriptive text that is completely disastrous right from sort of completely non grammatical has maybe a tiny bit of formatting here, right?",
            "And there's lots and lots and lots of this data out there on the web that has no structured representation that someone can just hand you in an XML feed and so information extraction technologies at a very high level can perhaps recover some of the structure that's here and do something."
        ],
        [
            "Sing with it.",
            "A third example, which probably most of you are very familiar with, and if not, I highly recommend it, is sightseers.",
            "So Sightseer is basically a search engine over research papers and how do they get their information right so there you can do search sort of the same way you can do general keyword search but you also they also do lots of interesting things to understand what's actually going on in the document right?",
            "So they parts the header of research paper to understand what is the title.",
            "Who are the authors where the authors located?",
            "When was it published?",
            "And they also look at the citation section of the paper to recover the individual citations and build the citation graph right?",
            "And so using information extraction, they've sort of can do some canonicalization and normalization, and you can then see quickly who's referencing this paper and what paper would I like to see next.",
            "We'll come back to this domain actually a little bit throughout the talk."
        ],
        [
            "'cause I think it's a really interesting application.",
            "OK.",
            "So.",
            "Those are some really interesting ideas and they should all convince you.",
            "I hope that information extraction is going to be a great driver of things to come in the world, but I do want to step back a little bit and formalize what I think of as information extraction, and I'm just going to use this example, which is just this little snippet of a webpage of a company called Buzz Metrics, which is a company I used to sort of work for.",
            "But list the board members of their of that company.",
            "So the sort of I think of information extraction really is having three components that are sort of broadly defineable, and the first is just recovering the individual data items that you want from formatted text.",
            "And so in this particular example, you would probably imagine that you would be interested in identifying people names and job titles, right?",
            "And so you might have.",
            "A person name extractor and a job title extractor and we can leave aside for the moment whether those are actually separate things or whether there's somehow combined together, but broadly, you hope that what you would recover from this green being the names and orange being the titles question.",
            "Already texted me a lot of things, at least competition.",
            "This is all.",
            "Turn on seven seas, and so forth and few stories I suppose to please.",
            "Board members, right?",
            "So I think certainly a degenerate case of formatted text is something that comes with no formatting.",
            "Right and.",
            "Send email to five different languages.",
            "Yes, but I'll talk a little bit about that.",
            "I'm actually the techniques and I'm pretty presenting are more geared towards free text.",
            "Generally, if you have something that's highly structured, you might use some sort of wrapper induction techniques which I'm not going to talk about too much today.",
            "But in general, even even text that is in ASCII right has format, right?",
            "You can for example, your SEC filings have tables that are presented in straight ASCII, right?",
            "And there is sort of implicit formatting going on there in terms of rows and columns, even if it's not immediately obvious, even if it's represented in sort of a sequence of characters.",
            "OK, I'll watch it."
        ],
        [
            "OK, good so the second important thing that I think about for information extraction is understanding the relations between the fields that you've extracted, right?",
            "So in particular, here you have a bunch of names and job titles, but the each job title is associated with exactly one person name, right?",
            "So in this case where you're extracting out several fields of the same instance and there are multiple instances, you often hear that referred to as record Association, but more generally in something like a table, for example, right there are more interesting semantic relationships going on than just simple records.",
            "And so more broadly, just want to understand what are the relations between the thing."
        ],
        [
            "You're extracting.",
            "And Lastly, an important part is that the result of those two processes gives you a bunch of records, or more generally something else.",
            "But you want to be able to do normalization and deduplication.",
            "So for example in this previous slide down here is this guy James O'Hara.",
            "Well, you know there are other James O'hara's out there on the web.",
            "Here's one from IMDb, right?",
            "And this is actually a very different person than this James O'Hara, right?",
            "Whereas this Jim O'Hara, from some other random piece of the web?",
            "Is actually the same person.",
            "I think senior vice president for via new media measurement that looks close enough like the ought to be able to think with high confidence that these two people are the same and these two people are different.",
            "So that's normalization to get deduplication."
        ],
        [
            "Good today, given that we just have an hour I'm going to talk almost exclusively about field identification, which is just extracting out the sort of named entities or other types of fields you might be interested in.",
            "Already having taxonomy here.",
            "What are you doing?",
            "No, what we're looking for names.",
            "Yes, that's a great question, so you may have prefaced exactly my next slide.",
            "No two slides from now, right so?",
            "I'm going to pose this as a machine learning problem and generally for today.",
            "Anyway, we assume that what you're interested in extracting.",
            "We know ahead of time is sort of told to us, so it's sort of a finite set of slots that we want to fill.",
            "There has been some work more generally in sort of extracting out sort of general purpose relations from things like the whole web web as a whole.",
            "Maybe Tom will touch on some of that stuff and his talk later today, but I don't know.",
            "OK."
        ],
        [
            "So there's a bunch of history of how information extraction got about.",
            "I don't think we really talk about it too much, but basically started with a bunch of hand coded rule type techniques and then sort of transitioned into machine learning and there are these sort of two approximate parallel tracks, one which have dealt primarily with text based sorts of things like news articles and then another track which is more of a wrapper induction sort of approach that dealt with highly structured web pages, and then they sort of fused."
        ],
        [
            "In the middle.",
            "There's your history lesson in one slide.",
            "OK, so one thing that's important to think about is how is information extraction machine learning problem.",
            "It's actually somewhat different than sort of standard supervised learning, right?",
            "So in Standard supervised learning, you have a bunch of instances, and you assume that they are independently and identically distributed, and each one is sort of you think of it anyways, being rather separate from the other, and here actually the training data you're given is usually like a bunch of documents.",
            "Right, and each document is annotated with the various fields as they occur in the document, right?",
            "So all the names would have the tag name.",
            "All the addresses would have the tag address around them, and so.",
            "It's it's not exactly each document you can assume is IID usually, but it's really the sequence is the sequence tokens of text or the text it's occurring, and there is actually sort of relations between the various types of fields you want to extract.",
            "Right?",
            "So text classification I think falls usually more into this idea, but the stuff you were talking about the end sort of was breaking that a little bit.",
            "Another interesting thing, just at a high level, is that.",
            "Whereas text classification is usually using sort of whole document types of features like which words occur in the document.",
            "Here because we're interested in just identifying specific words and phrases in somewhere in the document, it's the local features that are really, really important here, right?",
            "So you actually, if you're trying to extract out location like a room location, usually you have features that are features of the words that you want to extract, and then you have sort of hopefully trying to identify something indicative about the words.",
            "Immediately before it there in the prefix or immediately after it.",
            "And then also you also have features regarding the boundaries, right?",
            "So did something change between here and here that might indicate that you might want to suddenly start extracting something."
        ],
        [
            "And.",
            "Just my as sort of a machine learning practitioner as opposed to more of a researcher, although that's a bit of a Gray area, I like to always remind people of what actually makes machine learning algorithms work right and.",
            "What really makes machine learning algorithms work is a lot of data, really clean data and really.",
            "Smart feature engineering, right?",
            "And so here are a bunch of examples of types of features that are good for information extraction, right?",
            "And they fall into this sort of all the categories I just had in the previous slide, right?",
            "So does the word itself begin with a number?",
            "You can imagine the things like room locations that might be really important, right?",
            "Is it in bold font, right?",
            "Is that token only punctuation?",
            "So there are lots of work word level features.",
            "There are also things like is it in a list of city names you might have.",
            "Might be pretty easy to find some large gazetteer of locations.",
            "And that would be a really good feature, but it's unlikely that that list would be.",
            "Exhaustive, right and also you would have with words on that list like.",
            "Sam, I'm sure there's a city somewhere called Sam, right?",
            "But Sam is a terrible word by itself for a city, so these are why these are not necessarily just wrote list that you would want to apply, but something that would be good as a feature.",
            "Then there's also things like whitespace and formatting."
        ],
        [
            "And then.",
            "Again, sort of.",
            "Things like capitalization, punctuation, and other sorts of long lists of things and then various conjunctions of these features.",
            "So is it?",
            "Is it a bold?",
            "Is it in a four word for words that are involved and is it at the beginning of the line or the end of line?",
            "So these are just to give you a kind of flavor for the types of features that we expose and you perform machine learning on.",
            "These are just called from various.",
            "Papers that have been written about information extraction just to give you a sense, OK?"
        ],
        [
            "So there are one way of thinking about information extraction that broadly five categories of techniques, and I'm talking about three of them today and they are just sort of candidate generation algorithm.",
            "So you might imagine that if you're doing classification, trying to extract out peoples names, you might consider having some little tiny program that emits all, say, consecutive capitalized words, and probably true that.",
            "Almost all names fall into that category, but there's lots and lots of non things like the beginning of every sentence.",
            "So you might classify the results of that kind of generation.",
            "Then there are approaches that sort of consider various windows of in the document or the explicitly try to find beginning and end points, and I'll talk about some sort of sequence based methods and then I won't touch on wrapper induction, which is a technique for recognizing repeated structure, typically on web pages."
        ],
        [
            "OK.",
            "So let's jump right into techniques here."
        ],
        [
            "Any questions before we move into techniques that the setting OK good.",
            "So the 1st way to consider doing performing information extraction is that you want to extract out a particular thing, say location, right and you're given some documents here, right?",
            "And so abstractly.",
            "You can consider every."
        ],
        [
            "Possible window.",
            "Of begin end points and treat that as an instance, right?",
            "And just try to classify that instance right and so all these these would all be negative instead."
        ],
        [
            "Says for locate."
        ],
        [
            "You would hope, right?",
            "And here is the instance that you would actually hope to be positive."
        ],
        [
            "And this sort of falls if you pose information extraction in this way, it basically reduces approximately 2 sort of the standard supervised learning setting, right?",
            "Which is that if you have these annotated documents, the positive instances are those start and end positions that actually mapped to the extraction extract field that you want to extract out, and then basically any other of the N squared instances on that document are all negative instances.",
            "Right, and it's it's a little bit ridiculous to think about classifying all N squared things, but you know, if you fix the length of your window to something more reasonably sized, then that limits the complexity of that.",
            "So for example, one thing that was done by Dane Free Tag is that he set the minimum window length and the maximum window lengths to be considered based on the training data.",
            "So if the longest location was seven tokens long, then he just didn't consider any window longer than seven.",
            "Seems to do.",
            "Pretty well.",
            "I mean again, then you have a track where you have standard supervised learning.",
            "You have negative data, you have positive data.",
            "You have features based on all the kinds of features that we talked about in the previous slide, and you have any kind of black box so.",
            "In practice that works OK, but there's.",
            "Several people, three diff.",
            "Sort of the three big players, sort of in the late 90s, all found that variance of rule learning techniques all performs substantially better than something sort of like naive Bayes.",
            "And here's like an example rule that was learned by think Dane system.",
            "So something like a course number like 15433 would be an example, right?",
            "Has something is of course number if it has two tokens, like 15 and 443.",
            "If none of the tokens that you're hoping to extract look like there a title of a course 'cause there's a number, and then there's some token that is exactly before the one.",
            "Where you start to extract, there actually is a title, so this would be like.",
            "Introduction to computer Programming 1601 right and then there is some token B in your number that you're trying to extract that rippleton that has exactly 3 characters, and so this is 1, so that would be the 4:43.",
            "So this is one rule that was learned by Dane system and the way that this is formulated is that you know the rules that you're trying to learn.",
            "The hypothesis space uses a lot of conjunctions of features which.",
            "Turned out to be very important and also uses sort of relations in ordering among the word tokens.",
            "Right, so this is sort of.",
            "One intuition about what's important to make information extraction helpful.",
            "It's a lot of these conjunctions of features, and it's a lot of the sort of sequencing information of what comes before what comes after.",
            "And then if you're just going to give a bunch of features of each of the tokens, then you're going to have less a good chance of actually sort of breaking through and getting something interesting as a result."
        ],
        [
            "Good.",
            "So the next next set of techniques is boundary detection and Boundary section is pretty easy to understand as well.",
            "It's basically what you hope to do is find the starting point of some extraction, so you just."
        ],
        [
            "Inssider, every point in the document and you say, is this a starting point of."
        ],
        [
            "Location is."
        ],
        [
            "This is starting point location right and hopefully you say yes here and no everywhere else, and then you have a similar classification task for the endpoint of a location."
        ],
        [
            "Right, and you hope that all up to there would be negative and that this would be yes.",
            "And then you have some sort of magic bit of Association too.",
            "Associate the starting points with the ending points, and then you've got your extracted field.",
            "Right they can.",
            "That should make you all a little nervous.",
            "I hope.",
            "Right, 'cause there's this sort of weird decoupling between the big."
        ],
        [
            "Getting an ending of the field, so that's good.",
            "If you feel nervous 'cause you should feel nervous but actually you know.",
            "It turns out that in some instances actually doesn't work too badly.",
            "So here's the formulation that was done by Nickisch Merican Dean.",
            "They basically learn three models, but the sort of 1st two are the most important ones.",
            "They learned the starting.",
            "A class for the start, and that's a probabilistic model that actually they learned by boosting.",
            "And as well for the end.",
            "And then they combine these two to sort of try to find out where the actual field was by basically multiplying the probability of the start point times the probability of the endpoint plus the probability of the length, right?",
            "So taking some information into the relation between the start and the end where the length is just basically a histogram from your training data.",
            "So it's something it's very similar to sort of providing a maximum length for your sliding window.",
            "Although it does provide a little bit of a prior over the length of the thing.",
            "And the boosting was sort of run in a pretty straightforward way running over simple boundary patterns and features of the style that we've seen on the previous slide."
        ],
        [
            "So these two techniques, there's nothing too at the high level.",
            "Nothing to elaborate really is going on.",
            "And I personally feel a little uncomfortable with these types of techniques for sort of really feeling like we're doing the right thing, right?",
            "So here are some of the here's some of the problems.",
            "One is that.",
            "There's lots of Inter relations between the fields that you want to extract that are just completely going unconsidered.",
            "So, for example, in a seminar announcement.",
            "I know that you essentially never see the end time of a seminar announcement.",
            "Before you see the start time right, and there's nothing at any of these techniques that takes that type of knowledge into account.",
            "Right?",
            "Also, it's very unlikely that the start time and the location would actually overlap each other.",
            "Right, because typically a token would be either one or the other and there's nothing that takes that into account as well.",
            "Right, and as we talked about with the Boundary method, sort of putting down the start and end sort of strongly together by themselves and then try to sort of Patch the results together at the end should leave us feeling a little queasy."
        ],
        [
            "So that brings us to what I'm going to spend a pretty much the rest of the talk on, which are finite state machines, which sort of work directly to address this kind of problem."
        ],
        [
            "So the standard baseline implementation of what of a sort of machine learning for finite state machines is a hidden Markov model.",
            "So just to take a little bit of a pulse, how many people feel like they could write down some equations for hidden Markov models?",
            "Sort of there to that familiar.",
            "Anyone?",
            "OK, so maybe 1/3 to 1/2 and how many people sort of know what a hidden Markov model is or don't really know at all.",
            "OK, so also about a third good.",
            "OK, that's helpful.",
            "And did William talk at all about sort of naive Bayes?",
            "Being a generative model of text?",
            "A little bit OK good OK. Good, OK, so hidden Markov models are a generative model for text, which means that there is a probability distribution written down and you make this crazy assumption that all documents in the world or the ones that you're considering for your machine learning task, were actually written by this generative process.",
            "Right, so I know that we all sort of have written things before and we don't follow any kind of probabilistic model.",
            "And this probabilistic model is going to sound so insane that you should run away screaming thinking this could do a good job for information extraction, but it actually does quite well.",
            "So here's the model that we're going to consider right.",
            "Let's say that we're interested in only extracting two things, location and people.",
            "And so the simple thing we do is, we assume that words come from one of three possible states that we're in, or either in the state where we are writing down person names.",
            "Or in the state we're writing down a location name or were in the state where we are writing down any other kind of word, right?",
            "And so if we have the sentence like yes, Abraham Lincoln was born in Kentucky.",
            "Which is actually a false statement, I think.",
            "Then the sort of word yes was actually generated by us living in this state where the word Abraham in the word Lincoln was generated by us living in this red state.",
            "And the way that this sort of state transition matrix model works is that you do two things.",
            "You start off in some state like the background state, and you generate a word and the way that you generate a word is you have a probability distribution over all words in the English language.",
            "Randomly draw out of that probability distribution.",
            "And then what you do is you transition to another state and so from this great state there are three possible outgoing transitions.",
            "There's the transition back to itself.",
            "There's a transition to person, and there's a transition to location, and there's likewise probability distribution over those three possible next states.",
            "The independence assumptions that this model is making is that the word distribution, which is a unigram word distribution just over single words, is dependent only upon the state that you're in.",
            "So background, person and location all have different unigram models for words, and that the next state depends only on the current state that you're in, right?",
            "So from here there's these probabilities region over just those three different arcs, and you take one of those three.",
            "So this leads to the following sort of generative model, which is the joint probability for the words that are actually written down and the states that you were actually in when you generated those words is basically.",
            "The following general process, which is you're in a state and you generate a word X.",
            "You're in statewide networks and then you generate the next state Whitey based on the previous state, white minus one.",
            "And this is the product over the length of your sequence, and so you repeat that as many times as you want.",
            "Are there any questions about that?",
            "'cause this is a bit fundamental for the rest of where we're going.",
            "OK, good, very good.",
            "So if you imagine you have such a model right, the way you would actually.",
            "Use one of these models for information extraction is that we are given a bunch of text.",
            "And what we want to recover is the state that we were in when we generated each of those words.",
            "Right, So what we want to basically get is we're given these X is we want to figure out what those wise are right and if we are in a red state when we generate the word Abraham that we're going to call the word Abraham and name.",
            "And if we are in a green, if we're in the green state, then we're going to call Kentucky location.",
            "So basically you have a probability distribution.",
            "It turns out that you can pretty efficiently figure out the most likely state sequence using dynamic programming through a specific variant of unamic programming, called the Viterbi algorithm.",
            "Right, and this is not anything that's new or original.",
            "For information extraction, it Markov models have been around for a long time and they are used heavily in lots of communities such as speech recognition.",
            "Alright, good then the other thing to consider about hidden Markov models is that how do you actually create one, right?",
            "This is machine learning right?",
            "And so I give you a pile of annotated documents and I say well, how give me back a hidden Markov model.",
            "So the first thing you need to do is you need to write down one of these state transition diagrams.",
            "Right, and so you can do this in a naive way.",
            "You can basically create one state for every possible class that you're interested in extracting, and then one additional one for background.",
            "And you can create transitions from every state to every other state, and that's a very reasonable way to proceed.",
            "You might consider adding in some additional domain knowledge.",
            "So for example, if you wanted to enforce the fact that the end time of.",
            "Seminars came after the start time.",
            "You could have a state structure that requires you to pass through the start time node before you have any chance of ever getting to the end time node.",
            "So you can add domain knowledge through your state transition diagram.",
            "And then what you need to do is you need to basically estimate all these probabilities from your training data right?",
            "And so since this is a generative model, we do the sort of nice probabilistic thing which is we want to select the parameters that are going to maximize the likelihood of having created our training data.",
            "Right and 1st sort of standard hidden Markov models.",
            "You would use something like Baum Welch forward backward or something like expectation maximization, but here actually.",
            "Because every basically every word is annotated with what state it was generated from.",
            "Usually you can get away without doing anything like that and do something in closed form.",
            "Now sometimes that's not true, because you may, for example, have multiple person states in your model.",
            "If your model is more complicated, for example, you might have tagged Abraham Lincoln's name, but you might have a separate state for first names in a separate, safer last names, and in that case something like a bomb well should be more appropriate.",
            "OK, good."
        ],
        [
            "Let me show you 2 examples of hidden Markov models in action, and then we're going to go into something another sequence based technique called conditional random fields.",
            "That's a lot more powerful.",
            "OK, so this is an example based on paper headers, very similar to the sightseer data that we saw before.",
            "Where what you do is you get a computer science research paper and you want to be able to basically extract out things like the authors, the title, where the authors are affiliated with the abstract and things like that.",
            "And here we.",
            "Did something slightly different than just sort of a plain vanilla hidden Markov model, which is sort of two things of distinction.",
            "One is that we wanted to actually be able to learn the state structure that we were to extract, as opposed to just doing something naive, like providing a completely connected hmm or do something, spend a huge amount of time hand building a state transition diagram, and so the approach here was actually that.",
            "You sort of took each of your training instances and wrote down a complete state.",
            "Transition thing for it.",
            "So for example, if you had a title, author, affiliation, affiliation, author.",
            "If that was sort of the 1st.",
            "5 words you would actually just.",
            "Free State structure like this?",
            "So that's actually, and then you sort of repeat that for all your training examples and you put a little epsilon transition here at the beginning.",
            "And then, so that's a very very large state.",
            "Structure is basically one state per token in your training data, and then you start in order to get like a real compact representation.",
            "You start merging States and the way that you merge states as you do it in sort of Asian setting, which is that you have a structural prior sort of that prefers to have fewer States and fewer transitions, and you sort of combine that with the information that you're gaining or losing.",
            "So you start to do.",
            "You can consider various types of greedy pairwise mergings and end up with hopefully a more compact representation.",
            "The other interesting thing that happened is that.",
            "We improved the estimates of output probabilities using data that wasn't just the annotated data, right?",
            "So, for example, there's a state which is, you know, author names, right?",
            "And it's you know it's pretty expensive to hand label data, and so you might have.",
            "Maybe we labeled you know 500 or 1000 such papers, and so that gives you.",
            "Maybe, maybe there are two authors per paper that gives you like 1000 or 2000 names.",
            "Well, you're learning a pretty big probability distribution here, especially over something like person name, because there are just lots and lots of names you don't know about.",
            "But you know, there's actually a lot of data out there about the types of names that we see.",
            "In particular, you can think of big tech files, which are, you know, citation, basically collections of citations where the names are actually explicitly there as a field, right?",
            "And so you can collect a bunch of big tech files.",
            "I would hope that you would actually have a much better sense of what the word probability distribution over author is by looking at those files.",
            "Then if you just look at your small number of labeled training examples and so we took a bunch of these fields an improve their probability estimates.",
            "Using this, what we call distantly labeled data?"
        ],
        [
            "Second example of using hidden Markov models for information extraction is nimble project and this is probably the 1st.",
            "Really well known case of using sequence based techniques for information extraction.",
            "And they hear they had a high level.",
            "They had a state structure that was pretty simple.",
            "They had about 7 different things that they wanted to extract.",
            "Things like person and organization and various numeric things like money.",
            "And they basically to lie to you for a minute.",
            "They had this completely connected 7 state model or 8 state model.",
            "And then they just perform sort of standard hidden Markov model type modeling.",
            "Here the two things that they actually did quite differently.",
            "One is that instead of modeling each individual word probability distribution there as a unigram distribution, they modeled as a bigram distribution, and So what does that really mean?",
            "So sort of what that means is that this is not really one state.",
            "Right, it's really actually.",
            "V States of V is the number of words in the English language, right?",
            "And so you have, because the bigram model is basically from any single word.",
            "What's the probability of transitioning to any other word right?",
            "And so you can imagine this.",
            "Very large sort of V order V number of states with lots and lots of transitions.",
            "The transitions in between here, though, however, were all tide.",
            "So for example, if this is.",
            "So you're sort of person.",
            "You have lots of things like you know able and Arthur and and.",
            "Right and then you have all these because it's a, it's a bigram model.",
            "You have these all these different word probability distributions.",
            "And then you also have another sort of large state for organization.",
            "And while sort of conceptually, there are transitions from every state in here to every state in here.",
            "They are all of these transitions shared the same set of parameters.",
            "So you didn't.",
            "So basically you didn't sort of completely exacerbate the parameter estimation problem, and this is actually a pretty reasonably common technique.",
            "I think it's called like parameter tying, and there are other cases of these parameter tying things in the research literature.",
            "The other really interesting thing that they did.",
            "And which really gets to the core of why Hmm's are not completely satisfactory for information extraction is that they wanted to use various types of capitalization and features about the format of numbers, right?",
            "If you remember, one of the things I showed you in the earlier in the talk was this long list of features that had nothing to do with actually the identity of the word itself, right?",
            "It included the identity of the word, but it basically had like 50 other types of features based on formatting and capitalization.",
            "And other sorts of things that are not being used at all here by hidden Markov models, right?",
            "The sort of vanilla hidden Markov model is just looking at the identity of the word, and so one of the reasons we're really sad about hidden Markov models is that they aren't using this rich source of information, and so they sort of tried to hack around it by including this small set of perhaps a dozen features, and each word was really not a word distribution, but it was paired with one of these features.",
            "So for example.",
            "Most of these are various flavors of trying to recognize digits and money at the top, and then all these ones in the bottoms are various types of capitalization, sorts of features, and so the word Abraham would actually be a pair of the word Abraham and initial caps like Sally.",
            "So this.",
            "One of the facts is that this really expands the parameter space, right?",
            "You basically by creating 10 different things, you've almost haven't quite, but you really almost increased the number of parameters by an order of magnitude.",
            "It's not quite true because the word Abraham is never going to be a 2 digit number, but it could have any kind of Abraham, kind of any kind of this capitalization.",
            "Right, and so that's there's definitely tension there, right?",
            "You can't really creating that many parameters should be somewhat scary.",
            "Right, good, OK so.",
            "Hmm's worked pretty well in practice, but why should we not be completely satisfied?",
            "Hidden Markov models naturally have word entities, and they sort of don't allow for these features to be handled.",
            "So so nicely and then the other other thing that.",
            "We should be sad about is that?",
            "Hmm's are at their core generative technique right there.",
            "What we've actually written down mathematically is a way that we believe that the document was written right and so your parameter estimation is sort of depending on this fact and we know that to sort of be blatantly not true.",
            "Right, and in particular, the model is actually spending quite a bit of parameter estimation effort trying to figure out how it is that people order words and things like that and.",
            "But you really wish is that the the technique wouldn't actually spend that time and would really focus in on what we really care about, right?",
            "Which is what's the label of the word the word was given to us, and so we don't have to worry about generating it was already given to us.",
            "We have no choice.",
            "So we really want the model to account for and really pay attention to is trying to do a great job predicting that label right?",
            "And there's nothing in this generative model that tells it that.",
            "Yes, what we're really interested in is predicting the model.",
            "So this brings us to yes question.",
            "Question was problems or when do we happier.",
            "Except for example it's default, exponential duration model or when do we know that the Markov assumption is correct and sufficient in order?",
            "Is there like tweaking techniques that you would recommend for so?",
            "For example, what order Markov models actually use?",
            "Not sure I understand your question.",
            "Mark models will kind of space space.",
            "How do we examine data model?",
            "Fit.",
            "Aside from looking at accuracies complications, yeah, that's a good question.",
            "So I wouldn't say that hidden Markov models are particularly.",
            "Understandable models like it's not very easy to look at.",
            "Uh, hmm, in.",
            "See what's going on under the hood.",
            "Right, I don't think I have any strong suggestions for how to know when your model is good.",
            "I think there's a lot of room for domain expertise to play a role in crafting the state structure.",
            "And I highly recommend that, but I don't have any guidelines other than just be clever.",
            "OK alright OK.",
            "So conditional random fields at their heart or what they model is the probability distribution of Y, which are the output labels given X, which are the words are actually occurring and sort of give you some sense for the mathematics of conditional random fields.",
            "I wanted to just show you that hidden Markov models are a special case of conditional random fields, and I think this will help your understanding of what's really going on in conditional random fields.",
            "So I'm going to sort of take you from this initial formal.",
            "Formulation of hidden Markov models and sort of walk you through to show that.",
            "To arrive at the sort of functional form of a conditional random field.",
            "One thing to keep in mind that the conditional field will have a bunch of parameters that need to be estimated.",
            "If we set those parameters in some strange way, will up end up exactly at a hidden Markov model, but with the training for a real condition and field we end up with quite different parameters and we'll talk about that in a minute.",
            "OK, so here we have this generative distribution over first generating a next state and then generating a word for that state.",
            "I'm just going to basically do this exponentiation and log pair, so and that will convert this product into a sum at the logs, get pushed in and sort of separate out the addition of these two log probabilities.",
            "Then I want to try to make this conditional right, so I want to arrive at sort of probability of Y given X and for some disjoint Y.",
            "This is just a little truth that probably why you next few sort of divide by all the other possibilities of XY pairs.",
            "There it is, so I've done the same thing here, which is basically I've just introduced this Z, which is just some ugly normalizer that depends only on X, right?",
            "So X is the only thing that matters here.",
            "'cause why is taken care of by this some?",
            "And don't worry too much about that, don't let the Z bother you for now.",
            "Then I'm going to do something very strange, right?",
            "So just bear with me, which is that these this log probability over states?",
            "I'm just going to call this Lambda sub by J and that's going to eventually become a conditional random field thing that we're going to estimate that type of parameter right in hidden Markov models we estimated by taking this by sort of estimating the probability of our model.",
            "And then.",
            "I'm going to sum over in this particular case for the state transition thing, I'm going to sum over all pairs of States and have an indicator function that is 1 exactly when it's the transition from I to J that we have seen in the data.",
            "Right, so EFSA by Jay is this indicator function that is 1 when you're transitioning from state.",
            "I to say J and otherwise it's zero.",
            "So this seems like a very large summation with only one thing being one and that's true and they do the same thing over here for the word output distribution.",
            "We're just going to use a muina G. Hey then.",
            "Let me do this.",
            "I'm asking these things look sort of the same and so I'm going to basically make them the same.",
            "Which is I'm going to say that there are now sort of this large set of indicator functions.",
            "That are sometimes one and sometimes zero, and the indicator functions, because here it takes 2 States and here it takes a state and a word.",
            "I'm just going to generalize it actually in two ways.",
            "I'm going to give it the pair of States and the word, and in fact instead of giving it the word, I'm going to give it all of the words.",
            "All of the words in the document, and I'm just going to tell it which word we're actually paying attention to.",
            "Alright, so here's are the two states where we're at.",
            "Here is the word sequence, and here we are positioned 7 right now, and so you can imagine rewriting all these indicator functions to ignore some of those arguments as appropriate.",
            "And because we have sort of land does amuse, I'm just going to sort of combine them all into Lambda, right?",
            "And So what we have here is we have, I guess this is sort of we have.",
            "Capital Y is the states we have something like Y ^2 indicator functions here and we have sort of X * y indicator functions here, right?",
            "And so we just sort of throw them all into one big set and we're going to call that items from that set K. So here are all these indicator functions, so this doesn't really look anything like this, right?",
            "But basically I've just rewritten it in a series of steps such that for some particular choice of indicator functions we have exactly.",
            "But hidden Markov model, and for some particular way of setting these Lambda parameters which are these log probabilities?",
            "This is a hidden Markov model.",
            "Any questions?",
            "OK, good.",
            "So this is also, it turns out exactly the functional form of a conditional random field, and so what's really going on here?",
            "Let me just show you a few things so.",
            "You should be excited that this is a conditional model.",
            "This probability of Y given X, so we're not worrying at all about modeling the words themselves.",
            "We're focusing on modeling the labels of those words.",
            "We have these feature functions right and for hidden Markov models.",
            "We had this very specific way of choosing feature functions, but really this is as general as it looks.",
            "So any type of function that is a function of your current state, your previous state.",
            "All the words and some knowledge about the word you actually care about are.",
            "Up for grabs so you could have a feature function that says something like.",
            "I'm considering labeling this word to be location and the previous word.",
            "Was a capitalized word and this word is also capitalized, right?",
            "That would be a great feature function.",
            "You could even sort of, say, the previous word was in a list of city names, and this token is a lower case word that seems like a reasonable sort of thing.",
            "It could also just be very simple thing like the current word is Bob, right?",
            "Or the current state is location and the previous state is other, so it really opens the door for lots and lots of features, formatting features all the kinds of features we had before, and importantly, conjunctions of those features.",
            "Right?",
            "So then sort of be as creative as you want.",
            "Coming up with features and often these conditional random field models have hundreds of thousands of features or even millions of features because we often overload them with lots of conjunctions of features.",
            "And then we just have this simple task of estimating these Lambda parameters.",
            "Right, and we'll talk about that in one minute and then and then.",
            "We've got then.",
            "We've got a conditional model.",
            "We're going to estimate these Lambda parameters specifically with an eye towards the conditional probability, and that's why one of the reasons that they are so pleasing.",
            "This should look a lot like exponential form to those of you who are statistically inclined, and that should that is correct, right?",
            "It's not that it's.",
            "Sort of exponential form in the cliques of the dependency graph.",
            "But if that sounded strange, just ignore it.",
            "OK, so one of the questions now is there are two questions that we asked before for hidden Markov models.",
            "There's a sort of inference question which is given such a model.",
            "How do you actually perform the extraction and there you have essentially the same answer which is dynamic.",
            "Programming will do that for you efficiently.",
            "The more the harder question is OK, I give you a pile of annotated documents.",
            "How do you actually create such a such a conditional random field with estimated parameters?",
            "And the same the same things applied.",
            "You have a have to choose some kind of state structure and transition structure.",
            "You have to choose your set of features that you use and then the third thing is you need to estimate those parameters.",
            "Now hidden Markov models number.",
            "We're estimating generative parameters.",
            "Here we are going to specifically maximize the probability of the labels given the words or the state of the states given the words.",
            "It's a conditional likelihood.",
            "And one beautiful thing about this is that you can't write down the answer in closed form, but it's a convex space, right?",
            "And so there is a single global Maxima for the joint probability of Y given X and sort of abstractly, you are have all these exhibe Cazes variables and you have something to optimize an it's convex.",
            "So it has a gradient, has a global Maxima, and so.",
            "At the high level, any sort of optimization technique here you can plug in right, and so in practice things like gradient descent are not really feasible because it's too slow.",
            "With hundreds of thousands of features.",
            "So if gradient descent doesn't really work for you, you would think of doing something like Newtons method right?",
            "Well, Newton's method is problematic because you need this second derivative information and you know the number of 2nd derivatives is really large.",
            "Is the square of the number of features.",
            "You can't even store that in memory, and so you sat again.",
            "So some of the techniques that are used today are conjugant gradient or limited memory beefs that use a limited amount of approximate second derivative information and.",
            "These are not techniques that are specific to conditional random fields.",
            "These are general techniques known by this sort of optimization community and sort of taken from there.",
            "OK, so I'm going to go for five more minutes.",
            "And I want to tell you just one example quickly and then I want to tell you about sort of what sort of the current.",
            "Research in conditional random fields because.",
            "Really, conditional random fields were invented in 2001 and have been a very hot area of research for the last five years.",
            "OK, so.",
            "Here's a paper from 2004 which actually uses exactly the same data set as hidden Markov model data set for information extraction, and they use these three types of features that we've talked about over and over again.",
            "The local features, which are the word, the layout, and then various types of lexicons and things like that.",
            "And we can just try to field gets a substantial reduction in error from the hidden Markov model, or something that doesn't really pay attention to the sequences of support vector machine.",
            "Just random fields event."
        ],
        [
            "Used for lots and lots of different things.",
            "They've been used for a bunch of sort of base linguistic tasks like noun phrase segmentation, Chinese words, segmentation in different types of things like that part of speech tagging.",
            "They've been also used for, sort of what you would think of as classical information extraction.",
            "Things like named entity recognition, protein names, and biology aspect abstracts addresses in web pages and things like that, and then also been used for some fun things like.",
            "Semantic role assignment and.",
            "RNA structure alignment, which is actually a very different world.",
            "It's you know where your alphabet is 4 characters because you're doing it over DNA sequences."
        ],
        [
            "Here are sort of three examples of papers written in the last two years.",
            "This sort of are my sort of three representation for what sort of interesting and going on in the conditional random field research.",
            "So the first is actually work that was done here at CMU by Smiths are laggy and William from the last talk.",
            "So one slightly awkward thing about conditional random fields is that there really a sort of word by word or token by token type of model, but usually what you're interested in extracting are these sequences of words.",
            "Write an address or a name or something that's usually more than one word.",
            "Right, and so you might actually think that.",
            "There's really this 2 levels of what's going on.",
            "Is that first you have the fact that there was a sequence of words that you're interested in extracting, and then you might actually care about.",
            "What are the individual words within that segment.",
            "Right, and So what they did is they created a semi Markov CRF that has exactly this two level structure, which is that you have the top level model that has sequences of segments, which are these phrases.",
            "And then if one of those segments is actually going to be a location then you have the words that are within that location.",
            "So that's sort of an interesting extension beyond the baseline CRF.",
            "Another thing that was presented actually here at ICML this summer is a sort of improved optimization techniques so.",
            "You know, like people like the neural network community have been thinking about optimization for a long time, and one thing that works very well there that has been shown to have good results in CRF's is stochastic gradient optimization.",
            "So basically the high level don't use all your training data.",
            "Take one big step in this sort of parameter space.",
            "Just consider some small number of examples and take a step that way and it's much faster to consider just 50 examples and it is consider all your examples and so.",
            "This is basically shown to be an order of magnitude faster than existing techniques that have been used, and so gives you the same quality of results.",
            "And then the last technique that I just want to touch on very quickly is there now when we work in creating conditional random fields using not only just labeled data but labeled and unlabeled data, right?",
            "So combinations of using conditional likelihood optimization given to you by your labeled data, as well as bringing in some unlabeled data through the use of entropy minimization techniques and.",
            "I spent a long long time working on semi supervised techniques for text classification and of course the big frustration there is that you no longer have convex optimization.",
            "You now have lots of local Maxima and you need to try to deal with all the local Maxima that exist and sort of hope that you find yourself into a good one.",
            "Good.",
            "If you're interested in doing a little bit more work here, or sort of diving a little deeper, there are three things I have to recommend.",
            "The first is there's a really good tutorial that's think it's in a book, but it's also available online.",
            "It's very useful, and then there are two pieces.",
            "Two implementations that I know of for conditional random fields that are sort of open and freely available.",
            "One is a minor third, which is Williams Software, and the other is a mallet, which is done up at UMass by Andrew McCallum.",
            "The comparison you made so conditional random field is shown to be safe, perform better than SVM and estimate.",
            "What about comparing to save Cascade classifier of generative models such as entry map followed by SPM on top of say featuring information scores or other things you can derive from the generative model.",
            "So that way you kind of estimated probability an have the classification line.",
            "Course style sorts of things, yeah.",
            "Um?",
            "That's a good question.",
            "I'm not sure if anyone has explicitly made that comparison.",
            "Yeah, yeah.",
            "Right, yeah, I don't know.",
            "Anybody?",
            "How is your services that kind of thing appears me.",
            "You stories of workers in all major insurance event system.",
            "For the thing that you would actually want to extract would be a whole sentence.",
            "Or even Comcast services.",
            "So you know ahead of time that what you were interested in extracting or identifying with whole sentences, then I think I would actually use a different model where.",
            "Thought you might use something more like this semi Markov CRF where you're actually going for each instance itself, is a sentence right?",
            "And then you have lots and lots of features of the sentence.",
            "And you might sort of divide things up that way.",
            "So this is like a task where you want to say like this sentence would be a good.",
            "Summary sentence of the whole news article or something like that.",
            "Well, that's not exactly what I was asking with.",
            "We'll talk in the break.",
            "Good, any other questions?",
            "Alright, thanks a lot.",
            "I think you have a break until 3:30."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK good alright so.",
                    "label": 0
                },
                {
                    "sent": "William gave the grand overview of text classification.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to do the same thing in the same amount of time for information extraction.",
                    "label": 1
                },
                {
                    "sent": "Both.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it seems like impossible challenges, so we'll just see if we can get through in an hour.",
                    "label": 0
                },
                {
                    "sent": "So I first want to start with a little bit of motivation for why information extraction is really interesting problem with lots of potential applications, and then I'll tell you a little bit about sort of how I think about what information extraction is, and then we'll jump into some of the techniques.",
                    "label": 0
                },
                {
                    "sent": "So here's just a typical Google search.",
                    "label": 0
                },
                {
                    "sent": "And you know, don't tell anyone outside this room that I'm saying things that Google that don't look beautiful.",
                    "label": 0
                },
                {
                    "sent": "But you know, imagine that you are looking for a job as a Baker, right?",
                    "label": 0
                },
                {
                    "sent": "Sort of obvious thing that you would do if you were typing in a Google query is you would do something like Baker job opening.",
                    "label": 0
                },
                {
                    "sent": "Does your magic little red pen things that part of.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "They shoot anyone in the eye, please.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I got it wrong, but Luckily it was on my hand.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, good right?",
                    "label": 0
                },
                {
                    "sent": "So that you know the top top search results you should be slightly disappointed by right and the main intuition of what's going on is that Google doesn't really understand that when you say Baker in that context, we're really meaning Baker.",
                    "label": 0
                },
                {
                    "sent": "The title of some job that's actually out there, right?",
                    "label": 0
                },
                {
                    "sent": "You get the school district, you get a company and then finally down here.",
                    "label": 1
                },
                {
                    "sent": "Search result #5 you actually do get a job opening for Baker and Apprentice Baker actually right and so.",
                    "label": 0
                },
                {
                    "sent": "How could information extraction sort of try to make life better for you so?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About 6 five years ago, long and long time ago there was a company here in Pittsburgh called Whizbang Labs that lots of CME people.",
                    "label": 0
                },
                {
                    "sent": "William myself and Tom Mitchell were all pretty involved with that.",
                    "label": 0
                },
                {
                    "sent": "Was trying to take text classification and machine learning technologies and really apply them to real applications.",
                    "label": 0
                },
                {
                    "sent": "And so there was a site not unlike monster.com called Flip dog.com that had the largest number of job postings on the web at that time.",
                    "label": 0
                },
                {
                    "sent": "In your monsters business model is that people.",
                    "label": 0
                },
                {
                    "sent": "Pay to list jobs in monster right?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doug's approach is very different.",
                    "label": 0
                },
                {
                    "sent": "It was that while still providing this kind of structured search interface where you could say I want to search in the category of food services and the title, I want to be Baker in the location.",
                    "label": 0
                },
                {
                    "sent": "I want to be somewhere in the United States and then you get this nice listing.",
                    "label": 0
                },
                {
                    "sent": "The data wasn't being provided to us by merchants.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or something in some kind of nice XML feed or something, but we actually went out into the web, found instances of people listing jobs for sale and sort of deconstructed what was actually going on here and understood it at this level of semantics, which is that we were able to say.",
                    "label": 0
                },
                {
                    "sent": "Here's a job title which is ice cream grew.",
                    "label": 1
                },
                {
                    "sent": "Here's the email address of the person you actually want to contact if you're interested in applying, here's the description.",
                    "label": 0
                },
                {
                    "sent": "You might also be able to extract things like the location.",
                    "label": 0
                },
                {
                    "sent": "Upper Midwest and you might using something like text classification.",
                    "label": 0
                },
                {
                    "sent": "You might be able to look at the title and description and put it into some hierarchy of all the jobs from the web.",
                    "label": 1
                },
                {
                    "sent": "So travel hospitality or something like that.",
                    "label": 0
                },
                {
                    "sent": "And this in general having this kind of structured data is great for enabling various types of fast.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did search right?",
                    "label": 0
                },
                {
                    "sent": "So here's another example of faceted search that I personally really enjoy using.",
                    "label": 1
                },
                {
                    "sent": "Highly recommend it.",
                    "label": 0
                },
                {
                    "sent": "It's called eBay Express, which is not unlike something like shopping.com, which you know if you're looking for something to buy, you know you can just give it lots of different attributes and values, and it's sort of nicely organized and that it knows that if you're in the sort of jewelry world that has things like type of material.",
                    "label": 0
                },
                {
                    "sent": "If I choose gold to be the metal that I'm interested in, it knows automatically that they're white, gold, yellow gold.",
                    "label": 0
                },
                {
                    "sent": "Rose gold and I can drill down on all of those types of things, right?",
                    "label": 0
                },
                {
                    "sent": "You can even do things like the shape you know you want it to have sort of letters and initials in it.",
                    "label": 0
                },
                {
                    "sent": "Right, and this type of where is this information coming from?",
                    "label": 0
                },
                {
                    "sent": "What this I believe without knowing and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tells that it's provided by merchants, but merchants.",
                    "label": 0
                },
                {
                    "sent": "Often have descriptive text that is completely disastrous right from sort of completely non grammatical has maybe a tiny bit of formatting here, right?",
                    "label": 0
                },
                {
                    "sent": "And there's lots and lots and lots of this data out there on the web that has no structured representation that someone can just hand you in an XML feed and so information extraction technologies at a very high level can perhaps recover some of the structure that's here and do something.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sing with it.",
                    "label": 0
                },
                {
                    "sent": "A third example, which probably most of you are very familiar with, and if not, I highly recommend it, is sightseers.",
                    "label": 0
                },
                {
                    "sent": "So Sightseer is basically a search engine over research papers and how do they get their information right so there you can do search sort of the same way you can do general keyword search but you also they also do lots of interesting things to understand what's actually going on in the document right?",
                    "label": 0
                },
                {
                    "sent": "So they parts the header of research paper to understand what is the title.",
                    "label": 0
                },
                {
                    "sent": "Who are the authors where the authors located?",
                    "label": 0
                },
                {
                    "sent": "When was it published?",
                    "label": 0
                },
                {
                    "sent": "And they also look at the citation section of the paper to recover the individual citations and build the citation graph right?",
                    "label": 0
                },
                {
                    "sent": "And so using information extraction, they've sort of can do some canonicalization and normalization, and you can then see quickly who's referencing this paper and what paper would I like to see next.",
                    "label": 0
                },
                {
                    "sent": "We'll come back to this domain actually a little bit throughout the talk.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "'cause I think it's a really interesting application.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Those are some really interesting ideas and they should all convince you.",
                    "label": 0
                },
                {
                    "sent": "I hope that information extraction is going to be a great driver of things to come in the world, but I do want to step back a little bit and formalize what I think of as information extraction, and I'm just going to use this example, which is just this little snippet of a webpage of a company called Buzz Metrics, which is a company I used to sort of work for.",
                    "label": 0
                },
                {
                    "sent": "But list the board members of their of that company.",
                    "label": 0
                },
                {
                    "sent": "So the sort of I think of information extraction really is having three components that are sort of broadly defineable, and the first is just recovering the individual data items that you want from formatted text.",
                    "label": 1
                },
                {
                    "sent": "And so in this particular example, you would probably imagine that you would be interested in identifying people names and job titles, right?",
                    "label": 0
                },
                {
                    "sent": "And so you might have.",
                    "label": 0
                },
                {
                    "sent": "A person name extractor and a job title extractor and we can leave aside for the moment whether those are actually separate things or whether there's somehow combined together, but broadly, you hope that what you would recover from this green being the names and orange being the titles question.",
                    "label": 0
                },
                {
                    "sent": "Already texted me a lot of things, at least competition.",
                    "label": 0
                },
                {
                    "sent": "This is all.",
                    "label": 0
                },
                {
                    "sent": "Turn on seven seas, and so forth and few stories I suppose to please.",
                    "label": 0
                },
                {
                    "sent": "Board members, right?",
                    "label": 0
                },
                {
                    "sent": "So I think certainly a degenerate case of formatted text is something that comes with no formatting.",
                    "label": 0
                },
                {
                    "sent": "Right and.",
                    "label": 0
                },
                {
                    "sent": "Send email to five different languages.",
                    "label": 0
                },
                {
                    "sent": "Yes, but I'll talk a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "I'm actually the techniques and I'm pretty presenting are more geared towards free text.",
                    "label": 0
                },
                {
                    "sent": "Generally, if you have something that's highly structured, you might use some sort of wrapper induction techniques which I'm not going to talk about too much today.",
                    "label": 0
                },
                {
                    "sent": "But in general, even even text that is in ASCII right has format, right?",
                    "label": 0
                },
                {
                    "sent": "You can for example, your SEC filings have tables that are presented in straight ASCII, right?",
                    "label": 0
                },
                {
                    "sent": "And there is sort of implicit formatting going on there in terms of rows and columns, even if it's not immediately obvious, even if it's represented in sort of a sequence of characters.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll watch it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, good so the second important thing that I think about for information extraction is understanding the relations between the fields that you've extracted, right?",
                    "label": 1
                },
                {
                    "sent": "So in particular, here you have a bunch of names and job titles, but the each job title is associated with exactly one person name, right?",
                    "label": 0
                },
                {
                    "sent": "So in this case where you're extracting out several fields of the same instance and there are multiple instances, you often hear that referred to as record Association, but more generally in something like a table, for example, right there are more interesting semantic relationships going on than just simple records.",
                    "label": 0
                },
                {
                    "sent": "And so more broadly, just want to understand what are the relations between the thing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You're extracting.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, an important part is that the result of those two processes gives you a bunch of records, or more generally something else.",
                    "label": 0
                },
                {
                    "sent": "But you want to be able to do normalization and deduplication.",
                    "label": 1
                },
                {
                    "sent": "So for example in this previous slide down here is this guy James O'Hara.",
                    "label": 0
                },
                {
                    "sent": "Well, you know there are other James O'hara's out there on the web.",
                    "label": 0
                },
                {
                    "sent": "Here's one from IMDb, right?",
                    "label": 0
                },
                {
                    "sent": "And this is actually a very different person than this James O'Hara, right?",
                    "label": 0
                },
                {
                    "sent": "Whereas this Jim O'Hara, from some other random piece of the web?",
                    "label": 0
                },
                {
                    "sent": "Is actually the same person.",
                    "label": 0
                },
                {
                    "sent": "I think senior vice president for via new media measurement that looks close enough like the ought to be able to think with high confidence that these two people are the same and these two people are different.",
                    "label": 0
                },
                {
                    "sent": "So that's normalization to get deduplication.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good today, given that we just have an hour I'm going to talk almost exclusively about field identification, which is just extracting out the sort of named entities or other types of fields you might be interested in.",
                    "label": 0
                },
                {
                    "sent": "Already having taxonomy here.",
                    "label": 0
                },
                {
                    "sent": "What are you doing?",
                    "label": 0
                },
                {
                    "sent": "No, what we're looking for names.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's a great question, so you may have prefaced exactly my next slide.",
                    "label": 0
                },
                {
                    "sent": "No two slides from now, right so?",
                    "label": 0
                },
                {
                    "sent": "I'm going to pose this as a machine learning problem and generally for today.",
                    "label": 0
                },
                {
                    "sent": "Anyway, we assume that what you're interested in extracting.",
                    "label": 0
                },
                {
                    "sent": "We know ahead of time is sort of told to us, so it's sort of a finite set of slots that we want to fill.",
                    "label": 0
                },
                {
                    "sent": "There has been some work more generally in sort of extracting out sort of general purpose relations from things like the whole web web as a whole.",
                    "label": 0
                },
                {
                    "sent": "Maybe Tom will touch on some of that stuff and his talk later today, but I don't know.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a bunch of history of how information extraction got about.",
                    "label": 0
                },
                {
                    "sent": "I don't think we really talk about it too much, but basically started with a bunch of hand coded rule type techniques and then sort of transitioned into machine learning and there are these sort of two approximate parallel tracks, one which have dealt primarily with text based sorts of things like news articles and then another track which is more of a wrapper induction sort of approach that dealt with highly structured web pages, and then they sort of fused.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the middle.",
                    "label": 0
                },
                {
                    "sent": "There's your history lesson in one slide.",
                    "label": 0
                },
                {
                    "sent": "OK, so one thing that's important to think about is how is information extraction machine learning problem.",
                    "label": 1
                },
                {
                    "sent": "It's actually somewhat different than sort of standard supervised learning, right?",
                    "label": 0
                },
                {
                    "sent": "So in Standard supervised learning, you have a bunch of instances, and you assume that they are independently and identically distributed, and each one is sort of you think of it anyways, being rather separate from the other, and here actually the training data you're given is usually like a bunch of documents.",
                    "label": 0
                },
                {
                    "sent": "Right, and each document is annotated with the various fields as they occur in the document, right?",
                    "label": 0
                },
                {
                    "sent": "So all the names would have the tag name.",
                    "label": 0
                },
                {
                    "sent": "All the addresses would have the tag address around them, and so.",
                    "label": 0
                },
                {
                    "sent": "It's it's not exactly each document you can assume is IID usually, but it's really the sequence is the sequence tokens of text or the text it's occurring, and there is actually sort of relations between the various types of fields you want to extract.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So text classification I think falls usually more into this idea, but the stuff you were talking about the end sort of was breaking that a little bit.",
                    "label": 0
                },
                {
                    "sent": "Another interesting thing, just at a high level, is that.",
                    "label": 1
                },
                {
                    "sent": "Whereas text classification is usually using sort of whole document types of features like which words occur in the document.",
                    "label": 0
                },
                {
                    "sent": "Here because we're interested in just identifying specific words and phrases in somewhere in the document, it's the local features that are really, really important here, right?",
                    "label": 0
                },
                {
                    "sent": "So you actually, if you're trying to extract out location like a room location, usually you have features that are features of the words that you want to extract, and then you have sort of hopefully trying to identify something indicative about the words.",
                    "label": 0
                },
                {
                    "sent": "Immediately before it there in the prefix or immediately after it.",
                    "label": 0
                },
                {
                    "sent": "And then also you also have features regarding the boundaries, right?",
                    "label": 0
                },
                {
                    "sent": "So did something change between here and here that might indicate that you might want to suddenly start extracting something.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Just my as sort of a machine learning practitioner as opposed to more of a researcher, although that's a bit of a Gray area, I like to always remind people of what actually makes machine learning algorithms work right and.",
                    "label": 0
                },
                {
                    "sent": "What really makes machine learning algorithms work is a lot of data, really clean data and really.",
                    "label": 0
                },
                {
                    "sent": "Smart feature engineering, right?",
                    "label": 0
                },
                {
                    "sent": "And so here are a bunch of examples of types of features that are good for information extraction, right?",
                    "label": 1
                },
                {
                    "sent": "And they fall into this sort of all the categories I just had in the previous slide, right?",
                    "label": 0
                },
                {
                    "sent": "So does the word itself begin with a number?",
                    "label": 0
                },
                {
                    "sent": "You can imagine the things like room locations that might be really important, right?",
                    "label": 0
                },
                {
                    "sent": "Is it in bold font, right?",
                    "label": 1
                },
                {
                    "sent": "Is that token only punctuation?",
                    "label": 0
                },
                {
                    "sent": "So there are lots of work word level features.",
                    "label": 0
                },
                {
                    "sent": "There are also things like is it in a list of city names you might have.",
                    "label": 1
                },
                {
                    "sent": "Might be pretty easy to find some large gazetteer of locations.",
                    "label": 0
                },
                {
                    "sent": "And that would be a really good feature, but it's unlikely that that list would be.",
                    "label": 0
                },
                {
                    "sent": "Exhaustive, right and also you would have with words on that list like.",
                    "label": 0
                },
                {
                    "sent": "Sam, I'm sure there's a city somewhere called Sam, right?",
                    "label": 0
                },
                {
                    "sent": "But Sam is a terrible word by itself for a city, so these are why these are not necessarily just wrote list that you would want to apply, but something that would be good as a feature.",
                    "label": 0
                },
                {
                    "sent": "Then there's also things like whitespace and formatting.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Again, sort of.",
                    "label": 0
                },
                {
                    "sent": "Things like capitalization, punctuation, and other sorts of long lists of things and then various conjunctions of these features.",
                    "label": 0
                },
                {
                    "sent": "So is it?",
                    "label": 0
                },
                {
                    "sent": "Is it a bold?",
                    "label": 0
                },
                {
                    "sent": "Is it in a four word for words that are involved and is it at the beginning of the line or the end of line?",
                    "label": 0
                },
                {
                    "sent": "So these are just to give you a kind of flavor for the types of features that we expose and you perform machine learning on.",
                    "label": 0
                },
                {
                    "sent": "These are just called from various.",
                    "label": 0
                },
                {
                    "sent": "Papers that have been written about information extraction just to give you a sense, OK?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are one way of thinking about information extraction that broadly five categories of techniques, and I'm talking about three of them today and they are just sort of candidate generation algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you might imagine that if you're doing classification, trying to extract out peoples names, you might consider having some little tiny program that emits all, say, consecutive capitalized words, and probably true that.",
                    "label": 0
                },
                {
                    "sent": "Almost all names fall into that category, but there's lots and lots of non things like the beginning of every sentence.",
                    "label": 0
                },
                {
                    "sent": "So you might classify the results of that kind of generation.",
                    "label": 0
                },
                {
                    "sent": "Then there are approaches that sort of consider various windows of in the document or the explicitly try to find beginning and end points, and I'll talk about some sort of sequence based methods and then I won't touch on wrapper induction, which is a technique for recognizing repeated structure, typically on web pages.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's jump right into techniques here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions before we move into techniques that the setting OK good.",
                    "label": 0
                },
                {
                    "sent": "So the 1st way to consider doing performing information extraction is that you want to extract out a particular thing, say location, right and you're given some documents here, right?",
                    "label": 0
                },
                {
                    "sent": "And so abstractly.",
                    "label": 0
                },
                {
                    "sent": "You can consider every.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Possible window.",
                    "label": 0
                },
                {
                    "sent": "Of begin end points and treat that as an instance, right?",
                    "label": 0
                },
                {
                    "sent": "And just try to classify that instance right and so all these these would all be negative instead.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Says for locate.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You would hope, right?",
                    "label": 0
                },
                {
                    "sent": "And here is the instance that you would actually hope to be positive.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this sort of falls if you pose information extraction in this way, it basically reduces approximately 2 sort of the standard supervised learning setting, right?",
                    "label": 1
                },
                {
                    "sent": "Which is that if you have these annotated documents, the positive instances are those start and end positions that actually mapped to the extraction extract field that you want to extract out, and then basically any other of the N squared instances on that document are all negative instances.",
                    "label": 0
                },
                {
                    "sent": "Right, and it's it's a little bit ridiculous to think about classifying all N squared things, but you know, if you fix the length of your window to something more reasonably sized, then that limits the complexity of that.",
                    "label": 0
                },
                {
                    "sent": "So for example, one thing that was done by Dane Free Tag is that he set the minimum window length and the maximum window lengths to be considered based on the training data.",
                    "label": 0
                },
                {
                    "sent": "So if the longest location was seven tokens long, then he just didn't consider any window longer than seven.",
                    "label": 0
                },
                {
                    "sent": "Seems to do.",
                    "label": 0
                },
                {
                    "sent": "Pretty well.",
                    "label": 0
                },
                {
                    "sent": "I mean again, then you have a track where you have standard supervised learning.",
                    "label": 0
                },
                {
                    "sent": "You have negative data, you have positive data.",
                    "label": 1
                },
                {
                    "sent": "You have features based on all the kinds of features that we talked about in the previous slide, and you have any kind of black box so.",
                    "label": 0
                },
                {
                    "sent": "In practice that works OK, but there's.",
                    "label": 0
                },
                {
                    "sent": "Several people, three diff.",
                    "label": 0
                },
                {
                    "sent": "Sort of the three big players, sort of in the late 90s, all found that variance of rule learning techniques all performs substantially better than something sort of like naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "And here's like an example rule that was learned by think Dane system.",
                    "label": 0
                },
                {
                    "sent": "So something like a course number like 15433 would be an example, right?",
                    "label": 0
                },
                {
                    "sent": "Has something is of course number if it has two tokens, like 15 and 443.",
                    "label": 0
                },
                {
                    "sent": "If none of the tokens that you're hoping to extract look like there a title of a course 'cause there's a number, and then there's some token that is exactly before the one.",
                    "label": 0
                },
                {
                    "sent": "Where you start to extract, there actually is a title, so this would be like.",
                    "label": 0
                },
                {
                    "sent": "Introduction to computer Programming 1601 right and then there is some token B in your number that you're trying to extract that rippleton that has exactly 3 characters, and so this is 1, so that would be the 4:43.",
                    "label": 0
                },
                {
                    "sent": "So this is one rule that was learned by Dane system and the way that this is formulated is that you know the rules that you're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis space uses a lot of conjunctions of features which.",
                    "label": 0
                },
                {
                    "sent": "Turned out to be very important and also uses sort of relations in ordering among the word tokens.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is sort of.",
                    "label": 0
                },
                {
                    "sent": "One intuition about what's important to make information extraction helpful.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of these conjunctions of features, and it's a lot of the sort of sequencing information of what comes before what comes after.",
                    "label": 0
                },
                {
                    "sent": "And then if you're just going to give a bunch of features of each of the tokens, then you're going to have less a good chance of actually sort of breaking through and getting something interesting as a result.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "So the next next set of techniques is boundary detection and Boundary section is pretty easy to understand as well.",
                    "label": 0
                },
                {
                    "sent": "It's basically what you hope to do is find the starting point of some extraction, so you just.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inssider, every point in the document and you say, is this a starting point of.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Location is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is starting point location right and hopefully you say yes here and no everywhere else, and then you have a similar classification task for the endpoint of a location.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and you hope that all up to there would be negative and that this would be yes.",
                    "label": 0
                },
                {
                    "sent": "And then you have some sort of magic bit of Association too.",
                    "label": 0
                },
                {
                    "sent": "Associate the starting points with the ending points, and then you've got your extracted field.",
                    "label": 0
                },
                {
                    "sent": "Right they can.",
                    "label": 0
                },
                {
                    "sent": "That should make you all a little nervous.",
                    "label": 0
                },
                {
                    "sent": "I hope.",
                    "label": 0
                },
                {
                    "sent": "Right, 'cause there's this sort of weird decoupling between the big.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Getting an ending of the field, so that's good.",
                    "label": 0
                },
                {
                    "sent": "If you feel nervous 'cause you should feel nervous but actually you know.",
                    "label": 0
                },
                {
                    "sent": "It turns out that in some instances actually doesn't work too badly.",
                    "label": 0
                },
                {
                    "sent": "So here's the formulation that was done by Nickisch Merican Dean.",
                    "label": 0
                },
                {
                    "sent": "They basically learn three models, but the sort of 1st two are the most important ones.",
                    "label": 0
                },
                {
                    "sent": "They learned the starting.",
                    "label": 0
                },
                {
                    "sent": "A class for the start, and that's a probabilistic model that actually they learned by boosting.",
                    "label": 0
                },
                {
                    "sent": "And as well for the end.",
                    "label": 0
                },
                {
                    "sent": "And then they combine these two to sort of try to find out where the actual field was by basically multiplying the probability of the start point times the probability of the endpoint plus the probability of the length, right?",
                    "label": 0
                },
                {
                    "sent": "So taking some information into the relation between the start and the end where the length is just basically a histogram from your training data.",
                    "label": 0
                },
                {
                    "sent": "So it's something it's very similar to sort of providing a maximum length for your sliding window.",
                    "label": 0
                },
                {
                    "sent": "Although it does provide a little bit of a prior over the length of the thing.",
                    "label": 0
                },
                {
                    "sent": "And the boosting was sort of run in a pretty straightforward way running over simple boundary patterns and features of the style that we've seen on the previous slide.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these two techniques, there's nothing too at the high level.",
                    "label": 0
                },
                {
                    "sent": "Nothing to elaborate really is going on.",
                    "label": 0
                },
                {
                    "sent": "And I personally feel a little uncomfortable with these types of techniques for sort of really feeling like we're doing the right thing, right?",
                    "label": 0
                },
                {
                    "sent": "So here are some of the here's some of the problems.",
                    "label": 1
                },
                {
                    "sent": "One is that.",
                    "label": 0
                },
                {
                    "sent": "There's lots of Inter relations between the fields that you want to extract that are just completely going unconsidered.",
                    "label": 1
                },
                {
                    "sent": "So, for example, in a seminar announcement.",
                    "label": 1
                },
                {
                    "sent": "I know that you essentially never see the end time of a seminar announcement.",
                    "label": 0
                },
                {
                    "sent": "Before you see the start time right, and there's nothing at any of these techniques that takes that type of knowledge into account.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 1
                },
                {
                    "sent": "Also, it's very unlikely that the start time and the location would actually overlap each other.",
                    "label": 0
                },
                {
                    "sent": "Right, because typically a token would be either one or the other and there's nothing that takes that into account as well.",
                    "label": 0
                },
                {
                    "sent": "Right, and as we talked about with the Boundary method, sort of putting down the start and end sort of strongly together by themselves and then try to sort of Patch the results together at the end should leave us feeling a little queasy.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that brings us to what I'm going to spend a pretty much the rest of the talk on, which are finite state machines, which sort of work directly to address this kind of problem.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the standard baseline implementation of what of a sort of machine learning for finite state machines is a hidden Markov model.",
                    "label": 1
                },
                {
                    "sent": "So just to take a little bit of a pulse, how many people feel like they could write down some equations for hidden Markov models?",
                    "label": 0
                },
                {
                    "sent": "Sort of there to that familiar.",
                    "label": 0
                },
                {
                    "sent": "Anyone?",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe 1/3 to 1/2 and how many people sort of know what a hidden Markov model is or don't really know at all.",
                    "label": 0
                },
                {
                    "sent": "OK, so also about a third good.",
                    "label": 0
                },
                {
                    "sent": "OK, that's helpful.",
                    "label": 0
                },
                {
                    "sent": "And did William talk at all about sort of naive Bayes?",
                    "label": 0
                },
                {
                    "sent": "Being a generative model of text?",
                    "label": 0
                },
                {
                    "sent": "A little bit OK good OK. Good, OK, so hidden Markov models are a generative model for text, which means that there is a probability distribution written down and you make this crazy assumption that all documents in the world or the ones that you're considering for your machine learning task, were actually written by this generative process.",
                    "label": 0
                },
                {
                    "sent": "Right, so I know that we all sort of have written things before and we don't follow any kind of probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "And this probabilistic model is going to sound so insane that you should run away screaming thinking this could do a good job for information extraction, but it actually does quite well.",
                    "label": 0
                },
                {
                    "sent": "So here's the model that we're going to consider right.",
                    "label": 0
                },
                {
                    "sent": "Let's say that we're interested in only extracting two things, location and people.",
                    "label": 0
                },
                {
                    "sent": "And so the simple thing we do is, we assume that words come from one of three possible states that we're in, or either in the state where we are writing down person names.",
                    "label": 0
                },
                {
                    "sent": "Or in the state we're writing down a location name or were in the state where we are writing down any other kind of word, right?",
                    "label": 0
                },
                {
                    "sent": "And so if we have the sentence like yes, Abraham Lincoln was born in Kentucky.",
                    "label": 0
                },
                {
                    "sent": "Which is actually a false statement, I think.",
                    "label": 0
                },
                {
                    "sent": "Then the sort of word yes was actually generated by us living in this state where the word Abraham in the word Lincoln was generated by us living in this red state.",
                    "label": 0
                },
                {
                    "sent": "And the way that this sort of state transition matrix model works is that you do two things.",
                    "label": 0
                },
                {
                    "sent": "You start off in some state like the background state, and you generate a word and the way that you generate a word is you have a probability distribution over all words in the English language.",
                    "label": 0
                },
                {
                    "sent": "Randomly draw out of that probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And then what you do is you transition to another state and so from this great state there are three possible outgoing transitions.",
                    "label": 0
                },
                {
                    "sent": "There's the transition back to itself.",
                    "label": 0
                },
                {
                    "sent": "There's a transition to person, and there's a transition to location, and there's likewise probability distribution over those three possible next states.",
                    "label": 0
                },
                {
                    "sent": "The independence assumptions that this model is making is that the word distribution, which is a unigram word distribution just over single words, is dependent only upon the state that you're in.",
                    "label": 0
                },
                {
                    "sent": "So background, person and location all have different unigram models for words, and that the next state depends only on the current state that you're in, right?",
                    "label": 0
                },
                {
                    "sent": "So from here there's these probabilities region over just those three different arcs, and you take one of those three.",
                    "label": 0
                },
                {
                    "sent": "So this leads to the following sort of generative model, which is the joint probability for the words that are actually written down and the states that you were actually in when you generated those words is basically.",
                    "label": 0
                },
                {
                    "sent": "The following general process, which is you're in a state and you generate a word X.",
                    "label": 0
                },
                {
                    "sent": "You're in statewide networks and then you generate the next state Whitey based on the previous state, white minus one.",
                    "label": 0
                },
                {
                    "sent": "And this is the product over the length of your sequence, and so you repeat that as many times as you want.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about that?",
                    "label": 0
                },
                {
                    "sent": "'cause this is a bit fundamental for the rest of where we're going.",
                    "label": 0
                },
                {
                    "sent": "OK, good, very good.",
                    "label": 0
                },
                {
                    "sent": "So if you imagine you have such a model right, the way you would actually.",
                    "label": 0
                },
                {
                    "sent": "Use one of these models for information extraction is that we are given a bunch of text.",
                    "label": 0
                },
                {
                    "sent": "And what we want to recover is the state that we were in when we generated each of those words.",
                    "label": 0
                },
                {
                    "sent": "Right, So what we want to basically get is we're given these X is we want to figure out what those wise are right and if we are in a red state when we generate the word Abraham that we're going to call the word Abraham and name.",
                    "label": 0
                },
                {
                    "sent": "And if we are in a green, if we're in the green state, then we're going to call Kentucky location.",
                    "label": 0
                },
                {
                    "sent": "So basically you have a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can pretty efficiently figure out the most likely state sequence using dynamic programming through a specific variant of unamic programming, called the Viterbi algorithm.",
                    "label": 0
                },
                {
                    "sent": "Right, and this is not anything that's new or original.",
                    "label": 0
                },
                {
                    "sent": "For information extraction, it Markov models have been around for a long time and they are used heavily in lots of communities such as speech recognition.",
                    "label": 0
                },
                {
                    "sent": "Alright, good then the other thing to consider about hidden Markov models is that how do you actually create one, right?",
                    "label": 0
                },
                {
                    "sent": "This is machine learning right?",
                    "label": 0
                },
                {
                    "sent": "And so I give you a pile of annotated documents and I say well, how give me back a hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "So the first thing you need to do is you need to write down one of these state transition diagrams.",
                    "label": 0
                },
                {
                    "sent": "Right, and so you can do this in a naive way.",
                    "label": 0
                },
                {
                    "sent": "You can basically create one state for every possible class that you're interested in extracting, and then one additional one for background.",
                    "label": 0
                },
                {
                    "sent": "And you can create transitions from every state to every other state, and that's a very reasonable way to proceed.",
                    "label": 0
                },
                {
                    "sent": "You might consider adding in some additional domain knowledge.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you wanted to enforce the fact that the end time of.",
                    "label": 0
                },
                {
                    "sent": "Seminars came after the start time.",
                    "label": 0
                },
                {
                    "sent": "You could have a state structure that requires you to pass through the start time node before you have any chance of ever getting to the end time node.",
                    "label": 0
                },
                {
                    "sent": "So you can add domain knowledge through your state transition diagram.",
                    "label": 0
                },
                {
                    "sent": "And then what you need to do is you need to basically estimate all these probabilities from your training data right?",
                    "label": 0
                },
                {
                    "sent": "And so since this is a generative model, we do the sort of nice probabilistic thing which is we want to select the parameters that are going to maximize the likelihood of having created our training data.",
                    "label": 1
                },
                {
                    "sent": "Right and 1st sort of standard hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "You would use something like Baum Welch forward backward or something like expectation maximization, but here actually.",
                    "label": 0
                },
                {
                    "sent": "Because every basically every word is annotated with what state it was generated from.",
                    "label": 0
                },
                {
                    "sent": "Usually you can get away without doing anything like that and do something in closed form.",
                    "label": 0
                },
                {
                    "sent": "Now sometimes that's not true, because you may, for example, have multiple person states in your model.",
                    "label": 0
                },
                {
                    "sent": "If your model is more complicated, for example, you might have tagged Abraham Lincoln's name, but you might have a separate state for first names in a separate, safer last names, and in that case something like a bomb well should be more appropriate.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me show you 2 examples of hidden Markov models in action, and then we're going to go into something another sequence based technique called conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "That's a lot more powerful.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an example based on paper headers, very similar to the sightseer data that we saw before.",
                    "label": 0
                },
                {
                    "sent": "Where what you do is you get a computer science research paper and you want to be able to basically extract out things like the authors, the title, where the authors are affiliated with the abstract and things like that.",
                    "label": 0
                },
                {
                    "sent": "And here we.",
                    "label": 0
                },
                {
                    "sent": "Did something slightly different than just sort of a plain vanilla hidden Markov model, which is sort of two things of distinction.",
                    "label": 0
                },
                {
                    "sent": "One is that we wanted to actually be able to learn the state structure that we were to extract, as opposed to just doing something naive, like providing a completely connected hmm or do something, spend a huge amount of time hand building a state transition diagram, and so the approach here was actually that.",
                    "label": 0
                },
                {
                    "sent": "You sort of took each of your training instances and wrote down a complete state.",
                    "label": 0
                },
                {
                    "sent": "Transition thing for it.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you had a title, author, affiliation, affiliation, author.",
                    "label": 0
                },
                {
                    "sent": "If that was sort of the 1st.",
                    "label": 0
                },
                {
                    "sent": "5 words you would actually just.",
                    "label": 0
                },
                {
                    "sent": "Free State structure like this?",
                    "label": 0
                },
                {
                    "sent": "So that's actually, and then you sort of repeat that for all your training examples and you put a little epsilon transition here at the beginning.",
                    "label": 0
                },
                {
                    "sent": "And then, so that's a very very large state.",
                    "label": 0
                },
                {
                    "sent": "Structure is basically one state per token in your training data, and then you start in order to get like a real compact representation.",
                    "label": 0
                },
                {
                    "sent": "You start merging States and the way that you merge states as you do it in sort of Asian setting, which is that you have a structural prior sort of that prefers to have fewer States and fewer transitions, and you sort of combine that with the information that you're gaining or losing.",
                    "label": 0
                },
                {
                    "sent": "So you start to do.",
                    "label": 0
                },
                {
                    "sent": "You can consider various types of greedy pairwise mergings and end up with hopefully a more compact representation.",
                    "label": 0
                },
                {
                    "sent": "The other interesting thing that happened is that.",
                    "label": 0
                },
                {
                    "sent": "We improved the estimates of output probabilities using data that wasn't just the annotated data, right?",
                    "label": 0
                },
                {
                    "sent": "So, for example, there's a state which is, you know, author names, right?",
                    "label": 0
                },
                {
                    "sent": "And it's you know it's pretty expensive to hand label data, and so you might have.",
                    "label": 0
                },
                {
                    "sent": "Maybe we labeled you know 500 or 1000 such papers, and so that gives you.",
                    "label": 0
                },
                {
                    "sent": "Maybe, maybe there are two authors per paper that gives you like 1000 or 2000 names.",
                    "label": 0
                },
                {
                    "sent": "Well, you're learning a pretty big probability distribution here, especially over something like person name, because there are just lots and lots of names you don't know about.",
                    "label": 0
                },
                {
                    "sent": "But you know, there's actually a lot of data out there about the types of names that we see.",
                    "label": 0
                },
                {
                    "sent": "In particular, you can think of big tech files, which are, you know, citation, basically collections of citations where the names are actually explicitly there as a field, right?",
                    "label": 0
                },
                {
                    "sent": "And so you can collect a bunch of big tech files.",
                    "label": 0
                },
                {
                    "sent": "I would hope that you would actually have a much better sense of what the word probability distribution over author is by looking at those files.",
                    "label": 0
                },
                {
                    "sent": "Then if you just look at your small number of labeled training examples and so we took a bunch of these fields an improve their probability estimates.",
                    "label": 0
                },
                {
                    "sent": "Using this, what we call distantly labeled data?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second example of using hidden Markov models for information extraction is nimble project and this is probably the 1st.",
                    "label": 0
                },
                {
                    "sent": "Really well known case of using sequence based techniques for information extraction.",
                    "label": 0
                },
                {
                    "sent": "And they hear they had a high level.",
                    "label": 0
                },
                {
                    "sent": "They had a state structure that was pretty simple.",
                    "label": 0
                },
                {
                    "sent": "They had about 7 different things that they wanted to extract.",
                    "label": 0
                },
                {
                    "sent": "Things like person and organization and various numeric things like money.",
                    "label": 0
                },
                {
                    "sent": "And they basically to lie to you for a minute.",
                    "label": 0
                },
                {
                    "sent": "They had this completely connected 7 state model or 8 state model.",
                    "label": 0
                },
                {
                    "sent": "And then they just perform sort of standard hidden Markov model type modeling.",
                    "label": 0
                },
                {
                    "sent": "Here the two things that they actually did quite differently.",
                    "label": 0
                },
                {
                    "sent": "One is that instead of modeling each individual word probability distribution there as a unigram distribution, they modeled as a bigram distribution, and So what does that really mean?",
                    "label": 0
                },
                {
                    "sent": "So sort of what that means is that this is not really one state.",
                    "label": 0
                },
                {
                    "sent": "Right, it's really actually.",
                    "label": 0
                },
                {
                    "sent": "V States of V is the number of words in the English language, right?",
                    "label": 0
                },
                {
                    "sent": "And so you have, because the bigram model is basically from any single word.",
                    "label": 0
                },
                {
                    "sent": "What's the probability of transitioning to any other word right?",
                    "label": 0
                },
                {
                    "sent": "And so you can imagine this.",
                    "label": 0
                },
                {
                    "sent": "Very large sort of V order V number of states with lots and lots of transitions.",
                    "label": 0
                },
                {
                    "sent": "The transitions in between here, though, however, were all tide.",
                    "label": 0
                },
                {
                    "sent": "So for example, if this is.",
                    "label": 0
                },
                {
                    "sent": "So you're sort of person.",
                    "label": 0
                },
                {
                    "sent": "You have lots of things like you know able and Arthur and and.",
                    "label": 0
                },
                {
                    "sent": "Right and then you have all these because it's a, it's a bigram model.",
                    "label": 0
                },
                {
                    "sent": "You have these all these different word probability distributions.",
                    "label": 0
                },
                {
                    "sent": "And then you also have another sort of large state for organization.",
                    "label": 0
                },
                {
                    "sent": "And while sort of conceptually, there are transitions from every state in here to every state in here.",
                    "label": 0
                },
                {
                    "sent": "They are all of these transitions shared the same set of parameters.",
                    "label": 0
                },
                {
                    "sent": "So you didn't.",
                    "label": 0
                },
                {
                    "sent": "So basically you didn't sort of completely exacerbate the parameter estimation problem, and this is actually a pretty reasonably common technique.",
                    "label": 0
                },
                {
                    "sent": "I think it's called like parameter tying, and there are other cases of these parameter tying things in the research literature.",
                    "label": 0
                },
                {
                    "sent": "The other really interesting thing that they did.",
                    "label": 0
                },
                {
                    "sent": "And which really gets to the core of why Hmm's are not completely satisfactory for information extraction is that they wanted to use various types of capitalization and features about the format of numbers, right?",
                    "label": 0
                },
                {
                    "sent": "If you remember, one of the things I showed you in the earlier in the talk was this long list of features that had nothing to do with actually the identity of the word itself, right?",
                    "label": 0
                },
                {
                    "sent": "It included the identity of the word, but it basically had like 50 other types of features based on formatting and capitalization.",
                    "label": 0
                },
                {
                    "sent": "And other sorts of things that are not being used at all here by hidden Markov models, right?",
                    "label": 0
                },
                {
                    "sent": "The sort of vanilla hidden Markov model is just looking at the identity of the word, and so one of the reasons we're really sad about hidden Markov models is that they aren't using this rich source of information, and so they sort of tried to hack around it by including this small set of perhaps a dozen features, and each word was really not a word distribution, but it was paired with one of these features.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "Most of these are various flavors of trying to recognize digits and money at the top, and then all these ones in the bottoms are various types of capitalization, sorts of features, and so the word Abraham would actually be a pair of the word Abraham and initial caps like Sally.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "One of the facts is that this really expands the parameter space, right?",
                    "label": 0
                },
                {
                    "sent": "You basically by creating 10 different things, you've almost haven't quite, but you really almost increased the number of parameters by an order of magnitude.",
                    "label": 0
                },
                {
                    "sent": "It's not quite true because the word Abraham is never going to be a 2 digit number, but it could have any kind of Abraham, kind of any kind of this capitalization.",
                    "label": 0
                },
                {
                    "sent": "Right, and so that's there's definitely tension there, right?",
                    "label": 0
                },
                {
                    "sent": "You can't really creating that many parameters should be somewhat scary.",
                    "label": 0
                },
                {
                    "sent": "Right, good, OK so.",
                    "label": 0
                },
                {
                    "sent": "Hmm's worked pretty well in practice, but why should we not be completely satisfied?",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov models naturally have word entities, and they sort of don't allow for these features to be handled.",
                    "label": 0
                },
                {
                    "sent": "So so nicely and then the other other thing that.",
                    "label": 0
                },
                {
                    "sent": "We should be sad about is that?",
                    "label": 0
                },
                {
                    "sent": "Hmm's are at their core generative technique right there.",
                    "label": 0
                },
                {
                    "sent": "What we've actually written down mathematically is a way that we believe that the document was written right and so your parameter estimation is sort of depending on this fact and we know that to sort of be blatantly not true.",
                    "label": 0
                },
                {
                    "sent": "Right, and in particular, the model is actually spending quite a bit of parameter estimation effort trying to figure out how it is that people order words and things like that and.",
                    "label": 0
                },
                {
                    "sent": "But you really wish is that the the technique wouldn't actually spend that time and would really focus in on what we really care about, right?",
                    "label": 0
                },
                {
                    "sent": "Which is what's the label of the word the word was given to us, and so we don't have to worry about generating it was already given to us.",
                    "label": 0
                },
                {
                    "sent": "We have no choice.",
                    "label": 0
                },
                {
                    "sent": "So we really want the model to account for and really pay attention to is trying to do a great job predicting that label right?",
                    "label": 0
                },
                {
                    "sent": "And there's nothing in this generative model that tells it that.",
                    "label": 0
                },
                {
                    "sent": "Yes, what we're really interested in is predicting the model.",
                    "label": 0
                },
                {
                    "sent": "So this brings us to yes question.",
                    "label": 0
                },
                {
                    "sent": "Question was problems or when do we happier.",
                    "label": 0
                },
                {
                    "sent": "Except for example it's default, exponential duration model or when do we know that the Markov assumption is correct and sufficient in order?",
                    "label": 0
                },
                {
                    "sent": "Is there like tweaking techniques that you would recommend for so?",
                    "label": 0
                },
                {
                    "sent": "For example, what order Markov models actually use?",
                    "label": 0
                },
                {
                    "sent": "Not sure I understand your question.",
                    "label": 0
                },
                {
                    "sent": "Mark models will kind of space space.",
                    "label": 0
                },
                {
                    "sent": "How do we examine data model?",
                    "label": 0
                },
                {
                    "sent": "Fit.",
                    "label": 0
                },
                {
                    "sent": "Aside from looking at accuracies complications, yeah, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "So I wouldn't say that hidden Markov models are particularly.",
                    "label": 0
                },
                {
                    "sent": "Understandable models like it's not very easy to look at.",
                    "label": 0
                },
                {
                    "sent": "Uh, hmm, in.",
                    "label": 0
                },
                {
                    "sent": "See what's going on under the hood.",
                    "label": 0
                },
                {
                    "sent": "Right, I don't think I have any strong suggestions for how to know when your model is good.",
                    "label": 0
                },
                {
                    "sent": "I think there's a lot of room for domain expertise to play a role in crafting the state structure.",
                    "label": 0
                },
                {
                    "sent": "And I highly recommend that, but I don't have any guidelines other than just be clever.",
                    "label": 0
                },
                {
                    "sent": "OK alright OK.",
                    "label": 0
                },
                {
                    "sent": "So conditional random fields at their heart or what they model is the probability distribution of Y, which are the output labels given X, which are the words are actually occurring and sort of give you some sense for the mathematics of conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "I wanted to just show you that hidden Markov models are a special case of conditional random fields, and I think this will help your understanding of what's really going on in conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to sort of take you from this initial formal.",
                    "label": 0
                },
                {
                    "sent": "Formulation of hidden Markov models and sort of walk you through to show that.",
                    "label": 0
                },
                {
                    "sent": "To arrive at the sort of functional form of a conditional random field.",
                    "label": 0
                },
                {
                    "sent": "One thing to keep in mind that the conditional field will have a bunch of parameters that need to be estimated.",
                    "label": 0
                },
                {
                    "sent": "If we set those parameters in some strange way, will up end up exactly at a hidden Markov model, but with the training for a real condition and field we end up with quite different parameters and we'll talk about that in a minute.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we have this generative distribution over first generating a next state and then generating a word for that state.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to basically do this exponentiation and log pair, so and that will convert this product into a sum at the logs, get pushed in and sort of separate out the addition of these two log probabilities.",
                    "label": 0
                },
                {
                    "sent": "Then I want to try to make this conditional right, so I want to arrive at sort of probability of Y given X and for some disjoint Y.",
                    "label": 0
                },
                {
                    "sent": "This is just a little truth that probably why you next few sort of divide by all the other possibilities of XY pairs.",
                    "label": 0
                },
                {
                    "sent": "There it is, so I've done the same thing here, which is basically I've just introduced this Z, which is just some ugly normalizer that depends only on X, right?",
                    "label": 0
                },
                {
                    "sent": "So X is the only thing that matters here.",
                    "label": 0
                },
                {
                    "sent": "'cause why is taken care of by this some?",
                    "label": 0
                },
                {
                    "sent": "And don't worry too much about that, don't let the Z bother you for now.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to do something very strange, right?",
                    "label": 0
                },
                {
                    "sent": "So just bear with me, which is that these this log probability over states?",
                    "label": 0
                },
                {
                    "sent": "I'm just going to call this Lambda sub by J and that's going to eventually become a conditional random field thing that we're going to estimate that type of parameter right in hidden Markov models we estimated by taking this by sort of estimating the probability of our model.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sum over in this particular case for the state transition thing, I'm going to sum over all pairs of States and have an indicator function that is 1 exactly when it's the transition from I to J that we have seen in the data.",
                    "label": 0
                },
                {
                    "sent": "Right, so EFSA by Jay is this indicator function that is 1 when you're transitioning from state.",
                    "label": 0
                },
                {
                    "sent": "I to say J and otherwise it's zero.",
                    "label": 0
                },
                {
                    "sent": "So this seems like a very large summation with only one thing being one and that's true and they do the same thing over here for the word output distribution.",
                    "label": 0
                },
                {
                    "sent": "We're just going to use a muina G. Hey then.",
                    "label": 0
                },
                {
                    "sent": "Let me do this.",
                    "label": 0
                },
                {
                    "sent": "I'm asking these things look sort of the same and so I'm going to basically make them the same.",
                    "label": 0
                },
                {
                    "sent": "Which is I'm going to say that there are now sort of this large set of indicator functions.",
                    "label": 0
                },
                {
                    "sent": "That are sometimes one and sometimes zero, and the indicator functions, because here it takes 2 States and here it takes a state and a word.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to generalize it actually in two ways.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give it the pair of States and the word, and in fact instead of giving it the word, I'm going to give it all of the words.",
                    "label": 0
                },
                {
                    "sent": "All of the words in the document, and I'm just going to tell it which word we're actually paying attention to.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's are the two states where we're at.",
                    "label": 0
                },
                {
                    "sent": "Here is the word sequence, and here we are positioned 7 right now, and so you can imagine rewriting all these indicator functions to ignore some of those arguments as appropriate.",
                    "label": 0
                },
                {
                    "sent": "And because we have sort of land does amuse, I'm just going to sort of combine them all into Lambda, right?",
                    "label": 0
                },
                {
                    "sent": "And So what we have here is we have, I guess this is sort of we have.",
                    "label": 0
                },
                {
                    "sent": "Capital Y is the states we have something like Y ^2 indicator functions here and we have sort of X * y indicator functions here, right?",
                    "label": 0
                },
                {
                    "sent": "And so we just sort of throw them all into one big set and we're going to call that items from that set K. So here are all these indicator functions, so this doesn't really look anything like this, right?",
                    "label": 0
                },
                {
                    "sent": "But basically I've just rewritten it in a series of steps such that for some particular choice of indicator functions we have exactly.",
                    "label": 0
                },
                {
                    "sent": "But hidden Markov model, and for some particular way of setting these Lambda parameters which are these log probabilities?",
                    "label": 0
                },
                {
                    "sent": "This is a hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "So this is also, it turns out exactly the functional form of a conditional random field, and so what's really going on here?",
                    "label": 0
                },
                {
                    "sent": "Let me just show you a few things so.",
                    "label": 0
                },
                {
                    "sent": "You should be excited that this is a conditional model.",
                    "label": 0
                },
                {
                    "sent": "This probability of Y given X, so we're not worrying at all about modeling the words themselves.",
                    "label": 0
                },
                {
                    "sent": "We're focusing on modeling the labels of those words.",
                    "label": 0
                },
                {
                    "sent": "We have these feature functions right and for hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "We had this very specific way of choosing feature functions, but really this is as general as it looks.",
                    "label": 0
                },
                {
                    "sent": "So any type of function that is a function of your current state, your previous state.",
                    "label": 0
                },
                {
                    "sent": "All the words and some knowledge about the word you actually care about are.",
                    "label": 0
                },
                {
                    "sent": "Up for grabs so you could have a feature function that says something like.",
                    "label": 0
                },
                {
                    "sent": "I'm considering labeling this word to be location and the previous word.",
                    "label": 0
                },
                {
                    "sent": "Was a capitalized word and this word is also capitalized, right?",
                    "label": 0
                },
                {
                    "sent": "That would be a great feature function.",
                    "label": 0
                },
                {
                    "sent": "You could even sort of, say, the previous word was in a list of city names, and this token is a lower case word that seems like a reasonable sort of thing.",
                    "label": 0
                },
                {
                    "sent": "It could also just be very simple thing like the current word is Bob, right?",
                    "label": 0
                },
                {
                    "sent": "Or the current state is location and the previous state is other, so it really opens the door for lots and lots of features, formatting features all the kinds of features we had before, and importantly, conjunctions of those features.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So then sort of be as creative as you want.",
                    "label": 0
                },
                {
                    "sent": "Coming up with features and often these conditional random field models have hundreds of thousands of features or even millions of features because we often overload them with lots of conjunctions of features.",
                    "label": 0
                },
                {
                    "sent": "And then we just have this simple task of estimating these Lambda parameters.",
                    "label": 0
                },
                {
                    "sent": "Right, and we'll talk about that in one minute and then and then.",
                    "label": 0
                },
                {
                    "sent": "We've got then.",
                    "label": 0
                },
                {
                    "sent": "We've got a conditional model.",
                    "label": 0
                },
                {
                    "sent": "We're going to estimate these Lambda parameters specifically with an eye towards the conditional probability, and that's why one of the reasons that they are so pleasing.",
                    "label": 0
                },
                {
                    "sent": "This should look a lot like exponential form to those of you who are statistically inclined, and that should that is correct, right?",
                    "label": 0
                },
                {
                    "sent": "It's not that it's.",
                    "label": 0
                },
                {
                    "sent": "Sort of exponential form in the cliques of the dependency graph.",
                    "label": 0
                },
                {
                    "sent": "But if that sounded strange, just ignore it.",
                    "label": 0
                },
                {
                    "sent": "OK, so one of the questions now is there are two questions that we asked before for hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "There's a sort of inference question which is given such a model.",
                    "label": 0
                },
                {
                    "sent": "How do you actually perform the extraction and there you have essentially the same answer which is dynamic.",
                    "label": 0
                },
                {
                    "sent": "Programming will do that for you efficiently.",
                    "label": 0
                },
                {
                    "sent": "The more the harder question is OK, I give you a pile of annotated documents.",
                    "label": 0
                },
                {
                    "sent": "How do you actually create such a such a conditional random field with estimated parameters?",
                    "label": 0
                },
                {
                    "sent": "And the same the same things applied.",
                    "label": 0
                },
                {
                    "sent": "You have a have to choose some kind of state structure and transition structure.",
                    "label": 0
                },
                {
                    "sent": "You have to choose your set of features that you use and then the third thing is you need to estimate those parameters.",
                    "label": 0
                },
                {
                    "sent": "Now hidden Markov models number.",
                    "label": 0
                },
                {
                    "sent": "We're estimating generative parameters.",
                    "label": 0
                },
                {
                    "sent": "Here we are going to specifically maximize the probability of the labels given the words or the state of the states given the words.",
                    "label": 0
                },
                {
                    "sent": "It's a conditional likelihood.",
                    "label": 0
                },
                {
                    "sent": "And one beautiful thing about this is that you can't write down the answer in closed form, but it's a convex space, right?",
                    "label": 0
                },
                {
                    "sent": "And so there is a single global Maxima for the joint probability of Y given X and sort of abstractly, you are have all these exhibe Cazes variables and you have something to optimize an it's convex.",
                    "label": 0
                },
                {
                    "sent": "So it has a gradient, has a global Maxima, and so.",
                    "label": 0
                },
                {
                    "sent": "At the high level, any sort of optimization technique here you can plug in right, and so in practice things like gradient descent are not really feasible because it's too slow.",
                    "label": 0
                },
                {
                    "sent": "With hundreds of thousands of features.",
                    "label": 0
                },
                {
                    "sent": "So if gradient descent doesn't really work for you, you would think of doing something like Newtons method right?",
                    "label": 0
                },
                {
                    "sent": "Well, Newton's method is problematic because you need this second derivative information and you know the number of 2nd derivatives is really large.",
                    "label": 0
                },
                {
                    "sent": "Is the square of the number of features.",
                    "label": 0
                },
                {
                    "sent": "You can't even store that in memory, and so you sat again.",
                    "label": 0
                },
                {
                    "sent": "So some of the techniques that are used today are conjugant gradient or limited memory beefs that use a limited amount of approximate second derivative information and.",
                    "label": 0
                },
                {
                    "sent": "These are not techniques that are specific to conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "These are general techniques known by this sort of optimization community and sort of taken from there.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to go for five more minutes.",
                    "label": 0
                },
                {
                    "sent": "And I want to tell you just one example quickly and then I want to tell you about sort of what sort of the current.",
                    "label": 0
                },
                {
                    "sent": "Research in conditional random fields because.",
                    "label": 0
                },
                {
                    "sent": "Really, conditional random fields were invented in 2001 and have been a very hot area of research for the last five years.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here's a paper from 2004 which actually uses exactly the same data set as hidden Markov model data set for information extraction, and they use these three types of features that we've talked about over and over again.",
                    "label": 0
                },
                {
                    "sent": "The local features, which are the word, the layout, and then various types of lexicons and things like that.",
                    "label": 0
                },
                {
                    "sent": "And we can just try to field gets a substantial reduction in error from the hidden Markov model, or something that doesn't really pay attention to the sequences of support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Just random fields event.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Used for lots and lots of different things.",
                    "label": 0
                },
                {
                    "sent": "They've been used for a bunch of sort of base linguistic tasks like noun phrase segmentation, Chinese words, segmentation in different types of things like that part of speech tagging.",
                    "label": 0
                },
                {
                    "sent": "They've been also used for, sort of what you would think of as classical information extraction.",
                    "label": 0
                },
                {
                    "sent": "Things like named entity recognition, protein names, and biology aspect abstracts addresses in web pages and things like that, and then also been used for some fun things like.",
                    "label": 1
                },
                {
                    "sent": "Semantic role assignment and.",
                    "label": 0
                },
                {
                    "sent": "RNA structure alignment, which is actually a very different world.",
                    "label": 0
                },
                {
                    "sent": "It's you know where your alphabet is 4 characters because you're doing it over DNA sequences.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are sort of three examples of papers written in the last two years.",
                    "label": 1
                },
                {
                    "sent": "This sort of are my sort of three representation for what sort of interesting and going on in the conditional random field research.",
                    "label": 0
                },
                {
                    "sent": "So the first is actually work that was done here at CMU by Smiths are laggy and William from the last talk.",
                    "label": 0
                },
                {
                    "sent": "So one slightly awkward thing about conditional random fields is that there really a sort of word by word or token by token type of model, but usually what you're interested in extracting are these sequences of words.",
                    "label": 0
                },
                {
                    "sent": "Write an address or a name or something that's usually more than one word.",
                    "label": 0
                },
                {
                    "sent": "Right, and so you might actually think that.",
                    "label": 0
                },
                {
                    "sent": "There's really this 2 levels of what's going on.",
                    "label": 0
                },
                {
                    "sent": "Is that first you have the fact that there was a sequence of words that you're interested in extracting, and then you might actually care about.",
                    "label": 0
                },
                {
                    "sent": "What are the individual words within that segment.",
                    "label": 0
                },
                {
                    "sent": "Right, and So what they did is they created a semi Markov CRF that has exactly this two level structure, which is that you have the top level model that has sequences of segments, which are these phrases.",
                    "label": 1
                },
                {
                    "sent": "And then if one of those segments is actually going to be a location then you have the words that are within that location.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of an interesting extension beyond the baseline CRF.",
                    "label": 0
                },
                {
                    "sent": "Another thing that was presented actually here at ICML this summer is a sort of improved optimization techniques so.",
                    "label": 1
                },
                {
                    "sent": "You know, like people like the neural network community have been thinking about optimization for a long time, and one thing that works very well there that has been shown to have good results in CRF's is stochastic gradient optimization.",
                    "label": 0
                },
                {
                    "sent": "So basically the high level don't use all your training data.",
                    "label": 0
                },
                {
                    "sent": "Take one big step in this sort of parameter space.",
                    "label": 0
                },
                {
                    "sent": "Just consider some small number of examples and take a step that way and it's much faster to consider just 50 examples and it is consider all your examples and so.",
                    "label": 0
                },
                {
                    "sent": "This is basically shown to be an order of magnitude faster than existing techniques that have been used, and so gives you the same quality of results.",
                    "label": 1
                },
                {
                    "sent": "And then the last technique that I just want to touch on very quickly is there now when we work in creating conditional random fields using not only just labeled data but labeled and unlabeled data, right?",
                    "label": 0
                },
                {
                    "sent": "So combinations of using conditional likelihood optimization given to you by your labeled data, as well as bringing in some unlabeled data through the use of entropy minimization techniques and.",
                    "label": 0
                },
                {
                    "sent": "I spent a long long time working on semi supervised techniques for text classification and of course the big frustration there is that you no longer have convex optimization.",
                    "label": 0
                },
                {
                    "sent": "You now have lots of local Maxima and you need to try to deal with all the local Maxima that exist and sort of hope that you find yourself into a good one.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in doing a little bit more work here, or sort of diving a little deeper, there are three things I have to recommend.",
                    "label": 0
                },
                {
                    "sent": "The first is there's a really good tutorial that's think it's in a book, but it's also available online.",
                    "label": 0
                },
                {
                    "sent": "It's very useful, and then there are two pieces.",
                    "label": 0
                },
                {
                    "sent": "Two implementations that I know of for conditional random fields that are sort of open and freely available.",
                    "label": 0
                },
                {
                    "sent": "One is a minor third, which is Williams Software, and the other is a mallet, which is done up at UMass by Andrew McCallum.",
                    "label": 0
                },
                {
                    "sent": "The comparison you made so conditional random field is shown to be safe, perform better than SVM and estimate.",
                    "label": 0
                },
                {
                    "sent": "What about comparing to save Cascade classifier of generative models such as entry map followed by SPM on top of say featuring information scores or other things you can derive from the generative model.",
                    "label": 0
                },
                {
                    "sent": "So that way you kind of estimated probability an have the classification line.",
                    "label": 0
                },
                {
                    "sent": "Course style sorts of things, yeah.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if anyone has explicitly made that comparison.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Anybody?",
                    "label": 0
                },
                {
                    "sent": "How is your services that kind of thing appears me.",
                    "label": 0
                },
                {
                    "sent": "You stories of workers in all major insurance event system.",
                    "label": 0
                },
                {
                    "sent": "For the thing that you would actually want to extract would be a whole sentence.",
                    "label": 0
                },
                {
                    "sent": "Or even Comcast services.",
                    "label": 0
                },
                {
                    "sent": "So you know ahead of time that what you were interested in extracting or identifying with whole sentences, then I think I would actually use a different model where.",
                    "label": 0
                },
                {
                    "sent": "Thought you might use something more like this semi Markov CRF where you're actually going for each instance itself, is a sentence right?",
                    "label": 0
                },
                {
                    "sent": "And then you have lots and lots of features of the sentence.",
                    "label": 0
                },
                {
                    "sent": "And you might sort of divide things up that way.",
                    "label": 0
                },
                {
                    "sent": "So this is like a task where you want to say like this sentence would be a good.",
                    "label": 0
                },
                {
                    "sent": "Summary sentence of the whole news article or something like that.",
                    "label": 0
                },
                {
                    "sent": "Well, that's not exactly what I was asking with.",
                    "label": 0
                },
                {
                    "sent": "We'll talk in the break.",
                    "label": 0
                },
                {
                    "sent": "Good, any other questions?",
                    "label": 0
                },
                {
                    "sent": "Alright, thanks a lot.",
                    "label": 0
                },
                {
                    "sent": "I think you have a break until 3:30.",
                    "label": 0
                }
            ]
        }
    }
}