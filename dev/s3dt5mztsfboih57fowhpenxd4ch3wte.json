{
    "id": "s3dt5mztsfboih57fowhpenxd4ch3wte",
    "title": "A Quasi-Newton Approach to Nonsmooth Convex Optimization",
    "info": {
        "author": [
            "Jin Yu, NICTA, Australia's ICT Research Centre of Excellence"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/icml08_yu_aqna/",
    "segmentation": [
        [
            "OK, today I'm going to present our work on developing a quasi Newton approach to nonsmooth convex optimization, and this work is also supported by Nick to an Pascal 2."
        ],
        [
            "Now first of all, let me just give you a brief review of the classical course in Newton approach and later I will show you actually we can systematically extend this classic framework to nonsmooth convex optimization.",
            "Now, the most successful.",
            "Who is the Newton approach?",
            "Is the BFGS method an which is widely regarded as the workhorse of smooth nonlinear optimization?",
            "And here in this talk I will only consider convex objective functions."
        ],
        [
            "Now VFG is forms a local quadratic model of the objective function at iterate saying WT an.",
            "This local quadratic model is nothing but a second order Taylor expansion of the objective function at around WT anwer.",
            "The Matrix PT is a symmetric positive definite matrix.",
            "Which approach makes the inverse Hessian of the objective function now?"
        ],
        [
            "Having this local quadratic model BFG is minimized.",
            "Empty of W to derive its parameter update now here.",
            "PT is the so called course in Newton direction.",
            "It is negative BT metrics times the gradient of the objective function evaluated at current iterate WT.",
            "Now having this Sir."
        ],
        [
            "Each direction we need to find a positive step size ET to scale this search direction for the parameter update and."
        ],
        [
            "Normally this positive step size is determined by a line search of buying the Wolf conditions, and I will discuss more about Wolf conditions later in my talk."
        ],
        [
            "No, this BT metrics are appeared in the local quadratic model is then modified by a rank 2 update, not updating and storing this PT metrics require order D squared cost and D being the dimension of your objective function or the dimension of the optimization prob."
        ],
        [
            "Therefore, for high dimensional problems, limited memory variants of BFG method is preferred.",
            "It approximates the quasi Newton direction PT here via metrics free approach, i.e.",
            "Without calculating these metrics BT this reduces the cost from order D square to order M * D An being the size of.",
            "The memory and usually its value between 3 to 20."
        ],
        [
            "Now it is very important to notice that BFGS method assumes the objective function is smooth.",
            "However."
        ],
        [
            "In machine learning context, many objective functions are non smooth.",
            "As you can see in this figure.",
            "It is a plot for the hinge loss which is usually used for binary classification in machine learning it is convex but it is non smooth and it is not smooth at this hinge points now those."
        ],
        [
            "Nonsmooth convex functions are differentiable everywhere except on the set of measures 0 where a subgradient exists.",
            "Then for the definition of the subgradient that's look at the field."
        ],
        [
            "Here again.",
            "Suppose we are now sitting at this hinge point."
        ],
        [
            "I can draw several times."
        ],
        [
            "Initial supporting hyperplane passing through this hinge points and the normal vector of all these supporting hyperplanes are valid.",
            "Support are valid subgradients and then we call."
        ],
        [
            "The set of or subgradients the subdifferential an we denoted as partial J of W."
        ],
        [
            "Now it is good that we have a convex set of subgradients, but."
        ],
        [
            "Not like in smooth optimization where a net negative gradient direction is always a decent direction, IE we can update our parameter in such a direction to reduce the objective function value.",
            "But now since the objective function is non smooth we only have subgradients and negative subgradient may not be a decent direction an if."
        ],
        [
            "You are determined to find a decent direction.",
            "Then you have to make sure this direction has inner product with or sub gradients or negative inner product with or sub gradients in the subdifferential set.",
            "Now this."
        ],
        [
            "These complications breakdown three things in BFG's first of all."
        ],
        [
            "The local quadratic model is no longer well defined WHI because here we only have a set of sub gradients.",
            "But for the local quadratic model we require a unique gradient everywhere."
        ],
        [
            "And Secondly, the direction or the quasi Newton direction?",
            "Which use an arbitrary subgradient from the subdifferential set is not guaranteed to be a decent."
        ],
        [
            "Direction finally we have to modify line search such that we can find the positive step size.",
            "For the parameter up."
        ],
        [
            "Date.",
            "Now, do you do all this problems in classical BFG is for nonsmooth convex optimizations, various sub gradient based method such as subgradient descent or bundle methods are preferred Now here in this talk."
        ],
        [
            "I will present our solution which is systematically fix all these three problems so as to make FGS amenable to subgradient and hence applicable to non smooth convex opt."
        ],
        [
            "Azatian now first step, changing the model.",
            "Recall that the classical BFG's use a local quadratic quadratic model.",
            "Taking this form.",
            "Now this model becomes ambiguous because we don't have a unique gradient at sub differentiable points or non smooth points."
        ],
        [
            "And of course you can say.",
            "I just chose a random or arbitrary subgradient to construct such a local quadratic model an.",
            "If you look at the figure, all this dashed quadratic lines are possible quadratic models of the objective function, each corresponding to a particular choice of subgradient.",
            "But"
        ],
        [
            "The resulting Coc Newton direction is not necessarily a decent direction.",
            "This is a problem.",
            "So in order to solve this fundamental modeling problem, we first need to generalize the local quadratic model of the classical VFS method."
        ],
        [
            "The.",
            "This is.",
            "The model we proposed here, and if you look at this figure, this solid line plot the new model and as you can see here, it is the Supreme and of all possible quadratic approximations.",
            "Now it is clear to see here it is not quadratic anymore because the Supreme and can be attained at different elements of the subdifferential for different.",
            "W. But uh, is that we can view this new model as a pseudo quadratic fit to your objective function now."
        ],
        [
            "We have a new model to work with."
        ],
        [
            "Then we can minimize this pseudo quadratic model to derive the iterate for the next iteration IW T + 1."
        ],
        [
            "To solve this means a problem, we can simply rewrite it as a constraint optimization problem.",
            "But when you look at it, the constraints here are very strict.",
            "So what we can do?"
        ],
        [
            "First of all, we can relax this constraint and then we solve.",
            "This reduced constraint optimization problem and after each iteration we add one more constraint to update our problem and solve it again now."
        ],
        [
            "I just re parameterized the formula in terms of the search direction P here an our goal is to find.",
            "As such, P that has enough product with or.",
            "That has a negative inner product with or sub gradient in the sub differing."
        ],
        [
            "So set now to do."
        ],
        [
            "So we developed an iterative procedure that produced such a decent direction."
        ],
        [
            "Just to give you a flavor of the algorithm, I put down a simplified version of the algorithm here.",
            "This iterative procedure is based on Colin generation.",
            "Now let's look at step three.",
            "We pick a subgradient from the subdifferential set an using this arc soup operation.",
            "Now we call this subgradient the most violating subgradient.",
            "We add it into our constraint.",
            "Now at Step 5.",
            "Here we have an updated optimization problem to solve, and as you can see here, to solve such a problem you can use quadratic programming and then solve it exactly.",
            "But to do so.",
            "We may have to spend some computational cost, so instead we."
        ],
        [
            "Adult and alternative approach.",
            "The basic idea is we propose.",
            "I need search direction as a convex combination of the previous direction, an negative PT times.",
            "Than you subgradient, we just picked at Step 3.",
            "Now.",
            "This combination coefficient mu can be computed exactly for its optimal value based on the argument used.",
            "Based on an argument using.",
            "This maximum dual progress."
        ],
        [
            "Now we can also prove that this iterative procedure.",
            "Will rich the optimal dual value of.",
            "The optimization problem at an order one over epsilon rate and epsilon is the precision of our solution.",
            "Now we know how to."
        ],
        [
            "Find a decent direction.",
            "The next step is to design a line search scheme an in Smith optimization.",
            "A positive step size is normally find by using a line search of a Wolf conditions.",
            "Now Wolf conditions guarantee a sufficient decrease in the objective function and also events.",
            "Unnecessary small step sizes.",
            "Now if you look at this figure, this is the objective function plotted along a decent search direction as.",
            "A function of the step size and the Wolf condition says that only step sizes lie in this acceptable interval will be considered now, if the."
        ],
        [
            "The objective function is nonsmooth.",
            "We propose a subgradient reformulation of the Wolf conditions, so we require to take Supreme and.",
            "Of all inner products.",
            "Of the search direction with Subgradients, and again, this reformulation of the Wolf condition will reduce to the original one when the objective function is smooth and just.",
            "Look at this figure.",
            "This reformulation of the Wolf condition also enforce acceptance of non trivial step sizes which will decrease this objective function.",
            "Now we have fixed all three problems in the classical BFG's method, and we call this variance of efg S method SBG's an it is straightforward to derive a limited memory variants of SBG's method and we also show."
        ],
        [
            "So in our paper we can.",
            "Apply Subway FGS algorithm to L2 regularizer hinge loss minimization."
        ],
        [
            "And let me just plot this objective function.",
            "It seems very quadratic, but."
        ],
        [
            "Zooming.",
            "You can see there exist several sub differentiable points, so it is."
        ],
        [
            "Piecewise quadratic, and actually for this kind of objective function we can do something smarter like we can do the exact line search."
        ],
        [
            "Now I have some results of Sub LV FGS with exact line search.",
            "Several public available data sets, but here I only presented a result on amnist.",
            "Now we compare Subl.",
            "The limited memory variants of subgraphs, method to two other optimizers.",
            "One is VM.",
            "I'm a bundle methods over the other is optimized.",
            "Cutting plane methods which will be presented here in this conference.",
            "I'm.",
            "I know that those algorithms have very strong performance reported in their corresponding publications.",
            "Now we evaluate all these three algorithms in terms of.",
            "The objective function value versus CPU SEC an.",
            "If you look at this figure.",
            "You can see some LV FGS reduces the objective function noticeably faster than BMI M, But.",
            "Being I'm catches up some LV FGS gradually during the course of optimization the performance of subgraphs, an focus are very similar, but on this data set it is just slightly slightly outperformed.",
            "The other two algorithm an if you want to look at more results, please come to our poster."
        ],
        [
            "Now also to illustrate the utility of our iterative direction finding inner group, we plug it into variance of.",
            "LV FG SL with a custom build for L1 regularised logistic loss minimalization.",
            "It's called Awesome Wise Limited causing you to method and we compare."
        ],
        [
            "The."
        ],
        [
            "Marion.",
            "So far awesome Lisa Limited causing you to message which use our direction finding routine with the oringinal and we found that this variant gives very compareable results to the original one.",
            "But we also found that.",
            "Replacing the direction finding routine of the Oringinal authorized limited, forcing you to methods.",
            "Make it with our diet."
        ],
        [
            "And finding routine make it more robust to the choice of subgradient fed to its direction.",
            "Finding inner loop an again.",
            "For more details please come to our post."
        ],
        [
            "Sure.",
            "Now.",
            "To conclude.",
            "We have systematically modified the classical course in Newton method.",
            "To work with nonsmooth convex optimization problems."
        ],
        [
            "And the resulting sub BFGS method use.",
            "Gradient in the fundamentally different way from bundle method i.e.",
            "A use pass gradient to build up a local quadratic like model of the objective function and the bundle methods used path gradient to construct a piecewise function lower bound to the objective function and then minimize it.",
            "So these two schemes are funded."
        ],
        [
            "Actually different.",
            "And we can apply this new sub BFGS method to a wide variety of machine learning applications."
        ],
        [
            "Now for future."
        ],
        [
            "At work we are looking into ways to accelerate our algorithm when it gets closer to the optimal.",
            "As we discovered in our experiments, when at the beginning some LV FGS was subbed, GS perform very well compared to say, BMI and this bundle method solver.",
            "But at the end it slows down.",
            "An hour conjecture is that at the end of the accurate model of the objective function really matters an the local model of.",
            "The sub LBFGS cannot faithfully model this.",
            "Submit model this objective function close to the."
        ],
        [
            "Optimal.",
            "And also we are trying to extend our algorithm to deal with other non smooth objective functions.",
            "For example the generalization of hinge loss for multiclass or multi label classifications."
        ],
        [
            "Anne.",
            "Thank you for your attention.",
            "Which is.",
            "Get it is using this to align approaches like this was a scalable approaches which also don't require reading subgradient, so it seems at least theoretically.",
            "What is the benefit?",
            "He refusing the machinery over a simple subgradient approach?",
            "Oh so so you know the the it's it's a question of why we want to use 2nd order optimization problem or over over just simple gradient descent methods.",
            "I wanna grab something.",
            "Look on Christmas here.",
            "It seems in both cases we get one over epsilon.",
            "I think.",
            "Idea confusing.",
            "Is that what the other one itself is in a loop?",
            "It's not the whole algorithm.",
            "Convergence is wonderful.",
            "It's not the convergence of the overall algorithm be.",
            "Overall algorithm has the same convergence data set DF here, which is.",
            "So the order one of over epsilon convergence rate is just for this piece of direction finding inner loop, so it's not the same as the overall convergence rate of the algorithm itself, so.",
            "No, yeah we haven't written out the proof, but we believe using the proof for the classical VFS method.",
            "It's not hard to extend.",
            "The proof."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, today I'm going to present our work on developing a quasi Newton approach to nonsmooth convex optimization, and this work is also supported by Nick to an Pascal 2.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now first of all, let me just give you a brief review of the classical course in Newton approach and later I will show you actually we can systematically extend this classic framework to nonsmooth convex optimization.",
                    "label": 0
                },
                {
                    "sent": "Now, the most successful.",
                    "label": 0
                },
                {
                    "sent": "Who is the Newton approach?",
                    "label": 0
                },
                {
                    "sent": "Is the BFGS method an which is widely regarded as the workhorse of smooth nonlinear optimization?",
                    "label": 1
                },
                {
                    "sent": "And here in this talk I will only consider convex objective functions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now VFG is forms a local quadratic model of the objective function at iterate saying WT an.",
                    "label": 1
                },
                {
                    "sent": "This local quadratic model is nothing but a second order Taylor expansion of the objective function at around WT anwer.",
                    "label": 1
                },
                {
                    "sent": "The Matrix PT is a symmetric positive definite matrix.",
                    "label": 0
                },
                {
                    "sent": "Which approach makes the inverse Hessian of the objective function now?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Having this local quadratic model BFG is minimized.",
                    "label": 1
                },
                {
                    "sent": "Empty of W to derive its parameter update now here.",
                    "label": 0
                },
                {
                    "sent": "PT is the so called course in Newton direction.",
                    "label": 1
                },
                {
                    "sent": "It is negative BT metrics times the gradient of the objective function evaluated at current iterate WT.",
                    "label": 0
                },
                {
                    "sent": "Now having this Sir.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each direction we need to find a positive step size ET to scale this search direction for the parameter update and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Normally this positive step size is determined by a line search of buying the Wolf conditions, and I will discuss more about Wolf conditions later in my talk.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, this BT metrics are appeared in the local quadratic model is then modified by a rank 2 update, not updating and storing this PT metrics require order D squared cost and D being the dimension of your objective function or the dimension of the optimization prob.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Therefore, for high dimensional problems, limited memory variants of BFG method is preferred.",
                    "label": 0
                },
                {
                    "sent": "It approximates the quasi Newton direction PT here via metrics free approach, i.e.",
                    "label": 0
                },
                {
                    "sent": "Without calculating these metrics BT this reduces the cost from order D square to order M * D An being the size of.",
                    "label": 0
                },
                {
                    "sent": "The memory and usually its value between 3 to 20.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now it is very important to notice that BFGS method assumes the objective function is smooth.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In machine learning context, many objective functions are non smooth.",
                    "label": 1
                },
                {
                    "sent": "As you can see in this figure.",
                    "label": 0
                },
                {
                    "sent": "It is a plot for the hinge loss which is usually used for binary classification in machine learning it is convex but it is non smooth and it is not smooth at this hinge points now those.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nonsmooth convex functions are differentiable everywhere except on the set of measures 0 where a subgradient exists.",
                    "label": 0
                },
                {
                    "sent": "Then for the definition of the subgradient that's look at the field.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here again.",
                    "label": 0
                },
                {
                    "sent": "Suppose we are now sitting at this hinge point.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can draw several times.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Initial supporting hyperplane passing through this hinge points and the normal vector of all these supporting hyperplanes are valid.",
                    "label": 0
                },
                {
                    "sent": "Support are valid subgradients and then we call.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The set of or subgradients the subdifferential an we denoted as partial J of W.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now it is good that we have a convex set of subgradients, but.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not like in smooth optimization where a net negative gradient direction is always a decent direction, IE we can update our parameter in such a direction to reduce the objective function value.",
                    "label": 0
                },
                {
                    "sent": "But now since the objective function is non smooth we only have subgradients and negative subgradient may not be a decent direction an if.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You are determined to find a decent direction.",
                    "label": 0
                },
                {
                    "sent": "Then you have to make sure this direction has inner product with or sub gradients or negative inner product with or sub gradients in the subdifferential set.",
                    "label": 0
                },
                {
                    "sent": "Now this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These complications breakdown three things in BFG's first of all.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The local quadratic model is no longer well defined WHI because here we only have a set of sub gradients.",
                    "label": 0
                },
                {
                    "sent": "But for the local quadratic model we require a unique gradient everywhere.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Secondly, the direction or the quasi Newton direction?",
                    "label": 0
                },
                {
                    "sent": "Which use an arbitrary subgradient from the subdifferential set is not guaranteed to be a decent.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Direction finally we have to modify line search such that we can find the positive step size.",
                    "label": 0
                },
                {
                    "sent": "For the parameter up.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Date.",
                    "label": 0
                },
                {
                    "sent": "Now, do you do all this problems in classical BFG is for nonsmooth convex optimizations, various sub gradient based method such as subgradient descent or bundle methods are preferred Now here in this talk.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will present our solution which is systematically fix all these three problems so as to make FGS amenable to subgradient and hence applicable to non smooth convex opt.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Azatian now first step, changing the model.",
                    "label": 1
                },
                {
                    "sent": "Recall that the classical BFG's use a local quadratic quadratic model.",
                    "label": 0
                },
                {
                    "sent": "Taking this form.",
                    "label": 0
                },
                {
                    "sent": "Now this model becomes ambiguous because we don't have a unique gradient at sub differentiable points or non smooth points.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course you can say.",
                    "label": 0
                },
                {
                    "sent": "I just chose a random or arbitrary subgradient to construct such a local quadratic model an.",
                    "label": 1
                },
                {
                    "sent": "If you look at the figure, all this dashed quadratic lines are possible quadratic models of the objective function, each corresponding to a particular choice of subgradient.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The resulting Coc Newton direction is not necessarily a decent direction.",
                    "label": 1
                },
                {
                    "sent": "This is a problem.",
                    "label": 1
                },
                {
                    "sent": "So in order to solve this fundamental modeling problem, we first need to generalize the local quadratic model of the classical VFS method.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "The model we proposed here, and if you look at this figure, this solid line plot the new model and as you can see here, it is the Supreme and of all possible quadratic approximations.",
                    "label": 0
                },
                {
                    "sent": "Now it is clear to see here it is not quadratic anymore because the Supreme and can be attained at different elements of the subdifferential for different.",
                    "label": 0
                },
                {
                    "sent": "W. But uh, is that we can view this new model as a pseudo quadratic fit to your objective function now.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a new model to work with.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can minimize this pseudo quadratic model to derive the iterate for the next iteration IW T + 1.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To solve this means a problem, we can simply rewrite it as a constraint optimization problem.",
                    "label": 0
                },
                {
                    "sent": "But when you look at it, the constraints here are very strict.",
                    "label": 0
                },
                {
                    "sent": "So what we can do?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First of all, we can relax this constraint and then we solve.",
                    "label": 0
                },
                {
                    "sent": "This reduced constraint optimization problem and after each iteration we add one more constraint to update our problem and solve it again now.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just re parameterized the formula in terms of the search direction P here an our goal is to find.",
                    "label": 0
                },
                {
                    "sent": "As such, P that has enough product with or.",
                    "label": 0
                },
                {
                    "sent": "That has a negative inner product with or sub gradient in the sub differing.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So set now to do.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we developed an iterative procedure that produced such a decent direction.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to give you a flavor of the algorithm, I put down a simplified version of the algorithm here.",
                    "label": 0
                },
                {
                    "sent": "This iterative procedure is based on Colin generation.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at step three.",
                    "label": 0
                },
                {
                    "sent": "We pick a subgradient from the subdifferential set an using this arc soup operation.",
                    "label": 0
                },
                {
                    "sent": "Now we call this subgradient the most violating subgradient.",
                    "label": 0
                },
                {
                    "sent": "We add it into our constraint.",
                    "label": 0
                },
                {
                    "sent": "Now at Step 5.",
                    "label": 0
                },
                {
                    "sent": "Here we have an updated optimization problem to solve, and as you can see here, to solve such a problem you can use quadratic programming and then solve it exactly.",
                    "label": 0
                },
                {
                    "sent": "But to do so.",
                    "label": 0
                },
                {
                    "sent": "We may have to spend some computational cost, so instead we.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adult and alternative approach.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is we propose.",
                    "label": 0
                },
                {
                    "sent": "I need search direction as a convex combination of the previous direction, an negative PT times.",
                    "label": 0
                },
                {
                    "sent": "Than you subgradient, we just picked at Step 3.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This combination coefficient mu can be computed exactly for its optimal value based on the argument used.",
                    "label": 0
                },
                {
                    "sent": "Based on an argument using.",
                    "label": 0
                },
                {
                    "sent": "This maximum dual progress.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can also prove that this iterative procedure.",
                    "label": 0
                },
                {
                    "sent": "Will rich the optimal dual value of.",
                    "label": 0
                },
                {
                    "sent": "The optimization problem at an order one over epsilon rate and epsilon is the precision of our solution.",
                    "label": 0
                },
                {
                    "sent": "Now we know how to.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Find a decent direction.",
                    "label": 0
                },
                {
                    "sent": "The next step is to design a line search scheme an in Smith optimization.",
                    "label": 1
                },
                {
                    "sent": "A positive step size is normally find by using a line search of a Wolf conditions.",
                    "label": 1
                },
                {
                    "sent": "Now Wolf conditions guarantee a sufficient decrease in the objective function and also events.",
                    "label": 0
                },
                {
                    "sent": "Unnecessary small step sizes.",
                    "label": 1
                },
                {
                    "sent": "Now if you look at this figure, this is the objective function plotted along a decent search direction as.",
                    "label": 0
                },
                {
                    "sent": "A function of the step size and the Wolf condition says that only step sizes lie in this acceptable interval will be considered now, if the.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The objective function is nonsmooth.",
                    "label": 0
                },
                {
                    "sent": "We propose a subgradient reformulation of the Wolf conditions, so we require to take Supreme and.",
                    "label": 0
                },
                {
                    "sent": "Of all inner products.",
                    "label": 0
                },
                {
                    "sent": "Of the search direction with Subgradients, and again, this reformulation of the Wolf condition will reduce to the original one when the objective function is smooth and just.",
                    "label": 0
                },
                {
                    "sent": "Look at this figure.",
                    "label": 0
                },
                {
                    "sent": "This reformulation of the Wolf condition also enforce acceptance of non trivial step sizes which will decrease this objective function.",
                    "label": 0
                },
                {
                    "sent": "Now we have fixed all three problems in the classical BFG's method, and we call this variance of efg S method SBG's an it is straightforward to derive a limited memory variants of SBG's method and we also show.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in our paper we can.",
                    "label": 0
                },
                {
                    "sent": "Apply Subway FGS algorithm to L2 regularizer hinge loss minimization.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let me just plot this objective function.",
                    "label": 0
                },
                {
                    "sent": "It seems very quadratic, but.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zooming.",
                    "label": 0
                },
                {
                    "sent": "You can see there exist several sub differentiable points, so it is.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Piecewise quadratic, and actually for this kind of objective function we can do something smarter like we can do the exact line search.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I have some results of Sub LV FGS with exact line search.",
                    "label": 1
                },
                {
                    "sent": "Several public available data sets, but here I only presented a result on amnist.",
                    "label": 0
                },
                {
                    "sent": "Now we compare Subl.",
                    "label": 0
                },
                {
                    "sent": "The limited memory variants of subgraphs, method to two other optimizers.",
                    "label": 0
                },
                {
                    "sent": "One is VM.",
                    "label": 0
                },
                {
                    "sent": "I'm a bundle methods over the other is optimized.",
                    "label": 0
                },
                {
                    "sent": "Cutting plane methods which will be presented here in this conference.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "I know that those algorithms have very strong performance reported in their corresponding publications.",
                    "label": 0
                },
                {
                    "sent": "Now we evaluate all these three algorithms in terms of.",
                    "label": 0
                },
                {
                    "sent": "The objective function value versus CPU SEC an.",
                    "label": 0
                },
                {
                    "sent": "If you look at this figure.",
                    "label": 0
                },
                {
                    "sent": "You can see some LV FGS reduces the objective function noticeably faster than BMI M, But.",
                    "label": 0
                },
                {
                    "sent": "Being I'm catches up some LV FGS gradually during the course of optimization the performance of subgraphs, an focus are very similar, but on this data set it is just slightly slightly outperformed.",
                    "label": 0
                },
                {
                    "sent": "The other two algorithm an if you want to look at more results, please come to our poster.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now also to illustrate the utility of our iterative direction finding inner group, we plug it into variance of.",
                    "label": 0
                },
                {
                    "sent": "LV FG SL with a custom build for L1 regularised logistic loss minimalization.",
                    "label": 0
                },
                {
                    "sent": "It's called Awesome Wise Limited causing you to method and we compare.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Marion.",
                    "label": 0
                },
                {
                    "sent": "So far awesome Lisa Limited causing you to message which use our direction finding routine with the oringinal and we found that this variant gives very compareable results to the original one.",
                    "label": 0
                },
                {
                    "sent": "But we also found that.",
                    "label": 0
                },
                {
                    "sent": "Replacing the direction finding routine of the Oringinal authorized limited, forcing you to methods.",
                    "label": 0
                },
                {
                    "sent": "Make it with our diet.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finding routine make it more robust to the choice of subgradient fed to its direction.",
                    "label": 1
                },
                {
                    "sent": "Finding inner loop an again.",
                    "label": 0
                },
                {
                    "sent": "For more details please come to our post.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "To conclude.",
                    "label": 0
                },
                {
                    "sent": "We have systematically modified the classical course in Newton method.",
                    "label": 0
                },
                {
                    "sent": "To work with nonsmooth convex optimization problems.",
                    "label": 1
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the resulting sub BFGS method use.",
                    "label": 0
                },
                {
                    "sent": "Gradient in the fundamentally different way from bundle method i.e.",
                    "label": 1
                },
                {
                    "sent": "A use pass gradient to build up a local quadratic like model of the objective function and the bundle methods used path gradient to construct a piecewise function lower bound to the objective function and then minimize it.",
                    "label": 0
                },
                {
                    "sent": "So these two schemes are funded.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually different.",
                    "label": 0
                },
                {
                    "sent": "And we can apply this new sub BFGS method to a wide variety of machine learning applications.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for future.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At work we are looking into ways to accelerate our algorithm when it gets closer to the optimal.",
                    "label": 0
                },
                {
                    "sent": "As we discovered in our experiments, when at the beginning some LV FGS was subbed, GS perform very well compared to say, BMI and this bundle method solver.",
                    "label": 0
                },
                {
                    "sent": "But at the end it slows down.",
                    "label": 0
                },
                {
                    "sent": "An hour conjecture is that at the end of the accurate model of the objective function really matters an the local model of.",
                    "label": 0
                },
                {
                    "sent": "The sub LBFGS cannot faithfully model this.",
                    "label": 0
                },
                {
                    "sent": "Submit model this objective function close to the.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimal.",
                    "label": 0
                },
                {
                    "sent": "And also we are trying to extend our algorithm to deal with other non smooth objective functions.",
                    "label": 1
                },
                {
                    "sent": "For example the generalization of hinge loss for multiclass or multi label classifications.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "Get it is using this to align approaches like this was a scalable approaches which also don't require reading subgradient, so it seems at least theoretically.",
                    "label": 0
                },
                {
                    "sent": "What is the benefit?",
                    "label": 0
                },
                {
                    "sent": "He refusing the machinery over a simple subgradient approach?",
                    "label": 0
                },
                {
                    "sent": "Oh so so you know the the it's it's a question of why we want to use 2nd order optimization problem or over over just simple gradient descent methods.",
                    "label": 0
                },
                {
                    "sent": "I wanna grab something.",
                    "label": 0
                },
                {
                    "sent": "Look on Christmas here.",
                    "label": 0
                },
                {
                    "sent": "It seems in both cases we get one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "Idea confusing.",
                    "label": 0
                },
                {
                    "sent": "Is that what the other one itself is in a loop?",
                    "label": 0
                },
                {
                    "sent": "It's not the whole algorithm.",
                    "label": 0
                },
                {
                    "sent": "Convergence is wonderful.",
                    "label": 0
                },
                {
                    "sent": "It's not the convergence of the overall algorithm be.",
                    "label": 0
                },
                {
                    "sent": "Overall algorithm has the same convergence data set DF here, which is.",
                    "label": 0
                },
                {
                    "sent": "So the order one of over epsilon convergence rate is just for this piece of direction finding inner loop, so it's not the same as the overall convergence rate of the algorithm itself, so.",
                    "label": 0
                },
                {
                    "sent": "No, yeah we haven't written out the proof, but we believe using the proof for the classical VFS method.",
                    "label": 0
                },
                {
                    "sent": "It's not hard to extend.",
                    "label": 0
                },
                {
                    "sent": "The proof.",
                    "label": 0
                }
            ]
        }
    }
}