{
    "id": "rin5sby7zuob3k4jz3hokhtwirl3zfb7",
    "title": "The Double-Barrelled LASSO (Sparse Canonical Correlation Analysis)",
    "info": {
        "author": [
            "David R. Hardoon, London's Global University"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/lms08_hardoon_scca/",
    "segmentation": [
        [
            "The first talk is given by David Hardman.",
            "Joint work with John Shaw Taylor and you see the title cool Thank you.",
            "OK, so this talk is very much like a smooth continuation from the previous two ones that we had this morning about.",
            "Moving towards sparse can on the correlation analysis.",
            "Now the other running title is double barreled Lasso, which will.",
            "Later on, come into play of why use that so."
        ],
        [
            "OK, a brief outline is OK.",
            "There's been two talks about concrete analysis by just give a very brief overview of what it is.",
            "What is trying to solve.",
            "I'll give you the motivation of why we're trying to solve a sparse Canonical correlation analysis and.",
            "In our particular case, we're interested in what I refer to as a machine learning primal dual formulation, and the reason for machine learning is just to disambiguate it from the optimization optimization primal dual formulation.",
            "I'll go into the sparse CCA, give the other so analogy and some experiments."
        ],
        [
            "OK.",
            "So just another set of brief introduction.",
            "So basically what Canonical rational Canonical correlation analysis tries to do is find some linear combination.",
            "In this case W&WB such that you're maximizing the correlation between 2 views represented here is XA and XB in his potentially sees the covariance between these two views.",
            "Now we are referring to this as the primal CCA.",
            "The jewel formulation is an as I said, machine learning jewel is when you simply kernel eyes it.",
            "And you're representing the weights by some.",
            "Linear combination of the weights.",
            "So here you maximizing Alpha, which can be some nonlinear waiting.",
            "So this is the dual formulation.",
            "Now one one can think about is in the case of documents here in the primer formulation will be working in the word space, so W he'll be on the words, whereas in the dual formulation will be working on the actual document space.",
            "So.",
            "That's why OK, the motivation of a sparse.",
            "Formulation why do we want to have one?",
            "And more importantly in well in our particular case, why did we want the primal dual formulation of working in this kind of work?"
        ],
        [
            "Open document space so."
        ],
        [
            "The reason for this?"
        ],
        [
            "Is the type of applications we were trying to solve."
        ],
        [
            "It became very largely intuitive as to the motivations behind here we.",
            "Wanted to find for example, in the case of the documents, a correlation between words to document so we can interpret what are the words that give you the maximum correlation to a particular semantic representation in the document.",
            "An rather than having you know the entire dictionary as essentially what you'll have happen in a regular CCA, and then you were threshold, at which point you're interested in, you know the words with the highest weightings, and you discard the lowest one.",
            "We want to just find a sparse representation, so in a way or."
        ],
        [
            "Somatically discarding so.",
            "So."
        ],
        [
            "Consider the following cases.",
            "For example enzyme prediction, where hey, you have you want to find the correlation between some enzyme reactants and their functionality that can be represented as a graph structure."
        ],
        [
            "As I said in bilingual analysis, when one hand you have words or documents in one language French and then you have words or documents in the other language English and you want to find a correlation between these two languages in such a way that you want to find a relationship between English words to French documents.",
            "So if you have a French document talking about agriculture, you want to find the words in English that capture that semantic meaning of the document."
        ],
        [
            "An another thing which we looked on is for example brain analysis, so here.",
            "You would have in one view, let's say MRI data e.g data fMRI data on other hand rather than having a simple representation that can be regression of let's say doing a task or not doing a task, you want to have some complex label where if you remember the XN XP-1 way of thinking of it is rather than two views, it could have simply your data and then at some complex label of that data of another workshop is going on structured structured input and structured output.",
            "So that was the interest on brain and Alice."
        ],
        [
            "This was actually the real motivation behind this.",
            "We want to find a sparse representation of voxels, so neurons in the brain that were corresponding to some really high complex tasks that the person was doing or looking at or listening.",
            "So another."
        ],
        [
            "OK, so very trivially we can reformulate the CCF problem to the primal dual by just taking let's say our second view and just representing an outdoor presentation.",
            "And we have this maximum quotient problem."
        ],
        [
            "Yeah, and which we can simply buy.",
            "The solution is invariant with scaling, so we can simply write it as that subject to these constraints.",
            "OK, so."
        ],
        [
            "Yes, spicy.",
            "It's pretty obvious from this problem that this problem is non convex, so we started kind of taking very interesting road and how we can formulate this to a convex problem now.",
            "What we're able to observe is that as written over there, minimizing the correlation between the two vectors can be viewed as minimizing the angle between them, and I think it's alternating something.",
            "It's Brandon Friedman long time in statistics, So what it means, instead of having the maximized."
        ],
        [
            "Patient here, we can replace it as a minimization problem when we minimizing the angle between these two vectors.",
            "Now we know that if you want to minimize minimizing angle between vectors, we can fix the scaling of one of them and minimize angle from the other because it's again it's in."
        ],
        [
            "And to the scaling.",
            "So we drop the one of the constraints now.",
            "Thirdly.",
            "Yeah, we know that the solution is again and very to the scaling so.",
            "We would like to avoid using this this particular constraint again because."
        ],
        [
            "Nonconvexity, so we replace it.",
            "With an Infinity normally now we will explain why we chose this particular constraint.",
            "To make the whole mechanism work so we were starting to get there.",
            "But as I said in bottom, we're not quite there yet."
        ],
        [
            "OK, so the reason for the Infinity Norm essentially is.",
            "There's a few reasons one is really want to solve a sparse solution where starting from the case where both E. I'll jeweler presentation, let's say over the documents and our WR primary presentation of the words are zero, they're all zero 'cause we want to build up.",
            "It's a bottom up approach obviously can minimize further than that.",
            "So what this allows us is that we can essentially.",
            "Introduce, well, it's sort of a hack actually.",
            "Drop the Infinity norm constrained by setting one of the components within the jewel vector to being one, because Infinity norm is simply is the maximum of the vector is equal to 1 S. What this could entail is that you're fixing one 'cause if it's on the kernel matrices you fixing one of these semantic context to being your solution.",
            "You saying OK, this is going to be my anchor."
        ],
        [
            "Anne.",
            "Did not come up, sorry.",
            "Yeah, so we fixing one particular as anchor and now we're able to enforce the one norm.",
            "So sparsity constraint on the primal weights W and on the remaining dual wait so from E1 to E K -- 1 and Ek plus one to L. So in a way we have a particular anchor saying, OK, this is a semantic context we're interested in.",
            "We're going to find a sparse representation on the words, but we're also going to find other semantic context that are related to it.",
            "So.",
            "This essentially now allows us to have a particular starting point to find a sparse solution.",
            "OK, but oh."
        ],
        [
            "The wrong way.",
            "Yes, but again, what does it mean by setting Ek so?",
            "We took a very naive approach, which probably not the optimal one, but is sufficient for experiments where you solve for particular projection to flattus off another one.",
            "So if another one deflate and so on and so on.",
            "So what we did is we said OK for very first projection.",
            "Let's simply fix the first industry.",
            "One final sparse solution on the remaining of the dual weights deflate.",
            "They now fix the second component find from the sun, and so on, and so essentially get one of the diagonal.",
            "Like I said, a very naive approach where.",
            "If you solve for all L, essentially you're getting the full semantic context on all the documents now.",
            "An alternative approach, which probably bit more nice, is here.",
            "I is the identity matrix.",
            "Simply solving this maximum quotient where in each stage of the deflation it will tell you which one of these indices is actually giving you the most contribution.",
            "And then you can instead of having the first one, you simply you set K to be.",
            "Yeah, sorry, OK will be the maximum of I.",
            "And then you deflate.",
            "So for the next one.",
            "Set it here.",
            "Deflate.",
            "So for the next one, and so on and so on.",
            "So we have.",
            "A certain mechanism of actually finding sparsity."
        ],
        [
            "OK, so it goes back to this."
        ],
        [
            "So what this results is, and we now have a convex optimization problem where it's loosely constrained to having a correlation between our two views.",
            "Now notice here is that our one constraint is over the remaining components of the jewel."
        ],
        [
            "Wait?",
            "OK, but how do we solve this in practice?",
            "So we now propose."
        ],
        [
            "An algorithm IK solution to this.",
            "Again, in our particular problem, we were only interested in the positive spectrum movie we were working with documents and their intuitively there didn't seem to be much meaning of a negative contribution of the document.",
            "So we said we're just going to limit ourselves to the positive spectrum.",
            "But this can be obviously be included and solved as well.",
            "So with the weights with primal weights, we say, OK, we're going to have positive and negative, so we have the following Lagrangian.",
            "The previous solution, subject to the Lagrangian coefficients on the W plus way W minus an on E."
        ],
        [
            "So how do we go about setting sparsity?",
            "'cause again we we don't know what we found was after taking the derivative.",
            "Again, the falling to equation with respect to W plus and W minus is that when we added the two together we get this following property of the relationship between Alpha minus an Alpha plus now what we're seeing here is for the case when Alpha Mine Alpha minus is set to 0A plus is set to two mu and as you increasing Alpha minus essentially increasing the waiting on the negative, you're decreasing on the positive and you get this interaction essentially.",
            "So we're able to deduce that you essentially can say that Alpha minus an Alpha plus is bounded by zero to two mu.",
            "'cause this is the region in which they're interacting.",
            "Great, so we can now say given these equation, because we acquire there supposed to be equate to zero there, sorry.",
            "When we are updating.",
            "Any Alpha I which is less than zero or greater than two mu we know.",
            "OK we have to compensate in some degree, so we basically compensate by updating that particular WI.",
            "Anne.",
            "What we found is that most of our alphas were within the bound and only very few of them.",
            "We only have very few of those particular Wis.",
            "It needs to be updated so great and we able to the exact same thing with the jeweler presentation where the beta I is less than zero, we update for that particular GI.",
            "So in a way once and think of it as an active set learning."
        ],
        [
            "The analogy to less so is that we, ironically enough, only after we did everything, we notice that if you fix E in the sense that you have a particular vector there and you drop the optimization with regards to it, you simply end up with a very classical assault problem.",
            "So we found is that we came up with a new approach with an active set approach, which solves a double barreled love.",
            "So we're trying to minimize respect to both the W and the.",
            "But then again also solve this problem.",
            "By simply not updating on the remaining components.",
            "So this is Pence.",
            "The title of the Double barrel Lissa.",
            "OK, so."
        ],
        [
            "Moving on to the experiments.",
            "So as I kind of alluded to it, we were dealing with documents in a bilingual relationship, so we were interested in the relation between French, English and French, which had 300 samples documents in each category.",
            "260 two 1637 English features I words, 2951 French words and then the English Spanish corpus which.",
            "We limited the number.",
            "There was actually many much more samples, but we just we wanted to see how it scaled in the primal so we only have 1000 samples and 40,000 words in English and 57,000 words in Spanish.",
            "So here you would want to find a very kind of sparse set of documents and a sparse set of words.",
            "That are highly correlated.",
            "We process the data using term frequency info Circle C and we sent it."
        ],
        [
            "OK, so.",
            "As we're kind of proposing in a a sparse CC approach.",
            "Kind of naturally we compare it to the Canonical correlation, CCA, Canonical correlation analysis approach.",
            "Now what we do is we pose a mate retrieval problem.",
            "So we have paired documents in English and French, English and Spanish.",
            "So we're saying, OK, we learn a particular semantic space, and now on a new test corpus, given a document in one language, can you find the Mate document in the other one?",
            "So and then we computed an average precision where this is the average.",
            "Overall, the test queries here.",
            "Capital IJ is simply the location in which the mate is retrieved.",
            "So obviously if we have the perfect method, it will always be on the top of the list, so this will give us kind of an average indication of how good were working.",
            "OK, so I could explain this.",
            "Obviously if you can overfit when we don't have regularization in the CCA.",
            "So we have to set in regularization.",
            "Amateur and we cheated becausw and as we propose in a second slide, we don't have any parameters to tune for the sparse PCA, so we decided we're going to give Casey a slightly unfair advantage that we found a priority that regularization parameter that performed well."
        ],
        [
            "In this particular task, now, as I was saying.",
            "We proposed.",
            "AAA method.",
            "Which will not have any parameters to tune and that kind of seems counterintuitive, because if you're in back, you had that mu and gamma, which essentially control the sparsity.",
            "These are hyperparameters from the kind of solve literature, so I wasn't really interested in kind of twiddling around with cross validation and trying to figure out what is the optimal sparsity, because what is the optimal sparsity between two documents and so on.",
            "So what we found is that."
        ],
        [
            "We could actually have an automatic approach of settings party.",
            "We can automatically find what's the kind of ideal number of words.",
            "So initial step.",
            "Notice that W 0 and A0."
        ],
        [
            "They also."
        ],
        [
            "Zero so we can drop them from the equation, giving us an.",
            "An approximation for the hyperparameter which we kind of now abusing it and treating it as a Lagrangian coefficient.",
            "OK, right?"
        ],
        [
            "So.",
            "OK, and the simplification of CCA to the sole problem.",
            "We use it to just focus on whether this method actually works well and we get a kind of a good approximation for sparsity.",
            "Obviously if you flip the problem around, you can do the same thing."
        ],
        [
            "For the gamma parameter.",
            "So.",
            "We wanted to see whether our approach empirically performs well in detecting a good automatic."
        ],
        [
            "Approach sparsity, So what we did is we computed sparsity.",
            "As as the function of the hyperparameter value and what we're seeing here is on the Y axis, it's the ratio of words that have been retrieved in one language.",
            "So remember that retrieval problem we now giving the Word document in French and we're saying, generate essentially a number X number of words in English that is the same number of words as was in the original English native document.",
            "Now, did that make sense?",
            "Also, said again.",
            "You have training corpus.",
            "You learn a semantic space.",
            "You have left out documents.",
            "The pairwise you saying OK, French document generate new words.",
            "Given this query in English and now you have the original English document which you withheld and saying does that have the exact same number of words?",
            "And that's exactly what we're seeing here.",
            "So we find that for different hyperparameter values for the very small one we're generating a document with about 25 number scale of 2.5 times the number of words.",
            "And obviously as you continue will decrease to zero.",
            "And here is the opposite spectrum of about.",
            "Half the number of words the horizontal line is would be the ideal choice of sparsity, where you're essentially generating a new document which has exact same number of words as they withheld document.",
            "But in our case would be ideal and what we're finding is that our approach automatic approach is pretty damn close to it.",
            "Now, this is for only one query.",
            "Obviously there's many, many more, so we did."
        ],
        [
            "Basically a leave one out for all the French documents and all the English documents and took the average across all of these and what we're finding that our approach for automatic setting of new is.",
            "Basically it's generating a one to one ratio just about.",
            "And compared to non sparse approach with this was just CCA.",
            "We're generating a document with 20 three times the original document.",
            "OK, fantastic, we can generate the document with just about one to one.",
            "How good is it?"
        ],
        [
            "Oh yeah, sorry bout the quality.",
            "So here is the French English corpus with the mate retrieval problem on the.",
            "You're with your left left figure.",
            "We're using 50 documents for training and 250 documents for testing, and this is the Y axis.",
            "Is the error an the X axis is the number of the projection that we're using, and what we're able to find is the blue line is CCA and the red line is sparse PCA an as we're increasing the projections.",
            "Obviously the error decreases and about 40 as paciencia convergent perform pretty much identically the.",
            "Like you got the right figure is.",
            "The same thing, but when we use 100 documents for training and 200 for testing.",
            "OK, so in this case a non sparse method is beating this path method great, but we will also."
        ],
        [
            "So interested on interpretability.",
            "We wanted to be able to get a set that we can understand what's going on and not because the non sparse method is using the full corpus of documents and the full set of words or 2600 and something.",
            "It's not really helpful.",
            "So what we're able to see here that the sparse PCA as an increasing number of projection and this is a cumulative sum.",
            "The number of words maximum we reach a plateau of 143 words, whereas KCTS said users on your users the full 2700 words.",
            "So we.",
            "Able to learn basically almost at around 40 where we're performing the same as CCA.",
            "Where we using of much, much sparser semantic space, we're learning the semantic space using very, very few words in comparison to the CCA.",
            "And here is the.",
            "This is the primal view.",
            "So this is the number of words.",
            "And here is the dual view where it's a number of documents, and for example first projection using about just 25 documents, and obviously as we increase your reach, the full number of documents.",
            "And 40 was about just under 45 documents, OK?"
        ],
        [
            "So great now in the.",
            "Yeah, last two slides.",
            "So in English Spanish one we were the semantic space is much much richer.",
            "We have 50,000 words in one language and 40 thousand 50,000 words in each language.",
            "We find the CCA is performing about 5% worse in sparse PCA.",
            "An intuition behind this is that there's a lot a lot of noise going on there, whereas a sparse."
        ],
        [
            "PCA is able to remove it because as increasing the number of projections we're seeing, we're only using about maximum 400 words by Casey is using 50,000 words to learn a semantic space, so we're able to look pretty darn well and even better by using very sparse."
        ],
        [
            "I'll just quickly reach conclusion, so we propose a novel approach for sparse primal do CCA with an automatic approach for setting the sparsity.",
            "Essentially now there are obviously open questions.",
            "The most predominant one is the has to go about setting this Ek across now.",
            "One method could be using a balance or PCA an the other thing, which would be very natural approaches incorporating the negative spectrum of the dual features."
        ],
        [
            "Language.",
            "You will have one ratio cool if you combine English to Japanese for you.",
            "That's an interesting question.",
            "I mean there is, and I know there's a corpus at the Nanyang.",
            "I think it was that played around with such Japanese English and I think would be very worthwhile trying it out, yeah?",
            "You have this block."
        ],
        [
            "You said that you wanted the same amount of words in English as a French."
        ],
        [
            "So yeah."
        ],
        [
            "I didn't quite get that."
        ],
        [
            "In French it means no, no.",
            "So what I meant?",
            "So you have paired documents, you have a document in English and you have a document in French.",
            "Yes.",
            "Now I'm withholding the English document.",
            "I'm saying, given the French document, find me the words that would have constructed this mate.",
            "And then I'm saying, OK, I've constructed a set of documents of a new document essentially, is it similar in the sense of the number of words to the original English document?",
            "And so essentially using one language to generate a creation of another language.",
            "OK, let's thank David."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first talk is given by David Hardman.",
                    "label": 0
                },
                {
                    "sent": "Joint work with John Shaw Taylor and you see the title cool Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, so this talk is very much like a smooth continuation from the previous two ones that we had this morning about.",
                    "label": 0
                },
                {
                    "sent": "Moving towards sparse can on the correlation analysis.",
                    "label": 0
                },
                {
                    "sent": "Now the other running title is double barreled Lasso, which will.",
                    "label": 0
                },
                {
                    "sent": "Later on, come into play of why use that so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, a brief outline is OK.",
                    "label": 0
                },
                {
                    "sent": "There's been two talks about concrete analysis by just give a very brief overview of what it is.",
                    "label": 0
                },
                {
                    "sent": "What is trying to solve.",
                    "label": 0
                },
                {
                    "sent": "I'll give you the motivation of why we're trying to solve a sparse Canonical correlation analysis and.",
                    "label": 1
                },
                {
                    "sent": "In our particular case, we're interested in what I refer to as a machine learning primal dual formulation, and the reason for machine learning is just to disambiguate it from the optimization optimization primal dual formulation.",
                    "label": 0
                },
                {
                    "sent": "I'll go into the sparse CCA, give the other so analogy and some experiments.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So just another set of brief introduction.",
                    "label": 1
                },
                {
                    "sent": "So basically what Canonical rational Canonical correlation analysis tries to do is find some linear combination.",
                    "label": 1
                },
                {
                    "sent": "In this case W&WB such that you're maximizing the correlation between 2 views represented here is XA and XB in his potentially sees the covariance between these two views.",
                    "label": 1
                },
                {
                    "sent": "Now we are referring to this as the primal CCA.",
                    "label": 0
                },
                {
                    "sent": "The jewel formulation is an as I said, machine learning jewel is when you simply kernel eyes it.",
                    "label": 0
                },
                {
                    "sent": "And you're representing the weights by some.",
                    "label": 0
                },
                {
                    "sent": "Linear combination of the weights.",
                    "label": 0
                },
                {
                    "sent": "So here you maximizing Alpha, which can be some nonlinear waiting.",
                    "label": 0
                },
                {
                    "sent": "So this is the dual formulation.",
                    "label": 0
                },
                {
                    "sent": "Now one one can think about is in the case of documents here in the primer formulation will be working in the word space, so W he'll be on the words, whereas in the dual formulation will be working on the actual document space.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's why OK, the motivation of a sparse.",
                    "label": 0
                },
                {
                    "sent": "Formulation why do we want to have one?",
                    "label": 0
                },
                {
                    "sent": "And more importantly in well in our particular case, why did we want the primal dual formulation of working in this kind of work?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Open document space so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The reason for this?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the type of applications we were trying to solve.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It became very largely intuitive as to the motivations behind here we.",
                    "label": 1
                },
                {
                    "sent": "Wanted to find for example, in the case of the documents, a correlation between words to document so we can interpret what are the words that give you the maximum correlation to a particular semantic representation in the document.",
                    "label": 0
                },
                {
                    "sent": "An rather than having you know the entire dictionary as essentially what you'll have happen in a regular CCA, and then you were threshold, at which point you're interested in, you know the words with the highest weightings, and you discard the lowest one.",
                    "label": 1
                },
                {
                    "sent": "We want to just find a sparse representation, so in a way or.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Somatically discarding so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consider the following cases.",
                    "label": 0
                },
                {
                    "sent": "For example enzyme prediction, where hey, you have you want to find the correlation between some enzyme reactants and their functionality that can be represented as a graph structure.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I said in bilingual analysis, when one hand you have words or documents in one language French and then you have words or documents in the other language English and you want to find a correlation between these two languages in such a way that you want to find a relationship between English words to French documents.",
                    "label": 0
                },
                {
                    "sent": "So if you have a French document talking about agriculture, you want to find the words in English that capture that semantic meaning of the document.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An another thing which we looked on is for example brain analysis, so here.",
                    "label": 0
                },
                {
                    "sent": "You would have in one view, let's say MRI data e.g data fMRI data on other hand rather than having a simple representation that can be regression of let's say doing a task or not doing a task, you want to have some complex label where if you remember the XN XP-1 way of thinking of it is rather than two views, it could have simply your data and then at some complex label of that data of another workshop is going on structured structured input and structured output.",
                    "label": 0
                },
                {
                    "sent": "So that was the interest on brain and Alice.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was actually the real motivation behind this.",
                    "label": 0
                },
                {
                    "sent": "We want to find a sparse representation of voxels, so neurons in the brain that were corresponding to some really high complex tasks that the person was doing or looking at or listening.",
                    "label": 0
                },
                {
                    "sent": "So another.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so very trivially we can reformulate the CCF problem to the primal dual by just taking let's say our second view and just representing an outdoor presentation.",
                    "label": 0
                },
                {
                    "sent": "And we have this maximum quotient problem.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and which we can simply buy.",
                    "label": 0
                },
                {
                    "sent": "The solution is invariant with scaling, so we can simply write it as that subject to these constraints.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, spicy.",
                    "label": 0
                },
                {
                    "sent": "It's pretty obvious from this problem that this problem is non convex, so we started kind of taking very interesting road and how we can formulate this to a convex problem now.",
                    "label": 0
                },
                {
                    "sent": "What we're able to observe is that as written over there, minimizing the correlation between the two vectors can be viewed as minimizing the angle between them, and I think it's alternating something.",
                    "label": 1
                },
                {
                    "sent": "It's Brandon Friedman long time in statistics, So what it means, instead of having the maximized.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient here, we can replace it as a minimization problem when we minimizing the angle between these two vectors.",
                    "label": 0
                },
                {
                    "sent": "Now we know that if you want to minimize minimizing angle between vectors, we can fix the scaling of one of them and minimize angle from the other because it's again it's in.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to the scaling.",
                    "label": 0
                },
                {
                    "sent": "So we drop the one of the constraints now.",
                    "label": 0
                },
                {
                    "sent": "Thirdly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we know that the solution is again and very to the scaling so.",
                    "label": 0
                },
                {
                    "sent": "We would like to avoid using this this particular constraint again because.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nonconvexity, so we replace it.",
                    "label": 0
                },
                {
                    "sent": "With an Infinity normally now we will explain why we chose this particular constraint.",
                    "label": 0
                },
                {
                    "sent": "To make the whole mechanism work so we were starting to get there.",
                    "label": 0
                },
                {
                    "sent": "But as I said in bottom, we're not quite there yet.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the reason for the Infinity Norm essentially is.",
                    "label": 0
                },
                {
                    "sent": "There's a few reasons one is really want to solve a sparse solution where starting from the case where both E. I'll jeweler presentation, let's say over the documents and our WR primary presentation of the words are zero, they're all zero 'cause we want to build up.",
                    "label": 0
                },
                {
                    "sent": "It's a bottom up approach obviously can minimize further than that.",
                    "label": 0
                },
                {
                    "sent": "So what this allows us is that we can essentially.",
                    "label": 0
                },
                {
                    "sent": "Introduce, well, it's sort of a hack actually.",
                    "label": 0
                },
                {
                    "sent": "Drop the Infinity norm constrained by setting one of the components within the jewel vector to being one, because Infinity norm is simply is the maximum of the vector is equal to 1 S. What this could entail is that you're fixing one 'cause if it's on the kernel matrices you fixing one of these semantic context to being your solution.",
                    "label": 0
                },
                {
                    "sent": "You saying OK, this is going to be my anchor.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Did not come up, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we fixing one particular as anchor and now we're able to enforce the one norm.",
                    "label": 0
                },
                {
                    "sent": "So sparsity constraint on the primal weights W and on the remaining dual wait so from E1 to E K -- 1 and Ek plus one to L. So in a way we have a particular anchor saying, OK, this is a semantic context we're interested in.",
                    "label": 1
                },
                {
                    "sent": "We're going to find a sparse representation on the words, but we're also going to find other semantic context that are related to it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This essentially now allows us to have a particular starting point to find a sparse solution.",
                    "label": 0
                },
                {
                    "sent": "OK, but oh.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The wrong way.",
                    "label": 0
                },
                {
                    "sent": "Yes, but again, what does it mean by setting Ek so?",
                    "label": 1
                },
                {
                    "sent": "We took a very naive approach, which probably not the optimal one, but is sufficient for experiments where you solve for particular projection to flattus off another one.",
                    "label": 0
                },
                {
                    "sent": "So if another one deflate and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we said OK for very first projection.",
                    "label": 0
                },
                {
                    "sent": "Let's simply fix the first industry.",
                    "label": 0
                },
                {
                    "sent": "One final sparse solution on the remaining of the dual weights deflate.",
                    "label": 0
                },
                {
                    "sent": "They now fix the second component find from the sun, and so on, and so essentially get one of the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Like I said, a very naive approach where.",
                    "label": 0
                },
                {
                    "sent": "If you solve for all L, essentially you're getting the full semantic context on all the documents now.",
                    "label": 0
                },
                {
                    "sent": "An alternative approach, which probably bit more nice, is here.",
                    "label": 0
                },
                {
                    "sent": "I is the identity matrix.",
                    "label": 1
                },
                {
                    "sent": "Simply solving this maximum quotient where in each stage of the deflation it will tell you which one of these indices is actually giving you the most contribution.",
                    "label": 0
                },
                {
                    "sent": "And then you can instead of having the first one, you simply you set K to be.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry, OK will be the maximum of I.",
                    "label": 0
                },
                {
                    "sent": "And then you deflate.",
                    "label": 0
                },
                {
                    "sent": "So for the next one.",
                    "label": 0
                },
                {
                    "sent": "Set it here.",
                    "label": 0
                },
                {
                    "sent": "Deflate.",
                    "label": 0
                },
                {
                    "sent": "So for the next one, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "A certain mechanism of actually finding sparsity.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so it goes back to this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what this results is, and we now have a convex optimization problem where it's loosely constrained to having a correlation between our two views.",
                    "label": 0
                },
                {
                    "sent": "Now notice here is that our one constraint is over the remaining components of the jewel.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "OK, but how do we solve this in practice?",
                    "label": 1
                },
                {
                    "sent": "So we now propose.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An algorithm IK solution to this.",
                    "label": 0
                },
                {
                    "sent": "Again, in our particular problem, we were only interested in the positive spectrum movie we were working with documents and their intuitively there didn't seem to be much meaning of a negative contribution of the document.",
                    "label": 0
                },
                {
                    "sent": "So we said we're just going to limit ourselves to the positive spectrum.",
                    "label": 1
                },
                {
                    "sent": "But this can be obviously be included and solved as well.",
                    "label": 0
                },
                {
                    "sent": "So with the weights with primal weights, we say, OK, we're going to have positive and negative, so we have the following Lagrangian.",
                    "label": 1
                },
                {
                    "sent": "The previous solution, subject to the Lagrangian coefficients on the W plus way W minus an on E.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we go about setting sparsity?",
                    "label": 1
                },
                {
                    "sent": "'cause again we we don't know what we found was after taking the derivative.",
                    "label": 0
                },
                {
                    "sent": "Again, the falling to equation with respect to W plus and W minus is that when we added the two together we get this following property of the relationship between Alpha minus an Alpha plus now what we're seeing here is for the case when Alpha Mine Alpha minus is set to 0A plus is set to two mu and as you increasing Alpha minus essentially increasing the waiting on the negative, you're decreasing on the positive and you get this interaction essentially.",
                    "label": 0
                },
                {
                    "sent": "So we're able to deduce that you essentially can say that Alpha minus an Alpha plus is bounded by zero to two mu.",
                    "label": 0
                },
                {
                    "sent": "'cause this is the region in which they're interacting.",
                    "label": 0
                },
                {
                    "sent": "Great, so we can now say given these equation, because we acquire there supposed to be equate to zero there, sorry.",
                    "label": 1
                },
                {
                    "sent": "When we are updating.",
                    "label": 0
                },
                {
                    "sent": "Any Alpha I which is less than zero or greater than two mu we know.",
                    "label": 0
                },
                {
                    "sent": "OK we have to compensate in some degree, so we basically compensate by updating that particular WI.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "What we found is that most of our alphas were within the bound and only very few of them.",
                    "label": 0
                },
                {
                    "sent": "We only have very few of those particular Wis.",
                    "label": 0
                },
                {
                    "sent": "It needs to be updated so great and we able to the exact same thing with the jeweler presentation where the beta I is less than zero, we update for that particular GI.",
                    "label": 0
                },
                {
                    "sent": "So in a way once and think of it as an active set learning.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The analogy to less so is that we, ironically enough, only after we did everything, we notice that if you fix E in the sense that you have a particular vector there and you drop the optimization with regards to it, you simply end up with a very classical assault problem.",
                    "label": 0
                },
                {
                    "sent": "So we found is that we came up with a new approach with an active set approach, which solves a double barreled love.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to minimize respect to both the W and the.",
                    "label": 0
                },
                {
                    "sent": "But then again also solve this problem.",
                    "label": 0
                },
                {
                    "sent": "By simply not updating on the remaining components.",
                    "label": 0
                },
                {
                    "sent": "So this is Pence.",
                    "label": 0
                },
                {
                    "sent": "The title of the Double barrel Lissa.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Moving on to the experiments.",
                    "label": 0
                },
                {
                    "sent": "So as I kind of alluded to it, we were dealing with documents in a bilingual relationship, so we were interested in the relation between French, English and French, which had 300 samples documents in each category.",
                    "label": 0
                },
                {
                    "sent": "260 two 1637 English features I words, 2951 French words and then the English Spanish corpus which.",
                    "label": 1
                },
                {
                    "sent": "We limited the number.",
                    "label": 0
                },
                {
                    "sent": "There was actually many much more samples, but we just we wanted to see how it scaled in the primal so we only have 1000 samples and 40,000 words in English and 57,000 words in Spanish.",
                    "label": 0
                },
                {
                    "sent": "So here you would want to find a very kind of sparse set of documents and a sparse set of words.",
                    "label": 0
                },
                {
                    "sent": "That are highly correlated.",
                    "label": 0
                },
                {
                    "sent": "We process the data using term frequency info Circle C and we sent it.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "As we're kind of proposing in a a sparse CC approach.",
                    "label": 0
                },
                {
                    "sent": "Kind of naturally we compare it to the Canonical correlation, CCA, Canonical correlation analysis approach.",
                    "label": 0
                },
                {
                    "sent": "Now what we do is we pose a mate retrieval problem.",
                    "label": 1
                },
                {
                    "sent": "So we have paired documents in English and French, English and Spanish.",
                    "label": 0
                },
                {
                    "sent": "So we're saying, OK, we learn a particular semantic space, and now on a new test corpus, given a document in one language, can you find the Mate document in the other one?",
                    "label": 1
                },
                {
                    "sent": "So and then we computed an average precision where this is the average.",
                    "label": 0
                },
                {
                    "sent": "Overall, the test queries here.",
                    "label": 0
                },
                {
                    "sent": "Capital IJ is simply the location in which the mate is retrieved.",
                    "label": 0
                },
                {
                    "sent": "So obviously if we have the perfect method, it will always be on the top of the list, so this will give us kind of an average indication of how good were working.",
                    "label": 0
                },
                {
                    "sent": "OK, so I could explain this.",
                    "label": 0
                },
                {
                    "sent": "Obviously if you can overfit when we don't have regularization in the CCA.",
                    "label": 0
                },
                {
                    "sent": "So we have to set in regularization.",
                    "label": 0
                },
                {
                    "sent": "Amateur and we cheated becausw and as we propose in a second slide, we don't have any parameters to tune for the sparse PCA, so we decided we're going to give Casey a slightly unfair advantage that we found a priority that regularization parameter that performed well.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this particular task, now, as I was saying.",
                    "label": 0
                },
                {
                    "sent": "We proposed.",
                    "label": 0
                },
                {
                    "sent": "AAA method.",
                    "label": 0
                },
                {
                    "sent": "Which will not have any parameters to tune and that kind of seems counterintuitive, because if you're in back, you had that mu and gamma, which essentially control the sparsity.",
                    "label": 0
                },
                {
                    "sent": "These are hyperparameters from the kind of solve literature, so I wasn't really interested in kind of twiddling around with cross validation and trying to figure out what is the optimal sparsity, because what is the optimal sparsity between two documents and so on.",
                    "label": 0
                },
                {
                    "sent": "So what we found is that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We could actually have an automatic approach of settings party.",
                    "label": 1
                },
                {
                    "sent": "We can automatically find what's the kind of ideal number of words.",
                    "label": 0
                },
                {
                    "sent": "So initial step.",
                    "label": 0
                },
                {
                    "sent": "Notice that W 0 and A0.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They also.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zero so we can drop them from the equation, giving us an.",
                    "label": 0
                },
                {
                    "sent": "An approximation for the hyperparameter which we kind of now abusing it and treating it as a Lagrangian coefficient.",
                    "label": 0
                },
                {
                    "sent": "OK, right?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, and the simplification of CCA to the sole problem.",
                    "label": 1
                },
                {
                    "sent": "We use it to just focus on whether this method actually works well and we get a kind of a good approximation for sparsity.",
                    "label": 0
                },
                {
                    "sent": "Obviously if you flip the problem around, you can do the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the gamma parameter.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We wanted to see whether our approach empirically performs well in detecting a good automatic.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approach sparsity, So what we did is we computed sparsity.",
                    "label": 0
                },
                {
                    "sent": "As as the function of the hyperparameter value and what we're seeing here is on the Y axis, it's the ratio of words that have been retrieved in one language.",
                    "label": 0
                },
                {
                    "sent": "So remember that retrieval problem we now giving the Word document in French and we're saying, generate essentially a number X number of words in English that is the same number of words as was in the original English native document.",
                    "label": 0
                },
                {
                    "sent": "Now, did that make sense?",
                    "label": 0
                },
                {
                    "sent": "Also, said again.",
                    "label": 0
                },
                {
                    "sent": "You have training corpus.",
                    "label": 0
                },
                {
                    "sent": "You learn a semantic space.",
                    "label": 0
                },
                {
                    "sent": "You have left out documents.",
                    "label": 0
                },
                {
                    "sent": "The pairwise you saying OK, French document generate new words.",
                    "label": 0
                },
                {
                    "sent": "Given this query in English and now you have the original English document which you withheld and saying does that have the exact same number of words?",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what we're seeing here.",
                    "label": 0
                },
                {
                    "sent": "So we find that for different hyperparameter values for the very small one we're generating a document with about 25 number scale of 2.5 times the number of words.",
                    "label": 0
                },
                {
                    "sent": "And obviously as you continue will decrease to zero.",
                    "label": 0
                },
                {
                    "sent": "And here is the opposite spectrum of about.",
                    "label": 0
                },
                {
                    "sent": "Half the number of words the horizontal line is would be the ideal choice of sparsity, where you're essentially generating a new document which has exact same number of words as they withheld document.",
                    "label": 0
                },
                {
                    "sent": "But in our case would be ideal and what we're finding is that our approach automatic approach is pretty damn close to it.",
                    "label": 0
                },
                {
                    "sent": "Now, this is for only one query.",
                    "label": 0
                },
                {
                    "sent": "Obviously there's many, many more, so we did.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically a leave one out for all the French documents and all the English documents and took the average across all of these and what we're finding that our approach for automatic setting of new is.",
                    "label": 0
                },
                {
                    "sent": "Basically it's generating a one to one ratio just about.",
                    "label": 0
                },
                {
                    "sent": "And compared to non sparse approach with this was just CCA.",
                    "label": 0
                },
                {
                    "sent": "We're generating a document with 20 three times the original document.",
                    "label": 0
                },
                {
                    "sent": "OK, fantastic, we can generate the document with just about one to one.",
                    "label": 0
                },
                {
                    "sent": "How good is it?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh yeah, sorry bout the quality.",
                    "label": 0
                },
                {
                    "sent": "So here is the French English corpus with the mate retrieval problem on the.",
                    "label": 0
                },
                {
                    "sent": "You're with your left left figure.",
                    "label": 0
                },
                {
                    "sent": "We're using 50 documents for training and 250 documents for testing, and this is the Y axis.",
                    "label": 0
                },
                {
                    "sent": "Is the error an the X axis is the number of the projection that we're using, and what we're able to find is the blue line is CCA and the red line is sparse PCA an as we're increasing the projections.",
                    "label": 0
                },
                {
                    "sent": "Obviously the error decreases and about 40 as paciencia convergent perform pretty much identically the.",
                    "label": 0
                },
                {
                    "sent": "Like you got the right figure is.",
                    "label": 0
                },
                {
                    "sent": "The same thing, but when we use 100 documents for training and 200 for testing.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case a non sparse method is beating this path method great, but we will also.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So interested on interpretability.",
                    "label": 0
                },
                {
                    "sent": "We wanted to be able to get a set that we can understand what's going on and not because the non sparse method is using the full corpus of documents and the full set of words or 2600 and something.",
                    "label": 0
                },
                {
                    "sent": "It's not really helpful.",
                    "label": 0
                },
                {
                    "sent": "So what we're able to see here that the sparse PCA as an increasing number of projection and this is a cumulative sum.",
                    "label": 0
                },
                {
                    "sent": "The number of words maximum we reach a plateau of 143 words, whereas KCTS said users on your users the full 2700 words.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Able to learn basically almost at around 40 where we're performing the same as CCA.",
                    "label": 0
                },
                {
                    "sent": "Where we using of much, much sparser semantic space, we're learning the semantic space using very, very few words in comparison to the CCA.",
                    "label": 0
                },
                {
                    "sent": "And here is the.",
                    "label": 0
                },
                {
                    "sent": "This is the primal view.",
                    "label": 0
                },
                {
                    "sent": "So this is the number of words.",
                    "label": 0
                },
                {
                    "sent": "And here is the dual view where it's a number of documents, and for example first projection using about just 25 documents, and obviously as we increase your reach, the full number of documents.",
                    "label": 0
                },
                {
                    "sent": "And 40 was about just under 45 documents, OK?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So great now in the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, last two slides.",
                    "label": 0
                },
                {
                    "sent": "So in English Spanish one we were the semantic space is much much richer.",
                    "label": 0
                },
                {
                    "sent": "We have 50,000 words in one language and 40 thousand 50,000 words in each language.",
                    "label": 0
                },
                {
                    "sent": "We find the CCA is performing about 5% worse in sparse PCA.",
                    "label": 0
                },
                {
                    "sent": "An intuition behind this is that there's a lot a lot of noise going on there, whereas a sparse.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "PCA is able to remove it because as increasing the number of projections we're seeing, we're only using about maximum 400 words by Casey is using 50,000 words to learn a semantic space, so we're able to look pretty darn well and even better by using very sparse.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll just quickly reach conclusion, so we propose a novel approach for sparse primal do CCA with an automatic approach for setting the sparsity.",
                    "label": 1
                },
                {
                    "sent": "Essentially now there are obviously open questions.",
                    "label": 0
                },
                {
                    "sent": "The most predominant one is the has to go about setting this Ek across now.",
                    "label": 0
                },
                {
                    "sent": "One method could be using a balance or PCA an the other thing, which would be very natural approaches incorporating the negative spectrum of the dual features.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Language.",
                    "label": 0
                },
                {
                    "sent": "You will have one ratio cool if you combine English to Japanese for you.",
                    "label": 0
                },
                {
                    "sent": "That's an interesting question.",
                    "label": 0
                },
                {
                    "sent": "I mean there is, and I know there's a corpus at the Nanyang.",
                    "label": 0
                },
                {
                    "sent": "I think it was that played around with such Japanese English and I think would be very worthwhile trying it out, yeah?",
                    "label": 0
                },
                {
                    "sent": "You have this block.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You said that you wanted the same amount of words in English as a French.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I didn't quite get that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In French it means no, no.",
                    "label": 0
                },
                {
                    "sent": "So what I meant?",
                    "label": 0
                },
                {
                    "sent": "So you have paired documents, you have a document in English and you have a document in French.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Now I'm withholding the English document.",
                    "label": 0
                },
                {
                    "sent": "I'm saying, given the French document, find me the words that would have constructed this mate.",
                    "label": 0
                },
                {
                    "sent": "And then I'm saying, OK, I've constructed a set of documents of a new document essentially, is it similar in the sense of the number of words to the original English document?",
                    "label": 0
                },
                {
                    "sent": "And so essentially using one language to generate a creation of another language.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank David.",
                    "label": 0
                }
            ]
        }
    }
}