{
    "id": "w6zduql2nvwxj5btcno5nn7jnj63o4ur",
    "title": "Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning",
    "info": {
        "author": [
            "Francis R. Bach, INRIA - SIERRA project-team"
        ],
        "published": "Jan. 19, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Optimization Methods->Stochastic Optimization",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_bach_machine/",
    "segmentation": [
        [
            "So high, so I'm Francis since it don't work with Eric Moline, so the in this paper we consider the problem of the stochastic approximation, which leads to a class of techniques which are well adapted to large scale learning, because essentially they look at the data only once.",
            "So the problem is as follows.",
            "You want to."
        ],
        [
            "I'm at a function F which you only observe through on-demand biased estimates of his gradients, so the classical setup from us know processing is a one of additive noise.",
            "Here epsilon N, but in machine learning this app is a bit different and also very natural, where each knew observation here that that N will lead to its own random cost, and under memorandum gradients.",
            "So here FF status as a random function is a loss incurred by the predictor data with the data then.",
            "And the function F we want to minimize his directly.",
            "The expectation of FN, which is essentially the generalization error or the predictor data.",
            "So classical examples include the least squares and logistic regression which are at the bottom of this line."
        ],
        [
            "So in this paper we consider a convex context problems and we study in detail the effects of both of smoothness Anna strong convexity in the context of a very old algorithm from the 50s, which essentially stochastic gradient descent, also known as Robbins Monro accurate algorithm where at each direction you follow the negative gradient.",
            "We also consider a small modification which will replace the iterated reiterate data end by the average of all the previous iterations.",
            "Techniques, technique which is often called Project Cooper edging.",
            "So the key question we want to address in the paper is how you set up this linear sequence.",
            "So what is the size of the steps that we want to consider?",
            "But we would like to get is something which is robust to the difficulty of the problem and if possible robbers to the knowledge of the various smoothness constants."
        ],
        [
            "So here is a summary of our results, so we consider learning rates of the Form C / N to the Alpha and the key element which we're affect performance is strong convexity.",
            "So essentially your problem will be strongly convex if it is low dimensional and will be not non strongly convex.",
            "If it is high dimensional.",
            "So what is often advocated in machine learning is true to use the rate of the form one of our end.",
            "OK So what we show in the paper?",
            "This is OK, but only for a simple strongly convex problems where you achieve the right of one of our end which is known to be A to be optimal.",
            "However, if you do averaging you also get the optimal rate of one of our end, but for a much wider range of Alpha you can.",
            "You can imagine much longer step and if you do averaging you also get the same.",
            "One of the 10 rates, but you're a bit more robust to the setup of the constants, so this was well known in the signal processing bit less money machine learning.",
            "The other good aspect about averaging is that if the problem happens to be a bit harder than you thought then averaging we still lead to the to the optimal rate in the non Sonic on this case.",
            "And in that case this is one over root N an issue to use the right the learning rate 1 / N or constant times 1 / N you can converge a bit really slowly.",
            "So the take home message don't choose gamma and being one of our end, use something which will make lot longer steps and user edging.",
            "And if you want to know more please come to the poster."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So high, so I'm Francis since it don't work with Eric Moline, so the in this paper we consider the problem of the stochastic approximation, which leads to a class of techniques which are well adapted to large scale learning, because essentially they look at the data only once.",
                    "label": 0
                },
                {
                    "sent": "So the problem is as follows.",
                    "label": 0
                },
                {
                    "sent": "You want to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm at a function F which you only observe through on-demand biased estimates of his gradients, so the classical setup from us know processing is a one of additive noise.",
                    "label": 1
                },
                {
                    "sent": "Here epsilon N, but in machine learning this app is a bit different and also very natural, where each knew observation here that that N will lead to its own random cost, and under memorandum gradients.",
                    "label": 0
                },
                {
                    "sent": "So here FF status as a random function is a loss incurred by the predictor data with the data then.",
                    "label": 0
                },
                {
                    "sent": "And the function F we want to minimize his directly.",
                    "label": 0
                },
                {
                    "sent": "The expectation of FN, which is essentially the generalization error or the predictor data.",
                    "label": 0
                },
                {
                    "sent": "So classical examples include the least squares and logistic regression which are at the bottom of this line.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this paper we consider a convex context problems and we study in detail the effects of both of smoothness Anna strong convexity in the context of a very old algorithm from the 50s, which essentially stochastic gradient descent, also known as Robbins Monro accurate algorithm where at each direction you follow the negative gradient.",
                    "label": 1
                },
                {
                    "sent": "We also consider a small modification which will replace the iterated reiterate data end by the average of all the previous iterations.",
                    "label": 0
                },
                {
                    "sent": "Techniques, technique which is often called Project Cooper edging.",
                    "label": 0
                },
                {
                    "sent": "So the key question we want to address in the paper is how you set up this linear sequence.",
                    "label": 0
                },
                {
                    "sent": "So what is the size of the steps that we want to consider?",
                    "label": 0
                },
                {
                    "sent": "But we would like to get is something which is robust to the difficulty of the problem and if possible robbers to the knowledge of the various smoothness constants.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a summary of our results, so we consider learning rates of the Form C / N to the Alpha and the key element which we're affect performance is strong convexity.",
                    "label": 1
                },
                {
                    "sent": "So essentially your problem will be strongly convex if it is low dimensional and will be not non strongly convex.",
                    "label": 0
                },
                {
                    "sent": "If it is high dimensional.",
                    "label": 0
                },
                {
                    "sent": "So what is often advocated in machine learning is true to use the rate of the form one of our end.",
                    "label": 0
                },
                {
                    "sent": "OK So what we show in the paper?",
                    "label": 0
                },
                {
                    "sent": "This is OK, but only for a simple strongly convex problems where you achieve the right of one of our end which is known to be A to be optimal.",
                    "label": 1
                },
                {
                    "sent": "However, if you do averaging you also get the optimal rate of one of our end, but for a much wider range of Alpha you can.",
                    "label": 0
                },
                {
                    "sent": "You can imagine much longer step and if you do averaging you also get the same.",
                    "label": 0
                },
                {
                    "sent": "One of the 10 rates, but you're a bit more robust to the setup of the constants, so this was well known in the signal processing bit less money machine learning.",
                    "label": 1
                },
                {
                    "sent": "The other good aspect about averaging is that if the problem happens to be a bit harder than you thought then averaging we still lead to the to the optimal rate in the non Sonic on this case.",
                    "label": 0
                },
                {
                    "sent": "And in that case this is one over root N an issue to use the right the learning rate 1 / N or constant times 1 / N you can converge a bit really slowly.",
                    "label": 0
                },
                {
                    "sent": "So the take home message don't choose gamma and being one of our end, use something which will make lot longer steps and user edging.",
                    "label": 0
                },
                {
                    "sent": "And if you want to know more please come to the poster.",
                    "label": 0
                }
            ]
        }
    }
}