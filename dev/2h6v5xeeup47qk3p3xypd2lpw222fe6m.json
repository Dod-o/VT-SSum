{
    "id": "2h6v5xeeup47qk3p3xypd2lpw222fe6m",
    "title": "Some Aspects of Learning Rates for SVMs",
    "info": {
        "author": [
            "Ingo Steinwart, Los Alamos National Laboratory"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/mlss05us_steinwart_salrs/",
    "segmentation": [
        [
            "For establishing learning rates to the base risk, and this is mainly a joint work with Clint's cover from my Institute.",
            "The."
        ],
        [
            "We start with an overview.",
            "I will briefly recall what we want.",
            "Classification is about what consistency is.",
            "We've just seen that in the previous talk.",
            "I will discuss the question operates and I also will present a recently proposed margin condition.",
            "Support vector machines are then recalled briefly and then in order to establish rates.",
            "An general theorem which I will present later, states that we have to restrict the class of distributions and I will present a new assumption on class of distributions which are in some sense in geometric terms.",
            "This condition together this with this condition will then leads us to rates for support vector machines, and I will present the results and discuss some open questions.",
            "And finally, depending on the remaining time, I will sketch some ideas of the proof.",
            "OK.",
            "So."
        ],
        [
            "What is?",
            "Classification, so the major aim of this part is to fix some rotations.",
            "We have an unknown probability measure P on X * Y. X is our input space.",
            "Often a compact set subset of Rd and Capital Y is the set of labels minus one and one, so we are actually only considering binary classification.",
            "And we also have training set data set consisting of pairs xiy I which are assumed to be IID according to P. And the Y eyes indicate whether the point XI belongs to the minus one class.",
            "Or the one class?",
            "And then the aim of classification is to construct function FT depending on the training set T such that the classification risk which I denote by RP of FT is small, so the classification is simply measures the probability of the set of points where we do not predict the label correctly.",
            "A classifier then just for completeness, is an algorithm that constructs in FT to every given T and the.",
            "Formulation of this risk somewhere indicates that we are actually interested in classifiers which produce risk as small as possible, so the next.",
            "Quantity I need is the so called base risk which is the infima of all.",
            "Possible risk where F runs over measurable functions from X to Y.",
            "And.",
            "We've seen that in the previous talk, if we knew the distribution P, we can actually find a function F which realises this minimal risk.",
            "Namely, if we consider this conditional class probability.",
            "Denoted by Etta, often called the supervisor, then FP function FP from X to Y, realises the base risk if.",
            "On the set of points where the supervisor tends to produce positive labels.",
            "If this function is strictly positive on this set and negative.",
            "On the other hand, on the other set.",
            "So in this sense, the.",
            "Based decision function FP also recognizes the two classes X, one and X -- 1.",
            "Finally, we say that P is noise free if this supervisor letter is minus is.",
            "If it is 01 valued, that means that the labeling process is deterministic.",
            "So."
        ],
        [
            "OK.",
            "I've said that we are interested in algorithms which produce risk as small as possible, and one way to describe that in a mathematical sense is the notion of universal consistency.",
            "We say that a classifier is universally consistent if the average classification error tends to the base risk.",
            "Provided that the sample size goes to Infinity and this must hold for every probability measure P becausw, we do not know our distribution PR priore and therefore in order to ensure that the algorithm learns entotic Lee, we have to show that for every distribution P. The good thing?",
            "About this notion here is that there are actually many algorithms known which are universally consistent.",
            "The first one is the K nearest neighbor rule, for which it was shown to be universally consistent.",
            "Histogram rules.",
            "Certain neural networks later on with the support vector machines, two are also universally consistent, so in an asmt artikkel sense they these algorithms can learn.",
            "However, this is of course not the only question we may have, since having here convergence doesn't tell us anything about the speed of convergence.",
            "So if we are interested in a certain accuracy, say epsilon, we may ask what is the required number of samples we need to ensure this.",
            "Accuracy.",
            "Up to epsilon and this leads to the question of rates.",
            "So can we guarantee conversion rates rate in one that holds for all distributions?",
            "Again for all, since we do not know PR priore.",
            "And unfortunately the answer is no, which is quite old result by devoir.",
            "Which roughly speaking states that whatever classifier we have there exists distributions on X, provided that X is not a finite set such that we cannot guarantee a uniform convergence here.",
            "But that means that certain algorithms cannot learn certain distributions were and other distributions can be learned well by this.",
            "An algorithm mathematical.",
            "Way to resolve this problem is probably the most natural one is to consider just big subclasses of distributions P. And then it turns out that there are some sufficient conditions.",
            "I will present on the next slide in some insufficient conditions.",
            "For distributions which are there in sure.",
            "Uniform rates are not.",
            "But just to give you an idea what kind of.",
            "Bad conditions one can think of which do not give us the desired results, are for example, if.",
            "The marginal distribution has a specified density with respect to the lebec measure, so we fix a density we know everything about the marginal distribution and Additionally assume that P is noise free.",
            "No random process in the labeling.",
            "Then we still cannot guarantee rights here.",
            "Quite similar if we have this assumptions on the marginal distribution and assume that the supervisor is very smooth, SESI Infinity.",
            "We still cannot guarantee rights here.",
            "And a more geonet geometric condition.",
            "If P is noise free and one of the classes, say X one is compact and convex.",
            "We still cannot guarantee rates here, which gives us an idea why finding reasonable, interesting subclasses of P may be a hard problem.",
            "On the other hand.",
            "There are also known results.",
            "Which give us."
        ],
        [
            "Certain rates and I do not make any attempt here to be complete.",
            "They're just chosen in order to motivate the next results.",
            "So for example, if the supervisor satisfies certain smoothness assumptions, which include bounds on the derivatives like Sobolev conditions.",
            "Then certain plugin rules which are usually based on the square empirical risk minimizers lead to rates or perform end to the power minus Alpha, where Alpha is an exponent between zero and a half.",
            "Anne.",
            "We cannot be faster here.",
            "Structural risk minimization.",
            "We've seen that in the previous talk over hypothesis sets, FI will find it fizzy dimension can learn with right up to 1 / sqrt N if and this is crucial.",
            "Base function is contained in one of the FIS.",
            "And the reason why this is crucial is that.",
            "In this case, we simply cannot make.",
            "We simply do not have to consider the approximation error of the problem.",
            "So everything what we just heard about dimensionality is not a matter of problems here.",
            "Empirical risk minimization, which is very which is some of the basic step of the structural risk minimization, can actually learn faster if we assume P to be noise free, and then they're resulting rate can be up to 1 / N. So what we see here is first that there is a.",
            "Big gap between the arbitrary case where we do not make any assumptions on the noise and the noise free case, and we will see later how we can actually find rates in between.",
            "If we make certain.",
            "Conditions on the noise which measure the amount of noise that will be done on the next slide.",
            "The.",
            "Other interesting things here, which I think.",
            "Important for practices, for example, how realistic are smoothness assumptions?",
            "Oneta?",
            "Well, for regression these smoothness assumptions are widely accepted in many cases.",
            "If we have classification problems, it's rather unclear.",
            "Since the data itself is often determined by labeling of human beings, and I personally do not know whether they behave smoothly in a mathematical sense.",
            "So whether this is satisfied or not, one has to.",
            "Answer that for you for himself, but I personally think that might be a problem.",
            "What is also quite known is that it is usually very hard to find hypothesis sets which contain a base function.",
            "So you almost never be in the lucky situation where this is satisfied.",
            "Real world distributions are always almost always noisy, so this noise rate is also rather unrealistic.",
            "And then the only right you know is this rather poor rate.",
            "And in addition, both methods here have the problem that they are usually computationally not feasible, since for most interesting optimization problems the solution is NP hard to find.",
            "So from a theoretical point of view, there's still the goal to find efficient learning algorithms which also work well in practice, which can learn up to a fast rate of 1 / N under realistic conditions on P. And here realistic is again questionable what that means, but hopefully I can convince you that the results I will present later abit more realistic at least.",
            "OK.",
            "So."
        ],
        [
            "The next thing I want to mention is how we can get out of this black white scheme of no noise or just arbitrary noise where the rates are either 1 / N or 1 / sqrt N. And the solution here, which was recently proposed by super cost, is.",
            "Based on the idea that we have to measure the amount of noise.",
            "And before I present the definition, I like to make an observation.",
            "So this after here, which was defined as this regular conditional probability describes somehow the amount of noise.",
            "More precisely, if this two Atom minus one is close to 1.",
            "Then Atom must be either close to 0 or close to 1.",
            "And that is the supervisor.",
            "Data only produces a small amount of wrong, wrongly labeled samples in that specific point X.",
            "Right?",
            "Whereas if this quantity is close to 0, then Atom must be close to 1/2 and therefore we may observe we will observe a lot of contradicting samples in that specific point X if we only sample at this point.",
            "Yeah, so if this is close to zero we have a high noise in the labeling process at the point X and vice versa.",
            "And super cause noise condition or exponent now measures the size of the set where this happens.",
            "More precisely, a distribution P satisfies the margin condition for some Q between zero and Infinity.",
            "If the probability of the set of points where this term here is smaller than or equal to.",
            "Then T can be estimated by this right hand side here, where C is a fixed constant and Q is this exponent.",
            "What we see here is that if she goes to zero and we only interested in small T, then the right hand side goes to zero and the bigger Q is, the faster the convergence is.",
            "And therefore in this sense.",
            "The set of noise is smaller.",
            "This is the interpretation of that.",
            "In one of the extreme cases, Q = 0, we see that this is automatically satisfied for all distributions, so this is not a assumption at all, whereas in the other extreme case, Q equals Infinity, we see because of this assumption here, that Atom must be actually bounded away from the critical level at half.",
            "And every Q in between describes the situation in between.",
            "OK, so with this.",
            "Noise exponent supercop then showed that empirical risk minimization over our hypothesis class F which contains again the base function in order to avoid approximation in our considerations, can learn with this right here where Q.",
            "Is the margin condition and P measures the complexity of this size F?",
            "It's not a Videmment 50 dimensional type of measure.",
            "It's actually based on a rather complicated covering number.",
            "But without going here into too much details, let's consider first simplicity.",
            "The case where P is very close to zero, that is, the complexity is very small.",
            "Then this fraction here becomes more or less Q + 1 / Q + 2.",
            "What does that mean if Q = 0, we have the general rate of 1 / sqrt N, whereas if Q equals Infinity, this is the best case or rate.",
            "Essentially is 1 / N. And every Q in between describes a case in between.",
            "So the picture here is if we make.",
            "Quantitative statement about the about the amount of noise we actually obtain.",
            "Rates which are in between the two cases of no noise and complete noise or no assumptions on the noise."
        ],
        [
            "That's no turn to support vector machines.",
            "Most of the things on this slide are already explained in the morning, so let's make it brief.",
            "The symmetric function K on X * X two Rs.",
            "Quarter kernel.",
            "If all these metrics are positive semidefinite.",
            "This is equivalent to the existence of a feature space H. And a feature map 5 from X to H such that K realises the inner product of five with itself.",
            "And we also already have seen in the morning that this Hilbert spaces in general not unique.",
            "However, there is in some sense a unique or Canonical feature space, so called reproducing kernel Hilbert space, which up to isometry, is the smallest possible feature space, and one way to construct this again apply symmetry is to consider these linear combinations of kernel evaluations equipped that with the inner product.",
            "Define in this way and then consider the completion and it turns out that this is up to isometry the reproducing kernel Hilbert space.",
            "We mainly."
        ],
        [
            "Interested in one or two kernels?",
            "Since they are the most popular for support vector machines, the polynomial kernels are built from the inner product on Rd plus some positive coefficient A and a positive power M. I would not discuss these kernels in detail, but the Gaussian RBF kernels which are more relevant for practical issues too.",
            "I will consider in more detail later.",
            "So how are these builds?",
            "We consider the exponential function take a free parameter Sigma and then the Euclidean distance of the points X&X prime energy.",
            "Take the square of that and that's the.",
            "That's the kernel.",
            "That's the Gaussian RBF kernel.",
            "One nice thing about these kernels here is that they are reproducing kernel.",
            "Hilbert spaces are in a certain sense, which.",
            "More precisely, if we restrict our considerations on compact subsets of Rd, then the reproducing kernel Hilbert space of these kernels.",
            "Then in C of X.",
            "And this will be used for consistency results and the approximation error problem inconsistency results.",
            "So this space is a nice big space for considering approximation errors in general, and I will call these kernels universal.",
            "Another observation, which is immediately if.",
            "X is not finite.",
            "Then polynomial kernels are in general not.",
            "Universal Becausw the reproducing kernel hyperspaces simply consists of polynomials up to a certain degree.",
            "OK."
        ],
        [
            "So one way to view support vector machines is to interpret them as modified empirical risk minimizers.",
            "And let me explain that in more detail now.",
            "So for classification.",
            "One usually considers the so called hinge loss function, which I will denote by small L, which is defined by.",
            "The maximum of 0 and 1 -- T and this will be serve as a surrogate for the.",
            "Initial classification loss, which leads to NP hard problems.",
            "So the idea of using these?",
            "His essentials is mainly due to computational aspects.",
            "Anne.",
            "Then support vector machines.",
            "Try to find a minimizer of this term here.",
            "So here the second one is an empirical hinge loss.",
            "Risk of a function F and we add a regularization term where Lambda is a free parameter called the regularization parameter.",
            "Age is a fixed reproducing kernel Hilbert space, and here again for algorithmic reasons we take the squared norm of F. If we have found a solution of this optimization problem, say FT or FTBT Lambda, then this will be the decision function of the corresponding SVM.",
            "And I'm not completely sure whether it is legible, but this is rare here, and the reason for that is that in general.",
            "Practical applications use the black and red version, whereas in theoretical considerations one usually only considers the black version of that.",
            "But this is just a minor technical issue.",
            "OK. We've also seen that in practice one usually do not solve this problem directly, but instead considers the dual problem, which is a quadratic convex problem, and.",
            "Under certain circumstances, this can actually be solved quite efficiently, but I will skip these issues here."
        ],
        [
            "Instead, I will present some results.",
            "Some theoretical results on the learning ability of support vector machines.",
            "So first of all, the universally consistent if.",
            "The reproducing kernel Hilbert space is universal and the regularization parameter Lambda.",
            "Tends to zero with the sample size N going to Infinity, but not too fast.",
            "And here we have a black condition without which describes the situation without offset B and this black and red with offset.",
            "So there's a small gap between these two.",
            "There are also some rates known the first one.",
            "Which.",
            "Is quite interesting actually.",
            "If we have a noise for distribution at the classes X 1 -- 1 and X1 have strictly positive distance, so a very nice situation.",
            "No, noise strictly separated classes.",
            "Then SVM's can learn with right up to 1 / N and actually if one considers the proof, the resulting rate is much faster, but I just have chosen this one to compare it better with other rates.",
            "Another result, I'm not sure whether this is up to date.",
            "I think it's not, so.",
            "I apologize if this is wrongly cited.",
            "If this data is contained in some subspace, and the marginal distribution has bounded density with respect to the lebec measure, then an SVM using a Gaussian kernel with a fixed Sigma can learn with the logo rhythmic rate.",
            "And the reason for that is that the approximation error for fixed Sigma is quite better than cost in reproducing kernel Hilbert spaces, at least if one considers the least square loss function.",
            "So there's a small gap between the least square approximation error and the hinge loss.",
            "I'm not sure whether this gap has been filled yet, but it suggests at least that with a fixed Sigma and under this assumptions here, the right won't be substantially faster than this one.",
            "So I think there is another result more recently, but I haven't found it so fast.",
            "So anyway, I I like to say the conclusion the existing rates are rather unsatisfactory.",
            "Either becausw the assumptions are to strong order rates.",
            "Are too slow or something in between.",
            "And one of the reasons why this is the case is that the approximation error for support vector machines haven't been considered in detail yet, and.",
            "This will be done now."
        ],
        [
            "In order to present a new assumption on distributions P which allow us to deal with the approximation error, we first have to make some small notations.",
            "For simplicity, we assume here that the setter.",
            "Where are the ATAR attends the critical level?",
            "1/2 is empty.",
            "It can also be a set of measure 0, but then the resulting terms are slightly more complicated.",
            "So for educational reasons there's just assumed this one, and then we consider the quantity tile X for X.",
            "Element of the input space and this quantity measures the distance of X to the other class.",
            "So if X is in the minus one class, it measures the distance to the one class and vice versa.",
            "And the distance here is simply the distance with respect to the Euclidean norm by that doesn't really matter.",
            "So if we want to draw a picture, here is the minus one class.",
            "Here is the decision boundary.",
            "Hopefully it exists.",
            "Here's the one class and tough access.",
            "The distance of the point X to the decision boundary.",
            "OK, the next slide takes a while since there are some complicated pictures."
        ],
        [
            "Here we go.",
            "Um?",
            "So.",
            "I now like to present conditional distributions which somehow describe the behavior of the distribution in the neighborhood of the decision boundary.",
            "More precisely, if we say that P has a geometric noise exponent Alpha where Alpha is between zero and Infinity.",
            "If this integral here is finite.",
            "Let's see what that means.",
            "So if X goes to the decision boundary, then Tao of X goes to 0.",
            "And therefore the first term here goes to Infinity.",
            "In other words, if we want to ensure that the overall integral is finite.",
            "This weighted measure here.",
            "Must not be supported highly in the area.",
            "In the neighborhood of the decision boundary.",
            "It somehow has to vanish.",
            "How can we think about that?",
            "I consider 2 extreme cases.",
            "The first one is that.",
            "The Marshall Distribution PX is concentrated around the decision boundary.",
            "Think of a density which vanishes at the decision boundary and then what we see here is that it illustrates that here we observe a lot of samples.",
            "If we are away from the decision boundary.",
            "But the closer we get to the decision boundary, the less samples we observe.",
            "And.",
            "Vice versa in this direction.",
            "So this means this distribution is slowly concentrated.",
            "The other possibility is that this here is low.",
            "This ventures around the decision boundary and that means that data must be close to the critical level.",
            "1/2 in the neighborhood of the decision boundary.",
            "That is.",
            "I'm not completely sure whether it looks nice, so hopefully you recognize that this should be Violet, so we observe more and more contradicting samples around the decision boundary, but less contradicting samples.",
            "If we are away from the decision boundary.",
            "And the general situation is of course a mixture of these two extremely cases, so we may either observe less samples or more contradicting ones.",
            "Or something in between.",
            "Anne.",
            "I also like to mention that here in the definition of the dimension of the input space is.",
            "Put here so the larger the dimension is.",
            "Strong of the assumption on this second party, it has to be.",
            "But I'm pretty sure that considering the previous talk that if your data are actually concentrated close to a manifold, then the influence, then the D here itself is can be replaced by a smaller value.",
            "But I haven't considered that in detail.",
            "But since Gossen reproducing kernel Hilbert spaces are actually kind of local rule, is the kernel varize, then yes.",
            "You could reason to believe that."
        ],
        [
            "OK, here's another sufficient condition for ensuring this geometric.",
            "Margin geometric noise condition if P satisfies super costs, noise conditions.",
            "So the amount of noise is controlled.",
            "And in addition this editor behaves.",
            "Nice and neighborhood at the decision boundary in the way that.",
            "This term here can be bounded from above by constant C times the distance to the decision boundary to power gamma or gamma is bigger than 0.",
            "Then we can show that P has actually geometric noise exponent Alpha for all Alpha smaller than this fraction here.",
            "So what does that mean in this specific case where C is equal to 1 and gamma is equal to 1?",
            "Imagine a 1 dimensional situation where the positive class is from zero to Infinity.",
            "The negative classes from zero to minus Infinity.",
            "And then the right hand side here becomes this diagonal and what we see here is that two item minus one is non negative in this class.",
            "So this curve actually is in between the diagonal and this axis here and vice versa.",
            "But we also see it doesn't matter how these two Atom minus one behaves away from the from the decision boundary, it can be non continuous, it can be non differentiable but it has to be.",
            "In between these two lines in the neighborhood of the decision boundary.",
            "So this is the kind of local Holder assumption."
        ],
        [
            "OK, so this is the main result.",
            "It looks a bit complicated, so let's focus on the main ideas.",
            "Let's assume that we have a distribution P which both satisfies super costs.",
            "Noise assumption for Q between zero and Infinity.",
            "And has geometric noise exponent, so we have control over both the amount of noise and the location of the noise.",
            "And there's assume that we know these two exponents.",
            "This is as a later explained, not necessary.",
            "If we know these two exponents and define Lambda N. In this way, on this way, depending on the size of Alpha.",
            "And the Sigma and which will be the Gaussian kernels in this way?",
            "Then the result states that a support vector machine using this regularization parameter and this Sigma N satisfies with high probability that the classification risk can be bounded from above by the base classification risk.",
            "Time plus.",
            "This polynomial term in N. So what we see here is if Alpha is smaller than this term, this fraction here is rather small and therefore the resulting rates.",
            "Are not that fast, but if we consider the other case where Arthur is bigger than this term here.",
            "We obtain a rather complicated term.",
            "Which of is.",
            "Bigger than 1 /, 2 bigger than 1/2.",
            "Let's see two specific cases here.",
            "If Q goes to Infinity, that is, we have a strong assumption on the amount of noise.",
            "Then this fraction here goes to 2A over 2A plus three.",
            "That is, this is faster.",
            "This is larger than 1/2, so the rates are faster than 1 / sqrt N whenever Alpha is larger than.",
            "Three half.",
            "So under a strong assumption on the amount of noise and a weak assumption on the location of the noise, we can learn with fast rates.",
            "Provided, but we know these things.",
            "The other extreme case, if we make a strong assumption on the location of the noise, Alpha goes to Infinity.",
            "Then this fraction tends to Q + 1 / 2 + 2, and this is exactly the fraction super curve obtained.",
            "For his empirical risk minimization method.",
            "Without approximation error and in certain sense this extreme case Alpha equals Infinity exactly describes the no approximation error for support vector machines model or certain technical issues.",
            "So.",
            "They can learn with this fast rate.",
            "Under this assumption and in general.",
            "The situation is more complicated.",
            "OK, what's the time now?",
            "41 after three OK.",
            "So the rest of the talk will be a bit more technical."
        ],
        [
            "Now I like to give you at least an overview of the structure of the proof.",
            "I can't go into.",
            "Too many details, since the proof itself is rather long, but at least I would like to give you an idea.",
            "What happens in the proof and why?",
            "The proof is rather it's quite different from a standard analysis.",
            "The overall idea of the proof is to interpret support vector machines as an empirical risk minimization scheme.",
            "But if one remembers the objective function, apparently it's not to the hinge loss, and it's not with respect to the classification loss, so we have to find a new loss function such that it is an empirical risk minimization scheme with respect to this loss function.",
            "To this end, let us make some notations which we will we will use.",
            "The small error was the hinge loss and the associated risk.",
            "The other risk is simply defined by averaging over all possible hinge losses for a function F. And the averaging process is with respect to P. The smallest possible risk is denoted by RLP, and here the infamous runs of all functions.",
            "Measurable functions from X2R.",
            "We also define a new loss function, capital L, which only works on elements F of a reproducing kernel Hilbert space H. And this loss function.",
            "At a regularization term which comes from the regularization term of the support vector machines plus the hinge loss of F. So if we define the associated capital at risk by the obvious way.",
            "And the empirical version of that which is here.",
            "Sample set of size N. This way we see that the empirical support vector machine is nothing as as the minimum of this empirical capital risk.",
            "Yes, since this exactly resembles the target, the objective function of the optimization problem of the support vector machine.",
            "What we also need is so-called infinite sample support vector machine.",
            "Here we replace the empirical risk by the true underlying risk and the resulting.",
            "Solution which exists and is uniquely.",
            "Determined is denoted by FP Lambda.",
            "Finally, instead of considering the approximation error, it turns out to be more handy to consider this so called approximation error function, which is denoted by a of Lambda and consists of this infinite sample SVM objective function, minus the.",
            "Smallest possible.",
            "A risk, So what we see here is, for example, if we have a Gaussian reproducing kernel Hilbert space and Lambda goes to goes to 0, then a of Lambda goes to 0.",
            "And the small, the faster this convergence holds, the better the reproducing kernel Hilbert space can approximate the distribution P in this very specific sense.",
            "OK, and then the idea of the proof is based on a basic decomposition, so we consider here the excess classification risk of our VM's.",
            "Solution of our empirical SVM solution.",
            "This can be bounded from above by the excess hinge risk of the same.",
            "Classifier this is due to Jean.",
            "Then we make some algebra and it turns out that this access risk can be estimated from above by the so called estimation error, which I will consider first plus the approximation error function.",
            "It's going to die here.",
            "Um?",
            "So I call this the estimation error since it compares the capital at risk of the empirically found solution with a true solution.",
            "And."
        ],
        [
            "And the rest of the idea of the proof.",
            "I will mainly consider the estimation error part.",
            "I probably won't have time to consider the approximation error.",
            "OK, let's summarize observations.",
            "Support vector machines are empirical risk.",
            "Minimizing schemes over the reproducing kernel Hilbert space H. But rather trivial estimate also tells us that the normal solutions are never larger than one over square root of Lambda and therefore instead of considering the entire space H, we can actually minimize over the smaller scaled unit ball BH or the reproducing kernel Hilbert space.",
            "And what we see here is the smaller Lambda becomes the large.",
            "This set here becomes and therefore we should expect.",
            "Worth estimation error, whereas the better approximation error.",
            "Anne.",
            "Then we define and function class script G which consists of air composed F. Here is this regularised loss function define of the previous slide minus.",
            "The best one and F runs over a unit ball is scaled by a constant gamma and gamma is between one and one over square root of Lambda.",
            "And.",
            "The trivial estimates suggest that gamma should actually be equal to this term here, but we will see later on that in many situations we may actually find a better value for gamma.",
            "A substantially better one.",
            "OK, so the connection of this function Class G to the estimation error is rather simple.",
            "If we have an FT. Lambda which is.",
            "In this scaled ball and we consider G or this form here, then the expectation of G equals the estimation error.",
            "And for later users.",
            "We should, we should remember that this means.",
            "That the smaller this expectation is.",
            "The better FT Lambda or the closer FT. Lambda is to the real solution FP Lambda in terms of these risks.",
            "OK. Then"
        ],
        [
            "Having identified support vector machines as empirical risk minimizers, we will only use some standard methods nowadays, standard methods from.",
            "Empirical.",
            "Process theory we consider so-called local Rademacher averages, which in a certain sense complexity measures of the function class G. Localized by a parameter epsilon.",
            "And yeah, I do not want to go here to detail and these harder Maja averages can then be used together with telegrams.",
            "Concentration inequality proved in 1994.",
            "To obtain this kind of result.",
            "Let's focus on the main issues here, so assume that the variance of the elements G in script she can be bounded by the expectation in this way here.",
            "So we have a fixed constant here, a fixed exponent Alpha here and fixed Delta here, and this then holds for every G in script she this is not.",
            "Naturally satisfied, but let us assume that we have.",
            "This situation.",
            "Then whenever we should listen epsilon which is bigger than this quite large term on the right, we obtain that the capital L risk of the SVM solution.",
            "Of the empirical SVM solution is smaller than or equal to the.",
            "Capital L risk of the true SVM solution plus this estimation error epsilon.",
            "So what we see here is we have to bond the Radama averages of the local kotimaa verges find a fixed point here.",
            "And.",
            "Determine this kind of inequality and finally have to consider this, here."
        ],
        [
            "The local Rademacher averages can be bound for kernel methods in various ways.",
            "We decided here to consider simple techniques based on covering numbers and without going into too much details.",
            "It turns out that if we consider a Gaussian reproducing kernel Hilbert space on a compact subset X.",
            "Then the local Radama averages can be bounded from above by this term here, and I would like to say that this I this a here mainly depends on the used kernels.",
            "So that depends on Sigma.",
            "Again, the quantity depends on this camera which we have to specify, and it also depends on a free parameter P which we have later tissues in our analysis.",
            "The variance bound for support vector machines using hinge losses is.",
            "Anne.",
            "Mainly determined by the civic of noise exponent.",
            "More precisely, if this is satisfied for some Q.",
            "Then the variance of the elements in script.",
            "She can be bounded from above by a universal constant.",
            "I think it's 32.",
            "Times the scammer again and the exponent depends on Q.",
            "Here's the expectation, again with the exponent depending on Q and here what is interesting is the 2nd additive term here depends on the approximation error function.",
            "So this is the first part where we see that in order to find a good estimation error, we actually have to know the approximation error function first.",
            "And we will observe that in the next slide again.",
            "OK, and if we plug everything now together, we have turned the rather complicated version of this epsilon here and what we see here is that this size of the underlying ball of script G has a substantial influence of the behavior of this.",
            "Right hand side.",
            "So what we actually now have to find is a good value for gamma.",
            "And."
        ],
        [
            "This can be done in the following way.",
            "Um so.",
            "I'd like to begin with a simple observation.",
            "I already mentioned that that for the empirical solutions we have somehow trivial bounds on the norm of the solutions, namely, this term here can be bounded from above.",
            "By this plus the empirical error risk, since this is always non negative.",
            "And since this is the minimizer of this objective function, we compare that with zero.",
            "This is always not smaller, and it turns out immediately that RLT of zero is 1.",
            "In other words, the norm of FT Lambda can be bounded from above by one over square root of Lambda.",
            "What?",
            "This is not optimal.",
            "Namely.",
            "If we consider an infinite sample support vector machine.",
            "Then we can make almost the same game but with a small but substantial difference.",
            "Again, we estimate this here by this and now.",
            "Recall that our LP is the smallest possible error risk and therefore this term here is always non negative.",
            "And therefore we can make this estimate here.",
            "But then this entire function here is nothing else, as the approximation error function.",
            "Just by definition.",
            "So that means that our FP Lambda is always bounded from above by one over square root of Lambda times the square root of the approximation error function.",
            "And in general, if we can learn this approximation error function goes to zero and therefore this term here goes faster, doesn't increase that fast as this one.",
            "And this will be used in the analysis.",
            "Unfortunately, this analysis only works for the infinite sample support vector machine.",
            "But as I will describe.",
            "Now is that it approx that this this relation here approximately also holds for the empirically found solutions FT Lambda.",
            "How can this be shown?",
            "Well, the idea is to.",
            "Kind of Bootstrap method, maybe?",
            "So let us assume that we.",
            "Do not know anything about the problem so that we begin with a trivial estimate gamma N. Equals one over square root of Lambda N. Yeah, then we can use our concentration inequality and this gives that this regularization term.",
            "Is bounded from above by the regularization term plus the excess error risk.",
            "Since this is non negative.",
            "And now we use the concentration result, which says that.",
            "This overall term, which is the capital L risk.",
            "Can be bounded from above by this term here.",
            "Plus there the estimation error epsilon N. Yeah, and.",
            "The first three terms are actually the approximation error function, Lambda N. So what we see here is the left hand side looks good on the right hand side where we do not know, but it turns out that unfortunately the epsilon N dominates the approximation error function.",
            "But it still goes to zero polynomially.",
            "What does that mean?",
            "Well, it means that we have a slightly better estimate of this term than we previously had.",
            "With high probability.",
            "OK, we practiced in again.",
            "And obtain another slightly better estimate of this regularization term.",
            "Iterate the procedure and with high probability we almost reached this result here.",
            "OK."
        ],
        [
            "So.",
            "If we compare this analysis with them.",
            "The more standard way we see that they are built, both based on a rather standard decomposition of the excess risk in an estimation error and approximation error.",
            "But the standard analysis usually considers both error terms independently from each other.",
            "Whereas in our analysis analysis it turned out that the approximation error or the approximation error function.",
            "Has influence on the estimation or in two terms in two ways.",
            "First, it is.",
            "It occurs in the additive term of the variance bounds.",
            "And second, it also approximately determines the best value for gamma.",
            "Which has a substantial influence on the rates we obtained.",
            "So instead of considering these two independently in this very specific situation, it is.",
            "Wise to 1st consider approximation.",
            "Errors and then use the estimation error part.",
            "Otherwise the resulting rates are worse.",
            "OK, so I probably won't have time to consider the approximation error.",
            "Just a very last slide considering open questions."
        ],
        [
            "So.",
            "The first open question or open problem is.",
            "According to Oracle Inequality's, so this technique of finding a good value for gamma only works if we're preore fix the sequence Lambda N and Sigma N. If we do not know that our priority, the technique doesn't work, but we now know how to resolve this problem and hopefully the results are finished before the NIPS deadline.",
            "Anne.",
            "The Oracle inequalities are important 'cause they will give us away for parameter selection, so remember that the rates we obtained here.",
            "Require that we know these exponents Alpha and Q.",
            "Although we also obtain rates if we simply use other sequences of Lambda and Sigma, and the resulting rates are then worse.",
            "But we are actually interested in rates.",
            "Which work?",
            "When we do not know the exponents, but.",
            "Find them almost adaptive adaptivity.",
            "So.",
            "And chances are not bad that the parameter selection problem will be solved by these or Oracle inequalities.",
            "And next open question is whether the resulting rates are optimal or there are optimal in the sense that our methods won't give better rates.",
            "But this is of course not the question of interest, so it is completely open to us whether the results we obtained are.",
            "Optimal at least for interesting classes of distributions.",
            "OK, I think this is the last question, is not so important here good.",
            "Thank you.",
            "None of your race.",
            "She can't.",
            "This big constant depends on how your probability measure P. Where is it awesome presentation that it depends on?",
            "So it depends.",
            "Since appearing in both Super Coffee North Assumption and the margin and the and the geometric noise assumption, but perhaps also on your.",
            "Fication it.",
            "Also depends on Constance appearing and covering numbers, which are usually extremely hard to find.",
            "And it depends on a universal constant in a theorem, proof by Telegram.",
            "And these are the dependencies.",
            "There's no other dependency.",
            "And it depends on on the epsilon in this case.",
            "Sorry.",
            "It also depends on the epsilon, how close the rates are to the one we want actually want to describe.",
            "But it doesn't really depend on.",
            "I mean, somehow it will depend on the on the algorithm, but in the analysis it is clear that it mainly depends on these.",
            "Conditions describing the distribution and the epsilon.",
            "In general, you can compute them, although I think it's not a wise idea to do so.",
            "4:30 OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For establishing learning rates to the base risk, and this is mainly a joint work with Clint's cover from my Institute.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We start with an overview.",
                    "label": 0
                },
                {
                    "sent": "I will briefly recall what we want.",
                    "label": 0
                },
                {
                    "sent": "Classification is about what consistency is.",
                    "label": 0
                },
                {
                    "sent": "We've just seen that in the previous talk.",
                    "label": 0
                },
                {
                    "sent": "I will discuss the question operates and I also will present a recently proposed margin condition.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines are then recalled briefly and then in order to establish rates.",
                    "label": 0
                },
                {
                    "sent": "An general theorem which I will present later, states that we have to restrict the class of distributions and I will present a new assumption on class of distributions which are in some sense in geometric terms.",
                    "label": 1
                },
                {
                    "sent": "This condition together this with this condition will then leads us to rates for support vector machines, and I will present the results and discuss some open questions.",
                    "label": 1
                },
                {
                    "sent": "And finally, depending on the remaining time, I will sketch some ideas of the proof.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is?",
                    "label": 0
                },
                {
                    "sent": "Classification, so the major aim of this part is to fix some rotations.",
                    "label": 0
                },
                {
                    "sent": "We have an unknown probability measure P on X * Y. X is our input space.",
                    "label": 1
                },
                {
                    "sent": "Often a compact set subset of Rd and Capital Y is the set of labels minus one and one, so we are actually only considering binary classification.",
                    "label": 1
                },
                {
                    "sent": "And we also have training set data set consisting of pairs xiy I which are assumed to be IID according to P. And the Y eyes indicate whether the point XI belongs to the minus one class.",
                    "label": 0
                },
                {
                    "sent": "Or the one class?",
                    "label": 0
                },
                {
                    "sent": "And then the aim of classification is to construct function FT depending on the training set T such that the classification risk which I denote by RP of FT is small, so the classification is simply measures the probability of the set of points where we do not predict the label correctly.",
                    "label": 0
                },
                {
                    "sent": "A classifier then just for completeness, is an algorithm that constructs in FT to every given T and the.",
                    "label": 1
                },
                {
                    "sent": "Formulation of this risk somewhere indicates that we are actually interested in classifiers which produce risk as small as possible, so the next.",
                    "label": 0
                },
                {
                    "sent": "Quantity I need is the so called base risk which is the infima of all.",
                    "label": 0
                },
                {
                    "sent": "Possible risk where F runs over measurable functions from X to Y.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We've seen that in the previous talk, if we knew the distribution P, we can actually find a function F which realises this minimal risk.",
                    "label": 0
                },
                {
                    "sent": "Namely, if we consider this conditional class probability.",
                    "label": 0
                },
                {
                    "sent": "Denoted by Etta, often called the supervisor, then FP function FP from X to Y, realises the base risk if.",
                    "label": 0
                },
                {
                    "sent": "On the set of points where the supervisor tends to produce positive labels.",
                    "label": 0
                },
                {
                    "sent": "If this function is strictly positive on this set and negative.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, on the other set.",
                    "label": 0
                },
                {
                    "sent": "So in this sense, the.",
                    "label": 0
                },
                {
                    "sent": "Based decision function FP also recognizes the two classes X, one and X -- 1.",
                    "label": 0
                },
                {
                    "sent": "Finally, we say that P is noise free if this supervisor letter is minus is.",
                    "label": 0
                },
                {
                    "sent": "If it is 01 valued, that means that the labeling process is deterministic.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I've said that we are interested in algorithms which produce risk as small as possible, and one way to describe that in a mathematical sense is the notion of universal consistency.",
                    "label": 0
                },
                {
                    "sent": "We say that a classifier is universally consistent if the average classification error tends to the base risk.",
                    "label": 1
                },
                {
                    "sent": "Provided that the sample size goes to Infinity and this must hold for every probability measure P becausw, we do not know our distribution PR priore and therefore in order to ensure that the algorithm learns entotic Lee, we have to show that for every distribution P. The good thing?",
                    "label": 0
                },
                {
                    "sent": "About this notion here is that there are actually many algorithms known which are universally consistent.",
                    "label": 0
                },
                {
                    "sent": "The first one is the K nearest neighbor rule, for which it was shown to be universally consistent.",
                    "label": 0
                },
                {
                    "sent": "Histogram rules.",
                    "label": 0
                },
                {
                    "sent": "Certain neural networks later on with the support vector machines, two are also universally consistent, so in an asmt artikkel sense they these algorithms can learn.",
                    "label": 0
                },
                {
                    "sent": "However, this is of course not the only question we may have, since having here convergence doesn't tell us anything about the speed of convergence.",
                    "label": 0
                },
                {
                    "sent": "So if we are interested in a certain accuracy, say epsilon, we may ask what is the required number of samples we need to ensure this.",
                    "label": 0
                },
                {
                    "sent": "Accuracy.",
                    "label": 0
                },
                {
                    "sent": "Up to epsilon and this leads to the question of rates.",
                    "label": 0
                },
                {
                    "sent": "So can we guarantee conversion rates rate in one that holds for all distributions?",
                    "label": 1
                },
                {
                    "sent": "Again for all, since we do not know PR priore.",
                    "label": 0
                },
                {
                    "sent": "And unfortunately the answer is no, which is quite old result by devoir.",
                    "label": 0
                },
                {
                    "sent": "Which roughly speaking states that whatever classifier we have there exists distributions on X, provided that X is not a finite set such that we cannot guarantee a uniform convergence here.",
                    "label": 0
                },
                {
                    "sent": "But that means that certain algorithms cannot learn certain distributions were and other distributions can be learned well by this.",
                    "label": 0
                },
                {
                    "sent": "An algorithm mathematical.",
                    "label": 0
                },
                {
                    "sent": "Way to resolve this problem is probably the most natural one is to consider just big subclasses of distributions P. And then it turns out that there are some sufficient conditions.",
                    "label": 0
                },
                {
                    "sent": "I will present on the next slide in some insufficient conditions.",
                    "label": 0
                },
                {
                    "sent": "For distributions which are there in sure.",
                    "label": 0
                },
                {
                    "sent": "Uniform rates are not.",
                    "label": 0
                },
                {
                    "sent": "But just to give you an idea what kind of.",
                    "label": 0
                },
                {
                    "sent": "Bad conditions one can think of which do not give us the desired results, are for example, if.",
                    "label": 0
                },
                {
                    "sent": "The marginal distribution has a specified density with respect to the lebec measure, so we fix a density we know everything about the marginal distribution and Additionally assume that P is noise free.",
                    "label": 1
                },
                {
                    "sent": "No random process in the labeling.",
                    "label": 0
                },
                {
                    "sent": "Then we still cannot guarantee rights here.",
                    "label": 0
                },
                {
                    "sent": "Quite similar if we have this assumptions on the marginal distribution and assume that the supervisor is very smooth, SESI Infinity.",
                    "label": 1
                },
                {
                    "sent": "We still cannot guarantee rights here.",
                    "label": 0
                },
                {
                    "sent": "And a more geonet geometric condition.",
                    "label": 0
                },
                {
                    "sent": "If P is noise free and one of the classes, say X one is compact and convex.",
                    "label": 0
                },
                {
                    "sent": "We still cannot guarantee rates here, which gives us an idea why finding reasonable, interesting subclasses of P may be a hard problem.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                },
                {
                    "sent": "There are also known results.",
                    "label": 0
                },
                {
                    "sent": "Which give us.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Certain rates and I do not make any attempt here to be complete.",
                    "label": 1
                },
                {
                    "sent": "They're just chosen in order to motivate the next results.",
                    "label": 0
                },
                {
                    "sent": "So for example, if the supervisor satisfies certain smoothness assumptions, which include bounds on the derivatives like Sobolev conditions.",
                    "label": 1
                },
                {
                    "sent": "Then certain plugin rules which are usually based on the square empirical risk minimizers lead to rates or perform end to the power minus Alpha, where Alpha is an exponent between zero and a half.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We cannot be faster here.",
                    "label": 0
                },
                {
                    "sent": "Structural risk minimization.",
                    "label": 1
                },
                {
                    "sent": "We've seen that in the previous talk over hypothesis sets, FI will find it fizzy dimension can learn with right up to 1 / sqrt N if and this is crucial.",
                    "label": 1
                },
                {
                    "sent": "Base function is contained in one of the FIS.",
                    "label": 0
                },
                {
                    "sent": "And the reason why this is crucial is that.",
                    "label": 0
                },
                {
                    "sent": "In this case, we simply cannot make.",
                    "label": 0
                },
                {
                    "sent": "We simply do not have to consider the approximation error of the problem.",
                    "label": 0
                },
                {
                    "sent": "So everything what we just heard about dimensionality is not a matter of problems here.",
                    "label": 0
                },
                {
                    "sent": "Empirical risk minimization, which is very which is some of the basic step of the structural risk minimization, can actually learn faster if we assume P to be noise free, and then they're resulting rate can be up to 1 / N. So what we see here is first that there is a.",
                    "label": 0
                },
                {
                    "sent": "Big gap between the arbitrary case where we do not make any assumptions on the noise and the noise free case, and we will see later how we can actually find rates in between.",
                    "label": 0
                },
                {
                    "sent": "If we make certain.",
                    "label": 0
                },
                {
                    "sent": "Conditions on the noise which measure the amount of noise that will be done on the next slide.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 1
                },
                {
                    "sent": "Other interesting things here, which I think.",
                    "label": 0
                },
                {
                    "sent": "Important for practices, for example, how realistic are smoothness assumptions?",
                    "label": 0
                },
                {
                    "sent": "Oneta?",
                    "label": 0
                },
                {
                    "sent": "Well, for regression these smoothness assumptions are widely accepted in many cases.",
                    "label": 0
                },
                {
                    "sent": "If we have classification problems, it's rather unclear.",
                    "label": 0
                },
                {
                    "sent": "Since the data itself is often determined by labeling of human beings, and I personally do not know whether they behave smoothly in a mathematical sense.",
                    "label": 1
                },
                {
                    "sent": "So whether this is satisfied or not, one has to.",
                    "label": 0
                },
                {
                    "sent": "Answer that for you for himself, but I personally think that might be a problem.",
                    "label": 0
                },
                {
                    "sent": "What is also quite known is that it is usually very hard to find hypothesis sets which contain a base function.",
                    "label": 1
                },
                {
                    "sent": "So you almost never be in the lucky situation where this is satisfied.",
                    "label": 0
                },
                {
                    "sent": "Real world distributions are always almost always noisy, so this noise rate is also rather unrealistic.",
                    "label": 0
                },
                {
                    "sent": "And then the only right you know is this rather poor rate.",
                    "label": 0
                },
                {
                    "sent": "And in addition, both methods here have the problem that they are usually computationally not feasible, since for most interesting optimization problems the solution is NP hard to find.",
                    "label": 0
                },
                {
                    "sent": "So from a theoretical point of view, there's still the goal to find efficient learning algorithms which also work well in practice, which can learn up to a fast rate of 1 / N under realistic conditions on P. And here realistic is again questionable what that means, but hopefully I can convince you that the results I will present later abit more realistic at least.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next thing I want to mention is how we can get out of this black white scheme of no noise or just arbitrary noise where the rates are either 1 / N or 1 / sqrt N. And the solution here, which was recently proposed by super cost, is.",
                    "label": 0
                },
                {
                    "sent": "Based on the idea that we have to measure the amount of noise.",
                    "label": 1
                },
                {
                    "sent": "And before I present the definition, I like to make an observation.",
                    "label": 0
                },
                {
                    "sent": "So this after here, which was defined as this regular conditional probability describes somehow the amount of noise.",
                    "label": 0
                },
                {
                    "sent": "More precisely, if this two Atom minus one is close to 1.",
                    "label": 1
                },
                {
                    "sent": "Then Atom must be either close to 0 or close to 1.",
                    "label": 0
                },
                {
                    "sent": "And that is the supervisor.",
                    "label": 0
                },
                {
                    "sent": "Data only produces a small amount of wrong, wrongly labeled samples in that specific point X.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Whereas if this quantity is close to 0, then Atom must be close to 1/2 and therefore we may observe we will observe a lot of contradicting samples in that specific point X if we only sample at this point.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if this is close to zero we have a high noise in the labeling process at the point X and vice versa.",
                    "label": 0
                },
                {
                    "sent": "And super cause noise condition or exponent now measures the size of the set where this happens.",
                    "label": 0
                },
                {
                    "sent": "More precisely, a distribution P satisfies the margin condition for some Q between zero and Infinity.",
                    "label": 1
                },
                {
                    "sent": "If the probability of the set of points where this term here is smaller than or equal to.",
                    "label": 0
                },
                {
                    "sent": "Then T can be estimated by this right hand side here, where C is a fixed constant and Q is this exponent.",
                    "label": 0
                },
                {
                    "sent": "What we see here is that if she goes to zero and we only interested in small T, then the right hand side goes to zero and the bigger Q is, the faster the convergence is.",
                    "label": 0
                },
                {
                    "sent": "And therefore in this sense.",
                    "label": 0
                },
                {
                    "sent": "The set of noise is smaller.",
                    "label": 1
                },
                {
                    "sent": "This is the interpretation of that.",
                    "label": 0
                },
                {
                    "sent": "In one of the extreme cases, Q = 0, we see that this is automatically satisfied for all distributions, so this is not a assumption at all, whereas in the other extreme case, Q equals Infinity, we see because of this assumption here, that Atom must be actually bounded away from the critical level at half.",
                    "label": 1
                },
                {
                    "sent": "And every Q in between describes the situation in between.",
                    "label": 0
                },
                {
                    "sent": "OK, so with this.",
                    "label": 0
                },
                {
                    "sent": "Noise exponent supercop then showed that empirical risk minimization over our hypothesis class F which contains again the base function in order to avoid approximation in our considerations, can learn with this right here where Q.",
                    "label": 0
                },
                {
                    "sent": "Is the margin condition and P measures the complexity of this size F?",
                    "label": 0
                },
                {
                    "sent": "It's not a Videmment 50 dimensional type of measure.",
                    "label": 0
                },
                {
                    "sent": "It's actually based on a rather complicated covering number.",
                    "label": 0
                },
                {
                    "sent": "But without going here into too much details, let's consider first simplicity.",
                    "label": 0
                },
                {
                    "sent": "The case where P is very close to zero, that is, the complexity is very small.",
                    "label": 0
                },
                {
                    "sent": "Then this fraction here becomes more or less Q + 1 / Q + 2.",
                    "label": 0
                },
                {
                    "sent": "What does that mean if Q = 0, we have the general rate of 1 / sqrt N, whereas if Q equals Infinity, this is the best case or rate.",
                    "label": 0
                },
                {
                    "sent": "Essentially is 1 / N. And every Q in between describes a case in between.",
                    "label": 0
                },
                {
                    "sent": "So the picture here is if we make.",
                    "label": 0
                },
                {
                    "sent": "Quantitative statement about the about the amount of noise we actually obtain.",
                    "label": 0
                },
                {
                    "sent": "Rates which are in between the two cases of no noise and complete noise or no assumptions on the noise.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's no turn to support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Most of the things on this slide are already explained in the morning, so let's make it brief.",
                    "label": 0
                },
                {
                    "sent": "The symmetric function K on X * X two Rs.",
                    "label": 1
                },
                {
                    "sent": "Quarter kernel.",
                    "label": 0
                },
                {
                    "sent": "If all these metrics are positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent to the existence of a feature space H. And a feature map 5 from X to H such that K realises the inner product of five with itself.",
                    "label": 1
                },
                {
                    "sent": "And we also already have seen in the morning that this Hilbert spaces in general not unique.",
                    "label": 0
                },
                {
                    "sent": "However, there is in some sense a unique or Canonical feature space, so called reproducing kernel Hilbert space, which up to isometry, is the smallest possible feature space, and one way to construct this again apply symmetry is to consider these linear combinations of kernel evaluations equipped that with the inner product.",
                    "label": 0
                },
                {
                    "sent": "Define in this way and then consider the completion and it turns out that this is up to isometry the reproducing kernel Hilbert space.",
                    "label": 1
                },
                {
                    "sent": "We mainly.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interested in one or two kernels?",
                    "label": 0
                },
                {
                    "sent": "Since they are the most popular for support vector machines, the polynomial kernels are built from the inner product on Rd plus some positive coefficient A and a positive power M. I would not discuss these kernels in detail, but the Gaussian RBF kernels which are more relevant for practical issues too.",
                    "label": 1
                },
                {
                    "sent": "I will consider in more detail later.",
                    "label": 0
                },
                {
                    "sent": "So how are these builds?",
                    "label": 0
                },
                {
                    "sent": "We consider the exponential function take a free parameter Sigma and then the Euclidean distance of the points X&X prime energy.",
                    "label": 0
                },
                {
                    "sent": "Take the square of that and that's the.",
                    "label": 0
                },
                {
                    "sent": "That's the kernel.",
                    "label": 1
                },
                {
                    "sent": "That's the Gaussian RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "One nice thing about these kernels here is that they are reproducing kernel.",
                    "label": 0
                },
                {
                    "sent": "Hilbert spaces are in a certain sense, which.",
                    "label": 0
                },
                {
                    "sent": "More precisely, if we restrict our considerations on compact subsets of Rd, then the reproducing kernel Hilbert space of these kernels.",
                    "label": 0
                },
                {
                    "sent": "Then in C of X.",
                    "label": 0
                },
                {
                    "sent": "And this will be used for consistency results and the approximation error problem inconsistency results.",
                    "label": 0
                },
                {
                    "sent": "So this space is a nice big space for considering approximation errors in general, and I will call these kernels universal.",
                    "label": 0
                },
                {
                    "sent": "Another observation, which is immediately if.",
                    "label": 0
                },
                {
                    "sent": "X is not finite.",
                    "label": 0
                },
                {
                    "sent": "Then polynomial kernels are in general not.",
                    "label": 0
                },
                {
                    "sent": "Universal Becausw the reproducing kernel hyperspaces simply consists of polynomials up to a certain degree.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one way to view support vector machines is to interpret them as modified empirical risk minimizers.",
                    "label": 1
                },
                {
                    "sent": "And let me explain that in more detail now.",
                    "label": 0
                },
                {
                    "sent": "So for classification.",
                    "label": 0
                },
                {
                    "sent": "One usually considers the so called hinge loss function, which I will denote by small L, which is defined by.",
                    "label": 0
                },
                {
                    "sent": "The maximum of 0 and 1 -- T and this will be serve as a surrogate for the.",
                    "label": 0
                },
                {
                    "sent": "Initial classification loss, which leads to NP hard problems.",
                    "label": 0
                },
                {
                    "sent": "So the idea of using these?",
                    "label": 0
                },
                {
                    "sent": "His essentials is mainly due to computational aspects.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Then support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Try to find a minimizer of this term here.",
                    "label": 1
                },
                {
                    "sent": "So here the second one is an empirical hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Risk of a function F and we add a regularization term where Lambda is a free parameter called the regularization parameter.",
                    "label": 1
                },
                {
                    "sent": "Age is a fixed reproducing kernel Hilbert space, and here again for algorithmic reasons we take the squared norm of F. If we have found a solution of this optimization problem, say FT or FTBT Lambda, then this will be the decision function of the corresponding SVM.",
                    "label": 1
                },
                {
                    "sent": "And I'm not completely sure whether it is legible, but this is rare here, and the reason for that is that in general.",
                    "label": 0
                },
                {
                    "sent": "Practical applications use the black and red version, whereas in theoretical considerations one usually only considers the black version of that.",
                    "label": 0
                },
                {
                    "sent": "But this is just a minor technical issue.",
                    "label": 1
                },
                {
                    "sent": "OK. We've also seen that in practice one usually do not solve this problem directly, but instead considers the dual problem, which is a quadratic convex problem, and.",
                    "label": 0
                },
                {
                    "sent": "Under certain circumstances, this can actually be solved quite efficiently, but I will skip these issues here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instead, I will present some results.",
                    "label": 0
                },
                {
                    "sent": "Some theoretical results on the learning ability of support vector machines.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the universally consistent if.",
                    "label": 0
                },
                {
                    "sent": "The reproducing kernel Hilbert space is universal and the regularization parameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "Tends to zero with the sample size N going to Infinity, but not too fast.",
                    "label": 0
                },
                {
                    "sent": "And here we have a black condition without which describes the situation without offset B and this black and red with offset.",
                    "label": 0
                },
                {
                    "sent": "So there's a small gap between these two.",
                    "label": 0
                },
                {
                    "sent": "There are also some rates known the first one.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Is quite interesting actually.",
                    "label": 0
                },
                {
                    "sent": "If we have a noise for distribution at the classes X 1 -- 1 and X1 have strictly positive distance, so a very nice situation.",
                    "label": 0
                },
                {
                    "sent": "No, noise strictly separated classes.",
                    "label": 0
                },
                {
                    "sent": "Then SVM's can learn with right up to 1 / N and actually if one considers the proof, the resulting rate is much faster, but I just have chosen this one to compare it better with other rates.",
                    "label": 0
                },
                {
                    "sent": "Another result, I'm not sure whether this is up to date.",
                    "label": 0
                },
                {
                    "sent": "I think it's not, so.",
                    "label": 0
                },
                {
                    "sent": "I apologize if this is wrongly cited.",
                    "label": 0
                },
                {
                    "sent": "If this data is contained in some subspace, and the marginal distribution has bounded density with respect to the lebec measure, then an SVM using a Gaussian kernel with a fixed Sigma can learn with the logo rhythmic rate.",
                    "label": 1
                },
                {
                    "sent": "And the reason for that is that the approximation error for fixed Sigma is quite better than cost in reproducing kernel Hilbert spaces, at least if one considers the least square loss function.",
                    "label": 0
                },
                {
                    "sent": "So there's a small gap between the least square approximation error and the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure whether this gap has been filled yet, but it suggests at least that with a fixed Sigma and under this assumptions here, the right won't be substantially faster than this one.",
                    "label": 0
                },
                {
                    "sent": "So I think there is another result more recently, but I haven't found it so fast.",
                    "label": 0
                },
                {
                    "sent": "So anyway, I I like to say the conclusion the existing rates are rather unsatisfactory.",
                    "label": 0
                },
                {
                    "sent": "Either becausw the assumptions are to strong order rates.",
                    "label": 0
                },
                {
                    "sent": "Are too slow or something in between.",
                    "label": 0
                },
                {
                    "sent": "And one of the reasons why this is the case is that the approximation error for support vector machines haven't been considered in detail yet, and.",
                    "label": 0
                },
                {
                    "sent": "This will be done now.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to present a new assumption on distributions P which allow us to deal with the approximation error, we first have to make some small notations.",
                    "label": 0
                },
                {
                    "sent": "For simplicity, we assume here that the setter.",
                    "label": 0
                },
                {
                    "sent": "Where are the ATAR attends the critical level?",
                    "label": 0
                },
                {
                    "sent": "1/2 is empty.",
                    "label": 0
                },
                {
                    "sent": "It can also be a set of measure 0, but then the resulting terms are slightly more complicated.",
                    "label": 0
                },
                {
                    "sent": "So for educational reasons there's just assumed this one, and then we consider the quantity tile X for X.",
                    "label": 0
                },
                {
                    "sent": "Element of the input space and this quantity measures the distance of X to the other class.",
                    "label": 1
                },
                {
                    "sent": "So if X is in the minus one class, it measures the distance to the one class and vice versa.",
                    "label": 0
                },
                {
                    "sent": "And the distance here is simply the distance with respect to the Euclidean norm by that doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "So if we want to draw a picture, here is the minus one class.",
                    "label": 0
                },
                {
                    "sent": "Here is the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "Hopefully it exists.",
                    "label": 0
                },
                {
                    "sent": "Here's the one class and tough access.",
                    "label": 0
                },
                {
                    "sent": "The distance of the point X to the decision boundary.",
                    "label": 1
                },
                {
                    "sent": "OK, the next slide takes a while since there are some complicated pictures.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we go.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I now like to present conditional distributions which somehow describe the behavior of the distribution in the neighborhood of the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "More precisely, if we say that P has a geometric noise exponent Alpha where Alpha is between zero and Infinity.",
                    "label": 1
                },
                {
                    "sent": "If this integral here is finite.",
                    "label": 0
                },
                {
                    "sent": "Let's see what that means.",
                    "label": 0
                },
                {
                    "sent": "So if X goes to the decision boundary, then Tao of X goes to 0.",
                    "label": 1
                },
                {
                    "sent": "And therefore the first term here goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "In other words, if we want to ensure that the overall integral is finite.",
                    "label": 0
                },
                {
                    "sent": "This weighted measure here.",
                    "label": 0
                },
                {
                    "sent": "Must not be supported highly in the area.",
                    "label": 0
                },
                {
                    "sent": "In the neighborhood of the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "It somehow has to vanish.",
                    "label": 0
                },
                {
                    "sent": "How can we think about that?",
                    "label": 0
                },
                {
                    "sent": "I consider 2 extreme cases.",
                    "label": 0
                },
                {
                    "sent": "The first one is that.",
                    "label": 0
                },
                {
                    "sent": "The Marshall Distribution PX is concentrated around the decision boundary.",
                    "label": 1
                },
                {
                    "sent": "Think of a density which vanishes at the decision boundary and then what we see here is that it illustrates that here we observe a lot of samples.",
                    "label": 0
                },
                {
                    "sent": "If we are away from the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "But the closer we get to the decision boundary, the less samples we observe.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Vice versa in this direction.",
                    "label": 0
                },
                {
                    "sent": "So this means this distribution is slowly concentrated.",
                    "label": 1
                },
                {
                    "sent": "The other possibility is that this here is low.",
                    "label": 0
                },
                {
                    "sent": "This ventures around the decision boundary and that means that data must be close to the critical level.",
                    "label": 0
                },
                {
                    "sent": "1/2 in the neighborhood of the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "That is.",
                    "label": 0
                },
                {
                    "sent": "I'm not completely sure whether it looks nice, so hopefully you recognize that this should be Violet, so we observe more and more contradicting samples around the decision boundary, but less contradicting samples.",
                    "label": 0
                },
                {
                    "sent": "If we are away from the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "And the general situation is of course a mixture of these two extremely cases, so we may either observe less samples or more contradicting ones.",
                    "label": 0
                },
                {
                    "sent": "Or something in between.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I also like to mention that here in the definition of the dimension of the input space is.",
                    "label": 0
                },
                {
                    "sent": "Put here so the larger the dimension is.",
                    "label": 0
                },
                {
                    "sent": "Strong of the assumption on this second party, it has to be.",
                    "label": 0
                },
                {
                    "sent": "But I'm pretty sure that considering the previous talk that if your data are actually concentrated close to a manifold, then the influence, then the D here itself is can be replaced by a smaller value.",
                    "label": 0
                },
                {
                    "sent": "But I haven't considered that in detail.",
                    "label": 0
                },
                {
                    "sent": "But since Gossen reproducing kernel Hilbert spaces are actually kind of local rule, is the kernel varize, then yes.",
                    "label": 0
                },
                {
                    "sent": "You could reason to believe that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here's another sufficient condition for ensuring this geometric.",
                    "label": 0
                },
                {
                    "sent": "Margin geometric noise condition if P satisfies super costs, noise conditions.",
                    "label": 1
                },
                {
                    "sent": "So the amount of noise is controlled.",
                    "label": 0
                },
                {
                    "sent": "And in addition this editor behaves.",
                    "label": 0
                },
                {
                    "sent": "Nice and neighborhood at the decision boundary in the way that.",
                    "label": 0
                },
                {
                    "sent": "This term here can be bounded from above by constant C times the distance to the decision boundary to power gamma or gamma is bigger than 0.",
                    "label": 0
                },
                {
                    "sent": "Then we can show that P has actually geometric noise exponent Alpha for all Alpha smaller than this fraction here.",
                    "label": 1
                },
                {
                    "sent": "So what does that mean in this specific case where C is equal to 1 and gamma is equal to 1?",
                    "label": 0
                },
                {
                    "sent": "Imagine a 1 dimensional situation where the positive class is from zero to Infinity.",
                    "label": 0
                },
                {
                    "sent": "The negative classes from zero to minus Infinity.",
                    "label": 0
                },
                {
                    "sent": "And then the right hand side here becomes this diagonal and what we see here is that two item minus one is non negative in this class.",
                    "label": 0
                },
                {
                    "sent": "So this curve actually is in between the diagonal and this axis here and vice versa.",
                    "label": 0
                },
                {
                    "sent": "But we also see it doesn't matter how these two Atom minus one behaves away from the from the decision boundary, it can be non continuous, it can be non differentiable but it has to be.",
                    "label": 0
                },
                {
                    "sent": "In between these two lines in the neighborhood of the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of local Holder assumption.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the main result.",
                    "label": 0
                },
                {
                    "sent": "It looks a bit complicated, so let's focus on the main ideas.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that we have a distribution P which both satisfies super costs.",
                    "label": 1
                },
                {
                    "sent": "Noise assumption for Q between zero and Infinity.",
                    "label": 0
                },
                {
                    "sent": "And has geometric noise exponent, so we have control over both the amount of noise and the location of the noise.",
                    "label": 1
                },
                {
                    "sent": "And there's assume that we know these two exponents.",
                    "label": 1
                },
                {
                    "sent": "This is as a later explained, not necessary.",
                    "label": 0
                },
                {
                    "sent": "If we know these two exponents and define Lambda N. In this way, on this way, depending on the size of Alpha.",
                    "label": 1
                },
                {
                    "sent": "And the Sigma and which will be the Gaussian kernels in this way?",
                    "label": 0
                },
                {
                    "sent": "Then the result states that a support vector machine using this regularization parameter and this Sigma N satisfies with high probability that the classification risk can be bounded from above by the base classification risk.",
                    "label": 0
                },
                {
                    "sent": "Time plus.",
                    "label": 0
                },
                {
                    "sent": "This polynomial term in N. So what we see here is if Alpha is smaller than this term, this fraction here is rather small and therefore the resulting rates.",
                    "label": 0
                },
                {
                    "sent": "Are not that fast, but if we consider the other case where Arthur is bigger than this term here.",
                    "label": 0
                },
                {
                    "sent": "We obtain a rather complicated term.",
                    "label": 0
                },
                {
                    "sent": "Which of is.",
                    "label": 0
                },
                {
                    "sent": "Bigger than 1 /, 2 bigger than 1/2.",
                    "label": 0
                },
                {
                    "sent": "Let's see two specific cases here.",
                    "label": 0
                },
                {
                    "sent": "If Q goes to Infinity, that is, we have a strong assumption on the amount of noise.",
                    "label": 0
                },
                {
                    "sent": "Then this fraction here goes to 2A over 2A plus three.",
                    "label": 1
                },
                {
                    "sent": "That is, this is faster.",
                    "label": 0
                },
                {
                    "sent": "This is larger than 1/2, so the rates are faster than 1 / sqrt N whenever Alpha is larger than.",
                    "label": 0
                },
                {
                    "sent": "Three half.",
                    "label": 0
                },
                {
                    "sent": "So under a strong assumption on the amount of noise and a weak assumption on the location of the noise, we can learn with fast rates.",
                    "label": 0
                },
                {
                    "sent": "Provided, but we know these things.",
                    "label": 0
                },
                {
                    "sent": "The other extreme case, if we make a strong assumption on the location of the noise, Alpha goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Then this fraction tends to Q + 1 / 2 + 2, and this is exactly the fraction super curve obtained.",
                    "label": 0
                },
                {
                    "sent": "For his empirical risk minimization method.",
                    "label": 0
                },
                {
                    "sent": "Without approximation error and in certain sense this extreme case Alpha equals Infinity exactly describes the no approximation error for support vector machines model or certain technical issues.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "They can learn with this fast rate.",
                    "label": 0
                },
                {
                    "sent": "Under this assumption and in general.",
                    "label": 0
                },
                {
                    "sent": "The situation is more complicated.",
                    "label": 0
                },
                {
                    "sent": "OK, what's the time now?",
                    "label": 0
                },
                {
                    "sent": "41 after three OK.",
                    "label": 0
                },
                {
                    "sent": "So the rest of the talk will be a bit more technical.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I like to give you at least an overview of the structure of the proof.",
                    "label": 0
                },
                {
                    "sent": "I can't go into.",
                    "label": 0
                },
                {
                    "sent": "Too many details, since the proof itself is rather long, but at least I would like to give you an idea.",
                    "label": 0
                },
                {
                    "sent": "What happens in the proof and why?",
                    "label": 0
                },
                {
                    "sent": "The proof is rather it's quite different from a standard analysis.",
                    "label": 0
                },
                {
                    "sent": "The overall idea of the proof is to interpret support vector machines as an empirical risk minimization scheme.",
                    "label": 0
                },
                {
                    "sent": "But if one remembers the objective function, apparently it's not to the hinge loss, and it's not with respect to the classification loss, so we have to find a new loss function such that it is an empirical risk minimization scheme with respect to this loss function.",
                    "label": 0
                },
                {
                    "sent": "To this end, let us make some notations which we will we will use.",
                    "label": 0
                },
                {
                    "sent": "The small error was the hinge loss and the associated risk.",
                    "label": 0
                },
                {
                    "sent": "The other risk is simply defined by averaging over all possible hinge losses for a function F. And the averaging process is with respect to P. The smallest possible risk is denoted by RLP, and here the infamous runs of all functions.",
                    "label": 1
                },
                {
                    "sent": "Measurable functions from X2R.",
                    "label": 0
                },
                {
                    "sent": "We also define a new loss function, capital L, which only works on elements F of a reproducing kernel Hilbert space H. And this loss function.",
                    "label": 0
                },
                {
                    "sent": "At a regularization term which comes from the regularization term of the support vector machines plus the hinge loss of F. So if we define the associated capital at risk by the obvious way.",
                    "label": 0
                },
                {
                    "sent": "And the empirical version of that which is here.",
                    "label": 0
                },
                {
                    "sent": "Sample set of size N. This way we see that the empirical support vector machine is nothing as as the minimum of this empirical capital risk.",
                    "label": 0
                },
                {
                    "sent": "Yes, since this exactly resembles the target, the objective function of the optimization problem of the support vector machine.",
                    "label": 0
                },
                {
                    "sent": "What we also need is so-called infinite sample support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Here we replace the empirical risk by the true underlying risk and the resulting.",
                    "label": 0
                },
                {
                    "sent": "Solution which exists and is uniquely.",
                    "label": 0
                },
                {
                    "sent": "Determined is denoted by FP Lambda.",
                    "label": 0
                },
                {
                    "sent": "Finally, instead of considering the approximation error, it turns out to be more handy to consider this so called approximation error function, which is denoted by a of Lambda and consists of this infinite sample SVM objective function, minus the.",
                    "label": 1
                },
                {
                    "sent": "Smallest possible.",
                    "label": 0
                },
                {
                    "sent": "A risk, So what we see here is, for example, if we have a Gaussian reproducing kernel Hilbert space and Lambda goes to goes to 0, then a of Lambda goes to 0.",
                    "label": 0
                },
                {
                    "sent": "And the small, the faster this convergence holds, the better the reproducing kernel Hilbert space can approximate the distribution P in this very specific sense.",
                    "label": 0
                },
                {
                    "sent": "OK, and then the idea of the proof is based on a basic decomposition, so we consider here the excess classification risk of our VM's.",
                    "label": 0
                },
                {
                    "sent": "Solution of our empirical SVM solution.",
                    "label": 0
                },
                {
                    "sent": "This can be bounded from above by the excess hinge risk of the same.",
                    "label": 0
                },
                {
                    "sent": "Classifier this is due to Jean.",
                    "label": 0
                },
                {
                    "sent": "Then we make some algebra and it turns out that this access risk can be estimated from above by the so called estimation error, which I will consider first plus the approximation error function.",
                    "label": 0
                },
                {
                    "sent": "It's going to die here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I call this the estimation error since it compares the capital at risk of the empirically found solution with a true solution.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the rest of the idea of the proof.",
                    "label": 0
                },
                {
                    "sent": "I will mainly consider the estimation error part.",
                    "label": 0
                },
                {
                    "sent": "I probably won't have time to consider the approximation error.",
                    "label": 0
                },
                {
                    "sent": "OK, let's summarize observations.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines are empirical risk.",
                    "label": 0
                },
                {
                    "sent": "Minimizing schemes over the reproducing kernel Hilbert space H. But rather trivial estimate also tells us that the normal solutions are never larger than one over square root of Lambda and therefore instead of considering the entire space H, we can actually minimize over the smaller scaled unit ball BH or the reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "And what we see here is the smaller Lambda becomes the large.",
                    "label": 0
                },
                {
                    "sent": "This set here becomes and therefore we should expect.",
                    "label": 0
                },
                {
                    "sent": "Worth estimation error, whereas the better approximation error.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Then we define and function class script G which consists of air composed F. Here is this regularised loss function define of the previous slide minus.",
                    "label": 0
                },
                {
                    "sent": "The best one and F runs over a unit ball is scaled by a constant gamma and gamma is between one and one over square root of Lambda.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The trivial estimates suggest that gamma should actually be equal to this term here, but we will see later on that in many situations we may actually find a better value for gamma.",
                    "label": 0
                },
                {
                    "sent": "A substantially better one.",
                    "label": 0
                },
                {
                    "sent": "OK, so the connection of this function Class G to the estimation error is rather simple.",
                    "label": 1
                },
                {
                    "sent": "If we have an FT. Lambda which is.",
                    "label": 0
                },
                {
                    "sent": "In this scaled ball and we consider G or this form here, then the expectation of G equals the estimation error.",
                    "label": 0
                },
                {
                    "sent": "And for later users.",
                    "label": 0
                },
                {
                    "sent": "We should, we should remember that this means.",
                    "label": 0
                },
                {
                    "sent": "That the smaller this expectation is.",
                    "label": 0
                },
                {
                    "sent": "The better FT Lambda or the closer FT. Lambda is to the real solution FP Lambda in terms of these risks.",
                    "label": 0
                },
                {
                    "sent": "OK. Then",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Having identified support vector machines as empirical risk minimizers, we will only use some standard methods nowadays, standard methods from.",
                    "label": 0
                },
                {
                    "sent": "Empirical.",
                    "label": 0
                },
                {
                    "sent": "Process theory we consider so-called local Rademacher averages, which in a certain sense complexity measures of the function class G. Localized by a parameter epsilon.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I do not want to go here to detail and these harder Maja averages can then be used together with telegrams.",
                    "label": 0
                },
                {
                    "sent": "Concentration inequality proved in 1994.",
                    "label": 0
                },
                {
                    "sent": "To obtain this kind of result.",
                    "label": 0
                },
                {
                    "sent": "Let's focus on the main issues here, so assume that the variance of the elements G in script she can be bounded by the expectation in this way here.",
                    "label": 0
                },
                {
                    "sent": "So we have a fixed constant here, a fixed exponent Alpha here and fixed Delta here, and this then holds for every G in script she this is not.",
                    "label": 0
                },
                {
                    "sent": "Naturally satisfied, but let us assume that we have.",
                    "label": 0
                },
                {
                    "sent": "This situation.",
                    "label": 0
                },
                {
                    "sent": "Then whenever we should listen epsilon which is bigger than this quite large term on the right, we obtain that the capital L risk of the SVM solution.",
                    "label": 0
                },
                {
                    "sent": "Of the empirical SVM solution is smaller than or equal to the.",
                    "label": 0
                },
                {
                    "sent": "Capital L risk of the true SVM solution plus this estimation error epsilon.",
                    "label": 0
                },
                {
                    "sent": "So what we see here is we have to bond the Radama averages of the local kotimaa verges find a fixed point here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Determine this kind of inequality and finally have to consider this, here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The local Rademacher averages can be bound for kernel methods in various ways.",
                    "label": 0
                },
                {
                    "sent": "We decided here to consider simple techniques based on covering numbers and without going into too much details.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if we consider a Gaussian reproducing kernel Hilbert space on a compact subset X.",
                    "label": 0
                },
                {
                    "sent": "Then the local Radama averages can be bounded from above by this term here, and I would like to say that this I this a here mainly depends on the used kernels.",
                    "label": 0
                },
                {
                    "sent": "So that depends on Sigma.",
                    "label": 0
                },
                {
                    "sent": "Again, the quantity depends on this camera which we have to specify, and it also depends on a free parameter P which we have later tissues in our analysis.",
                    "label": 0
                },
                {
                    "sent": "The variance bound for support vector machines using hinge losses is.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Mainly determined by the civic of noise exponent.",
                    "label": 0
                },
                {
                    "sent": "More precisely, if this is satisfied for some Q.",
                    "label": 0
                },
                {
                    "sent": "Then the variance of the elements in script.",
                    "label": 0
                },
                {
                    "sent": "She can be bounded from above by a universal constant.",
                    "label": 0
                },
                {
                    "sent": "I think it's 32.",
                    "label": 0
                },
                {
                    "sent": "Times the scammer again and the exponent depends on Q.",
                    "label": 0
                },
                {
                    "sent": "Here's the expectation, again with the exponent depending on Q and here what is interesting is the 2nd additive term here depends on the approximation error function.",
                    "label": 0
                },
                {
                    "sent": "So this is the first part where we see that in order to find a good estimation error, we actually have to know the approximation error function first.",
                    "label": 0
                },
                {
                    "sent": "And we will observe that in the next slide again.",
                    "label": 0
                },
                {
                    "sent": "OK, and if we plug everything now together, we have turned the rather complicated version of this epsilon here and what we see here is that this size of the underlying ball of script G has a substantial influence of the behavior of this.",
                    "label": 0
                },
                {
                    "sent": "Right hand side.",
                    "label": 0
                },
                {
                    "sent": "So what we actually now have to find is a good value for gamma.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This can be done in the following way.",
                    "label": 0
                },
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "I'd like to begin with a simple observation.",
                    "label": 0
                },
                {
                    "sent": "I already mentioned that that for the empirical solutions we have somehow trivial bounds on the norm of the solutions, namely, this term here can be bounded from above.",
                    "label": 0
                },
                {
                    "sent": "By this plus the empirical error risk, since this is always non negative.",
                    "label": 0
                },
                {
                    "sent": "And since this is the minimizer of this objective function, we compare that with zero.",
                    "label": 0
                },
                {
                    "sent": "This is always not smaller, and it turns out immediately that RLT of zero is 1.",
                    "label": 0
                },
                {
                    "sent": "In other words, the norm of FT Lambda can be bounded from above by one over square root of Lambda.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "This is not optimal.",
                    "label": 0
                },
                {
                    "sent": "Namely.",
                    "label": 0
                },
                {
                    "sent": "If we consider an infinite sample support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Then we can make almost the same game but with a small but substantial difference.",
                    "label": 0
                },
                {
                    "sent": "Again, we estimate this here by this and now.",
                    "label": 0
                },
                {
                    "sent": "Recall that our LP is the smallest possible error risk and therefore this term here is always non negative.",
                    "label": 0
                },
                {
                    "sent": "And therefore we can make this estimate here.",
                    "label": 0
                },
                {
                    "sent": "But then this entire function here is nothing else, as the approximation error function.",
                    "label": 0
                },
                {
                    "sent": "Just by definition.",
                    "label": 0
                },
                {
                    "sent": "So that means that our FP Lambda is always bounded from above by one over square root of Lambda times the square root of the approximation error function.",
                    "label": 0
                },
                {
                    "sent": "And in general, if we can learn this approximation error function goes to zero and therefore this term here goes faster, doesn't increase that fast as this one.",
                    "label": 0
                },
                {
                    "sent": "And this will be used in the analysis.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, this analysis only works for the infinite sample support vector machine.",
                    "label": 1
                },
                {
                    "sent": "But as I will describe.",
                    "label": 0
                },
                {
                    "sent": "Now is that it approx that this this relation here approximately also holds for the empirically found solutions FT Lambda.",
                    "label": 0
                },
                {
                    "sent": "How can this be shown?",
                    "label": 1
                },
                {
                    "sent": "Well, the idea is to.",
                    "label": 0
                },
                {
                    "sent": "Kind of Bootstrap method, maybe?",
                    "label": 0
                },
                {
                    "sent": "So let us assume that we.",
                    "label": 0
                },
                {
                    "sent": "Do not know anything about the problem so that we begin with a trivial estimate gamma N. Equals one over square root of Lambda N. Yeah, then we can use our concentration inequality and this gives that this regularization term.",
                    "label": 1
                },
                {
                    "sent": "Is bounded from above by the regularization term plus the excess error risk.",
                    "label": 0
                },
                {
                    "sent": "Since this is non negative.",
                    "label": 0
                },
                {
                    "sent": "And now we use the concentration result, which says that.",
                    "label": 0
                },
                {
                    "sent": "This overall term, which is the capital L risk.",
                    "label": 0
                },
                {
                    "sent": "Can be bounded from above by this term here.",
                    "label": 0
                },
                {
                    "sent": "Plus there the estimation error epsilon N. Yeah, and.",
                    "label": 0
                },
                {
                    "sent": "The first three terms are actually the approximation error function, Lambda N. So what we see here is the left hand side looks good on the right hand side where we do not know, but it turns out that unfortunately the epsilon N dominates the approximation error function.",
                    "label": 0
                },
                {
                    "sent": "But it still goes to zero polynomially.",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "Well, it means that we have a slightly better estimate of this term than we previously had.",
                    "label": 1
                },
                {
                    "sent": "With high probability.",
                    "label": 0
                },
                {
                    "sent": "OK, we practiced in again.",
                    "label": 0
                },
                {
                    "sent": "And obtain another slightly better estimate of this regularization term.",
                    "label": 0
                },
                {
                    "sent": "Iterate the procedure and with high probability we almost reached this result here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we compare this analysis with them.",
                    "label": 0
                },
                {
                    "sent": "The more standard way we see that they are built, both based on a rather standard decomposition of the excess risk in an estimation error and approximation error.",
                    "label": 1
                },
                {
                    "sent": "But the standard analysis usually considers both error terms independently from each other.",
                    "label": 1
                },
                {
                    "sent": "Whereas in our analysis analysis it turned out that the approximation error or the approximation error function.",
                    "label": 0
                },
                {
                    "sent": "Has influence on the estimation or in two terms in two ways.",
                    "label": 0
                },
                {
                    "sent": "First, it is.",
                    "label": 0
                },
                {
                    "sent": "It occurs in the additive term of the variance bounds.",
                    "label": 1
                },
                {
                    "sent": "And second, it also approximately determines the best value for gamma.",
                    "label": 1
                },
                {
                    "sent": "Which has a substantial influence on the rates we obtained.",
                    "label": 0
                },
                {
                    "sent": "So instead of considering these two independently in this very specific situation, it is.",
                    "label": 0
                },
                {
                    "sent": "Wise to 1st consider approximation.",
                    "label": 0
                },
                {
                    "sent": "Errors and then use the estimation error part.",
                    "label": 0
                },
                {
                    "sent": "Otherwise the resulting rates are worse.",
                    "label": 0
                },
                {
                    "sent": "OK, so I probably won't have time to consider the approximation error.",
                    "label": 0
                },
                {
                    "sent": "Just a very last slide considering open questions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The first open question or open problem is.",
                    "label": 1
                },
                {
                    "sent": "According to Oracle Inequality's, so this technique of finding a good value for gamma only works if we're preore fix the sequence Lambda N and Sigma N. If we do not know that our priority, the technique doesn't work, but we now know how to resolve this problem and hopefully the results are finished before the NIPS deadline.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The Oracle inequalities are important 'cause they will give us away for parameter selection, so remember that the rates we obtained here.",
                    "label": 0
                },
                {
                    "sent": "Require that we know these exponents Alpha and Q.",
                    "label": 0
                },
                {
                    "sent": "Although we also obtain rates if we simply use other sequences of Lambda and Sigma, and the resulting rates are then worse.",
                    "label": 0
                },
                {
                    "sent": "But we are actually interested in rates.",
                    "label": 0
                },
                {
                    "sent": "Which work?",
                    "label": 0
                },
                {
                    "sent": "When we do not know the exponents, but.",
                    "label": 1
                },
                {
                    "sent": "Find them almost adaptive adaptivity.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "And chances are not bad that the parameter selection problem will be solved by these or Oracle inequalities.",
                    "label": 1
                },
                {
                    "sent": "And next open question is whether the resulting rates are optimal or there are optimal in the sense that our methods won't give better rates.",
                    "label": 0
                },
                {
                    "sent": "But this is of course not the question of interest, so it is completely open to us whether the results we obtained are.",
                    "label": 0
                },
                {
                    "sent": "Optimal at least for interesting classes of distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, I think this is the last question, is not so important here good.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "None of your race.",
                    "label": 0
                },
                {
                    "sent": "She can't.",
                    "label": 0
                },
                {
                    "sent": "This big constant depends on how your probability measure P. Where is it awesome presentation that it depends on?",
                    "label": 0
                },
                {
                    "sent": "So it depends.",
                    "label": 0
                },
                {
                    "sent": "Since appearing in both Super Coffee North Assumption and the margin and the and the geometric noise assumption, but perhaps also on your.",
                    "label": 1
                },
                {
                    "sent": "Fication it.",
                    "label": 0
                },
                {
                    "sent": "Also depends on Constance appearing and covering numbers, which are usually extremely hard to find.",
                    "label": 0
                },
                {
                    "sent": "And it depends on a universal constant in a theorem, proof by Telegram.",
                    "label": 0
                },
                {
                    "sent": "And these are the dependencies.",
                    "label": 0
                },
                {
                    "sent": "There's no other dependency.",
                    "label": 0
                },
                {
                    "sent": "And it depends on on the epsilon in this case.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "It also depends on the epsilon, how close the rates are to the one we want actually want to describe.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't really depend on.",
                    "label": 0
                },
                {
                    "sent": "I mean, somehow it will depend on the on the algorithm, but in the analysis it is clear that it mainly depends on these.",
                    "label": 0
                },
                {
                    "sent": "Conditions describing the distribution and the epsilon.",
                    "label": 0
                },
                {
                    "sent": "In general, you can compute them, although I think it's not a wise idea to do so.",
                    "label": 0
                },
                {
                    "sent": "4:30 OK.",
                    "label": 0
                }
            ]
        }
    }
}