{
    "id": "ledwkq7xlaut2es35azi4p3hzdq7tzxo",
    "title": "Bayesian Clustering for Email Campaign Detection",
    "info": {
        "author": [
            "Peter Haider, Institute of Computer Science, University of Potsdam"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_haider_bcecd/",
    "segmentation": [
        [
            "OK, welcome to my talk about Bayesian clustering for email campaign detection and presenting joint work with Toby's Scheffer."
        ],
        [
            "The purpose of email campaign detection is to cluster a set of emails according to the campaigns they were sent with.",
            "Um?",
            "Usually mass email campaigns are created by deploying.",
            "Probabilistic templates to sending hosts, for example, hijacked consumer PC's and those sending hosts then.",
            "Generate emails by filling out the placeholders in those templates randomly.",
            "For example, in the top left you see an example of an email template, and there are some words fixed in some placeholders that can be filled either entirely at random or.",
            "With a selection of a predefined set of options.",
            "We can specify such a probabilistic template by a parameter vector Theta that is drawn from a prior distribution P of Theta.",
            "And the generation of emails out of this template corresponds then to the drawing of an X from the distribution P of X given Theta.",
            "So now there is a set of emails, some of them belong to same campaigns.",
            "Some of them might be Singleton emails that we treat as campaigns of size 1."
        ],
        [
            "We want to cluster them now according to the sources they have most likely been created by.",
            "So we want to find the most likely clustering C. Given the set of emails, Capital X.",
            "And during the process of this clustering, at no time we know the actual templates that have generated these emails.",
            "Instead, we update our posterior beliefs about the distributions on templates.",
            "So in order to calculate the joint likelihoods of the emails in a cluster, we have to integrate over the whole space of possible parameter vectors Theta.",
            "Now you might ask what uses it to know the campaign structure of a set of emails.",
            "If all you want to do is filter spam's.",
            "And the answer is no."
        ],
        [
            "To which cluster or campaign and email belongs makes it very easy to filter it.",
            "Post we have a source of of emails that we rely abli know of that they are spam.",
            "For example if we publish email addresses in visibly on the web or have IP address blacklists, then we have a constant stream of emails that we know as spam.",
            "Then if we cluster them according to their campaigns we can use the clusters to reliably classify newly incoming emails as spam or non spam.",
            "We do this by calculating the conditional likelihoods of the new emails given the clusters in the set, respectively.",
            "The conditional likelihoods of the new emails, given that they have been created independently and putting a threshold on the ratio of those conditional likelihoods gives us a classification score.",
            "Whether a classification, whether those emails are spam or non spam."
        ],
        [
            "The abstract problem setting is now as follows.",
            "We have an unknown number of sources I each associated with a latent parameter vector Theta drawn from the prior P of Theta, and those sources generate a set of emails, capital X that we want to cluster according to their latent campaigns.",
            "And that corresponds to finding the most likely clustering C, given the set X.",
            "In the following part of the talk I will refer."
        ],
        [
            "Quickly review.",
            "The closed form solution one gets for the Bayesian clustering equations.",
            "If one makes an independence assumption on the attributes of the objects.",
            "After that I will present any feature transformation that allows us to drop this independence assumption by transforming all input examples into a different space and making an independence assumption there.",
            "That way we can handle arbitrary dependencies of the dimensions in the input space while preserving the availability of close form solutions of the integrals over the parameter space.",
            "After that I will present an optimization problem, an algorithm for learning such a feature transformation that minimizes the induced approximation error.",
            "And finally I will show you some results in a case study about spam filtering using campaign detection.",
            "That uses our basic clustering with feature transformation.",
            "It is well known that if one makes an independence assumption on."
        ],
        [
            "The dimensions of an input example.",
            "Then the likelihood can be easily computed by vectorizing over the dimensions.",
            "The same holds true for the prior parameters Theta.",
            "We can define it as a product over the priors on the separate.",
            "Components of the vector.",
            "Now it is convenient to impose a beta distributed prior on the latent parameter components.",
            "Because the beta distribution is conjugate to the Bernoulli distribution and then the integrals that we need to compute the conditional likelihoods and joint likelihoods have an analytic solution.",
            "On the lower.",
            "To lower most equations.",
            "But the problem obviously is that the independence assumption on the input dimensions is in most cases in appropriate.",
            "For example, if our input attributes are encode the absence or presence of words, then it's obvious that they are not independent.",
            "Our remedy for this problem is now too."
        ],
        [
            "Introduce a feature transformation Phi that transforms the examples the binary vectors into different binary vectors in a potentially higher dimensional space.",
            "And then make an independence assumption on the transform space.",
            "That way they can.",
            "Still we can still handle dependencies between the dimensions in the input space.",
            "And at the same time have an analytic solution to the integrals that we need for calculating the likelihoods.",
            "So the formulas for calculating the likelihoods are basically the same as before.",
            "Only the products don't range over the input dimensions, but over the transform dimensions.",
            "Now the question is how do we get?"
        ],
        [
            "Veteran feature transformation fi.",
            "What comes first to mind is we would like to find a file that guarantees us that the transformed examples have independent attributes.",
            "But in fact, that's not what we actually want.",
            "Actually, we want to find.",
            "A transformation such that.",
            "The quantity that is defined as the product over the transformed dimension is as close as possible approximation to the true distribution.",
            "So we want to.",
            "Find a file that Q supply of X given Capital X is as closely as possible to the true distribution P of X given capital X.",
            "When we do this, we have to ensure that this quantity Q can be used as a proper probability distribution.",
            "That means we have to make certain that.",
            "It sums to at most one overall X and in the paper we prove a theorem that says.",
            "This is the case if and only if the transformation Phi is injective.",
            "Now how do we define such a feature?"
        ],
        [
            "Information 5.",
            "Here we make use of the fact that every mapping from binary vectors to other binary vectors can be represented as a set of Boolean functions, and every Boolean function can in turn be constructed as a concatenation of elementary Boolean operations.",
            "We use the following two types of elementary operations.",
            "The first is Cy A, which takes an input vector and replaces two of its dimensions with the logical end combinations thereof, and leaves all other dimensions untouched.",
            "The second type of operations we use is SY X, which replaced places 2 dimensions which their logical, exclusive or combination.",
            "And if we take those two types of elementary operations for all possible pairs of dimensions.",
            "And then take all possible concatenations of those elementary operations.",
            "We spend the whole space of.",
            "Boolean transformations and at the same time all those transformations are guaranteed to be injective.",
            "In principle we could use other types of elementary operations, but those two with chosen we've chosen have the additional advantage that they preserve the sparsity of the input vectors, which helps a lot for efficient computation.",
            "Now that we've defined the search space for five.",
            "How do we?"
        ],
        [
            "Find the optimal file in this space.",
            "As I said before, we want to approximate the true distribution P as closely as possible.",
            "With Q sub fi.",
            "And we do this by minimizing the expected Kolberg Libra divergance between Q&P and then if you rearrange this term a little bit and drop some constant terms, we can approximate this expectation with some over empirical sample S and arrive at this final optimization problem, which says out of all possible concatenations of elementary Boolean.",
            "Operations find the one that maximizes the sum over the approximated conditional log likelihood's over the empirical sample S. This is a."
        ],
        [
            "Highly nonconvex optimization problem, but we use a simple greedy procedure that starts with the identity transformation and in each step finds the next best elementary operation PSI and concatenates it to the previous transformation file and the algorithm stops if the average conditional likelihoods stop increasing.",
            "In order to make this concept of feature transformations and the algorithm abit more clear, give."
        ],
        [
            "A short example.",
            "Suppose we have an input space with four dimensions.",
            "We call them QRS and T. Then the algorithm proceeds with the first step finding.",
            "For example, the best elementary operation site A on the dimensions Q&R.",
            "Concatenates this elementary operation with the current.",
            "Transformation Phi that is now the identity transformation and that gives us a representation of the output vectors as this 5 dimensional binary vector.",
            "In the second step, maybe the optimal elementary operation is.",
            "PSI X on the dimensions Q and R&S.",
            "Which then in turn gets concatenated with the previous transformation.",
            "Which gives us this representation for the output feature transformation.",
            "This concludes the part about feature transformations.",
            "What remains to specify is how do."
        ],
        [
            "We maintain a. Clustering in real time on the emails.",
            "In our spam trap.",
            "Here we use a sequential clustering algorithm that iteratively takes the next email in the stream and classes it in an online fashion.",
            "To the current clustering.",
            "So at each step we compute the.",
            "Conditional likelihoods of the current email given all clusters we have now in the spam trap.",
            "NB Edit To the most likely cluster cluster.",
            "Respectively, we create a new cluster for the email, and so on."
        ],
        [
            "Finally come to our case study on email campaign detection for spam filtering.",
            "In general, the problem with.",
            "Unsupervised clustering methods is that in order to evaluate them we would need a ground truth, but in most cases this is not available.",
            "But Fortunately, in our application there's a very natural evaluation criterion.",
            "Under this, how do the produced clusterings help in order to reliably filter spam's?",
            "Nowhere.",
            "Experimental setup is as following.",
            "We have a stream of spent trip emails that we acquired by using an IP address blacklist.",
            "In total we have about 70,000 spam emails and at each point in time we take the 5000 most recent spams in this pen trip, cluster them with the Bayesian clustering with feature transformation and then use the produced class rings.",
            "To classify newly incoming emails that is the test emails in our test stream consists of chronologically ordered.",
            "60,000 spins and about 40,000 non sperms.",
            "And they are classified by.",
            "Versus the likelihood that they are generated independently.",
            "Our first experimental setting.",
            "Evaluates the performance on."
        ],
        [
            "Emails that we have.",
            "That are from a distribution that we know.",
            "That is, we take as training examples.",
            "10,000 non sperms that are from the same distribution at the test distribution.",
            "This is a rather unrealistic setting, but.",
            "It's kind of an upper bound on the expected true performance, so we train.",
            "The feature transformation for the Bayesian clustering on those 10,000.",
            "Non spam emails from the same distribution as the test distribution.",
            "And we use a baseline support vector machine that is trained on those same nonce perms and the 5000 most recent emails in the spam trap.",
            "Additionally, we compare against a Bayesian clustering method without any feature transformation.",
            "And what's important in spam filtering is not the overall accuracy.",
            "Cause a misclassified non sperm.",
            "Incurs a much greater loss than a misclassified spam, so we tune our hyperparameters are calling too.",
            "The optimal number of true positively."
        ],
        [
            "Classified sperms?",
            "Given we have no false positives.",
            "And the results are that.",
            "The basic clustering method with feature transformation.",
            "Gives a increase in the number of true positives while there are zero false positives compared to the SVM baseline.",
            "And we can also see that.",
            "The Bayesian clustering without feature transformation performs significantly worse.",
            "In a setting second setting, we want to evaluate how does this scheme perform if we do not know the non spam test distribution.",
            "So we train our feature transformation SVM on no non spends at all there."
        ],
        [
            "So slightly pessimistic estimate of the performance, but it's kind of a lower bound on the expected performance.",
            "In practice, one would expect a mixture of the 1st and the 2nd setting.",
            "So we train the feature transformation of the base and clustering method only on.",
            "Non sperms and the SVM is trained in a one class fashion as well only on non sperms on sperms I mean."
        ],
        [
            "And the results are.",
            "The following.",
            "We can achieve with the Bayesian clustering method.",
            "An increase of the proportion of filtered out sperms.",
            "Given that we.",
            "Correctly classify all non sperms.",
            "An increase from 10% to 75% compared to the one class SVM.",
            "So to sum up what you should take home with is."
        ],
        [
            "We have derived a closed form solution for Bayesian clustering that allows arbitrary dependencies of the input dimensions.",
            "By applying a feature transformation.",
            "And.",
            "Making an independence assumption on the transformed space.",
            "And.",
            "We have presented a. Optimization problem, an algorithm to learn such a feature transformation that minimizes the induced approximation error.",
            "And finally, our case study has shown that using Bayesian clustering with feature transformation we can greatly increase the proportion of filtered sperms.",
            "Given that we correctly classify all non sperms.",
            "Compared to an SVM baseline.",
            "Thank you for attention."
        ],
        [
            "Provide me answer questions maybe next week.",
            "We can turn off this.",
            "Air conditioning for this.",
            "Blue Yep.",
            "More download space track and representative.",
            "I didn't understand.",
            "Spanish rap.",
            "Oh, it's a.",
            "Taking from a commercial email provider and.",
            "The blacklist we use is from Spamhaus.",
            "It's manually maintained and we think it's a very representative sample out of all spam's that I sent.",
            "Stop spamming supposed to Blacklist IP addresses.",
            "It's a black list of IP addresses.",
            "It's not depending on the content.",
            "Why are features intended in the transport space?",
            "We just make this assumption in order.",
            "Wife awards number.",
            "Then assuming independence in original space, because we learn the feature transformation in a way that this assumption hurts us as less as possible.",
            "Because if we would create an approximation that metrics exactly the true distribution, then we have shown that the independence assumption doesn't hurt at all.",
            "How many clusters you end up finding?",
            "In practice?",
            "It depends on the hyperparameter setting in this setting where we optimize for maximum number of true positives given zero false positives, we get on the order of a few thousands of clusters.",
            "That's the optimization criterion.",
            "We derive the optimization problem by minimizing the callback library divergent to the true distribution.",
            "I have also question.",
            "District Magistrate You can transformation of the features that seems to be a cool trick.",
            "Does that also work in general for namespace classifiers first, but that's something I want to look into the future, definitely."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, welcome to my talk about Bayesian clustering for email campaign detection and presenting joint work with Toby's Scheffer.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The purpose of email campaign detection is to cluster a set of emails according to the campaigns they were sent with.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Usually mass email campaigns are created by deploying.",
                    "label": 1
                },
                {
                    "sent": "Probabilistic templates to sending hosts, for example, hijacked consumer PC's and those sending hosts then.",
                    "label": 0
                },
                {
                    "sent": "Generate emails by filling out the placeholders in those templates randomly.",
                    "label": 0
                },
                {
                    "sent": "For example, in the top left you see an example of an email template, and there are some words fixed in some placeholders that can be filled either entirely at random or.",
                    "label": 0
                },
                {
                    "sent": "With a selection of a predefined set of options.",
                    "label": 0
                },
                {
                    "sent": "We can specify such a probabilistic template by a parameter vector Theta that is drawn from a prior distribution P of Theta.",
                    "label": 0
                },
                {
                    "sent": "And the generation of emails out of this template corresponds then to the drawing of an X from the distribution P of X given Theta.",
                    "label": 0
                },
                {
                    "sent": "So now there is a set of emails, some of them belong to same campaigns.",
                    "label": 0
                },
                {
                    "sent": "Some of them might be Singleton emails that we treat as campaigns of size 1.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We want to cluster them now according to the sources they have most likely been created by.",
                    "label": 0
                },
                {
                    "sent": "So we want to find the most likely clustering C. Given the set of emails, Capital X.",
                    "label": 0
                },
                {
                    "sent": "And during the process of this clustering, at no time we know the actual templates that have generated these emails.",
                    "label": 0
                },
                {
                    "sent": "Instead, we update our posterior beliefs about the distributions on templates.",
                    "label": 0
                },
                {
                    "sent": "So in order to calculate the joint likelihoods of the emails in a cluster, we have to integrate over the whole space of possible parameter vectors Theta.",
                    "label": 0
                },
                {
                    "sent": "Now you might ask what uses it to know the campaign structure of a set of emails.",
                    "label": 0
                },
                {
                    "sent": "If all you want to do is filter spam's.",
                    "label": 0
                },
                {
                    "sent": "And the answer is no.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To which cluster or campaign and email belongs makes it very easy to filter it.",
                    "label": 0
                },
                {
                    "sent": "Post we have a source of of emails that we rely abli know of that they are spam.",
                    "label": 0
                },
                {
                    "sent": "For example if we publish email addresses in visibly on the web or have IP address blacklists, then we have a constant stream of emails that we know as spam.",
                    "label": 0
                },
                {
                    "sent": "Then if we cluster them according to their campaigns we can use the clusters to reliably classify newly incoming emails as spam or non spam.",
                    "label": 0
                },
                {
                    "sent": "We do this by calculating the conditional likelihoods of the new emails given the clusters in the set, respectively.",
                    "label": 0
                },
                {
                    "sent": "The conditional likelihoods of the new emails, given that they have been created independently and putting a threshold on the ratio of those conditional likelihoods gives us a classification score.",
                    "label": 0
                },
                {
                    "sent": "Whether a classification, whether those emails are spam or non spam.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The abstract problem setting is now as follows.",
                    "label": 1
                },
                {
                    "sent": "We have an unknown number of sources I each associated with a latent parameter vector Theta drawn from the prior P of Theta, and those sources generate a set of emails, capital X that we want to cluster according to their latent campaigns.",
                    "label": 1
                },
                {
                    "sent": "And that corresponds to finding the most likely clustering C, given the set X.",
                    "label": 0
                },
                {
                    "sent": "In the following part of the talk I will refer.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quickly review.",
                    "label": 0
                },
                {
                    "sent": "The closed form solution one gets for the Bayesian clustering equations.",
                    "label": 0
                },
                {
                    "sent": "If one makes an independence assumption on the attributes of the objects.",
                    "label": 0
                },
                {
                    "sent": "After that I will present any feature transformation that allows us to drop this independence assumption by transforming all input examples into a different space and making an independence assumption there.",
                    "label": 0
                },
                {
                    "sent": "That way we can handle arbitrary dependencies of the dimensions in the input space while preserving the availability of close form solutions of the integrals over the parameter space.",
                    "label": 0
                },
                {
                    "sent": "After that I will present an optimization problem, an algorithm for learning such a feature transformation that minimizes the induced approximation error.",
                    "label": 1
                },
                {
                    "sent": "And finally I will show you some results in a case study about spam filtering using campaign detection.",
                    "label": 1
                },
                {
                    "sent": "That uses our basic clustering with feature transformation.",
                    "label": 0
                },
                {
                    "sent": "It is well known that if one makes an independence assumption on.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The dimensions of an input example.",
                    "label": 0
                },
                {
                    "sent": "Then the likelihood can be easily computed by vectorizing over the dimensions.",
                    "label": 0
                },
                {
                    "sent": "The same holds true for the prior parameters Theta.",
                    "label": 0
                },
                {
                    "sent": "We can define it as a product over the priors on the separate.",
                    "label": 0
                },
                {
                    "sent": "Components of the vector.",
                    "label": 0
                },
                {
                    "sent": "Now it is convenient to impose a beta distributed prior on the latent parameter components.",
                    "label": 0
                },
                {
                    "sent": "Because the beta distribution is conjugate to the Bernoulli distribution and then the integrals that we need to compute the conditional likelihoods and joint likelihoods have an analytic solution.",
                    "label": 0
                },
                {
                    "sent": "On the lower.",
                    "label": 0
                },
                {
                    "sent": "To lower most equations.",
                    "label": 0
                },
                {
                    "sent": "But the problem obviously is that the independence assumption on the input dimensions is in most cases in appropriate.",
                    "label": 0
                },
                {
                    "sent": "For example, if our input attributes are encode the absence or presence of words, then it's obvious that they are not independent.",
                    "label": 0
                },
                {
                    "sent": "Our remedy for this problem is now too.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Introduce a feature transformation Phi that transforms the examples the binary vectors into different binary vectors in a potentially higher dimensional space.",
                    "label": 1
                },
                {
                    "sent": "And then make an independence assumption on the transform space.",
                    "label": 0
                },
                {
                    "sent": "That way they can.",
                    "label": 0
                },
                {
                    "sent": "Still we can still handle dependencies between the dimensions in the input space.",
                    "label": 0
                },
                {
                    "sent": "And at the same time have an analytic solution to the integrals that we need for calculating the likelihoods.",
                    "label": 0
                },
                {
                    "sent": "So the formulas for calculating the likelihoods are basically the same as before.",
                    "label": 0
                },
                {
                    "sent": "Only the products don't range over the input dimensions, but over the transform dimensions.",
                    "label": 0
                },
                {
                    "sent": "Now the question is how do we get?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Veteran feature transformation fi.",
                    "label": 0
                },
                {
                    "sent": "What comes first to mind is we would like to find a file that guarantees us that the transformed examples have independent attributes.",
                    "label": 0
                },
                {
                    "sent": "But in fact, that's not what we actually want.",
                    "label": 0
                },
                {
                    "sent": "Actually, we want to find.",
                    "label": 0
                },
                {
                    "sent": "A transformation such that.",
                    "label": 0
                },
                {
                    "sent": "The quantity that is defined as the product over the transformed dimension is as close as possible approximation to the true distribution.",
                    "label": 0
                },
                {
                    "sent": "So we want to.",
                    "label": 0
                },
                {
                    "sent": "Find a file that Q supply of X given Capital X is as closely as possible to the true distribution P of X given capital X.",
                    "label": 0
                },
                {
                    "sent": "When we do this, we have to ensure that this quantity Q can be used as a proper probability distribution.",
                    "label": 1
                },
                {
                    "sent": "That means we have to make certain that.",
                    "label": 0
                },
                {
                    "sent": "It sums to at most one overall X and in the paper we prove a theorem that says.",
                    "label": 0
                },
                {
                    "sent": "This is the case if and only if the transformation Phi is injective.",
                    "label": 0
                },
                {
                    "sent": "Now how do we define such a feature?",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Information 5.",
                    "label": 0
                },
                {
                    "sent": "Here we make use of the fact that every mapping from binary vectors to other binary vectors can be represented as a set of Boolean functions, and every Boolean function can in turn be constructed as a concatenation of elementary Boolean operations.",
                    "label": 1
                },
                {
                    "sent": "We use the following two types of elementary operations.",
                    "label": 0
                },
                {
                    "sent": "The first is Cy A, which takes an input vector and replaces two of its dimensions with the logical end combinations thereof, and leaves all other dimensions untouched.",
                    "label": 0
                },
                {
                    "sent": "The second type of operations we use is SY X, which replaced places 2 dimensions which their logical, exclusive or combination.",
                    "label": 0
                },
                {
                    "sent": "And if we take those two types of elementary operations for all possible pairs of dimensions.",
                    "label": 0
                },
                {
                    "sent": "And then take all possible concatenations of those elementary operations.",
                    "label": 0
                },
                {
                    "sent": "We spend the whole space of.",
                    "label": 0
                },
                {
                    "sent": "Boolean transformations and at the same time all those transformations are guaranteed to be injective.",
                    "label": 0
                },
                {
                    "sent": "In principle we could use other types of elementary operations, but those two with chosen we've chosen have the additional advantage that they preserve the sparsity of the input vectors, which helps a lot for efficient computation.",
                    "label": 0
                },
                {
                    "sent": "Now that we've defined the search space for five.",
                    "label": 0
                },
                {
                    "sent": "How do we?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Find the optimal file in this space.",
                    "label": 0
                },
                {
                    "sent": "As I said before, we want to approximate the true distribution P as closely as possible.",
                    "label": 0
                },
                {
                    "sent": "With Q sub fi.",
                    "label": 0
                },
                {
                    "sent": "And we do this by minimizing the expected Kolberg Libra divergance between Q&P and then if you rearrange this term a little bit and drop some constant terms, we can approximate this expectation with some over empirical sample S and arrive at this final optimization problem, which says out of all possible concatenations of elementary Boolean.",
                    "label": 0
                },
                {
                    "sent": "Operations find the one that maximizes the sum over the approximated conditional log likelihood's over the empirical sample S. This is a.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Highly nonconvex optimization problem, but we use a simple greedy procedure that starts with the identity transformation and in each step finds the next best elementary operation PSI and concatenates it to the previous transformation file and the algorithm stops if the average conditional likelihoods stop increasing.",
                    "label": 0
                },
                {
                    "sent": "In order to make this concept of feature transformations and the algorithm abit more clear, give.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A short example.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have an input space with four dimensions.",
                    "label": 0
                },
                {
                    "sent": "We call them QRS and T. Then the algorithm proceeds with the first step finding.",
                    "label": 0
                },
                {
                    "sent": "For example, the best elementary operation site A on the dimensions Q&R.",
                    "label": 0
                },
                {
                    "sent": "Concatenates this elementary operation with the current.",
                    "label": 0
                },
                {
                    "sent": "Transformation Phi that is now the identity transformation and that gives us a representation of the output vectors as this 5 dimensional binary vector.",
                    "label": 0
                },
                {
                    "sent": "In the second step, maybe the optimal elementary operation is.",
                    "label": 0
                },
                {
                    "sent": "PSI X on the dimensions Q and R&S.",
                    "label": 0
                },
                {
                    "sent": "Which then in turn gets concatenated with the previous transformation.",
                    "label": 0
                },
                {
                    "sent": "Which gives us this representation for the output feature transformation.",
                    "label": 1
                },
                {
                    "sent": "This concludes the part about feature transformations.",
                    "label": 0
                },
                {
                    "sent": "What remains to specify is how do.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We maintain a. Clustering in real time on the emails.",
                    "label": 0
                },
                {
                    "sent": "In our spam trap.",
                    "label": 0
                },
                {
                    "sent": "Here we use a sequential clustering algorithm that iteratively takes the next email in the stream and classes it in an online fashion.",
                    "label": 0
                },
                {
                    "sent": "To the current clustering.",
                    "label": 0
                },
                {
                    "sent": "So at each step we compute the.",
                    "label": 1
                },
                {
                    "sent": "Conditional likelihoods of the current email given all clusters we have now in the spam trap.",
                    "label": 0
                },
                {
                    "sent": "NB Edit To the most likely cluster cluster.",
                    "label": 1
                },
                {
                    "sent": "Respectively, we create a new cluster for the email, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally come to our case study on email campaign detection for spam filtering.",
                    "label": 1
                },
                {
                    "sent": "In general, the problem with.",
                    "label": 1
                },
                {
                    "sent": "Unsupervised clustering methods is that in order to evaluate them we would need a ground truth, but in most cases this is not available.",
                    "label": 1
                },
                {
                    "sent": "But Fortunately, in our application there's a very natural evaluation criterion.",
                    "label": 0
                },
                {
                    "sent": "Under this, how do the produced clusterings help in order to reliably filter spam's?",
                    "label": 0
                },
                {
                    "sent": "Nowhere.",
                    "label": 0
                },
                {
                    "sent": "Experimental setup is as following.",
                    "label": 0
                },
                {
                    "sent": "We have a stream of spent trip emails that we acquired by using an IP address blacklist.",
                    "label": 1
                },
                {
                    "sent": "In total we have about 70,000 spam emails and at each point in time we take the 5000 most recent spams in this pen trip, cluster them with the Bayesian clustering with feature transformation and then use the produced class rings.",
                    "label": 0
                },
                {
                    "sent": "To classify newly incoming emails that is the test emails in our test stream consists of chronologically ordered.",
                    "label": 0
                },
                {
                    "sent": "60,000 spins and about 40,000 non sperms.",
                    "label": 0
                },
                {
                    "sent": "And they are classified by.",
                    "label": 0
                },
                {
                    "sent": "Versus the likelihood that they are generated independently.",
                    "label": 0
                },
                {
                    "sent": "Our first experimental setting.",
                    "label": 0
                },
                {
                    "sent": "Evaluates the performance on.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Emails that we have.",
                    "label": 0
                },
                {
                    "sent": "That are from a distribution that we know.",
                    "label": 0
                },
                {
                    "sent": "That is, we take as training examples.",
                    "label": 0
                },
                {
                    "sent": "10,000 non sperms that are from the same distribution at the test distribution.",
                    "label": 0
                },
                {
                    "sent": "This is a rather unrealistic setting, but.",
                    "label": 0
                },
                {
                    "sent": "It's kind of an upper bound on the expected true performance, so we train.",
                    "label": 0
                },
                {
                    "sent": "The feature transformation for the Bayesian clustering on those 10,000.",
                    "label": 1
                },
                {
                    "sent": "Non spam emails from the same distribution as the test distribution.",
                    "label": 1
                },
                {
                    "sent": "And we use a baseline support vector machine that is trained on those same nonce perms and the 5000 most recent emails in the spam trap.",
                    "label": 1
                },
                {
                    "sent": "Additionally, we compare against a Bayesian clustering method without any feature transformation.",
                    "label": 0
                },
                {
                    "sent": "And what's important in spam filtering is not the overall accuracy.",
                    "label": 0
                },
                {
                    "sent": "Cause a misclassified non sperm.",
                    "label": 0
                },
                {
                    "sent": "Incurs a much greater loss than a misclassified spam, so we tune our hyperparameters are calling too.",
                    "label": 0
                },
                {
                    "sent": "The optimal number of true positively.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classified sperms?",
                    "label": 0
                },
                {
                    "sent": "Given we have no false positives.",
                    "label": 0
                },
                {
                    "sent": "And the results are that.",
                    "label": 0
                },
                {
                    "sent": "The basic clustering method with feature transformation.",
                    "label": 1
                },
                {
                    "sent": "Gives a increase in the number of true positives while there are zero false positives compared to the SVM baseline.",
                    "label": 1
                },
                {
                    "sent": "And we can also see that.",
                    "label": 0
                },
                {
                    "sent": "The Bayesian clustering without feature transformation performs significantly worse.",
                    "label": 0
                },
                {
                    "sent": "In a setting second setting, we want to evaluate how does this scheme perform if we do not know the non spam test distribution.",
                    "label": 0
                },
                {
                    "sent": "So we train our feature transformation SVM on no non spends at all there.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So slightly pessimistic estimate of the performance, but it's kind of a lower bound on the expected performance.",
                    "label": 0
                },
                {
                    "sent": "In practice, one would expect a mixture of the 1st and the 2nd setting.",
                    "label": 0
                },
                {
                    "sent": "So we train the feature transformation of the base and clustering method only on.",
                    "label": 0
                },
                {
                    "sent": "Non sperms and the SVM is trained in a one class fashion as well only on non sperms on sperms I mean.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the results are.",
                    "label": 0
                },
                {
                    "sent": "The following.",
                    "label": 0
                },
                {
                    "sent": "We can achieve with the Bayesian clustering method.",
                    "label": 0
                },
                {
                    "sent": "An increase of the proportion of filtered out sperms.",
                    "label": 0
                },
                {
                    "sent": "Given that we.",
                    "label": 0
                },
                {
                    "sent": "Correctly classify all non sperms.",
                    "label": 0
                },
                {
                    "sent": "An increase from 10% to 75% compared to the one class SVM.",
                    "label": 0
                },
                {
                    "sent": "So to sum up what you should take home with is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have derived a closed form solution for Bayesian clustering that allows arbitrary dependencies of the input dimensions.",
                    "label": 1
                },
                {
                    "sent": "By applying a feature transformation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Making an independence assumption on the transformed space.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "We have presented a. Optimization problem, an algorithm to learn such a feature transformation that minimizes the induced approximation error.",
                    "label": 0
                },
                {
                    "sent": "And finally, our case study has shown that using Bayesian clustering with feature transformation we can greatly increase the proportion of filtered sperms.",
                    "label": 0
                },
                {
                    "sent": "Given that we correctly classify all non sperms.",
                    "label": 0
                },
                {
                    "sent": "Compared to an SVM baseline.",
                    "label": 0
                },
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Provide me answer questions maybe next week.",
                    "label": 0
                },
                {
                    "sent": "We can turn off this.",
                    "label": 0
                },
                {
                    "sent": "Air conditioning for this.",
                    "label": 0
                },
                {
                    "sent": "Blue Yep.",
                    "label": 0
                },
                {
                    "sent": "More download space track and representative.",
                    "label": 0
                },
                {
                    "sent": "I didn't understand.",
                    "label": 0
                },
                {
                    "sent": "Spanish rap.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's a.",
                    "label": 0
                },
                {
                    "sent": "Taking from a commercial email provider and.",
                    "label": 0
                },
                {
                    "sent": "The blacklist we use is from Spamhaus.",
                    "label": 0
                },
                {
                    "sent": "It's manually maintained and we think it's a very representative sample out of all spam's that I sent.",
                    "label": 0
                },
                {
                    "sent": "Stop spamming supposed to Blacklist IP addresses.",
                    "label": 0
                },
                {
                    "sent": "It's a black list of IP addresses.",
                    "label": 0
                },
                {
                    "sent": "It's not depending on the content.",
                    "label": 0
                },
                {
                    "sent": "Why are features intended in the transport space?",
                    "label": 0
                },
                {
                    "sent": "We just make this assumption in order.",
                    "label": 0
                },
                {
                    "sent": "Wife awards number.",
                    "label": 0
                },
                {
                    "sent": "Then assuming independence in original space, because we learn the feature transformation in a way that this assumption hurts us as less as possible.",
                    "label": 0
                },
                {
                    "sent": "Because if we would create an approximation that metrics exactly the true distribution, then we have shown that the independence assumption doesn't hurt at all.",
                    "label": 0
                },
                {
                    "sent": "How many clusters you end up finding?",
                    "label": 0
                },
                {
                    "sent": "In practice?",
                    "label": 0
                },
                {
                    "sent": "It depends on the hyperparameter setting in this setting where we optimize for maximum number of true positives given zero false positives, we get on the order of a few thousands of clusters.",
                    "label": 0
                },
                {
                    "sent": "That's the optimization criterion.",
                    "label": 0
                },
                {
                    "sent": "We derive the optimization problem by minimizing the callback library divergent to the true distribution.",
                    "label": 0
                },
                {
                    "sent": "I have also question.",
                    "label": 0
                },
                {
                    "sent": "District Magistrate You can transformation of the features that seems to be a cool trick.",
                    "label": 0
                },
                {
                    "sent": "Does that also work in general for namespace classifiers first, but that's something I want to look into the future, definitely.",
                    "label": 0
                }
            ]
        }
    }
}