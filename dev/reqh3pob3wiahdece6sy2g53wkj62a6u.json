{
    "id": "reqh3pob3wiahdece6sy2g53wkj62a6u",
    "title": "Learning When to Stop Thinking and Do Something!",
    "info": {
        "author": [
            "Barnab\u00e1s P\u00f3czos, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_poczos_lwts/",
    "segmentation": [
        [
            "This is a joint work with you.",
            "Value to sell grain and not on throat event and I'm bored and bash both search and.",
            "Doesn't can you?",
            "Work.",
            "It's just for the camp."
        ],
        [
            "OK so I will talk about anytime algorithms, especially when we should interrupt anytime algorithms.",
            "Then both the accuracy and both the efficiency are important.",
            "So I will formally define our problems and then I will propose two algorithms.",
            "Both of them are policy search approaches for the problem and the first one will be a policy gradient approach.",
            "We will estimate the gradient of the policy and then we will investigate the quality of the estimated gradient.",
            "And after that it turns out that when we are close to the local minima then even if our estimation is good, that is.",
            "The estimated gradient is small as well.",
            "It can happen that still degrades the estimated gradient points to a wrong direction, and because of this the convergence can be quite slow, so we will overcome this difficulty proposing a stopping rule for preventing this slow convergence.",
            "Another problem with the gradient based approach is that they can easily get stuck in local minima, so we will combine this method with the global optimization method called cross entropy method and we will investigate to problems toy problem simulating Mail sorting and the real face detection problem."
        ],
        [
            "OK, so.",
            "Any times algorithms look like this that you have an algorithm and you can interrupt this algorithm basically at anytime.",
            "But if you spend more times on syncing then the quality of your algorithm will be better.",
            "So for example you want to maximize some reward and.",
            "If you spend more times on thinking on that specific problem, then you can achieve higher reward if you spend more times so.",
            "Specific problems we are interested in can be, for example a Mail sorter.",
            "Then your task is that you have a Mail sorter.",
            "And you have to process the envelopes.",
            "You have to detect the zip codes.",
            "And you have to classify the ZIP codes as well.",
            "So in this case it's important to be efficient, because if you don't classify them as.",
            "In a good way than the males can be dis.",
            "The mayors can be posted to wrong places, which is not good, but on the other hand you should be efficient as is.",
            "I mean, you cannot spend as much time as you want on.",
            "Processing the mayors.",
            "Because then the mayors can be queued up and it's not good as well.",
            "Other problems can be license plate recognition's on Hwy or when you have to detect possible terrorists on airports.",
            "OK, so this is."
        ],
        [
            "The formal definition of our problem, so we assume that we have IID samples denoted by X one X2 and so on.",
            "The serious series of envelopes and on each XD we start the thinking process, which consists of maximum case stages, and these stages give us some responses because all these.",
            "Extracted features.",
            "And we will have a stopping policy denoted by Q in each stages.",
            "So this is actually a distribution, and if it's a binary distribution and it's each policy on a stage K depends on on the actual feature value YTK and if this policy.",
            "If the distribution has value zero, it means that we want to continue thinking, and if we have one from this distribution, then it means that we want to quit thinking and our task is to define a good policy for this problem so.",
            "For this problem we will define a reward, which means that when we decided to quit thinking, then we can make an action.",
            "So we extracted some features and based on these features we can classify the zip codes of the mayors.",
            "This mapping will be denoted by mu.",
            "And.",
            "And each stage will have some Tau random thinking time.",
            "Is that clear the notations?"
        ],
        [
            "OK, so let Lt random variable, then with the demo the stage when we quit, so the total syncing time on instance XD is nothing else, just the sum of the sinking times of the stages.",
            "And let the denote.",
            "The actual action that, but we perform after treating on on the stage Lt.",
            "When we preprocess XD instance and now we can define our ultimate goal.",
            "So our ultimate goal is nothing else.",
            "Just to maximize this objective, which is the expected value of the limousine?",
            "Ferrier of the some of the rewards divided by the all syncing time they spent on these examples.",
            "So we have to maximize this in our policy queue."
        ],
        [
            "And here is the trick that we can use.",
            "So if XT is an IID series then we can see that the rewards as well as the thinking times are I ID as well and because of the law of large numbers then this objective will be nothing else, just the expected rewards divided by the expected thinking times.",
            "And this objective is just much easier.",
            "To handle them the original 1.",
            "OK, so we will use these shorthand notations and with these notations the.",
            "The gradient of our objective will be this guy, so we assume that we have.",
            "That our policy is parameterized by some parameter Theta, and we want to calculate the.",
            "The derivative of the objective with respect to Theta and this will be this theoretical gradient here."
        ],
        [
            "OK, so this is the theoretical gradient we have to estimate this from samples.",
            "So.",
            "There are terms that are quite easy because the expected expected sinking time and the expected reward can be estimated simply by the sample averages.",
            "But we need the gradient of the expected reward and the gradient of the expected time as well, but we can.",
            "Consider this problem as an episodic problem where at the end of the episode we got our rewards and end of the episode is where we read thinking about the actual instance.",
            "So basically we can use the so-called reinforce algorithm for calculating the gradients of this guy and this guy I will show."
        ],
        [
            "Dude, that S how.",
            "So.",
            "Let's give the key is a random variable from the distribution of our stopping policy, and Ltd knows the time.",
            "Then we quit and to is the analysis.",
            "We will introduce these notations as well that Q had TK is just the series of zeros and ones and.",
            "And it's one at that time, then we quit.",
            "For processing this instance and we will define these trajectories denoted by Y~ G. Um?",
            "And let.",
            "80 is just the action that is taken on the instance 60 and we will use these notations as well that is delivered on instance XD is nothing else, just it will be denoted by this expression as well.",
            "That is the Rivard on this trajectory.",
            "So let us know that disregard depends on the first half of the trajectory, then that path and we create and the others.",
            "Do not matter here.",
            "OK, so.",
            "We have to calculate the gradient of the expected reward.",
            "This is our goal and for this let me introduce this notation.",
            "That is F data.",
            "By Tilda is just the density function of the trajectory that we introduced here.",
            "So if you use these notations then the expected reward is can be calculated by this expression and under some mine under some regularity conditions the the derivative and integral operator are interchangeable, interchangeable.",
            "And because of this it turns out that if we want to calculate the gradient of the expected to divert.",
            "Basically we have to calculate the derivative of the logarithm of the density.",
            "OK, so let's see how to calculate the derivative of the logarithm of the density."
        ],
        [
            "So now comes another trick that we can assume that we have a Markov property along these trajectories, which means that the features on the next stage depends on the features of the actual stages, which basically means that we continue thinking that we continue the syncing from that stage where we we are right now.",
            "And because of this, the.",
            "So we have this.",
            "Nice expression, which means that the logarithm of the density is just the sum of some simple terms and then its gradient can be calculated in this close form."
        ],
        [
            "OK, so now we calculated the gradient of this guy and because of this we can calculate.",
            "The the gradient of the expected, diverse and expected time as well.",
            "And now we have estimate.",
            "We have unbiased estimation for these guys.",
            "So.",
            "We have these."
        ],
        [
            "Unbiased estimators.",
            "And this is the theoretical gradient we want to estimate.",
            "We have unbiased estimators for each terms here, and if we put the pieces together then we will have an estimator for the gradient.",
            "OK, so."
        ],
        [
            "So.",
            "We had an objective.",
            "We calculated its gradient and now we can.",
            "Propose an algorithm which is nothing else, just gradient ascent.",
            "The question the first obvious question.",
            "That's OK. We proposed an.",
            "We estimated this gradient, but how good is this estimation and using having bounds?",
            "We can."
        ],
        [
            "We can propose a theorem, which means that when we have enough samples trajectories, then the.",
            "The.",
            "We can bound the probability that the empirical gradient will be close to the true."
        ],
        [
            "Radiant.",
            "OK, and a problem with this method is that when we are close to the global optimal then it can happen that even if the empirical gradient is that close to the true gradient, they can point to wrong to the opposite directions and which is not good for us but.",
            "If we bound this guy by the.",
            "This G the norm of G over half.",
            "Then we can.",
            "It's quite easy to see then in this case the.",
            "Estimated gradient via point to good direction and if we do daily and descent then our algorithm will increase the our objective function.",
            "Oh so.",
            "Now."
        ],
        [
            "Problem with this gradient methods that they can get stuck easily in local minima.",
            "So we modified this algorithm with this so called cross entropy method, which is nothing else.",
            "Just we generated some random parameters from a normal Dist.",
            "Normal distribution.",
            "We evaluated this parameters.",
            "And then we took the best.",
            "10% parameters and we modified this only the best 10% parameters using the previously proposed gradient methods.",
            "Then we calculated it's mean and covariance again and we repeated this whole procedure."
        ],
        [
            "And now I would like to show an application on face detection.",
            "So here the task is that we have an image and we have to find.",
            "The faces."
        ],
        [
            "On this image actually what we did that simplified this problem and our task was only that given a subwindow from this image, we just have to.",
            "Decide whether it's face or not.",
            "OK, and we use the original Viola Jones algorithm for this, which is which looked like this that.",
            "So it's a.",
            "It's actually, it's an anytime algorithm which consists of 20 two stages and each stage.",
            "The algorithm calculates some features and if the value of these features are larger than.",
            "For example, in the first stage they are larger than Alpha and then the algorithm proceeds on the next stage, otherwise it's quits and says that that sub Indo is not a phase, and so on.",
            "And it repeats this.",
            "Procedure until it.",
            "It reaches the the last stage and if at the last stage the feature values are larger than the parameters on the last stage, then the algorithm will say that that sub window is a face, otherwise it's classifies that some video subwindow as not to face.",
            "The problem with this approach that you have to.",
            "To.",
            "That you have to reach the very last stage to be able to say that that face that that sub window is."
        ],
        [
            "Phase so we modified this structure in this way.",
            "That instead of this 22 parameters, we had 44 parameters and instead of this one sided decision in each stage we had two sided decisions with Alpha and beta parameters.",
            "And if the feature values.",
            "And this stage well less than Alpha.",
            "Then we said that it's not face if the feature values on that on the first stage were larger than a parameter beta, then we said it's a fish.",
            "Otherwise, we say that we at this stage we don't know whether it's face or not and we continue on another.",
            "Stage.",
            "Oh"
        ],
        [
            "And here are our results.",
            "So you can see the value regions algorithm the our proposed algorithm, the expected reward, the expected stage number, the true positive rate, and the false positive rate of the classification, and you can see that our algorithm achieved higher expected reward.",
            "Higher true positive rate than smaller false positive rate.",
            "The original Viola Jones algorithm.",
            "This random algorithm is nothing else, just we tried 100 random parameters and we were interested what results we can get.",
            "But the interesting point here that our algorithm used much far far.",
            "Smaller number of stages than the original biologists algorithm, which is important if you want to do the face."
        ],
        [
            "Fast.",
            "In this figure I just show the wrong domain which is.",
            "The.",
            "We generated lots of random parameters and we evaluated the performance on the 1st positive to positive.",
            "Coordinate system here you can see the biology, the performance of the Viola Jones algorithm and here this is our algorithm.",
            "So.",
            "So."
        ],
        [
            "I think I finish now, so thanks for your attention.",
            "So we investigated.",
            "And then we should stop thinking about that specific task when we consider both the quality of the solution and the cost of.",
            "Thinking so if you have questions, please ask and thanks for your attention.",
            "Yes.",
            "Expectation over Asia."
        ],
        [
            "Expectational yeah, but now we have IID samples.",
            "OK, and because of this you can see that this guy equals this guy.",
            "And this guy you to the law of large number.",
            "It's approaching the expected value of this right and this guy is converging to.",
            "This guy has valid but you are right.",
            "It's generally it's not true.",
            "And this is the trick that we could use here.",
            "Yes.",
            "So can you say Sylvia, High level Huai Huai this problem is different and we why we can't treat it as like a standard reinforcement learning model?",
            "So now you have two goals.",
            "I think that's the crucial part here.",
            "So first you want to maximize some reward and the next you don't.",
            "You have a cost function cost as well, so you want to maximize something and minimize something as well.",
            "It's not possible to treat it as average reward.",
            "That would be different, so that would be interesting to investigate that, but I'm not sure how to do that.",
            "This is average what problem, but it has some specific properties that you try to take advantage of it and this is what you do.",
            "So if you don't do this, then you would try to come up like you tried without this trip.",
            "Seem to be easy, yeah, so it's it's easier to just say.",
            "It's a special place and you could treat it in the general case, but then you would expect to do better if you can take advantage of this special.",
            "On the top of it, I don't really know how to treat the generic using this in this setting.",
            "So in this case we could have some theorems on the quality of the estimated gradient.",
            "On the other case, I don't know whether we could do those things.",
            "One quick question.",
            "This will lessen the assumption of IID or the X variables.",
            "Where do these appear in the vision application?",
            "Individual application the X the images.",
            "And we assume that the next image is independent of the previous image due process name, yeah?",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a joint work with you.",
                    "label": 0
                },
                {
                    "sent": "Value to sell grain and not on throat event and I'm bored and bash both search and.",
                    "label": 0
                },
                {
                    "sent": "Doesn't can you?",
                    "label": 0
                },
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "It's just for the camp.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I will talk about anytime algorithms, especially when we should interrupt anytime algorithms.",
                    "label": 0
                },
                {
                    "sent": "Then both the accuracy and both the efficiency are important.",
                    "label": 0
                },
                {
                    "sent": "So I will formally define our problems and then I will propose two algorithms.",
                    "label": 0
                },
                {
                    "sent": "Both of them are policy search approaches for the problem and the first one will be a policy gradient approach.",
                    "label": 0
                },
                {
                    "sent": "We will estimate the gradient of the policy and then we will investigate the quality of the estimated gradient.",
                    "label": 1
                },
                {
                    "sent": "And after that it turns out that when we are close to the local minima then even if our estimation is good, that is.",
                    "label": 0
                },
                {
                    "sent": "The estimated gradient is small as well.",
                    "label": 1
                },
                {
                    "sent": "It can happen that still degrades the estimated gradient points to a wrong direction, and because of this the convergence can be quite slow, so we will overcome this difficulty proposing a stopping rule for preventing this slow convergence.",
                    "label": 0
                },
                {
                    "sent": "Another problem with the gradient based approach is that they can easily get stuck in local minima, so we will combine this method with the global optimization method called cross entropy method and we will investigate to problems toy problem simulating Mail sorting and the real face detection problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Any times algorithms look like this that you have an algorithm and you can interrupt this algorithm basically at anytime.",
                    "label": 0
                },
                {
                    "sent": "But if you spend more times on syncing then the quality of your algorithm will be better.",
                    "label": 0
                },
                {
                    "sent": "So for example you want to maximize some reward and.",
                    "label": 0
                },
                {
                    "sent": "If you spend more times on thinking on that specific problem, then you can achieve higher reward if you spend more times so.",
                    "label": 0
                },
                {
                    "sent": "Specific problems we are interested in can be, for example a Mail sorter.",
                    "label": 0
                },
                {
                    "sent": "Then your task is that you have a Mail sorter.",
                    "label": 1
                },
                {
                    "sent": "And you have to process the envelopes.",
                    "label": 0
                },
                {
                    "sent": "You have to detect the zip codes.",
                    "label": 0
                },
                {
                    "sent": "And you have to classify the ZIP codes as well.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's important to be efficient, because if you don't classify them as.",
                    "label": 1
                },
                {
                    "sent": "In a good way than the males can be dis.",
                    "label": 0
                },
                {
                    "sent": "The mayors can be posted to wrong places, which is not good, but on the other hand you should be efficient as is.",
                    "label": 0
                },
                {
                    "sent": "I mean, you cannot spend as much time as you want on.",
                    "label": 0
                },
                {
                    "sent": "Processing the mayors.",
                    "label": 0
                },
                {
                    "sent": "Because then the mayors can be queued up and it's not good as well.",
                    "label": 0
                },
                {
                    "sent": "Other problems can be license plate recognition's on Hwy or when you have to detect possible terrorists on airports.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The formal definition of our problem, so we assume that we have IID samples denoted by X one X2 and so on.",
                    "label": 1
                },
                {
                    "sent": "The serious series of envelopes and on each XD we start the thinking process, which consists of maximum case stages, and these stages give us some responses because all these.",
                    "label": 0
                },
                {
                    "sent": "Extracted features.",
                    "label": 0
                },
                {
                    "sent": "And we will have a stopping policy denoted by Q in each stages.",
                    "label": 0
                },
                {
                    "sent": "So this is actually a distribution, and if it's a binary distribution and it's each policy on a stage K depends on on the actual feature value YTK and if this policy.",
                    "label": 0
                },
                {
                    "sent": "If the distribution has value zero, it means that we want to continue thinking, and if we have one from this distribution, then it means that we want to quit thinking and our task is to define a good policy for this problem so.",
                    "label": 0
                },
                {
                    "sent": "For this problem we will define a reward, which means that when we decided to quit thinking, then we can make an action.",
                    "label": 0
                },
                {
                    "sent": "So we extracted some features and based on these features we can classify the zip codes of the mayors.",
                    "label": 0
                },
                {
                    "sent": "This mapping will be denoted by mu.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And each stage will have some Tau random thinking time.",
                    "label": 0
                },
                {
                    "sent": "Is that clear the notations?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let Lt random variable, then with the demo the stage when we quit, so the total syncing time on instance XD is nothing else, just the sum of the sinking times of the stages.",
                    "label": 0
                },
                {
                    "sent": "And let the denote.",
                    "label": 0
                },
                {
                    "sent": "The actual action that, but we perform after treating on on the stage Lt.",
                    "label": 0
                },
                {
                    "sent": "When we preprocess XD instance and now we can define our ultimate goal.",
                    "label": 0
                },
                {
                    "sent": "So our ultimate goal is nothing else.",
                    "label": 1
                },
                {
                    "sent": "Just to maximize this objective, which is the expected value of the limousine?",
                    "label": 0
                },
                {
                    "sent": "Ferrier of the some of the rewards divided by the all syncing time they spent on these examples.",
                    "label": 0
                },
                {
                    "sent": "So we have to maximize this in our policy queue.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is the trick that we can use.",
                    "label": 0
                },
                {
                    "sent": "So if XT is an IID series then we can see that the rewards as well as the thinking times are I ID as well and because of the law of large numbers then this objective will be nothing else, just the expected rewards divided by the expected thinking times.",
                    "label": 0
                },
                {
                    "sent": "And this objective is just much easier.",
                    "label": 0
                },
                {
                    "sent": "To handle them the original 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so we will use these shorthand notations and with these notations the.",
                    "label": 0
                },
                {
                    "sent": "The gradient of our objective will be this guy, so we assume that we have.",
                    "label": 1
                },
                {
                    "sent": "That our policy is parameterized by some parameter Theta, and we want to calculate the.",
                    "label": 0
                },
                {
                    "sent": "The derivative of the objective with respect to Theta and this will be this theoretical gradient here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the theoretical gradient we have to estimate this from samples.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are terms that are quite easy because the expected expected sinking time and the expected reward can be estimated simply by the sample averages.",
                    "label": 1
                },
                {
                    "sent": "But we need the gradient of the expected reward and the gradient of the expected time as well, but we can.",
                    "label": 0
                },
                {
                    "sent": "Consider this problem as an episodic problem where at the end of the episode we got our rewards and end of the episode is where we read thinking about the actual instance.",
                    "label": 0
                },
                {
                    "sent": "So basically we can use the so-called reinforce algorithm for calculating the gradients of this guy and this guy I will show.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dude, that S how.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's give the key is a random variable from the distribution of our stopping policy, and Ltd knows the time.",
                    "label": 0
                },
                {
                    "sent": "Then we quit and to is the analysis.",
                    "label": 0
                },
                {
                    "sent": "We will introduce these notations as well that Q had TK is just the series of zeros and ones and.",
                    "label": 0
                },
                {
                    "sent": "And it's one at that time, then we quit.",
                    "label": 0
                },
                {
                    "sent": "For processing this instance and we will define these trajectories denoted by Y~ G. Um?",
                    "label": 0
                },
                {
                    "sent": "And let.",
                    "label": 0
                },
                {
                    "sent": "80 is just the action that is taken on the instance 60 and we will use these notations as well that is delivered on instance XD is nothing else, just it will be denoted by this expression as well.",
                    "label": 0
                },
                {
                    "sent": "That is the Rivard on this trajectory.",
                    "label": 0
                },
                {
                    "sent": "So let us know that disregard depends on the first half of the trajectory, then that path and we create and the others.",
                    "label": 0
                },
                {
                    "sent": "Do not matter here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We have to calculate the gradient of the expected reward.",
                    "label": 0
                },
                {
                    "sent": "This is our goal and for this let me introduce this notation.",
                    "label": 0
                },
                {
                    "sent": "That is F data.",
                    "label": 0
                },
                {
                    "sent": "By Tilda is just the density function of the trajectory that we introduced here.",
                    "label": 0
                },
                {
                    "sent": "So if you use these notations then the expected reward is can be calculated by this expression and under some mine under some regularity conditions the the derivative and integral operator are interchangeable, interchangeable.",
                    "label": 0
                },
                {
                    "sent": "And because of this it turns out that if we want to calculate the gradient of the expected to divert.",
                    "label": 0
                },
                {
                    "sent": "Basically we have to calculate the derivative of the logarithm of the density.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see how to calculate the derivative of the logarithm of the density.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now comes another trick that we can assume that we have a Markov property along these trajectories, which means that the features on the next stage depends on the features of the actual stages, which basically means that we continue thinking that we continue the syncing from that stage where we we are right now.",
                    "label": 0
                },
                {
                    "sent": "And because of this, the.",
                    "label": 0
                },
                {
                    "sent": "So we have this.",
                    "label": 0
                },
                {
                    "sent": "Nice expression, which means that the logarithm of the density is just the sum of some simple terms and then its gradient can be calculated in this close form.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we calculated the gradient of this guy and because of this we can calculate.",
                    "label": 0
                },
                {
                    "sent": "The the gradient of the expected, diverse and expected time as well.",
                    "label": 0
                },
                {
                    "sent": "And now we have estimate.",
                    "label": 0
                },
                {
                    "sent": "We have unbiased estimation for these guys.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have these.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Unbiased estimators.",
                    "label": 0
                },
                {
                    "sent": "And this is the theoretical gradient we want to estimate.",
                    "label": 0
                },
                {
                    "sent": "We have unbiased estimators for each terms here, and if we put the pieces together then we will have an estimator for the gradient.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We had an objective.",
                    "label": 0
                },
                {
                    "sent": "We calculated its gradient and now we can.",
                    "label": 0
                },
                {
                    "sent": "Propose an algorithm which is nothing else, just gradient ascent.",
                    "label": 0
                },
                {
                    "sent": "The question the first obvious question.",
                    "label": 0
                },
                {
                    "sent": "That's OK. We proposed an.",
                    "label": 0
                },
                {
                    "sent": "We estimated this gradient, but how good is this estimation and using having bounds?",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can propose a theorem, which means that when we have enough samples trajectories, then the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "We can bound the probability that the empirical gradient will be close to the true.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Radiant.",
                    "label": 0
                },
                {
                    "sent": "OK, and a problem with this method is that when we are close to the global optimal then it can happen that even if the empirical gradient is that close to the true gradient, they can point to wrong to the opposite directions and which is not good for us but.",
                    "label": 0
                },
                {
                    "sent": "If we bound this guy by the.",
                    "label": 0
                },
                {
                    "sent": "This G the norm of G over half.",
                    "label": 0
                },
                {
                    "sent": "Then we can.",
                    "label": 0
                },
                {
                    "sent": "It's quite easy to see then in this case the.",
                    "label": 0
                },
                {
                    "sent": "Estimated gradient via point to good direction and if we do daily and descent then our algorithm will increase the our objective function.",
                    "label": 0
                },
                {
                    "sent": "Oh so.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem with this gradient methods that they can get stuck easily in local minima.",
                    "label": 0
                },
                {
                    "sent": "So we modified this algorithm with this so called cross entropy method, which is nothing else.",
                    "label": 1
                },
                {
                    "sent": "Just we generated some random parameters from a normal Dist.",
                    "label": 0
                },
                {
                    "sent": "Normal distribution.",
                    "label": 0
                },
                {
                    "sent": "We evaluated this parameters.",
                    "label": 0
                },
                {
                    "sent": "And then we took the best.",
                    "label": 0
                },
                {
                    "sent": "10% parameters and we modified this only the best 10% parameters using the previously proposed gradient methods.",
                    "label": 0
                },
                {
                    "sent": "Then we calculated it's mean and covariance again and we repeated this whole procedure.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now I would like to show an application on face detection.",
                    "label": 0
                },
                {
                    "sent": "So here the task is that we have an image and we have to find.",
                    "label": 0
                },
                {
                    "sent": "The faces.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this image actually what we did that simplified this problem and our task was only that given a subwindow from this image, we just have to.",
                    "label": 0
                },
                {
                    "sent": "Decide whether it's face or not.",
                    "label": 0
                },
                {
                    "sent": "OK, and we use the original Viola Jones algorithm for this, which is which looked like this that.",
                    "label": 0
                },
                {
                    "sent": "So it's a.",
                    "label": 0
                },
                {
                    "sent": "It's actually, it's an anytime algorithm which consists of 20 two stages and each stage.",
                    "label": 0
                },
                {
                    "sent": "The algorithm calculates some features and if the value of these features are larger than.",
                    "label": 0
                },
                {
                    "sent": "For example, in the first stage they are larger than Alpha and then the algorithm proceeds on the next stage, otherwise it's quits and says that that sub Indo is not a phase, and so on.",
                    "label": 0
                },
                {
                    "sent": "And it repeats this.",
                    "label": 0
                },
                {
                    "sent": "Procedure until it.",
                    "label": 0
                },
                {
                    "sent": "It reaches the the last stage and if at the last stage the feature values are larger than the parameters on the last stage, then the algorithm will say that that sub window is a face, otherwise it's classifies that some video subwindow as not to face.",
                    "label": 0
                },
                {
                    "sent": "The problem with this approach that you have to.",
                    "label": 0
                },
                {
                    "sent": "To.",
                    "label": 0
                },
                {
                    "sent": "That you have to reach the very last stage to be able to say that that face that that sub window is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Phase so we modified this structure in this way.",
                    "label": 0
                },
                {
                    "sent": "That instead of this 22 parameters, we had 44 parameters and instead of this one sided decision in each stage we had two sided decisions with Alpha and beta parameters.",
                    "label": 0
                },
                {
                    "sent": "And if the feature values.",
                    "label": 0
                },
                {
                    "sent": "And this stage well less than Alpha.",
                    "label": 0
                },
                {
                    "sent": "Then we said that it's not face if the feature values on that on the first stage were larger than a parameter beta, then we said it's a fish.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, we say that we at this stage we don't know whether it's face or not and we continue on another.",
                    "label": 0
                },
                {
                    "sent": "Stage.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here are our results.",
                    "label": 0
                },
                {
                    "sent": "So you can see the value regions algorithm the our proposed algorithm, the expected reward, the expected stage number, the true positive rate, and the false positive rate of the classification, and you can see that our algorithm achieved higher expected reward.",
                    "label": 0
                },
                {
                    "sent": "Higher true positive rate than smaller false positive rate.",
                    "label": 1
                },
                {
                    "sent": "The original Viola Jones algorithm.",
                    "label": 1
                },
                {
                    "sent": "This random algorithm is nothing else, just we tried 100 random parameters and we were interested what results we can get.",
                    "label": 0
                },
                {
                    "sent": "But the interesting point here that our algorithm used much far far.",
                    "label": 0
                },
                {
                    "sent": "Smaller number of stages than the original biologists algorithm, which is important if you want to do the face.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fast.",
                    "label": 0
                },
                {
                    "sent": "In this figure I just show the wrong domain which is.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "We generated lots of random parameters and we evaluated the performance on the 1st positive to positive.",
                    "label": 0
                },
                {
                    "sent": "Coordinate system here you can see the biology, the performance of the Viola Jones algorithm and here this is our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think I finish now, so thanks for your attention.",
                    "label": 0
                },
                {
                    "sent": "So we investigated.",
                    "label": 0
                },
                {
                    "sent": "And then we should stop thinking about that specific task when we consider both the quality of the solution and the cost of.",
                    "label": 1
                },
                {
                    "sent": "Thinking so if you have questions, please ask and thanks for your attention.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Expectation over Asia.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Expectational yeah, but now we have IID samples.",
                    "label": 0
                },
                {
                    "sent": "OK, and because of this you can see that this guy equals this guy.",
                    "label": 0
                },
                {
                    "sent": "And this guy you to the law of large number.",
                    "label": 0
                },
                {
                    "sent": "It's approaching the expected value of this right and this guy is converging to.",
                    "label": 0
                },
                {
                    "sent": "This guy has valid but you are right.",
                    "label": 0
                },
                {
                    "sent": "It's generally it's not true.",
                    "label": 0
                },
                {
                    "sent": "And this is the trick that we could use here.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So can you say Sylvia, High level Huai Huai this problem is different and we why we can't treat it as like a standard reinforcement learning model?",
                    "label": 0
                },
                {
                    "sent": "So now you have two goals.",
                    "label": 0
                },
                {
                    "sent": "I think that's the crucial part here.",
                    "label": 0
                },
                {
                    "sent": "So first you want to maximize some reward and the next you don't.",
                    "label": 0
                },
                {
                    "sent": "You have a cost function cost as well, so you want to maximize something and minimize something as well.",
                    "label": 0
                },
                {
                    "sent": "It's not possible to treat it as average reward.",
                    "label": 0
                },
                {
                    "sent": "That would be different, so that would be interesting to investigate that, but I'm not sure how to do that.",
                    "label": 0
                },
                {
                    "sent": "This is average what problem, but it has some specific properties that you try to take advantage of it and this is what you do.",
                    "label": 0
                },
                {
                    "sent": "So if you don't do this, then you would try to come up like you tried without this trip.",
                    "label": 0
                },
                {
                    "sent": "Seem to be easy, yeah, so it's it's easier to just say.",
                    "label": 0
                },
                {
                    "sent": "It's a special place and you could treat it in the general case, but then you would expect to do better if you can take advantage of this special.",
                    "label": 0
                },
                {
                    "sent": "On the top of it, I don't really know how to treat the generic using this in this setting.",
                    "label": 0
                },
                {
                    "sent": "So in this case we could have some theorems on the quality of the estimated gradient.",
                    "label": 1
                },
                {
                    "sent": "On the other case, I don't know whether we could do those things.",
                    "label": 0
                },
                {
                    "sent": "One quick question.",
                    "label": 0
                },
                {
                    "sent": "This will lessen the assumption of IID or the X variables.",
                    "label": 0
                },
                {
                    "sent": "Where do these appear in the vision application?",
                    "label": 0
                },
                {
                    "sent": "Individual application the X the images.",
                    "label": 0
                },
                {
                    "sent": "And we assume that the next image is independent of the previous image due process name, yeah?",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}