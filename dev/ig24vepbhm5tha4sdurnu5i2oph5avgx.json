{
    "id": "ig24vepbhm5tha4sdurnu5i2oph5avgx",
    "title": "PAC-Bayes Theory in Supervised Learning",
    "info": {
        "author": [
            "Fran\u00e7ois Laviolette, Universit\u00e9 Laval"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_laviolette_pbts/",
    "segmentation": [
        [
            "OK, so um thanks for coming to this workshop.",
            "I think it's it's an interesting.",
            "Topics if you I really think that Backbase will will have.",
            "A second life in the future years for I would say for three reasons, maybe.",
            "Maybe other people that will make presentation can add other reason, but I I think backbase approach is interesting for first reason is because of its simplicity.",
            "It's something that is.",
            "The proof are not difficult if you compare for example with the ramaker approach, where when you want to do something it's very difficult, so I think this is a strength for the back based approach, because then we can really deeply understand what's going on and generalizing very values direction easily.",
            "The second reason I would say.",
            "Hey, the reason why I think that PAC Bayes will will have more important in future is I think we will achieve better in the understanding on the Bayesian approach.",
            "'cause the beige and is based on the.",
            "Some kind of magic from my point of view, because it's simply that they have something at Prairie and then you use the Bayes rule to output what is the best situation.",
            "But what this means from a frequentist it's something that is not clear.",
            "I think with back based theory we can achieve better understanding.",
            "Matthias Seeger, for example in Stasis, says make some progress on that.",
            "I think more is to come, and the third reason why I think.",
            "Basically it will be more important the future is we try recently to make algorithm out of PAC Bayes bounds simply by minimizing bounds, and it works quite well.",
            "In fact, we re discover some existing algorithm SVM, Ridge regression, regularizer, the boost all those algorithm can be view as minimizing some kind of pack basement.",
            "So for those two reasons, I think that's interesting.",
            "I will.",
            "I will try to talk in my in my tutorial of two of those three, so the simplicity of the map.",
            "So I had to do a tutorial basically, so I will try to show out and proof are working on.",
            "And why are the ideas I will not have the time to explain exactly all the duality that is going on in here, but I expect someone in the audience that will do.",
            "Wendy will be there.",
            "Time to talk OK and I will also show you how we can retrieve existing algorithm simply by minimizing pack basement OK?",
            "So."
        ],
        [
            "So this is the outline so."
        ],
        [
            "Look at the mat and then look at what we can do if we want to make algorithm out of."
        ],
        [
            "Bounce."
        ],
        [
            "OK, rapidly."
        ],
        [
            "Yeah, I suppose that something is.",
            "I will restrict myself to supervised learning an basically to binary classification.",
            "The easy case, I think it's already my hour will be already taken fully by this way.",
            "Of course everything is possible to generalize to other setting to regression to structure output to ranking and so on.",
            "So the true risk I will write for a classifier.",
            "HI will write RFH and the empirical risk based on the training set S are subsets of age.",
            "Of course, it's the usual definition that I will look, but in the pack based theory.",
            "Basically what we want to achieve, its a good performance, not on the set of learners that we have, but on the majority vote of them.",
            "And the trick is basically to define the good weighting of the majority vote.",
            "In such that you will achieve the best possible outcome.",
            "The best performance OK?",
            "This classifier is also it's a majority vote.",
            "Everybody you see that.",
            "So you you look you give some weight to every classifier and then use some an if most of them are thinking plus one then the sign with output plus one and if most of them think minus one then the sign will output minus one.",
            "For simplicity if the majority vote is even we can say that.",
            "It's wrong by definition, or we can sell flip a coin, or maybe possibility OK."
        ],
        [
            "So the idea is that so we have a bunch of of classifier we call learners or voters and we look for majority vote.",
            "All of them OK."
        ],
        [
            "The problem of the pack based approach, I think the main problem that he had is even if we are interested in the band retrieval classifier.",
            "The bound that we can consume."
        ],
        [
            "Fact is, on the Gibbs classifier which is."
        ],
        [
            "Acoustic classifier."
        ],
        [
            "An hour together define.",
            "For a given level X, the Gibbs classifier will draw according to QA classifier into the set and it will be that classifier only that will classify it.",
            "If you give another hex then according to Q we find a Nook another classifier, another learner from the Class H and it's this learner only that will classify it right?",
            "So there's some link between.",
            "The gifts and the base, but the base is a deterministic classifier and the Gibbs is a stochastic classifier.",
            "You give twice the same input.",
            "He can give you different outputs.",
            "OK. Of course the risk of Gibbs classifier is is the average of the risk of the voters, because it's exactly what you do.",
            "And the same thing for the M."
        ],
        [
            "Typical estimate OK?"
        ],
        [
            "Unk"
        ],
        [
            "So I was, I tell you, we are interested in the majority vote.",
            "But the PAC Bayes bound will gives us something about the Gibbs classifier.",
            "There is a link between those two.",
            "Basically the links is the Bayes risk is never bigger than twice the Gibbs risk.",
            "You can almost reduce under margin assumption.",
            "You can reduce this factor of two almost to one, but nevertheless it's bad, right?",
            "Why this is bad?",
            "I will discuss this later on, but you can suppose, for example, that the voters are very weak.",
            "Like weak learner in boosting for example.",
            "So the average of all those guys will be very weak, so you will end up with a very bad Gibbs risk.",
            "But you know that the majority vote that go out of the boosting because the boosting is a kind of majority.",
            "Vote is usually very strong, so this compression is not necessarily interesting.",
            "We see that it will be interesting in the case where all the voters are a large part of them are good, are very good.",
            "Then we will put all the weight on those.",
            "Good ones and we will have a good bound here and there for a good bound for the base.",
            "But it's an indirect approach, OK?",
            "Another thing that is important in fact based theory and it makes use of a prior distribution that you you must define.",
            "So the welcoming will be at the end of the session.",
            "OK, so the PAC Bayes use what we call a prior distribution on the set of the classifier.",
            "Pay attention, it's not the prior on the on on the data that you want to look at.",
            "It's on the set of the classifier that you have.",
            "It's your apriori knowledge of which classifier is good and which is not.",
            "If you have no knowledge at all, you can use the uniform, for example.",
            "But if you know something.",
            "About for for some reason that some classifier has a better chance to be good, then you will put more weight appear right?",
            "But you do have to do that before seeing the data OK."
        ],
        [
            "Anne.",
            "Uh.",
            "There is bound will.",
            "Will most of the time will make appear what we call the callback library divergents because you have a prior distribution on H. Ann, you have it before seeing the data.",
            "Then you look at the data and you want to make a good majority vote out of the data.",
            "So you will try to find it posterior distribution Q that will be have a good majority vote basically.",
            "And if it's very similar to the prior, then you have very good chance to have good bounds, but if it's very different then you will have.",
            "Abound that will be less tight.",
            "OK.",
            "So everything is fine.",
            "I'm not sure if it's triviality for you or not OK, so OK."
        ],
        [
            "So here is the theorem that I will start with OK.",
            "I will just cite Tolkien Ann Langford.",
            "This is some kind of theorem that is is a theorem from which we can derive most of the known pacbase bound.",
            "The proof of this theorem is very simple, that's why I presented you here and I will show you how we can do it.",
            "And since the proof is simple, since the proof is simple, we will be able to identify.",
            "Where are the things where we can work and what is going on where?",
            "Why is something works and something is not working, etc.",
            "OK, maybe just make an interpretation of it.",
            "OK, so.",
            "This you can see it as a similarity notion, distance or divergent.",
            "OK, so.",
            "You will do the empirical risk that you observe will be.",
            "Close to your true risk of the Gibbs, of course.",
            "Provided that you posterior and your prior are quite similar, everybody know the combat library divergent its variance between distribution plus some.",
            "Weird term here.",
            "OK.",
            "It's OK this time is not usable as it is.",
            "It's by the way this term is true even if we don't have the IID assumption an and so on.",
            "OK, but this theorem is not usable as it is because we can in general.",
            "Not calculate what is the value of that guy.",
            "And for example, if it's not IID, this can be very big.",
            "But we will see that if it's high ID then the empirical risk will be will follow a binomial distribution and probability.",
            "The parameter of the binomial distribution of this random variable will be exactly the true risk and because of that we will be able to.",
            "Calculate exactly what is this term in some situation, not all.",
            "OK.",
            "But this is basically the theorem.",
            "OK."
        ],
        [
            "In fact, we have a bit more general theorem.",
            "It's not necessary to compare the empirical Gibbs risk to the true Gibbs risk.",
            "You can compare any two function on ash on age and you so you can compare the average value of A to the average value of B. ANRB can depend on the training set S it's not a problem, OK?",
            "And also here you in the present slide you have one over him.",
            "You can if you want to relax this part, you can have a better constant M If you want.",
            "OK, I won't go into this part.",
            "I will restrain myself to gives Rex basically, but this is something that is exactly the same proof that I will show you, but I will not prove one.",
            "Plus I will prove term one on the preceding slide OK?"
        ],
        [
            "Everything is fine.",
            "OK, So what are the proof is going?",
            "We consider a render weird random variable, which is the expected value according to P of the exponential of the product of M and the distance between the empirical and the true risk.",
            "It's not so weird.",
            "OK, it's basically what we call a lap last transform or moment generating function of the random variable.",
            "That is here OK.",
            "So it's not something that is appearing from anywhere.",
            "OK, it's work quite well.",
            "Is it essential that we start with that?",
            "No, but we didn't find yet better things.",
            "I will say basically because it's a Laplace transform and there are some strong duality theorem based on convexity.",
            "Oh yeah, D has to be convex.",
            "That makes thing works.",
            "I won't go into this duality.",
            "I hope someone else might see something, but I won't have time.",
            "OK, so since it's a random variable and it's not negative, we can apply Markov.",
            "OK, and Markov will say, well, you random variable is smaller than equal, then one over Delta it's mean.",
            "With probability at least one minus Delta."
        ],
        [
            "OK. And then.",
            "You take the logarithm both side.",
            "OK, can you transfer?",
            "This is an expectation on P. You transfer the expected to do nothing P on an expectation on Q.",
            "Simply by multiplying by P /, Q right?",
            "And this is a very important trick, because this means that the bound that I will obtain at the end will be simultaneously valid for all Q.",
            "Not only for one.",
            "For the one that will be output by my algorithm, for example, no for all possible Q simultaneously, right?",
            "Because it was true for my prior and this is simply an equivalence mathematical equivalence.",
            "OK, so."
        ],
        [
            "Next step, we exploit the fact that the longer it is account function to put the expectation out.",
            "An yeah, of course this is Jensen inequality."
        ],
        [
            "And then what we do is we realize that.",
            "This basically.",
            "Is minus Nickelback library divergent?",
            "That's why the callback library versions appear.",
            "And basically you can see that it is really related to the fact that the random variable we start with was the last transform.",
            "That's why we obtain the callback library divergent with another veranda variable that we start with will obtain something else.",
            "Maybe it will be better, but we didn't find veritex yet.",
            "OK and.",
            "So if we rewrite this term, we obtain minus the callback library and of course the log of the exponential cancel, and so we we obtained almost what we want."
        ],
        [
            "So what is going next?",
            "We have supposed that D is convex.",
            "OK, so since D is convex, you can put the exponent the expectation inside.",
            "Again, we use what we call Jensen inequality.",
            "Then we obtain that."
        ],
        [
            "And bingo, that's it.",
            "Yes.",
            "Thank you both have full support image.",
            "Uh, no, but we have to take care about the fact that the P that the support of P must include this part of Q.",
            "But this is it, but you can deal without this fact.",
            "But yeah, this is something that you have to take care.",
            "What?",
            "Yep.",
            "But OK, everything is fine.",
            "I succeed in convincing Q that pack based bound is easy.",
            "So my first task is is obtained."
        ],
        [
            "But in fact we don't have finished.",
            "As I tell you this term.",
            "Is something that we cannot evaluate in general the expectation on all possible training set not only the one that we've seen?",
            "Of the expectation on the prior of some weird's function.",
            "OK so."
        ],
        [
            "Can we deal with that?",
            "So sorry David.",
            "I start with the Seegars bound, but the first one was too.",
            "I will talk about rapidly to yours, but I start with the one that is basically the Titus one.",
            "An so Segers simply it didn't prove it the way I show you.",
            "OK, it was a totally different approach based on financial legend, duality and things like that.",
            "But basically it choose for the convex function, the small KL divergent.",
            "What is the small KL divergences?",
            "Basically this function which correspond to the KL divergent between binary Bernoulli distribution.",
            "Whyf because of the duality stuff, I don't want to talk about.",
            "OK, This is why he found this right.",
            "Brilliant ideas.",
            "OK, and it didn't exactly obtain this.",
            "Because instead of making the art calculation that I will show you it just take what we call.",
            "Concentration inequality OK, but basically"
        ],
        [
            "The proof is quite simple again.",
            "Oh yeah, what is Zita FM.",
            "It's a mess, but it's something that is basically two 2 * sqrt M. Roughly OK, Ann, but in the in the paper of Seger, what we see is N + 1 because this is very easy to achieve.",
            "And basically."
        ],
        [
            "Well, what is this small Cal?",
            "Is some function that does look like that that will have two vertical accepted 1 zero and one on one and we have it at the minimum at the value of the empirical risk right?",
            "And So what you have is the small KL is less on equal then blah blah blah.",
            "So this here is the value of blah blah blah.",
            "You know that if the empirical basis .1 the true risk will probability 1 minus Delta at least will be.",
            "Between those"
        ],
        [
            "True value.",
            "Understood.",
            "OK. Whitework"
        ],
        [
            "So well.",
            "Basically because if you just make the calculation.",
            "Yeah, if you replace D by by the small CLN and you make the calculation what happened, it's the empirical risk totally disappear if you if you stratify it simply by all the possible value that he can have, right?",
            "And also because the empirical risk if we are in the ideas we have the idea assumption, the empirical risk follower, binomio law.",
            "Meaning that we can know we can express exactly what is this value.",
            "It's basically the binomial value for.",
            "P is equal to true risk number of of a number of parameters.",
            "M is the number of elements of this path, and this is the value of the output of the random variable.",
            "You put it together and everything is canceled easy.",
            "Good.",
            "And you see, if we don't have the IID assumption, we have a problem because we don't know how to make this stratification.",
            "Right?"
        ],
        [
            "Good.",
            "OK."
        ],
        [
            "No."
        ],
        [
            "OK, now for David McAllister bound.",
            "The first one that not exactly yours but very similar to yours is obtain.",
            "If you put your convex function to be 1/2 of the empirical is minus the tourist to the square.",
            "Can you bake?",
            "Basically make almost the same kind of manipulation?",
            "It's easy to prove that this is always bigger than the smaller skews, than the combat library versions between Q&P.",
            "But mostly all the same calculation goes up.",
            "OK, but this is a interesting because this show that the do.",
            "The rate of convergence of the pack back in the pack base bound is one of our router.",
            "It was not so clear when you look at the figures, but.",
            "Good."
        ],
        [
            "OK. Another approach OK. We can do any any divergent OK and 11 Divergent.",
            "One convex function that might be interesting is a fun convex function that will be linear in the empirical risk.",
            "While it will be interesting because then you will have some expressions, say the true risk is smaller and equal then blah blah blah times the empirical risk plus problem.",
            "OK, so this is a nice idea.",
            "Ann, this is the idea of Olivia Katuni an it."
        ],
        [
            "It's interesting too.",
            "To look at what we can think about it, OK, first, what goes out?",
            "It's something like that.",
            "It's a bit more technical, but you know that one minus the expectation of something is equal to minus that something.",
            "So you can get rid of this part.",
            "Put a plus here an minus know it's plus here.",
            "Minus no, I think I make a mistake here.",
            "Now you got brackets around the whole thing here.",
            "OK, OK, my bracket is is there OK?",
            "Oh yeah, OK, so that's why so it's a.",
            "No something wrong anyway.",
            "It's almost that.",
            "And this is interesting because if you if you take this inequality that say that this is something that you can get rid of, then you have a bound that say, well, that risk is less than equal.",
            "Then some constants time.",
            "The empirical risk plus something that goes to zero when N goes to Infinity.",
            "OK, and you see the tradeoff that you can have here.",
            "It's basically that if you can assume that this is very small.",
            "Then you can take a very small constant.",
            "It constantly is very close to 0 and then you will almost have that.",
            "The risk of the of the true risk is small in equal then very small constant times do by small, not very bigger than one.",
            "Then the empirical risk plus something.",
            "But if this is big then you will not be able to take a constant that is very close to 1 right?",
            "And then you will still have a bound but not necessarily as tight as well as with you."
        ],
        [
            "OK. And and this is interesting also for another reason it you can see why it was a good idea to use the random variable that was the Laplace transform.",
            "I prefer to see it as something that is related to the generating moment function.",
            "While this is interesting because.",
            "You know that generating moment function of a sum or difference because it's an exponential can be split.",
            "And the generating moment function of that guy since Q is the empirical risk, will therefore be the generating moment offer binomial for which we have a nice closed form.",
            "Right and when we say that we say, well, OK, So what we can do with the function F whynott take the inverse of the generating function, then they will cancel and then this ugly term.",
            "Will be one.",
            "No key term that we have, so problem with will be a simple one.",
            "Good.",
            "Fine.",
            "OK, and this is basically the generating function of the.",
            "The normal right and then so you put F to be something that makes this product equal to 1 and with this particle F you obtain the backbase bound of ticketing.",
            "Fine.",
            "Again, simple things.",
            "That's very interesting for me, OK?"
        ],
        [
            "Another thing that I was will we will see in during my presentation.",
            "Even if the Seeger bound is tighter in some way, I prefer the Olivia Catoni bound for many reasons.",
            "The first reason is.",
            "We have if we want to minimize the back base bound, we have something that is linear in the empirical risk that."
        ],
        [
            "Interesting."
        ],
        [
            "For me, OK. Also, we have an eye perimeter.",
            "That can be tuned by cross validation.",
            "And some somebody would say, well, people who want to minimize bound what you you think about making some cross validation up to now.",
            "We cannot say we can use a lot of bound to minimizing some bounds and produce nice algorithm, but we need to.",
            "We didn't find a way not to use the any any cross validation.",
            "It's needed if we want to achieve a performance as good or even a little better than the state of the art.",
            "If we doubt it's still a good algorithm, but it's bit not significantly, but a bit not as strong as the state of the art.",
            "So that's strange because people who like bounds would like not to make any cross validation but aren't up to now.",
            "It's something that still is necessary, will give you my intuition at the end, OK?",
            "Also.",
            "The Seeger bound is always better than the Catanese bow.",
            "If we don't consider this term in the in the Seeger bound Ln of CFM, which is a line of one in the Catanese Band.",
            "Ann wise so.",
            "Hi, I think how I didn't put the proof OK.",
            "It's not very difficult.",
            "OK, and it's basically again related to this.",
            "Essential Luxry aliti and so if you look at if you take the.",
            "The Seeger bound you replace this Sigma M bayawan.",
            "Then the minimum of the best params parameters C of the catoni bound will make the two bound fit exactly.",
            "For any possible empirical risk, of course, for different progress, the API parameter must be choose differently.",
            "OK.",
            "If you think that this is quite stable to find the parameter C, maybe you can test some, but not as many as this, which is about 2 * sqrt M different parameter and then you will achieve by this way I bound.",
            "It is tighter than the cigar bar and also if you cross validate those maybe you will be able to find a seat that will not be the one that will give the tightest bound.",
            "That will give the bound that is the most give the most the better correspondence between what you achieve in as Gibbs risk on the training set and what is the true majority vote risk because we are minimizing a Gibbs risk, but we are looking for the base one.",
            "So this cross validation of the hyperparameter is something nice here."
        ],
        [
            "OK. Also, another thing that is interesting with the Catanese bound is we can find what is exactly the expression for the.",
            "The optimal posterior for a given.",
            "See what is the optimal posterior.",
            "Basically, is the Gibbs distribution, Z is a normalization constant.",
            "For the cigarette and David McAllister bow we can we can have some information of what should the optimal posture should look like, but we cannot see that.",
            "There are many possibilities with Addiciton is bound we have exactly 1.",
            "So this is very interesting and This is why I tell you that we have possibly some information.",
            "Ideally there that will help to have a better understanding of the Bayesian approach, because this is really close to what the patient."
        ],
        [
            "I'm doing OK.",
            "There's a small problem.",
            "This posterior it's the same problem that Bayesian have.",
            "This poster is very difficult to calculate in general.",
            "You can do it by Monte Carlo methods, and if you can control the mixing time and things like that, but it's nevertheless low.",
            "So even if we have this exact form because of this normalization constant, that is very difficult to calculate, it's difficult to estimate it to know what it is.",
            "OK, but we can do some Monte Carlo estimate.",
            "We did that and basically it's what it's good.",
            "It worked well, right?"
        ],
        [
            "OK, how to avoid MSMC?",
            "Do the same trick has the Bayesian do go back to Goshen distribution?",
            "Because then you will be able to calculate more easily what is the posterior that you look for, right?",
            "But before going into linear classification in the proof that I show you both Olivia and Seegars one, what you see is the following.",
            "You have.",
            "The basic idea was we make exact calculation based on the idea that the empirical risk follow is random variable that follow a binomial distribution that has the exact true risk.",
            "As parimeter P. OK. And because of that we can manage to make it work, but is there other ways to do that?",
            "And the answer is yes, OK?",
            "And we can do it by concentration inequality.",
            "And basically I think David.",
            "It's how you did that, John.",
            "You did that this way when you make some, you're the proof of the Seeger bound.",
            "I think you use you use large deviation approach also in your in your setting.",
            "So it's a very nice way to do it.",
            "Of course, long time after you."
        ],
        [
            "Find a way to make exact location, but I think the approach of using concentration inequality is interesting and powerful because we now interested in something that is more general than just classification and the calculation should be much more difficult to do.",
            "And if we can just relate to some very interesting concentration."
        ],
        [
            "Inequality that should be good.",
            "OK, so as I said Langford did that.",
            "Seager Matthew to generalize the Seeger bound to the transactive setting, find a nice concentration inequality due to Vegeta.",
            "Anne."
        ],
        [
            "I basically proved that the same bound work both for inductive and transductive setting."
        ],
        [
            "And also leave a olivola and two other people use the Jensen inequality.",
            "To prove version of the PAC Bayes bound in the non IID setting.",
            "That's quite impressive.",
            "Because as you tell, I tell you from the beginning, all the argument I used was heavily based on the IID setting, so they may be able to do something interesting in that, and there's another way we discovered this year I'm not aware about any people who did."
        ],
        [
            "That be before that, but there's."
        ],
        [
            "Another way.",
            "To to get rid of this ugly term using Martin Gould approach.",
            "It's it's quite elegant work well an we only start to understand so leverage all is guy leverage on an OK. And so there's a lot of room for that, especially if we get out of the IID case, or if we are interesting in you statistic of higher order.",
            "OK, well if I thought I have time I will go back to this idea.",
            "But basically what we do with the back base bound up to now is something that is based on the Gibbs risk or really related to the Gibbs risk.",
            "OK and this is some expectation according to draw of X according to the unknown distribution D. But maybe we want to see a pair of example are related.",
            "Doing graphical model for example and then we go up into the order of eustatic.",
            "We are interested in.",
            "But if we take care of example from a training set is IID, the set of all pairs is not IID anymore.",
            "So to have non IID pack baseball in that situation should be interesting and we have two way to deal with that with the approach of lever that I will show you and with the approach of guy that will be.",
            "I think you talk this morning.",
            "Oh so."
        ],
        [
            "OK.",
            "So, uh, yeah, times goes fast so I will just give a give a rapid explanation of how lever availa succeed in or you just can leave it.",
            "OK, OK and so.",
            "I would just explain rapidly.",
            "Oliver managed to.",
            "To to have this bound that works for non IID because at first sight this seems to be surprising, right?",
            "Because if you, if you really understand the proof that I show you, it's it seems impossible to get rid of it.",
            "And basically."
        ],
        [
            "You did the full OK. What is the problem?",
            "Of course, define what is the risk.",
            "So now the distribution is producing the training set in one shot.",
            "There may be some correlation between the examples, so you have the same definition for the risk, except that you have.",
            "You suppose that you you are not drying IID, so you have to consider all of them right?",
            "You cannot say that the risk is.",
            "Oh, you didn't?",
            "I put a~ here.",
            "I just don't understand why he didn't I. I suppose that I put in the other slide, so D is distribution that just give you the M example in one shot so you can reproduce this example.",
            "The filter is not known OK, but you suppose that you can do some got somewhere that can give you another training set, for example an and what you want to say is in the average.",
            "Of course you cannot calculate the true risk services as usual.",
            "In the average, how good is some particle classifier?",
            "And then you cannot say that it is equivalent to the probability of making an error on your new example.",
            "It's the average of Eravu will make when you will see a new training set of the same size."
        ],
        [
            "OK.",
            "But we have the same test question.",
            "Basically we want to see how good we can out, how good we can give some guarantee of the performance of some classifier using, But yes.",
            "It's not the case you still you still keep with the 01 loss does not seem to be less.",
            "Ya hey I I'm not sure time is going fast.",
            "I have some resolve that I was supposed to present you tomorrow this morning on what happened.",
            "If you go to other loss OK and of course it just gives some interesting POV forsake of simplicity.",
            "I restrict 271 last bus.",
            "Yes, in fact it's not necessarily the loss.",
            "You can use basically any loss for that and also you can manage to use a different loss for your Gibbs classifier.",
            "It's not.",
            "It's not necessary that it is simply the averaging of the risk.",
            "You can have some other function on that so you can do a lot of things.",
            "To to generalize and very very."
        ],
        [
            "This way OK. OK, so so here's my~ So what can we do with that in general?",
            "If the Taylor is a mess, nothing because it can be so big than your bound will be one.",
            "OK, but"
        ],
        [
            "If you make some assumption, for example that yes, your training set is not IID, but it's basically a function of IID variable.",
            "Ann not too many.",
            "IID variable."
        ],
        [
            "Then you can do something and and the approach is to take advantage of that.",
            "OK, an.",
            "Are you can subdivide, as in various IID subset SJ, that can overlap.",
            "OK. And you can put some kind of weight over those FJ."
        ],
        [
            "OK, in such a way that if there is some overlap.",
            "The way the total weight of all SJ that contain the given example will be one.",
            "For those people who know about graph theory, it's about to make a fractional call."
        ],
        [
            "Ring of the graph of the interaction OK. Anne."
        ],
        [
            "So it is exactly the idea that they have an.",
            "There is a concentration inequality debt.",
            "Just say that you can do this.",
            "An averaging is fine and the theorem is going at follow.",
            "Basically you instead of having."
        ],
        [
            "Based on S you have it on a family that overlap in some way.",
            "You will be sure that.",
            "You have this constraint that it's summer through to one or.",
            "At not more than one, of course, to find the optimal way of making this color fractional coloring, it's something that's NP complete in general, so this is not about that is easy to use, but for very important situation, namely ranking.",
            "You statistic of order two or three and so on.",
            "We can manage to have a nice.",
            "Set of SGI and basically we don't lose too many things.",
            "A factor of 2.",
            "If we do ranking instead of doing simple classification.",
            "OK.",
            "But again, this is the ugly term because the rest of the we can manage, but this ugly term.",
            "Basically you can subdivide it in structure that are IID and treat them either as katani I've done or cigarettes to OK. OK. Well, I won't have time to do everything what I want to do, but the idea is the problem, bonding."
        ],
        [
            "The Gibbs risk.",
            "Instead of the Bayes risk, this is something that has not been really solved yet, right?",
            "And I will suggest two possible answer.",
            "To solve this problem, OK, the first one is if you can manage to have non two small part of your classifier voters H. OK, that are quite strong.",
            "Then you will manage to.",
            "When you see the data you will be able to find a posterior Q that will be not too different to do your prior.",
            "If suppose your prior was uniform, OK and since they are good, the empirical risk Gibbs risk will be good.",
            "Twice something that is good remains good.",
            "Right and basically it's working.",
            "This approach is working.",
            "It's basically the idea of John Langford, John Schoettler to have the I think the tightest bound for SVM I know.",
            "It was basically this idea.",
            "OK. Anne.",
            "I will propose.",
            "I will not propose it because I will not have time.",
            "But there's another possibility also that we can do.",
            "It's something that's a."
        ],
        [
            "Well, the PAC Bayes bound we used to take the Gibbs risk, but we can make any estimation on on the on the on the training set that we want.",
            "And compare it with true value.",
            "We know that if we take the Gibbs risk, we have that the Gibbs twice the Gibbs risk is a pound of the base.",
            "But we can have other kind of estimate that will give this indirect band.",
            "Maybe it will be better if we want to find a good algorithm out of that.",
            "Because if you look at the PAC Bayes bound on the Gibbs risk and you think that your voters are all week, they are all.",
            "About a risk of 4849%, what will do the back Bays Mountain?",
            "He basically will try to put the most important weight to the better one.",
            "OK, and so you will end up with a better one, which is bad.",
            "OK, but if you think about the majority vote should deal together in order that the weakness of the water will compensate one to the other.",
            "Then you have a chance.",
            "Give some interesting but interesting value."
        ],
        [
            "Right?",
            "And this is what I want to do with you.",
            "OK, so the first approach, my first answer.",
            "OK, what you can do, you look for strong, strong people."
        ],
        [
            "OK, So what you can do is the following.",
            "You can simply say, well, I will go into a feature space that is very good.",
            "I'll be of kernel for example, or something very strong right?",
            "Ann, I will manage the complexity by the kernel trick.",
            "So here I have.",
            "I know that I will have a bunch of classifier that will be very good on the training set, OK?"
        ],
        [
            "An an.",
            "So basically I will take majority vote of of my voters which will exactly is the same thing as the scalar product in that feature space.",
            "OK, so each classifier here is basically a member of.",
            "The the Big Fisher space and I take the majority vote simply by taking the DOT product and everything is easy to do it.",
            "It's a kernel trick juice price here OK?",
            "An because it's difficult to find the best posterior in general, even with the Catanese bound we."
        ],
        [
            "Proposed to use prior and posterior data Goshen.",
            "It should be better, more easy algorithmically."
        ],
        [
            "And look what with what we end up with."
        ],
        [
            "First, since we are going using Goshen, the majority vote of the Goshen OK is exactly exactly the same outcome as the the voter.",
            "That is the center of the Goshen, because basically they are cancelling otherwise.",
            "So that's nice because for example for SVM what we have is it we want to have the abound on one vector, which is the combination of the correct combination of the supervector.",
            "OK, but the PAC Bayes bound is giving us something that is an averaging of possible vector.",
            "But if we are using lotion or something that is symmetrical then we know that the the majority vote out of that Goshen.",
            "That is the continuous measure vote.",
            "Is equivalent as the same outcome as the value of the only voters.",
            "That is, the center of the Goshen that you look at OK. And of course, so the the output of the SVM will be equal to the Goshen that I told you, and I will be able to bound it by twice the Gibbs risk.",
            "An If I manage to be in a very, very strong feature space, this will not be so bad.",
            "Right, I skipped a lot of details, but basically this is the idea that leads to disbound of John and John OK. And as prior."
        ],
        [
            "We say that we would use Goshen, so that's nice because the callback library divisions between 2 Goshen as a very nice form.",
            "To the photo calculation, it's easy."
        ],
        [
            "OK, also we want to compute.",
            "We want to be able to compute the Gibbs risk and this is not so difficult.",
            "Also you make a lot of calculation and basically it's.",
            "The profit function that goes there plus the norm of the W plus gamma, which is basically very similar to the match.",
            "Anne."
        ],
        [
            "So if you look at the probit loss.",
            "And you compare with the inch loss.",
            "You can see that dangerous is convex.",
            "The probit loss is not OK.",
            "But if you want to make a conversation, a convex relaxation of my profit loss, dangerous is a good idea."
        ],
        [
            "And this is it."
        ],
        [
            "Exactly.",
            "What happened so we look at the Catanese bound and Cantonese bound said, well, if you want to minimize me, you have to minimize this expression.",
            "You replace what is this guy you replace?",
            "What is this guy?",
            "And so here is the problem.",
            "It's not so easy to find the solution because I show you the probit loss is not convex, so there may be many minimum, but we can make some restart gradient decent."
        ],
        [
            "Things is quite easy to do OK."
        ],
        [
            "Anne."
        ],
        [
            "Make the comparison with the SVM.",
            "So this VM you minimize the hinge loss instead of minimizing the probit loss.",
            "Of course it's convex.",
            "So suppose that we have a pack based theory in the 80s and we want to minimize fact based bound and we say, well, probably class is something that is too complicated to minimize.",
            "Let's make some convex relation of it and do the trick.",
            "We have another interpretation of the.",
            "See it.",
            "This is the hyperparameter of the Catanese bound.",
            "This is the seed that is related to dual variable right, but basically, especially if you don't have any knowledge a priori and you choose to have a Goshen centered in zero for your prior, you end up basically within SVM.",
            "By minimizing Pacbase bound on Goshen.",
            "Good."
        ],
        [
            "Clear."
        ],
        [
            "OK, and is it working well well again?",
            "If you don't want to have any parameter to tune, so instead of using the Catanese map, use the Segers bound.",
            "It's not as good as this VM.",
            "But it's difficult on UCI data set to make difference because all I got in is quite good.",
            "But in in the in the overall has VM is a bit better.",
            "OK if you want to take half of the data to learn the prior and the other half of the data to just make the minimization.",
            "It's a bit better.",
            "The bank is tighter.",
            "But the accuracy is not as good as SVM.",
            "But if you cross validate the parameters C of the Catanese Band.",
            "And but then you end up with a lot of minimum.",
            "It's not convex, so you have to make a lot of restarts that good algorithm in as a performing, but for accuracy.",
            "It's a bit better.",
            "India average.",
            "You seed the optimization problem with the.",
            "Output of optimization of Angeles.",
            "D. On the Internet optimization, we get it.",
            "We get a solution that's convex.",
            "Yeah, we could use it to see the optimization of the public loss.",
            "Preps avoid a lot of the restarting right, and then if it's it's get out with a better solution, it will be better.",
            "Yes, good idea.",
            "We try it.",
            "No, maybe we should.",
            "No, it's a good idea, yeah, so yeah, yeah.",
            "But you know, you don't start at random.",
            "You start at some good solution so.",
            "Good."
        ],
        [
            "So it's OK."
        ],
        [
            "Anne."
        ],
        [
            "Now if we have weak learner like boosting, for example, is dealing with weak learner.",
            "If, as I tell you, if we do that then we end up with something that will not work, minimizing the back base bound among very bad classifier will end up with.",
            "We will choose the less bad classifier or."
        ],
        [
            "Some of the less one.",
            "And it's work.",
            "Well, this is what decision stump again."
        ],
        [
            "We compare with that at the boots with the same decision stump PD one.",
            "We use the Seeger bound, so no parameter to tune.",
            "It was about the same.",
            "We tried to run the prior a bit better.",
            "If we learn the prior, the bound are tighter and we make our validation for the hyperparameters seek.",
            "And then it's better than.",
            "Then we're back on HUCI data set.",
            "Nothing is significant because it's always basically the same.",
            "But if you go in the overall here, the bold ones are not easy to see, but basically they are mostly in the third one.",
            "So by doing that again, we are bit better than state of the art, but it's not a idea because we have to work because we are we don't have convex situation situation.",
            "But it's a bit better.",
            "We can also make a complexification.",
            "But then we go back basically to SVM, OK?"
        ],
        [
            "OK."
        ],
        [
            "May I?",
            "OK.",
            "It's OK.",
            "Thank you.",
            "OK.",
            "So the idea is the following.",
            "It will not consider the Gibbs risk as the important thing that I want to look at.",
            "I will consider.",
            "A function of the margin.",
            "The margin is is simply the expected value of.",
            "The product of white.",
            "I'm X so look at it.",
            "If the margin is positive, that means that the weight of the classifier that are OK for that example is bigger than the weight of the one that are wrong.",
            "If the so the majority vote will be.",
            "Good, you will not make an error on that example if the margin is negative then it's really the contrary, OK?",
            "So I think this is something important and I say well, I will make backbase bound not on margin but on.",
            "Polynomial function, in fact Taylor series.",
            "Based on that expression, the margin."
        ],
        [
            "And as I tell you, I want to have a bound and then direct bound on the majority vote.",
            "Basically the majority vote is the risk of the majority.",
            "Vote is exactly that when the margin is negative, the majority vote makes an error.",
            "OK?",
            "Yeah, here you can put or equal to 0, supposing that if the majority vote is even, then you declare that you make an error just to simplify the map, but you can not also.",
            "OK. And so I want to have a function that are simply the property to be.",
            "Bigger than."
        ],
        [
            "This indication function and meaning basically.",
            "Something like that."
        ],
        [
            "OK, so where is the indication function?",
            "OK, I did that on my picture.",
            "I do not see to see ties my function.",
            "It says it.",
            "And so, as I tell you, if the margin is positive, then the majority vote is OK.",
            "So yes, no loss.",
            "And if it's a margin is negative, then the majority vote is making an error.",
            "So he has one loss.",
            "And basically what we are doing is we are making the averaging on all the example of the training set.",
            "Of that, to see what is the empirical risk of the majority vote.",
            "OK, if instead of looking at this we look at the green line, which is a function of the margin.",
            "Which is 1 minus MQ and so we will make will evaluate the empirical loss that we will obtain on that.",
            "So for each example we look at the margin and we compute what its value.",
            "And we make the average OK. And this will be the risk on that particle loss, OK?",
            "Good, this green loss is interesting because one minus the margin.",
            "It's easy to prove that is twice the Gibbs risk.",
            "And then when I tell you in the in the beginning that the Bayes risk the risk of the base is always smaller than equal than twice the Gibbs risk, here's the proof.",
            "If you make the calculation, of course, since this one is always over the black one.",
            "Expectation of that will always be bigger, OK, but we can do other kind of function.",
            "For example, we can use a parabola.",
            "An exponential function and so on, provided that.",
            "By our function is over this indicative function that represents the loss of the."
        ],
        [
            "OK fine.",
            "And basically we have a pack based on that.",
            "Here is the catoni.",
            "It's a bit more complicated.",
            "Basically it's about the same.",
            "OK, so again we have this trend constant.",
            "We have the true.",
            "It's not a risk now through Zeta value, the Zeta value that is calculated on the training set, so they choose data value is always smaller than equal.",
            "Then this empirical estimates.",
            "An then we have some new constant that appear that are called CA that does not appear in the Catanese bound because the value is 1 and escape bar also that corresponds to the value of the Zeta function on one and the value of the deffective of visitor function on one OK. Yeah, I've to say that to have this simple calculation we have to suppose that the coefficient on the."
        ],
        [
            "The AK are all positive, otherwise we can manage to have something else.",
            "Something that works too, but Recalculation is a bit more.",
            "Delicate, OK?"
        ],
        [
            "So what is the trick?",
            "You trick is quite easy.",
            "Basically it's simply that this function of the margin I tell you that basically the Gibbs risk is equivalent to have a bound on the margin exactly or linear function of the magic.",
            "OK, So what we what you can do is to transform your problem, you transform the set of classifier H into a new set of classifier H bar and managed to.",
            "That does it, a value of the margin is exactly a constant times the true margin in this new problem.",
            "And then you can apply the theorem using the Gibbs risk, basically with some constants to manage OK. And the trick is basically the following.",
            "It's the set of your set of classifiers, basically it's.",
            "To to evaluate, because as you can see it's margin.",
            "This is something that is associated to the moment.",
            "The first moment that gives risk.",
            "OK, the square of the margins, the second moment of the margin basically, and things like that.",
            "And So what you can do is face basically to be able to evaluate the linear term of Jose to function you.",
            "Basically you only have to calculate the Gibbs risk.",
            "So you take one classifier.",
            "Can you look how is good or not and you make the average on that for the squared term you have to take to classifier and look how they work together.",
            "And then make a computation and this computation is basically then like this function I'm getting out of time.",
            "Pascal will talk a bit about that later in in this session OK, and so if we manage to have this very non Gibbs rates problem."
        ],
        [
            "Rick Dick if we can retrieve it gives this problem, then we can simply apply the usual back based on that and see what happens.",
            "OK, and the calculation of the Combat library is quite easy to make.",
            "Of course the combat labor division between those two will be bigger because we are working on a very much bigger set of classifiers, but we can manage and then if we use a Catanese bound."
        ],
        [
            "Basically what he says is we have to minimize this."
        ],
        [
            "I will go a bit faster."
        ],
        [
            "But we were interesting in two type of loss.",
            "The exponential loss and a quiet class with some parameter gamma.",
            "OK Huawei, the exponential US boosting was inspiring and why the quadratic class?",
            "Because this is the easiest after the linear case, which we knew was not good."
        ],
        [
            "An we try to minimize things.",
            "It works quite well.",
            "Again it can go yeah and we compare with Ridge regression at the boost.",
            "And basically we are in the state of the art.",
            "We are not deeply beating them, but we are in a state of the art."
        ],
        [
            "Anne."
        ],
        [
            "Internet we now have to say it fastly, but what is interesting is the if you restrict yourself in some particular case.",
            "OK so you have this situation where if you have a classifier you have this compliment.",
            "You have a situation of the posterior distribution is allying on, the prior Pascal will talk about what it means in particular.",
            "Then it's quite easy to make the calculation of the combat librarian.",
            "Basically you end up with.",
            "This formula OK."
        ],
        [
            "Why this formula is interesting?"
        ],
        [
            "It's because then, then here is the function that Pacbase says that you have to minimize if you want to work.",
            "OK."
        ],
        [
            "OK, just."
        ],
        [
            "Just to finish."
        ],
        [
            "If this data function is the exponential loss you obtain.",
            "D exponential The boosting algorithm but with some regularization term?",
            "Which year is the square but the true one is basically the kill that library OK?",
            "And if you use a quadratic class, you end up with the Ridge regression.",
            "Donkey"
        ],
        [
            "In conclusion, we tried to."
        ],
        [
            "Find Newton new algorithm out."
        ],
        [
            "Of the pacbase"
        ],
        [
            "By minimizing it and."
        ],
        [
            "It's good in some sense because we re discover the best known algorithm naturally."
        ],
        [
            "It's bad because we were not the first."
        ],
        [
            "But maybe if we go further in."
        ],
        [
            "In the regression instruc."
        ],
        [
            "True output ranking."
        ],
        [
            "Maybe we can find some."
        ],
        [
            "Something that has."
        ],
        [
            "Not been found yet."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so um thanks for coming to this workshop.",
                    "label": 0
                },
                {
                    "sent": "I think it's it's an interesting.",
                    "label": 0
                },
                {
                    "sent": "Topics if you I really think that Backbase will will have.",
                    "label": 0
                },
                {
                    "sent": "A second life in the future years for I would say for three reasons, maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe other people that will make presentation can add other reason, but I I think backbase approach is interesting for first reason is because of its simplicity.",
                    "label": 0
                },
                {
                    "sent": "It's something that is.",
                    "label": 0
                },
                {
                    "sent": "The proof are not difficult if you compare for example with the ramaker approach, where when you want to do something it's very difficult, so I think this is a strength for the back based approach, because then we can really deeply understand what's going on and generalizing very values direction easily.",
                    "label": 0
                },
                {
                    "sent": "The second reason I would say.",
                    "label": 0
                },
                {
                    "sent": "Hey, the reason why I think that PAC Bayes will will have more important in future is I think we will achieve better in the understanding on the Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "'cause the beige and is based on the.",
                    "label": 0
                },
                {
                    "sent": "Some kind of magic from my point of view, because it's simply that they have something at Prairie and then you use the Bayes rule to output what is the best situation.",
                    "label": 0
                },
                {
                    "sent": "But what this means from a frequentist it's something that is not clear.",
                    "label": 0
                },
                {
                    "sent": "I think with back based theory we can achieve better understanding.",
                    "label": 0
                },
                {
                    "sent": "Matthias Seeger, for example in Stasis, says make some progress on that.",
                    "label": 0
                },
                {
                    "sent": "I think more is to come, and the third reason why I think.",
                    "label": 0
                },
                {
                    "sent": "Basically it will be more important the future is we try recently to make algorithm out of PAC Bayes bounds simply by minimizing bounds, and it works quite well.",
                    "label": 1
                },
                {
                    "sent": "In fact, we re discover some existing algorithm SVM, Ridge regression, regularizer, the boost all those algorithm can be view as minimizing some kind of pack basement.",
                    "label": 0
                },
                {
                    "sent": "So for those two reasons, I think that's interesting.",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                },
                {
                    "sent": "I will try to talk in my in my tutorial of two of those three, so the simplicity of the map.",
                    "label": 1
                },
                {
                    "sent": "So I had to do a tutorial basically, so I will try to show out and proof are working on.",
                    "label": 0
                },
                {
                    "sent": "And why are the ideas I will not have the time to explain exactly all the duality that is going on in here, but I expect someone in the audience that will do.",
                    "label": 0
                },
                {
                    "sent": "Wendy will be there.",
                    "label": 0
                },
                {
                    "sent": "Time to talk OK and I will also show you how we can retrieve existing algorithm simply by minimizing pack basement OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the outline so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the mat and then look at what we can do if we want to make algorithm out of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bounce.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, rapidly.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, I suppose that something is.",
                    "label": 0
                },
                {
                    "sent": "I will restrict myself to supervised learning an basically to binary classification.",
                    "label": 1
                },
                {
                    "sent": "The easy case, I think it's already my hour will be already taken fully by this way.",
                    "label": 0
                },
                {
                    "sent": "Of course everything is possible to generalize to other setting to regression to structure output to ranking and so on.",
                    "label": 0
                },
                {
                    "sent": "So the true risk I will write for a classifier.",
                    "label": 1
                },
                {
                    "sent": "HI will write RFH and the empirical risk based on the training set S are subsets of age.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's the usual definition that I will look, but in the pack based theory.",
                    "label": 1
                },
                {
                    "sent": "Basically what we want to achieve, its a good performance, not on the set of learners that we have, but on the majority vote of them.",
                    "label": 0
                },
                {
                    "sent": "And the trick is basically to define the good weighting of the majority vote.",
                    "label": 1
                },
                {
                    "sent": "In such that you will achieve the best possible outcome.",
                    "label": 0
                },
                {
                    "sent": "The best performance OK?",
                    "label": 0
                },
                {
                    "sent": "This classifier is also it's a majority vote.",
                    "label": 1
                },
                {
                    "sent": "Everybody you see that.",
                    "label": 0
                },
                {
                    "sent": "So you you look you give some weight to every classifier and then use some an if most of them are thinking plus one then the sign with output plus one and if most of them think minus one then the sign will output minus one.",
                    "label": 0
                },
                {
                    "sent": "For simplicity if the majority vote is even we can say that.",
                    "label": 0
                },
                {
                    "sent": "It's wrong by definition, or we can sell flip a coin, or maybe possibility OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea is that so we have a bunch of of classifier we call learners or voters and we look for majority vote.",
                    "label": 0
                },
                {
                    "sent": "All of them OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem of the pack based approach, I think the main problem that he had is even if we are interested in the band retrieval classifier.",
                    "label": 0
                },
                {
                    "sent": "The bound that we can consume.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fact is, on the Gibbs classifier which is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Acoustic classifier.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An hour together define.",
                    "label": 0
                },
                {
                    "sent": "For a given level X, the Gibbs classifier will draw according to QA classifier into the set and it will be that classifier only that will classify it.",
                    "label": 0
                },
                {
                    "sent": "If you give another hex then according to Q we find a Nook another classifier, another learner from the Class H and it's this learner only that will classify it right?",
                    "label": 0
                },
                {
                    "sent": "So there's some link between.",
                    "label": 0
                },
                {
                    "sent": "The gifts and the base, but the base is a deterministic classifier and the Gibbs is a stochastic classifier.",
                    "label": 0
                },
                {
                    "sent": "You give twice the same input.",
                    "label": 0
                },
                {
                    "sent": "He can give you different outputs.",
                    "label": 0
                },
                {
                    "sent": "OK. Of course the risk of Gibbs classifier is is the average of the risk of the voters, because it's exactly what you do.",
                    "label": 1
                },
                {
                    "sent": "And the same thing for the M.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Typical estimate OK?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unk",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I was, I tell you, we are interested in the majority vote.",
                    "label": 0
                },
                {
                    "sent": "But the PAC Bayes bound will gives us something about the Gibbs classifier.",
                    "label": 0
                },
                {
                    "sent": "There is a link between those two.",
                    "label": 0
                },
                {
                    "sent": "Basically the links is the Bayes risk is never bigger than twice the Gibbs risk.",
                    "label": 0
                },
                {
                    "sent": "You can almost reduce under margin assumption.",
                    "label": 0
                },
                {
                    "sent": "You can reduce this factor of two almost to one, but nevertheless it's bad, right?",
                    "label": 0
                },
                {
                    "sent": "Why this is bad?",
                    "label": 0
                },
                {
                    "sent": "I will discuss this later on, but you can suppose, for example, that the voters are very weak.",
                    "label": 0
                },
                {
                    "sent": "Like weak learner in boosting for example.",
                    "label": 0
                },
                {
                    "sent": "So the average of all those guys will be very weak, so you will end up with a very bad Gibbs risk.",
                    "label": 0
                },
                {
                    "sent": "But you know that the majority vote that go out of the boosting because the boosting is a kind of majority.",
                    "label": 0
                },
                {
                    "sent": "Vote is usually very strong, so this compression is not necessarily interesting.",
                    "label": 0
                },
                {
                    "sent": "We see that it will be interesting in the case where all the voters are a large part of them are good, are very good.",
                    "label": 0
                },
                {
                    "sent": "Then we will put all the weight on those.",
                    "label": 0
                },
                {
                    "sent": "Good ones and we will have a good bound here and there for a good bound for the base.",
                    "label": 0
                },
                {
                    "sent": "But it's an indirect approach, OK?",
                    "label": 0
                },
                {
                    "sent": "Another thing that is important in fact based theory and it makes use of a prior distribution that you you must define.",
                    "label": 1
                },
                {
                    "sent": "So the welcoming will be at the end of the session.",
                    "label": 0
                },
                {
                    "sent": "OK, so the PAC Bayes use what we call a prior distribution on the set of the classifier.",
                    "label": 0
                },
                {
                    "sent": "Pay attention, it's not the prior on the on on the data that you want to look at.",
                    "label": 1
                },
                {
                    "sent": "It's on the set of the classifier that you have.",
                    "label": 0
                },
                {
                    "sent": "It's your apriori knowledge of which classifier is good and which is not.",
                    "label": 0
                },
                {
                    "sent": "If you have no knowledge at all, you can use the uniform, for example.",
                    "label": 0
                },
                {
                    "sent": "But if you know something.",
                    "label": 0
                },
                {
                    "sent": "About for for some reason that some classifier has a better chance to be good, then you will put more weight appear right?",
                    "label": 0
                },
                {
                    "sent": "But you do have to do that before seeing the data OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "There is bound will.",
                    "label": 0
                },
                {
                    "sent": "Will most of the time will make appear what we call the callback library divergents because you have a prior distribution on H. Ann, you have it before seeing the data.",
                    "label": 1
                },
                {
                    "sent": "Then you look at the data and you want to make a good majority vote out of the data.",
                    "label": 0
                },
                {
                    "sent": "So you will try to find it posterior distribution Q that will be have a good majority vote basically.",
                    "label": 0
                },
                {
                    "sent": "And if it's very similar to the prior, then you have very good chance to have good bounds, but if it's very different then you will have.",
                    "label": 0
                },
                {
                    "sent": "Abound that will be less tight.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So everything is fine.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if it's triviality for you or not OK, so OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the theorem that I will start with OK.",
                    "label": 0
                },
                {
                    "sent": "I will just cite Tolkien Ann Langford.",
                    "label": 0
                },
                {
                    "sent": "This is some kind of theorem that is is a theorem from which we can derive most of the known pacbase bound.",
                    "label": 0
                },
                {
                    "sent": "The proof of this theorem is very simple, that's why I presented you here and I will show you how we can do it.",
                    "label": 0
                },
                {
                    "sent": "And since the proof is simple, since the proof is simple, we will be able to identify.",
                    "label": 0
                },
                {
                    "sent": "Where are the things where we can work and what is going on where?",
                    "label": 0
                },
                {
                    "sent": "Why is something works and something is not working, etc.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe just make an interpretation of it.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This you can see it as a similarity notion, distance or divergent.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "You will do the empirical risk that you observe will be.",
                    "label": 0
                },
                {
                    "sent": "Close to your true risk of the Gibbs, of course.",
                    "label": 0
                },
                {
                    "sent": "Provided that you posterior and your prior are quite similar, everybody know the combat library divergent its variance between distribution plus some.",
                    "label": 0
                },
                {
                    "sent": "Weird term here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It's OK this time is not usable as it is.",
                    "label": 0
                },
                {
                    "sent": "It's by the way this term is true even if we don't have the IID assumption an and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, but this theorem is not usable as it is because we can in general.",
                    "label": 0
                },
                {
                    "sent": "Not calculate what is the value of that guy.",
                    "label": 0
                },
                {
                    "sent": "And for example, if it's not IID, this can be very big.",
                    "label": 0
                },
                {
                    "sent": "But we will see that if it's high ID then the empirical risk will be will follow a binomial distribution and probability.",
                    "label": 0
                },
                {
                    "sent": "The parameter of the binomial distribution of this random variable will be exactly the true risk and because of that we will be able to.",
                    "label": 0
                },
                {
                    "sent": "Calculate exactly what is this term in some situation, not all.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But this is basically the theorem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, we have a bit more general theorem.",
                    "label": 0
                },
                {
                    "sent": "It's not necessary to compare the empirical Gibbs risk to the true Gibbs risk.",
                    "label": 0
                },
                {
                    "sent": "You can compare any two function on ash on age and you so you can compare the average value of A to the average value of B. ANRB can depend on the training set S it's not a problem, OK?",
                    "label": 0
                },
                {
                    "sent": "And also here you in the present slide you have one over him.",
                    "label": 0
                },
                {
                    "sent": "You can if you want to relax this part, you can have a better constant M If you want.",
                    "label": 0
                },
                {
                    "sent": "OK, I won't go into this part.",
                    "label": 0
                },
                {
                    "sent": "I will restrain myself to gives Rex basically, but this is something that is exactly the same proof that I will show you, but I will not prove one.",
                    "label": 0
                },
                {
                    "sent": "Plus I will prove term one on the preceding slide OK?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everything is fine.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are the proof is going?",
                    "label": 0
                },
                {
                    "sent": "We consider a render weird random variable, which is the expected value according to P of the exponential of the product of M and the distance between the empirical and the true risk.",
                    "label": 0
                },
                {
                    "sent": "It's not so weird.",
                    "label": 0
                },
                {
                    "sent": "OK, it's basically what we call a lap last transform or moment generating function of the random variable.",
                    "label": 0
                },
                {
                    "sent": "That is here OK.",
                    "label": 0
                },
                {
                    "sent": "So it's not something that is appearing from anywhere.",
                    "label": 0
                },
                {
                    "sent": "OK, it's work quite well.",
                    "label": 0
                },
                {
                    "sent": "Is it essential that we start with that?",
                    "label": 0
                },
                {
                    "sent": "No, but we didn't find yet better things.",
                    "label": 0
                },
                {
                    "sent": "I will say basically because it's a Laplace transform and there are some strong duality theorem based on convexity.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, D has to be convex.",
                    "label": 0
                },
                {
                    "sent": "That makes thing works.",
                    "label": 0
                },
                {
                    "sent": "I won't go into this duality.",
                    "label": 0
                },
                {
                    "sent": "I hope someone else might see something, but I won't have time.",
                    "label": 0
                },
                {
                    "sent": "OK, so since it's a random variable and it's not negative, we can apply Markov.",
                    "label": 0
                },
                {
                    "sent": "OK, and Markov will say, well, you random variable is smaller than equal, then one over Delta it's mean.",
                    "label": 0
                },
                {
                    "sent": "With probability at least one minus Delta.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. And then.",
                    "label": 0
                },
                {
                    "sent": "You take the logarithm both side.",
                    "label": 0
                },
                {
                    "sent": "OK, can you transfer?",
                    "label": 0
                },
                {
                    "sent": "This is an expectation on P. You transfer the expected to do nothing P on an expectation on Q.",
                    "label": 0
                },
                {
                    "sent": "Simply by multiplying by P /, Q right?",
                    "label": 0
                },
                {
                    "sent": "And this is a very important trick, because this means that the bound that I will obtain at the end will be simultaneously valid for all Q.",
                    "label": 0
                },
                {
                    "sent": "Not only for one.",
                    "label": 0
                },
                {
                    "sent": "For the one that will be output by my algorithm, for example, no for all possible Q simultaneously, right?",
                    "label": 0
                },
                {
                    "sent": "Because it was true for my prior and this is simply an equivalence mathematical equivalence.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next step, we exploit the fact that the longer it is account function to put the expectation out.",
                    "label": 0
                },
                {
                    "sent": "An yeah, of course this is Jensen inequality.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then what we do is we realize that.",
                    "label": 0
                },
                {
                    "sent": "This basically.",
                    "label": 0
                },
                {
                    "sent": "Is minus Nickelback library divergent?",
                    "label": 0
                },
                {
                    "sent": "That's why the callback library versions appear.",
                    "label": 0
                },
                {
                    "sent": "And basically you can see that it is really related to the fact that the random variable we start with was the last transform.",
                    "label": 1
                },
                {
                    "sent": "That's why we obtain the callback library divergent with another veranda variable that we start with will obtain something else.",
                    "label": 0
                },
                {
                    "sent": "Maybe it will be better, but we didn't find veritex yet.",
                    "label": 0
                },
                {
                    "sent": "OK and.",
                    "label": 1
                },
                {
                    "sent": "So if we rewrite this term, we obtain minus the callback library and of course the log of the exponential cancel, and so we we obtained almost what we want.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is going next?",
                    "label": 0
                },
                {
                    "sent": "We have supposed that D is convex.",
                    "label": 0
                },
                {
                    "sent": "OK, so since D is convex, you can put the exponent the expectation inside.",
                    "label": 0
                },
                {
                    "sent": "Again, we use what we call Jensen inequality.",
                    "label": 0
                },
                {
                    "sent": "Then we obtain that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And bingo, that's it.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Thank you both have full support image.",
                    "label": 0
                },
                {
                    "sent": "Uh, no, but we have to take care about the fact that the P that the support of P must include this part of Q.",
                    "label": 0
                },
                {
                    "sent": "But this is it, but you can deal without this fact.",
                    "label": 0
                },
                {
                    "sent": "But yeah, this is something that you have to take care.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "But OK, everything is fine.",
                    "label": 0
                },
                {
                    "sent": "I succeed in convincing Q that pack based bound is easy.",
                    "label": 0
                },
                {
                    "sent": "So my first task is is obtained.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in fact we don't have finished.",
                    "label": 0
                },
                {
                    "sent": "As I tell you this term.",
                    "label": 0
                },
                {
                    "sent": "Is something that we cannot evaluate in general the expectation on all possible training set not only the one that we've seen?",
                    "label": 0
                },
                {
                    "sent": "Of the expectation on the prior of some weird's function.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can we deal with that?",
                    "label": 0
                },
                {
                    "sent": "So sorry David.",
                    "label": 0
                },
                {
                    "sent": "I start with the Seegars bound, but the first one was too.",
                    "label": 0
                },
                {
                    "sent": "I will talk about rapidly to yours, but I start with the one that is basically the Titus one.",
                    "label": 0
                },
                {
                    "sent": "An so Segers simply it didn't prove it the way I show you.",
                    "label": 0
                },
                {
                    "sent": "OK, it was a totally different approach based on financial legend, duality and things like that.",
                    "label": 0
                },
                {
                    "sent": "But basically it choose for the convex function, the small KL divergent.",
                    "label": 0
                },
                {
                    "sent": "What is the small KL divergences?",
                    "label": 0
                },
                {
                    "sent": "Basically this function which correspond to the KL divergent between binary Bernoulli distribution.",
                    "label": 0
                },
                {
                    "sent": "Whyf because of the duality stuff, I don't want to talk about.",
                    "label": 0
                },
                {
                    "sent": "OK, This is why he found this right.",
                    "label": 0
                },
                {
                    "sent": "Brilliant ideas.",
                    "label": 0
                },
                {
                    "sent": "OK, and it didn't exactly obtain this.",
                    "label": 0
                },
                {
                    "sent": "Because instead of making the art calculation that I will show you it just take what we call.",
                    "label": 0
                },
                {
                    "sent": "Concentration inequality OK, but basically",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The proof is quite simple again.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, what is Zita FM.",
                    "label": 0
                },
                {
                    "sent": "It's a mess, but it's something that is basically two 2 * sqrt M. Roughly OK, Ann, but in the in the paper of Seger, what we see is N + 1 because this is very easy to achieve.",
                    "label": 0
                },
                {
                    "sent": "And basically.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, what is this small Cal?",
                    "label": 0
                },
                {
                    "sent": "Is some function that does look like that that will have two vertical accepted 1 zero and one on one and we have it at the minimum at the value of the empirical risk right?",
                    "label": 0
                },
                {
                    "sent": "And So what you have is the small KL is less on equal then blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "So this here is the value of blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "You know that if the empirical basis .1 the true risk will probability 1 minus Delta at least will be.",
                    "label": 0
                },
                {
                    "sent": "Between those",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "True value.",
                    "label": 0
                },
                {
                    "sent": "Understood.",
                    "label": 0
                },
                {
                    "sent": "OK. Whitework",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So well.",
                    "label": 0
                },
                {
                    "sent": "Basically because if you just make the calculation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you replace D by by the small CLN and you make the calculation what happened, it's the empirical risk totally disappear if you if you stratify it simply by all the possible value that he can have, right?",
                    "label": 0
                },
                {
                    "sent": "And also because the empirical risk if we are in the ideas we have the idea assumption, the empirical risk follower, binomio law.",
                    "label": 1
                },
                {
                    "sent": "Meaning that we can know we can express exactly what is this value.",
                    "label": 1
                },
                {
                    "sent": "It's basically the binomial value for.",
                    "label": 0
                },
                {
                    "sent": "P is equal to true risk number of of a number of parameters.",
                    "label": 0
                },
                {
                    "sent": "M is the number of elements of this path, and this is the value of the output of the random variable.",
                    "label": 1
                },
                {
                    "sent": "You put it together and everything is canceled easy.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "And you see, if we don't have the IID assumption, we have a problem because we don't know how to make this stratification.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now for David McAllister bound.",
                    "label": 0
                },
                {
                    "sent": "The first one that not exactly yours but very similar to yours is obtain.",
                    "label": 0
                },
                {
                    "sent": "If you put your convex function to be 1/2 of the empirical is minus the tourist to the square.",
                    "label": 0
                },
                {
                    "sent": "Can you bake?",
                    "label": 0
                },
                {
                    "sent": "Basically make almost the same kind of manipulation?",
                    "label": 0
                },
                {
                    "sent": "It's easy to prove that this is always bigger than the smaller skews, than the combat library versions between Q&P.",
                    "label": 0
                },
                {
                    "sent": "But mostly all the same calculation goes up.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is a interesting because this show that the do.",
                    "label": 0
                },
                {
                    "sent": "The rate of convergence of the pack back in the pack base bound is one of our router.",
                    "label": 0
                },
                {
                    "sent": "It was not so clear when you look at the figures, but.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Another approach OK. We can do any any divergent OK and 11 Divergent.",
                    "label": 0
                },
                {
                    "sent": "One convex function that might be interesting is a fun convex function that will be linear in the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "While it will be interesting because then you will have some expressions, say the true risk is smaller and equal then blah blah blah times the empirical risk plus problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a nice idea.",
                    "label": 0
                },
                {
                    "sent": "Ann, this is the idea of Olivia Katuni an it.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's interesting too.",
                    "label": 0
                },
                {
                    "sent": "To look at what we can think about it, OK, first, what goes out?",
                    "label": 0
                },
                {
                    "sent": "It's something like that.",
                    "label": 0
                },
                {
                    "sent": "It's a bit more technical, but you know that one minus the expectation of something is equal to minus that something.",
                    "label": 0
                },
                {
                    "sent": "So you can get rid of this part.",
                    "label": 0
                },
                {
                    "sent": "Put a plus here an minus know it's plus here.",
                    "label": 0
                },
                {
                    "sent": "Minus no, I think I make a mistake here.",
                    "label": 0
                },
                {
                    "sent": "Now you got brackets around the whole thing here.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, my bracket is is there OK?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, OK, so that's why so it's a.",
                    "label": 0
                },
                {
                    "sent": "No something wrong anyway.",
                    "label": 0
                },
                {
                    "sent": "It's almost that.",
                    "label": 0
                },
                {
                    "sent": "And this is interesting because if you if you take this inequality that say that this is something that you can get rid of, then you have a bound that say, well, that risk is less than equal.",
                    "label": 0
                },
                {
                    "sent": "Then some constants time.",
                    "label": 0
                },
                {
                    "sent": "The empirical risk plus something that goes to zero when N goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "OK, and you see the tradeoff that you can have here.",
                    "label": 0
                },
                {
                    "sent": "It's basically that if you can assume that this is very small.",
                    "label": 0
                },
                {
                    "sent": "Then you can take a very small constant.",
                    "label": 0
                },
                {
                    "sent": "It constantly is very close to 0 and then you will almost have that.",
                    "label": 0
                },
                {
                    "sent": "The risk of the of the true risk is small in equal then very small constant times do by small, not very bigger than one.",
                    "label": 0
                },
                {
                    "sent": "Then the empirical risk plus something.",
                    "label": 0
                },
                {
                    "sent": "But if this is big then you will not be able to take a constant that is very close to 1 right?",
                    "label": 0
                },
                {
                    "sent": "And then you will still have a bound but not necessarily as tight as well as with you.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. And and this is interesting also for another reason it you can see why it was a good idea to use the random variable that was the Laplace transform.",
                    "label": 0
                },
                {
                    "sent": "I prefer to see it as something that is related to the generating moment function.",
                    "label": 0
                },
                {
                    "sent": "While this is interesting because.",
                    "label": 0
                },
                {
                    "sent": "You know that generating moment function of a sum or difference because it's an exponential can be split.",
                    "label": 0
                },
                {
                    "sent": "And the generating moment function of that guy since Q is the empirical risk, will therefore be the generating moment offer binomial for which we have a nice closed form.",
                    "label": 0
                },
                {
                    "sent": "Right and when we say that we say, well, OK, So what we can do with the function F whynott take the inverse of the generating function, then they will cancel and then this ugly term.",
                    "label": 0
                },
                {
                    "sent": "Will be one.",
                    "label": 0
                },
                {
                    "sent": "No key term that we have, so problem with will be a simple one.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Fine.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is basically the generating function of the.",
                    "label": 0
                },
                {
                    "sent": "The normal right and then so you put F to be something that makes this product equal to 1 and with this particle F you obtain the backbase bound of ticketing.",
                    "label": 0
                },
                {
                    "sent": "Fine.",
                    "label": 0
                },
                {
                    "sent": "Again, simple things.",
                    "label": 0
                },
                {
                    "sent": "That's very interesting for me, OK?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing that I was will we will see in during my presentation.",
                    "label": 0
                },
                {
                    "sent": "Even if the Seeger bound is tighter in some way, I prefer the Olivia Catoni bound for many reasons.",
                    "label": 0
                },
                {
                    "sent": "The first reason is.",
                    "label": 0
                },
                {
                    "sent": "We have if we want to minimize the back base bound, we have something that is linear in the empirical risk that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For me, OK. Also, we have an eye perimeter.",
                    "label": 0
                },
                {
                    "sent": "That can be tuned by cross validation.",
                    "label": 0
                },
                {
                    "sent": "And some somebody would say, well, people who want to minimize bound what you you think about making some cross validation up to now.",
                    "label": 0
                },
                {
                    "sent": "We cannot say we can use a lot of bound to minimizing some bounds and produce nice algorithm, but we need to.",
                    "label": 0
                },
                {
                    "sent": "We didn't find a way not to use the any any cross validation.",
                    "label": 0
                },
                {
                    "sent": "It's needed if we want to achieve a performance as good or even a little better than the state of the art.",
                    "label": 0
                },
                {
                    "sent": "If we doubt it's still a good algorithm, but it's bit not significantly, but a bit not as strong as the state of the art.",
                    "label": 0
                },
                {
                    "sent": "So that's strange because people who like bounds would like not to make any cross validation but aren't up to now.",
                    "label": 0
                },
                {
                    "sent": "It's something that still is necessary, will give you my intuition at the end, OK?",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "The Seeger bound is always better than the Catanese bow.",
                    "label": 0
                },
                {
                    "sent": "If we don't consider this term in the in the Seeger bound Ln of CFM, which is a line of one in the Catanese Band.",
                    "label": 1
                },
                {
                    "sent": "Ann wise so.",
                    "label": 0
                },
                {
                    "sent": "Hi, I think how I didn't put the proof OK.",
                    "label": 0
                },
                {
                    "sent": "It's not very difficult.",
                    "label": 0
                },
                {
                    "sent": "OK, and it's basically again related to this.",
                    "label": 0
                },
                {
                    "sent": "Essential Luxry aliti and so if you look at if you take the.",
                    "label": 0
                },
                {
                    "sent": "The Seeger bound you replace this Sigma M bayawan.",
                    "label": 0
                },
                {
                    "sent": "Then the minimum of the best params parameters C of the catoni bound will make the two bound fit exactly.",
                    "label": 0
                },
                {
                    "sent": "For any possible empirical risk, of course, for different progress, the API parameter must be choose differently.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If you think that this is quite stable to find the parameter C, maybe you can test some, but not as many as this, which is about 2 * sqrt M different parameter and then you will achieve by this way I bound.",
                    "label": 0
                },
                {
                    "sent": "It is tighter than the cigar bar and also if you cross validate those maybe you will be able to find a seat that will not be the one that will give the tightest bound.",
                    "label": 0
                },
                {
                    "sent": "That will give the bound that is the most give the most the better correspondence between what you achieve in as Gibbs risk on the training set and what is the true majority vote risk because we are minimizing a Gibbs risk, but we are looking for the base one.",
                    "label": 0
                },
                {
                    "sent": "So this cross validation of the hyperparameter is something nice here.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Also, another thing that is interesting with the Catanese bound is we can find what is exactly the expression for the.",
                    "label": 1
                },
                {
                    "sent": "The optimal posterior for a given.",
                    "label": 1
                },
                {
                    "sent": "See what is the optimal posterior.",
                    "label": 0
                },
                {
                    "sent": "Basically, is the Gibbs distribution, Z is a normalization constant.",
                    "label": 0
                },
                {
                    "sent": "For the cigarette and David McAllister bow we can we can have some information of what should the optimal posture should look like, but we cannot see that.",
                    "label": 1
                },
                {
                    "sent": "There are many possibilities with Addiciton is bound we have exactly 1.",
                    "label": 0
                },
                {
                    "sent": "So this is very interesting and This is why I tell you that we have possibly some information.",
                    "label": 0
                },
                {
                    "sent": "Ideally there that will help to have a better understanding of the Bayesian approach, because this is really close to what the patient.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm doing OK.",
                    "label": 0
                },
                {
                    "sent": "There's a small problem.",
                    "label": 0
                },
                {
                    "sent": "This posterior it's the same problem that Bayesian have.",
                    "label": 0
                },
                {
                    "sent": "This poster is very difficult to calculate in general.",
                    "label": 0
                },
                {
                    "sent": "You can do it by Monte Carlo methods, and if you can control the mixing time and things like that, but it's nevertheless low.",
                    "label": 0
                },
                {
                    "sent": "So even if we have this exact form because of this normalization constant, that is very difficult to calculate, it's difficult to estimate it to know what it is.",
                    "label": 0
                },
                {
                    "sent": "OK, but we can do some Monte Carlo estimate.",
                    "label": 0
                },
                {
                    "sent": "We did that and basically it's what it's good.",
                    "label": 0
                },
                {
                    "sent": "It worked well, right?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, how to avoid MSMC?",
                    "label": 0
                },
                {
                    "sent": "Do the same trick has the Bayesian do go back to Goshen distribution?",
                    "label": 0
                },
                {
                    "sent": "Because then you will be able to calculate more easily what is the posterior that you look for, right?",
                    "label": 0
                },
                {
                    "sent": "But before going into linear classification in the proof that I show you both Olivia and Seegars one, what you see is the following.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                },
                {
                    "sent": "The basic idea was we make exact calculation based on the idea that the empirical risk follow is random variable that follow a binomial distribution that has the exact true risk.",
                    "label": 0
                },
                {
                    "sent": "As parimeter P. OK. And because of that we can manage to make it work, but is there other ways to do that?",
                    "label": 0
                },
                {
                    "sent": "And the answer is yes, OK?",
                    "label": 0
                },
                {
                    "sent": "And we can do it by concentration inequality.",
                    "label": 0
                },
                {
                    "sent": "And basically I think David.",
                    "label": 0
                },
                {
                    "sent": "It's how you did that, John.",
                    "label": 0
                },
                {
                    "sent": "You did that this way when you make some, you're the proof of the Seeger bound.",
                    "label": 0
                },
                {
                    "sent": "I think you use you use large deviation approach also in your in your setting.",
                    "label": 0
                },
                {
                    "sent": "So it's a very nice way to do it.",
                    "label": 0
                },
                {
                    "sent": "Of course, long time after you.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find a way to make exact location, but I think the approach of using concentration inequality is interesting and powerful because we now interested in something that is more general than just classification and the calculation should be much more difficult to do.",
                    "label": 0
                },
                {
                    "sent": "And if we can just relate to some very interesting concentration.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inequality that should be good.",
                    "label": 0
                },
                {
                    "sent": "OK, so as I said Langford did that.",
                    "label": 0
                },
                {
                    "sent": "Seager Matthew to generalize the Seeger bound to the transactive setting, find a nice concentration inequality due to Vegeta.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I basically proved that the same bound work both for inductive and transductive setting.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also leave a olivola and two other people use the Jensen inequality.",
                    "label": 0
                },
                {
                    "sent": "To prove version of the PAC Bayes bound in the non IID setting.",
                    "label": 1
                },
                {
                    "sent": "That's quite impressive.",
                    "label": 0
                },
                {
                    "sent": "Because as you tell, I tell you from the beginning, all the argument I used was heavily based on the IID setting, so they may be able to do something interesting in that, and there's another way we discovered this year I'm not aware about any people who did.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That be before that, but there's.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another way.",
                    "label": 0
                },
                {
                    "sent": "To to get rid of this ugly term using Martin Gould approach.",
                    "label": 0
                },
                {
                    "sent": "It's it's quite elegant work well an we only start to understand so leverage all is guy leverage on an OK. And so there's a lot of room for that, especially if we get out of the IID case, or if we are interesting in you statistic of higher order.",
                    "label": 0
                },
                {
                    "sent": "OK, well if I thought I have time I will go back to this idea.",
                    "label": 0
                },
                {
                    "sent": "But basically what we do with the back base bound up to now is something that is based on the Gibbs risk or really related to the Gibbs risk.",
                    "label": 0
                },
                {
                    "sent": "OK and this is some expectation according to draw of X according to the unknown distribution D. But maybe we want to see a pair of example are related.",
                    "label": 0
                },
                {
                    "sent": "Doing graphical model for example and then we go up into the order of eustatic.",
                    "label": 0
                },
                {
                    "sent": "We are interested in.",
                    "label": 0
                },
                {
                    "sent": "But if we take care of example from a training set is IID, the set of all pairs is not IID anymore.",
                    "label": 0
                },
                {
                    "sent": "So to have non IID pack baseball in that situation should be interesting and we have two way to deal with that with the approach of lever that I will show you and with the approach of guy that will be.",
                    "label": 0
                },
                {
                    "sent": "I think you talk this morning.",
                    "label": 0
                },
                {
                    "sent": "Oh so.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, uh, yeah, times goes fast so I will just give a give a rapid explanation of how lever availa succeed in or you just can leave it.",
                    "label": 0
                },
                {
                    "sent": "OK, OK and so.",
                    "label": 0
                },
                {
                    "sent": "I would just explain rapidly.",
                    "label": 0
                },
                {
                    "sent": "Oliver managed to.",
                    "label": 0
                },
                {
                    "sent": "To to have this bound that works for non IID because at first sight this seems to be surprising, right?",
                    "label": 0
                },
                {
                    "sent": "Because if you, if you really understand the proof that I show you, it's it seems impossible to get rid of it.",
                    "label": 0
                },
                {
                    "sent": "And basically.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You did the full OK. What is the problem?",
                    "label": 0
                },
                {
                    "sent": "Of course, define what is the risk.",
                    "label": 0
                },
                {
                    "sent": "So now the distribution is producing the training set in one shot.",
                    "label": 0
                },
                {
                    "sent": "There may be some correlation between the examples, so you have the same definition for the risk, except that you have.",
                    "label": 0
                },
                {
                    "sent": "You suppose that you you are not drying IID, so you have to consider all of them right?",
                    "label": 0
                },
                {
                    "sent": "You cannot say that the risk is.",
                    "label": 0
                },
                {
                    "sent": "Oh, you didn't?",
                    "label": 0
                },
                {
                    "sent": "I put a~ here.",
                    "label": 0
                },
                {
                    "sent": "I just don't understand why he didn't I. I suppose that I put in the other slide, so D is distribution that just give you the M example in one shot so you can reproduce this example.",
                    "label": 0
                },
                {
                    "sent": "The filter is not known OK, but you suppose that you can do some got somewhere that can give you another training set, for example an and what you want to say is in the average.",
                    "label": 0
                },
                {
                    "sent": "Of course you cannot calculate the true risk services as usual.",
                    "label": 0
                },
                {
                    "sent": "In the average, how good is some particle classifier?",
                    "label": 1
                },
                {
                    "sent": "And then you cannot say that it is equivalent to the probability of making an error on your new example.",
                    "label": 0
                },
                {
                    "sent": "It's the average of Eravu will make when you will see a new training set of the same size.",
                    "label": 1
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But we have the same test question.",
                    "label": 0
                },
                {
                    "sent": "Basically we want to see how good we can out, how good we can give some guarantee of the performance of some classifier using, But yes.",
                    "label": 0
                },
                {
                    "sent": "It's not the case you still you still keep with the 01 loss does not seem to be less.",
                    "label": 0
                },
                {
                    "sent": "Ya hey I I'm not sure time is going fast.",
                    "label": 0
                },
                {
                    "sent": "I have some resolve that I was supposed to present you tomorrow this morning on what happened.",
                    "label": 0
                },
                {
                    "sent": "If you go to other loss OK and of course it just gives some interesting POV forsake of simplicity.",
                    "label": 0
                },
                {
                    "sent": "I restrict 271 last bus.",
                    "label": 0
                },
                {
                    "sent": "Yes, in fact it's not necessarily the loss.",
                    "label": 0
                },
                {
                    "sent": "You can use basically any loss for that and also you can manage to use a different loss for your Gibbs classifier.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not necessary that it is simply the averaging of the risk.",
                    "label": 0
                },
                {
                    "sent": "You can have some other function on that so you can do a lot of things.",
                    "label": 0
                },
                {
                    "sent": "To to generalize and very very.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This way OK. OK, so so here's my~ So what can we do with that in general?",
                    "label": 0
                },
                {
                    "sent": "If the Taylor is a mess, nothing because it can be so big than your bound will be one.",
                    "label": 0
                },
                {
                    "sent": "OK, but",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you make some assumption, for example that yes, your training set is not IID, but it's basically a function of IID variable.",
                    "label": 0
                },
                {
                    "sent": "Ann not too many.",
                    "label": 0
                },
                {
                    "sent": "IID variable.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can do something and and the approach is to take advantage of that.",
                    "label": 0
                },
                {
                    "sent": "OK, an.",
                    "label": 0
                },
                {
                    "sent": "Are you can subdivide, as in various IID subset SJ, that can overlap.",
                    "label": 0
                },
                {
                    "sent": "OK. And you can put some kind of weight over those FJ.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, in such a way that if there is some overlap.",
                    "label": 0
                },
                {
                    "sent": "The way the total weight of all SJ that contain the given example will be one.",
                    "label": 0
                },
                {
                    "sent": "For those people who know about graph theory, it's about to make a fractional call.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ring of the graph of the interaction OK. Anne.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it is exactly the idea that they have an.",
                    "label": 0
                },
                {
                    "sent": "There is a concentration inequality debt.",
                    "label": 0
                },
                {
                    "sent": "Just say that you can do this.",
                    "label": 0
                },
                {
                    "sent": "An averaging is fine and the theorem is going at follow.",
                    "label": 0
                },
                {
                    "sent": "Basically you instead of having.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Based on S you have it on a family that overlap in some way.",
                    "label": 1
                },
                {
                    "sent": "You will be sure that.",
                    "label": 0
                },
                {
                    "sent": "You have this constraint that it's summer through to one or.",
                    "label": 0
                },
                {
                    "sent": "At not more than one, of course, to find the optimal way of making this color fractional coloring, it's something that's NP complete in general, so this is not about that is easy to use, but for very important situation, namely ranking.",
                    "label": 0
                },
                {
                    "sent": "You statistic of order two or three and so on.",
                    "label": 0
                },
                {
                    "sent": "We can manage to have a nice.",
                    "label": 0
                },
                {
                    "sent": "Set of SGI and basically we don't lose too many things.",
                    "label": 0
                },
                {
                    "sent": "A factor of 2.",
                    "label": 0
                },
                {
                    "sent": "If we do ranking instead of doing simple classification.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But again, this is the ugly term because the rest of the we can manage, but this ugly term.",
                    "label": 1
                },
                {
                    "sent": "Basically you can subdivide it in structure that are IID and treat them either as katani I've done or cigarettes to OK. OK. Well, I won't have time to do everything what I want to do, but the idea is the problem, bonding.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Gibbs risk.",
                    "label": 0
                },
                {
                    "sent": "Instead of the Bayes risk, this is something that has not been really solved yet, right?",
                    "label": 0
                },
                {
                    "sent": "And I will suggest two possible answer.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem, OK, the first one is if you can manage to have non two small part of your classifier voters H. OK, that are quite strong.",
                    "label": 0
                },
                {
                    "sent": "Then you will manage to.",
                    "label": 0
                },
                {
                    "sent": "When you see the data you will be able to find a posterior Q that will be not too different to do your prior.",
                    "label": 0
                },
                {
                    "sent": "If suppose your prior was uniform, OK and since they are good, the empirical risk Gibbs risk will be good.",
                    "label": 0
                },
                {
                    "sent": "Twice something that is good remains good.",
                    "label": 0
                },
                {
                    "sent": "Right and basically it's working.",
                    "label": 0
                },
                {
                    "sent": "This approach is working.",
                    "label": 0
                },
                {
                    "sent": "It's basically the idea of John Langford, John Schoettler to have the I think the tightest bound for SVM I know.",
                    "label": 0
                },
                {
                    "sent": "It was basically this idea.",
                    "label": 0
                },
                {
                    "sent": "OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "I will propose.",
                    "label": 0
                },
                {
                    "sent": "I will not propose it because I will not have time.",
                    "label": 0
                },
                {
                    "sent": "But there's another possibility also that we can do.",
                    "label": 0
                },
                {
                    "sent": "It's something that's a.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, the PAC Bayes bound we used to take the Gibbs risk, but we can make any estimation on on the on the on the training set that we want.",
                    "label": 1
                },
                {
                    "sent": "And compare it with true value.",
                    "label": 0
                },
                {
                    "sent": "We know that if we take the Gibbs risk, we have that the Gibbs twice the Gibbs risk is a pound of the base.",
                    "label": 1
                },
                {
                    "sent": "But we can have other kind of estimate that will give this indirect band.",
                    "label": 0
                },
                {
                    "sent": "Maybe it will be better if we want to find a good algorithm out of that.",
                    "label": 0
                },
                {
                    "sent": "Because if you look at the PAC Bayes bound on the Gibbs risk and you think that your voters are all week, they are all.",
                    "label": 0
                },
                {
                    "sent": "About a risk of 4849%, what will do the back Bays Mountain?",
                    "label": 0
                },
                {
                    "sent": "He basically will try to put the most important weight to the better one.",
                    "label": 0
                },
                {
                    "sent": "OK, and so you will end up with a better one, which is bad.",
                    "label": 0
                },
                {
                    "sent": "OK, but if you think about the majority vote should deal together in order that the weakness of the water will compensate one to the other.",
                    "label": 0
                },
                {
                    "sent": "Then you have a chance.",
                    "label": 0
                },
                {
                    "sent": "Give some interesting but interesting value.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And this is what I want to do with you.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first approach, my first answer.",
                    "label": 0
                },
                {
                    "sent": "OK, what you can do, you look for strong, strong people.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what you can do is the following.",
                    "label": 0
                },
                {
                    "sent": "You can simply say, well, I will go into a feature space that is very good.",
                    "label": 0
                },
                {
                    "sent": "I'll be of kernel for example, or something very strong right?",
                    "label": 0
                },
                {
                    "sent": "Ann, I will manage the complexity by the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "So here I have.",
                    "label": 0
                },
                {
                    "sent": "I know that I will have a bunch of classifier that will be very good on the training set, OK?",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An an.",
                    "label": 0
                },
                {
                    "sent": "So basically I will take majority vote of of my voters which will exactly is the same thing as the scalar product in that feature space.",
                    "label": 0
                },
                {
                    "sent": "OK, so each classifier here is basically a member of.",
                    "label": 0
                },
                {
                    "sent": "The the Big Fisher space and I take the majority vote simply by taking the DOT product and everything is easy to do it.",
                    "label": 0
                },
                {
                    "sent": "It's a kernel trick juice price here OK?",
                    "label": 0
                },
                {
                    "sent": "An because it's difficult to find the best posterior in general, even with the Catanese bound we.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Proposed to use prior and posterior data Goshen.",
                    "label": 0
                },
                {
                    "sent": "It should be better, more easy algorithmically.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And look what with what we end up with.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, since we are going using Goshen, the majority vote of the Goshen OK is exactly exactly the same outcome as the the voter.",
                    "label": 1
                },
                {
                    "sent": "That is the center of the Goshen, because basically they are cancelling otherwise.",
                    "label": 0
                },
                {
                    "sent": "So that's nice because for example for SVM what we have is it we want to have the abound on one vector, which is the combination of the correct combination of the supervector.",
                    "label": 0
                },
                {
                    "sent": "OK, but the PAC Bayes bound is giving us something that is an averaging of possible vector.",
                    "label": 0
                },
                {
                    "sent": "But if we are using lotion or something that is symmetrical then we know that the the majority vote out of that Goshen.",
                    "label": 1
                },
                {
                    "sent": "That is the continuous measure vote.",
                    "label": 0
                },
                {
                    "sent": "Is equivalent as the same outcome as the value of the only voters.",
                    "label": 0
                },
                {
                    "sent": "That is, the center of the Goshen that you look at OK. And of course, so the the output of the SVM will be equal to the Goshen that I told you, and I will be able to bound it by twice the Gibbs risk.",
                    "label": 0
                },
                {
                    "sent": "An If I manage to be in a very, very strong feature space, this will not be so bad.",
                    "label": 0
                },
                {
                    "sent": "Right, I skipped a lot of details, but basically this is the idea that leads to disbound of John and John OK. And as prior.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We say that we would use Goshen, so that's nice because the callback library divisions between 2 Goshen as a very nice form.",
                    "label": 0
                },
                {
                    "sent": "To the photo calculation, it's easy.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, also we want to compute.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to compute the Gibbs risk and this is not so difficult.",
                    "label": 0
                },
                {
                    "sent": "Also you make a lot of calculation and basically it's.",
                    "label": 0
                },
                {
                    "sent": "The profit function that goes there plus the norm of the W plus gamma, which is basically very similar to the match.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you look at the probit loss.",
                    "label": 0
                },
                {
                    "sent": "And you compare with the inch loss.",
                    "label": 0
                },
                {
                    "sent": "You can see that dangerous is convex.",
                    "label": 0
                },
                {
                    "sent": "The probit loss is not OK.",
                    "label": 0
                },
                {
                    "sent": "But if you want to make a conversation, a convex relaxation of my profit loss, dangerous is a good idea.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is it.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly.",
                    "label": 0
                },
                {
                    "sent": "What happened so we look at the Catanese bound and Cantonese bound said, well, if you want to minimize me, you have to minimize this expression.",
                    "label": 0
                },
                {
                    "sent": "You replace what is this guy you replace?",
                    "label": 0
                },
                {
                    "sent": "What is this guy?",
                    "label": 0
                },
                {
                    "sent": "And so here is the problem.",
                    "label": 0
                },
                {
                    "sent": "It's not so easy to find the solution because I show you the probit loss is not convex, so there may be many minimum, but we can make some restart gradient decent.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things is quite easy to do OK.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make the comparison with the SVM.",
                    "label": 0
                },
                {
                    "sent": "So this VM you minimize the hinge loss instead of minimizing the probit loss.",
                    "label": 1
                },
                {
                    "sent": "Of course it's convex.",
                    "label": 1
                },
                {
                    "sent": "So suppose that we have a pack based theory in the 80s and we want to minimize fact based bound and we say, well, probably class is something that is too complicated to minimize.",
                    "label": 0
                },
                {
                    "sent": "Let's make some convex relation of it and do the trick.",
                    "label": 0
                },
                {
                    "sent": "We have another interpretation of the.",
                    "label": 0
                },
                {
                    "sent": "See it.",
                    "label": 1
                },
                {
                    "sent": "This is the hyperparameter of the Catanese bound.",
                    "label": 0
                },
                {
                    "sent": "This is the seed that is related to dual variable right, but basically, especially if you don't have any knowledge a priori and you choose to have a Goshen centered in zero for your prior, you end up basically within SVM.",
                    "label": 0
                },
                {
                    "sent": "By minimizing Pacbase bound on Goshen.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clear.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and is it working well well again?",
                    "label": 0
                },
                {
                    "sent": "If you don't want to have any parameter to tune, so instead of using the Catanese map, use the Segers bound.",
                    "label": 0
                },
                {
                    "sent": "It's not as good as this VM.",
                    "label": 0
                },
                {
                    "sent": "But it's difficult on UCI data set to make difference because all I got in is quite good.",
                    "label": 0
                },
                {
                    "sent": "But in in the in the overall has VM is a bit better.",
                    "label": 0
                },
                {
                    "sent": "OK if you want to take half of the data to learn the prior and the other half of the data to just make the minimization.",
                    "label": 0
                },
                {
                    "sent": "It's a bit better.",
                    "label": 0
                },
                {
                    "sent": "The bank is tighter.",
                    "label": 0
                },
                {
                    "sent": "But the accuracy is not as good as SVM.",
                    "label": 0
                },
                {
                    "sent": "But if you cross validate the parameters C of the Catanese Band.",
                    "label": 0
                },
                {
                    "sent": "And but then you end up with a lot of minimum.",
                    "label": 0
                },
                {
                    "sent": "It's not convex, so you have to make a lot of restarts that good algorithm in as a performing, but for accuracy.",
                    "label": 0
                },
                {
                    "sent": "It's a bit better.",
                    "label": 0
                },
                {
                    "sent": "India average.",
                    "label": 0
                },
                {
                    "sent": "You seed the optimization problem with the.",
                    "label": 0
                },
                {
                    "sent": "Output of optimization of Angeles.",
                    "label": 0
                },
                {
                    "sent": "D. On the Internet optimization, we get it.",
                    "label": 0
                },
                {
                    "sent": "We get a solution that's convex.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we could use it to see the optimization of the public loss.",
                    "label": 0
                },
                {
                    "sent": "Preps avoid a lot of the restarting right, and then if it's it's get out with a better solution, it will be better.",
                    "label": 0
                },
                {
                    "sent": "Yes, good idea.",
                    "label": 0
                },
                {
                    "sent": "We try it.",
                    "label": 0
                },
                {
                    "sent": "No, maybe we should.",
                    "label": 0
                },
                {
                    "sent": "No, it's a good idea, yeah, so yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "But you know, you don't start at random.",
                    "label": 0
                },
                {
                    "sent": "You start at some good solution so.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's OK.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if we have weak learner like boosting, for example, is dealing with weak learner.",
                    "label": 0
                },
                {
                    "sent": "If, as I tell you, if we do that then we end up with something that will not work, minimizing the back base bound among very bad classifier will end up with.",
                    "label": 0
                },
                {
                    "sent": "We will choose the less bad classifier or.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of the less one.",
                    "label": 0
                },
                {
                    "sent": "And it's work.",
                    "label": 0
                },
                {
                    "sent": "Well, this is what decision stump again.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compare with that at the boots with the same decision stump PD one.",
                    "label": 0
                },
                {
                    "sent": "We use the Seeger bound, so no parameter to tune.",
                    "label": 0
                },
                {
                    "sent": "It was about the same.",
                    "label": 0
                },
                {
                    "sent": "We tried to run the prior a bit better.",
                    "label": 0
                },
                {
                    "sent": "If we learn the prior, the bound are tighter and we make our validation for the hyperparameters seek.",
                    "label": 0
                },
                {
                    "sent": "And then it's better than.",
                    "label": 0
                },
                {
                    "sent": "Then we're back on HUCI data set.",
                    "label": 0
                },
                {
                    "sent": "Nothing is significant because it's always basically the same.",
                    "label": 0
                },
                {
                    "sent": "But if you go in the overall here, the bold ones are not easy to see, but basically they are mostly in the third one.",
                    "label": 0
                },
                {
                    "sent": "So by doing that again, we are bit better than state of the art, but it's not a idea because we have to work because we are we don't have convex situation situation.",
                    "label": 0
                },
                {
                    "sent": "But it's a bit better.",
                    "label": 0
                },
                {
                    "sent": "We can also make a complexification.",
                    "label": 0
                },
                {
                    "sent": "But then we go back basically to SVM, OK?",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "May I?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It's OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "It will not consider the Gibbs risk as the important thing that I want to look at.",
                    "label": 0
                },
                {
                    "sent": "I will consider.",
                    "label": 0
                },
                {
                    "sent": "A function of the margin.",
                    "label": 0
                },
                {
                    "sent": "The margin is is simply the expected value of.",
                    "label": 0
                },
                {
                    "sent": "The product of white.",
                    "label": 0
                },
                {
                    "sent": "I'm X so look at it.",
                    "label": 0
                },
                {
                    "sent": "If the margin is positive, that means that the weight of the classifier that are OK for that example is bigger than the weight of the one that are wrong.",
                    "label": 1
                },
                {
                    "sent": "If the so the majority vote will be.",
                    "label": 0
                },
                {
                    "sent": "Good, you will not make an error on that example if the margin is negative then it's really the contrary, OK?",
                    "label": 1
                },
                {
                    "sent": "So I think this is something important and I say well, I will make backbase bound not on margin but on.",
                    "label": 0
                },
                {
                    "sent": "Polynomial function, in fact Taylor series.",
                    "label": 0
                },
                {
                    "sent": "Based on that expression, the margin.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as I tell you, I want to have a bound and then direct bound on the majority vote.",
                    "label": 1
                },
                {
                    "sent": "Basically the majority vote is the risk of the majority.",
                    "label": 1
                },
                {
                    "sent": "Vote is exactly that when the margin is negative, the majority vote makes an error.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, here you can put or equal to 0, supposing that if the majority vote is even, then you declare that you make an error just to simplify the map, but you can not also.",
                    "label": 0
                },
                {
                    "sent": "OK. And so I want to have a function that are simply the property to be.",
                    "label": 0
                },
                {
                    "sent": "Bigger than.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This indication function and meaning basically.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so where is the indication function?",
                    "label": 0
                },
                {
                    "sent": "OK, I did that on my picture.",
                    "label": 0
                },
                {
                    "sent": "I do not see to see ties my function.",
                    "label": 0
                },
                {
                    "sent": "It says it.",
                    "label": 0
                },
                {
                    "sent": "And so, as I tell you, if the margin is positive, then the majority vote is OK.",
                    "label": 0
                },
                {
                    "sent": "So yes, no loss.",
                    "label": 0
                },
                {
                    "sent": "And if it's a margin is negative, then the majority vote is making an error.",
                    "label": 0
                },
                {
                    "sent": "So he has one loss.",
                    "label": 0
                },
                {
                    "sent": "And basically what we are doing is we are making the averaging on all the example of the training set.",
                    "label": 0
                },
                {
                    "sent": "Of that, to see what is the empirical risk of the majority vote.",
                    "label": 1
                },
                {
                    "sent": "OK, if instead of looking at this we look at the green line, which is a function of the margin.",
                    "label": 0
                },
                {
                    "sent": "Which is 1 minus MQ and so we will make will evaluate the empirical loss that we will obtain on that.",
                    "label": 1
                },
                {
                    "sent": "So for each example we look at the margin and we compute what its value.",
                    "label": 0
                },
                {
                    "sent": "And we make the average OK. And this will be the risk on that particle loss, OK?",
                    "label": 0
                },
                {
                    "sent": "Good, this green loss is interesting because one minus the margin.",
                    "label": 0
                },
                {
                    "sent": "It's easy to prove that is twice the Gibbs risk.",
                    "label": 0
                },
                {
                    "sent": "And then when I tell you in the in the beginning that the Bayes risk the risk of the base is always smaller than equal than twice the Gibbs risk, here's the proof.",
                    "label": 0
                },
                {
                    "sent": "If you make the calculation, of course, since this one is always over the black one.",
                    "label": 1
                },
                {
                    "sent": "Expectation of that will always be bigger, OK, but we can do other kind of function.",
                    "label": 0
                },
                {
                    "sent": "For example, we can use a parabola.",
                    "label": 0
                },
                {
                    "sent": "An exponential function and so on, provided that.",
                    "label": 0
                },
                {
                    "sent": "By our function is over this indicative function that represents the loss of the.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK fine.",
                    "label": 0
                },
                {
                    "sent": "And basically we have a pack based on that.",
                    "label": 0
                },
                {
                    "sent": "Here is the catoni.",
                    "label": 0
                },
                {
                    "sent": "It's a bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "Basically it's about the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so again we have this trend constant.",
                    "label": 0
                },
                {
                    "sent": "We have the true.",
                    "label": 0
                },
                {
                    "sent": "It's not a risk now through Zeta value, the Zeta value that is calculated on the training set, so they choose data value is always smaller than equal.",
                    "label": 0
                },
                {
                    "sent": "Then this empirical estimates.",
                    "label": 0
                },
                {
                    "sent": "An then we have some new constant that appear that are called CA that does not appear in the Catanese bound because the value is 1 and escape bar also that corresponds to the value of the Zeta function on one and the value of the deffective of visitor function on one OK. Yeah, I've to say that to have this simple calculation we have to suppose that the coefficient on the.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The AK are all positive, otherwise we can manage to have something else.",
                    "label": 0
                },
                {
                    "sent": "Something that works too, but Recalculation is a bit more.",
                    "label": 0
                },
                {
                    "sent": "Delicate, OK?",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_93": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the trick?",
                    "label": 1
                },
                {
                    "sent": "You trick is quite easy.",
                    "label": 0
                },
                {
                    "sent": "Basically it's simply that this function of the margin I tell you that basically the Gibbs risk is equivalent to have a bound on the margin exactly or linear function of the magic.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we what you can do is to transform your problem, you transform the set of classifier H into a new set of classifier H bar and managed to.",
                    "label": 0
                },
                {
                    "sent": "That does it, a value of the margin is exactly a constant times the true margin in this new problem.",
                    "label": 0
                },
                {
                    "sent": "And then you can apply the theorem using the Gibbs risk, basically with some constants to manage OK. And the trick is basically the following.",
                    "label": 0
                },
                {
                    "sent": "It's the set of your set of classifiers, basically it's.",
                    "label": 0
                },
                {
                    "sent": "To to evaluate, because as you can see it's margin.",
                    "label": 0
                },
                {
                    "sent": "This is something that is associated to the moment.",
                    "label": 0
                },
                {
                    "sent": "The first moment that gives risk.",
                    "label": 1
                },
                {
                    "sent": "OK, the square of the margins, the second moment of the margin basically, and things like that.",
                    "label": 0
                },
                {
                    "sent": "And So what you can do is face basically to be able to evaluate the linear term of Jose to function you.",
                    "label": 0
                },
                {
                    "sent": "Basically you only have to calculate the Gibbs risk.",
                    "label": 0
                },
                {
                    "sent": "So you take one classifier.",
                    "label": 0
                },
                {
                    "sent": "Can you look how is good or not and you make the average on that for the squared term you have to take to classifier and look how they work together.",
                    "label": 1
                },
                {
                    "sent": "And then make a computation and this computation is basically then like this function I'm getting out of time.",
                    "label": 0
                },
                {
                    "sent": "Pascal will talk a bit about that later in in this session OK, and so if we manage to have this very non Gibbs rates problem.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rick Dick if we can retrieve it gives this problem, then we can simply apply the usual back based on that and see what happens.",
                    "label": 0
                },
                {
                    "sent": "OK, and the calculation of the Combat library is quite easy to make.",
                    "label": 0
                },
                {
                    "sent": "Of course the combat labor division between those two will be bigger because we are working on a very much bigger set of classifiers, but we can manage and then if we use a Catanese bound.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically what he says is we have to minimize this.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will go a bit faster.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we were interesting in two type of loss.",
                    "label": 0
                },
                {
                    "sent": "The exponential loss and a quiet class with some parameter gamma.",
                    "label": 0
                },
                {
                    "sent": "OK Huawei, the exponential US boosting was inspiring and why the quadratic class?",
                    "label": 0
                },
                {
                    "sent": "Because this is the easiest after the linear case, which we knew was not good.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An we try to minimize things.",
                    "label": 0
                },
                {
                    "sent": "It works quite well.",
                    "label": 0
                },
                {
                    "sent": "Again it can go yeah and we compare with Ridge regression at the boost.",
                    "label": 0
                },
                {
                    "sent": "And basically we are in the state of the art.",
                    "label": 0
                },
                {
                    "sent": "We are not deeply beating them, but we are in a state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Internet we now have to say it fastly, but what is interesting is the if you restrict yourself in some particular case.",
                    "label": 0
                },
                {
                    "sent": "OK so you have this situation where if you have a classifier you have this compliment.",
                    "label": 0
                },
                {
                    "sent": "You have a situation of the posterior distribution is allying on, the prior Pascal will talk about what it means in particular.",
                    "label": 1
                },
                {
                    "sent": "Then it's quite easy to make the calculation of the combat librarian.",
                    "label": 0
                },
                {
                    "sent": "Basically you end up with.",
                    "label": 0
                },
                {
                    "sent": "This formula OK.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why this formula is interesting?",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's because then, then here is the function that Pacbase says that you have to minimize if you want to work.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to finish.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If this data function is the exponential loss you obtain.",
                    "label": 0
                },
                {
                    "sent": "D exponential The boosting algorithm but with some regularization term?",
                    "label": 0
                },
                {
                    "sent": "Which year is the square but the true one is basically the kill that library OK?",
                    "label": 0
                },
                {
                    "sent": "And if you use a quadratic class, you end up with the Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "Donkey",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In conclusion, we tried to.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find Newton new algorithm out.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the pacbase",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By minimizing it and.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's good in some sense because we re discover the best known algorithm naturally.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's bad because we were not the first.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But maybe if we go further in.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the regression instruc.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "True output ranking.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe we can find some.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something that has.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not been found yet.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}