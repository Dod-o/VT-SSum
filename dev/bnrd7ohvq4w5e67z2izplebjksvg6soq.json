{
    "id": "bnrd7ohvq4w5e67z2izplebjksvg6soq",
    "title": "Bayesian Data Fusion with Gaussian Process Priors : An Application to Protein Fold Recognition",
    "info": {
        "author": [
            "Mark Girolami, School of Computing Science, University of Glasgow"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "June 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/pmsb06_girolami_apfr/",
    "segmentation": [
        [
            "Max.",
            "Both.",
            "Application.",
            "OK, thank you so.",
            "This is typically the time when everyone gets very tired and sleepy because it's just after lunch and at the end of the course to the end of the workshop.",
            "So I'll try and be quite brief here, so the whole idea of this work was to see if it would be feasible to use Gaussian process priors for.",
            "Typical computational biology problems such as coding for protection.",
            "Or coding function protection where there were diverse sets of data and where there were multiple classes or multiple labels are Invoker, Ross give are very nice talk which will motivate a lot of that.",
            "Idea this morning.",
            "So I'm really following up or not work.",
            "I'm presenting another inferential paradigm based on Gaussian processes."
        ],
        [
            "For this sort of problem, an so I'll get some motivation for Bayesian inference in particular another classic."
        ],
        [
            "Asian predictive setting.",
            "And.",
            "Many of you will be more than familiar with Gaussian process priors, but some of you won't sort out.",
            "I'll just give up a brief."
        ],
        [
            "Introduction to what they are.",
            "And then discuss the whole notion of of composite covariance functions to allow us to enter data.",
            "And again, this is very very similar to the water."
        ],
        [
            "Dentify, Volker this morning and then I'll give us a brief proof of concept application, which was this protein fold prediction problem."
        ],
        [
            "So if we want to infer the class membership over multiple classes are multiple labels of classes of objects, then certainly."
        ],
        [
            "Employing probabilistic inference is quite desirable, especially if we have associated misclassification costs and so forth.",
            "And if you're very high dimensional data or structured heterogeneous data an then it may well be required that we would.",
            "We need to do a further level of influence.",
            "So for example we might need to infer the particular ratings oh of each of these datasets as to which ones are predictive of the."
        ],
        [
            "Guns.",
            "And of course, in terms of nonparametric classification methods, these support vector machines have been our large margin.",
            "Machines certainly have been very, very successful.",
            "I don't think a month goes by without a paper appearing in the Journal of Bioinformatics where someone hasn't applied an SVM to see some problem an it's testament to to the their success, especially high dimensional problems."
        ],
        [
            "I dimensional feature sets.",
            "So of course, because of the nature of my talk, I would have to say in both attention that baseball games are non probabilistic.",
            "And we can obtain some form of probabilistic semantics and sort of postpone."
        ],
        [
            "Mono and it's not without problems.",
            "Anne.",
            "Implicitly, the SVM acoustic will define a binary classifier, and if we want to look at multiple classes on multiple labels and then we need to look at either one versus 1, one versus race combinations or.",
            "Directed acyclic graphs.",
            "Although having said that and some people live in some publications where film multiclass SCN representations have been developed."
        ],
        [
            "Of course, if we want to do any influence over the relevance of particular features or the relevance of a particular datasets, then of course we need to look at other potential methods, either using cross validation or the same indefinite programming method of get blanket, which again, is Dawn said attention this morning is really only."
        ],
        [
            "Powered off, Asmodee developed an entirely new case.",
            "But if we employ and if we adopt goes in process priors, then we have a very nice consistent framework to do inference over our classification problem.",
            "Whether we are working over multiple classes.",
            "If we wish to infer the relevance or the way things that various features will have, how we integrate certain different types of data, whether it be by composite kernel functions, covariance functions, or.",
            "Camera combinations.",
            "And so we thought, then I.",
            "Think it would be worthwhile having a look at these Gaussian?"
        ],
        [
            "Who saved lives?",
            "So I guess in process really defines a distribution over functions, so we have some functions space which Maps some."
        ],
        [
            "The meeting went to the real lines and the process of course is stochastic process, but it is defined simply by a train function of the main function.",
            "And some covariance function.",
            "And."
        ],
        [
            "The Gaussian process would be the famed of probability of F for any subset of the domain.",
            "If the marginal distribution, the probability of these nonlinear functions of X races, either or whatever, some index of ascetics.",
            "If this is a multivariate goes in, then we have this in process so."
        ],
        [
            "It means if we have NN samples from.",
            "Some feature feet space of an objects.",
            "Then the end by 1 dimensional.",
            "Ghost in process.",
            "Set of random variables will be defined by a multivariate Gaussian with some mean function and some covariance."
        ],
        [
            "And that is.",
            "In a sense, the prior that.",
            "We will then use for subsequent influence in our classification system, which is really the focus of this talk, so it includes some sort of knowledge of certainly some set of assumptions as to the functional class or the smoothness of the roughness.",
            "Of the functions."
        ],
        [
            "That we would be employing Anna very simple example here is, let's assume that we have are.",
            "O train function.",
            "And covariance function, which is basically defined by this mean of you, will be familiar with this is our radial basis function as this stationary function.",
            "And we can then of course."
        ],
        [
            "Draw samples of functions.",
            "So if we had 30 particular points and we were just to draw sample functions from this Gaussian process where we have defined affected the hyperparameter variables is the scale and this this length steel parameter here.",
            "To these values, to unit value into 1 / 10.",
            "Then we get these.",
            "If I join the dots then we get these nice smooth functions.",
            "If I reduce the reliance steel by increasing this particular."
        ],
        [
            "Parameter, then the form of priors that weaken the functions that we can draw from this prior had become."
        ],
        [
            "And increasingly non smooth and rough."
        ],
        [
            "OK, so those are very simple."
        ],
        [
            "Says fires."
        ],
        [
            "An I guess the key thing is is that the covariance function and the parameters which which defined that will represent our prior assumptions or."
        ],
        [
            "In the smoothness of functions, which would be using and the very standard examples, this little simple regression."
        ],
        [
            "Problem.",
            "So we have a truth."
        ],
        [
            "& X / X and we make some observations of X and.",
            "He where he is.",
            "The true function value plus some error terms of noise term, which we assumed to be to be Gaussian, particular variance.",
            "So we're assuming Gaussian errors in our."
        ],
        [
            "Nation is it will.",
            "Now, if you please apply it on on the particular functions.",
            "We we condition a parameters of a covariance function under the deploy."
        ],
        [
            "Anne.",
            "We've spoken about this prior the likelihoods because we've assumed Gaussian errors, of course, is just a standard product."
        ],
        [
            "And because we're dealing now with Gaussian errors, I goes in prior then the posterior over goes in process variables is also going to be."
        ],
        [
            "Goes in and out.",
            "It will go soon with covariance function which is dependent on the function of the prior plus the noise term and then of course we have a. I mean value which is some representing some representation based on our our target values around data.",
            "And the predicted distribution over any new data samples is also going to be ghosting, as is the marginal likelihood the normalizing coefficient here.",
            "So everything is Gaussian."
        ],
        [
            "Which means that infants can can take place with well but one line of code basically.",
            "So here we have 100 data samples from this sign X / X function with additive noise just quite quite a small level of variance, and the in red you can see the true function value that little blue dots.",
            "Surely the samples that we've observed of X and the.",
            "Brown black dotted line gives us a posterior knee estimate and these are plus minus one standard deviation, so you can see clearly where we've got beta.",
            "Then our predictions are fairly confident and.",
            "Advective standard deviations are relatively small in regions where we don't have a lot of data, then clearly the posterior variance increases, and here produces virtually no deal at all.",
            "You can see that the variance spikes off to the prior variance."
        ],
        [
            "And of course, if we change the parameters of our covariance function.",
            "So we assume a rougher function then.",
            "This is exactly what we did.",
            "You get our.",
            "Our posterior inference indicates our posterior mean, which tries to fit the data points.",
            "Much cleaner.",
            "No, so this is all very nice.",
            "We can do through posterior inference if we assume that we have fixed these are hyperparameters.",
            "So if we want to unfair, do any influence on these hyperparameters, then we need to either one resort to Markov chain Monte Carlo.",
            "Which isn't such a hardship or becausw the marginal likelihood.",
            "It is also a Gaussian form and if the covariance function happens to be differentiable and then we could employ some sort of type 2 maximum likelihood to do inference on these.",
            "So this is all very."
        ],
        [
            "Of course everything breaks down though.",
            "When we use a likelihood term which is no longer Gaussian.",
            "And for example, in classification we have Anang."
        ],
        [
            "Using likelihoods on, we therefore have to.",
            "Really simulate samples from our posterior using some sort of Markov chain metroplus Hastings, and then any predictive likelihoods would have to be computed using some Monte Carlo, and I've given some details in the outside of our blocked Gibbs sampler with our Metropolis subsampling, which works reasonably well.",
            "Of course, for those of you who worked with started sessions, this is a talk would be over now, 'cause we've defined our prior.",
            "We define the likelihood we just look at 2.",
            "Quality switch on our gift sampler and then pick up the results when we combine button."
        ],
        [
            "Point faces if we could.",
            "Um?",
            "Make some approximations which are are good and some sense.",
            "And of course the NIPS community.",
            "Ice email communities seem to spend all of their lives or life, so we can get good approximations and then surface sessions looked on.",
            "Bemusedly comes in fiber."
        ],
        [
            "Spending all this time, but I think it's very important so the sort of approximations that we can employ.",
            "Well, we can employ a maximum a posteriori estimator.",
            "Approximation to approximate are.",
            "Our posterius we can use variational approximations where we employ a mean field method and of course there are other things like expectation propagation which we could employ.",
            "So all these tools that are available to us, but what I would like to do is.",
            "Sure, yeah."
        ],
        [
            "Quite a nice trick which actually comes from the statistics community, and if we consider a very simple case of our binary classification problem where our target values are going to take on one or zero.",
            "So we would just dichotomizing music two classes.",
            "I very classical likelihood term, of course, is the probability you know the profit function.",
            "Now what's interesting about.",
            "Doing GP influence with approve it function well.",
            "First of all we would need to use a metropolis sampler to do posterior inference.",
            "But if we introduce an auxiliary variable, so one which really doesn't have anything to do with a particular model, which is just simply I standardized.",
            "Goshen, which is a distribution which is standardized ghost incentive.",
            "That particular value of your Gaussian process variable.",
            "Then, of course, the prover function is simply the marginal form of the joint distribution of your auxiliary variable and your target label.",
            "Which we can write like this."
        ],
        [
            "Conditional forum.",
            "Now the interesting thing is that if we define the probability of a target variable being one given this auxiliary variable as being a Delta function, if Y is greater than 0.",
            "Basically we're just splitting why into positive and negative line, then the well.",
            "Of course the launch and was just this private function which is a normalizing constant of our left truncated Gaussian, but the key point here."
        ],
        [
            "Start.",
            "If we include the auxiliary variable.",
            "Then remember that we have a ghost.",
            "In process we have a multivariate ghost.",
            "In prior on our on these random variables if.",
            "And what it means is that we know how that goes in joint distribution and from here within our overall likelihood, which means that we can make progress and defining one against sampler because the conditional likelihoods.",
            "Of F&Y will be multivariate Gaussian and then a truncated goes in which we can sample from.",
            "So we don't need to resort to Metropolis sample for approval function.",
            "We can use a nice gift sample.",
            "Because you're also going to get sample, then it means that."
        ],
        [
            "We can stop thinking about.",
            "Mean field approximations.",
            "So that's four.",
            "Binary classification of course, we're really interested in is multiclass, where in the full recognition problem there are multiple folds 2013 whatever that we need to predict.",
            "So in that case we can use rather than a binary probit function.",
            "We can use our multinomial probit function and there's a paper quite a quite a long people.",
            "It's just appear which gives details of this here, but basically what we end up doing is introducing.",
            "A random variable which is now key dimensional, which again has zero mean it's a zero mean Gaussian with unit covariance.",
            "And this likelihood turn here the multinomial prove.",
            "It basically takes this key dimensional space, which are our Gaussian variable sets and and splits it into key nonoverlapping cones.",
            "OK."
        ],
        [
            "So we have these coins now.",
            "On again we can define our gift sampler for this Anna very straightforward minor.",
            "The details are in the abstract.",
            "But the nice thing now is that we can we can employ variational methods without making any explicit approximations to the likelihood theorem.",
            "All that we will do is we will assume in the usual standard mean field approximations that the full posterior over our Gaussian process variables under auxiliary variables is block factored."
        ],
        [
            "I'm going through.",
            "All the the derivations which are fairly straightforward.",
            "It's close of you who are familiar with these methods.",
            "All you need to do is look at the Gibbs sampler and you can get these approximate.",
            "Posteriors from the conditionals and basically what we would have then, is a product of key multivariate Gaussians, where we have this posterior.",
            "Expectation over our ability variables."
        ],
        [
            "And we find that the approximate posterior over auxiliary variables is simply a product of these.",
            "Completed oceans and the zirconic, truncations of the multivariate goes in and this key dimensional space where if tiene a target variable corresponds to the earth class then the I TH variant.",
            "In this of this, auxiliary variable will be the largest.",
            "So basically we were placing why into the icon in this key dimensional space, and hopefully the posterior expectation for each Y which would require up here I actually turns out to be just a simple nonlinear function over the posterior expectations of our.",
            "Our GPS random variables, which again we can obtain from well, it's just simply this here."
        ],
        [
            "I'm so.",
            "That means that you have a very."
        ],
        [
            "A nice way of."
        ],
        [
            "Do you have full procedure inferencing and essence all over acquires this updating of the?",
            "The key.",
            "Sets of GP random variables."
        ],
        [
            "Where each element in this meet in this vector here is defined by this nonlinear function of the.",
            "The corresponding values of our GPU random."
        ],
        [
            "Universe so this is quite nice, no ceiling.",
            "Is all the cane cubed?",
            "That immediately tells you that clearly GPS I'm not going to be applied on datasets where there are millions and millions of data points, but it's certainly."
        ],
        [
            "A lot nicer than doing full MSMC.",
            "So the point of all this is that we can also do through variational inference on the hyper parameters of the covariance."
        ],
        [
            "Function just by employing an important sample."
        ],
        [
            "No quest."
        ],
        [
            "As well who disagree."
        ],
        [
            "The approximation well again."
        ],
        [
            "Measuring productively for who's with the VB approximation in comparison to give some of that, there is very little over may datasets.",
            "Simply that it's very little difference is statistically significant.",
            "I'm.",
            "If I know just.",
            "Move onto.",
            "The main point here."
        ],
        [
            "I should just point out that this is inference over.",
            "10 parameters of a covariance function and the the data is such that has been defined that the multiple classes require two parameters are two features and the other eight.",
            "I'm not predictive of the target values, so I've overlaid the output samples from a metropolis subsample.",
            "Talk of the posterior mean estimates, and we're talking about round about 150 times faster.",
            "If we if we just employees deterministic, I mean field approximations.",
            "So it's certainly."
        ],
        [
            "I'm like why Sophia protective lately it's.",
            "So it's sad."
        ],
        [
            "They are quite a nice.",
            "Alternative to the Fool MCMC, no.",
            "Of course, a convenience function can be some linear combination of base covariance functions, and this is exactly that.",
            "The form that was presented this morning by Volker, and actually, that this this isn't in any way I'd hope this actually comes from assuming our linear or non linear additive model over our GP priors.",
            "This would then be the composite covariance function."
        ],
        [
            "So."
        ],
        [
            "Who?",
            "Who?"
        ],
        [
            "When I want to do is then."
        ],
        [
            "Do inference."
        ],
        [
            "For multiple repres."
        ],
        [
            "Indications, and of course we had this morning, but the headline casework on protein function prediction, as have been hard, is also combined.",
            "Multiple came over multiple covariance functions and trying to predict protein protein interactions an."
        ],
        [
            "Is also been work on enzyme networks."
        ],
        [
            "So forth."
        ],
        [
            "Well, in this case, all that we really need to do is.",
            "Just we're learning the camera combinations.",
            "There really is nothing more that we need to do.",
            "We just do the variational inference over the linear."
        ],
        [
            "Tweets of the kernel combinations underdone.",
            "So the.",
            "For example here.",
            "Which was on.",
            "One of which was presented by."
        ],
        [
            "My thing and do Chuck we Viking 2000 and they were trying to using some very low homology sets of proteins.",
            "They were trying to predict the fold over 27 possible scope folds and beer."
        ],
        [
            "Six different datasets derived from the protein sequences the employed one versus one combinations of ACMS and then some voting method."
        ],
        [
            "To combine them all."
        ],
        [
            "I know I'm dependent at set of protein.",
            "They were getting a raw score of about 43 1/2% correct predictions and then with some manual massage ING they could get up to 56 1/2%.",
            "And I noticed that bioinformatics just this month and other people has come out using the same data set and managed to tweak the performance of its 62% in Windows 20."
        ],
        [
            "7.",
            "So the 60 to stateside amino acid composition is the secondary structural elements, hydrophobicity profiles, polarizability, polarity in the border."
        ],
        [
            "So I'm.",
            "I basically just took.",
            "Certain covariance functions and I use this middle basis function for each.",
            "Each of the different datasets and I play star set of independent gamma priors or on each of these."
        ],
        [
            "Covariance function weighting coefficients."
        ],
        [
            "Around the variational Bayes."
        ],
        [
            "Teen and here's the performance that we get.",
            "If I just look at the raw performance on each of the single datasets that this is the percentage accuracy, so we're about 53% using just amino acids.",
            "Running about 45% or if we just employees taking this structural elements and so forth.",
            "So if we combine all of those multiple datasets and a composite covariance function maybe get a boost up to 6263%.",
            "Now the interesting thing is, is that for this problem, if we just throw all of these into the covariance function and equally weak them, so we just have one six on each of them, then the prediction accuracy is exactly the same."
        ],
        [
            "What's interesting, though, is that the predictably for his, so how?",
            "How confident are the predictions that are correct?",
            "Then when we when we try and fair and optimal waiting, we do get are more confident set of predictions and it's interesting to get lancret his key paper.",
            "There was one particular prediction problem where the just a straightforward linear, uniform weighted combination of his datasets.",
            "Really make any difference in comparison to the optimized said.",
            "So we see the same here, but there's certain in terms of projected performance, the 2nd."
        ],
        [
            "Label in France is doing something useful, and what's interesting is that the.",
            "The posterior mean estimates that we get simply indicate that the amino acid composition and and the predicted secondary structural elements are really the main components of the multiple datasets that are required for this particular problem, and these are really pretty well down weighted."
        ],
        [
            "So I think I'm coming to the end, yeah, and the inference is fairly quick, so so the learning this combination.",
            "Of data."
        ],
        [
            "Really quick so.",
            "To summarize, in few minutes where.",
            "If we use Bayesian inference in a general classification setting so this is over multiple classes, and indeed this, this can be employed over multiple labels as well.",
            "Then we can do it feeling actually an appeal."
        ],
        [
            "Straightforward way if we want to integrate different datasets.",
            "I've very briefly and I'm handwaving minor, discussed the variational approximations that can."
        ],
        [
            "Employed instead of resorting to through MCMC, we achieved what is currently the state of the art performance on this whole recognition program without a manual massage ING."
        ],
        [
            "Or cheating an?",
            "Of course we could employ this on heterogeneous data combinations similar to the ones that Volker presented this morning and that's yet to be done.",
            "Not set.",
            "Mike quick question.",
            "Assess the utility of your predictions.",
            "It would be nice to have a lot of confidence of connections and accuracy.",
            "Yes, have you you have such a sense of what it might look like?",
            "At this point it is with countless because you saw the overall average confidence and the predictions.",
            "I would be interested in yes, like very confidently be more confident.",
            "Good point and other streaming.",
            "OK. You spoke about it.",
            "Combinations.",
            "Would you be able to do a local combinations of your career covers?",
            "Mattress is so local means that you have different coefficients.",
            "Spectre was some of the data points out is that each of those queries functions and put their own hyperparameter switch.",
            "You just in fair in any case.",
            "So if you wanted to actually do certain features of whatever their covariance functioning in computer program files and those and then just do that, yeah, we can talk about that later.",
            "Sweet.",
            "So there's this.",
            "Coefficients in those response when you have many kernels, right?",
            "So there's a one normalization on top of that.",
            "Do you also find this in your approach?",
            "So we had some previous market represented Ice email last year by employing these gamma type files and find that you have employed initially type fire which is in essence what you were doing by making a convex combination, then yes, you would actually get quite a lot of the uninformative creating functions within reason to invalidate a small function.",
            "Very small bones.",
            "And then it turned out.",
            "I mean the good thing here is that it's somehow scaling invariant, so it is.",
            "I mean in the morning.",
            "Depending on the scale of the multiply, the kernels in exchange I mean the coefficient changes.",
            "Also read it.",
            "Yeah, but I think if you were to employ our context to constrain the combination moves.",
            "Scaling problem.",
            "Well, you can normalize external somehow performance so still.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Max.",
                    "label": 0
                },
                {
                    "sent": "Both.",
                    "label": 0
                },
                {
                    "sent": "Application.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you so.",
                    "label": 0
                },
                {
                    "sent": "This is typically the time when everyone gets very tired and sleepy because it's just after lunch and at the end of the course to the end of the workshop.",
                    "label": 0
                },
                {
                    "sent": "So I'll try and be quite brief here, so the whole idea of this work was to see if it would be feasible to use Gaussian process priors for.",
                    "label": 0
                },
                {
                    "sent": "Typical computational biology problems such as coding for protection.",
                    "label": 0
                },
                {
                    "sent": "Or coding function protection where there were diverse sets of data and where there were multiple classes or multiple labels are Invoker, Ross give are very nice talk which will motivate a lot of that.",
                    "label": 0
                },
                {
                    "sent": "Idea this morning.",
                    "label": 0
                },
                {
                    "sent": "So I'm really following up or not work.",
                    "label": 0
                },
                {
                    "sent": "I'm presenting another inferential paradigm based on Gaussian processes.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this sort of problem, an so I'll get some motivation for Bayesian inference in particular another classic.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asian predictive setting.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Many of you will be more than familiar with Gaussian process priors, but some of you won't sort out.",
                    "label": 1
                },
                {
                    "sent": "I'll just give up a brief.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Introduction to what they are.",
                    "label": 0
                },
                {
                    "sent": "And then discuss the whole notion of of composite covariance functions to allow us to enter data.",
                    "label": 1
                },
                {
                    "sent": "And again, this is very very similar to the water.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dentify, Volker this morning and then I'll give us a brief proof of concept application, which was this protein fold prediction problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we want to infer the class membership over multiple classes are multiple labels of classes of objects, then certainly.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Employing probabilistic inference is quite desirable, especially if we have associated misclassification costs and so forth.",
                    "label": 0
                },
                {
                    "sent": "And if you're very high dimensional data or structured heterogeneous data an then it may well be required that we would.",
                    "label": 1
                },
                {
                    "sent": "We need to do a further level of influence.",
                    "label": 0
                },
                {
                    "sent": "So for example we might need to infer the particular ratings oh of each of these datasets as to which ones are predictive of the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guns.",
                    "label": 0
                },
                {
                    "sent": "And of course, in terms of nonparametric classification methods, these support vector machines have been our large margin.",
                    "label": 0
                },
                {
                    "sent": "Machines certainly have been very, very successful.",
                    "label": 0
                },
                {
                    "sent": "I don't think a month goes by without a paper appearing in the Journal of Bioinformatics where someone hasn't applied an SVM to see some problem an it's testament to to the their success, especially high dimensional problems.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I dimensional feature sets.",
                    "label": 0
                },
                {
                    "sent": "So of course, because of the nature of my talk, I would have to say in both attention that baseball games are non probabilistic.",
                    "label": 0
                },
                {
                    "sent": "And we can obtain some form of probabilistic semantics and sort of postpone.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mono and it's not without problems.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Implicitly, the SVM acoustic will define a binary classifier, and if we want to look at multiple classes on multiple labels and then we need to look at either one versus 1, one versus race combinations or.",
                    "label": 1
                },
                {
                    "sent": "Directed acyclic graphs.",
                    "label": 0
                },
                {
                    "sent": "Although having said that and some people live in some publications where film multiclass SCN representations have been developed.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, if we want to do any influence over the relevance of particular features or the relevance of a particular datasets, then of course we need to look at other potential methods, either using cross validation or the same indefinite programming method of get blanket, which again, is Dawn said attention this morning is really only.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Powered off, Asmodee developed an entirely new case.",
                    "label": 0
                },
                {
                    "sent": "But if we employ and if we adopt goes in process priors, then we have a very nice consistent framework to do inference over our classification problem.",
                    "label": 0
                },
                {
                    "sent": "Whether we are working over multiple classes.",
                    "label": 0
                },
                {
                    "sent": "If we wish to infer the relevance or the way things that various features will have, how we integrate certain different types of data, whether it be by composite kernel functions, covariance functions, or.",
                    "label": 0
                },
                {
                    "sent": "Camera combinations.",
                    "label": 0
                },
                {
                    "sent": "And so we thought, then I.",
                    "label": 0
                },
                {
                    "sent": "Think it would be worthwhile having a look at these Gaussian?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who saved lives?",
                    "label": 0
                },
                {
                    "sent": "So I guess in process really defines a distribution over functions, so we have some functions space which Maps some.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The meeting went to the real lines and the process of course is stochastic process, but it is defined simply by a train function of the main function.",
                    "label": 0
                },
                {
                    "sent": "And some covariance function.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Gaussian process would be the famed of probability of F for any subset of the domain.",
                    "label": 1
                },
                {
                    "sent": "If the marginal distribution, the probability of these nonlinear functions of X races, either or whatever, some index of ascetics.",
                    "label": 1
                },
                {
                    "sent": "If this is a multivariate goes in, then we have this in process so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It means if we have NN samples from.",
                    "label": 0
                },
                {
                    "sent": "Some feature feet space of an objects.",
                    "label": 0
                },
                {
                    "sent": "Then the end by 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Ghost in process.",
                    "label": 0
                },
                {
                    "sent": "Set of random variables will be defined by a multivariate Gaussian with some mean function and some covariance.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that is.",
                    "label": 0
                },
                {
                    "sent": "In a sense, the prior that.",
                    "label": 0
                },
                {
                    "sent": "We will then use for subsequent influence in our classification system, which is really the focus of this talk, so it includes some sort of knowledge of certainly some set of assumptions as to the functional class or the smoothness of the roughness.",
                    "label": 0
                },
                {
                    "sent": "Of the functions.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we would be employing Anna very simple example here is, let's assume that we have are.",
                    "label": 0
                },
                {
                    "sent": "O train function.",
                    "label": 0
                },
                {
                    "sent": "And covariance function, which is basically defined by this mean of you, will be familiar with this is our radial basis function as this stationary function.",
                    "label": 0
                },
                {
                    "sent": "And we can then of course.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Draw samples of functions.",
                    "label": 0
                },
                {
                    "sent": "So if we had 30 particular points and we were just to draw sample functions from this Gaussian process where we have defined affected the hyperparameter variables is the scale and this this length steel parameter here.",
                    "label": 0
                },
                {
                    "sent": "To these values, to unit value into 1 / 10.",
                    "label": 0
                },
                {
                    "sent": "Then we get these.",
                    "label": 0
                },
                {
                    "sent": "If I join the dots then we get these nice smooth functions.",
                    "label": 0
                },
                {
                    "sent": "If I reduce the reliance steel by increasing this particular.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameter, then the form of priors that weaken the functions that we can draw from this prior had become.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And increasingly non smooth and rough.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so those are very simple.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Says fires.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An I guess the key thing is is that the covariance function and the parameters which which defined that will represent our prior assumptions or.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the smoothness of functions, which would be using and the very standard examples, this little simple regression.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "So we have a truth.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "& X / X and we make some observations of X and.",
                    "label": 0
                },
                {
                    "sent": "He where he is.",
                    "label": 0
                },
                {
                    "sent": "The true function value plus some error terms of noise term, which we assumed to be to be Gaussian, particular variance.",
                    "label": 0
                },
                {
                    "sent": "So we're assuming Gaussian errors in our.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation is it will.",
                    "label": 0
                },
                {
                    "sent": "Now, if you please apply it on on the particular functions.",
                    "label": 0
                },
                {
                    "sent": "We we condition a parameters of a covariance function under the deploy.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We've spoken about this prior the likelihoods because we've assumed Gaussian errors, of course, is just a standard product.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And because we're dealing now with Gaussian errors, I goes in prior then the posterior over goes in process variables is also going to be.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Goes in and out.",
                    "label": 0
                },
                {
                    "sent": "It will go soon with covariance function which is dependent on the function of the prior plus the noise term and then of course we have a. I mean value which is some representing some representation based on our our target values around data.",
                    "label": 0
                },
                {
                    "sent": "And the predicted distribution over any new data samples is also going to be ghosting, as is the marginal likelihood the normalizing coefficient here.",
                    "label": 1
                },
                {
                    "sent": "So everything is Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which means that infants can can take place with well but one line of code basically.",
                    "label": 0
                },
                {
                    "sent": "So here we have 100 data samples from this sign X / X function with additive noise just quite quite a small level of variance, and the in red you can see the true function value that little blue dots.",
                    "label": 0
                },
                {
                    "sent": "Surely the samples that we've observed of X and the.",
                    "label": 0
                },
                {
                    "sent": "Brown black dotted line gives us a posterior knee estimate and these are plus minus one standard deviation, so you can see clearly where we've got beta.",
                    "label": 0
                },
                {
                    "sent": "Then our predictions are fairly confident and.",
                    "label": 0
                },
                {
                    "sent": "Advective standard deviations are relatively small in regions where we don't have a lot of data, then clearly the posterior variance increases, and here produces virtually no deal at all.",
                    "label": 0
                },
                {
                    "sent": "You can see that the variance spikes off to the prior variance.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, if we change the parameters of our covariance function.",
                    "label": 0
                },
                {
                    "sent": "So we assume a rougher function then.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what we did.",
                    "label": 0
                },
                {
                    "sent": "You get our.",
                    "label": 0
                },
                {
                    "sent": "Our posterior inference indicates our posterior mean, which tries to fit the data points.",
                    "label": 0
                },
                {
                    "sent": "Much cleaner.",
                    "label": 0
                },
                {
                    "sent": "No, so this is all very nice.",
                    "label": 0
                },
                {
                    "sent": "We can do through posterior inference if we assume that we have fixed these are hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So if we want to unfair, do any influence on these hyperparameters, then we need to either one resort to Markov chain Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Which isn't such a hardship or becausw the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "It is also a Gaussian form and if the covariance function happens to be differentiable and then we could employ some sort of type 2 maximum likelihood to do inference on these.",
                    "label": 0
                },
                {
                    "sent": "So this is all very.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course everything breaks down though.",
                    "label": 0
                },
                {
                    "sent": "When we use a likelihood term which is no longer Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And for example, in classification we have Anang.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using likelihoods on, we therefore have to.",
                    "label": 0
                },
                {
                    "sent": "Really simulate samples from our posterior using some sort of Markov chain metroplus Hastings, and then any predictive likelihoods would have to be computed using some Monte Carlo, and I've given some details in the outside of our blocked Gibbs sampler with our Metropolis subsampling, which works reasonably well.",
                    "label": 1
                },
                {
                    "sent": "Of course, for those of you who worked with started sessions, this is a talk would be over now, 'cause we've defined our prior.",
                    "label": 0
                },
                {
                    "sent": "We define the likelihood we just look at 2.",
                    "label": 0
                },
                {
                    "sent": "Quality switch on our gift sampler and then pick up the results when we combine button.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point faces if we could.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Make some approximations which are are good and some sense.",
                    "label": 0
                },
                {
                    "sent": "And of course the NIPS community.",
                    "label": 0
                },
                {
                    "sent": "Ice email communities seem to spend all of their lives or life, so we can get good approximations and then surface sessions looked on.",
                    "label": 0
                },
                {
                    "sent": "Bemusedly comes in fiber.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spending all this time, but I think it's very important so the sort of approximations that we can employ.",
                    "label": 0
                },
                {
                    "sent": "Well, we can employ a maximum a posteriori estimator.",
                    "label": 0
                },
                {
                    "sent": "Approximation to approximate are.",
                    "label": 0
                },
                {
                    "sent": "Our posterius we can use variational approximations where we employ a mean field method and of course there are other things like expectation propagation which we could employ.",
                    "label": 0
                },
                {
                    "sent": "So all these tools that are available to us, but what I would like to do is.",
                    "label": 0
                },
                {
                    "sent": "Sure, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quite a nice trick which actually comes from the statistics community, and if we consider a very simple case of our binary classification problem where our target values are going to take on one or zero.",
                    "label": 0
                },
                {
                    "sent": "So we would just dichotomizing music two classes.",
                    "label": 0
                },
                {
                    "sent": "I very classical likelihood term, of course, is the probability you know the profit function.",
                    "label": 0
                },
                {
                    "sent": "Now what's interesting about.",
                    "label": 0
                },
                {
                    "sent": "Doing GP influence with approve it function well.",
                    "label": 0
                },
                {
                    "sent": "First of all we would need to use a metropolis sampler to do posterior inference.",
                    "label": 0
                },
                {
                    "sent": "But if we introduce an auxiliary variable, so one which really doesn't have anything to do with a particular model, which is just simply I standardized.",
                    "label": 0
                },
                {
                    "sent": "Goshen, which is a distribution which is standardized ghost incentive.",
                    "label": 0
                },
                {
                    "sent": "That particular value of your Gaussian process variable.",
                    "label": 0
                },
                {
                    "sent": "Then, of course, the prover function is simply the marginal form of the joint distribution of your auxiliary variable and your target label.",
                    "label": 0
                },
                {
                    "sent": "Which we can write like this.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conditional forum.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing is that if we define the probability of a target variable being one given this auxiliary variable as being a Delta function, if Y is greater than 0.",
                    "label": 1
                },
                {
                    "sent": "Basically we're just splitting why into positive and negative line, then the well.",
                    "label": 0
                },
                {
                    "sent": "Of course the launch and was just this private function which is a normalizing constant of our left truncated Gaussian, but the key point here.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start.",
                    "label": 0
                },
                {
                    "sent": "If we include the auxiliary variable.",
                    "label": 1
                },
                {
                    "sent": "Then remember that we have a ghost.",
                    "label": 0
                },
                {
                    "sent": "In process we have a multivariate ghost.",
                    "label": 0
                },
                {
                    "sent": "In prior on our on these random variables if.",
                    "label": 0
                },
                {
                    "sent": "And what it means is that we know how that goes in joint distribution and from here within our overall likelihood, which means that we can make progress and defining one against sampler because the conditional likelihoods.",
                    "label": 0
                },
                {
                    "sent": "Of F&Y will be multivariate Gaussian and then a truncated goes in which we can sample from.",
                    "label": 0
                },
                {
                    "sent": "So we don't need to resort to Metropolis sample for approval function.",
                    "label": 0
                },
                {
                    "sent": "We can use a nice gift sample.",
                    "label": 0
                },
                {
                    "sent": "Because you're also going to get sample, then it means that.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can stop thinking about.",
                    "label": 0
                },
                {
                    "sent": "Mean field approximations.",
                    "label": 0
                },
                {
                    "sent": "So that's four.",
                    "label": 0
                },
                {
                    "sent": "Binary classification of course, we're really interested in is multiclass, where in the full recognition problem there are multiple folds 2013 whatever that we need to predict.",
                    "label": 0
                },
                {
                    "sent": "So in that case we can use rather than a binary probit function.",
                    "label": 0
                },
                {
                    "sent": "We can use our multinomial probit function and there's a paper quite a quite a long people.",
                    "label": 0
                },
                {
                    "sent": "It's just appear which gives details of this here, but basically what we end up doing is introducing.",
                    "label": 0
                },
                {
                    "sent": "A random variable which is now key dimensional, which again has zero mean it's a zero mean Gaussian with unit covariance.",
                    "label": 0
                },
                {
                    "sent": "And this likelihood turn here the multinomial prove.",
                    "label": 0
                },
                {
                    "sent": "It basically takes this key dimensional space, which are our Gaussian variable sets and and splits it into key nonoverlapping cones.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have these coins now.",
                    "label": 0
                },
                {
                    "sent": "On again we can define our gift sampler for this Anna very straightforward minor.",
                    "label": 0
                },
                {
                    "sent": "The details are in the abstract.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing now is that we can we can employ variational methods without making any explicit approximations to the likelihood theorem.",
                    "label": 0
                },
                {
                    "sent": "All that we will do is we will assume in the usual standard mean field approximations that the full posterior over our Gaussian process variables under auxiliary variables is block factored.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going through.",
                    "label": 0
                },
                {
                    "sent": "All the the derivations which are fairly straightforward.",
                    "label": 0
                },
                {
                    "sent": "It's close of you who are familiar with these methods.",
                    "label": 0
                },
                {
                    "sent": "All you need to do is look at the Gibbs sampler and you can get these approximate.",
                    "label": 0
                },
                {
                    "sent": "Posteriors from the conditionals and basically what we would have then, is a product of key multivariate Gaussians, where we have this posterior.",
                    "label": 0
                },
                {
                    "sent": "Expectation over our ability variables.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we find that the approximate posterior over auxiliary variables is simply a product of these.",
                    "label": 0
                },
                {
                    "sent": "Completed oceans and the zirconic, truncations of the multivariate goes in and this key dimensional space where if tiene a target variable corresponds to the earth class then the I TH variant.",
                    "label": 0
                },
                {
                    "sent": "In this of this, auxiliary variable will be the largest.",
                    "label": 0
                },
                {
                    "sent": "So basically we were placing why into the icon in this key dimensional space, and hopefully the posterior expectation for each Y which would require up here I actually turns out to be just a simple nonlinear function over the posterior expectations of our.",
                    "label": 0
                },
                {
                    "sent": "Our GPS random variables, which again we can obtain from well, it's just simply this here.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm so.",
                    "label": 0
                },
                {
                    "sent": "That means that you have a very.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A nice way of.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do you have full procedure inferencing and essence all over acquires this updating of the?",
                    "label": 0
                },
                {
                    "sent": "The key.",
                    "label": 0
                },
                {
                    "sent": "Sets of GP random variables.",
                    "label": 1
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where each element in this meet in this vector here is defined by this nonlinear function of the.",
                    "label": 0
                },
                {
                    "sent": "The corresponding values of our GPU random.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Universe so this is quite nice, no ceiling.",
                    "label": 0
                },
                {
                    "sent": "Is all the cane cubed?",
                    "label": 0
                },
                {
                    "sent": "That immediately tells you that clearly GPS I'm not going to be applied on datasets where there are millions and millions of data points, but it's certainly.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot nicer than doing full MSMC.",
                    "label": 0
                },
                {
                    "sent": "So the point of all this is that we can also do through variational inference on the hyper parameters of the covariance.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function just by employing an important sample.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No quest.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well who disagree.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The approximation well again.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Measuring productively for who's with the VB approximation in comparison to give some of that, there is very little over may datasets.",
                    "label": 0
                },
                {
                    "sent": "Simply that it's very little difference is statistically significant.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "If I know just.",
                    "label": 0
                },
                {
                    "sent": "Move onto.",
                    "label": 0
                },
                {
                    "sent": "The main point here.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I should just point out that this is inference over.",
                    "label": 0
                },
                {
                    "sent": "10 parameters of a covariance function and the the data is such that has been defined that the multiple classes require two parameters are two features and the other eight.",
                    "label": 0
                },
                {
                    "sent": "I'm not predictive of the target values, so I've overlaid the output samples from a metropolis subsample.",
                    "label": 0
                },
                {
                    "sent": "Talk of the posterior mean estimates, and we're talking about round about 150 times faster.",
                    "label": 0
                },
                {
                    "sent": "If we if we just employees deterministic, I mean field approximations.",
                    "label": 0
                },
                {
                    "sent": "So it's certainly.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm like why Sophia protective lately it's.",
                    "label": 0
                },
                {
                    "sent": "So it's sad.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They are quite a nice.",
                    "label": 0
                },
                {
                    "sent": "Alternative to the Fool MCMC, no.",
                    "label": 0
                },
                {
                    "sent": "Of course, a convenience function can be some linear combination of base covariance functions, and this is exactly that.",
                    "label": 0
                },
                {
                    "sent": "The form that was presented this morning by Volker, and actually, that this this isn't in any way I'd hope this actually comes from assuming our linear or non linear additive model over our GP priors.",
                    "label": 0
                },
                {
                    "sent": "This would then be the composite covariance function.",
                    "label": 1
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who?",
                    "label": 0
                },
                {
                    "sent": "Who?",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When I want to do is then.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do inference.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For multiple repres.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Indications, and of course we had this morning, but the headline casework on protein function prediction, as have been hard, is also combined.",
                    "label": 0
                },
                {
                    "sent": "Multiple came over multiple covariance functions and trying to predict protein protein interactions an.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is also been work on enzyme networks.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So forth.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, in this case, all that we really need to do is.",
                    "label": 0
                },
                {
                    "sent": "Just we're learning the camera combinations.",
                    "label": 0
                },
                {
                    "sent": "There really is nothing more that we need to do.",
                    "label": 0
                },
                {
                    "sent": "We just do the variational inference over the linear.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tweets of the kernel combinations underdone.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "For example here.",
                    "label": 0
                },
                {
                    "sent": "Which was on.",
                    "label": 0
                },
                {
                    "sent": "One of which was presented by.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My thing and do Chuck we Viking 2000 and they were trying to using some very low homology sets of proteins.",
                    "label": 0
                },
                {
                    "sent": "They were trying to predict the fold over 27 possible scope folds and beer.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Six different datasets derived from the protein sequences the employed one versus one combinations of ACMS and then some voting method.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To combine them all.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I know I'm dependent at set of protein.",
                    "label": 0
                },
                {
                    "sent": "They were getting a raw score of about 43 1/2% correct predictions and then with some manual massage ING they could get up to 56 1/2%.",
                    "label": 0
                },
                {
                    "sent": "And I noticed that bioinformatics just this month and other people has come out using the same data set and managed to tweak the performance of its 62% in Windows 20.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "7.",
                    "label": 0
                },
                {
                    "sent": "So the 60 to stateside amino acid composition is the secondary structural elements, hydrophobicity profiles, polarizability, polarity in the border.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm.",
                    "label": 0
                },
                {
                    "sent": "I basically just took.",
                    "label": 0
                },
                {
                    "sent": "Certain covariance functions and I use this middle basis function for each.",
                    "label": 0
                },
                {
                    "sent": "Each of the different datasets and I play star set of independent gamma priors or on each of these.",
                    "label": 1
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Covariance function weighting coefficients.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Around the variational Bayes.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Teen and here's the performance that we get.",
                    "label": 0
                },
                {
                    "sent": "If I just look at the raw performance on each of the single datasets that this is the percentage accuracy, so we're about 53% using just amino acids.",
                    "label": 0
                },
                {
                    "sent": "Running about 45% or if we just employees taking this structural elements and so forth.",
                    "label": 0
                },
                {
                    "sent": "So if we combine all of those multiple datasets and a composite covariance function maybe get a boost up to 6263%.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing is, is that for this problem, if we just throw all of these into the covariance function and equally weak them, so we just have one six on each of them, then the prediction accuracy is exactly the same.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's interesting, though, is that the predictably for his, so how?",
                    "label": 0
                },
                {
                    "sent": "How confident are the predictions that are correct?",
                    "label": 0
                },
                {
                    "sent": "Then when we when we try and fair and optimal waiting, we do get are more confident set of predictions and it's interesting to get lancret his key paper.",
                    "label": 0
                },
                {
                    "sent": "There was one particular prediction problem where the just a straightforward linear, uniform weighted combination of his datasets.",
                    "label": 0
                },
                {
                    "sent": "Really make any difference in comparison to the optimized said.",
                    "label": 0
                },
                {
                    "sent": "So we see the same here, but there's certain in terms of projected performance, the 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Label in France is doing something useful, and what's interesting is that the.",
                    "label": 0
                },
                {
                    "sent": "The posterior mean estimates that we get simply indicate that the amino acid composition and and the predicted secondary structural elements are really the main components of the multiple datasets that are required for this particular problem, and these are really pretty well down weighted.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think I'm coming to the end, yeah, and the inference is fairly quick, so so the learning this combination.",
                    "label": 0
                },
                {
                    "sent": "Of data.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really quick so.",
                    "label": 0
                },
                {
                    "sent": "To summarize, in few minutes where.",
                    "label": 0
                },
                {
                    "sent": "If we use Bayesian inference in a general classification setting so this is over multiple classes, and indeed this, this can be employed over multiple labels as well.",
                    "label": 1
                },
                {
                    "sent": "Then we can do it feeling actually an appeal.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Straightforward way if we want to integrate different datasets.",
                    "label": 0
                },
                {
                    "sent": "I've very briefly and I'm handwaving minor, discussed the variational approximations that can.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Employed instead of resorting to through MCMC, we achieved what is currently the state of the art performance on this whole recognition program without a manual massage ING.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or cheating an?",
                    "label": 0
                },
                {
                    "sent": "Of course we could employ this on heterogeneous data combinations similar to the ones that Volker presented this morning and that's yet to be done.",
                    "label": 1
                },
                {
                    "sent": "Not set.",
                    "label": 0
                },
                {
                    "sent": "Mike quick question.",
                    "label": 0
                },
                {
                    "sent": "Assess the utility of your predictions.",
                    "label": 0
                },
                {
                    "sent": "It would be nice to have a lot of confidence of connections and accuracy.",
                    "label": 0
                },
                {
                    "sent": "Yes, have you you have such a sense of what it might look like?",
                    "label": 0
                },
                {
                    "sent": "At this point it is with countless because you saw the overall average confidence and the predictions.",
                    "label": 0
                },
                {
                    "sent": "I would be interested in yes, like very confidently be more confident.",
                    "label": 0
                },
                {
                    "sent": "Good point and other streaming.",
                    "label": 0
                },
                {
                    "sent": "OK. You spoke about it.",
                    "label": 0
                },
                {
                    "sent": "Combinations.",
                    "label": 0
                },
                {
                    "sent": "Would you be able to do a local combinations of your career covers?",
                    "label": 0
                },
                {
                    "sent": "Mattress is so local means that you have different coefficients.",
                    "label": 0
                },
                {
                    "sent": "Spectre was some of the data points out is that each of those queries functions and put their own hyperparameter switch.",
                    "label": 0
                },
                {
                    "sent": "You just in fair in any case.",
                    "label": 0
                },
                {
                    "sent": "So if you wanted to actually do certain features of whatever their covariance functioning in computer program files and those and then just do that, yeah, we can talk about that later.",
                    "label": 0
                },
                {
                    "sent": "Sweet.",
                    "label": 0
                },
                {
                    "sent": "So there's this.",
                    "label": 0
                },
                {
                    "sent": "Coefficients in those response when you have many kernels, right?",
                    "label": 0
                },
                {
                    "sent": "So there's a one normalization on top of that.",
                    "label": 0
                },
                {
                    "sent": "Do you also find this in your approach?",
                    "label": 0
                },
                {
                    "sent": "So we had some previous market represented Ice email last year by employing these gamma type files and find that you have employed initially type fire which is in essence what you were doing by making a convex combination, then yes, you would actually get quite a lot of the uninformative creating functions within reason to invalidate a small function.",
                    "label": 0
                },
                {
                    "sent": "Very small bones.",
                    "label": 0
                },
                {
                    "sent": "And then it turned out.",
                    "label": 0
                },
                {
                    "sent": "I mean the good thing here is that it's somehow scaling invariant, so it is.",
                    "label": 0
                },
                {
                    "sent": "I mean in the morning.",
                    "label": 0
                },
                {
                    "sent": "Depending on the scale of the multiply, the kernels in exchange I mean the coefficient changes.",
                    "label": 0
                },
                {
                    "sent": "Also read it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I think if you were to employ our context to constrain the combination moves.",
                    "label": 0
                },
                {
                    "sent": "Scaling problem.",
                    "label": 0
                },
                {
                    "sent": "Well, you can normalize external somehow performance so still.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}