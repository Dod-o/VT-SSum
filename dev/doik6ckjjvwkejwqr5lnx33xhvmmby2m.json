{
    "id": "doik6ckjjvwkejwqr5lnx33xhvmmby2m",
    "title": "Probabilistic Decision-Making Under Model Uncertainty",
    "info": {
        "author": [
            "Joelle Pineau, School of Computer Science, McGill University"
        ],
        "published": "Jan. 15, 2009",
        "recorded": "October 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/cmulls08_pineau_pdm/",
    "segmentation": [
        [
            "So.",
            "Everybody we are honored to have, you know with us today as well is an alumnus of CMU and one of the world's leading experts on partially observable Markov decision processes.",
            "She's going to.",
            "She's returned here for Nick Armstrong's proposal, which was just successfully concluded, which is a.",
            "And also to tell us about probabilistic decision making under model uncertainty."
        ],
        [
            "Take it away from thanks Jeff.",
            "It's great to be back home.",
            "It's really, really fun to be here.",
            "There's a few familiar faces though.",
            "I see in a few years there's been a big change of the guard.",
            "Hopefully I'll have a few new ideas.",
            "Last time I talked in this setting was probably about five years ago on the topic of Palm DP's, so hopefully there's a few new things today.",
            "This is a talk about probabilistic decision making under model uncertainty.",
            "I should point out that there's sort of two parts to the talk in the second half of the part.",
            "Is actually done in collaboration with Stephen Ross.",
            "He was a Masters student at McGill University and he's a PhD student now and he's in the back there so all the hard questions will go to him.",
            "But the first half was done in collaboration with a few other people.",
            "Might be fired in pink sun.",
            "Will things move ahead?"
        ],
        [
            "Good, so it's mostly I debated for awhile about what to talk about.",
            "Fun robots and things or more technical things, and I felt I'm coming to CMU to the machine.",
            "Learning lunch is sort of the time to pull out the interesting mathematics and models and algorithms.",
            "I'll just start with a few pictures of the robot, but you won't get a lot of that in the rest of the talk.",
            "This is the smart Wheeler platform, which we're building right now at McGill University in assistive device, and the context is an intelligent wheelchair.",
            "In the main goal of this project is to help people with mobility disorders.",
            "Obviously in the longer term, but in the shorter term there are really hard questions in terms of human robot interaction that need to be addressed.",
            "Perspective we sort of assume that all the navigation stuff has been solved for us.",
            "We're mostly in indoor environments and will assume that part of the problem is solved pretty robustly by existing methods.",
            "But I think we still lack good mathematical frameworks to reason about how human robots should interact.",
            "This is a brief, probably fuzzy picture of what our architecture looks like in terms of the interaction.",
            "The important part is you have the user right here, and it's interacting with the chair through some touch screen.",
            "Visual tactile input in through speech.",
            "And we have all these parts in place, but the real hard question from our point of view is how the robot should choose actions during its interaction.",
            "What should it say?",
            "What should it do in response to input from the human?",
            "And there's all sorts of things that make this difficult in terms of the noise.",
            "The complexity of the observation space, and so on.",
            "And funnily enough, our target population really isn't very tolerant of the robot not performing well, despite the fact that all of these things are difficult and we really are targeting deployment of this chair.",
            "In fact, we have tests with the target population that we're aiming to conduct starting in January, so this isn't some kind of hypothetical scenario.",
            "In January, there will be a disabled person sitting in that chair, not that one specifically, another prototype that's slightly more comfortable, but there will be a person sitting in there.",
            "And the interaction manager better be doing something reasonable, so this is a story about how do we do this.",
            "In the past alot of this has been done with customized solution."
        ],
        [
            "One takes the time and the care to developed a really nice script.",
            "Maybe they put it in a finite state machine or look up table or something like that that will closely script the interaction between the human user and the wheelchair.",
            "In many people have looked at how we can introduce learning into that and said well, we don't want our finite state."
        ],
        [
            "Machine to be fixed from the beginning will collect some data.",
            "Look at how human and wheelchairs interact for awhile, and then we'll pour this data into our model.",
            "And then we'll run a little bit of automated planning.",
            "All this and there's been some work on that also."
        ],
        [
            "And the framework that we're adopting is a little bit more interactive now.",
            "This line between supervised learning and reinforcement learning is a little bit fuzzy, but the distinction we're making is really that the robot has some initiative to play during the learning.",
            "It gets to pick what it wants to observe, and it uses that to drive the learning.",
            "Hopefully by doing this we can get models that are more robust because the learning is targeted at figuring out the parts of the model that we need to find out.",
            "Hopefully also we can be a little bit more data efficient when we do it through this framework.",
            "And because I couldn't just leave the examples there and plunged."
        ],
        [
            "Into the more technical part, I thought I'd bring up a second example, which I'll raise a few times later in the talk.",
            "This is another project going on in the lab right now, which also is motivating a lot of the results I present today.",
            "This really has to do with medical treatment design.",
            "For those of you who are interested in working in reinforcement learning but finding real world applications, it turns out that are really rich application domains outside of robotics, medical treatment design being one of them in one particular project, we're trying to do deep brain stimulation using reinforcement learning for people who suffer from epilepsy.",
            "Turns out, there's now these little devices called neurostimulators, which people get implanted, and there's a sensing part of it where through.",
            "Electrodes you can see the activities in the brain, EG type activities and there is also an electrode that can deliver electrical stimulation to the brain and people have been designing these devices, deploying them in humans and seeing how effective they can be to treat people with epilepsy.",
            "Except right now all of the stimulation strategies are set by hand, so this is sort of the customized version of it.",
            "They design A strategy, they tweak the parameters through trial and error.",
            "And there's a few people who have started looking at learning in this context, so we're trying to apply reinforcement learning in this context.",
            "And if you compare this task domain compared to a standard robotics test domain, there's a few things that makes this even harder.",
            "The number one thing I think is that we really don't have good notion even of of what are the right set of features to be looking at.",
            "How do we describe the observation for this space?",
            "The other thing that makes it hard is we have no data.",
            "The amount of you know with robots we have use of accumulated recording data with these devices.",
            "They're on the order of a few dozen people who've been implanted with these devices, so just in terms of passive data, we don't have anything #3 in terms of collecting data.",
            "I can probably find a good number of people in this room and around the world who are willing to lend me their robots for a couple hours of data collection.",
            "I'd be hard pressed to find medical doctors who are going to let me deploy my reinforcement learning algorithm on these type of system.",
            "The good thing is I don't need people quite yet.",
            "And we have good partners who have a lot with a lab or they do individual work.",
            "Sort of slices of rat brain.",
            "In addition, they're letting us do a little bit of that.",
            "But again, a lot of the same difficulties arise in this kind of context, which we see in the robotics context.",
            "So this is."
        ],
        [
            "It's the idea to motivate that there's a rich set of real world problems out there.",
            "And if we want to be able to tackle these using reinforcement learning type of approach, we need to solve a few problems.",
            "And big one, as far as I'm concerned is really this problem?",
            "How do we learn models in a good way?",
            "How do we do that in terms of being data efficient?",
            "And how do we do that in terms of having some robust models at the end?",
            "And so this is really what's been driving my research since I left CMU getting a sense of how?",
            "What are the right models and algorithms to handle this kind of problem?"
        ],
        [
            "One thing hasn't changed since I left CMU and this is the fact that I'm really basing a lot of this work in the Palm DP framework.",
            "I assume most of you are relatively familiar with this work.",
            "Reed told me he's teaching it and I think it's a machine learning class.",
            "I think a few places where people have picked up this background.",
            "The basic definition is quite standard for any type of decision making or planning task.",
            "There's a set of states that describe the state of the system.",
            "There's a set of action.",
            "This is a decision making part.",
            "This isn't a passive system.",
            "You can take action which causes some change in the system, and in the case of palm DP, that change is described using a probabilistic model.",
            "And then there's also a notion of reward or goal or something that gives some indication of whether the right actions to choose in which state.",
            "In the palm DP setup, there's a few complicating factors, the biggest one being the fact that the state of the system isn't directly observable, so instead there's instead.",
            "There's a set of observation which captures the part of the state that is in fact observable in the case of our epilepsy brain stimulation system, the observations that we get are these readings from the electrode, so e.g type readings which somehow capture what's going on in the brain.",
            "What we'd really like to know is which neurons are firing, when, and how.",
            "We can't get that level of information, so we have instead this reading.",
            "There's observation probabilities that describe how the observation are admitted as a function of what's going on in the system, and the last little bit that we have is a distribution over state, and this is what captures really our state of information about the system.",
            "We don't know the state of the underlying system, but we have a probability distribution over this state.",
            "That tells us something about what's going on in the system.",
            "And there's 2 interesting things to do when you."
        ],
        [
            "Have a dynamic system models as a palm DP.",
            "The first one is tracking this belief.",
            "So as you learn more about the system as you observe your e.g signal, you need to make some info."
        ],
        [
            "Friends about what's going on in the brain.",
            "How does the probability of state changes so you can do that with Simple Bayes rule?",
            "Bayesian filter in this case?"
        ],
        [
            "The other interesting thing to do is figuring out what's the expected return of applying certain actions on this system.",
            "So this is a simple version of Bellman's equation for the POMDP case.",
            "This value function tells you if I'm in a certain belief.",
            "So if I think there's a certain distribution over my state, what is the expected long-term return of applying different actions?",
            "So if I take the best action I can possibly apply, well, get some immediate reward.",
            "Maybe it'll stop the seizure, or maybe I'll just deliver electrical stimulation and burn a neuron.",
            "So there's some immediate reward.",
            "And then there's the effect of what my action has, which takes me to some other part of my belief state, and then I can acquire more return.",
            "So it's a standard dynamic programming kind of equation.",
            "In terms of the model, these things are really well defined.",
            "They've been defined for the last 60 years, 40 or 50 years or so.",
            "What's more interesting is how do we do these two steps?",
            "The belief monitoring and the value function?",
            "In real time.",
            "In cases where your model is not really clear and by model I really mean the transition probabilities in the observation probabilities.",
            "If you don't know what these are, you a little bit in trouble here.",
            "An in cases where the state space is very large or your observation space ultimately is really large also."
        ],
        [
            "So I sort of set up a grand agenda.",
            "It's probably going to keep me busy for a number of years.",
            "I'll tell you which part of this problem I'm tackling right now.",
            "We'll assume we have a problem which can reasonably well be modeled using a palm DP framework, and we assume we have the ability to sample trajectory somehow, get some data from this domain."
        ],
        [
            "So for the purposes of today's talk, I want to talk about two separate result in this framework.",
            "The first one is assume that the trajectories have labeled state information.",
            "So someone's gone and annotated the state of the system.",
            "But you can't control the choice of action.",
            "We don't often work in this framework for the reinforcement learning domain in robotics, it's not particularly interesting when you start dealing with medical application.",
            "This is much more typical in that people are not giving you the control over the choice of action at first.",
            "They are willing to record data from certain strategies or policies, and they're willing to give you this data, but they're not willing to give you on line control of the system.",
            "Unless you've shown that you have the ability to do something reasonable.",
            "So this is the first half of the talk for."
        ],
        [
            "Today and the second half is the following will do things a little bit with more flexibility.",
            "Now we assume we can control the agent during data collection, so we get to pick the actions while we collect data.",
            "But then the states are only partially observable.",
            "No ones annotating the states for us.",
            "So there's the batch case, the online case, and we'll talk a little bit more about what kinds of algorithms we can use for each of these cases, and what are the effects of that."
        ],
        [
            "So let's start with a simple case, the batch case.",
            "We have a palm DP problem domain.",
            "We assume that the dynamics are not known, but we assume someone is giving us, giving us a batch of data and to simplify things a little bit more, asking even simpler question, which is if I have sampled trajectories from 2 policy's.",
            "So I take my system, I fix an action selection policy.",
            "I diploid, this collective batch of data.",
            "Now I do the same thing but with a different strategy.",
            "Can I tell which of these two policies is the better one?",
            "And can I say whether I'm confident in this choice?",
            "And this seems like a really simple problem.",
            "And Palm DP's are reasonably good at doing this part.",
            "We've been doing policy evaluation for a long time.",
            "We haven't done this at all in terms of palm.",
            "DP's most people who do robotics may not care too much as long as their robot doesn't fall down the stairs.",
            "They don't care.",
            "People who do medical treatment design care a lot about this.",
            "All of the experimental design and optimization of treatment system is based on this notion of testing for significance.",
            "Can I show that a certain treatment is significantly better than a placebo treatment or standard care?",
            "So if we want to use palm DP models to optimize choice of treatment, we need to be able to answer this very simple question.",
            "In the wheelchair domain, which is why."
        ],
        [
            "We're using this also.",
            "It has some nice applications.",
            "I've been talking to the people in the speech and dialogue group at AT&T and they do this a lot.",
            "They fix their dialogue strategy when you have the automated phone in system where you call in at Auntie, there's a whole menu of what you need to answer before they tell you the information you need.",
            "These policies are usually fixed by hand and there's people in the research group at 8:24 interested in using Palm DP's to do this, but most of the data they collect is from these fixed policy.",
            "AT&T isn't interested in putting in the palm DP right in the loop before the researchers can demonstrate that there's good confidence that the policy is going to be of a good quality.",
            "So in the case of our robot, really, if we take a simple scenario, really we have the option either to ask for clarification or to go through a given location.",
            "So this is a very simple simplified example, but which has all of the characteristics of what we're looking for.",
            "And because we're dealing with human robot interaction again, we really don't have good models.",
            "Of how to do this?"
        ],
        [
            "Few quick recap of terminology for those who might not have seen Palm DPS recently when I talk about policy, what I really mean is a function mapping belief states to action.",
            "So given that my Bayes filter has.",
            "Maintain a certain belief.",
            "What action should I do in that belief?",
            "And based on that then the value function as I defined a little bit earlier is a function mapping belief space to the expected return of running that particular policy.",
            "And there's some nice properties about Palm DP's.",
            "For those of you who were here.",
            "For Nick's proposal this morning, he did a nice job at defining some of these, but just to remind you, for the ones who didn't get up quite so early, the policy can be shown as a finite state controller.",
            "Where the nodes are the action of the system and the arrows really are the observation.",
            "So I can start by asking where the robot wants me to go and if I get a clear answer that they want to go to why I might ask again, and if it confirms again then I might go to that specific location and there's a nice duality between the policy representation and the value function representation.",
            "If that part is fuzzy in the back of your brain, you really don't need to understand it so much for the purpose of today's talk.",
            "What's important is that we can capture the policy with a finite state controller.",
            "Because the result we developed is really based on this representation of a comedy policy."
        ],
        [
            "So I've stated a reasonably simple problem.",
            "All I want to do is compare to policies and be able to tell which is best and by how much or if there's a nice difference between the two, so we can set this up as a policy evaluation.",
            "This is just my Bellman equation, but in matrix form I have my value function, my reward, my transition probability, observation policy, observation probability.",
            "This is the value at the next time step in this.",
            "Capital Pi are the state transition of my finite state controller, so This is why I needed that representation of the policy.",
            "This is dates back to the work of Sondik in the 70s.",
            "This definition of policy evaluation, it turns out you can express it just in terms of the value function, and now you get a nice closed form solution for your policy evaluation."
        ],
        [
            "And so.",
            "This is good so far, but most of the time when people do policy evaluation, they may take data samples, estimate their transition their observation model, but then they just apply policy evaluation as it is what we really want to do today is estimate the variance in the value function.",
            "So we need some way to go from our recorded trajectories with labeled state information to assume to estimate these transition and these observation models."
        ],
        [
            "So we'll start with a frequentist approach to estimating the model, and This is why it was important to get our state labels.",
            "We need to know when did the system go from state I to state J.",
            "We can count this using standard basic frequentist statistics and we can see when our system is in state.",
            "I.",
            "How many times does it see observation J and I just need to kind of count this from my batch of data.",
            "Nothing is complicated here.",
            "And in the case, in the limit, where have a really big batch of data.",
            "Then these estimates are going to be really very good, but in most cases, specially with medical data, we deal with finite samples.",
            "So you've only tried heat treatment on a few patients.",
            "If you've run a clinical trial, maybe you have 20, maybe 40 patients, each of whom got the same treatment, but you don't have enough data to assume that your estimation are very are absolutely correct, so we can assume that our estimate of the transition is really the correct transition plus.",
            "Some error term and will make some assumption about these error terms.",
            "We assume that it's an additive model for both the transition and the observation, and we'll assume the noise in this case is unbiased and the noise isn't correlated between the transition and observation, and that we can estimate the covariance terms from data.",
            "So the question we're really asking is what is the variance in the value fun?"
        ],
        [
            "And if I have that estimation of variance in the value function, then I can tell apart two policies.",
            "So there's a little bit of math that goes on in how you do this, But it turns out you can approximate this reasonably well.",
            "This is just the Bellman equation that I showed a little bit earlier, and I can substitute the model error.",
            "So instead of having my model estimates and here I put the true and the error term, and this is where we need to start approximating things.",
            "You can approximate this using a Taylor expansion.",
            "It turns out there's a lot of technical detail in the paper which I won't go into today, but the point is that you can get a nice second order.",
            "Taylor approximation for that particular equation.",
            "That gives you a good estimate for the variance in your value function."
        ],
        [
            "And once you have that, you can actually ask what's the what's.",
            "The bias is my estimate of the value function with the variance, and that was the covariance and you can get closed form expressions for this.",
            "For the case where you have a batch of data, finite sample set where you've estimated these things.",
            "And so using this."
        ],
        [
            "We can actually apply this in the context of a simple dialog example, and the way we usually apply this as we assume we fix the true models in order to test this.",
            "Get some data.",
            "It's a little bit of a bootstrapping kind of estimate.",
            "We fix the true models and then we generate a set of N test cases, each containing a fixed number of sample, and for each test case we get an estimate of the value function and we calculate the standard deviation using the method I just described an you can measure how often the true value function, which we calculated with the true model is off from our estimate in terms of one or two standard deviations, and see how that relates to our estimate.",
            "Of that variance.",
            "So this first step is just validating that our estimate is actually pretty good.",
            "Remember, we did this second order Taylor expansion approximation.",
            "To estimate that variance, so we first want to check whether that variance estimation is reasonably good.",
            "And what we find out is in fact.",
            "Gives us a pretty good act."
        ],
        [
            "Risky in terms of our estimate, this is.",
            "For standard deviation, one standard deviation 2 standard deviation as a function of the number of samples.",
            "But even with a smallish number of sample things are reasonably good.",
            "These are not tiny samples, mind you, you need reasonably enough data, but when you do, the accuracy of the variance estimate is actually quite good.",
            "So."
        ],
        [
            "Once you do that, you can start asking interesting questions.",
            "Such as what is a good strategy for my wheelchair in terms of confirming information?",
            "I have three strategies here.",
            "Is the ask once where the person wants to go ask twice?",
            "Ask three times.",
            "And remember that we don't have any notion of what are the observation probabilities, so what's the noise in terms of the response rate that noise usually comes from the speech recognizer, But these results show is that ask twice is definitely the better strategy to take if you compare, ask one, or as three times.",
            "In expectation, ask once is better than ask three times, but when you take the variance into account then there is not much difference.",
            "They're really quite overlapping in here.",
            "I think it's one standard deviation interval for the three different policies, but you can actually compare these strategies.",
            "And if for some reason AT&T decides that asking twice cannot be deployed and you can only ask once or ask three times whatever the reason, maybe then you have good grounds to decide which of the two you may want to apply.",
            "If you want a variance minimizing kind of system so that people are very familiar with the system is going to do every time everyone has the same experience, then maybe you want to ask three times.",
            "If you're willing to make a lot more mistakes on a few subjects, but not everyone, then you can ask only once.",
            "So we have a way to talk about how we can compare these.",
            "Now if we apply the same framework in the case of treatment strategies, you also get some interesting result."
        ],
        [
            "This is not using the epilepsy data that I talked about briefly earlier this is using data from random clinical trial of people with depression.",
            "We had 4000 patients registered in this trial.",
            "I say we in fact, researchers, some of them at University of Pittsburgh and number of other centers collected this data over five years and an interesting question we can ask is given two treatment strategies which are the same, but one of them includes psychotherapy.",
            "So an appointment with a shrink once a week and one of them doesn't include psychotherapy.",
            "What's the difference?",
            "So these two sets of people receive the same drug treatment, but half of them actually also meet a psychiatrist on a regular basis, and half of them don't.",
            "And the reason it's important to compare these two strategies is a lot of people don't really like the extra burden of going to the psychotherapist.",
            "They might just prefer to take their drug in the morning and not bother with anything more.",
            "So can we see what is the effect of the psychotherapy?",
            "And what we see is that if you just look at expectation, it seems that using psychotherapy.",
            "Has a positive effect, but if you also include the variance estimation well.",
            "There's not a meaningful effect, and this is 2 standard deviations and this is for our patient population in terms of the number of samples we had for each of these two different policies.",
            "If you had many more patients registered in the study, maybe you'd see a meaningful difference, but based on the data we've collected in our clinical trial, this would be the recommendation in this case, and this is the kind of results that our partners in the health side are really interested in that you can add some confidence bounds.",
            "To your estimate of the value function as opposed to just saying my reinforcement learning system recommends that everyone be registered for psychotherapy.",
            "And by the way, in this we didn't even include the cost of the psychotherapy.",
            "All we included in terms of a reward function was the expectation of remission.",
            "Whether people would actually achieve a reduction of their depression symptoms over some period of fixed period of time.",
            "So at some point you could add the cost of cost of psychotherapy if you wanted to, true.",
            "There's sort of a night strategy, which is doesn't have any layers actually value function right after, you just count the rewards along the sample paths, yet average.",
            "Look at that rate.",
            "What's the relevant benefits?",
            "Um?",
            "Right, so you could.",
            "We haven't tried it in terms of the empirical sense in terms of whether there's some real benefit.",
            "In this case, I think you.",
            "What you get in terms of doing the analysis this way?",
            "I don't know if you get more, I haven't thought about it very much, but.",
            "The whole yeah, the hope is that do somehow can leverage your data in a better way by estimating the error on these models precisely.",
            "And if there's some in dependencies in terms of the errors and so on, you can get away with better results with a smaller sample set.",
            "Now I have no theoretical or empirical results to support this, But that's certainly our intuition.",
            "Their approach, which I thought you wouldn't do that, there's some plenty of work and robust MVP's, right?",
            "Yeah, there's some of that.",
            "There's a whole literature on that too, and we're looking at some of that, so we haven't applied it in this context yet.",
            "State machine it's the same, yeah?"
        ],
        [
            "So that's sort of what I wanted to say on this particular question, I should clarify that the work we did is actually a generalization of earlier work that had been done in the MVP case, so some of the techniques in terms of the estimation of the variance were there before it was just a question of generalizing it to the palm DP and the and the controller finite policy controller representation.",
            "But we think this is really useful to quantify performance when you have some critical task domains.",
            "Not necessary everywhere.",
            "Lot of robotic systems you really don't need to worry about it.",
            "But in some domains which have a critical component and this can give you a lot of power to your analysis.",
            "Yes.",
            "The control online.",
            "There's nothing here about how to adjust the control on line if you're doing optimization of your value function, we don't quite know how to do this yet because then you have this Max operator, which really complicates things a little bit.",
            "We have some ideas of how to do that, but we haven't gotten there yet.",
            "It turns out when your data is collected in a batch.",
            "This works pretty well because there's a fixed strategy to collect the data.",
            "Whether it's a randomized strategy or whether it's a strategy prescribed by clinical practice.",
            "That strategy usually is there.",
            "The same with dialogue system, but we say nothing yet about how to estimate the value function directly.",
            "So unless there's other questions, I'll move on to the second part.",
            "I just have another question, go ahead.",
            "In the difference between Robert or Medical experiment as you decide about which finite set to do your comparison.",
            "Well, we treat the two projects pretty independently and then.",
            "It's more relevant because many this is the state.",
            "For this case, yeah, we do them completely independently.",
            "We take the method and we get data from the medical setting, and we learn the policy estimation.",
            "The errors for that setting, and completely separately we do.",
            "The estimation method is the same, but otherwise the models and estimation are completely separate.",
            "OK, so the set up for the second part is a little bit different.",
            "We're going to complicate things in two ways.",
            "The first one is that we assume that we don't have the state labels.",
            "This is a lot more realistic.",
            "And the second one is that we're going to assume we have control over how the data is collected in a lot of robotics domain this is perfectly fine in the medical domain this is not fine in terms of certainly no one is offering to let me run a randomized clinical trial anytime soon, and even in the epilepsy case, I'm not going to collect data on humans.",
            "But as I've mentioned for epilepsy, we have a nice rat model where we have collaborators which allow us to come in with our palm DP based decision maker and plug it into the.",
            "Right brain and let us collect data however we want, and we're doing this sort of on a biweekly basis, so we have this kind of setup right now where we don't have the state labels.",
            "We really don't know what's the state of the neurons at anytime in point, or even some indicators of epilepsy.",
            "But we do get control over how the data is collected.",
            "This is both a blessing and a curse.",
            "It's great that we have the control, but we need to use it wisely.",
            "We need to pick actions that make sense.",
            "Otherwise, what's the point of having that control?"
        ],
        [
            "So the online case."
        ],
        [
            "We're also going to adopt A Bayesian framework in this case, rather than a frequentist framework, and the general Bayesian framework is, I'm sure, familiar to everyone here.",
            "We assume we have some prior distribution over unknown parameters, so we'll still assume that the model of the transitions and the observation is not known apriori.",
            "But we have some notion of what might be good parameters.",
            "We're going to update the posterior via Bayes rule as experience is acquired and the more interesting part is how do we pick our actions with respect to that posterior.",
            "So standard Bayesian framework, but in the palm DP setting."
        ],
        [
            "And the advantage from our perspective is that it gives us a nice way to include prior knowledge explicitly.",
            "In the epilepsy domain, we really have very little prior knowledge, so we pick very agnostic priors in the dialogue management problem.",
            "We have better priors.",
            "We have some sense of when the robot says something the person is going to reply something specific, but there's some noise factor, but we still have some sense of what the model might look like.",
            "So this framework lets us include that knowledge.",
            "It let's us perform learning as necessary because we're in this online setting where we get to pick our actions.",
            "So we get the advantage of that.",
            "And the last point, which is when that I think is useful.",
            "It really allows us to explicitly consider the uncertainty during our planning to pick actions.",
            "Yes, that will give us good reward and yes, that will try to figure out what the state of the system is, but in particular that will tradeoff between when do I need to learn more about my model in one day and I need to be pretty confident about what I know and pick the strategies which I think are good.",
            "So just a quick recall."
        ],
        [
            "Those of you with short memory or post food induced coma about the palm DP model the same setup as before and the question here is really how should we choose actions if parameters tiano are uncertain.",
            "Will assume for the purpose of today's talk that the reward function is known.",
            "I did that for the first half.",
            "I'm doing that.",
            "Again, both techniques can be generalized quite easily to the case where the reward is not known, but it keeps things cleaner and simpler for today so."
        ],
        [
            "This notion of Bayesian learning has been explored in the MVP setup and the case where you know the state of the system.",
            "Duff did a lot of work dirden poop on in France and in that set up.",
            "The idea is quite simple.",
            "It's a little bit like the frequentist idea.",
            "You count the number of times.",
            "That you transition from State S to S prime appan taking action A.",
            "So you're just counting as we did before, but you start from some prior.",
            "And that project you can think of as imagined experiences.",
            "I find pretty sure that transition is going to happen.",
            "It will be like seeing it 10 times, and then I'll start counting how many more times I see it.",
            "So these counts define the Irish light distribution over the transition model.",
            "I'll assume most people are familiar with the Jewish light distribution.",
            "The good thing is it's a nice closed.",
            "Close form update for it.",
            "Upon seeing the information and so these counts are sufficient statistics for defining the jewishly posterior, so it's easy to update.",
            "And now all we need to do is plan according.",
            "To these counts, so you can formulate an UNDP model which the state space is defined as the original physical state and the information state meaning our count vectors.",
            "And now the probability depends on both being in a state and having a certain count.",
            "Seeing a certain action will take you to a new state.",
            "Any set of counts.",
            "So the count account part is deterministic and the state to state part can be probabilistic.",
            "And so once you set up this extended MVP, we can solve it just a standard MDP or counselor discreet if your state or discrete you can run your favorite MDP solver and things are all well."
        ],
        [
            "Good.",
            "In finite palm GPS we can extend the same framework.",
            "Instead of having only counts over transition, now we need the counts over transition an account over the observation and now we have a decision problem over an extended states with include the physical state, the transition count.",
            "And the observation count."
        ],
        [
            "And so.",
            "We can set up a full.",
            "Asian palm DP model.",
            "Based on this I have my extended state space.",
            "Physical state accounts.",
            "The other counts the action and observation stay the same and I can design A joint observation action function which says if I'm in a certain state, take a certain action, see a certain observation.",
            "What's my next state?",
            "The reward function stays the same.",
            "It's just another palm DP problem where the goal is to maximize the return.",
            "Under partial observe ability of my state is nothing different in terms of formulating the model.",
            "There's a little bit of a complication compared to the MDP state."
        ],
        [
            "So what's the complication?",
            "Here I have an infinite state MDP 'cause my counts can be very big with a known model.",
            "Here I have an infinite state palm DP with a known model state is defined over.",
            "This state is defined over there.",
            "This case at every time step my state is observable.",
            "An my count file is updated in the Palm DP at every time.",
            "Step S is not observable.",
            "So my accounts are not observable.",
            "I don't know which state I went from too, so I don't know which account to update.",
            "That makes things a little bit tricky, not in terms of the model.",
            "The model is super clean, but in terms of using this, in practice, we need to deal with that little bit of complication, which is how can we update these counters?"
        ],
        [
            "How do we know?",
            "Where to give the credit for a certain transition?",
            "All we know is what action was taken, in what observation was perceived, but we don't know the states that are Dirichlet distributions are defined based on those dates.",
            "Not to worry, this is the classical problem for palm DP.",
            "This is the reason learning in Palm DPS is so much harder than learning and MVP's and why lot of the focus so far in the literature has been strictly on planning for the palm DP case.",
            "So.",
            "Will just turn."
        ],
        [
            "Under palm DP machinery here, instead of having a belief state only over the original state, space will also have initial counts.",
            "And we also use some way of monitoring the belief and the belief now will be expressed over the States and over the counts.",
            "Now to define the beliefs over these counts, we need to sort of hypothesize that when I see this, it could be because this account needs to be updated in this account needs to be updated.",
            "So all we need is a mixture of Jewish lay models, which tells you all of the different Jewish lies which could happen.",
            "And that gives us a way to update this palm DP model using standard Bayesian inference.",
            "From a practical point of view, maintaining that mixture is a little bit difficult because the number of components in that mixture grows quickly.",
            "The more things you do, the more different ways your account could be updated so it doesn't scale very elegantly.",
            "Here T is your planning horizon, so a little bit problematic in terms of.",
            "The planning horizon.",
            "Before I tell you how we can make this tractable."
        ],
        [
            "I'll just mention 2 quick theoretical results that come with this POMDP model.",
            "This is a slide with a lot of complicated math.",
            "It's not a really complicated result that slide just says.",
            "Assume that you're interested in estimating the value function for specific physical state specific count vectors, and you want to compare to the same value function at that same physical state, but slightly different count vectors.",
            "Can you bound the error in the estimation of this difference?",
            "There comes the complicated expression, but there's four parts to that complicated expression.",
            "The first part says that this difference really depends on the expected value of the two count vectors, so I have my 2:00 to reach light distribution Phi and Phi prime in expectation there going to be a little bit different.",
            "That's that term, and in expectation the size are going to be a little bit different.",
            "And these two terms just says, well, that's all fine and dandy, but in some case FI and five prime are the same in expectation.",
            "But the magnitude of the number of counts is different, and so that term just deals with the uncertainty in the jury's distribution over the transition, and certainly in the distribution over the observation.",
            "The estimates you have.",
            "So I have you, as you have more and more accounts.",
            "This starts going away, but for small numbers of accounts you still need to put that into play, so this is over the value function."
        ],
        [
            "So this is the first theoretical result.",
            "This is something we can measure.",
            "It seems like big complicated in expression, which isn't very useful.",
            "It turns out that that expression comes at the core of one of the approximation methods we use, and being able to bound that difference really helps us figure out how to approximate the mixture of Dirichlet's two more slides."
        ],
        [
            "The second theoretical result that comes with this framework is the following one.",
            "So far I've told you that I can bound the difference in value function when the physical state is the same, but the counts are different.",
            "What I'm telling you here is given that I have some value function at some state and counts how different is that in terms of.",
            "The correct value function and I've thrown in some projection operator here.",
            "If you want the formal definition of that, you can go in the paper.",
            "The point here is that this is the case where the counts are going to be bounded to some upper value, and so we're interested in saying what if I bound my accounts to some upper value and I compared to the case where I don't found accounts 'cause in the Infinity feel that the accounts run up.",
            "You'll do really well practice.",
            "You might not want to do that and you might not want to stop early, so you want to know what's the performance cost you're losing.",
            "And it turns out you can bound this.",
            "Epsilon precision.",
            "And this result tells you how many times you need to observe certain transitions and observation in order to have that epsilon precision.",
            "So don't worry about the details of the expression, but you can use this result to figure out how many times you need to see different counts."
        ],
        [
            "To get an epsilon optimal solution.",
            "In general, the answer is a lot.",
            "These are really boost bound.",
            "You need a lot of counts before these things."
        ],
        [
            "You really get your epsilon approximate solution, so let me tell you a little bit about how we can make these a bit more practical in terms of real algorithms for tracking the belief and planning the first problem.",
            "Is how do we maintain this belief?",
            "How do we deal with this mixture of darish like components?",
            "In particular the different possible count vectors?"
        ],
        [
            "And we use standard machinery for approximating complex distributions over beliefs.",
            "Particle filters are really the way to go here.",
            "And the only thing is we need to figure out how much of these different count vectors to maintain.",
            "Its really these are the number of count vectors you need to maintain that grows exponentially, so we're going to limit that decay, different count vectors, and if you use a simple Monte Carlo then will perform belief update will start with our mixture with K components, will project them forward one state according to action.",
            "An observation that we see, and then we'll pick keep K of those so we always keep K samples at every time it grows, but we always bring it down to K of them and you just randomly pick Caleb."
        ],
        [
            "If you want to be a little bit more sophisticated, instead of keeping K random of them, keep the K with the highest probability.",
            "How is a good thing to do?",
            "It doesn't cost you much to calculate the weight and to keep that."
        ],
        [
            "But since we spent a Steven in particular spend so much time working out these theoretical results, maybe this would be a good place to use them.",
            "And So what does distance metric does?",
            "Is it says OK?",
            "I have my K different count vectors.",
            "I project them forward based on a possible action observation.",
            "Now have a lot more of them, but I want to keep only care of them.",
            "The K of them which best fit the posterior according to the distance metric we had in theorem one.",
            "So all of these differ in terms of the counts.",
            "Look at these Capehart.",
            "These more than K particles and pick the ones of them that best span the space.",
            "And in theorem one all we had was a distance metric in terms of different count vectors.",
            "So we can use that readily here.",
            "This is more expensive to compute, not more by exponential factor.",
            "There's some kind of linear factor, but it is definitely more expensive than any of these.",
            "Maybe it means we can maintain fewer particles, but maybe it gives us much better information.",
            "Remember that here we're using this belief for planning, so it's not a standard particle filter where you really need to keep a good density of your belief.",
            "You really need to keep the set of beliefs which characterize the possible things that could happen, such that then you can create a good plan.",
            "So this is what this does is really characterizes the distribution.",
            "It's not necessarily proportional, but it characterizes no way that's useful for planning.",
            "Now that we have a way to do approximate belief maintenance in these system, we also need some way to do planning.",
            "There's a lot of palm DP planning machinery out there."
        ],
        [
            "That's been developed in the last few years.",
            "We use something a little bit simpler, which is just similar to receding Horizon control.",
            "We assume that we do forward search.",
            "This is just a forward heuristic search.",
            "In the space of beliefs.",
            "Well, when you think beliefs you have to think physical state and counts.",
            "So if I have a certain physical state an initial counts, I could do this action, then I'll see this observation and it will take me to this distribution of counts an physical states.",
            "So this is a forward search in the space of these beliefs.",
            "It's very efficient depending on the branching factor, you may not be able to go very far and this is often what happens to us.",
            "But the belief tracking approximate method I showed in the previous slide can be used to prune off some of these branches.",
            "So this is what we do."
        ],
        [
            "This hasn't quite made it onto the robot or the medical devices yet.",
            "We worked with a very.",
            "Simplified problem inspired by human robot interaction.",
            "The idea here is that the robot has to follow an individual within some kind of known discretized environment, but it turns out there's two different individuals at the robot can follow, and these individual really differ in terms of their motion model.",
            "One of them is a graceful labeled Walker that walks at a good speed and the other one is more ambling, an walking much slower.",
            "Robot knows are two individuals, but the robot doesn't have a preset model of how these people move.",
            "So the robot simultaneously has to track this individual without losing track of them.",
            "Learn what is the motion model of this individual also.",
            "So pick actions that are able to do that.",
            "You might not want to learn the model perfectly if it happens that the two of them move in similar ways, then you can learn an aggregate model, but somehow do this simultaneously.",
            "Learning and planning task.",
            "So the Bayesian framework is quite good for this."
        ],
        [
            "What I show you here is the effect of the choice of belief.",
            "Belief maintenance approximation algorithm.",
            "So what happened if you have the exact model?",
            "And you use just simple POMDP planning on this.",
            "It's not that bigger problem, you can do that quite readily and this is the optimal return and.",
            "Now, if you take your prior model and don't do any learning and just plan based on your prior model.",
            "This is how well you're going to do and the others.",
            "All of this Basean framework with a receding horizon control.",
            "The difference is how you pick the K. Particles decay, different count vectors.",
            "You're going to maintain overtime.",
            "What we see is that Monte Carlo doesn't do very well.",
            "But that both the K most probable and the weighted distance do quite well.",
            "The number here in the buckets is how many particles we use.",
            "What's that K?",
            "So for both Monte Carlo and most probable we use 64 different count vectors which we maintain overtime.",
            "In the case of the weighted distance, we could get away with only 16 years old because they capture that distribution much more cleanly.",
            "If we look at the model accuracy so overtime, what's the error terms are for estimate of the motion model for?"
        ],
        [
            "The two individuals.",
            "Again, this is not surprising given the previous result.",
            "Monte Carlo really isn't able to capture a very good model.",
            "The weighted distance and the most probable are actually quite good with weighted distance capturing a slightly better model in terms of this metric.",
            "We don't expect this necessarily to go to zero because it could be that there's some parts of the model which are just not useful to learn.",
            "You don't need these parts of the model to be able to behave well, and because this is a joint learning planning approach, it's only going to learn how much it needs to learn to be able to behave well, so this won't necessarily go to zero.",
            "I don't know how better it gets as we run it, but it certainly seems to have flattened out quite a bit."
        ],
        [
            "Now.",
            "Comes the interesting question, how much longer do we have to pay in terms of planning time for using that complicated metric that we propose?",
            "Well, it turns out not a whole lot longer.",
            "It is a bit slower, but in terms of planning time, it still comes out to be faster than the other two methods for this kind of an example."
        ],
        [
            "So just a quick summary of this.",
            "The idea here is really to take some of these ideas in terms of Bayesian reinforcement learning that were developed in the context of MDP.",
            "And extend them to the Palm DP framework.",
            "The extension in terms of the model wasn't too complicated, but there's a lot of interesting questions that come up in terms of how do we do belief updating and how do we do planning in this kind of a framework.",
            "And when we put all of this together we get the system that's able to do these three things together, which is quite important when you have one of these systems.",
            "the Monte Carlo methods are really essential to allow you to do these kinds of approximation.",
            "This isn't very.",
            "Surprising, but it's interesting to see terms of how you should behave.",
            "Sir Monte Carlo sampling based on what you know of this."
        ],
        [
            "Particular domain.",
            "Especially with this second half are really not ready yet to apply this to real world domains, so maybe we're not quite as far as it seems.",
            "The observation features are used are really simplified so far.",
            "This state description we assumed was very simple and when we need to deal with these problems, we need something a little bit richer in terms of the observation set.",
            "The good news is Steven work a little bit more after this and developed a version of the beige and reinforcement learning that's applicable for continuous domains, so that's really helpful, especially for here, where we're getting continuous observation and also a version that's applied to structured domain.",
            "So if you have a factored.",
            "Representation in terms of your state, there's extension for both of these, and I'm not talking about it at all today.",
            "So if you want to know more, you can invite Steven to give a follow up talk or look up the papers.",
            "But we're moving slowly towards some of these problems, with more interesting complexity.",
            "So just.",
            "Few finishing words."
        ],
        [
            "Probably most of you have seen this quote before.",
            "There's question ability about whether you thought it was meaningful, but I think there's something to be said here.",
            "Is that my talk today is really about taking these unknown unknowns and somehow trying to quantify something about them and trying to understand better uncertainty in the context of models for Palm DP's.",
            "So that's all for today few people technology, but thank you."
        ],
        [
            "I don't have those questions or if people need to run off, yeah."
        ],
        [
            "How would you go about?",
            "Yeah, it's a really in a sense.",
            "It's a very difficult.",
            "Question, because certainly in the case of the epilepsy, it's like completely open."
        ],
        [
            "Not just how do we go about picking the structure?",
            "It's like what is our observation space.",
            "What is our action space, let alone what is our state space and how?",
            "What are the dependencies within them?",
            "So at that level it's really very difficult.",
            "The good thing I would say that if you take a vision approach, you can be very generous about the space of structures you're willing to consider.",
            "You can say I'm willing to consider all of these different state features and then let the Bayesian inference pick the right type of model to fit the data you have and some of the results we got with structured domains really included that influence over the structure in terms of the relations.",
            "It's only a preliminary case where we were only doing inference in terms of the dependencies between the state variables, but the results are very nice in terms of what it infers based on the data it has.",
            "When you have very small datasets, you'll pick very small observation sets.",
            "You'll pick a very compact way of describing your model.",
            "This is just Occam's razor.",
            "If you have a lot of data, then you want a lot more detail in the structure of your model and the Bayesian framework does that really nicely in terms of the.",
            "Mathematical framework Now how you get it to work in terms of the computational side.",
            "Then that's a little bit more difficult, but there's some things that can be done in a lot of that is done in terms of inferring the structure in Bayes Nets and so on.",
            "This is all things we can leverage here, so we have lots of other people working on these problems when they figure it all out.",
            "Will Porten and music here.",
            "What is the policy?",
            "Yeah, you have to be a little bit careful about that.",
            "If you really wanted to do that inversion.",
            "It turns out in our case we don't need to do the actual inversion.",
            "We're really just estimating the error terms and so we don't want so much into that.",
            "I presume there's lots of tricks you can do in terms of conditioning these matrices, which would help.",
            "I don't know these tricks very well, but I presume there out there.",
            "But it's a big question because a lot of times these matrices are quite sparse in terms of the amount of data you have in them, so you have to be a little bit concerned about that.",
            "If you're interested in just doing strict policy evaluation, but there's some gradient methods, I think that can get around that to some degree.",
            "Other questions.",
            "OK, well thank you for coming."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Everybody we are honored to have, you know with us today as well is an alumnus of CMU and one of the world's leading experts on partially observable Markov decision processes.",
                    "label": 0
                },
                {
                    "sent": "She's going to.",
                    "label": 0
                },
                {
                    "sent": "She's returned here for Nick Armstrong's proposal, which was just successfully concluded, which is a.",
                    "label": 0
                },
                {
                    "sent": "And also to tell us about probabilistic decision making under model uncertainty.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Take it away from thanks Jeff.",
                    "label": 0
                },
                {
                    "sent": "It's great to be back home.",
                    "label": 0
                },
                {
                    "sent": "It's really, really fun to be here.",
                    "label": 0
                },
                {
                    "sent": "There's a few familiar faces though.",
                    "label": 0
                },
                {
                    "sent": "I see in a few years there's been a big change of the guard.",
                    "label": 0
                },
                {
                    "sent": "Hopefully I'll have a few new ideas.",
                    "label": 0
                },
                {
                    "sent": "Last time I talked in this setting was probably about five years ago on the topic of Palm DP's, so hopefully there's a few new things today.",
                    "label": 0
                },
                {
                    "sent": "This is a talk about probabilistic decision making under model uncertainty.",
                    "label": 1
                },
                {
                    "sent": "I should point out that there's sort of two parts to the talk in the second half of the part.",
                    "label": 0
                },
                {
                    "sent": "Is actually done in collaboration with Stephen Ross.",
                    "label": 0
                },
                {
                    "sent": "He was a Masters student at McGill University and he's a PhD student now and he's in the back there so all the hard questions will go to him.",
                    "label": 0
                },
                {
                    "sent": "But the first half was done in collaboration with a few other people.",
                    "label": 0
                },
                {
                    "sent": "Might be fired in pink sun.",
                    "label": 0
                },
                {
                    "sent": "Will things move ahead?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, so it's mostly I debated for awhile about what to talk about.",
                    "label": 0
                },
                {
                    "sent": "Fun robots and things or more technical things, and I felt I'm coming to CMU to the machine.",
                    "label": 0
                },
                {
                    "sent": "Learning lunch is sort of the time to pull out the interesting mathematics and models and algorithms.",
                    "label": 0
                },
                {
                    "sent": "I'll just start with a few pictures of the robot, but you won't get a lot of that in the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "This is the smart Wheeler platform, which we're building right now at McGill University in assistive device, and the context is an intelligent wheelchair.",
                    "label": 0
                },
                {
                    "sent": "In the main goal of this project is to help people with mobility disorders.",
                    "label": 0
                },
                {
                    "sent": "Obviously in the longer term, but in the shorter term there are really hard questions in terms of human robot interaction that need to be addressed.",
                    "label": 0
                },
                {
                    "sent": "Perspective we sort of assume that all the navigation stuff has been solved for us.",
                    "label": 0
                },
                {
                    "sent": "We're mostly in indoor environments and will assume that part of the problem is solved pretty robustly by existing methods.",
                    "label": 0
                },
                {
                    "sent": "But I think we still lack good mathematical frameworks to reason about how human robots should interact.",
                    "label": 0
                },
                {
                    "sent": "This is a brief, probably fuzzy picture of what our architecture looks like in terms of the interaction.",
                    "label": 0
                },
                {
                    "sent": "The important part is you have the user right here, and it's interacting with the chair through some touch screen.",
                    "label": 0
                },
                {
                    "sent": "Visual tactile input in through speech.",
                    "label": 0
                },
                {
                    "sent": "And we have all these parts in place, but the real hard question from our point of view is how the robot should choose actions during its interaction.",
                    "label": 0
                },
                {
                    "sent": "What should it say?",
                    "label": 0
                },
                {
                    "sent": "What should it do in response to input from the human?",
                    "label": 0
                },
                {
                    "sent": "And there's all sorts of things that make this difficult in terms of the noise.",
                    "label": 0
                },
                {
                    "sent": "The complexity of the observation space, and so on.",
                    "label": 0
                },
                {
                    "sent": "And funnily enough, our target population really isn't very tolerant of the robot not performing well, despite the fact that all of these things are difficult and we really are targeting deployment of this chair.",
                    "label": 0
                },
                {
                    "sent": "In fact, we have tests with the target population that we're aiming to conduct starting in January, so this isn't some kind of hypothetical scenario.",
                    "label": 0
                },
                {
                    "sent": "In January, there will be a disabled person sitting in that chair, not that one specifically, another prototype that's slightly more comfortable, but there will be a person sitting in there.",
                    "label": 0
                },
                {
                    "sent": "And the interaction manager better be doing something reasonable, so this is a story about how do we do this.",
                    "label": 0
                },
                {
                    "sent": "In the past alot of this has been done with customized solution.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One takes the time and the care to developed a really nice script.",
                    "label": 0
                },
                {
                    "sent": "Maybe they put it in a finite state machine or look up table or something like that that will closely script the interaction between the human user and the wheelchair.",
                    "label": 0
                },
                {
                    "sent": "In many people have looked at how we can introduce learning into that and said well, we don't want our finite state.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Machine to be fixed from the beginning will collect some data.",
                    "label": 0
                },
                {
                    "sent": "Look at how human and wheelchairs interact for awhile, and then we'll pour this data into our model.",
                    "label": 0
                },
                {
                    "sent": "And then we'll run a little bit of automated planning.",
                    "label": 0
                },
                {
                    "sent": "All this and there's been some work on that also.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the framework that we're adopting is a little bit more interactive now.",
                    "label": 0
                },
                {
                    "sent": "This line between supervised learning and reinforcement learning is a little bit fuzzy, but the distinction we're making is really that the robot has some initiative to play during the learning.",
                    "label": 0
                },
                {
                    "sent": "It gets to pick what it wants to observe, and it uses that to drive the learning.",
                    "label": 0
                },
                {
                    "sent": "Hopefully by doing this we can get models that are more robust because the learning is targeted at figuring out the parts of the model that we need to find out.",
                    "label": 0
                },
                {
                    "sent": "Hopefully also we can be a little bit more data efficient when we do it through this framework.",
                    "label": 0
                },
                {
                    "sent": "And because I couldn't just leave the examples there and plunged.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into the more technical part, I thought I'd bring up a second example, which I'll raise a few times later in the talk.",
                    "label": 0
                },
                {
                    "sent": "This is another project going on in the lab right now, which also is motivating a lot of the results I present today.",
                    "label": 0
                },
                {
                    "sent": "This really has to do with medical treatment design.",
                    "label": 1
                },
                {
                    "sent": "For those of you who are interested in working in reinforcement learning but finding real world applications, it turns out that are really rich application domains outside of robotics, medical treatment design being one of them in one particular project, we're trying to do deep brain stimulation using reinforcement learning for people who suffer from epilepsy.",
                    "label": 0
                },
                {
                    "sent": "Turns out, there's now these little devices called neurostimulators, which people get implanted, and there's a sensing part of it where through.",
                    "label": 0
                },
                {
                    "sent": "Electrodes you can see the activities in the brain, EG type activities and there is also an electrode that can deliver electrical stimulation to the brain and people have been designing these devices, deploying them in humans and seeing how effective they can be to treat people with epilepsy.",
                    "label": 0
                },
                {
                    "sent": "Except right now all of the stimulation strategies are set by hand, so this is sort of the customized version of it.",
                    "label": 0
                },
                {
                    "sent": "They design A strategy, they tweak the parameters through trial and error.",
                    "label": 0
                },
                {
                    "sent": "And there's a few people who have started looking at learning in this context, so we're trying to apply reinforcement learning in this context.",
                    "label": 0
                },
                {
                    "sent": "And if you compare this task domain compared to a standard robotics test domain, there's a few things that makes this even harder.",
                    "label": 0
                },
                {
                    "sent": "The number one thing I think is that we really don't have good notion even of of what are the right set of features to be looking at.",
                    "label": 0
                },
                {
                    "sent": "How do we describe the observation for this space?",
                    "label": 0
                },
                {
                    "sent": "The other thing that makes it hard is we have no data.",
                    "label": 0
                },
                {
                    "sent": "The amount of you know with robots we have use of accumulated recording data with these devices.",
                    "label": 0
                },
                {
                    "sent": "They're on the order of a few dozen people who've been implanted with these devices, so just in terms of passive data, we don't have anything #3 in terms of collecting data.",
                    "label": 0
                },
                {
                    "sent": "I can probably find a good number of people in this room and around the world who are willing to lend me their robots for a couple hours of data collection.",
                    "label": 0
                },
                {
                    "sent": "I'd be hard pressed to find medical doctors who are going to let me deploy my reinforcement learning algorithm on these type of system.",
                    "label": 0
                },
                {
                    "sent": "The good thing is I don't need people quite yet.",
                    "label": 0
                },
                {
                    "sent": "And we have good partners who have a lot with a lab or they do individual work.",
                    "label": 0
                },
                {
                    "sent": "Sort of slices of rat brain.",
                    "label": 0
                },
                {
                    "sent": "In addition, they're letting us do a little bit of that.",
                    "label": 0
                },
                {
                    "sent": "But again, a lot of the same difficulties arise in this kind of context, which we see in the robotics context.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's the idea to motivate that there's a rich set of real world problems out there.",
                    "label": 0
                },
                {
                    "sent": "And if we want to be able to tackle these using reinforcement learning type of approach, we need to solve a few problems.",
                    "label": 0
                },
                {
                    "sent": "And big one, as far as I'm concerned is really this problem?",
                    "label": 0
                },
                {
                    "sent": "How do we learn models in a good way?",
                    "label": 0
                },
                {
                    "sent": "How do we do that in terms of being data efficient?",
                    "label": 0
                },
                {
                    "sent": "And how do we do that in terms of having some robust models at the end?",
                    "label": 0
                },
                {
                    "sent": "And so this is really what's been driving my research since I left CMU getting a sense of how?",
                    "label": 0
                },
                {
                    "sent": "What are the right models and algorithms to handle this kind of problem?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One thing hasn't changed since I left CMU and this is the fact that I'm really basing a lot of this work in the Palm DP framework.",
                    "label": 0
                },
                {
                    "sent": "I assume most of you are relatively familiar with this work.",
                    "label": 0
                },
                {
                    "sent": "Reed told me he's teaching it and I think it's a machine learning class.",
                    "label": 0
                },
                {
                    "sent": "I think a few places where people have picked up this background.",
                    "label": 0
                },
                {
                    "sent": "The basic definition is quite standard for any type of decision making or planning task.",
                    "label": 0
                },
                {
                    "sent": "There's a set of states that describe the state of the system.",
                    "label": 1
                },
                {
                    "sent": "There's a set of action.",
                    "label": 0
                },
                {
                    "sent": "This is a decision making part.",
                    "label": 0
                },
                {
                    "sent": "This isn't a passive system.",
                    "label": 0
                },
                {
                    "sent": "You can take action which causes some change in the system, and in the case of palm DP, that change is described using a probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "And then there's also a notion of reward or goal or something that gives some indication of whether the right actions to choose in which state.",
                    "label": 0
                },
                {
                    "sent": "In the palm DP setup, there's a few complicating factors, the biggest one being the fact that the state of the system isn't directly observable, so instead there's instead.",
                    "label": 0
                },
                {
                    "sent": "There's a set of observation which captures the part of the state that is in fact observable in the case of our epilepsy brain stimulation system, the observations that we get are these readings from the electrode, so e.g type readings which somehow capture what's going on in the brain.",
                    "label": 0
                },
                {
                    "sent": "What we'd really like to know is which neurons are firing, when, and how.",
                    "label": 1
                },
                {
                    "sent": "We can't get that level of information, so we have instead this reading.",
                    "label": 0
                },
                {
                    "sent": "There's observation probabilities that describe how the observation are admitted as a function of what's going on in the system, and the last little bit that we have is a distribution over state, and this is what captures really our state of information about the system.",
                    "label": 0
                },
                {
                    "sent": "We don't know the state of the underlying system, but we have a probability distribution over this state.",
                    "label": 0
                },
                {
                    "sent": "That tells us something about what's going on in the system.",
                    "label": 0
                },
                {
                    "sent": "And there's 2 interesting things to do when you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have a dynamic system models as a palm DP.",
                    "label": 0
                },
                {
                    "sent": "The first one is tracking this belief.",
                    "label": 0
                },
                {
                    "sent": "So as you learn more about the system as you observe your e.g signal, you need to make some info.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Friends about what's going on in the brain.",
                    "label": 0
                },
                {
                    "sent": "How does the probability of state changes so you can do that with Simple Bayes rule?",
                    "label": 0
                },
                {
                    "sent": "Bayesian filter in this case?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other interesting thing to do is figuring out what's the expected return of applying certain actions on this system.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple version of Bellman's equation for the POMDP case.",
                    "label": 0
                },
                {
                    "sent": "This value function tells you if I'm in a certain belief.",
                    "label": 1
                },
                {
                    "sent": "So if I think there's a certain distribution over my state, what is the expected long-term return of applying different actions?",
                    "label": 0
                },
                {
                    "sent": "So if I take the best action I can possibly apply, well, get some immediate reward.",
                    "label": 0
                },
                {
                    "sent": "Maybe it'll stop the seizure, or maybe I'll just deliver electrical stimulation and burn a neuron.",
                    "label": 0
                },
                {
                    "sent": "So there's some immediate reward.",
                    "label": 0
                },
                {
                    "sent": "And then there's the effect of what my action has, which takes me to some other part of my belief state, and then I can acquire more return.",
                    "label": 0
                },
                {
                    "sent": "So it's a standard dynamic programming kind of equation.",
                    "label": 0
                },
                {
                    "sent": "In terms of the model, these things are really well defined.",
                    "label": 0
                },
                {
                    "sent": "They've been defined for the last 60 years, 40 or 50 years or so.",
                    "label": 0
                },
                {
                    "sent": "What's more interesting is how do we do these two steps?",
                    "label": 0
                },
                {
                    "sent": "The belief monitoring and the value function?",
                    "label": 1
                },
                {
                    "sent": "In real time.",
                    "label": 0
                },
                {
                    "sent": "In cases where your model is not really clear and by model I really mean the transition probabilities in the observation probabilities.",
                    "label": 1
                },
                {
                    "sent": "If you don't know what these are, you a little bit in trouble here.",
                    "label": 0
                },
                {
                    "sent": "An in cases where the state space is very large or your observation space ultimately is really large also.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I sort of set up a grand agenda.",
                    "label": 0
                },
                {
                    "sent": "It's probably going to keep me busy for a number of years.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you which part of this problem I'm tackling right now.",
                    "label": 0
                },
                {
                    "sent": "We'll assume we have a problem which can reasonably well be modeled using a palm DP framework, and we assume we have the ability to sample trajectory somehow, get some data from this domain.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the purposes of today's talk, I want to talk about two separate result in this framework.",
                    "label": 0
                },
                {
                    "sent": "The first one is assume that the trajectories have labeled state information.",
                    "label": 1
                },
                {
                    "sent": "So someone's gone and annotated the state of the system.",
                    "label": 0
                },
                {
                    "sent": "But you can't control the choice of action.",
                    "label": 1
                },
                {
                    "sent": "We don't often work in this framework for the reinforcement learning domain in robotics, it's not particularly interesting when you start dealing with medical application.",
                    "label": 0
                },
                {
                    "sent": "This is much more typical in that people are not giving you the control over the choice of action at first.",
                    "label": 1
                },
                {
                    "sent": "They are willing to record data from certain strategies or policies, and they're willing to give you this data, but they're not willing to give you on line control of the system.",
                    "label": 0
                },
                {
                    "sent": "Unless you've shown that you have the ability to do something reasonable.",
                    "label": 0
                },
                {
                    "sent": "So this is the first half of the talk for.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Today and the second half is the following will do things a little bit with more flexibility.",
                    "label": 0
                },
                {
                    "sent": "Now we assume we can control the agent during data collection, so we get to pick the actions while we collect data.",
                    "label": 1
                },
                {
                    "sent": "But then the states are only partially observable.",
                    "label": 0
                },
                {
                    "sent": "No ones annotating the states for us.",
                    "label": 0
                },
                {
                    "sent": "So there's the batch case, the online case, and we'll talk a little bit more about what kinds of algorithms we can use for each of these cases, and what are the effects of that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with a simple case, the batch case.",
                    "label": 1
                },
                {
                    "sent": "We have a palm DP problem domain.",
                    "label": 0
                },
                {
                    "sent": "We assume that the dynamics are not known, but we assume someone is giving us, giving us a batch of data and to simplify things a little bit more, asking even simpler question, which is if I have sampled trajectories from 2 policy's.",
                    "label": 0
                },
                {
                    "sent": "So I take my system, I fix an action selection policy.",
                    "label": 0
                },
                {
                    "sent": "I diploid, this collective batch of data.",
                    "label": 0
                },
                {
                    "sent": "Now I do the same thing but with a different strategy.",
                    "label": 0
                },
                {
                    "sent": "Can I tell which of these two policies is the better one?",
                    "label": 1
                },
                {
                    "sent": "And can I say whether I'm confident in this choice?",
                    "label": 0
                },
                {
                    "sent": "And this seems like a really simple problem.",
                    "label": 0
                },
                {
                    "sent": "And Palm DP's are reasonably good at doing this part.",
                    "label": 0
                },
                {
                    "sent": "We've been doing policy evaluation for a long time.",
                    "label": 0
                },
                {
                    "sent": "We haven't done this at all in terms of palm.",
                    "label": 0
                },
                {
                    "sent": "DP's most people who do robotics may not care too much as long as their robot doesn't fall down the stairs.",
                    "label": 0
                },
                {
                    "sent": "They don't care.",
                    "label": 0
                },
                {
                    "sent": "People who do medical treatment design care a lot about this.",
                    "label": 0
                },
                {
                    "sent": "All of the experimental design and optimization of treatment system is based on this notion of testing for significance.",
                    "label": 0
                },
                {
                    "sent": "Can I show that a certain treatment is significantly better than a placebo treatment or standard care?",
                    "label": 0
                },
                {
                    "sent": "So if we want to use palm DP models to optimize choice of treatment, we need to be able to answer this very simple question.",
                    "label": 0
                },
                {
                    "sent": "In the wheelchair domain, which is why.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're using this also.",
                    "label": 0
                },
                {
                    "sent": "It has some nice applications.",
                    "label": 0
                },
                {
                    "sent": "I've been talking to the people in the speech and dialogue group at AT&T and they do this a lot.",
                    "label": 0
                },
                {
                    "sent": "They fix their dialogue strategy when you have the automated phone in system where you call in at Auntie, there's a whole menu of what you need to answer before they tell you the information you need.",
                    "label": 0
                },
                {
                    "sent": "These policies are usually fixed by hand and there's people in the research group at 8:24 interested in using Palm DP's to do this, but most of the data they collect is from these fixed policy.",
                    "label": 0
                },
                {
                    "sent": "AT&T isn't interested in putting in the palm DP right in the loop before the researchers can demonstrate that there's good confidence that the policy is going to be of a good quality.",
                    "label": 0
                },
                {
                    "sent": "So in the case of our robot, really, if we take a simple scenario, really we have the option either to ask for clarification or to go through a given location.",
                    "label": 1
                },
                {
                    "sent": "So this is a very simple simplified example, but which has all of the characteristics of what we're looking for.",
                    "label": 0
                },
                {
                    "sent": "And because we're dealing with human robot interaction again, we really don't have good models.",
                    "label": 0
                },
                {
                    "sent": "Of how to do this?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Few quick recap of terminology for those who might not have seen Palm DPS recently when I talk about policy, what I really mean is a function mapping belief states to action.",
                    "label": 1
                },
                {
                    "sent": "So given that my Bayes filter has.",
                    "label": 0
                },
                {
                    "sent": "Maintain a certain belief.",
                    "label": 0
                },
                {
                    "sent": "What action should I do in that belief?",
                    "label": 0
                },
                {
                    "sent": "And based on that then the value function as I defined a little bit earlier is a function mapping belief space to the expected return of running that particular policy.",
                    "label": 1
                },
                {
                    "sent": "And there's some nice properties about Palm DP's.",
                    "label": 1
                },
                {
                    "sent": "For those of you who were here.",
                    "label": 0
                },
                {
                    "sent": "For Nick's proposal this morning, he did a nice job at defining some of these, but just to remind you, for the ones who didn't get up quite so early, the policy can be shown as a finite state controller.",
                    "label": 0
                },
                {
                    "sent": "Where the nodes are the action of the system and the arrows really are the observation.",
                    "label": 0
                },
                {
                    "sent": "So I can start by asking where the robot wants me to go and if I get a clear answer that they want to go to why I might ask again, and if it confirms again then I might go to that specific location and there's a nice duality between the policy representation and the value function representation.",
                    "label": 1
                },
                {
                    "sent": "If that part is fuzzy in the back of your brain, you really don't need to understand it so much for the purpose of today's talk.",
                    "label": 0
                },
                {
                    "sent": "What's important is that we can capture the policy with a finite state controller.",
                    "label": 0
                },
                {
                    "sent": "Because the result we developed is really based on this representation of a comedy policy.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I've stated a reasonably simple problem.",
                    "label": 0
                },
                {
                    "sent": "All I want to do is compare to policies and be able to tell which is best and by how much or if there's a nice difference between the two, so we can set this up as a policy evaluation.",
                    "label": 0
                },
                {
                    "sent": "This is just my Bellman equation, but in matrix form I have my value function, my reward, my transition probability, observation policy, observation probability.",
                    "label": 1
                },
                {
                    "sent": "This is the value at the next time step in this.",
                    "label": 0
                },
                {
                    "sent": "Capital Pi are the state transition of my finite state controller, so This is why I needed that representation of the policy.",
                    "label": 1
                },
                {
                    "sent": "This is dates back to the work of Sondik in the 70s.",
                    "label": 0
                },
                {
                    "sent": "This definition of policy evaluation, it turns out you can express it just in terms of the value function, and now you get a nice closed form solution for your policy evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "This is good so far, but most of the time when people do policy evaluation, they may take data samples, estimate their transition their observation model, but then they just apply policy evaluation as it is what we really want to do today is estimate the variance in the value function.",
                    "label": 1
                },
                {
                    "sent": "So we need some way to go from our recorded trajectories with labeled state information to assume to estimate these transition and these observation models.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we'll start with a frequentist approach to estimating the model, and This is why it was important to get our state labels.",
                    "label": 1
                },
                {
                    "sent": "We need to know when did the system go from state I to state J.",
                    "label": 0
                },
                {
                    "sent": "We can count this using standard basic frequentist statistics and we can see when our system is in state.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "How many times does it see observation J and I just need to kind of count this from my batch of data.",
                    "label": 0
                },
                {
                    "sent": "Nothing is complicated here.",
                    "label": 0
                },
                {
                    "sent": "And in the case, in the limit, where have a really big batch of data.",
                    "label": 1
                },
                {
                    "sent": "Then these estimates are going to be really very good, but in most cases, specially with medical data, we deal with finite samples.",
                    "label": 0
                },
                {
                    "sent": "So you've only tried heat treatment on a few patients.",
                    "label": 1
                },
                {
                    "sent": "If you've run a clinical trial, maybe you have 20, maybe 40 patients, each of whom got the same treatment, but you don't have enough data to assume that your estimation are very are absolutely correct, so we can assume that our estimate of the transition is really the correct transition plus.",
                    "label": 0
                },
                {
                    "sent": "Some error term and will make some assumption about these error terms.",
                    "label": 0
                },
                {
                    "sent": "We assume that it's an additive model for both the transition and the observation, and we'll assume the noise in this case is unbiased and the noise isn't correlated between the transition and observation, and that we can estimate the covariance terms from data.",
                    "label": 0
                },
                {
                    "sent": "So the question we're really asking is what is the variance in the value fun?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if I have that estimation of variance in the value function, then I can tell apart two policies.",
                    "label": 0
                },
                {
                    "sent": "So there's a little bit of math that goes on in how you do this, But it turns out you can approximate this reasonably well.",
                    "label": 0
                },
                {
                    "sent": "This is just the Bellman equation that I showed a little bit earlier, and I can substitute the model error.",
                    "label": 0
                },
                {
                    "sent": "So instead of having my model estimates and here I put the true and the error term, and this is where we need to start approximating things.",
                    "label": 0
                },
                {
                    "sent": "You can approximate this using a Taylor expansion.",
                    "label": 1
                },
                {
                    "sent": "It turns out there's a lot of technical detail in the paper which I won't go into today, but the point is that you can get a nice second order.",
                    "label": 0
                },
                {
                    "sent": "Taylor approximation for that particular equation.",
                    "label": 0
                },
                {
                    "sent": "That gives you a good estimate for the variance in your value function.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And once you have that, you can actually ask what's the what's.",
                    "label": 0
                },
                {
                    "sent": "The bias is my estimate of the value function with the variance, and that was the covariance and you can get closed form expressions for this.",
                    "label": 0
                },
                {
                    "sent": "For the case where you have a batch of data, finite sample set where you've estimated these things.",
                    "label": 0
                },
                {
                    "sent": "And so using this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can actually apply this in the context of a simple dialog example, and the way we usually apply this as we assume we fix the true models in order to test this.",
                    "label": 0
                },
                {
                    "sent": "Get some data.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit of a bootstrapping kind of estimate.",
                    "label": 0
                },
                {
                    "sent": "We fix the true models and then we generate a set of N test cases, each containing a fixed number of sample, and for each test case we get an estimate of the value function and we calculate the standard deviation using the method I just described an you can measure how often the true value function, which we calculated with the true model is off from our estimate in terms of one or two standard deviations, and see how that relates to our estimate.",
                    "label": 1
                },
                {
                    "sent": "Of that variance.",
                    "label": 0
                },
                {
                    "sent": "So this first step is just validating that our estimate is actually pretty good.",
                    "label": 0
                },
                {
                    "sent": "Remember, we did this second order Taylor expansion approximation.",
                    "label": 0
                },
                {
                    "sent": "To estimate that variance, so we first want to check whether that variance estimation is reasonably good.",
                    "label": 0
                },
                {
                    "sent": "And what we find out is in fact.",
                    "label": 0
                },
                {
                    "sent": "Gives us a pretty good act.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Risky in terms of our estimate, this is.",
                    "label": 0
                },
                {
                    "sent": "For standard deviation, one standard deviation 2 standard deviation as a function of the number of samples.",
                    "label": 1
                },
                {
                    "sent": "But even with a smallish number of sample things are reasonably good.",
                    "label": 1
                },
                {
                    "sent": "These are not tiny samples, mind you, you need reasonably enough data, but when you do, the accuracy of the variance estimate is actually quite good.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once you do that, you can start asking interesting questions.",
                    "label": 0
                },
                {
                    "sent": "Such as what is a good strategy for my wheelchair in terms of confirming information?",
                    "label": 0
                },
                {
                    "sent": "I have three strategies here.",
                    "label": 0
                },
                {
                    "sent": "Is the ask once where the person wants to go ask twice?",
                    "label": 1
                },
                {
                    "sent": "Ask three times.",
                    "label": 0
                },
                {
                    "sent": "And remember that we don't have any notion of what are the observation probabilities, so what's the noise in terms of the response rate that noise usually comes from the speech recognizer, But these results show is that ask twice is definitely the better strategy to take if you compare, ask one, or as three times.",
                    "label": 0
                },
                {
                    "sent": "In expectation, ask once is better than ask three times, but when you take the variance into account then there is not much difference.",
                    "label": 1
                },
                {
                    "sent": "They're really quite overlapping in here.",
                    "label": 0
                },
                {
                    "sent": "I think it's one standard deviation interval for the three different policies, but you can actually compare these strategies.",
                    "label": 1
                },
                {
                    "sent": "And if for some reason AT&T decides that asking twice cannot be deployed and you can only ask once or ask three times whatever the reason, maybe then you have good grounds to decide which of the two you may want to apply.",
                    "label": 0
                },
                {
                    "sent": "If you want a variance minimizing kind of system so that people are very familiar with the system is going to do every time everyone has the same experience, then maybe you want to ask three times.",
                    "label": 0
                },
                {
                    "sent": "If you're willing to make a lot more mistakes on a few subjects, but not everyone, then you can ask only once.",
                    "label": 0
                },
                {
                    "sent": "So we have a way to talk about how we can compare these.",
                    "label": 0
                },
                {
                    "sent": "Now if we apply the same framework in the case of treatment strategies, you also get some interesting result.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is not using the epilepsy data that I talked about briefly earlier this is using data from random clinical trial of people with depression.",
                    "label": 1
                },
                {
                    "sent": "We had 4000 patients registered in this trial.",
                    "label": 0
                },
                {
                    "sent": "I say we in fact, researchers, some of them at University of Pittsburgh and number of other centers collected this data over five years and an interesting question we can ask is given two treatment strategies which are the same, but one of them includes psychotherapy.",
                    "label": 1
                },
                {
                    "sent": "So an appointment with a shrink once a week and one of them doesn't include psychotherapy.",
                    "label": 0
                },
                {
                    "sent": "What's the difference?",
                    "label": 0
                },
                {
                    "sent": "So these two sets of people receive the same drug treatment, but half of them actually also meet a psychiatrist on a regular basis, and half of them don't.",
                    "label": 0
                },
                {
                    "sent": "And the reason it's important to compare these two strategies is a lot of people don't really like the extra burden of going to the psychotherapist.",
                    "label": 0
                },
                {
                    "sent": "They might just prefer to take their drug in the morning and not bother with anything more.",
                    "label": 0
                },
                {
                    "sent": "So can we see what is the effect of the psychotherapy?",
                    "label": 0
                },
                {
                    "sent": "And what we see is that if you just look at expectation, it seems that using psychotherapy.",
                    "label": 0
                },
                {
                    "sent": "Has a positive effect, but if you also include the variance estimation well.",
                    "label": 0
                },
                {
                    "sent": "There's not a meaningful effect, and this is 2 standard deviations and this is for our patient population in terms of the number of samples we had for each of these two different policies.",
                    "label": 1
                },
                {
                    "sent": "If you had many more patients registered in the study, maybe you'd see a meaningful difference, but based on the data we've collected in our clinical trial, this would be the recommendation in this case, and this is the kind of results that our partners in the health side are really interested in that you can add some confidence bounds.",
                    "label": 0
                },
                {
                    "sent": "To your estimate of the value function as opposed to just saying my reinforcement learning system recommends that everyone be registered for psychotherapy.",
                    "label": 0
                },
                {
                    "sent": "And by the way, in this we didn't even include the cost of the psychotherapy.",
                    "label": 1
                },
                {
                    "sent": "All we included in terms of a reward function was the expectation of remission.",
                    "label": 0
                },
                {
                    "sent": "Whether people would actually achieve a reduction of their depression symptoms over some period of fixed period of time.",
                    "label": 0
                },
                {
                    "sent": "So at some point you could add the cost of cost of psychotherapy if you wanted to, true.",
                    "label": 0
                },
                {
                    "sent": "There's sort of a night strategy, which is doesn't have any layers actually value function right after, you just count the rewards along the sample paths, yet average.",
                    "label": 0
                },
                {
                    "sent": "Look at that rate.",
                    "label": 0
                },
                {
                    "sent": "What's the relevant benefits?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right, so you could.",
                    "label": 0
                },
                {
                    "sent": "We haven't tried it in terms of the empirical sense in terms of whether there's some real benefit.",
                    "label": 0
                },
                {
                    "sent": "In this case, I think you.",
                    "label": 0
                },
                {
                    "sent": "What you get in terms of doing the analysis this way?",
                    "label": 0
                },
                {
                    "sent": "I don't know if you get more, I haven't thought about it very much, but.",
                    "label": 0
                },
                {
                    "sent": "The whole yeah, the hope is that do somehow can leverage your data in a better way by estimating the error on these models precisely.",
                    "label": 0
                },
                {
                    "sent": "And if there's some in dependencies in terms of the errors and so on, you can get away with better results with a smaller sample set.",
                    "label": 0
                },
                {
                    "sent": "Now I have no theoretical or empirical results to support this, But that's certainly our intuition.",
                    "label": 0
                },
                {
                    "sent": "Their approach, which I thought you wouldn't do that, there's some plenty of work and robust MVP's, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's some of that.",
                    "label": 0
                },
                {
                    "sent": "There's a whole literature on that too, and we're looking at some of that, so we haven't applied it in this context yet.",
                    "label": 0
                },
                {
                    "sent": "State machine it's the same, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's sort of what I wanted to say on this particular question, I should clarify that the work we did is actually a generalization of earlier work that had been done in the MVP case, so some of the techniques in terms of the estimation of the variance were there before it was just a question of generalizing it to the palm DP and the and the controller finite policy controller representation.",
                    "label": 0
                },
                {
                    "sent": "But we think this is really useful to quantify performance when you have some critical task domains.",
                    "label": 1
                },
                {
                    "sent": "Not necessary everywhere.",
                    "label": 0
                },
                {
                    "sent": "Lot of robotic systems you really don't need to worry about it.",
                    "label": 0
                },
                {
                    "sent": "But in some domains which have a critical component and this can give you a lot of power to your analysis.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The control online.",
                    "label": 0
                },
                {
                    "sent": "There's nothing here about how to adjust the control on line if you're doing optimization of your value function, we don't quite know how to do this yet because then you have this Max operator, which really complicates things a little bit.",
                    "label": 0
                },
                {
                    "sent": "We have some ideas of how to do that, but we haven't gotten there yet.",
                    "label": 0
                },
                {
                    "sent": "It turns out when your data is collected in a batch.",
                    "label": 0
                },
                {
                    "sent": "This works pretty well because there's a fixed strategy to collect the data.",
                    "label": 0
                },
                {
                    "sent": "Whether it's a randomized strategy or whether it's a strategy prescribed by clinical practice.",
                    "label": 0
                },
                {
                    "sent": "That strategy usually is there.",
                    "label": 1
                },
                {
                    "sent": "The same with dialogue system, but we say nothing yet about how to estimate the value function directly.",
                    "label": 0
                },
                {
                    "sent": "So unless there's other questions, I'll move on to the second part.",
                    "label": 0
                },
                {
                    "sent": "I just have another question, go ahead.",
                    "label": 0
                },
                {
                    "sent": "In the difference between Robert or Medical experiment as you decide about which finite set to do your comparison.",
                    "label": 0
                },
                {
                    "sent": "Well, we treat the two projects pretty independently and then.",
                    "label": 0
                },
                {
                    "sent": "It's more relevant because many this is the state.",
                    "label": 0
                },
                {
                    "sent": "For this case, yeah, we do them completely independently.",
                    "label": 0
                },
                {
                    "sent": "We take the method and we get data from the medical setting, and we learn the policy estimation.",
                    "label": 0
                },
                {
                    "sent": "The errors for that setting, and completely separately we do.",
                    "label": 0
                },
                {
                    "sent": "The estimation method is the same, but otherwise the models and estimation are completely separate.",
                    "label": 1
                },
                {
                    "sent": "OK, so the set up for the second part is a little bit different.",
                    "label": 0
                },
                {
                    "sent": "We're going to complicate things in two ways.",
                    "label": 0
                },
                {
                    "sent": "The first one is that we assume that we don't have the state labels.",
                    "label": 1
                },
                {
                    "sent": "This is a lot more realistic.",
                    "label": 0
                },
                {
                    "sent": "And the second one is that we're going to assume we have control over how the data is collected in a lot of robotics domain this is perfectly fine in the medical domain this is not fine in terms of certainly no one is offering to let me run a randomized clinical trial anytime soon, and even in the epilepsy case, I'm not going to collect data on humans.",
                    "label": 1
                },
                {
                    "sent": "But as I've mentioned for epilepsy, we have a nice rat model where we have collaborators which allow us to come in with our palm DP based decision maker and plug it into the.",
                    "label": 0
                },
                {
                    "sent": "Right brain and let us collect data however we want, and we're doing this sort of on a biweekly basis, so we have this kind of setup right now where we don't have the state labels.",
                    "label": 0
                },
                {
                    "sent": "We really don't know what's the state of the neurons at anytime in point, or even some indicators of epilepsy.",
                    "label": 0
                },
                {
                    "sent": "But we do get control over how the data is collected.",
                    "label": 0
                },
                {
                    "sent": "This is both a blessing and a curse.",
                    "label": 0
                },
                {
                    "sent": "It's great that we have the control, but we need to use it wisely.",
                    "label": 0
                },
                {
                    "sent": "We need to pick actions that make sense.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, what's the point of having that control?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the online case.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're also going to adopt A Bayesian framework in this case, rather than a frequentist framework, and the general Bayesian framework is, I'm sure, familiar to everyone here.",
                    "label": 0
                },
                {
                    "sent": "We assume we have some prior distribution over unknown parameters, so we'll still assume that the model of the transitions and the observation is not known apriori.",
                    "label": 0
                },
                {
                    "sent": "But we have some notion of what might be good parameters.",
                    "label": 0
                },
                {
                    "sent": "We're going to update the posterior via Bayes rule as experience is acquired and the more interesting part is how do we pick our actions with respect to that posterior.",
                    "label": 1
                },
                {
                    "sent": "So standard Bayesian framework, but in the palm DP setting.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the advantage from our perspective is that it gives us a nice way to include prior knowledge explicitly.",
                    "label": 1
                },
                {
                    "sent": "In the epilepsy domain, we really have very little prior knowledge, so we pick very agnostic priors in the dialogue management problem.",
                    "label": 0
                },
                {
                    "sent": "We have better priors.",
                    "label": 0
                },
                {
                    "sent": "We have some sense of when the robot says something the person is going to reply something specific, but there's some noise factor, but we still have some sense of what the model might look like.",
                    "label": 0
                },
                {
                    "sent": "So this framework lets us include that knowledge.",
                    "label": 1
                },
                {
                    "sent": "It let's us perform learning as necessary because we're in this online setting where we get to pick our actions.",
                    "label": 0
                },
                {
                    "sent": "So we get the advantage of that.",
                    "label": 0
                },
                {
                    "sent": "And the last point, which is when that I think is useful.",
                    "label": 1
                },
                {
                    "sent": "It really allows us to explicitly consider the uncertainty during our planning to pick actions.",
                    "label": 0
                },
                {
                    "sent": "Yes, that will give us good reward and yes, that will try to figure out what the state of the system is, but in particular that will tradeoff between when do I need to learn more about my model in one day and I need to be pretty confident about what I know and pick the strategies which I think are good.",
                    "label": 0
                },
                {
                    "sent": "So just a quick recall.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those of you with short memory or post food induced coma about the palm DP model the same setup as before and the question here is really how should we choose actions if parameters tiano are uncertain.",
                    "label": 1
                },
                {
                    "sent": "Will assume for the purpose of today's talk that the reward function is known.",
                    "label": 0
                },
                {
                    "sent": "I did that for the first half.",
                    "label": 0
                },
                {
                    "sent": "I'm doing that.",
                    "label": 0
                },
                {
                    "sent": "Again, both techniques can be generalized quite easily to the case where the reward is not known, but it keeps things cleaner and simpler for today so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This notion of Bayesian learning has been explored in the MVP setup and the case where you know the state of the system.",
                    "label": 0
                },
                {
                    "sent": "Duff did a lot of work dirden poop on in France and in that set up.",
                    "label": 0
                },
                {
                    "sent": "The idea is quite simple.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit like the frequentist idea.",
                    "label": 0
                },
                {
                    "sent": "You count the number of times.",
                    "label": 1
                },
                {
                    "sent": "That you transition from State S to S prime appan taking action A.",
                    "label": 0
                },
                {
                    "sent": "So you're just counting as we did before, but you start from some prior.",
                    "label": 0
                },
                {
                    "sent": "And that project you can think of as imagined experiences.",
                    "label": 0
                },
                {
                    "sent": "I find pretty sure that transition is going to happen.",
                    "label": 0
                },
                {
                    "sent": "It will be like seeing it 10 times, and then I'll start counting how many more times I see it.",
                    "label": 1
                },
                {
                    "sent": "So these counts define the Irish light distribution over the transition model.",
                    "label": 0
                },
                {
                    "sent": "I'll assume most people are familiar with the Jewish light distribution.",
                    "label": 0
                },
                {
                    "sent": "The good thing is it's a nice closed.",
                    "label": 0
                },
                {
                    "sent": "Close form update for it.",
                    "label": 0
                },
                {
                    "sent": "Upon seeing the information and so these counts are sufficient statistics for defining the jewishly posterior, so it's easy to update.",
                    "label": 1
                },
                {
                    "sent": "And now all we need to do is plan according.",
                    "label": 0
                },
                {
                    "sent": "To these counts, so you can formulate an UNDP model which the state space is defined as the original physical state and the information state meaning our count vectors.",
                    "label": 0
                },
                {
                    "sent": "And now the probability depends on both being in a state and having a certain count.",
                    "label": 0
                },
                {
                    "sent": "Seeing a certain action will take you to a new state.",
                    "label": 0
                },
                {
                    "sent": "Any set of counts.",
                    "label": 0
                },
                {
                    "sent": "So the count account part is deterministic and the state to state part can be probabilistic.",
                    "label": 0
                },
                {
                    "sent": "And so once you set up this extended MVP, we can solve it just a standard MDP or counselor discreet if your state or discrete you can run your favorite MDP solver and things are all well.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "In finite palm GPS we can extend the same framework.",
                    "label": 1
                },
                {
                    "sent": "Instead of having only counts over transition, now we need the counts over transition an account over the observation and now we have a decision problem over an extended states with include the physical state, the transition count.",
                    "label": 1
                },
                {
                    "sent": "And the observation count.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "We can set up a full.",
                    "label": 0
                },
                {
                    "sent": "Asian palm DP model.",
                    "label": 0
                },
                {
                    "sent": "Based on this I have my extended state space.",
                    "label": 0
                },
                {
                    "sent": "Physical state accounts.",
                    "label": 0
                },
                {
                    "sent": "The other counts the action and observation stay the same and I can design A joint observation action function which says if I'm in a certain state, take a certain action, see a certain observation.",
                    "label": 0
                },
                {
                    "sent": "What's my next state?",
                    "label": 0
                },
                {
                    "sent": "The reward function stays the same.",
                    "label": 0
                },
                {
                    "sent": "It's just another palm DP problem where the goal is to maximize the return.",
                    "label": 0
                },
                {
                    "sent": "Under partial observe ability of my state is nothing different in terms of formulating the model.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of a complication compared to the MDP state.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the complication?",
                    "label": 0
                },
                {
                    "sent": "Here I have an infinite state MDP 'cause my counts can be very big with a known model.",
                    "label": 0
                },
                {
                    "sent": "Here I have an infinite state palm DP with a known model state is defined over.",
                    "label": 1
                },
                {
                    "sent": "This state is defined over there.",
                    "label": 1
                },
                {
                    "sent": "This case at every time step my state is observable.",
                    "label": 1
                },
                {
                    "sent": "An my count file is updated in the Palm DP at every time.",
                    "label": 0
                },
                {
                    "sent": "Step S is not observable.",
                    "label": 0
                },
                {
                    "sent": "So my accounts are not observable.",
                    "label": 0
                },
                {
                    "sent": "I don't know which state I went from too, so I don't know which account to update.",
                    "label": 0
                },
                {
                    "sent": "That makes things a little bit tricky, not in terms of the model.",
                    "label": 0
                },
                {
                    "sent": "The model is super clean, but in terms of using this, in practice, we need to deal with that little bit of complication, which is how can we update these counters?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do we know?",
                    "label": 0
                },
                {
                    "sent": "Where to give the credit for a certain transition?",
                    "label": 0
                },
                {
                    "sent": "All we know is what action was taken, in what observation was perceived, but we don't know the states that are Dirichlet distributions are defined based on those dates.",
                    "label": 0
                },
                {
                    "sent": "Not to worry, this is the classical problem for palm DP.",
                    "label": 1
                },
                {
                    "sent": "This is the reason learning in Palm DPS is so much harder than learning and MVP's and why lot of the focus so far in the literature has been strictly on planning for the palm DP case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Will just turn.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Under palm DP machinery here, instead of having a belief state only over the original state, space will also have initial counts.",
                    "label": 1
                },
                {
                    "sent": "And we also use some way of monitoring the belief and the belief now will be expressed over the States and over the counts.",
                    "label": 0
                },
                {
                    "sent": "Now to define the beliefs over these counts, we need to sort of hypothesize that when I see this, it could be because this account needs to be updated in this account needs to be updated.",
                    "label": 1
                },
                {
                    "sent": "So all we need is a mixture of Jewish lay models, which tells you all of the different Jewish lies which could happen.",
                    "label": 0
                },
                {
                    "sent": "And that gives us a way to update this palm DP model using standard Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "From a practical point of view, maintaining that mixture is a little bit difficult because the number of components in that mixture grows quickly.",
                    "label": 0
                },
                {
                    "sent": "The more things you do, the more different ways your account could be updated so it doesn't scale very elegantly.",
                    "label": 0
                },
                {
                    "sent": "Here T is your planning horizon, so a little bit problematic in terms of.",
                    "label": 0
                },
                {
                    "sent": "The planning horizon.",
                    "label": 0
                },
                {
                    "sent": "Before I tell you how we can make this tractable.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll just mention 2 quick theoretical results that come with this POMDP model.",
                    "label": 1
                },
                {
                    "sent": "This is a slide with a lot of complicated math.",
                    "label": 0
                },
                {
                    "sent": "It's not a really complicated result that slide just says.",
                    "label": 0
                },
                {
                    "sent": "Assume that you're interested in estimating the value function for specific physical state specific count vectors, and you want to compare to the same value function at that same physical state, but slightly different count vectors.",
                    "label": 1
                },
                {
                    "sent": "Can you bound the error in the estimation of this difference?",
                    "label": 1
                },
                {
                    "sent": "There comes the complicated expression, but there's four parts to that complicated expression.",
                    "label": 0
                },
                {
                    "sent": "The first part says that this difference really depends on the expected value of the two count vectors, so I have my 2:00 to reach light distribution Phi and Phi prime in expectation there going to be a little bit different.",
                    "label": 0
                },
                {
                    "sent": "That's that term, and in expectation the size are going to be a little bit different.",
                    "label": 0
                },
                {
                    "sent": "And these two terms just says, well, that's all fine and dandy, but in some case FI and five prime are the same in expectation.",
                    "label": 0
                },
                {
                    "sent": "But the magnitude of the number of counts is different, and so that term just deals with the uncertainty in the jury's distribution over the transition, and certainly in the distribution over the observation.",
                    "label": 1
                },
                {
                    "sent": "The estimates you have.",
                    "label": 0
                },
                {
                    "sent": "So I have you, as you have more and more accounts.",
                    "label": 0
                },
                {
                    "sent": "This starts going away, but for small numbers of accounts you still need to put that into play, so this is over the value function.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the first theoretical result.",
                    "label": 0
                },
                {
                    "sent": "This is something we can measure.",
                    "label": 0
                },
                {
                    "sent": "It seems like big complicated in expression, which isn't very useful.",
                    "label": 0
                },
                {
                    "sent": "It turns out that that expression comes at the core of one of the approximation methods we use, and being able to bound that difference really helps us figure out how to approximate the mixture of Dirichlet's two more slides.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second theoretical result that comes with this framework is the following one.",
                    "label": 0
                },
                {
                    "sent": "So far I've told you that I can bound the difference in value function when the physical state is the same, but the counts are different.",
                    "label": 1
                },
                {
                    "sent": "What I'm telling you here is given that I have some value function at some state and counts how different is that in terms of.",
                    "label": 1
                },
                {
                    "sent": "The correct value function and I've thrown in some projection operator here.",
                    "label": 0
                },
                {
                    "sent": "If you want the formal definition of that, you can go in the paper.",
                    "label": 0
                },
                {
                    "sent": "The point here is that this is the case where the counts are going to be bounded to some upper value, and so we're interested in saying what if I bound my accounts to some upper value and I compared to the case where I don't found accounts 'cause in the Infinity feel that the accounts run up.",
                    "label": 0
                },
                {
                    "sent": "You'll do really well practice.",
                    "label": 0
                },
                {
                    "sent": "You might not want to do that and you might not want to stop early, so you want to know what's the performance cost you're losing.",
                    "label": 0
                },
                {
                    "sent": "And it turns out you can bound this.",
                    "label": 0
                },
                {
                    "sent": "Epsilon precision.",
                    "label": 1
                },
                {
                    "sent": "And this result tells you how many times you need to observe certain transitions and observation in order to have that epsilon precision.",
                    "label": 0
                },
                {
                    "sent": "So don't worry about the details of the expression, but you can use this result to figure out how many times you need to see different counts.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get an epsilon optimal solution.",
                    "label": 0
                },
                {
                    "sent": "In general, the answer is a lot.",
                    "label": 0
                },
                {
                    "sent": "These are really boost bound.",
                    "label": 0
                },
                {
                    "sent": "You need a lot of counts before these things.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You really get your epsilon approximate solution, so let me tell you a little bit about how we can make these a bit more practical in terms of real algorithms for tracking the belief and planning the first problem.",
                    "label": 0
                },
                {
                    "sent": "Is how do we maintain this belief?",
                    "label": 0
                },
                {
                    "sent": "How do we deal with this mixture of darish like components?",
                    "label": 0
                },
                {
                    "sent": "In particular the different possible count vectors?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we use standard machinery for approximating complex distributions over beliefs.",
                    "label": 0
                },
                {
                    "sent": "Particle filters are really the way to go here.",
                    "label": 0
                },
                {
                    "sent": "And the only thing is we need to figure out how much of these different count vectors to maintain.",
                    "label": 0
                },
                {
                    "sent": "Its really these are the number of count vectors you need to maintain that grows exponentially, so we're going to limit that decay, different count vectors, and if you use a simple Monte Carlo then will perform belief update will start with our mixture with K components, will project them forward one state according to action.",
                    "label": 1
                },
                {
                    "sent": "An observation that we see, and then we'll pick keep K of those so we always keep K samples at every time it grows, but we always bring it down to K of them and you just randomly pick Caleb.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you want to be a little bit more sophisticated, instead of keeping K random of them, keep the K with the highest probability.",
                    "label": 0
                },
                {
                    "sent": "How is a good thing to do?",
                    "label": 0
                },
                {
                    "sent": "It doesn't cost you much to calculate the weight and to keep that.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But since we spent a Steven in particular spend so much time working out these theoretical results, maybe this would be a good place to use them.",
                    "label": 0
                },
                {
                    "sent": "And So what does distance metric does?",
                    "label": 0
                },
                {
                    "sent": "Is it says OK?",
                    "label": 0
                },
                {
                    "sent": "I have my K different count vectors.",
                    "label": 0
                },
                {
                    "sent": "I project them forward based on a possible action observation.",
                    "label": 0
                },
                {
                    "sent": "Now have a lot more of them, but I want to keep only care of them.",
                    "label": 0
                },
                {
                    "sent": "The K of them which best fit the posterior according to the distance metric we had in theorem one.",
                    "label": 1
                },
                {
                    "sent": "So all of these differ in terms of the counts.",
                    "label": 0
                },
                {
                    "sent": "Look at these Capehart.",
                    "label": 1
                },
                {
                    "sent": "These more than K particles and pick the ones of them that best span the space.",
                    "label": 0
                },
                {
                    "sent": "And in theorem one all we had was a distance metric in terms of different count vectors.",
                    "label": 0
                },
                {
                    "sent": "So we can use that readily here.",
                    "label": 0
                },
                {
                    "sent": "This is more expensive to compute, not more by exponential factor.",
                    "label": 0
                },
                {
                    "sent": "There's some kind of linear factor, but it is definitely more expensive than any of these.",
                    "label": 0
                },
                {
                    "sent": "Maybe it means we can maintain fewer particles, but maybe it gives us much better information.",
                    "label": 0
                },
                {
                    "sent": "Remember that here we're using this belief for planning, so it's not a standard particle filter where you really need to keep a good density of your belief.",
                    "label": 0
                },
                {
                    "sent": "You really need to keep the set of beliefs which characterize the possible things that could happen, such that then you can create a good plan.",
                    "label": 0
                },
                {
                    "sent": "So this is what this does is really characterizes the distribution.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily proportional, but it characterizes no way that's useful for planning.",
                    "label": 0
                },
                {
                    "sent": "Now that we have a way to do approximate belief maintenance in these system, we also need some way to do planning.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of palm DP planning machinery out there.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's been developed in the last few years.",
                    "label": 0
                },
                {
                    "sent": "We use something a little bit simpler, which is just similar to receding Horizon control.",
                    "label": 0
                },
                {
                    "sent": "We assume that we do forward search.",
                    "label": 0
                },
                {
                    "sent": "This is just a forward heuristic search.",
                    "label": 0
                },
                {
                    "sent": "In the space of beliefs.",
                    "label": 0
                },
                {
                    "sent": "Well, when you think beliefs you have to think physical state and counts.",
                    "label": 0
                },
                {
                    "sent": "So if I have a certain physical state an initial counts, I could do this action, then I'll see this observation and it will take me to this distribution of counts an physical states.",
                    "label": 0
                },
                {
                    "sent": "So this is a forward search in the space of these beliefs.",
                    "label": 0
                },
                {
                    "sent": "It's very efficient depending on the branching factor, you may not be able to go very far and this is often what happens to us.",
                    "label": 0
                },
                {
                    "sent": "But the belief tracking approximate method I showed in the previous slide can be used to prune off some of these branches.",
                    "label": 0
                },
                {
                    "sent": "So this is what we do.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This hasn't quite made it onto the robot or the medical devices yet.",
                    "label": 0
                },
                {
                    "sent": "We worked with a very.",
                    "label": 0
                },
                {
                    "sent": "Simplified problem inspired by human robot interaction.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that the robot has to follow an individual within some kind of known discretized environment, but it turns out there's two different individuals at the robot can follow, and these individual really differ in terms of their motion model.",
                    "label": 1
                },
                {
                    "sent": "One of them is a graceful labeled Walker that walks at a good speed and the other one is more ambling, an walking much slower.",
                    "label": 0
                },
                {
                    "sent": "Robot knows are two individuals, but the robot doesn't have a preset model of how these people move.",
                    "label": 0
                },
                {
                    "sent": "So the robot simultaneously has to track this individual without losing track of them.",
                    "label": 0
                },
                {
                    "sent": "Learn what is the motion model of this individual also.",
                    "label": 0
                },
                {
                    "sent": "So pick actions that are able to do that.",
                    "label": 0
                },
                {
                    "sent": "You might not want to learn the model perfectly if it happens that the two of them move in similar ways, then you can learn an aggregate model, but somehow do this simultaneously.",
                    "label": 0
                },
                {
                    "sent": "Learning and planning task.",
                    "label": 0
                },
                {
                    "sent": "So the Bayesian framework is quite good for this.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I show you here is the effect of the choice of belief.",
                    "label": 0
                },
                {
                    "sent": "Belief maintenance approximation algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what happened if you have the exact model?",
                    "label": 1
                },
                {
                    "sent": "And you use just simple POMDP planning on this.",
                    "label": 0
                },
                {
                    "sent": "It's not that bigger problem, you can do that quite readily and this is the optimal return and.",
                    "label": 0
                },
                {
                    "sent": "Now, if you take your prior model and don't do any learning and just plan based on your prior model.",
                    "label": 0
                },
                {
                    "sent": "This is how well you're going to do and the others.",
                    "label": 0
                },
                {
                    "sent": "All of this Basean framework with a receding horizon control.",
                    "label": 0
                },
                {
                    "sent": "The difference is how you pick the K. Particles decay, different count vectors.",
                    "label": 0
                },
                {
                    "sent": "You're going to maintain overtime.",
                    "label": 1
                },
                {
                    "sent": "What we see is that Monte Carlo doesn't do very well.",
                    "label": 0
                },
                {
                    "sent": "But that both the K most probable and the weighted distance do quite well.",
                    "label": 1
                },
                {
                    "sent": "The number here in the buckets is how many particles we use.",
                    "label": 0
                },
                {
                    "sent": "What's that K?",
                    "label": 0
                },
                {
                    "sent": "So for both Monte Carlo and most probable we use 64 different count vectors which we maintain overtime.",
                    "label": 0
                },
                {
                    "sent": "In the case of the weighted distance, we could get away with only 16 years old because they capture that distribution much more cleanly.",
                    "label": 0
                },
                {
                    "sent": "If we look at the model accuracy so overtime, what's the error terms are for estimate of the motion model for?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The two individuals.",
                    "label": 0
                },
                {
                    "sent": "Again, this is not surprising given the previous result.",
                    "label": 0
                },
                {
                    "sent": "Monte Carlo really isn't able to capture a very good model.",
                    "label": 1
                },
                {
                    "sent": "The weighted distance and the most probable are actually quite good with weighted distance capturing a slightly better model in terms of this metric.",
                    "label": 1
                },
                {
                    "sent": "We don't expect this necessarily to go to zero because it could be that there's some parts of the model which are just not useful to learn.",
                    "label": 0
                },
                {
                    "sent": "You don't need these parts of the model to be able to behave well, and because this is a joint learning planning approach, it's only going to learn how much it needs to learn to be able to behave well, so this won't necessarily go to zero.",
                    "label": 0
                },
                {
                    "sent": "I don't know how better it gets as we run it, but it certainly seems to have flattened out quite a bit.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Comes the interesting question, how much longer do we have to pay in terms of planning time for using that complicated metric that we propose?",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out not a whole lot longer.",
                    "label": 0
                },
                {
                    "sent": "It is a bit slower, but in terms of planning time, it still comes out to be faster than the other two methods for this kind of an example.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just a quick summary of this.",
                    "label": 0
                },
                {
                    "sent": "The idea here is really to take some of these ideas in terms of Bayesian reinforcement learning that were developed in the context of MDP.",
                    "label": 0
                },
                {
                    "sent": "And extend them to the Palm DP framework.",
                    "label": 0
                },
                {
                    "sent": "The extension in terms of the model wasn't too complicated, but there's a lot of interesting questions that come up in terms of how do we do belief updating and how do we do planning in this kind of a framework.",
                    "label": 0
                },
                {
                    "sent": "And when we put all of this together we get the system that's able to do these three things together, which is quite important when you have one of these systems.",
                    "label": 0
                },
                {
                    "sent": "the Monte Carlo methods are really essential to allow you to do these kinds of approximation.",
                    "label": 1
                },
                {
                    "sent": "This isn't very.",
                    "label": 0
                },
                {
                    "sent": "Surprising, but it's interesting to see terms of how you should behave.",
                    "label": 0
                },
                {
                    "sent": "Sir Monte Carlo sampling based on what you know of this.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Particular domain.",
                    "label": 0
                },
                {
                    "sent": "Especially with this second half are really not ready yet to apply this to real world domains, so maybe we're not quite as far as it seems.",
                    "label": 0
                },
                {
                    "sent": "The observation features are used are really simplified so far.",
                    "label": 0
                },
                {
                    "sent": "This state description we assumed was very simple and when we need to deal with these problems, we need something a little bit richer in terms of the observation set.",
                    "label": 0
                },
                {
                    "sent": "The good news is Steven work a little bit more after this and developed a version of the beige and reinforcement learning that's applicable for continuous domains, so that's really helpful, especially for here, where we're getting continuous observation and also a version that's applied to structured domain.",
                    "label": 0
                },
                {
                    "sent": "So if you have a factored.",
                    "label": 0
                },
                {
                    "sent": "Representation in terms of your state, there's extension for both of these, and I'm not talking about it at all today.",
                    "label": 0
                },
                {
                    "sent": "So if you want to know more, you can invite Steven to give a follow up talk or look up the papers.",
                    "label": 0
                },
                {
                    "sent": "But we're moving slowly towards some of these problems, with more interesting complexity.",
                    "label": 0
                },
                {
                    "sent": "So just.",
                    "label": 0
                },
                {
                    "sent": "Few finishing words.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Probably most of you have seen this quote before.",
                    "label": 0
                },
                {
                    "sent": "There's question ability about whether you thought it was meaningful, but I think there's something to be said here.",
                    "label": 0
                },
                {
                    "sent": "Is that my talk today is really about taking these unknown unknowns and somehow trying to quantify something about them and trying to understand better uncertainty in the context of models for Palm DP's.",
                    "label": 1
                },
                {
                    "sent": "So that's all for today few people technology, but thank you.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't have those questions or if people need to run off, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How would you go about?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a really in a sense.",
                    "label": 0
                },
                {
                    "sent": "It's a very difficult.",
                    "label": 0
                },
                {
                    "sent": "Question, because certainly in the case of the epilepsy, it's like completely open.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not just how do we go about picking the structure?",
                    "label": 0
                },
                {
                    "sent": "It's like what is our observation space.",
                    "label": 0
                },
                {
                    "sent": "What is our action space, let alone what is our state space and how?",
                    "label": 0
                },
                {
                    "sent": "What are the dependencies within them?",
                    "label": 0
                },
                {
                    "sent": "So at that level it's really very difficult.",
                    "label": 0
                },
                {
                    "sent": "The good thing I would say that if you take a vision approach, you can be very generous about the space of structures you're willing to consider.",
                    "label": 0
                },
                {
                    "sent": "You can say I'm willing to consider all of these different state features and then let the Bayesian inference pick the right type of model to fit the data you have and some of the results we got with structured domains really included that influence over the structure in terms of the relations.",
                    "label": 1
                },
                {
                    "sent": "It's only a preliminary case where we were only doing inference in terms of the dependencies between the state variables, but the results are very nice in terms of what it infers based on the data it has.",
                    "label": 0
                },
                {
                    "sent": "When you have very small datasets, you'll pick very small observation sets.",
                    "label": 0
                },
                {
                    "sent": "You'll pick a very compact way of describing your model.",
                    "label": 1
                },
                {
                    "sent": "This is just Occam's razor.",
                    "label": 0
                },
                {
                    "sent": "If you have a lot of data, then you want a lot more detail in the structure of your model and the Bayesian framework does that really nicely in terms of the.",
                    "label": 0
                },
                {
                    "sent": "Mathematical framework Now how you get it to work in terms of the computational side.",
                    "label": 0
                },
                {
                    "sent": "Then that's a little bit more difficult, but there's some things that can be done in a lot of that is done in terms of inferring the structure in Bayes Nets and so on.",
                    "label": 0
                },
                {
                    "sent": "This is all things we can leverage here, so we have lots of other people working on these problems when they figure it all out.",
                    "label": 0
                },
                {
                    "sent": "Will Porten and music here.",
                    "label": 0
                },
                {
                    "sent": "What is the policy?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you have to be a little bit careful about that.",
                    "label": 0
                },
                {
                    "sent": "If you really wanted to do that inversion.",
                    "label": 0
                },
                {
                    "sent": "It turns out in our case we don't need to do the actual inversion.",
                    "label": 0
                },
                {
                    "sent": "We're really just estimating the error terms and so we don't want so much into that.",
                    "label": 0
                },
                {
                    "sent": "I presume there's lots of tricks you can do in terms of conditioning these matrices, which would help.",
                    "label": 0
                },
                {
                    "sent": "I don't know these tricks very well, but I presume there out there.",
                    "label": 0
                },
                {
                    "sent": "But it's a big question because a lot of times these matrices are quite sparse in terms of the amount of data you have in them, so you have to be a little bit concerned about that.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in just doing strict policy evaluation, but there's some gradient methods, I think that can get around that to some degree.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, well thank you for coming.",
                    "label": 0
                }
            ]
        }
    }
}