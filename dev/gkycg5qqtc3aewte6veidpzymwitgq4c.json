{
    "id": "gkycg5qqtc3aewte6veidpzymwitgq4c",
    "title": "The Variational Fair Autoencoder",
    "info": {
        "author": [
            "Christos Louizos, University of Amsterdam"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_louizos_fair_autoencoder/",
    "segmentation": [
        [
            "Hello everyone, I'm going to present the variational fare autoencoder, it's a joint work between the University of Amsterdam and the University of Toronto."
        ],
        [
            "We're diving into the model.",
            "First, provide some motivation of why we actually even consider such a model.",
            "Let's consider some sample tasks.",
            "Assume that we want to identify the idea of the person in a data set that looks like this.",
            "It's quite hard, since actually there is a lot of noise.",
            "In this case, the lighting conditions that obfuscate the real predictive information that our models capture.",
            "Now some similar tasks you might consider that we have an algorithm that tries to detect if someone is a possible."
        ],
        [
            "Suspect in a police investigation just by looking from photos now you might be constrained from the law to actually not take into.",
            "Like on some sensitive information that you might have available for your suspects, for example race and gender.",
            "Now another similar kind of application can be to detect from MRI images whether a given patient has eyes heimer.",
            "Usually you have MRI images from multiple machines, and you know from before hand that all of this machine related variation that is available in the data is not really productive for the ice climber disease."
        ],
        [
            "So tackling tackling such problems is not really that much trivial, since we can just exclude this particular bits from our inputs, as all of the other dimensions still contain information about those particular bits.",
            "So ideally what you want to do is to transform the data to a new representation.",
            "There now we can explicitly accord its properties and concerning the aforementioned problems we can form for some sort of invariants with respect to summer priority.",
            "Known information."
        ],
        [
            "So we're not the first have actually consider such tasks.",
            "The first kind of work that dealt with fair representations and essentially fair representations are representations that try to enforce some kind of invariants with respect to sensitive information.",
            "It was a simple, discriminative clustering approach.",
            "Then there are also your network methods that try to match feature distributions among different groups.",
            "There's a whole line of works with neural networks that use the maximum discrepancy penalty more independently later.",
            "Since you also use it.",
            "And there are social works that formulate this problem as a minimax kind of optimization, where the network tries to produce representations such that an adversary cannot distinguish the group members."
        ],
        [
            "So our contribution to this type of problems is the variational fair auto encoder which the generative model where some a priority known target factors variation are explicitly removed.",
            "As a result, we were able to obtain a new representation that is invariant with respect to this information and as well, so later in the experiments we achieve better performance on the fair classification domain adaptation at general feature learning task.",
            "So."
        ],
        [
            "Let's start simple and consider the first day and supervised version of this model can also be understood as a fully observed M2 model from the saving supervised variational auto encoder.",
            "For those of you that you're familiar with this work, we can assume that our data point X is the result of three dependent factors of variation.",
            "One is the fully observed variable S which we denote as insensitive or nuisance factor variation, and then there is also continuously variable Z that models all of the remaining information.",
            "Since you are using autoencoders with parameterise the generative model with parameters data as a neural network that takes as input ZNS and produces the data point X, and since inference tractable, we use another neural network as the inference model with parameters Phi and this act as the variational posterior.",
            "Now training both of Satan 5 can be done on a single objective function, and that's the lower bound of this graphical model that also the previous speakers explained.",
            "It nicely separates into two terms.",
            "The first is the expected log likelihood of our data under the late invariant distribution and the second one is a regularization term that forces the posterior to not diverge that much from the prior."
        ],
        [
            "However, if we are actually interested in doing some kind of supervised task afterwards, it makes sense to also try to correlate this invariant representation Z1.",
            "Now with our actual prediction tasks, since if we do not, we might actually end up with a useless kind of representation.",
            "So to target this effect, we also introduce again to separate source of variation for the environment.",
            "The 101 is a semi observed variable Y which basically is the class label for that particular data point.",
            "And then we also have a continuously variable Z2 that basically models all of the information is you want that cannot be explained by the class alone.",
            "So similarly as before, we have a neural network is the generative model and we have another neural network as the inference model.",
            "We assume that the posterior distribution over the other variables factorizes into the product of three independent posteriors.",
            "One is the posterior over the environment presentation as we call it, the one.",
            "Then we also have the predictive posterior that takes you one and provides a probability distribution over the labels.",
            "So that's the actual posterior we use at Test time to do predictions, and then we also have the position over the late in very busy too.",
            "Training as before can be done in a single objective function that nicely separates into a lower bound for the label data lower bound for the unlabeled data, plus one extra term that forces the actual predicted posterior to learn both from labeled and unlabeled data.",
            "So this model would be the note does VA in our experiments later on."
        ],
        [
            "Now, despite the fact that our modeling cards independence between between Z1 S Apriori as you can see here, some dependencies actually might still remain in the approximate marginal posteriors over Z1.",
            "So, for example, if the.",
            "Viable as it did not before hand, and the actual label.",
            "Why are correlated then the actual predicted posterior that we use might actually leak information into the posterior distribution over Z1, since that will actually increase the classification accuracy with respect to Y.",
            "So to tackle this, we just introduce a simple extra penalty term to the various not encoder and try to force the model to avoid information about this as much as possible.",
            "So."
        ],
        [
            "For this penalty, which shows the maximum discrepancy that basically measures the distance between two subsets, this distance is a squared L2 norm of averages of feature transformations on those sample sets, and DMV expands this norm and expresses it as a function of kernels and has been shown that for universal kernels, for example, the radial basis function, it is asymptotically zero if both sample sets are drawn from the same distribution.",
            "Now one."
        ],
        [
            "Problem with me is that it's not very scalable As for its minibots.",
            "During optimization we have to compute the kernel matrix that scales quadratically with the size of the minibots.",
            "So what we do is that we instead.",
            "Work with their prime on space before we expand the norm and we use random Kitchen seeks to obtain EPCI feature transformation such that the value we get from this primal space estimator is approximates.",
            "The value would have gotten if we use an RBF kernel.",
            "Now this case linearly with the size of the mini bots.",
            "So this feature in transformation can be quite simply understood.",
            "Does another stochastic neural network layer that takes the input and transforms according to a random matrix drawn from a standard normal distribution with adds random bias distributed uniformly on the circle and then applies the cosine and linearity?"
        ],
        [
            "So thus we arrive at the variational ferritin coder.",
            "We just add this independent variational auto encoder.",
            "We split the samples over the posterior over Z1 according to the state of S and we essentially treat those samples are samples from the marginal posteriors over Z1.",
            "And then we just minimize the squared L2 norm of these random kitchen sinks feature expansions.",
            "So now moving on to the."
        ],
        [
            "Parents are experimenting three tasks fair classification, domain adaptation and general feature learning tasks."
        ],
        [
            "First, a couple of words about the evaluation criteria.",
            "What we want is that from the new representation that we obtain, we should have.",
            "Below or ideally random chance accuracy with respect to the variable S, so this actually the nodes that we did not model any kind of information about this in our new representation.",
            "And Furthermore, it would sort of should also have high accuracy and why.",
            "Namely, we managed to keep all of the predicted information.",
            "Finally, for the fair classification, we also require that the one should not discriminate and by discrimination they mean that if we subsequently train a classifier on top of the one representation, the classifier should not provide bias decisions.",
            "And in the case for binary variable S, this bias decisions can be measured as the absolute value of the discrepancy of the decision given by the classifier.",
            "When this was zero and when this was one.",
            "So."
        ],
        [
            "For classification first, since you left, I'm just gonna explain 2 datasets.",
            "First.",
            "Data set is the adult data set.",
            "The actual label is to predict whether an account has more than $50,000 and the variable X is the gender of the individual.",
            "And then we also have the health data set whether we try to predict whether a given individual will be admitted to the hospital and the variable S now is age as a baseline.",
            "We use the learning fair representations."
        ],
        [
            "So as you can see here on the other data set on the left you see the accuracy on this.",
            "In the blue it's the accuracy obtained by the random forest classifier, and on the red color is the accuracy obtained by the logistical question.",
            "So as you can see on the original input SpaceX the accuracy and is quite high.",
            "But as soon as we use all of the other models, the at least the linear accuracy drops to random chance, and it seems that the learning fair representations did maintain some kind of information about this, since the random forest was able to have higher than random chance accuracy.",
            "However, as soon as we use either the variational auto encoder, the variational ferritin code are both linear and nonlinear accuracy stroke, random chance.",
            "Now, with respect to discrimination on this, we see that we have got high discrimination on the original data.",
            "SpaceX, it seems that the learning fair representations had the lowest kind of discrimination.",
            "However, the various not fair to encoder compared to the variational auto encoder seems to have also lower discrimination, so the MD panel is simply help there.",
            "Finally, with respect to the accuracy and why, it seems that more or less it was the same for all of the methods.",
            "Now moving on to the health data set.",
            "Similar picture here.",
            "Quite high accuracy on this and it seems that intriguingly, also the variational auto encoder did not manage to drop below random chance from the accuracy, even with a simple logistic regression classifier.",
            "Various not fair open code, on the other hand, did minus to decrease the linear accuracy.",
            "Moving on to the discrimination metrics, it seems again that the variational auto encoder maintains some kind of discrimination which was subsequently reduced by the variation of Farrah encoder.",
            "Finally, the accuracy and why was more or less the same again?",
            "Now."
        ],
        [
            "So more of a kind of a visual experiment to understand what actually happens in those representations we have here 2 dimensional embeddings of the first plot.",
            "It's of the original data X, and it's point is color coded according to the state of the variables.",
            "So as you can see, the two groups are highly identifiable in the original input space, and they're also highly identifiable on the second plot, which is basically a semi supervised variational autoencoder trained without any kind of factorization assumptions in the prior between CNS and without any.",
            "The penalty.",
            "Now, as soon as you use the factorization assumption, Sandy MMD penalty.",
            "It seems that the two groups become overlapping and sensually are.",
            "Removing the information about this."
        ],
        [
            "So now moving on the domain adaptation experiment."
        ],
        [
            "Experiment on a kind of benchmark data set for demand attention.",
            "the Amazon reviews data set.",
            "It's a sentiment analysis task, so the label wise predict whether we have a positive or a negative review.",
            "The variable X is the domain of the review and as a baseline user domain reseller networks."
        ],
        [
            "So as you can see, first with respect the accuracy on this, so that's the accuracy with respect the domain.",
            "It seems that our model really pushes the accuracy and S towards random chance, which is 0.5 for this data set and compared to the accuracy and why it's compared to the done architecture, I'll be there little better at some of the domains."
        ],
        [
            "Finally."
        ],
        [
            "The general Picture learning task.",
            "Their experiment with extent DLB data set.",
            "It consists of face images of 38 people under different lighting conditions.",
            "The actual label Y is the person ID and the variable X denotes the lighting condition of the photo as a baseline are used to hidden layer.",
            "Neural network within MMD penalty at the second layer."
        ],
        [
            "So as you can see here, our model similarly reduces the accuracy on this significantly, and Furthermore also increases the accuracy and Y and it's better than both training a class parallel.",
            "We originally in SpaceX and also in your letter within MMD penalty.",
            "So this can also be better understood by observing 2 dimensional embeddings.",
            "So here you can see the two dimensional embedding on the original images and you can see that this images are actually mostly classical according to the lighting condition.",
            "So the dominant factor variation in the data is that lighting condition.",
            "Now, as soon as we use our model and two dimensional embedding so the new representation, we see that this factor variation essentially removed and now the actual images are clustered according to the person ID.",
            "So the dominant factor variation now is the actual predictive information."
        ],
        [
            "So to conclude and provide some future work, it seems that the variational ferritin Coder provides better tradeoffs in actually predicting Y and obfuscating S. Incorporating DMD seems to help, and as a result of the model that's effective in fair classification, domain adaptation and invariant feature learning task.",
            "As for future work, since the elegant variational auto Encoder framework provides US access with all of these distributions, we could instead use a mutual kind of information metric to regularize the posterior.",
            "And Furthermore, an extension to a recommender system is also something that we plan to do weather Now the task would be to do recommendations that do not actually depend on some sensitive demographic information that you might have in your data."
        ],
        [
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone, I'm going to present the variational fare autoencoder, it's a joint work between the University of Amsterdam and the University of Toronto.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're diving into the model.",
                    "label": 0
                },
                {
                    "sent": "First, provide some motivation of why we actually even consider such a model.",
                    "label": 0
                },
                {
                    "sent": "Let's consider some sample tasks.",
                    "label": 0
                },
                {
                    "sent": "Assume that we want to identify the idea of the person in a data set that looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's quite hard, since actually there is a lot of noise.",
                    "label": 1
                },
                {
                    "sent": "In this case, the lighting conditions that obfuscate the real predictive information that our models capture.",
                    "label": 0
                },
                {
                    "sent": "Now some similar tasks you might consider that we have an algorithm that tries to detect if someone is a possible.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Suspect in a police investigation just by looking from photos now you might be constrained from the law to actually not take into.",
                    "label": 0
                },
                {
                    "sent": "Like on some sensitive information that you might have available for your suspects, for example race and gender.",
                    "label": 1
                },
                {
                    "sent": "Now another similar kind of application can be to detect from MRI images whether a given patient has eyes heimer.",
                    "label": 1
                },
                {
                    "sent": "Usually you have MRI images from multiple machines, and you know from before hand that all of this machine related variation that is available in the data is not really productive for the ice climber disease.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So tackling tackling such problems is not really that much trivial, since we can just exclude this particular bits from our inputs, as all of the other dimensions still contain information about those particular bits.",
                    "label": 1
                },
                {
                    "sent": "So ideally what you want to do is to transform the data to a new representation.",
                    "label": 0
                },
                {
                    "sent": "There now we can explicitly accord its properties and concerning the aforementioned problems we can form for some sort of invariants with respect to summer priority.",
                    "label": 0
                },
                {
                    "sent": "Known information.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're not the first have actually consider such tasks.",
                    "label": 0
                },
                {
                    "sent": "The first kind of work that dealt with fair representations and essentially fair representations are representations that try to enforce some kind of invariants with respect to sensitive information.",
                    "label": 0
                },
                {
                    "sent": "It was a simple, discriminative clustering approach.",
                    "label": 1
                },
                {
                    "sent": "Then there are also your network methods that try to match feature distributions among different groups.",
                    "label": 1
                },
                {
                    "sent": "There's a whole line of works with neural networks that use the maximum discrepancy penalty more independently later.",
                    "label": 0
                },
                {
                    "sent": "Since you also use it.",
                    "label": 0
                },
                {
                    "sent": "And there are social works that formulate this problem as a minimax kind of optimization, where the network tries to produce representations such that an adversary cannot distinguish the group members.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our contribution to this type of problems is the variational fair auto encoder which the generative model where some a priority known target factors variation are explicitly removed.",
                    "label": 1
                },
                {
                    "sent": "As a result, we were able to obtain a new representation that is invariant with respect to this information and as well, so later in the experiments we achieve better performance on the fair classification domain adaptation at general feature learning task.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's start simple and consider the first day and supervised version of this model can also be understood as a fully observed M2 model from the saving supervised variational auto encoder.",
                    "label": 0
                },
                {
                    "sent": "For those of you that you're familiar with this work, we can assume that our data point X is the result of three dependent factors of variation.",
                    "label": 1
                },
                {
                    "sent": "One is the fully observed variable S which we denote as insensitive or nuisance factor variation, and then there is also continuously variable Z that models all of the remaining information.",
                    "label": 1
                },
                {
                    "sent": "Since you are using autoencoders with parameterise the generative model with parameters data as a neural network that takes as input ZNS and produces the data point X, and since inference tractable, we use another neural network as the inference model with parameters Phi and this act as the variational posterior.",
                    "label": 1
                },
                {
                    "sent": "Now training both of Satan 5 can be done on a single objective function, and that's the lower bound of this graphical model that also the previous speakers explained.",
                    "label": 0
                },
                {
                    "sent": "It nicely separates into two terms.",
                    "label": 0
                },
                {
                    "sent": "The first is the expected log likelihood of our data under the late invariant distribution and the second one is a regularization term that forces the posterior to not diverge that much from the prior.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, if we are actually interested in doing some kind of supervised task afterwards, it makes sense to also try to correlate this invariant representation Z1.",
                    "label": 0
                },
                {
                    "sent": "Now with our actual prediction tasks, since if we do not, we might actually end up with a useless kind of representation.",
                    "label": 0
                },
                {
                    "sent": "So to target this effect, we also introduce again to separate source of variation for the environment.",
                    "label": 0
                },
                {
                    "sent": "The 101 is a semi observed variable Y which basically is the class label for that particular data point.",
                    "label": 0
                },
                {
                    "sent": "And then we also have a continuously variable Z2 that basically models all of the information is you want that cannot be explained by the class alone.",
                    "label": 0
                },
                {
                    "sent": "So similarly as before, we have a neural network is the generative model and we have another neural network as the inference model.",
                    "label": 1
                },
                {
                    "sent": "We assume that the posterior distribution over the other variables factorizes into the product of three independent posteriors.",
                    "label": 0
                },
                {
                    "sent": "One is the posterior over the environment presentation as we call it, the one.",
                    "label": 0
                },
                {
                    "sent": "Then we also have the predictive posterior that takes you one and provides a probability distribution over the labels.",
                    "label": 0
                },
                {
                    "sent": "So that's the actual posterior we use at Test time to do predictions, and then we also have the position over the late in very busy too.",
                    "label": 0
                },
                {
                    "sent": "Training as before can be done in a single objective function that nicely separates into a lower bound for the label data lower bound for the unlabeled data, plus one extra term that forces the actual predicted posterior to learn both from labeled and unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So this model would be the note does VA in our experiments later on.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, despite the fact that our modeling cards independence between between Z1 S Apriori as you can see here, some dependencies actually might still remain in the approximate marginal posteriors over Z1.",
                    "label": 1
                },
                {
                    "sent": "So, for example, if the.",
                    "label": 0
                },
                {
                    "sent": "Viable as it did not before hand, and the actual label.",
                    "label": 0
                },
                {
                    "sent": "Why are correlated then the actual predicted posterior that we use might actually leak information into the posterior distribution over Z1, since that will actually increase the classification accuracy with respect to Y.",
                    "label": 1
                },
                {
                    "sent": "So to tackle this, we just introduce a simple extra penalty term to the various not encoder and try to force the model to avoid information about this as much as possible.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this penalty, which shows the maximum discrepancy that basically measures the distance between two subsets, this distance is a squared L2 norm of averages of feature transformations on those sample sets, and DMV expands this norm and expresses it as a function of kernels and has been shown that for universal kernels, for example, the radial basis function, it is asymptotically zero if both sample sets are drawn from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "Now one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem with me is that it's not very scalable As for its minibots.",
                    "label": 0
                },
                {
                    "sent": "During optimization we have to compute the kernel matrix that scales quadratically with the size of the minibots.",
                    "label": 1
                },
                {
                    "sent": "So what we do is that we instead.",
                    "label": 1
                },
                {
                    "sent": "Work with their prime on space before we expand the norm and we use random Kitchen seeks to obtain EPCI feature transformation such that the value we get from this primal space estimator is approximates.",
                    "label": 0
                },
                {
                    "sent": "The value would have gotten if we use an RBF kernel.",
                    "label": 1
                },
                {
                    "sent": "Now this case linearly with the size of the mini bots.",
                    "label": 0
                },
                {
                    "sent": "So this feature in transformation can be quite simply understood.",
                    "label": 0
                },
                {
                    "sent": "Does another stochastic neural network layer that takes the input and transforms according to a random matrix drawn from a standard normal distribution with adds random bias distributed uniformly on the circle and then applies the cosine and linearity?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thus we arrive at the variational ferritin coder.",
                    "label": 0
                },
                {
                    "sent": "We just add this independent variational auto encoder.",
                    "label": 0
                },
                {
                    "sent": "We split the samples over the posterior over Z1 according to the state of S and we essentially treat those samples are samples from the marginal posteriors over Z1.",
                    "label": 1
                },
                {
                    "sent": "And then we just minimize the squared L2 norm of these random kitchen sinks feature expansions.",
                    "label": 0
                },
                {
                    "sent": "So now moving on to the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parents are experimenting three tasks fair classification, domain adaptation and general feature learning tasks.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, a couple of words about the evaluation criteria.",
                    "label": 1
                },
                {
                    "sent": "What we want is that from the new representation that we obtain, we should have.",
                    "label": 1
                },
                {
                    "sent": "Below or ideally random chance accuracy with respect to the variable S, so this actually the nodes that we did not model any kind of information about this in our new representation.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, it would sort of should also have high accuracy and why.",
                    "label": 1
                },
                {
                    "sent": "Namely, we managed to keep all of the predicted information.",
                    "label": 0
                },
                {
                    "sent": "Finally, for the fair classification, we also require that the one should not discriminate and by discrimination they mean that if we subsequently train a classifier on top of the one representation, the classifier should not provide bias decisions.",
                    "label": 0
                },
                {
                    "sent": "And in the case for binary variable S, this bias decisions can be measured as the absolute value of the discrepancy of the decision given by the classifier.",
                    "label": 0
                },
                {
                    "sent": "When this was zero and when this was one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For classification first, since you left, I'm just gonna explain 2 datasets.",
                    "label": 0
                },
                {
                    "sent": "First.",
                    "label": 0
                },
                {
                    "sent": "Data set is the adult data set.",
                    "label": 0
                },
                {
                    "sent": "The actual label is to predict whether an account has more than $50,000 and the variable X is the gender of the individual.",
                    "label": 0
                },
                {
                    "sent": "And then we also have the health data set whether we try to predict whether a given individual will be admitted to the hospital and the variable S now is age as a baseline.",
                    "label": 0
                },
                {
                    "sent": "We use the learning fair representations.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as you can see here on the other data set on the left you see the accuracy on this.",
                    "label": 0
                },
                {
                    "sent": "In the blue it's the accuracy obtained by the random forest classifier, and on the red color is the accuracy obtained by the logistical question.",
                    "label": 0
                },
                {
                    "sent": "So as you can see on the original input SpaceX the accuracy and is quite high.",
                    "label": 0
                },
                {
                    "sent": "But as soon as we use all of the other models, the at least the linear accuracy drops to random chance, and it seems that the learning fair representations did maintain some kind of information about this, since the random forest was able to have higher than random chance accuracy.",
                    "label": 0
                },
                {
                    "sent": "However, as soon as we use either the variational auto encoder, the variational ferritin code are both linear and nonlinear accuracy stroke, random chance.",
                    "label": 0
                },
                {
                    "sent": "Now, with respect to discrimination on this, we see that we have got high discrimination on the original data.",
                    "label": 0
                },
                {
                    "sent": "SpaceX, it seems that the learning fair representations had the lowest kind of discrimination.",
                    "label": 0
                },
                {
                    "sent": "However, the various not fair to encoder compared to the variational auto encoder seems to have also lower discrimination, so the MD panel is simply help there.",
                    "label": 0
                },
                {
                    "sent": "Finally, with respect to the accuracy and why, it seems that more or less it was the same for all of the methods.",
                    "label": 0
                },
                {
                    "sent": "Now moving on to the health data set.",
                    "label": 0
                },
                {
                    "sent": "Similar picture here.",
                    "label": 0
                },
                {
                    "sent": "Quite high accuracy on this and it seems that intriguingly, also the variational auto encoder did not manage to drop below random chance from the accuracy, even with a simple logistic regression classifier.",
                    "label": 0
                },
                {
                    "sent": "Various not fair open code, on the other hand, did minus to decrease the linear accuracy.",
                    "label": 0
                },
                {
                    "sent": "Moving on to the discrimination metrics, it seems again that the variational auto encoder maintains some kind of discrimination which was subsequently reduced by the variation of Farrah encoder.",
                    "label": 0
                },
                {
                    "sent": "Finally, the accuracy and why was more or less the same again?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So more of a kind of a visual experiment to understand what actually happens in those representations we have here 2 dimensional embeddings of the first plot.",
                    "label": 0
                },
                {
                    "sent": "It's of the original data X, and it's point is color coded according to the state of the variables.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, the two groups are highly identifiable in the original input space, and they're also highly identifiable on the second plot, which is basically a semi supervised variational autoencoder trained without any kind of factorization assumptions in the prior between CNS and without any.",
                    "label": 0
                },
                {
                    "sent": "The penalty.",
                    "label": 0
                },
                {
                    "sent": "Now, as soon as you use the factorization assumption, Sandy MMD penalty.",
                    "label": 0
                },
                {
                    "sent": "It seems that the two groups become overlapping and sensually are.",
                    "label": 0
                },
                {
                    "sent": "Removing the information about this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now moving on the domain adaptation experiment.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiment on a kind of benchmark data set for demand attention.",
                    "label": 0
                },
                {
                    "sent": "the Amazon reviews data set.",
                    "label": 1
                },
                {
                    "sent": "It's a sentiment analysis task, so the label wise predict whether we have a positive or a negative review.",
                    "label": 0
                },
                {
                    "sent": "The variable X is the domain of the review and as a baseline user domain reseller networks.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as you can see, first with respect the accuracy on this, so that's the accuracy with respect the domain.",
                    "label": 0
                },
                {
                    "sent": "It seems that our model really pushes the accuracy and S towards random chance, which is 0.5 for this data set and compared to the accuracy and why it's compared to the done architecture, I'll be there little better at some of the domains.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The general Picture learning task.",
                    "label": 0
                },
                {
                    "sent": "Their experiment with extent DLB data set.",
                    "label": 0
                },
                {
                    "sent": "It consists of face images of 38 people under different lighting conditions.",
                    "label": 1
                },
                {
                    "sent": "The actual label Y is the person ID and the variable X denotes the lighting condition of the photo as a baseline are used to hidden layer.",
                    "label": 0
                },
                {
                    "sent": "Neural network within MMD penalty at the second layer.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as you can see here, our model similarly reduces the accuracy on this significantly, and Furthermore also increases the accuracy and Y and it's better than both training a class parallel.",
                    "label": 0
                },
                {
                    "sent": "We originally in SpaceX and also in your letter within MMD penalty.",
                    "label": 0
                },
                {
                    "sent": "So this can also be better understood by observing 2 dimensional embeddings.",
                    "label": 0
                },
                {
                    "sent": "So here you can see the two dimensional embedding on the original images and you can see that this images are actually mostly classical according to the lighting condition.",
                    "label": 0
                },
                {
                    "sent": "So the dominant factor variation in the data is that lighting condition.",
                    "label": 0
                },
                {
                    "sent": "Now, as soon as we use our model and two dimensional embedding so the new representation, we see that this factor variation essentially removed and now the actual images are clustered according to the person ID.",
                    "label": 0
                },
                {
                    "sent": "So the dominant factor variation now is the actual predictive information.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude and provide some future work, it seems that the variational ferritin Coder provides better tradeoffs in actually predicting Y and obfuscating S. Incorporating DMD seems to help, and as a result of the model that's effective in fair classification, domain adaptation and invariant feature learning task.",
                    "label": 1
                },
                {
                    "sent": "As for future work, since the elegant variational auto Encoder framework provides US access with all of these distributions, we could instead use a mutual kind of information metric to regularize the posterior.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, an extension to a recommender system is also something that we plan to do weather Now the task would be to do recommendations that do not actually depend on some sensitive demographic information that you might have in your data.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}