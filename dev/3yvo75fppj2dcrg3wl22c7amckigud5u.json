{
    "id": "3yvo75fppj2dcrg3wl22c7amckigud5u",
    "title": "On-Line Learning Algorithms for Path Experts with Non-Additive Losses",
    "info": {
        "author": [
            "Vitaly Kuznetsov, Courant Institute of Mathematical Sciences, New York University (NYU)"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_kuznetsov_path_experts/",
    "segmentation": [
        [
            "Thank you so indeed today I'm presenting joint work with Carina Cortez, Mirror Mori and Manfred Warmuth on online learning algorithms for path experts with nonadditive flosses.",
            "Prediction with."
        ],
        [
            "Expert advice is a standard paradigm in online learning.",
            "Over the years it has been studied extensively and by now admits a number of well known algorithmic solutions, and these include follow the perturb leader randomized weighted majority and hedge algorithms, and by the way, I will treat the last two as being essentially the same thing for the purposes of this talk, these algorithms have favorable regret guarantees that scale logarithmically with the number of.",
            "Experts suggesting that learning should still be possible even when the number of experts is large.",
            "Of course, the price to pay for this is in computational complexity, since typically it scales linearly with the number of experts.",
            "However, for certain problems."
        ],
        [
            "Admit a suitable structure.",
            "Efficient implementations of these algorithms have been proposed.",
            "And a particular example of such problem is so called online shortest path problem.",
            "In this problem we are given a directed graph like the one on the left and each path in this graph is treated as an expert and each path from an initial state to the final state.",
            "And I denote the initial state by a Bolt circle and final state by double circles at each round.",
            "These experts make their predictions.",
            "And we encode their predictions using the automaton in the Center for instance, expert represented by Vertis is 0234 is making a prediction BAA at round T and jointly we can represent experts and predictions using the transducer on the right.",
            "Once the predictions are made, adversary announces the correct label and each of the experts incurs a loss, and the crucial assumption for efficient implementations of the existing online learning algorithms is that the loss function is additive.",
            "In other words, a loss of an expert is the sum of the losses at each edge of that path.",
            "How?"
        ],
        [
            "Over.",
            "In key applications of machine learning, natural loss functions turn out to be non additive.",
            "In other words, it's not possible to express the loss of an expert as the sum of the losses at each edge, because the notion of the loss of an edge may not even make sense and at least just a few of the many such applications on this slide.",
            "So Edit distance is used in speech recognition and natural language processing, so called Bleu score is used in machine translation, and it is based on.",
            "Ngram counts for two prediction sequences, various other similarity measures based on ngram.",
            "Counts are used in computational biology.",
            "None of the existing online learning algorithms is tractable for the problems with nonadditive losses, so it is natural to ask whether efficient online learning is possible in this kind of settings, and it is precisely this question that we address in our work."
        ],
        [
            "So in the rest of this talk, I'm going to introduce to you 2 broad families of loss functions, rational losses and tropical losses, and these two broad families cover all of the applications that they have just mentioned.",
            "Next I'm going to present an extension of FPL algorithm to rational losses.",
            "In the paper will also give extension of FPL to tropical losses as well as extensions of randomized weighted majority to both of these families.",
            "But before I can even start talking about losses and algorithms, I actually need to give you a crash course on weighted automata and weighted transducers, since these are in fact used to define both the loss functions and the algorithms.",
            "And the first step for me is going to be to tell you what the weights are in weighted automaton transducers."
        ],
        [
            "So the weights are coming from an algebraic structure which is called a semiring.",
            "A simmering is simply a set equipped with two binary operations of addition and multiplication, which make this set into a ring.",
            "But this ring is allowed to lack negation, and it is going to be useful for us to keep a few examples of semirings in mind.",
            "So the simplest example of the semiring is Boolean semiring, which is simply a binary set equipped.",
            "With the standard Boolean algebra operations.",
            "Another familiar example is probability simmering, which is a set of non negative real numbers equipped with the standard operations of reals of addition and multiplication.",
            "And more interesting, and the more familiar example, a more relevant example for us is the log simmering in the log, simmering the standard addition of reals plays the role of times operation and soft mean plays the role of plus operation.",
            "And another relevant simmering for us is going to be a tropical semiring, where again, the standard addition of reals plays the role of times operation and mean plays the role of class separation.",
            "And whenever I say addition or multiplication and the rest of this talk, I always mean addition or multiplication of the semiring in the given context.",
            "So what are?"
        ],
        [
            "Are now weighted automaton transducers?",
            "I waited, automata can simply be defined as an automaton whose edges are augmented with weights from a particular semiring.",
            "And awaited transducer is a weighted automaton whose edges are in addition augmented with symbols of output alphabet.",
            "And we can use weighted transducers and weighted automaton to assign weights.",
            "Two pairs of strings X&Y and the way we're going to do it is by defining an accepting path to be a path from initial state of the transducer to the final state of the transducer, and we're going to say that the weight of such path is the product of the weights along this path.",
            "And then we will say that weight assigned to a pair of strings X&Y is simply the sum of all the weights of the accepting paths labeled with despair.",
            "So for instance, an example of an accepting path for a pair, a BA and second string.",
            "BBA is a path corresponding toward.",
            "This is 01 and three and three again.",
            "So the weight of that path is going to be 0.1 zero point 4.",
            "* 0.6 * 0.7 and if we sum over all the accepting path with these labels.",
            "Will get the weight of that pair.",
            "We will also require two operations for weighted automata and transducers to define our loss functions and the first operation that we're going to need."
        ],
        [
            "Is that of a composition composition is mathematically defined as a transducer that assigns the weight to the Paris X&Y according to the formula at the bottom of this slide, and what we do we look at the way that T1 assigns to the pair XZ, and we look at the way the T2 assigns to the pair, that why we multiply this weights and then we sum over all these strings that.",
            "Turns out there is an efficient algorithm for computing compositions, and I give an example of this algorithm on this slide.",
            "I'm gonna skip the details of the algorithm due to the time constraints.",
            "This the second operation that we're going to need is that of the."
        ],
        [
            "Decomposition, sorry, determinization and.",
            "And automaton is set to be deterministic if No2 transitions leaving the same state shared the same label.",
            "So for instance, an automaton at the top of this slide is not deterministic, because there are two transitions living initial state that are labeled with a. Determinization is an extension of the classical subset construction in automata theory to the weighted case and given an automaton aided the determinization algorithm returns an equivalent deterministic accountant.",
            "So now we are experts in the material that we need from weighted automata and transducers theory, so we're ready to define our loss functions and algorithms."
        ],
        [
            "So the definition of rational losses is inspired by so-called rational kernels that are widely used in NLP and computational biology.",
            "Any transducer you over a probability semiring with matching input and output alphabets defines a rational kernel.",
            "And given such transducer you we define rational loss to be negative log of you, and we're going to denote the transducer corresponding to that as you tilled.",
            "So on the left side of this slide I give an example how to construct rational kernel and corresponding rational loss, we can take bigram transducer T. Which can be used to compute number of occurrences of any bigram in any given string, and we can compose this transducer T with its own inverse, which is the same transducer, but with input and output labels flipped.",
            "And if we do so, then we obtain.",
            "Transducer U which is essentially a blue score transducer based on bigram, counts.",
            "It is a rational kernel and it can be used to define a corresponding rational loss.",
            "The definition of tropical losses is inspired by the fact that any generalized edit distance can be computed using flower automaton that you see on the right of this slide.",
            "The top transition corresponds to substitutions.",
            "The left transition corresponds to emissions and insertions, and the right transition corresponds to no cost for.",
            "Correct labels, and of course, since this is a flower automaton the same the relevant semiring here is going to be tropical one.",
            "And given any transducer you over at tropical semiring with non negative French and matching input and output alphabets, we're going to define a tropical loss to simply be you."
        ],
        [
            "So I promised you an extension of the FPL algorithm to rational losses.",
            "So how can this be done in our context?",
            "And if you remember FPL algorithm, you know that each round what you need to do is compute cumulative losses.",
            "For each of the experts which we have exponentially, many then perturb these losses with noise and select the best perturbed expert.",
            "So how can this be done in our context?",
            "Well."
        ],
        [
            "So the key idea is to 1st use composition to simultaneously compute the losses of all of the experts, in particular, riforma composition of YT utility sub T, where YT is an automaton representing correct label.",
            "You~ is irrational loss and T sub tease expert predictions.",
            "And once we do that we can use composition again to perturb this losses.",
            "And this time we're going to be composing with and noise automaton which has the same topology as the expert graph, but the weights.",
            "Now our Laplacian random variables.",
            "Next we would like to be able to select the best perturbed expert.",
            "But the difficulty here is that now in the composition, multiple paths may be labeled with the same expert, making it hard to find the best one.",
            "And the solution now is to use determinization after the determinization weight of any path can be determined only only using times operation.",
            "So we can use a shortest path algorithm to find the best expert.",
            "So on the."
        ],
        [
            "Next slide they have the pseudocode for this algorithm, but I think in the interest of time I'm not going to go in details over this pseudocode and instead I'll answer the key question.",
            "What is the computational complexity of this algorithm?",
            "And I already said that composition has an efficient implementation and bad news here is that."
        ],
        [
            "However, the worst case complexity of the determinization algorithm that is required for for us is exponential.",
            "But the good news is that we are able to show that in our context, determinization will have polynomial time complexity and we're using GNU string combinatorics arguments for this.",
            "Again, I'm not going to go over these arguments in this talk, but instead I'm going to move on to something probably more familiar to."
        ],
        [
            "This audiences regret guarantees.",
            "Standard regret guarantees do not apply to our FPL algorithm because of the nature of the losses that we're dealing with here.",
            "But instead we prove our own regret guarantees which match almost exactly the standard regret guarantees of the additive losses.",
            "In fact, syntactically, the bound looks exactly the same, and the only difference is in the definition of the terms that appear in the bound, as in the case of additive losses.",
            "L sub mean is the loss of the best expert.",
            "Ann is the number of edges in the expert automaton.",
            "Diameter of the set of experts, and in the case of edit, if loss is a, is the largest possible loss of an expert, and in our case this term also depends on the topology of the expert graph.",
            "So."
        ],
        [
            "In the conclusion we presented a new framework for online learning that significantly broadens its applicability.",
            "We give extensions of FPL and randomized weighted majority algorithms to rational losses, and remarkably extensions of these algorithms to tropical losses only require a change of the semiring.",
            "So wherever I use log simmering for rational losses, I replace it with tropical semiring and this will give me an algorithm for tropical losses.",
            "Finally, our algorithms admit the straightforward implementation using open source software tools such as Open FST library.",
            "That concludes my presentation, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you so indeed today I'm presenting joint work with Carina Cortez, Mirror Mori and Manfred Warmuth on online learning algorithms for path experts with nonadditive flosses.",
                    "label": 0
                },
                {
                    "sent": "Prediction with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Expert advice is a standard paradigm in online learning.",
                    "label": 1
                },
                {
                    "sent": "Over the years it has been studied extensively and by now admits a number of well known algorithmic solutions, and these include follow the perturb leader randomized weighted majority and hedge algorithms, and by the way, I will treat the last two as being essentially the same thing for the purposes of this talk, these algorithms have favorable regret guarantees that scale logarithmically with the number of.",
                    "label": 0
                },
                {
                    "sent": "Experts suggesting that learning should still be possible even when the number of experts is large.",
                    "label": 0
                },
                {
                    "sent": "Of course, the price to pay for this is in computational complexity, since typically it scales linearly with the number of experts.",
                    "label": 0
                },
                {
                    "sent": "However, for certain problems.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Admit a suitable structure.",
                    "label": 0
                },
                {
                    "sent": "Efficient implementations of these algorithms have been proposed.",
                    "label": 0
                },
                {
                    "sent": "And a particular example of such problem is so called online shortest path problem.",
                    "label": 0
                },
                {
                    "sent": "In this problem we are given a directed graph like the one on the left and each path in this graph is treated as an expert and each path from an initial state to the final state.",
                    "label": 0
                },
                {
                    "sent": "And I denote the initial state by a Bolt circle and final state by double circles at each round.",
                    "label": 0
                },
                {
                    "sent": "These experts make their predictions.",
                    "label": 0
                },
                {
                    "sent": "And we encode their predictions using the automaton in the Center for instance, expert represented by Vertis is 0234 is making a prediction BAA at round T and jointly we can represent experts and predictions using the transducer on the right.",
                    "label": 0
                },
                {
                    "sent": "Once the predictions are made, adversary announces the correct label and each of the experts incurs a loss, and the crucial assumption for efficient implementations of the existing online learning algorithms is that the loss function is additive.",
                    "label": 0
                },
                {
                    "sent": "In other words, a loss of an expert is the sum of the losses at each edge of that path.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over.",
                    "label": 0
                },
                {
                    "sent": "In key applications of machine learning, natural loss functions turn out to be non additive.",
                    "label": 0
                },
                {
                    "sent": "In other words, it's not possible to express the loss of an expert as the sum of the losses at each edge, because the notion of the loss of an edge may not even make sense and at least just a few of the many such applications on this slide.",
                    "label": 0
                },
                {
                    "sent": "So Edit distance is used in speech recognition and natural language processing, so called Bleu score is used in machine translation, and it is based on.",
                    "label": 1
                },
                {
                    "sent": "Ngram counts for two prediction sequences, various other similarity measures based on ngram.",
                    "label": 0
                },
                {
                    "sent": "Counts are used in computational biology.",
                    "label": 0
                },
                {
                    "sent": "None of the existing online learning algorithms is tractable for the problems with nonadditive losses, so it is natural to ask whether efficient online learning is possible in this kind of settings, and it is precisely this question that we address in our work.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the rest of this talk, I'm going to introduce to you 2 broad families of loss functions, rational losses and tropical losses, and these two broad families cover all of the applications that they have just mentioned.",
                    "label": 1
                },
                {
                    "sent": "Next I'm going to present an extension of FPL algorithm to rational losses.",
                    "label": 0
                },
                {
                    "sent": "In the paper will also give extension of FPL to tropical losses as well as extensions of randomized weighted majority to both of these families.",
                    "label": 0
                },
                {
                    "sent": "But before I can even start talking about losses and algorithms, I actually need to give you a crash course on weighted automata and weighted transducers, since these are in fact used to define both the loss functions and the algorithms.",
                    "label": 0
                },
                {
                    "sent": "And the first step for me is going to be to tell you what the weights are in weighted automaton transducers.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the weights are coming from an algebraic structure which is called a semiring.",
                    "label": 1
                },
                {
                    "sent": "A simmering is simply a set equipped with two binary operations of addition and multiplication, which make this set into a ring.",
                    "label": 0
                },
                {
                    "sent": "But this ring is allowed to lack negation, and it is going to be useful for us to keep a few examples of semirings in mind.",
                    "label": 1
                },
                {
                    "sent": "So the simplest example of the semiring is Boolean semiring, which is simply a binary set equipped.",
                    "label": 0
                },
                {
                    "sent": "With the standard Boolean algebra operations.",
                    "label": 0
                },
                {
                    "sent": "Another familiar example is probability simmering, which is a set of non negative real numbers equipped with the standard operations of reals of addition and multiplication.",
                    "label": 0
                },
                {
                    "sent": "And more interesting, and the more familiar example, a more relevant example for us is the log simmering in the log, simmering the standard addition of reals plays the role of times operation and soft mean plays the role of plus operation.",
                    "label": 0
                },
                {
                    "sent": "And another relevant simmering for us is going to be a tropical semiring, where again, the standard addition of reals plays the role of times operation and mean plays the role of class separation.",
                    "label": 0
                },
                {
                    "sent": "And whenever I say addition or multiplication and the rest of this talk, I always mean addition or multiplication of the semiring in the given context.",
                    "label": 0
                },
                {
                    "sent": "So what are?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are now weighted automaton transducers?",
                    "label": 0
                },
                {
                    "sent": "I waited, automata can simply be defined as an automaton whose edges are augmented with weights from a particular semiring.",
                    "label": 0
                },
                {
                    "sent": "And awaited transducer is a weighted automaton whose edges are in addition augmented with symbols of output alphabet.",
                    "label": 0
                },
                {
                    "sent": "And we can use weighted transducers and weighted automaton to assign weights.",
                    "label": 0
                },
                {
                    "sent": "Two pairs of strings X&Y and the way we're going to do it is by defining an accepting path to be a path from initial state of the transducer to the final state of the transducer, and we're going to say that the weight of such path is the product of the weights along this path.",
                    "label": 0
                },
                {
                    "sent": "And then we will say that weight assigned to a pair of strings X&Y is simply the sum of all the weights of the accepting paths labeled with despair.",
                    "label": 0
                },
                {
                    "sent": "So for instance, an example of an accepting path for a pair, a BA and second string.",
                    "label": 0
                },
                {
                    "sent": "BBA is a path corresponding toward.",
                    "label": 0
                },
                {
                    "sent": "This is 01 and three and three again.",
                    "label": 0
                },
                {
                    "sent": "So the weight of that path is going to be 0.1 zero point 4.",
                    "label": 0
                },
                {
                    "sent": "* 0.6 * 0.7 and if we sum over all the accepting path with these labels.",
                    "label": 0
                },
                {
                    "sent": "Will get the weight of that pair.",
                    "label": 0
                },
                {
                    "sent": "We will also require two operations for weighted automata and transducers to define our loss functions and the first operation that we're going to need.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that of a composition composition is mathematically defined as a transducer that assigns the weight to the Paris X&Y according to the formula at the bottom of this slide, and what we do we look at the way that T1 assigns to the pair XZ, and we look at the way the T2 assigns to the pair, that why we multiply this weights and then we sum over all these strings that.",
                    "label": 0
                },
                {
                    "sent": "Turns out there is an efficient algorithm for computing compositions, and I give an example of this algorithm on this slide.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna skip the details of the algorithm due to the time constraints.",
                    "label": 0
                },
                {
                    "sent": "This the second operation that we're going to need is that of the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Decomposition, sorry, determinization and.",
                    "label": 0
                },
                {
                    "sent": "And automaton is set to be deterministic if No2 transitions leaving the same state shared the same label.",
                    "label": 0
                },
                {
                    "sent": "So for instance, an automaton at the top of this slide is not deterministic, because there are two transitions living initial state that are labeled with a. Determinization is an extension of the classical subset construction in automata theory to the weighted case and given an automaton aided the determinization algorithm returns an equivalent deterministic accountant.",
                    "label": 1
                },
                {
                    "sent": "So now we are experts in the material that we need from weighted automata and transducers theory, so we're ready to define our loss functions and algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the definition of rational losses is inspired by so-called rational kernels that are widely used in NLP and computational biology.",
                    "label": 0
                },
                {
                    "sent": "Any transducer you over a probability semiring with matching input and output alphabets defines a rational kernel.",
                    "label": 0
                },
                {
                    "sent": "And given such transducer you we define rational loss to be negative log of you, and we're going to denote the transducer corresponding to that as you tilled.",
                    "label": 0
                },
                {
                    "sent": "So on the left side of this slide I give an example how to construct rational kernel and corresponding rational loss, we can take bigram transducer T. Which can be used to compute number of occurrences of any bigram in any given string, and we can compose this transducer T with its own inverse, which is the same transducer, but with input and output labels flipped.",
                    "label": 0
                },
                {
                    "sent": "And if we do so, then we obtain.",
                    "label": 0
                },
                {
                    "sent": "Transducer U which is essentially a blue score transducer based on bigram, counts.",
                    "label": 0
                },
                {
                    "sent": "It is a rational kernel and it can be used to define a corresponding rational loss.",
                    "label": 0
                },
                {
                    "sent": "The definition of tropical losses is inspired by the fact that any generalized edit distance can be computed using flower automaton that you see on the right of this slide.",
                    "label": 0
                },
                {
                    "sent": "The top transition corresponds to substitutions.",
                    "label": 0
                },
                {
                    "sent": "The left transition corresponds to emissions and insertions, and the right transition corresponds to no cost for.",
                    "label": 0
                },
                {
                    "sent": "Correct labels, and of course, since this is a flower automaton the same the relevant semiring here is going to be tropical one.",
                    "label": 0
                },
                {
                    "sent": "And given any transducer you over at tropical semiring with non negative French and matching input and output alphabets, we're going to define a tropical loss to simply be you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I promised you an extension of the FPL algorithm to rational losses.",
                    "label": 1
                },
                {
                    "sent": "So how can this be done in our context?",
                    "label": 1
                },
                {
                    "sent": "And if you remember FPL algorithm, you know that each round what you need to do is compute cumulative losses.",
                    "label": 0
                },
                {
                    "sent": "For each of the experts which we have exponentially, many then perturb these losses with noise and select the best perturbed expert.",
                    "label": 0
                },
                {
                    "sent": "So how can this be done in our context?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the key idea is to 1st use composition to simultaneously compute the losses of all of the experts, in particular, riforma composition of YT utility sub T, where YT is an automaton representing correct label.",
                    "label": 1
                },
                {
                    "sent": "You~ is irrational loss and T sub tease expert predictions.",
                    "label": 0
                },
                {
                    "sent": "And once we do that we can use composition again to perturb this losses.",
                    "label": 1
                },
                {
                    "sent": "And this time we're going to be composing with and noise automaton which has the same topology as the expert graph, but the weights.",
                    "label": 0
                },
                {
                    "sent": "Now our Laplacian random variables.",
                    "label": 0
                },
                {
                    "sent": "Next we would like to be able to select the best perturbed expert.",
                    "label": 0
                },
                {
                    "sent": "But the difficulty here is that now in the composition, multiple paths may be labeled with the same expert, making it hard to find the best one.",
                    "label": 0
                },
                {
                    "sent": "And the solution now is to use determinization after the determinization weight of any path can be determined only only using times operation.",
                    "label": 0
                },
                {
                    "sent": "So we can use a shortest path algorithm to find the best expert.",
                    "label": 1
                },
                {
                    "sent": "So on the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next slide they have the pseudocode for this algorithm, but I think in the interest of time I'm not going to go in details over this pseudocode and instead I'll answer the key question.",
                    "label": 0
                },
                {
                    "sent": "What is the computational complexity of this algorithm?",
                    "label": 0
                },
                {
                    "sent": "And I already said that composition has an efficient implementation and bad news here is that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, the worst case complexity of the determinization algorithm that is required for for us is exponential.",
                    "label": 1
                },
                {
                    "sent": "But the good news is that we are able to show that in our context, determinization will have polynomial time complexity and we're using GNU string combinatorics arguments for this.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm not going to go over these arguments in this talk, but instead I'm going to move on to something probably more familiar to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This audiences regret guarantees.",
                    "label": 0
                },
                {
                    "sent": "Standard regret guarantees do not apply to our FPL algorithm because of the nature of the losses that we're dealing with here.",
                    "label": 1
                },
                {
                    "sent": "But instead we prove our own regret guarantees which match almost exactly the standard regret guarantees of the additive losses.",
                    "label": 0
                },
                {
                    "sent": "In fact, syntactically, the bound looks exactly the same, and the only difference is in the definition of the terms that appear in the bound, as in the case of additive losses.",
                    "label": 1
                },
                {
                    "sent": "L sub mean is the loss of the best expert.",
                    "label": 1
                },
                {
                    "sent": "Ann is the number of edges in the expert automaton.",
                    "label": 0
                },
                {
                    "sent": "Diameter of the set of experts, and in the case of edit, if loss is a, is the largest possible loss of an expert, and in our case this term also depends on the topology of the expert graph.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the conclusion we presented a new framework for online learning that significantly broadens its applicability.",
                    "label": 0
                },
                {
                    "sent": "We give extensions of FPL and randomized weighted majority algorithms to rational losses, and remarkably extensions of these algorithms to tropical losses only require a change of the semiring.",
                    "label": 1
                },
                {
                    "sent": "So wherever I use log simmering for rational losses, I replace it with tropical semiring and this will give me an algorithm for tropical losses.",
                    "label": 0
                },
                {
                    "sent": "Finally, our algorithms admit the straightforward implementation using open source software tools such as Open FST library.",
                    "label": 0
                },
                {
                    "sent": "That concludes my presentation, thank you.",
                    "label": 0
                }
            ]
        }
    }
}