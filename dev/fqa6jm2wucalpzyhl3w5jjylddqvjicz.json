{
    "id": "fqa6jm2wucalpzyhl3w5jjylddqvjicz",
    "title": "Generative and Discriminative Models in Statistical Parsing",
    "info": {
        "author": [
            "Michael Collins, Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology, MIT"
        ],
        "published": "March 26, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_collins_gdmsp/",
    "segmentation": [
        [
            "Mike Collins did his PhD at Penn, where he worked on syntactic parsing for natural language processing, and he developed something called the Collins parser, which is widely used and known in the NLP community."
        ],
        [
            "And it was a really big success story and it was a generative model.",
            "Later he went to AT&T, where he developed a lot of theoretical analysis for discriminative methods an including things like the average perceptron, which has been highly influential.",
            "Now he's a professor at MIT and he.",
            "Works on kind of both, and he has some neat results that show you can do get substantial improvements by doing hybrid, so I'll turn it over to Mike.",
            "Go back.",
            "So all the talks today will be recorded.",
            "If you had an objection for that, you can lessen remove it from the hard drive.",
            "But so after after your presentation, just sign.",
            "We have authorization form for that and it will be on videovideolectures.net.",
            "We also have a USB key, so if you can put your slides on you think you can synchronize and it won't be like in PowerPoint form.",
            "So if you have like notes that you don't want people to."
        ],
        [
            "Great, OK, so as Percy said so I'm going to try to give an overview of both generative and discriminative modeling in natural language parsing.",
            "So the problem is as."
        ],
        [
            "Actually, you know, take a sentence.",
            "We're going to predict some syntactic structure for that sentence.",
            "And since the early 90s, we've had pretty large resource is of labeled examples, maybe thousands or 10s of thousands of examples of sentences laboriously hand labeled with these structures.",
            "And so actually, I'll talk about."
        ],
        [
            "Well, let me give you an overview of the talk.",
            "I'm ready to go back to the start.",
            "Innocence Patter was a model which came out of IBM in the early 90s, but I think it'll be really valuable to look back at this.",
            "This was a discriminative model and this is one of the first attempts.",
            "One of the first serious attempts at treebank parsing and then I'm going to go through five lexicalized models.",
            "OK, so this is going to be very personal and very selective you, but I'm just going to concentrate on these lexicalized models, which actually go back and forth between discriminative and generative approaches, and as I do this I'll try to give you some insights as to the motivation and the issues people ran into, and so on.",
            "So what I hope is that although this talk is specific to work on parsing, you know this is a as a structured prediction prob."
        ],
        [
            "Pretty interesting one, and many of the lessons we'll see as we."
        ],
        [
            "So along will be generally relevant to other problems into this issue of generative versus discriminative models, and then finally to finish up.",
            "Briefly, I'll talk about some recent work we've been doing where we actually combine generative and discriminative approaches at a very high level.",
            "The idea is going to be to use generative models to induce representations from unlabeled data, which we then use within a discriminative approach, and that seems to be pretty promising.",
            "OK."
        ],
        [
            "So spatter so as I said, this was really the first parser with a really serious result on, say the Penn treebank.",
            "Zero thought a model, by the way, is just to use a vanilla probabilistic context free grammar that works horribly.",
            "OK, so that was the first, probably the first experiment people did.",
            "This was the first model which worked really well.",
            "So in the abstract, the idea is very similar to an M M2 maximum entry Markov model, which many of you will have seen as a sort of precursor to Markov random fields.",
            "So let's see.",
            "So some notation.",
            "I'll give you a picture in a second about this, but we have some input sentence X.",
            "We have some pause tree Y and the first critical step is going to be to think of a parse tree being represented as a sequence of decisions which build those part parse tree and essentially the sequence of decisions they use.",
            "Sort of built the parse tree incrementally in a left to right fashion and bottom up I'll show you an example in a second.",
            "And then having done this, we can just use the chain rule and say that PY given X is sort of a product of terms where each point I condition the probability of the ice decision on the previous I -- 1 decisions.",
            "And at every point you can condition on any previous history.",
            "These models are often called history based models.",
            "Condition on any of the previously built structure.",
            "And you can also condition on any features of the input sentence.",
            "OK, in practice they estimated these terms using decision trees with fairly elaborate smoothing and so on.",
            "Later add weight.",
            "Right now Parkey built versions of these models where you use log linear models to estimate these terms OK. And you could see an immediate sort of appealing feature of these models, which is that you know with a sufficiently rich kind of discriminative conditional model here say decision tree or maximum entropy model, or something like this.",
            "You can really include all kinds of features, so you can include all kinds of representations, so this is one of the really appealing factors in these kind of models.",
            "OK, so what's the problem?",
            "So it turns."
        ],
        [
            "Now the following happened so many of you will have seen the label bias problem for MMSE, so the original paper on conditional random fields motivates conditional random fields as an alternative to Ms.",
            "Largely based on this thing called the label bias problem.",
            "OK, now in reality, if you look at the label bias problem for parsing, it is terrifically acute.",
            "So if you think there's a problem for MMSE, just basically kind of sequence labeling tasks.",
            "You should just try to apply these kind of models to pausing, so let me give you an example of how this might kick in in the pausing case.",
            "So as I said, with these models we're going to build things incrementally left to right through a sequence of decisions.",
            "And so we've gone to this particular stage, have built some intermediate structure, and I've got a noun phrase above this word Mary.",
            "OK, so this sentence is locally ambiguous.",
            "OK, and you see this everywhere in natural language parsing you see points of local ambiguity.",
            "If I had just said Bill likes Mary and Jane so I didn't have the end of the sentence here.",
            "So Bill likes Mary and Jane, I'd have one structure where these these two noun phrases, Mary and Jane, would be combined into a single constituent.",
            "OK, but now I have a sense Bill likes Mary and Jane likes Bill.",
            "That means that this is essentially going to be one segment, the sentence, and this is going to be another segment, another set of sentence.",
            "So essentially, there's a point of local ambiguity here where we have to figure out which of these two alternatives is the truth.",
            "And these decision based models will have to estimate this probability based on the current state.",
            "So at this point we've built none of the forward structure, and in fact this relevant context is quite far in the future.",
            "There's no structure here to indicate what's going on, and you really get into a bind because you're essentially going to have to estimate the probability.",
            "The alternative between these two structures, based on this very local context.",
            "OK, that means essentially that if you make the wrong decision at this point, which I think is for this constituent to go right as opposed to left.",
            "You're down a dead end, OK, and yet all the probability mass will have to be allocated to parse trees which include that decision.",
            "So you essentially end up assigning significant probability mass to garden paths to these ridiculous structures further down the line.",
            "OK, so you know the short summary is that these points of local ambiguity.",
            "There's real problems for these kind of models.",
            "OK.",
            "So let's go on to."
        ],
        [
            "The second model, so the second discriminative model.",
            "OK, so this is a change of representation which will be used throughout the generative and discriminative models.",
            "I show you from now on.",
            "OK, so.",
            "The key trick here is that.",
            "So if we look at this pause tree, actually the spatter folks had introduced one very important innovation, which was to add lexical items higher in the tree.",
            "So this was a representational change.",
            "They had a deterministic set of rules which would sort of propagate lexical items up through the tree.",
            "For example, would soar would be associated with this mode here, and this node here, and this is crucial and allowing lexical information to influence higher levels in the tree.",
            "OK, so one observation which is going to drive all of the remaining models that I'm going to show you is that given this innovation.",
            "We can then think of a kind of change of representation where we essentially represent a pause tree as a set of water called dependencies.",
            "OK, so what's going on here?",
            "Each dependency involves ahead word and a modifier word.",
            "So for example, saw in Mary and some label which is essentially taken from the parse tree.",
            "Let me show you how this works.",
            "So if we have the very top level we have saw Mary, we have this sort of rule here.",
            "That rule essentially contributes a dependency saw.",
            "Mary and this label, which is just taken from these three nonterminals OK.",
            "So, given a parse tree, we can just think of a representational change.",
            "There's a deterministic function to something like this.",
            "We finally have sore, by the way, is the root of the entire sentence.",
            "That's the sort of the top word at the very root.",
            "So we have deterministic transform, two sets of dependencies like this.",
            "And in fact there will be N dependencies for a sentence of length N. OK, so let me just give you a sketch of a model that I built in 1996, which was one attempt at using these kind of representations and what is actually a very naive probabilistic model.",
            "So the probability would scare quotes.",
            "You should be scared by this model.",
            "It's pretty rough, but it works quite well.",
            "Is the following, so we're simply going to say the probability of a pastor is a product of terms.",
            "And what we do is for any pair of words in the sentence.",
            "You can estimate estimate a distribution of a possible labels on dependency arcs between those two words, where one option is just that there's no dependency.",
            "OK, so we can estimate a conditional and this is a discriminative problem.",
            "Conditional distribution over no dependency versus any one of the possible labels.",
            "Of course, there's a pretty large label set here.",
            "So when we evaluate the probability of any parse tree, when we try to rank past trees in order of probability.",
            "We just multiply terms like this so we have one term for each dependency.",
            "OK, so we start to look at the plausibility of these like school dependencies.",
            "And these dependencies are very useful in disambiguating pollastri structures.",
            "That's been a consistent finding in these models.",
            "OK, so that's the basic model to estimate these terms I used simple kind of backed off estimation, some fairly fairly simple, backed off smoothed estimation that you see very often in natural language.",
            "OK, so here is the first couple of results.",
            "This is kind."
        ],
        [
            "F measure accuracy in recovering in devote individual constituents in parse trees.",
            "Spatter is like 84.1 what like just described is 85.5 in addition.",
            "My mind is pretty clear this is a much simpler approach.",
            "The estimation techniques used with much simpler decision tree approach was really pretty complex.",
            "So this second discriminative model I've shown you give some improvements is considerably simpler, but it's really pretty suspect as a probabilistic model, and the original paper I tried to give some justification for this, but it's really pretty suspect, and from that point of view it's difficult to extend it.",
            "And you've also got to wonder what's going on from a consistency POV, and so on and so on.",
            "OK."
        ],
        [
            "So that brings us to generative models, and so these models kind of clean things up and actually led to a pretty big gain in performance.",
            "Let me actually go to the next slide.",
            "To give."
        ],
        [
            "And intuition."
        ],
        [
            "So what we're going to see now is."
        ],
        [
            "Is these generative models called Markov grammars?",
            "So I worked on this.",
            "This is actually."
        ],
        [
            "Thesis Pozor in 1997, Eugene Charniak had a model in 1997 and a later model with improvements in 1999.",
            "And what we're going to see is sort of coherent generative model, which includes terms associated with."
        ],
        [
            "Tendencies, like the ones I've shown you here.",
            "OK, so how does this work?",
            "So let's think about."
        ],
        [
            "Generative process, which generates a sentence so the first step is to sort of choose.",
            "The head word of the entire sentence.",
            "So let's say for the sake of argument that eats is the main verb with the sentence with some probability conditioned on the fact we are at the root, we're going to generate the word eats together with a little fragmente of parse tree.",
            "OK, so this is basically saying I'm going to have eats its part of speech tag is going to be a verb is going to project up to a VP&S level.",
            "OK, so that's sort of the process which starts things.",
            "And So what you see here is we have a unit which you might call a spine.",
            "OK, so you know we call these things spines.",
            "They have a series of projections in the tree.",
            "And now we're going to see a recursive process where each spine essentially is going to generate its own modifyers at each level in the tree.",
            "So the left and right here on the left and the right.",
            "Here we can generate.",
            "So called modifier spines and we can basically define sort of a Markov process or hidden Markov process generating these modifyers.",
            "So let me show you how this might go.",
            "OK, so the first step say."
        ],
        [
            "Generating the sequence of modifyers to the right of this VP level.",
            "We might choose, you know, an NP spine with cake.",
            "OK, and this is going to have some probability.",
            "This is the item we're generating.",
            "We condition on some structure in the tree.",
            "The fact we're generating to the right and the fact that we are adjacent to this node.",
            "OK, so each point we're going to generate a modifier based on some context based on some previous history which captures some notion of stays essentially."
        ],
        [
            "OK, we keep going.",
            "We might generate this spine here again with some probability conditioned on what we've already generated.",
            "And then finally we generate some stop symbol which is basically something which terminates the sequence.",
            "OK, so at each point we."
        ],
        [
            "Generated modifier we stop.",
            "And you know this."
        ],
        [
            "This goes on.",
            "We might generate some modified to left here.",
            "We might immediately just stop, so there are no modifyers."
        ],
        [
            "We generate some modifiers to the right.",
            "At this level we just stop."
        ],
        [
            "May we generate one modified to the left so UN?"
        ],
        [
            "Up with this structure where you know each of these sequences modifies being generated using a Markov process.",
            "And of course you know the process is recursive, so now we have these modifier spines.",
            "And they themselves can go off and recursively generate left and right modifyers.",
            "And this is how the entire tree is built.",
            "OK, So what you essentially end up with is a model where.",
            "You know the parameters."
        ],
        [
            "Our initial parameters like this which start the process.",
            "And then these modify parameters."
        ],
        [
            "Which linked together 2 spines with some with some probability."
        ],
        [
            "OK."
        ],
        [
            "So let's look at results for this so."
        ],
        [
            "G1 is my thesis model, G2 is Eugene Charniak's model, which is a little later.",
            "What you see is, you know, we get some pretty considerable improvements over the discriminative model.",
            "I showed you that the first generative sorry dependency based model, this one.",
            "You know there's some nice things about these models.",
            "There's certainly coherent probabilistic models, so you no longer have this uneasy feeling that what you're doing is not really justified.",
            "They certainly give improvements.",
            "But the problem is there are many, many details in getting these models right.",
            "OK, So what I haven't really spoken about.",
            "Is that as you generate these?"
        ],
        [
            "Modifyers at each point you're going to condition on some previous history.",
            "Potentially including the previous modifier.",
            "The original headward eats maybe on the presence of punctuation.",
            "Maybe you have to worry about coordination.",
            "On top of that, you have to smooth these models correctly.",
            "You have to somehow decompose things in a very similar way to a directed graphical model where you generate sub sub segments of this spine piece by piece.",
            "There are a lot of details and it's very difficult to extend these models in that if you want to condition on more context, the estimation techniques that we used really started to fall down.",
            "OK, so we were using simple backs off estimation, the kind of estimation you'd see say in a trigram language model, and as you add more and more conditioning factors to this model, it becomes increasingly difficult to use these simple techniques to estimate things.",
            "Now that might push you to a model where you try to estimate this term using a log linear model right?",
            "There would be one way of bringing in all kinds of contextual features, but that would involve a partition function over all possible spines and at that point.",
            "You've got to wonder if you're using a log linear model in the generative sense.",
            "Why not just go to discriminative models?",
            "Sorry.",
            "Short story, these models gave him."
        ],
        [
            "Movements, but there really a pain in the neck to extend.",
            "That's at least my experience.",
            "Having built these for my thesis and then try to extend it.",
            "OK, so.",
            "Smooth."
        ],
        [
            "Onto the sort of final model we're going to get towards D4.",
            "But firstly, I'm just going to show you a precursor to this.",
            "So.",
            "This is another discriminative model, so this is probably the first example.",
            "The most modern discriminative models for parsing due to Ryan McDonald and collaborators at U Penn.",
            "And now we're actually going to directly go to dependency structures.",
            "So rather than trying to recover constituency structures.",
            "Will simply try to recover dependency structures, so this is a structure where we have directed arcs between heads and modifyers.",
            "For example, it says that the headward sore is taking Jaune.",
            "As its modifier OK. Now here's a discriminative model for dependency parsing.",
            "This is really structured prediction model, kind of conditional random field style model.",
            "The highest scoring dependency structure is simply going to be the structure Y.",
            "The maximizes a score, and what I have here is a sum over dependencies in the past, so each R is a tuple.",
            "HM, just specifying 2 words.",
            "So each dependency is going to feature vector.",
            "And then each dependencies again, getting to school.",
            "That's an inner product of the parameter vector in the feature vector.",
            "OK, so F is a feature vector associated with dependency and you can include very rich features in these feature vectors.",
            "You can for example when you're trying to consider the possibility of John and saw being connected, you could potentially look at any of the surrounding context in any of the Central conference context.",
            "These models do not run into the label bias problem.",
            "OK, so the problem I showed you is fatter.",
            "The label bias problem doesn't really arise with these models.",
            "OK, so in terms of results so this is a very simple approach.",
            "You train it, say using the average perceptron.",
            "In terms of dependency results.",
            "It's right."
        ],
        [
            "Around these kind of models.",
            "OK, so if you look at just the accuracy and recovering dependencies."
        ],
        [
            "It really was or is state of the art from that point of view.",
            "It's a very simple and direct model.",
            "It's very easy to extend it in terms of features and something which is perhaps underappreciated is that in addition, it's a very easy model to replicate, so we replicated this model.",
            "All I really need to give you is the definition of these feature vectors.",
            "OK, there's no smoothing, there's no.",
            "Yeah, it's it's a very, very simple model to replicate.",
            "OK, so the final thing I'm going to show you in this part of the."
        ],
        [
            "Talk and then we'll get into the hybrid models is.",
            "How to extend these to constituency pausing and it's fairly natural idea.",
            "I'll just give you."
        ],
        [
            "Sketch of this.",
            "I showed you earlier this idea of these generative models which generate modifyers one by one sort of head out outwards.",
            "And we are simply going to convert these discriminative models by associating feature vectors with these sort of generative moves.",
            "OK, so here's one example.",
            "Say this spinete is going to generate the spine."
        ],
        [
            "Take in exactly the same way as I showed you before in the original models would have a generative term conditional probability of this.",
            "Given this, now we're simply going to associate a feature vector with this sort of transition in the parse tree.",
            "So the feature vector could look at."
        ],
        [
            "The identity of these two spines, the two words."
        ],
        [
            "Sure involved the position in which this modifier is being generated."
        ],
        [
            "And so on and so on.",
            "In addition, it could look at any sort of surf."
        ],
        [
            "This structure, any surrounding features and the surface input.",
            "So we simply gone from.",
            "We go back here."
        ],
        [
            "The.",
            "Probabilities associated with these transitions.",
            "2.",
            "Feature."
        ],
        [
            "OK, and then we're going to use another kind of conditional random field style model where."
        ],
        [
            "And the score for repository is just going to some of inner product with these feature vectors.",
            "OK, it's."
        ],
        [
            "Easy enough to extend these models too."
        ],
        [
            "To allow you to look at richer notions of state, for example, you can look at when you're generating this modifier, what the previous modifier was, and these kind of features are extremely useful in parsing.",
            "Or"
        ],
        [
            "You can look at kind of grandparent dependencies when you know when we look at this modifier, we actually look up two levels up in the tree.",
            "OK, and this doesn't have too much of an impact on efficiency.",
            "You can still pass these kind of models relatively efficiently."
        ],
        [
            "OK, and so that's sort of it for the regular sort of supervised models.",
            "The results for this model that before that I've just shown you about 91.1% accuracy.",
            "It's sort of interesting to contrast this with spatter, which as I said was the start.",
            "So we've come full circle.",
            "We've gone from discriminative discriminative.",
            "He was spatter where the high scoring tree maximizes the sum of sort of local decisions.",
            "Here's the new model where the high scoring tree maximizes the sum of these dependencies scores and two crucial things have already happened.",
            "One is, we've moved from these sort of history based models too.",
            "Dependency structures OK, so this move to dependency, let's close representations, and Secondly we've moved from MMM style models to more sort of conditional random field structured prediction style models.",
            "And the improvement is pretty huge.",
            "We've almost half the error rate here.",
            "OK, so."
        ],
        [
            "Just to wrap up, I want to talk about some recent work we've been doing on two very simple approaches which are hybrid and which actually make use of both generative and discriminative models.",
            "So the first is the following, so we're going to go back actually to dependency parsing, which is a slightly simpler case.",
            "So these feature vectors I spoke about which are associated with dependencies depend very heavily on lexical items, so they condition very heavy on heavily on the identity of the words in the input.",
            "And those statistics are extremely sparse.",
            "OK, so one very simple idea is to use unlabeled data, which you have large quantities.",
            "To induce word clusters actually through probabilistic generative model.",
            "So we first run a preprocessing step where we use unlabeled data and we actually get a hierarchical clustering of all the words in the language and then we can incorporate features induced from the unlabeled data within these feature vectors.",
            "So it's very simple to incorporate these features, and here are some results, so this is for to 2nd order."
        ],
        [
            "Tennessee polls are pretty much state of the art.",
            "Paws are different, training set sizes everything from 1000.",
            "Sentence is up to about 40,000 at this point.",
            "And simply by including these word cluster features induced by generative model, we get some pretty significant improvements.",
            "So for example, at 1K we go from about 82 percent, 25 and so on and so on.",
            "Even at the full.",
            "Extent we almost get a 1% improvement in performance.",
            "OK, and this is getting what I see is a critical bottleneck in pausing, namely that these lexical statistics are very sparse and you need to have.",
            "A better idea about this?"
        ],
        [
            "Mantex of words which could potentially induce from unlabeled data.",
            "OK, let me show you the second hybrid discriminative model.",
            "So this is work by Jun Suzuki and myself.",
            "And again is a very simple idea.",
            "In some sense, it's rather heuristic.",
            "I wish we understood it a bit better, but it seems to work quite well.",
            "So the method proceeds in three steps.",
            "So firstly we train the CRF style dependency model on the labeled examples.",
            "So just using regular methods in the second step you use the model from step one to label again a very large amount of unlabeled data.",
            "Ann from that unlabeled data, you actually estimate a fairly large number of generative models, maybe 100 different generative models, different generative models of these parse trees.",
            "I'll explain in a second what these generative models look like.",
            "And then finally we simply add these general generative models.",
            "Actually, the log probabilities under these generative models.",
            "As features in the supervised model.",
            "OK, so we just feed these back in and we retrain a model of this form.",
            "OK."
        ],
        [
            "So June Suzuki and others have done some previous work on this.",
            "One of the appeals of this method is how they define the generative models.",
            "So they have this nice insight where the K generative models are derived directly from the original feature vectors.",
            "So you take your original feature vector definition.",
            "And you just partition it into K different subsets of features.",
            "There's usually a very natural way to do this according to feature type.",
            "And we basically define sort of simple naive Bayes style model on these different segments of these different feature vectors.",
            "So like I said, I wish we understood this better, but it's a very simple approach.",
            "And here are some results."
        ],
        [
            "So what I've shown you in blue is just a purely supervised method.",
            "In green is the results of the method.",
            "The first hybrid method I showed you.",
            "Where we have word clusters.",
            "And in red, I've shown you the method for the second hybrid method where we actually use this representation too.",
            "So we do use the word clusters in this representation too, and you see, you know we get a 2% gain when we have about 2000 training examples and even.",
            "With 40,000 training examples, we're still getting some appreciable gains from this method.",
            "And actually, we seem to have more or less doubled the amount of gain we get when we add this second hybrid approach.",
            "OK, so."
        ],
        [
            "I think I'm out of time, so some final thoughts.",
            "Some pros and cons of generative models as I see it as a subset.",
            "Obviously, so advantages generative models they are very fast to train.",
            "So one thing I didn't say was models that myself and Eugene Charniak developed.",
            "It's simply a matter of reading counts of a corpus.",
            "They're very, very fast to train.",
            "It's very useful way to think about semi supervised learning.",
            "In particular, I think using generative models to induce representations on unlabeled data is very useful.",
            "They are absolutely invaluable as language models in speech recognition and machine translation.",
            "OK, so these generative models, weather of strings or pause trees are used everywhere, and speech machine translation.",
            "This final point, I've sort of put up 'cause I think this is maybe a little over emphasized.",
            "Possibly a myth, I'm not sure.",
            "German models are better than discriminative models with small amounts of training data.",
            "This goes back to the Andrew ING and Michael Jordan results.",
            "That's very interesting paper, but they were looking at unregularized logistic regression.",
            "I'm pretty skeptical about this.",
            "If you really regularizer discriminative model.",
            "I'm not sure this is true.",
            "Advantage of discriminative models is very easy to incorporate new features and this for me is by far the biggest win.",
            "And also something which is maybe underappreciated, that there are easy to implement and replicate this notion of smoothing independence assumptions and so on.",
            "All you really need the feature definitions which go into these models, and you're there, OK, and that's.",
            "That's that I think is also important.",
            "OK, I think that's it.",
            "So in the vision.",
            "And.",
            "Areas where.",
            "Models.",
            "Or speech recognition for that.",
            "Best is that you first train generatively and then we refine description so all the speech systems are trained that way.",
            "Older animation systems, I notice that all the another problem thing is all about that supervised model and then you can refine supervised so you get the best of both worlds to some extent.",
            "Yeah, so trying the.",
            "The intuition of it, so I'm guessing that might be because these models typically use hidden alignments, and if you train immediately discriminatively, you get stuck in weird local minima.",
            "Or is it?",
            "That might be part of it?",
            "Passing number of parameters.",
            "Yeah, there's no way you can strangle this parameter.",
            "Launch in numbers of parameters.",
            "These pausing models are using millions of parameters, 10 minutes, so we also have very large premise faces.",
            "My intuition is we don't have hidden variables are easier to regularize.",
            "That's right.",
            "Yeah, I think.",
            "Cannot use in your Department, raised images or so you have to use.",
            "Socially, models in this regard.",
            "Yeah.",
            "Yes.",
            "And there.",
            "Pre training supervisor.",
            "Yeah.",
            "Well, there's something to that actually, speech recognition you.",
            "We have some set up, translation of the model, but it just training criteria is different.",
            "In this case you actually have two different sets of models runs.",
            "Gender models are true, though it doesn't share any plans are.",
            "It's harder to for me to imagine actually, do you do this like sleep like speech recognition away from training for maximum likelihood, become condition like?",
            "I just kind of difficult to see what's going to happen that one wishes to partition whatever criteria you won't deliver, you know.",
            "Umm I like to call it in my structure formation with the speech, which is the same as here.",
            "I've you know, it's.",
            "I could go for for maximum marginal conceptuality there's a whole bunch of functions you can use for the main difference with speech, speech and writing models is that you don't have any organization of the models you have to.",
            "Speech will you'll have to learn the means and covariance matrices for the Gaussian model.",
            "And those are not linearly related to the liquid, so we had to.",
            "You know it's about when you go to come so many intuitions I've shown here.",
            "When you go to continuous about high, relatively high dimensional continuous value data, the intuitions are very, very differently.",
            "But we've done.",
            "It works because people have been using this for about 20 years.",
            "To worship.",
            "So I actually don't understand the last trip.",
            "The trip here last week and after you do something that we should understand why it works.",
            "So I understand you train the model distributively and then you pass the unsupervised unlabeled data and get partial like it's using partial directors get much applause trees.",
            "Yeah, I'm able data and you said that we would reach way using label.",
            "No, you induce these generative models, so you from these these label examples on the unlabeled data you learn.",
            "100 very simple generative models of the tallest trees which models, but assuming late.",
            "So it was like a coat, raincoat, conciseness you're going from a complex model, supermodel Mini Cooper.",
            "But if you use down the actual Origin label data will check it out.",
            "No."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mike Collins did his PhD at Penn, where he worked on syntactic parsing for natural language processing, and he developed something called the Collins parser, which is widely used and known in the NLP community.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it was a really big success story and it was a generative model.",
                    "label": 0
                },
                {
                    "sent": "Later he went to AT&T, where he developed a lot of theoretical analysis for discriminative methods an including things like the average perceptron, which has been highly influential.",
                    "label": 0
                },
                {
                    "sent": "Now he's a professor at MIT and he.",
                    "label": 0
                },
                {
                    "sent": "Works on kind of both, and he has some neat results that show you can do get substantial improvements by doing hybrid, so I'll turn it over to Mike.",
                    "label": 0
                },
                {
                    "sent": "Go back.",
                    "label": 0
                },
                {
                    "sent": "So all the talks today will be recorded.",
                    "label": 0
                },
                {
                    "sent": "If you had an objection for that, you can lessen remove it from the hard drive.",
                    "label": 0
                },
                {
                    "sent": "But so after after your presentation, just sign.",
                    "label": 0
                },
                {
                    "sent": "We have authorization form for that and it will be on videovideolectures.net.",
                    "label": 0
                },
                {
                    "sent": "We also have a USB key, so if you can put your slides on you think you can synchronize and it won't be like in PowerPoint form.",
                    "label": 0
                },
                {
                    "sent": "So if you have like notes that you don't want people to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great, OK, so as Percy said so I'm going to try to give an overview of both generative and discriminative modeling in natural language parsing.",
                    "label": 0
                },
                {
                    "sent": "So the problem is as.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, you know, take a sentence.",
                    "label": 0
                },
                {
                    "sent": "We're going to predict some syntactic structure for that sentence.",
                    "label": 0
                },
                {
                    "sent": "And since the early 90s, we've had pretty large resource is of labeled examples, maybe thousands or 10s of thousands of examples of sentences laboriously hand labeled with these structures.",
                    "label": 0
                },
                {
                    "sent": "And so actually, I'll talk about.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, let me give you an overview of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'm ready to go back to the start.",
                    "label": 0
                },
                {
                    "sent": "Innocence Patter was a model which came out of IBM in the early 90s, but I think it'll be really valuable to look back at this.",
                    "label": 0
                },
                {
                    "sent": "This was a discriminative model and this is one of the first attempts.",
                    "label": 0
                },
                {
                    "sent": "One of the first serious attempts at treebank parsing and then I'm going to go through five lexicalized models.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is going to be very personal and very selective you, but I'm just going to concentrate on these lexicalized models, which actually go back and forth between discriminative and generative approaches, and as I do this I'll try to give you some insights as to the motivation and the issues people ran into, and so on.",
                    "label": 0
                },
                {
                    "sent": "So what I hope is that although this talk is specific to work on parsing, you know this is a as a structured prediction prob.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty interesting one, and many of the lessons we'll see as we.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So along will be generally relevant to other problems into this issue of generative versus discriminative models, and then finally to finish up.",
                    "label": 0
                },
                {
                    "sent": "Briefly, I'll talk about some recent work we've been doing where we actually combine generative and discriminative approaches at a very high level.",
                    "label": 1
                },
                {
                    "sent": "The idea is going to be to use generative models to induce representations from unlabeled data, which we then use within a discriminative approach, and that seems to be pretty promising.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So spatter so as I said, this was really the first parser with a really serious result on, say the Penn treebank.",
                    "label": 0
                },
                {
                    "sent": "Zero thought a model, by the way, is just to use a vanilla probabilistic context free grammar that works horribly.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was the first, probably the first experiment people did.",
                    "label": 0
                },
                {
                    "sent": "This was the first model which worked really well.",
                    "label": 0
                },
                {
                    "sent": "So in the abstract, the idea is very similar to an M M2 maximum entry Markov model, which many of you will have seen as a sort of precursor to Markov random fields.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                },
                {
                    "sent": "So some notation.",
                    "label": 0
                },
                {
                    "sent": "I'll give you a picture in a second about this, but we have some input sentence X.",
                    "label": 0
                },
                {
                    "sent": "We have some pause tree Y and the first critical step is going to be to think of a parse tree being represented as a sequence of decisions which build those part parse tree and essentially the sequence of decisions they use.",
                    "label": 1
                },
                {
                    "sent": "Sort of built the parse tree incrementally in a left to right fashion and bottom up I'll show you an example in a second.",
                    "label": 0
                },
                {
                    "sent": "And then having done this, we can just use the chain rule and say that PY given X is sort of a product of terms where each point I condition the probability of the ice decision on the previous I -- 1 decisions.",
                    "label": 0
                },
                {
                    "sent": "And at every point you can condition on any previous history.",
                    "label": 0
                },
                {
                    "sent": "These models are often called history based models.",
                    "label": 0
                },
                {
                    "sent": "Condition on any of the previously built structure.",
                    "label": 0
                },
                {
                    "sent": "And you can also condition on any features of the input sentence.",
                    "label": 0
                },
                {
                    "sent": "OK, in practice they estimated these terms using decision trees with fairly elaborate smoothing and so on.",
                    "label": 0
                },
                {
                    "sent": "Later add weight.",
                    "label": 0
                },
                {
                    "sent": "Right now Parkey built versions of these models where you use log linear models to estimate these terms OK. And you could see an immediate sort of appealing feature of these models, which is that you know with a sufficiently rich kind of discriminative conditional model here say decision tree or maximum entropy model, or something like this.",
                    "label": 0
                },
                {
                    "sent": "You can really include all kinds of features, so you can include all kinds of representations, so this is one of the really appealing factors in these kind of models.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the problem?",
                    "label": 0
                },
                {
                    "sent": "So it turns.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the following happened so many of you will have seen the label bias problem for MMSE, so the original paper on conditional random fields motivates conditional random fields as an alternative to Ms.",
                    "label": 0
                },
                {
                    "sent": "Largely based on this thing called the label bias problem.",
                    "label": 0
                },
                {
                    "sent": "OK, now in reality, if you look at the label bias problem for parsing, it is terrifically acute.",
                    "label": 0
                },
                {
                    "sent": "So if you think there's a problem for MMSE, just basically kind of sequence labeling tasks.",
                    "label": 1
                },
                {
                    "sent": "You should just try to apply these kind of models to pausing, so let me give you an example of how this might kick in in the pausing case.",
                    "label": 0
                },
                {
                    "sent": "So as I said, with these models we're going to build things incrementally left to right through a sequence of decisions.",
                    "label": 0
                },
                {
                    "sent": "And so we've gone to this particular stage, have built some intermediate structure, and I've got a noun phrase above this word Mary.",
                    "label": 0
                },
                {
                    "sent": "OK, so this sentence is locally ambiguous.",
                    "label": 0
                },
                {
                    "sent": "OK, and you see this everywhere in natural language parsing you see points of local ambiguity.",
                    "label": 0
                },
                {
                    "sent": "If I had just said Bill likes Mary and Jane so I didn't have the end of the sentence here.",
                    "label": 0
                },
                {
                    "sent": "So Bill likes Mary and Jane, I'd have one structure where these these two noun phrases, Mary and Jane, would be combined into a single constituent.",
                    "label": 0
                },
                {
                    "sent": "OK, but now I have a sense Bill likes Mary and Jane likes Bill.",
                    "label": 0
                },
                {
                    "sent": "That means that this is essentially going to be one segment, the sentence, and this is going to be another segment, another set of sentence.",
                    "label": 0
                },
                {
                    "sent": "So essentially, there's a point of local ambiguity here where we have to figure out which of these two alternatives is the truth.",
                    "label": 0
                },
                {
                    "sent": "And these decision based models will have to estimate this probability based on the current state.",
                    "label": 0
                },
                {
                    "sent": "So at this point we've built none of the forward structure, and in fact this relevant context is quite far in the future.",
                    "label": 0
                },
                {
                    "sent": "There's no structure here to indicate what's going on, and you really get into a bind because you're essentially going to have to estimate the probability.",
                    "label": 0
                },
                {
                    "sent": "The alternative between these two structures, based on this very local context.",
                    "label": 0
                },
                {
                    "sent": "OK, that means essentially that if you make the wrong decision at this point, which I think is for this constituent to go right as opposed to left.",
                    "label": 0
                },
                {
                    "sent": "You're down a dead end, OK, and yet all the probability mass will have to be allocated to parse trees which include that decision.",
                    "label": 0
                },
                {
                    "sent": "So you essentially end up assigning significant probability mass to garden paths to these ridiculous structures further down the line.",
                    "label": 0
                },
                {
                    "sent": "OK, so you know the short summary is that these points of local ambiguity.",
                    "label": 0
                },
                {
                    "sent": "There's real problems for these kind of models.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's go on to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second model, so the second discriminative model.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is a change of representation which will be used throughout the generative and discriminative models.",
                    "label": 0
                },
                {
                    "sent": "I show you from now on.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The key trick here is that.",
                    "label": 0
                },
                {
                    "sent": "So if we look at this pause tree, actually the spatter folks had introduced one very important innovation, which was to add lexical items higher in the tree.",
                    "label": 0
                },
                {
                    "sent": "So this was a representational change.",
                    "label": 0
                },
                {
                    "sent": "They had a deterministic set of rules which would sort of propagate lexical items up through the tree.",
                    "label": 0
                },
                {
                    "sent": "For example, would soar would be associated with this mode here, and this node here, and this is crucial and allowing lexical information to influence higher levels in the tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so one observation which is going to drive all of the remaining models that I'm going to show you is that given this innovation.",
                    "label": 0
                },
                {
                    "sent": "We can then think of a kind of change of representation where we essentially represent a pause tree as a set of water called dependencies.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Each dependency involves ahead word and a modifier word.",
                    "label": 0
                },
                {
                    "sent": "So for example, saw in Mary and some label which is essentially taken from the parse tree.",
                    "label": 0
                },
                {
                    "sent": "Let me show you how this works.",
                    "label": 1
                },
                {
                    "sent": "So if we have the very top level we have saw Mary, we have this sort of rule here.",
                    "label": 0
                },
                {
                    "sent": "That rule essentially contributes a dependency saw.",
                    "label": 0
                },
                {
                    "sent": "Mary and this label, which is just taken from these three nonterminals OK.",
                    "label": 0
                },
                {
                    "sent": "So, given a parse tree, we can just think of a representational change.",
                    "label": 0
                },
                {
                    "sent": "There's a deterministic function to something like this.",
                    "label": 0
                },
                {
                    "sent": "We finally have sore, by the way, is the root of the entire sentence.",
                    "label": 0
                },
                {
                    "sent": "That's the sort of the top word at the very root.",
                    "label": 0
                },
                {
                    "sent": "So we have deterministic transform, two sets of dependencies like this.",
                    "label": 0
                },
                {
                    "sent": "And in fact there will be N dependencies for a sentence of length N. OK, so let me just give you a sketch of a model that I built in 1996, which was one attempt at using these kind of representations and what is actually a very naive probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "So the probability would scare quotes.",
                    "label": 0
                },
                {
                    "sent": "You should be scared by this model.",
                    "label": 0
                },
                {
                    "sent": "It's pretty rough, but it works quite well.",
                    "label": 0
                },
                {
                    "sent": "Is the following, so we're simply going to say the probability of a pastor is a product of terms.",
                    "label": 0
                },
                {
                    "sent": "And what we do is for any pair of words in the sentence.",
                    "label": 0
                },
                {
                    "sent": "You can estimate estimate a distribution of a possible labels on dependency arcs between those two words, where one option is just that there's no dependency.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can estimate a conditional and this is a discriminative problem.",
                    "label": 0
                },
                {
                    "sent": "Conditional distribution over no dependency versus any one of the possible labels.",
                    "label": 0
                },
                {
                    "sent": "Of course, there's a pretty large label set here.",
                    "label": 0
                },
                {
                    "sent": "So when we evaluate the probability of any parse tree, when we try to rank past trees in order of probability.",
                    "label": 1
                },
                {
                    "sent": "We just multiply terms like this so we have one term for each dependency.",
                    "label": 0
                },
                {
                    "sent": "OK, so we start to look at the plausibility of these like school dependencies.",
                    "label": 0
                },
                {
                    "sent": "And these dependencies are very useful in disambiguating pollastri structures.",
                    "label": 0
                },
                {
                    "sent": "That's been a consistent finding in these models.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the basic model to estimate these terms I used simple kind of backed off estimation, some fairly fairly simple, backed off smoothed estimation that you see very often in natural language.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the first couple of results.",
                    "label": 0
                },
                {
                    "sent": "This is kind.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "F measure accuracy in recovering in devote individual constituents in parse trees.",
                    "label": 0
                },
                {
                    "sent": "Spatter is like 84.1 what like just described is 85.5 in addition.",
                    "label": 0
                },
                {
                    "sent": "My mind is pretty clear this is a much simpler approach.",
                    "label": 0
                },
                {
                    "sent": "The estimation techniques used with much simpler decision tree approach was really pretty complex.",
                    "label": 0
                },
                {
                    "sent": "So this second discriminative model I've shown you give some improvements is considerably simpler, but it's really pretty suspect as a probabilistic model, and the original paper I tried to give some justification for this, but it's really pretty suspect, and from that point of view it's difficult to extend it.",
                    "label": 1
                },
                {
                    "sent": "And you've also got to wonder what's going on from a consistency POV, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that brings us to generative models, and so these models kind of clean things up and actually led to a pretty big gain in performance.",
                    "label": 0
                },
                {
                    "sent": "Let me actually go to the next slide.",
                    "label": 0
                },
                {
                    "sent": "To give.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And intuition.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we're going to see now is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is these generative models called Markov grammars?",
                    "label": 0
                },
                {
                    "sent": "So I worked on this.",
                    "label": 0
                },
                {
                    "sent": "This is actually.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thesis Pozor in 1997, Eugene Charniak had a model in 1997 and a later model with improvements in 1999.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to see is sort of coherent generative model, which includes terms associated with.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tendencies, like the ones I've shown you here.",
                    "label": 0
                },
                {
                    "sent": "OK, so how does this work?",
                    "label": 0
                },
                {
                    "sent": "So let's think about.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generative process, which generates a sentence so the first step is to sort of choose.",
                    "label": 0
                },
                {
                    "sent": "The head word of the entire sentence.",
                    "label": 1
                },
                {
                    "sent": "So let's say for the sake of argument that eats is the main verb with the sentence with some probability conditioned on the fact we are at the root, we're going to generate the word eats together with a little fragmente of parse tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is basically saying I'm going to have eats its part of speech tag is going to be a verb is going to project up to a VP&S level.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's sort of the process which starts things.",
                    "label": 0
                },
                {
                    "sent": "And So what you see here is we have a unit which you might call a spine.",
                    "label": 0
                },
                {
                    "sent": "OK, so you know we call these things spines.",
                    "label": 1
                },
                {
                    "sent": "They have a series of projections in the tree.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to see a recursive process where each spine essentially is going to generate its own modifyers at each level in the tree.",
                    "label": 1
                },
                {
                    "sent": "So the left and right here on the left and the right.",
                    "label": 0
                },
                {
                    "sent": "Here we can generate.",
                    "label": 0
                },
                {
                    "sent": "So called modifier spines and we can basically define sort of a Markov process or hidden Markov process generating these modifyers.",
                    "label": 0
                },
                {
                    "sent": "So let me show you how this might go.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first step say.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generating the sequence of modifyers to the right of this VP level.",
                    "label": 0
                },
                {
                    "sent": "We might choose, you know, an NP spine with cake.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is going to have some probability.",
                    "label": 0
                },
                {
                    "sent": "This is the item we're generating.",
                    "label": 0
                },
                {
                    "sent": "We condition on some structure in the tree.",
                    "label": 0
                },
                {
                    "sent": "The fact we're generating to the right and the fact that we are adjacent to this node.",
                    "label": 0
                },
                {
                    "sent": "OK, so each point we're going to generate a modifier based on some context based on some previous history which captures some notion of stays essentially.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we keep going.",
                    "label": 0
                },
                {
                    "sent": "We might generate this spine here again with some probability conditioned on what we've already generated.",
                    "label": 0
                },
                {
                    "sent": "And then finally we generate some stop symbol which is basically something which terminates the sequence.",
                    "label": 0
                },
                {
                    "sent": "OK, so at each point we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generated modifier we stop.",
                    "label": 0
                },
                {
                    "sent": "And you know this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This goes on.",
                    "label": 0
                },
                {
                    "sent": "We might generate some modified to left here.",
                    "label": 0
                },
                {
                    "sent": "We might immediately just stop, so there are no modifyers.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We generate some modifiers to the right.",
                    "label": 0
                },
                {
                    "sent": "At this level we just stop.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "May we generate one modified to the left so UN?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up with this structure where you know each of these sequences modifies being generated using a Markov process.",
                    "label": 0
                },
                {
                    "sent": "And of course you know the process is recursive, so now we have these modifier spines.",
                    "label": 0
                },
                {
                    "sent": "And they themselves can go off and recursively generate left and right modifyers.",
                    "label": 0
                },
                {
                    "sent": "And this is how the entire tree is built.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you essentially end up with is a model where.",
                    "label": 0
                },
                {
                    "sent": "You know the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our initial parameters like this which start the process.",
                    "label": 0
                },
                {
                    "sent": "And then these modify parameters.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which linked together 2 spines with some with some probability.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at results for this so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "G1 is my thesis model, G2 is Eugene Charniak's model, which is a little later.",
                    "label": 0
                },
                {
                    "sent": "What you see is, you know, we get some pretty considerable improvements over the discriminative model.",
                    "label": 0
                },
                {
                    "sent": "I showed you that the first generative sorry dependency based model, this one.",
                    "label": 0
                },
                {
                    "sent": "You know there's some nice things about these models.",
                    "label": 0
                },
                {
                    "sent": "There's certainly coherent probabilistic models, so you no longer have this uneasy feeling that what you're doing is not really justified.",
                    "label": 0
                },
                {
                    "sent": "They certainly give improvements.",
                    "label": 0
                },
                {
                    "sent": "But the problem is there are many, many details in getting these models right.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I haven't really spoken about.",
                    "label": 0
                },
                {
                    "sent": "Is that as you generate these?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Modifyers at each point you're going to condition on some previous history.",
                    "label": 0
                },
                {
                    "sent": "Potentially including the previous modifier.",
                    "label": 0
                },
                {
                    "sent": "The original headward eats maybe on the presence of punctuation.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have to worry about coordination.",
                    "label": 0
                },
                {
                    "sent": "On top of that, you have to smooth these models correctly.",
                    "label": 0
                },
                {
                    "sent": "You have to somehow decompose things in a very similar way to a directed graphical model where you generate sub sub segments of this spine piece by piece.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of details and it's very difficult to extend these models in that if you want to condition on more context, the estimation techniques that we used really started to fall down.",
                    "label": 0
                },
                {
                    "sent": "OK, so we were using simple backs off estimation, the kind of estimation you'd see say in a trigram language model, and as you add more and more conditioning factors to this model, it becomes increasingly difficult to use these simple techniques to estimate things.",
                    "label": 0
                },
                {
                    "sent": "Now that might push you to a model where you try to estimate this term using a log linear model right?",
                    "label": 0
                },
                {
                    "sent": "There would be one way of bringing in all kinds of contextual features, but that would involve a partition function over all possible spines and at that point.",
                    "label": 0
                },
                {
                    "sent": "You've got to wonder if you're using a log linear model in the generative sense.",
                    "label": 0
                },
                {
                    "sent": "Why not just go to discriminative models?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Short story, these models gave him.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Movements, but there really a pain in the neck to extend.",
                    "label": 0
                },
                {
                    "sent": "That's at least my experience.",
                    "label": 0
                },
                {
                    "sent": "Having built these for my thesis and then try to extend it.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Smooth.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Onto the sort of final model we're going to get towards D4.",
                    "label": 0
                },
                {
                    "sent": "But firstly, I'm just going to show you a precursor to this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is another discriminative model, so this is probably the first example.",
                    "label": 0
                },
                {
                    "sent": "The most modern discriminative models for parsing due to Ryan McDonald and collaborators at U Penn.",
                    "label": 0
                },
                {
                    "sent": "And now we're actually going to directly go to dependency structures.",
                    "label": 0
                },
                {
                    "sent": "So rather than trying to recover constituency structures.",
                    "label": 0
                },
                {
                    "sent": "Will simply try to recover dependency structures, so this is a structure where we have directed arcs between heads and modifyers.",
                    "label": 0
                },
                {
                    "sent": "For example, it says that the headward sore is taking Jaune.",
                    "label": 0
                },
                {
                    "sent": "As its modifier OK. Now here's a discriminative model for dependency parsing.",
                    "label": 0
                },
                {
                    "sent": "This is really structured prediction model, kind of conditional random field style model.",
                    "label": 0
                },
                {
                    "sent": "The highest scoring dependency structure is simply going to be the structure Y.",
                    "label": 0
                },
                {
                    "sent": "The maximizes a score, and what I have here is a sum over dependencies in the past, so each R is a tuple.",
                    "label": 0
                },
                {
                    "sent": "HM, just specifying 2 words.",
                    "label": 0
                },
                {
                    "sent": "So each dependency is going to feature vector.",
                    "label": 0
                },
                {
                    "sent": "And then each dependencies again, getting to school.",
                    "label": 0
                },
                {
                    "sent": "That's an inner product of the parameter vector in the feature vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so F is a feature vector associated with dependency and you can include very rich features in these feature vectors.",
                    "label": 0
                },
                {
                    "sent": "You can for example when you're trying to consider the possibility of John and saw being connected, you could potentially look at any of the surrounding context in any of the Central conference context.",
                    "label": 0
                },
                {
                    "sent": "These models do not run into the label bias problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so the problem I showed you is fatter.",
                    "label": 0
                },
                {
                    "sent": "The label bias problem doesn't really arise with these models.",
                    "label": 0
                },
                {
                    "sent": "OK, so in terms of results so this is a very simple approach.",
                    "label": 0
                },
                {
                    "sent": "You train it, say using the average perceptron.",
                    "label": 0
                },
                {
                    "sent": "In terms of dependency results.",
                    "label": 0
                },
                {
                    "sent": "It's right.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Around these kind of models.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you look at just the accuracy and recovering dependencies.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It really was or is state of the art from that point of view.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple and direct model.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to extend it in terms of features and something which is perhaps underappreciated is that in addition, it's a very easy model to replicate, so we replicated this model.",
                    "label": 0
                },
                {
                    "sent": "All I really need to give you is the definition of these feature vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, there's no smoothing, there's no.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's it's a very, very simple model to replicate.",
                    "label": 0
                },
                {
                    "sent": "OK, so the final thing I'm going to show you in this part of the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk and then we'll get into the hybrid models is.",
                    "label": 0
                },
                {
                    "sent": "How to extend these to constituency pausing and it's fairly natural idea.",
                    "label": 0
                },
                {
                    "sent": "I'll just give you.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sketch of this.",
                    "label": 0
                },
                {
                    "sent": "I showed you earlier this idea of these generative models which generate modifyers one by one sort of head out outwards.",
                    "label": 0
                },
                {
                    "sent": "And we are simply going to convert these discriminative models by associating feature vectors with these sort of generative moves.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's one example.",
                    "label": 0
                },
                {
                    "sent": "Say this spinete is going to generate the spine.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take in exactly the same way as I showed you before in the original models would have a generative term conditional probability of this.",
                    "label": 0
                },
                {
                    "sent": "Given this, now we're simply going to associate a feature vector with this sort of transition in the parse tree.",
                    "label": 0
                },
                {
                    "sent": "So the feature vector could look at.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The identity of these two spines, the two words.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure involved the position in which this modifier is being generated.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so on and so on.",
                    "label": 0
                },
                {
                    "sent": "In addition, it could look at any sort of surf.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This structure, any surrounding features and the surface input.",
                    "label": 0
                },
                {
                    "sent": "So we simply gone from.",
                    "label": 0
                },
                {
                    "sent": "We go back here.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Probabilities associated with these transitions.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Feature.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then we're going to use another kind of conditional random field style model where.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the score for repository is just going to some of inner product with these feature vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, it's.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Easy enough to extend these models too.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To allow you to look at richer notions of state, for example, you can look at when you're generating this modifier, what the previous modifier was, and these kind of features are extremely useful in parsing.",
                    "label": 0
                },
                {
                    "sent": "Or",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can look at kind of grandparent dependencies when you know when we look at this modifier, we actually look up two levels up in the tree.",
                    "label": 0
                },
                {
                    "sent": "OK, and this doesn't have too much of an impact on efficiency.",
                    "label": 0
                },
                {
                    "sent": "You can still pass these kind of models relatively efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so that's sort of it for the regular sort of supervised models.",
                    "label": 0
                },
                {
                    "sent": "The results for this model that before that I've just shown you about 91.1% accuracy.",
                    "label": 0
                },
                {
                    "sent": "It's sort of interesting to contrast this with spatter, which as I said was the start.",
                    "label": 0
                },
                {
                    "sent": "So we've come full circle.",
                    "label": 0
                },
                {
                    "sent": "We've gone from discriminative discriminative.",
                    "label": 0
                },
                {
                    "sent": "He was spatter where the high scoring tree maximizes the sum of sort of local decisions.",
                    "label": 0
                },
                {
                    "sent": "Here's the new model where the high scoring tree maximizes the sum of these dependencies scores and two crucial things have already happened.",
                    "label": 0
                },
                {
                    "sent": "One is, we've moved from these sort of history based models too.",
                    "label": 0
                },
                {
                    "sent": "Dependency structures OK, so this move to dependency, let's close representations, and Secondly we've moved from MMM style models to more sort of conditional random field structured prediction style models.",
                    "label": 0
                },
                {
                    "sent": "And the improvement is pretty huge.",
                    "label": 0
                },
                {
                    "sent": "We've almost half the error rate here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to wrap up, I want to talk about some recent work we've been doing on two very simple approaches which are hybrid and which actually make use of both generative and discriminative models.",
                    "label": 0
                },
                {
                    "sent": "So the first is the following, so we're going to go back actually to dependency parsing, which is a slightly simpler case.",
                    "label": 0
                },
                {
                    "sent": "So these feature vectors I spoke about which are associated with dependencies depend very heavily on lexical items, so they condition very heavy on heavily on the identity of the words in the input.",
                    "label": 0
                },
                {
                    "sent": "And those statistics are extremely sparse.",
                    "label": 0
                },
                {
                    "sent": "OK, so one very simple idea is to use unlabeled data, which you have large quantities.",
                    "label": 0
                },
                {
                    "sent": "To induce word clusters actually through probabilistic generative model.",
                    "label": 0
                },
                {
                    "sent": "So we first run a preprocessing step where we use unlabeled data and we actually get a hierarchical clustering of all the words in the language and then we can incorporate features induced from the unlabeled data within these feature vectors.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple to incorporate these features, and here are some results, so this is for to 2nd order.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tennessee polls are pretty much state of the art.",
                    "label": 0
                },
                {
                    "sent": "Paws are different, training set sizes everything from 1000.",
                    "label": 0
                },
                {
                    "sent": "Sentence is up to about 40,000 at this point.",
                    "label": 0
                },
                {
                    "sent": "And simply by including these word cluster features induced by generative model, we get some pretty significant improvements.",
                    "label": 0
                },
                {
                    "sent": "So for example, at 1K we go from about 82 percent, 25 and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "Even at the full.",
                    "label": 0
                },
                {
                    "sent": "Extent we almost get a 1% improvement in performance.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is getting what I see is a critical bottleneck in pausing, namely that these lexical statistics are very sparse and you need to have.",
                    "label": 0
                },
                {
                    "sent": "A better idea about this?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mantex of words which could potentially induce from unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "OK, let me show you the second hybrid discriminative model.",
                    "label": 0
                },
                {
                    "sent": "So this is work by Jun Suzuki and myself.",
                    "label": 0
                },
                {
                    "sent": "And again is a very simple idea.",
                    "label": 0
                },
                {
                    "sent": "In some sense, it's rather heuristic.",
                    "label": 0
                },
                {
                    "sent": "I wish we understood it a bit better, but it seems to work quite well.",
                    "label": 0
                },
                {
                    "sent": "So the method proceeds in three steps.",
                    "label": 0
                },
                {
                    "sent": "So firstly we train the CRF style dependency model on the labeled examples.",
                    "label": 0
                },
                {
                    "sent": "So just using regular methods in the second step you use the model from step one to label again a very large amount of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Ann from that unlabeled data, you actually estimate a fairly large number of generative models, maybe 100 different generative models, different generative models of these parse trees.",
                    "label": 0
                },
                {
                    "sent": "I'll explain in a second what these generative models look like.",
                    "label": 0
                },
                {
                    "sent": "And then finally we simply add these general generative models.",
                    "label": 0
                },
                {
                    "sent": "Actually, the log probabilities under these generative models.",
                    "label": 0
                },
                {
                    "sent": "As features in the supervised model.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just feed these back in and we retrain a model of this form.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So June Suzuki and others have done some previous work on this.",
                    "label": 0
                },
                {
                    "sent": "One of the appeals of this method is how they define the generative models.",
                    "label": 0
                },
                {
                    "sent": "So they have this nice insight where the K generative models are derived directly from the original feature vectors.",
                    "label": 0
                },
                {
                    "sent": "So you take your original feature vector definition.",
                    "label": 0
                },
                {
                    "sent": "And you just partition it into K different subsets of features.",
                    "label": 0
                },
                {
                    "sent": "There's usually a very natural way to do this according to feature type.",
                    "label": 0
                },
                {
                    "sent": "And we basically define sort of simple naive Bayes style model on these different segments of these different feature vectors.",
                    "label": 0
                },
                {
                    "sent": "So like I said, I wish we understood this better, but it's a very simple approach.",
                    "label": 0
                },
                {
                    "sent": "And here are some results.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I've shown you in blue is just a purely supervised method.",
                    "label": 0
                },
                {
                    "sent": "In green is the results of the method.",
                    "label": 0
                },
                {
                    "sent": "The first hybrid method I showed you.",
                    "label": 0
                },
                {
                    "sent": "Where we have word clusters.",
                    "label": 0
                },
                {
                    "sent": "And in red, I've shown you the method for the second hybrid method where we actually use this representation too.",
                    "label": 0
                },
                {
                    "sent": "So we do use the word clusters in this representation too, and you see, you know we get a 2% gain when we have about 2000 training examples and even.",
                    "label": 0
                },
                {
                    "sent": "With 40,000 training examples, we're still getting some appreciable gains from this method.",
                    "label": 0
                },
                {
                    "sent": "And actually, we seem to have more or less doubled the amount of gain we get when we add this second hybrid approach.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think I'm out of time, so some final thoughts.",
                    "label": 0
                },
                {
                    "sent": "Some pros and cons of generative models as I see it as a subset.",
                    "label": 1
                },
                {
                    "sent": "Obviously, so advantages generative models they are very fast to train.",
                    "label": 1
                },
                {
                    "sent": "So one thing I didn't say was models that myself and Eugene Charniak developed.",
                    "label": 0
                },
                {
                    "sent": "It's simply a matter of reading counts of a corpus.",
                    "label": 0
                },
                {
                    "sent": "They're very, very fast to train.",
                    "label": 0
                },
                {
                    "sent": "It's very useful way to think about semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "In particular, I think using generative models to induce representations on unlabeled data is very useful.",
                    "label": 0
                },
                {
                    "sent": "They are absolutely invaluable as language models in speech recognition and machine translation.",
                    "label": 0
                },
                {
                    "sent": "OK, so these generative models, weather of strings or pause trees are used everywhere, and speech machine translation.",
                    "label": 0
                },
                {
                    "sent": "This final point, I've sort of put up 'cause I think this is maybe a little over emphasized.",
                    "label": 0
                },
                {
                    "sent": "Possibly a myth, I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "German models are better than discriminative models with small amounts of training data.",
                    "label": 0
                },
                {
                    "sent": "This goes back to the Andrew ING and Michael Jordan results.",
                    "label": 0
                },
                {
                    "sent": "That's very interesting paper, but they were looking at unregularized logistic regression.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty skeptical about this.",
                    "label": 0
                },
                {
                    "sent": "If you really regularizer discriminative model.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure this is true.",
                    "label": 0
                },
                {
                    "sent": "Advantage of discriminative models is very easy to incorporate new features and this for me is by far the biggest win.",
                    "label": 1
                },
                {
                    "sent": "And also something which is maybe underappreciated, that there are easy to implement and replicate this notion of smoothing independence assumptions and so on.",
                    "label": 0
                },
                {
                    "sent": "All you really need the feature definitions which go into these models, and you're there, OK, and that's.",
                    "label": 0
                },
                {
                    "sent": "That's that I think is also important.",
                    "label": 0
                },
                {
                    "sent": "OK, I think that's it.",
                    "label": 0
                },
                {
                    "sent": "So in the vision.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Areas where.",
                    "label": 0
                },
                {
                    "sent": "Models.",
                    "label": 0
                },
                {
                    "sent": "Or speech recognition for that.",
                    "label": 0
                },
                {
                    "sent": "Best is that you first train generatively and then we refine description so all the speech systems are trained that way.",
                    "label": 0
                },
                {
                    "sent": "Older animation systems, I notice that all the another problem thing is all about that supervised model and then you can refine supervised so you get the best of both worlds to some extent.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so trying the.",
                    "label": 0
                },
                {
                    "sent": "The intuition of it, so I'm guessing that might be because these models typically use hidden alignments, and if you train immediately discriminatively, you get stuck in weird local minima.",
                    "label": 0
                },
                {
                    "sent": "Or is it?",
                    "label": 0
                },
                {
                    "sent": "That might be part of it?",
                    "label": 0
                },
                {
                    "sent": "Passing number of parameters.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's no way you can strangle this parameter.",
                    "label": 0
                },
                {
                    "sent": "Launch in numbers of parameters.",
                    "label": 0
                },
                {
                    "sent": "These pausing models are using millions of parameters, 10 minutes, so we also have very large premise faces.",
                    "label": 0
                },
                {
                    "sent": "My intuition is we don't have hidden variables are easier to regularize.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think.",
                    "label": 0
                },
                {
                    "sent": "Cannot use in your Department, raised images or so you have to use.",
                    "label": 0
                },
                {
                    "sent": "Socially, models in this regard.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And there.",
                    "label": 0
                },
                {
                    "sent": "Pre training supervisor.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, there's something to that actually, speech recognition you.",
                    "label": 0
                },
                {
                    "sent": "We have some set up, translation of the model, but it just training criteria is different.",
                    "label": 1
                },
                {
                    "sent": "In this case you actually have two different sets of models runs.",
                    "label": 1
                },
                {
                    "sent": "Gender models are true, though it doesn't share any plans are.",
                    "label": 0
                },
                {
                    "sent": "It's harder to for me to imagine actually, do you do this like sleep like speech recognition away from training for maximum likelihood, become condition like?",
                    "label": 0
                },
                {
                    "sent": "I just kind of difficult to see what's going to happen that one wishes to partition whatever criteria you won't deliver, you know.",
                    "label": 0
                },
                {
                    "sent": "Umm I like to call it in my structure formation with the speech, which is the same as here.",
                    "label": 0
                },
                {
                    "sent": "I've you know, it's.",
                    "label": 0
                },
                {
                    "sent": "I could go for for maximum marginal conceptuality there's a whole bunch of functions you can use for the main difference with speech, speech and writing models is that you don't have any organization of the models you have to.",
                    "label": 0
                },
                {
                    "sent": "Speech will you'll have to learn the means and covariance matrices for the Gaussian model.",
                    "label": 0
                },
                {
                    "sent": "And those are not linearly related to the liquid, so we had to.",
                    "label": 0
                },
                {
                    "sent": "You know it's about when you go to come so many intuitions I've shown here.",
                    "label": 0
                },
                {
                    "sent": "When you go to continuous about high, relatively high dimensional continuous value data, the intuitions are very, very differently.",
                    "label": 0
                },
                {
                    "sent": "But we've done.",
                    "label": 0
                },
                {
                    "sent": "It works because people have been using this for about 20 years.",
                    "label": 0
                },
                {
                    "sent": "To worship.",
                    "label": 0
                },
                {
                    "sent": "So I actually don't understand the last trip.",
                    "label": 0
                },
                {
                    "sent": "The trip here last week and after you do something that we should understand why it works.",
                    "label": 0
                },
                {
                    "sent": "So I understand you train the model distributively and then you pass the unsupervised unlabeled data and get partial like it's using partial directors get much applause trees.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm able data and you said that we would reach way using label.",
                    "label": 0
                },
                {
                    "sent": "No, you induce these generative models, so you from these these label examples on the unlabeled data you learn.",
                    "label": 0
                },
                {
                    "sent": "100 very simple generative models of the tallest trees which models, but assuming late.",
                    "label": 0
                },
                {
                    "sent": "So it was like a coat, raincoat, conciseness you're going from a complex model, supermodel Mini Cooper.",
                    "label": 0
                },
                {
                    "sent": "But if you use down the actual Origin label data will check it out.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        }
    }
}