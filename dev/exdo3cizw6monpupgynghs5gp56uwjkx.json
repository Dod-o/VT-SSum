{
    "id": "exdo3cizw6monpupgynghs5gp56uwjkx",
    "title": "Neural Networks with Few Multiplications",
    "info": {
        "author": [
            "Zhouhan Lin, Universit\u00e9 de Montr\u00e9al"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_lin_neural_networks/",
    "segmentation": [
        [
            "Hi everyone, my name is Johan Ling.",
            "I'm from University of Montreal.",
            "I'm going to present this work neural networks with few multiplications is a joint work with material Indonesia and the great talk before just majorly focuses on compressing the neural networks and our work is majorly focusing on reducing the computation of neural networks.",
            "So I think the neural networks were like us a lot because we are making themselves skinny.",
            "So why?"
        ],
        [
            "Don't we want massive multiplications in your networks as majorly three reasons?",
            "So first, computationally multiplication of there are very expensive because you see that floating .32 takes 32 times more time to to compute the multiplication rather than floating point add.",
            "And we also wanted to distribute our system into consumer level applications or low power devices, which requires us to.",
            "Use as less power as possible.",
            "And on the other hand, we are all these issues that we know that how we suffer from waiting for days or even weeks to wait for our results to come out.",
            "And if we have a multiplier free network and we have find some way to do a hardware realization which can be computing the multiple computing the neural networks a million times faster, that's definitely a great thing to do so."
        ],
        [
            "People in the past two decades have tried various ways to deal with illuminating the multiplications, Anne.",
            "Some of them kind of reduce the computation a lot, but at the cost of reducing the model performance a lot.",
            "Some of them still need a floating .4 for precision training stage, but this managed to.",
            "Retains the test town in performance without losing too much accuracy."
        ],
        [
            "You want to doing our work is very simple.",
            "It emerged from two points first.",
            "We know that the low position is ringing in present needs to the neural network which is making the network.",
            "Is that not performing that well?",
            "But on the other hand we know that the guest City is a good thing for training like it's kind of proved that the dropout blackout, noisy gradients, noisy activation functions they are operating the casting in some way which actually reduce reduce the overfitting.",
            "Bring actual regularization and.",
            "Improve the model performance.",
            "So can we take advantage of the impreciseness and the suggested at the same time so that we if we do stochastic binarization, can we have reduced the computation and extra regular addition at the same time?"
        ],
        [
            "Yes, yes.",
            "Our approach is quite simple.",
            "It includes only two parts.",
            "1st is, we binarize the weight values and Secondly we use away called quantized backpropagation."
        ],
        [
            "Note on the 1st first part first and then the second one."
        ],
        [
            "Imagine."
        ],
        [
            "That we have a binarized.",
            "We have a weight value which is in full precision and we wanted to binarize it.",
            "What we do is we first clip the weight values.",
            "The outlier of the weight values so that the weight values will standing bonded small interval and then we use binary connect or ternary connect to binarize the weight values into two or three possible values.",
            "So in the binary connect case.",
            "There's two ways to do the binarization.",
            "Wednesday Castec ones deterministic don't look at the formulas is the intuition is really simple.",
            "It's like we keep the real valued weights in the back end, but we sampled them to be an integral value every time we want it, and there's the more likely the weight to be sampled.",
            "Say one is related to how close to one that real valued weight.",
            "Yes, and the deterministic one is even simpler.",
            "We just set a fixed threshold and if the weight value is larger than that threshold it becomes one."
        ],
        [
            "Similar thing happens for the ternary connect case for ternary Connect case.",
            "We allow the weights to be sampled as zero and we have to set the two different thresholds in the deterministic part.",
            "So in this way we kind of getting rid of the multiplications in the weight multiplication."
        ],
        [
            "The other party is quantized backpropagation.",
            "We need there's still multiplications left in the back propagation and we have to use this way to get rid of the multiplications left image."
        ],
        [
            "And we have the neural network which has a number of input an end number of output and the Delta the error signal flows from upside down.",
            "And basically what we need to do to update the weights in that layer is included in these three functions.",
            "The three equations here where massive multiplication exists in the 1st and 2nd equation here.",
            "Since we have already, binarized awaits so multiplications in the third equation will be eliminated, but there's still multiplication is left in the outer product in the first equation."
        ],
        [
            "So we need to do we need to do quantization exponential quantization mechanics to eliminate the multiplications there.",
            "What we do is that we have real valued matrix whose value distributions like that and we basically say settlement is A to be one and.",
            "We have the weight values that to be 2 to the integral.",
            "Power 2.",
            "At 2, two to the integer power of two and the in that way the multiplication is just becoming binary shifts."
        ],
        [
            "So we can do the mechanic on both the left side of the multiplication or right side one.",
            "We have to choose one of them since we don't know the actual scale of the Delta, so we don't know how each prime multiplies.",
            "Delta will be, so we have to quantize X instead.",
            "But wait, after that, there's still multiplications left in these three equations."
        ],
        [
            "The multiplication is majorly exists in the elementwise product in the second third equation.",
            "This product takes 3 * N multiplications in in total.",
            "Compared to a standard back propagation scheme which takes 2, two NN multiply 3 + 3 M multiplications.",
            "This kind of three in multiplication is is is totally illegible.",
            "Anne."
        ],
        [
            "We actually take a full layer.",
            "MLP trained on amnesty as a testbed and we compute exactly how many multiplications we need to do in that network.",
            "Both with and without batch normalization, we find that the ratio ratio to the rightmost column stands for how many multiplications left in the neural network.",
            "It's at most .4% in the worst case, assuming we're using STD.",
            "If we're using other kind of training algorithm like Adams, maybe to be doubled, but it's less than 1%, so it's completely negligible."
        ],
        [
            "Also, it's important to take care of the hidden representation scales 'cause we are quantizing the hidden representation during the quantized backpropagation.",
            "We looked at the hidden representation at each layer we find that they are quite stable.",
            "This is a snapshot we took in the middle of training and we find that the log of hidden presentation is majorly standing between minus five and two, so there's not a lot interval that they lie really lying."
        ],
        [
            "We also tried to limit the range of exponent.",
            "By doing quantizing the hidden representations, we tried to limit it as strict as only two bits.",
            "And it turns out it doesn't hurt the model performance at all.",
            "After that."
        ],
        [
            "There's a we we apply the technique on 3 middle 2.",
            "Small to medium size data set, it turns out our mechanism is improving the performance or all of these three datasets and we can see from the convergence that it is actually making the network performing better regularization and reduce the computation at the same time."
        ],
        [
            "There's some recent advances in related works going on.",
            "Mcdill has a denigrate working binarize in both weights and activations, and he has kind of implemented real time, not real time really accelerated implementation and also we're trying exponential quantization or both over both forward and backward pass, and someone else is trying.",
            "Trying this on larger, more serious datasets like image, net and we're looking for actual dedicated hardware realization.",
            "Anne, that's so much for this."
        ],
        [
            "Any questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi everyone, my name is Johan Ling.",
                    "label": 0
                },
                {
                    "sent": "I'm from University of Montreal.",
                    "label": 1
                },
                {
                    "sent": "I'm going to present this work neural networks with few multiplications is a joint work with material Indonesia and the great talk before just majorly focuses on compressing the neural networks and our work is majorly focusing on reducing the computation of neural networks.",
                    "label": 0
                },
                {
                    "sent": "So I think the neural networks were like us a lot because we are making themselves skinny.",
                    "label": 0
                },
                {
                    "sent": "So why?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't we want massive multiplications in your networks as majorly three reasons?",
                    "label": 0
                },
                {
                    "sent": "So first, computationally multiplication of there are very expensive because you see that floating .32 takes 32 times more time to to compute the multiplication rather than floating point add.",
                    "label": 0
                },
                {
                    "sent": "And we also wanted to distribute our system into consumer level applications or low power devices, which requires us to.",
                    "label": 0
                },
                {
                    "sent": "Use as less power as possible.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand, we are all these issues that we know that how we suffer from waiting for days or even weeks to wait for our results to come out.",
                    "label": 0
                },
                {
                    "sent": "And if we have a multiplier free network and we have find some way to do a hardware realization which can be computing the multiple computing the neural networks a million times faster, that's definitely a great thing to do so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People in the past two decades have tried various ways to deal with illuminating the multiplications, Anne.",
                    "label": 0
                },
                {
                    "sent": "Some of them kind of reduce the computation a lot, but at the cost of reducing the model performance a lot.",
                    "label": 0
                },
                {
                    "sent": "Some of them still need a floating .4 for precision training stage, but this managed to.",
                    "label": 0
                },
                {
                    "sent": "Retains the test town in performance without losing too much accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You want to doing our work is very simple.",
                    "label": 0
                },
                {
                    "sent": "It emerged from two points first.",
                    "label": 0
                },
                {
                    "sent": "We know that the low position is ringing in present needs to the neural network which is making the network.",
                    "label": 0
                },
                {
                    "sent": "Is that not performing that well?",
                    "label": 0
                },
                {
                    "sent": "But on the other hand we know that the guest City is a good thing for training like it's kind of proved that the dropout blackout, noisy gradients, noisy activation functions they are operating the casting in some way which actually reduce reduce the overfitting.",
                    "label": 1
                },
                {
                    "sent": "Bring actual regularization and.",
                    "label": 0
                },
                {
                    "sent": "Improve the model performance.",
                    "label": 0
                },
                {
                    "sent": "So can we take advantage of the impreciseness and the suggested at the same time so that we if we do stochastic binarization, can we have reduced the computation and extra regular addition at the same time?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Our approach is quite simple.",
                    "label": 0
                },
                {
                    "sent": "It includes only two parts.",
                    "label": 0
                },
                {
                    "sent": "1st is, we binarize the weight values and Secondly we use away called quantized backpropagation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Note on the 1st first part first and then the second one.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imagine.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we have a binarized.",
                    "label": 0
                },
                {
                    "sent": "We have a weight value which is in full precision and we wanted to binarize it.",
                    "label": 0
                },
                {
                    "sent": "What we do is we first clip the weight values.",
                    "label": 0
                },
                {
                    "sent": "The outlier of the weight values so that the weight values will standing bonded small interval and then we use binary connect or ternary connect to binarize the weight values into two or three possible values.",
                    "label": 0
                },
                {
                    "sent": "So in the binary connect case.",
                    "label": 0
                },
                {
                    "sent": "There's two ways to do the binarization.",
                    "label": 0
                },
                {
                    "sent": "Wednesday Castec ones deterministic don't look at the formulas is the intuition is really simple.",
                    "label": 0
                },
                {
                    "sent": "It's like we keep the real valued weights in the back end, but we sampled them to be an integral value every time we want it, and there's the more likely the weight to be sampled.",
                    "label": 0
                },
                {
                    "sent": "Say one is related to how close to one that real valued weight.",
                    "label": 0
                },
                {
                    "sent": "Yes, and the deterministic one is even simpler.",
                    "label": 0
                },
                {
                    "sent": "We just set a fixed threshold and if the weight value is larger than that threshold it becomes one.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar thing happens for the ternary connect case for ternary Connect case.",
                    "label": 0
                },
                {
                    "sent": "We allow the weights to be sampled as zero and we have to set the two different thresholds in the deterministic part.",
                    "label": 0
                },
                {
                    "sent": "So in this way we kind of getting rid of the multiplications in the weight multiplication.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other party is quantized backpropagation.",
                    "label": 0
                },
                {
                    "sent": "We need there's still multiplications left in the back propagation and we have to use this way to get rid of the multiplications left image.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have the neural network which has a number of input an end number of output and the Delta the error signal flows from upside down.",
                    "label": 0
                },
                {
                    "sent": "And basically what we need to do to update the weights in that layer is included in these three functions.",
                    "label": 0
                },
                {
                    "sent": "The three equations here where massive multiplication exists in the 1st and 2nd equation here.",
                    "label": 0
                },
                {
                    "sent": "Since we have already, binarized awaits so multiplications in the third equation will be eliminated, but there's still multiplication is left in the outer product in the first equation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we need to do we need to do quantization exponential quantization mechanics to eliminate the multiplications there.",
                    "label": 0
                },
                {
                    "sent": "What we do is that we have real valued matrix whose value distributions like that and we basically say settlement is A to be one and.",
                    "label": 0
                },
                {
                    "sent": "We have the weight values that to be 2 to the integral.",
                    "label": 0
                },
                {
                    "sent": "Power 2.",
                    "label": 0
                },
                {
                    "sent": "At 2, two to the integer power of two and the in that way the multiplication is just becoming binary shifts.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can do the mechanic on both the left side of the multiplication or right side one.",
                    "label": 0
                },
                {
                    "sent": "We have to choose one of them since we don't know the actual scale of the Delta, so we don't know how each prime multiplies.",
                    "label": 0
                },
                {
                    "sent": "Delta will be, so we have to quantize X instead.",
                    "label": 0
                },
                {
                    "sent": "But wait, after that, there's still multiplications left in these three equations.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The multiplication is majorly exists in the elementwise product in the second third equation.",
                    "label": 0
                },
                {
                    "sent": "This product takes 3 * N multiplications in in total.",
                    "label": 0
                },
                {
                    "sent": "Compared to a standard back propagation scheme which takes 2, two NN multiply 3 + 3 M multiplications.",
                    "label": 1
                },
                {
                    "sent": "This kind of three in multiplication is is is totally illegible.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We actually take a full layer.",
                    "label": 0
                },
                {
                    "sent": "MLP trained on amnesty as a testbed and we compute exactly how many multiplications we need to do in that network.",
                    "label": 0
                },
                {
                    "sent": "Both with and without batch normalization, we find that the ratio ratio to the rightmost column stands for how many multiplications left in the neural network.",
                    "label": 0
                },
                {
                    "sent": "It's at most .4% in the worst case, assuming we're using STD.",
                    "label": 0
                },
                {
                    "sent": "If we're using other kind of training algorithm like Adams, maybe to be doubled, but it's less than 1%, so it's completely negligible.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, it's important to take care of the hidden representation scales 'cause we are quantizing the hidden representation during the quantized backpropagation.",
                    "label": 0
                },
                {
                    "sent": "We looked at the hidden representation at each layer we find that they are quite stable.",
                    "label": 0
                },
                {
                    "sent": "This is a snapshot we took in the middle of training and we find that the log of hidden presentation is majorly standing between minus five and two, so there's not a lot interval that they lie really lying.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also tried to limit the range of exponent.",
                    "label": 0
                },
                {
                    "sent": "By doing quantizing the hidden representations, we tried to limit it as strict as only two bits.",
                    "label": 0
                },
                {
                    "sent": "And it turns out it doesn't hurt the model performance at all.",
                    "label": 0
                },
                {
                    "sent": "After that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a we we apply the technique on 3 middle 2.",
                    "label": 0
                },
                {
                    "sent": "Small to medium size data set, it turns out our mechanism is improving the performance or all of these three datasets and we can see from the convergence that it is actually making the network performing better regularization and reduce the computation at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's some recent advances in related works going on.",
                    "label": 0
                },
                {
                    "sent": "Mcdill has a denigrate working binarize in both weights and activations, and he has kind of implemented real time, not real time really accelerated implementation and also we're trying exponential quantization or both over both forward and backward pass, and someone else is trying.",
                    "label": 0
                },
                {
                    "sent": "Trying this on larger, more serious datasets like image, net and we're looking for actual dedicated hardware realization.",
                    "label": 0
                },
                {
                    "sent": "Anne, that's so much for this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions?",
                    "label": 0
                }
            ]
        }
    }
}