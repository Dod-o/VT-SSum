{
    "id": "udyywnoeoqklbrfht33alwcx6mc6jnsy",
    "title": "Multi-Task Compressive Sensing with Dirichlet Process Priors",
    "info": {
        "author": [
            "Lawrence Carin, Department of Electrical and Computer Engineering, Duke University"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning->Dirichlet Processes"
        ]
    },
    "url": "http://videolectures.net/icml08_carin_mtcs/",
    "segmentation": [
        [
            "OK, thank you and good morning.",
            "OK so.",
            "So I'm going to talk about.",
            "A relatively new new field.",
            "Compressive sensing.",
            "And applying nonparametric techniques dearsley process."
        ],
        [
            "To this.",
            "OK, so first of all, first of all let me just point out that tomorrow there's going to be a workshop UI workshop and Rich Baraniuk is going to be there.",
            "Who's one of the key guys?",
            "So if you can come would be good to come and learn about compressive sensing, but in lieu of that, if you can't come let me tell you a little bit about it.",
            "So compressive sensing is.",
            "Motivated by a pretty simple idea, if you if you take a if you take an image like this and you represent it in a wavelet domain and I think most of you know that wavelets are responsible for JPEG 2000, the state of the art compression technology, you notice that in this domain where you see black, that means that the coefficients are approximately 0, and so if you throw away all the black coefficients, which means that you can throw away.",
            "Quite a bit of data and then you do an inverse wavelet transform.",
            "You can reconstruct the image essentially perfectly, and in fact it doesn't have to be a wavelet transform, it can be a DCT as well, which is for JPEG.",
            "But the idea is that you take a picture or in other words you do some sensing.",
            "You do a transform through a DCT or wavelet transform or whatever transform you want.",
            "You throw away all of the small coefficients and then you do an inverse transform.",
            "You recover the signal.",
            "And so the this is this.",
            "This underpins everything we do and transform coding today.",
            "It's it's very important technology, but there's one aspect of this that was bothersome to people, which is that if you're going to take an image and you're going to take a transform of that image, and then you're going to throw away 80% of the data, it seems very wasteful for you to have taken all of the data in the 1st place, right?",
            "Because in many applications it's expensive to take measurements, so in some applications it is literally expensive.",
            "In other words, you have a battery an you have a airborne UAV with with batteries and it has.",
            "You know you've got finite lifetime.",
            "You want to do is minimal sensing is possible, but one of the so called killer apps of compressive sensing is an application for which you don't really have any energy constraints.",
            "It's kind of an unusual constraint, and that is that if you take an MRI, so people, if you ever had an MRI you have to go into a tube.",
            "And this is to be this system is plugged into the wall, so there's no energy issues.",
            "But people don't like being in that tube and so therefore if you can get them out of the tube as soon as possible by taking fewer measurements, that would be a good thing.",
            "So therefore, compressive sensing is a technology that is appropriate anytime for which you want to take a few measurements as possible, and so MRI."
        ],
        [
            "It turns out to be a really good application of this, and it turns out that medical applications were kind of the the place where compressive sensing was was discovered, so, so the idea is that if you take if you take an image and you represent that image in the Fourier domain.",
            "And in the two dimensional Fourier domain.",
            "An you throw away 83% of the Fourier coefficients.",
            "And then you do an inverse.",
            "Well, you do an L1 inverted reconstruction.",
            "It turns out that this this reconstruction is not good, it's perfect.",
            "In other words, you get perfect.",
            "Reconstruction is probably perfect, so this is this is a remarkable result.",
            "And So what happened was my understanding is that some engineers and people like that this kind of ran into this, and then they went and talked to people like Emmanuel Candes and Terence Tao and David Donoho.",
            "And so this seems really odd, and then that was how the whole field of compressive sensing was founded.",
            "OK, so so the idea in compressive sensing is the following, so I don't have a whole lot of time, so I'll try to do this quickly.",
            "So The thing is, is that this it turns out that if you take the data represented in the 48 domain and then you randomly.",
            "Retain 17% of the coefficients, and then you do an L1 reconstruction.",
            "You get perfect reconstruction, So what that tells you is that you really only have to measure 17% of the data in the Fourier domain, and you do that totally randomly, which 40 components you choose are selected totally randomly, and the neat thing is that when you do measurements like this, the measurement you take is a 48 measurement.",
            "That's the natural measurement to take, so this is really a nice."
        ],
        [
            "But then what happened was David Donoho, Candes and Tao and others recognize that this is really a more general, more general thing.",
            "So let me now try to explain this to assume that the signal of interest to you is you.",
            "And it is compressible in some basis.",
            "OK, it turns out that this signal is compressible in the original pixel space because all of these guys are zero.",
            "So this happened to be a really lucky example because in the pixel space it's sparse, OK, but they recognize that this is a more general principle, and that is if you take any signal and you represent it in an orthonormal basis, which is what this PSI is.",
            "And Theta corresponds to the coefficients of that transform, and the idea is that in many transforms and for many signals Theta is sparse.",
            "And it turns out that the way to take a compressive sensing measurement is is that instead of measuring you, we're going to measure something called V. And the way that we're going to do that, the measurements is by selecting the components of the matrix T totally randomly.",
            "And it really seems like kind of a weird thing that you would do that randomly, but it's actually probably the optimal thing to do.",
            "So the idea is that this is going to be a projection matrix totally constituted totally randomly.",
            "And so therefore, if you remember that you can be represented in this way.",
            "Then then, the compressive sensing measurement looks like this, and So what we do is we measure V. Theta is what we really want.",
            "We know that this is sparse.",
            "An fi is a projection matrix which is a combination of the basis in which were sparse projected onto random matrix."
        ],
        [
            "And so it turns out that you can prove that if you take enough, so the dimensionality of that is, and this is the number of compressive sensing measurements that we take.",
            "It turns out that you can prove that if the number of compressive sensing measurements an satisfies this expression, where N corresponds to the number of non zero coefficients in the transform, an M corresponds to the dimensionality of the data.",
            "Then if the number of compressive sensing measurements N satisfies this equation, then this inversion is not good.",
            "This inversion is exactly so that this is an amazing thing.",
            "You get perfect reconstruction.",
            "OK, so and so if you look at this log M even for very large M log M is now going to be fairly small and consequently the number of compressive sensing measurements you have to take N is actually."
        ],
        [
            "Small and so.",
            "So what we we came into this was is that if you if you look at this problem.",
            "This is really a linear regression problem.",
            "V is the data that you're going to measure.",
            "Theta are the sparse set of coefficients that we want that characterize the signal of interest fee.",
            "Is that projection phenomenon that matrix that I talked about and in reality and in real problems the signal is not exactly sparse?",
            "In other words, I do everything I just said, assume that the signal was sparse.",
            "But actually it's not sparse.",
            "It's nearly sparse, and consequently there's an error that it is manifested.",
            "But if you look at this, this is exactly linear regression with a sparseness prior.",
            "And this is something that people in the machine learning community know a lot about, right we we can, we can impose our belief, and so we call this BCFBCS Bayesian compressive sensing.",
            "We impose our belief that the coefficients Theta, which we don't know.",
            "Are drawn from zero mean Gaussian with precision.",
            "AA is A is a vector.",
            "And then we're going to impose the belief that Theta is sparse, and so therefore we're going to put a gamma prior on this, and so the combination of that gamma prior with that with Gaussian yields a student T distribution.",
            "An we Alternatively could use an inverse gamma here and then we would get a double double exponential or Laplace prior for Theta."
        ],
        [
            "OK, so So what we what we have done with what we call multi task compressive sensing is the following whenever."
        ],
        [
            "Are you?",
            "I working in a radiology Department.",
            "You don't take one image a day and then go home.",
            "You gotta take a lot of images and so the idea that you would do inverse compressive sensing on every image one by one really doesn't make sense.",
            "You have a lot of data and if you could do this in a multitask setting, which of course is something we do in machine learning.",
            "We should."
        ],
        [
            "To be able to improve our ability to do CSS inversion.",
            "In other words, the number of measurements we have to take should go down, even even."
        ],
        [
            "Further and so, the way that we do this is.",
            "Is using their sleep process type framework, so now what we're going to do is we're going to say that V sub I is the compressive sensing measurement that I take for the I TH.",
            "Measurement fysa by is the projections that I take for the Earth measurement and Theta.",
            "I are sparse coefficients for the I TH.",
            "I've data which of course my objective is to try to find Theta I."
        ],
        [
            "And so now what we're going to do is we're going to again assume that the parameters of the transform the transform coefficients are going to be drawn from a zero mean Gaussian like we had before, but now the precision Alpha is going to be drawn from a distribution G, and that G will be drawn from a DP.",
            "And So what this is saying?",
            "So if you don't understand DP, I'm not going to really be able to teach it to you now.",
            "But what we're trying to do now is impose the belief that many different samples from our multiple data samples are going to share the same type of sparseness properties, and the deer say process is going to figure that out."
        ],
        [
            "I'm for us OK, and so the base measure that we use in RDP.",
            "Is this a product, a product of gammas and consequently we marginalise out those that precision?",
            "This is basically a student T, so again, I'm not going to really be able to.",
            "Teach you what with DP is if you don't already know, but the DP is going to be doing is it is going to be simultaneously clustering the data and inferring the underlying spa."
        ],
        [
            "Richness simultaneously and so this is the model.",
            "So I suggest you just read the paper 'cause it's a little bit involved.",
            "The nice thing here is that every step in this model."
        ],
        [
            "Is in the conjugate exponential family, which means that we can do efficient inference.",
            "In other words, variational Bayes inference.",
            "So that's another thing, of course that comes from this."
        ],
        [
            "Machine learning community.",
            "So the main thing I guess I wanted to say in this talk is is that the compressive sensing is a really important field and machine learning.",
            "He's got a lot of tools that are useful for.",
            "For compressive sensing, so now let me let me show you an example.",
            "So let's assume that I have 10.",
            "I have 10 sparse signals, so these are 10 sparse signals.",
            "And what I'm going to do is for each one of these 10 sparse signals, I'm going to generate randomly 5 sparse signals which basically look the same as these with some random perturbation.",
            "OK, so I'm going to have a total of 50 sparse measurements, each one of which is sparse.",
            "What I expect to uncover.",
            "Is that there are 10 underlying clusters of."
        ],
        [
            "Of data, so that's what this is.",
            "So what we're looking at here is our posterior belief on the number of clusters in the 50 measurements, and so remember, I made the data to have 10 and what you're looking at on the right hand side as a function of the number of compressive sensing measurements.",
            "I take a hundred 110 up to 140.",
            "How good I am at inferring.",
            "The underlying number of clusters in the data.",
            "And then what you're looking at here is the reconstruction accuracy as a function of the number of compressive sensing measurements, and So what this what this result is?",
            "This is of course the result of the daresay process, and there's a. I constituted that data randomly, so the error bars represent variation relative to that those random draws.",
            "And what this is, is what you would get if you did not use DP.",
            "An you just basically clustered all of that data together.",
            "In other words, you just assume that it all belong together and you do very poorly.",
            "This is percent reconstruction accuracy, so the thing that we're we're doing here is by using the DP.",
            "The number of CS measurements we need to take is reduced even further relative to the Candes Donoho theory, and Moreover.",
            "We are able to uncover the underlying number of different types of clusters in the data."
        ],
        [
            "And so then what we can do?"
        ],
        [
            "Is.",
            "We"
        ],
        [
            "Instead of taking ten of these, I can take five of them, or 4.",
            "Then I can reduce the number of clusters."
        ],
        [
            "See how well we could."
        ],
        [
            "Do and So what you're looking at here is that we have 5 underlying clusters, three to an and one.",
            "So the thing that we so the couple of things to notice is that we're able to infer as the number of underlying clusters changes were able to infer that that through this posterior.",
            "But the thing to notice is that as the number of underlying clusters.",
            "Of different types of sparseness, fenomena decreases down to 3, two and of course one the DP compressive sensing.",
            "Compared to just using a much simpler multitask setting, which I guess I haven't really talked about, but it's in the paper, the difference is negligible.",
            "So what this tells us is that the utility of DP is most most evident when the number of underlying clusters or different types of sparsest phenomenon is relatively large."
        ],
        [
            "And so the question is why?",
            "Why is that we were a little bit surprised about this.",
            "So let's let's assume that you have two underlying types of sparseness, so these guys are completely decorrelated.",
            "If you look at where the non zero coefficients are, they're completely decorrelated.",
            "OK, so therefore you would think that it would not make sense to cluster these together.",
            "However, these are so sparse in and of themselves, even whenever you cluster them together, the signal is still very sparse."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you and good morning.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "A relatively new new field.",
                    "label": 0
                },
                {
                    "sent": "Compressive sensing.",
                    "label": 0
                },
                {
                    "sent": "And applying nonparametric techniques dearsley process.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To this.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, first of all let me just point out that tomorrow there's going to be a workshop UI workshop and Rich Baraniuk is going to be there.",
                    "label": 0
                },
                {
                    "sent": "Who's one of the key guys?",
                    "label": 0
                },
                {
                    "sent": "So if you can come would be good to come and learn about compressive sensing, but in lieu of that, if you can't come let me tell you a little bit about it.",
                    "label": 0
                },
                {
                    "sent": "So compressive sensing is.",
                    "label": 0
                },
                {
                    "sent": "Motivated by a pretty simple idea, if you if you take a if you take an image like this and you represent it in a wavelet domain and I think most of you know that wavelets are responsible for JPEG 2000, the state of the art compression technology, you notice that in this domain where you see black, that means that the coefficients are approximately 0, and so if you throw away all the black coefficients, which means that you can throw away.",
                    "label": 0
                },
                {
                    "sent": "Quite a bit of data and then you do an inverse wavelet transform.",
                    "label": 0
                },
                {
                    "sent": "You can reconstruct the image essentially perfectly, and in fact it doesn't have to be a wavelet transform, it can be a DCT as well, which is for JPEG.",
                    "label": 0
                },
                {
                    "sent": "But the idea is that you take a picture or in other words you do some sensing.",
                    "label": 0
                },
                {
                    "sent": "You do a transform through a DCT or wavelet transform or whatever transform you want.",
                    "label": 0
                },
                {
                    "sent": "You throw away all of the small coefficients and then you do an inverse transform.",
                    "label": 1
                },
                {
                    "sent": "You recover the signal.",
                    "label": 0
                },
                {
                    "sent": "And so the this is this.",
                    "label": 0
                },
                {
                    "sent": "This underpins everything we do and transform coding today.",
                    "label": 0
                },
                {
                    "sent": "It's it's very important technology, but there's one aspect of this that was bothersome to people, which is that if you're going to take an image and you're going to take a transform of that image, and then you're going to throw away 80% of the data, it seems very wasteful for you to have taken all of the data in the 1st place, right?",
                    "label": 1
                },
                {
                    "sent": "Because in many applications it's expensive to take measurements, so in some applications it is literally expensive.",
                    "label": 0
                },
                {
                    "sent": "In other words, you have a battery an you have a airborne UAV with with batteries and it has.",
                    "label": 0
                },
                {
                    "sent": "You know you've got finite lifetime.",
                    "label": 0
                },
                {
                    "sent": "You want to do is minimal sensing is possible, but one of the so called killer apps of compressive sensing is an application for which you don't really have any energy constraints.",
                    "label": 0
                },
                {
                    "sent": "It's kind of an unusual constraint, and that is that if you take an MRI, so people, if you ever had an MRI you have to go into a tube.",
                    "label": 0
                },
                {
                    "sent": "And this is to be this system is plugged into the wall, so there's no energy issues.",
                    "label": 0
                },
                {
                    "sent": "But people don't like being in that tube and so therefore if you can get them out of the tube as soon as possible by taking fewer measurements, that would be a good thing.",
                    "label": 0
                },
                {
                    "sent": "So therefore, compressive sensing is a technology that is appropriate anytime for which you want to take a few measurements as possible, and so MRI.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It turns out to be a really good application of this, and it turns out that medical applications were kind of the the place where compressive sensing was was discovered, so, so the idea is that if you take if you take an image and you represent that image in the Fourier domain.",
                    "label": 0
                },
                {
                    "sent": "And in the two dimensional Fourier domain.",
                    "label": 0
                },
                {
                    "sent": "An you throw away 83% of the Fourier coefficients.",
                    "label": 1
                },
                {
                    "sent": "And then you do an inverse.",
                    "label": 0
                },
                {
                    "sent": "Well, you do an L1 inverted reconstruction.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this this reconstruction is not good, it's perfect.",
                    "label": 0
                },
                {
                    "sent": "In other words, you get perfect.",
                    "label": 0
                },
                {
                    "sent": "Reconstruction is probably perfect, so this is this is a remarkable result.",
                    "label": 0
                },
                {
                    "sent": "And So what happened was my understanding is that some engineers and people like that this kind of ran into this, and then they went and talked to people like Emmanuel Candes and Terence Tao and David Donoho.",
                    "label": 0
                },
                {
                    "sent": "And so this seems really odd, and then that was how the whole field of compressive sensing was founded.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the idea in compressive sensing is the following, so I don't have a whole lot of time, so I'll try to do this quickly.",
                    "label": 0
                },
                {
                    "sent": "So The thing is, is that this it turns out that if you take the data represented in the 48 domain and then you randomly.",
                    "label": 0
                },
                {
                    "sent": "Retain 17% of the coefficients, and then you do an L1 reconstruction.",
                    "label": 0
                },
                {
                    "sent": "You get perfect reconstruction, So what that tells you is that you really only have to measure 17% of the data in the Fourier domain, and you do that totally randomly, which 40 components you choose are selected totally randomly, and the neat thing is that when you do measurements like this, the measurement you take is a 48 measurement.",
                    "label": 0
                },
                {
                    "sent": "That's the natural measurement to take, so this is really a nice.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But then what happened was David Donoho, Candes and Tao and others recognize that this is really a more general, more general thing.",
                    "label": 0
                },
                {
                    "sent": "So let me now try to explain this to assume that the signal of interest to you is you.",
                    "label": 0
                },
                {
                    "sent": "And it is compressible in some basis.",
                    "label": 0
                },
                {
                    "sent": "OK, it turns out that this signal is compressible in the original pixel space because all of these guys are zero.",
                    "label": 0
                },
                {
                    "sent": "So this happened to be a really lucky example because in the pixel space it's sparse, OK, but they recognize that this is a more general principle, and that is if you take any signal and you represent it in an orthonormal basis, which is what this PSI is.",
                    "label": 0
                },
                {
                    "sent": "And Theta corresponds to the coefficients of that transform, and the idea is that in many transforms and for many signals Theta is sparse.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the way to take a compressive sensing measurement is is that instead of measuring you, we're going to measure something called V. And the way that we're going to do that, the measurements is by selecting the components of the matrix T totally randomly.",
                    "label": 0
                },
                {
                    "sent": "And it really seems like kind of a weird thing that you would do that randomly, but it's actually probably the optimal thing to do.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that this is going to be a projection matrix totally constituted totally randomly.",
                    "label": 0
                },
                {
                    "sent": "And so therefore, if you remember that you can be represented in this way.",
                    "label": 0
                },
                {
                    "sent": "Then then, the compressive sensing measurement looks like this, and So what we do is we measure V. Theta is what we really want.",
                    "label": 1
                },
                {
                    "sent": "We know that this is sparse.",
                    "label": 0
                },
                {
                    "sent": "An fi is a projection matrix which is a combination of the basis in which were sparse projected onto random matrix.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so it turns out that you can prove that if you take enough, so the dimensionality of that is, and this is the number of compressive sensing measurements that we take.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can prove that if the number of compressive sensing measurements an satisfies this expression, where N corresponds to the number of non zero coefficients in the transform, an M corresponds to the dimensionality of the data.",
                    "label": 1
                },
                {
                    "sent": "Then if the number of compressive sensing measurements N satisfies this equation, then this inversion is not good.",
                    "label": 0
                },
                {
                    "sent": "This inversion is exactly so that this is an amazing thing.",
                    "label": 0
                },
                {
                    "sent": "You get perfect reconstruction.",
                    "label": 0
                },
                {
                    "sent": "OK, so and so if you look at this log M even for very large M log M is now going to be fairly small and consequently the number of compressive sensing measurements you have to take N is actually.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Small and so.",
                    "label": 0
                },
                {
                    "sent": "So what we we came into this was is that if you if you look at this problem.",
                    "label": 0
                },
                {
                    "sent": "This is really a linear regression problem.",
                    "label": 0
                },
                {
                    "sent": "V is the data that you're going to measure.",
                    "label": 0
                },
                {
                    "sent": "Theta are the sparse set of coefficients that we want that characterize the signal of interest fee.",
                    "label": 0
                },
                {
                    "sent": "Is that projection phenomenon that matrix that I talked about and in reality and in real problems the signal is not exactly sparse?",
                    "label": 0
                },
                {
                    "sent": "In other words, I do everything I just said, assume that the signal was sparse.",
                    "label": 0
                },
                {
                    "sent": "But actually it's not sparse.",
                    "label": 0
                },
                {
                    "sent": "It's nearly sparse, and consequently there's an error that it is manifested.",
                    "label": 0
                },
                {
                    "sent": "But if you look at this, this is exactly linear regression with a sparseness prior.",
                    "label": 0
                },
                {
                    "sent": "And this is something that people in the machine learning community know a lot about, right we we can, we can impose our belief, and so we call this BCFBCS Bayesian compressive sensing.",
                    "label": 0
                },
                {
                    "sent": "We impose our belief that the coefficients Theta, which we don't know.",
                    "label": 0
                },
                {
                    "sent": "Are drawn from zero mean Gaussian with precision.",
                    "label": 0
                },
                {
                    "sent": "AA is A is a vector.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to impose the belief that Theta is sparse, and so therefore we're going to put a gamma prior on this, and so the combination of that gamma prior with that with Gaussian yields a student T distribution.",
                    "label": 0
                },
                {
                    "sent": "An we Alternatively could use an inverse gamma here and then we would get a double double exponential or Laplace prior for Theta.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so So what we what we have done with what we call multi task compressive sensing is the following whenever.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are you?",
                    "label": 0
                },
                {
                    "sent": "I working in a radiology Department.",
                    "label": 0
                },
                {
                    "sent": "You don't take one image a day and then go home.",
                    "label": 0
                },
                {
                    "sent": "You gotta take a lot of images and so the idea that you would do inverse compressive sensing on every image one by one really doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of data and if you could do this in a multitask setting, which of course is something we do in machine learning.",
                    "label": 0
                },
                {
                    "sent": "We should.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be able to improve our ability to do CSS inversion.",
                    "label": 0
                },
                {
                    "sent": "In other words, the number of measurements we have to take should go down, even even.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Further and so, the way that we do this is.",
                    "label": 0
                },
                {
                    "sent": "Is using their sleep process type framework, so now what we're going to do is we're going to say that V sub I is the compressive sensing measurement that I take for the I TH.",
                    "label": 0
                },
                {
                    "sent": "Measurement fysa by is the projections that I take for the Earth measurement and Theta.",
                    "label": 0
                },
                {
                    "sent": "I are sparse coefficients for the I TH.",
                    "label": 0
                },
                {
                    "sent": "I've data which of course my objective is to try to find Theta I.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so now what we're going to do is we're going to again assume that the parameters of the transform the transform coefficients are going to be drawn from a zero mean Gaussian like we had before, but now the precision Alpha is going to be drawn from a distribution G, and that G will be drawn from a DP.",
                    "label": 0
                },
                {
                    "sent": "And So what this is saying?",
                    "label": 0
                },
                {
                    "sent": "So if you don't understand DP, I'm not going to really be able to teach it to you now.",
                    "label": 0
                },
                {
                    "sent": "But what we're trying to do now is impose the belief that many different samples from our multiple data samples are going to share the same type of sparseness properties, and the deer say process is going to figure that out.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm for us OK, and so the base measure that we use in RDP.",
                    "label": 0
                },
                {
                    "sent": "Is this a product, a product of gammas and consequently we marginalise out those that precision?",
                    "label": 0
                },
                {
                    "sent": "This is basically a student T, so again, I'm not going to really be able to.",
                    "label": 0
                },
                {
                    "sent": "Teach you what with DP is if you don't already know, but the DP is going to be doing is it is going to be simultaneously clustering the data and inferring the underlying spa.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Richness simultaneously and so this is the model.",
                    "label": 0
                },
                {
                    "sent": "So I suggest you just read the paper 'cause it's a little bit involved.",
                    "label": 0
                },
                {
                    "sent": "The nice thing here is that every step in this model.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is in the conjugate exponential family, which means that we can do efficient inference.",
                    "label": 0
                },
                {
                    "sent": "In other words, variational Bayes inference.",
                    "label": 0
                },
                {
                    "sent": "So that's another thing, of course that comes from this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Machine learning community.",
                    "label": 0
                },
                {
                    "sent": "So the main thing I guess I wanted to say in this talk is is that the compressive sensing is a really important field and machine learning.",
                    "label": 0
                },
                {
                    "sent": "He's got a lot of tools that are useful for.",
                    "label": 0
                },
                {
                    "sent": "For compressive sensing, so now let me let me show you an example.",
                    "label": 0
                },
                {
                    "sent": "So let's assume that I have 10.",
                    "label": 0
                },
                {
                    "sent": "I have 10 sparse signals, so these are 10 sparse signals.",
                    "label": 1
                },
                {
                    "sent": "And what I'm going to do is for each one of these 10 sparse signals, I'm going to generate randomly 5 sparse signals which basically look the same as these with some random perturbation.",
                    "label": 1
                },
                {
                    "sent": "OK, so I'm going to have a total of 50 sparse measurements, each one of which is sparse.",
                    "label": 0
                },
                {
                    "sent": "What I expect to uncover.",
                    "label": 0
                },
                {
                    "sent": "Is that there are 10 underlying clusters of.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of data, so that's what this is.",
                    "label": 0
                },
                {
                    "sent": "So what we're looking at here is our posterior belief on the number of clusters in the 50 measurements, and so remember, I made the data to have 10 and what you're looking at on the right hand side as a function of the number of compressive sensing measurements.",
                    "label": 0
                },
                {
                    "sent": "I take a hundred 110 up to 140.",
                    "label": 0
                },
                {
                    "sent": "How good I am at inferring.",
                    "label": 0
                },
                {
                    "sent": "The underlying number of clusters in the data.",
                    "label": 1
                },
                {
                    "sent": "And then what you're looking at here is the reconstruction accuracy as a function of the number of compressive sensing measurements, and So what this what this result is?",
                    "label": 0
                },
                {
                    "sent": "This is of course the result of the daresay process, and there's a. I constituted that data randomly, so the error bars represent variation relative to that those random draws.",
                    "label": 0
                },
                {
                    "sent": "And what this is, is what you would get if you did not use DP.",
                    "label": 0
                },
                {
                    "sent": "An you just basically clustered all of that data together.",
                    "label": 0
                },
                {
                    "sent": "In other words, you just assume that it all belong together and you do very poorly.",
                    "label": 0
                },
                {
                    "sent": "This is percent reconstruction accuracy, so the thing that we're we're doing here is by using the DP.",
                    "label": 0
                },
                {
                    "sent": "The number of CS measurements we need to take is reduced even further relative to the Candes Donoho theory, and Moreover.",
                    "label": 0
                },
                {
                    "sent": "We are able to uncover the underlying number of different types of clusters in the data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so then what we can do?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of taking ten of these, I can take five of them, or 4.",
                    "label": 0
                },
                {
                    "sent": "Then I can reduce the number of clusters.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See how well we could.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do and So what you're looking at here is that we have 5 underlying clusters, three to an and one.",
                    "label": 1
                },
                {
                    "sent": "So the thing that we so the couple of things to notice is that we're able to infer as the number of underlying clusters changes were able to infer that that through this posterior.",
                    "label": 0
                },
                {
                    "sent": "But the thing to notice is that as the number of underlying clusters.",
                    "label": 1
                },
                {
                    "sent": "Of different types of sparseness, fenomena decreases down to 3, two and of course one the DP compressive sensing.",
                    "label": 0
                },
                {
                    "sent": "Compared to just using a much simpler multitask setting, which I guess I haven't really talked about, but it's in the paper, the difference is negligible.",
                    "label": 0
                },
                {
                    "sent": "So what this tells us is that the utility of DP is most most evident when the number of underlying clusters or different types of sparsest phenomenon is relatively large.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the question is why?",
                    "label": 0
                },
                {
                    "sent": "Why is that we were a little bit surprised about this.",
                    "label": 0
                },
                {
                    "sent": "So let's let's assume that you have two underlying types of sparseness, so these guys are completely decorrelated.",
                    "label": 0
                },
                {
                    "sent": "If you look at where the non zero coefficients are, they're completely decorrelated.",
                    "label": 0
                },
                {
                    "sent": "OK, so therefore you would think that it would not make sense to cluster these together.",
                    "label": 0
                },
                {
                    "sent": "However, these are so sparse in and of themselves, even whenever you cluster them together, the signal is still very sparse.",
                    "label": 0
                }
            ]
        }
    }
}