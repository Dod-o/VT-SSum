{
    "id": "txfd2mwfgqelsc4bii4c6gqhbg2zjc5g",
    "title": "Large Linear Classification When Data Cannot Fit In Memory",
    "info": {
        "author": [
            "Hsiang-Fu Yu, Department of Computer Science and Information Engineering, National Taiwan University"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/kdd2010_yu_llcw/",
    "segmentation": [
        [
            "OK, hi everyone, I'm shamefully I come on National Taiwan University and the topic I want to talk it is about large linear classification.",
            "When data cannot fit in memory and this is joint work with CJ shared her boots on and see their link."
        ],
        [
            "This is today's online.",
            "I will give some introduction of our work and block nization framework for linear SVM and some implementation for SVN under our framework and some techniques to reduce training time and functionality.",
            "We can support the under our framework and some experiment and conclusion."
        ],
        [
            "So we know linear classification is a popular research topic.",
            "Recently bilinear here, we mean that we do not impose nonlinear kernel.",
            "Although accuracy by linear may not be as good as nonlinear, but for some problems such as documented vacation accurus by linear is as good as nonlinear.",
            "In addition, it also enjoy faster training and testing speed.",
            "So in this talk will focus on the large linear classification."
        ],
        [
            "Existing approaches for large any linear classification can be categorized into two types.",
            "For large data, which can still fit in memory, loss of vision methods are well developed, but for data larger than this size we don't need a distributed solution for it.",
            "So here comes our question, can we have something in there between a single setting for the data larger than memory but smaller than disk?",
            "Actually, some have proposed a method to handle this situation, but their approach can only handle data with small number of features."
        ],
        [
            "Here we see an example experiment to see the performance of typical linear solvers under situation that data cannot fit in memory.",
            "We run a typical linear solve linear on a machine with only one GB memory, and here is the result where the X axis is the size data size in gigabytes and Y axis is the time in seconds.",
            "And this vertical line indicate the real.",
            "Available memory capacity and you can see that when the data size approaching the memory capacity, the curve becomes deeper.",
            "This is because this what throw down the training speed."
        ],
        [
            "So our goal is to construct large linear classifiers for ordinary users on a single machine, such as your laptop.",
            "We have some assumptions we assume that are larger than memory, but smaller than disk, and we also assume that submarine causes low accuracy and we have one requirement is that our solution must be simple so that we can support lots of functionality such as multiclass classification, parameter selection and so on."
        ],
        [
            "In our work we model the training time into two parts.",
            "Time to check in memory data and time to access data from disk, and we pay more attention to the second part, actually decode the loading time at Dominator training procedure.",
            "So even that half can fit in memory.",
            "Second part is already an issue.",
            "Here we see an example.",
            "We run linear on a large data set as if you are, which contains hope more than half millions of.",
            "Documents you can see that it takes larger than one minute to load or data, say while less than 5 seconds to computing the model."
        ],
        [
            "So under our assumption and requirement for available method, it must satisfy the following conditions.",
            "First, each optimization step is a continuous chunk of training data.",
            "To avoid random access on a disk, second optimization procedure must converge.",
            "Is even each time only a subset of data are available?",
            "3rd, the number of optimizations that cannot be too large.",
            "Otherwise we may read data from disk too many times."
        ],
        [
            "So in our work we can see the SVM as a linear classifier even though our method can be easily applied on logic regression.",
            "In linear SVM we are given a set of training pair ysi.",
            "Each XI is a visual vector in on dimensional space and each Y is the label could be plus or minus one.",
            "Where is the number of features and Elo is a number of data.",
            "And there are two types of SVM, primal SVM, softly optimization problem and the variable of the primaries.",
            "VM is W, which has the same dimensional as the number of features the other phone is still as VM itself.",
            "This optimization method, where the East is a vector of ones and Q is the metrics we sell like this and the dual variable of Alpha as a dimensional hello.",
            "Therefore, each do available after I is corresponding to a facial feature vector XI.",
            "This is an important property we will use later."
        ],
        [
            "So this is our blocking Edition framework for linear SVM, first with three data, one to allow to a partition.",
            "We want to be at such that each block of data can fit in memory, and we store data into M file accordingly, who at the end is the number of blocks.",
            "OK, then we set initial value for Alpha or W, then for each outer iteration we sequentially load data from box 1 by 1.",
            "OK, at each inner iteration.",
            "Why is the product of the data are loaded?",
            "We let conduct some operation on each.",
            "An update of our double content and here we do not specify operation for each block here.",
            "So we can apply another operation later."
        ],
        [
            "So as we know, documentation is a classical optimization method which usually choose a block available to update at each time and is widely used in nonlinear SVM with recorded decomposition method.",
            "However, in our framework we need a connection between a block of data and block of variables.",
            "In the situation lab, data larger than memory.",
            "To avoid random Ness and disk, we cannot use holistic method to select block variable because at each time only a block of data from a fixed partition is available in memory."
        ],
        [
            "So here's some issue about our framework.",
            "We need to decide the number of blocks.",
            "Actually, if we assume oblock have similar size, the lender number of bugs is inversely proportional to the block size here.",
            "So in terms of block size, it cannot be too large because each block must fit in memory.",
            "But on the other hand it cannot be too small.",
            "Actually, we think that should be as large as possible.",
            "And our analysis is follow.",
            "We first met the total time for an alteration is likely.",
            "The T&B is a time cost of 1.",
            "Email iteration in memory and a TDB is a tank cost of reading block over data from disk and both T&B&T DVR functions of block size.",
            "Then"
        ],
        [
            "In a pest analysis, let usually consider T&B only there for the total time we can like this TN B * 8 / B.",
            "Then because the growth rate over T&B is usually more linear to the block size, therefore previous SVN work suggests that smaller block sizes beta.",
            "But now we take TDB into consideration and each file opening will cause some initial cost like this.",
            "Therefore total reading time will become like the initial cost times at all over block size and a constant time.",
            "Here you can clearly see that larger block size will be better, and actually we've done some some experiment to justify this translator."
        ],
        [
            "Here's our implementation for linear."
        ],
        [
            "Yeah, first we consider deal SVM.",
            "Let FRB radio function like this, then recall the probability overdue SVM that each block available correspond to a block of data level one super block of data already in memory.",
            "We can consult some problem like this, where the.",
            "Bob Bob is a country Mantova Vijay and found this contract.",
            "You can see that only variable corresponding to the block busy can be changed in this problem, and although the subproblem involves all data, but we can handle it by some technique and we omitted details here."
        ],
        [
            "OK, now you see the implementation for Adidas VM at least.",
            "Algorithm Two is a special case of algorithm one and only difference from algorithm.",
            "Once this tab B as Dev C, once a block of data are loaded, we can approximate this software South problem an update and obtain a temporary solution.",
            "Deep learning update Alpha accordingly.",
            "But"
        ],
        [
            "The flock of data are in memory, so any bank contract can be applied to solve some problem.",
            "And here we can see the Libor linear which is according a decent measure.",
            "This made our implementation and two level block nization and the two labeled concept is used in some algorithms before, but ours might be the first.",
            "Consider the inner level in memory while outer level in the disk.",
            "But actually, in practice we usually obtain an approximate solution for each subproblem, therefore thus stopping criteria for each subproblem must be carefully designed to ensure the global convergence of the whole algorithms and we."
        ],
        [
            "Opposed to stopping condition for sub problem.",
            "That's why we can fix the number of passes to variable in the block.",
            "Busy for example we can set a number of tests become 123 and so on.",
            "The only problem is that we need to decide a proper number, the other.",
            "Is gradient best having competition which made us easier to know how accurate the subproblems?",
            "Solution is an in our implementation.",
            "We used the default stopping condition in liver linear, which is a gradient best serving condition and we carefully check that the convergence holds for both conditions."
        ],
        [
            "And in addition to do SVM, our framework can be applied for a primal SVM.",
            "Here we left.",
            "Primary function like this and recall that the.",
            "The prime available W here has some property that a block of a primal variable corresponding to a subset of features it does not have direct connection to the block of data, so we consider stochastic gradient descent approaches.",
            "A good property of FGD is that for each update, only a block of data are needed.",
            "So and now implementation we use packages updating rule."
        ],
        [
            "Here is the implementation that we apply pictures for each block.",
            "Ann is still a special case of algorithm.",
            "What and the difference here?",
            "So once the block of.",
            "Data are loaded in memory.",
            "We can choose a private access update on the whole block in instead of that we can also split the block of data into a smaller partition then a price packs update on the smaller block.",
            "For example, if the Alpha here.",
            "Is equal to 1 plumbing.",
            "We only apply one packet update on whole block.",
            "If our bar is equal to the block size, then we applied the updates and one for each data instance in a block."
        ],
        [
            "And we also developed some techniques to reduce the training time."
        ],
        [
            "Hello techniques is data compilation.",
            "If it's designed for this reading time.",
            "PDB?",
            "OK, so we know it's an initial cost.",
            "TDB is proportional to the data size, therefore the data conversion can effectively reduce the Dicks reading time and there are some details here.",
            "We omit it now.",
            "OK, another technique is initially is applied in the initial speed of data.",
            "Imagine the situation that data are ordered according to their labels, then lots of blocks are always the same.",
            "Label is clear slowdown the training speed.",
            "Therefore a random street in the initial is leaning needed."
        ],
        [
            "Therefore, in the beginning we not.",
            "We need to count down not only random speed, but also data compilation.",
            "Consider that data larger than memory to avoid reading that out front.",
            "This too many times we need to carry design mechanism such that we can conduct both tasks.",
            "Actually we design algorithm can conduct both task by going data only once."
        ],
        [
            "OK, so function."
        ],
        [
            "Due to the simplicity and block design of our framework, we can support large functionality.",
            "Such cross validation, multiclass complication.",
            "Incremental, decremental stating and will Mr detail here."
        ],
        [
            "Yes."
        ],
        [
            "Treatment and we run three data set Yahoo Korea web.",
            "Spend an absolute the top two.",
            "Then I said our document data set and we conduct all experiment on the machine with one GB memory and you can see that the larger data set web spend data size 20 times larger than memory.",
            "Therefore we can simulate the real situation that data cannot fit in memory."
        ],
        [
            "And we compare several method where the prefix Black Star star means documentation method and the first one is blocked with solving the SVN and we apply liblinear for each subproblem and we go through each block of variable and runs and any could be 110 or 20 and brachetto D we applied liberally near default stopping condition for each block.",
            "Fraga PB we.",
            "Solving a primal SVM and we apply apply pictures on each on the whole block.",
            "Let me for each block.",
            "We only have 1 update.",
            "Rock Pi we apply Pegasus update on each data instance.",
            "Therefore we have be updates for each blocks.",
            "I live linear.",
            "We used and deliver linear without any modification."
        ],
        [
            "OK, here is the result.",
            "LeBron you spell Webspinner robot for alqueria.",
            "The X axis is the time in local scale and Y axis is the relative function value reduction.",
            "So the lower the bait.",
            "And the vertical line here is the time for initial black spirit we mentioned the earlier and you can see that.",
            "The right one is linear and left one is our block block measure you can."
        ],
        [
            "You see that.",
            "Proposed Method Office Libre Linear This because liberal linear southbound severe this swapping."
        ],
        [
            "Now pay attention to these two circle.",
            "We enlarge it on the right.",
            "OK so you can see that this one is the block PB and this one is a block.",
            "PII was land block at All Star measure OK, especially for the."
        ],
        [
            "Fraga PB.",
            "Apply only one update on each block, so the information of a block is underutilized.",
            "Therefore in general."
        ],
        [
            "Consider the long reading time.",
            "We search that we should put more efforts on each block."
        ],
        [
            "OK, here's another extreme.",
            "And for random spread enroll where the upper curve is the law.",
            "We mean that data are older according to labels.",
            "Therefore lots of blocks always re sellable.",
            "And the lower one we apply render, spreading the initial and from this figure."
        ],
        [
            "And see that render speed is useful."
        ],
        [
            "This is another experience for a different block size and where N is the number blocks and you found this figure.",
            "You can see that if an equal 1442 hundred and 400 and 1000, you can see that smaller M is better.",
            "That is, we should use larger blocksize.",
            "OK, so it's justified analysis earlier."
        ],
        [
            "OK, now it's our conclusion."
        ],
        [
            "We have proposed method can handle large data set.",
            "An experiment results show that we can handle effectively handle data 20 times larger than a memory and our implementation is available at least address.",
            "And now you can change a pretty large data set on your laptop.",
            "Thank you.",
            "So we have time for questions.",
            "Please, please could choose a microphone.",
            "Yeah please.",
            "Hi, it's very interesting work but I want to know have you evaluated if you do the subsampling of the training data and you do evaluations and you compare with this do not do the subsampling, how worth it is?",
            "I mean how useful this compared to the data subsampling?",
            "OK, an yes substantially will get us pretty Greece already, but in our work.",
            "We consider to use hard data set which hope at least usage can get us a better improvement.",
            "Even the information will be later.",
            "So I think the my major concern is I think one thing, so the missing from your studies that you do not show the classification accuracy on testing data set now.",
            "So actually this back to my orange.",
            "My main question is, is actually any need at all to go through this data multiple times?",
            "I mean if you follow standard theory of stochastic approximation there will be reduced on the rate of wild square root of an is the number of training examples.",
            "So I really don't know if there's any need at all to actually go through the data multiple times to get in so called accuracy results.",
            "I don't know, but do you have any comments on that extra?",
            "We have accuracy graph, we put it on a poster.",
            "So OK now we have lot bigger but laughing so we can see that accuracy of pretty good when the number attention is not so large.",
            "For example, we can run five or three.",
            "Iteration accuracy is pretty good.",
            "That's not my question.",
            "My question is this.",
            "Consider you just running this experience with very simple stochastic using the.",
            "Resources, which is the one version of it.",
            "But if you run this very simple stochastic approximation approach, just stream one run and we know that the error rate was reduced on the one of the square of the end.",
            "I was just curious about your classification accuracy compared with this extremely simple strategy which goes through data exactly one single terms.",
            "OK, well, well charity later, yeah?",
            "Last one.",
            "Question about there there's assumption here about the initial modeling about hard disk reading time.",
            "And now we we have a data center.",
            "We have all memory an entire hard disk was replaced with the SSD, so we do have some comments about algorithm info.",
            "Instead of using traditional hard drive and we use the SSD.",
            "IPad and I.",
            "Basically with external storage is not hard.",
            "Drive is not typical, hard drive is the solid state drive.",
            "On yeah OK, actually we haven't tried Lattice earlier in the past in our.",
            "Yes, OK, I think we have to postpone the other discussions offline.",
            "So let's like speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, hi everyone, I'm shamefully I come on National Taiwan University and the topic I want to talk it is about large linear classification.",
                    "label": 0
                },
                {
                    "sent": "When data cannot fit in memory and this is joint work with CJ shared her boots on and see their link.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is today's online.",
                    "label": 0
                },
                {
                    "sent": "I will give some introduction of our work and block nization framework for linear SVM and some implementation for SVN under our framework and some techniques to reduce training time and functionality.",
                    "label": 1
                },
                {
                    "sent": "We can support the under our framework and some experiment and conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we know linear classification is a popular research topic.",
                    "label": 1
                },
                {
                    "sent": "Recently bilinear here, we mean that we do not impose nonlinear kernel.",
                    "label": 0
                },
                {
                    "sent": "Although accuracy by linear may not be as good as nonlinear, but for some problems such as documented vacation accurus by linear is as good as nonlinear.",
                    "label": 1
                },
                {
                    "sent": "In addition, it also enjoy faster training and testing speed.",
                    "label": 0
                },
                {
                    "sent": "So in this talk will focus on the large linear classification.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Existing approaches for large any linear classification can be categorized into two types.",
                    "label": 1
                },
                {
                    "sent": "For large data, which can still fit in memory, loss of vision methods are well developed, but for data larger than this size we don't need a distributed solution for it.",
                    "label": 0
                },
                {
                    "sent": "So here comes our question, can we have something in there between a single setting for the data larger than memory but smaller than disk?",
                    "label": 1
                },
                {
                    "sent": "Actually, some have proposed a method to handle this situation, but their approach can only handle data with small number of features.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we see an example experiment to see the performance of typical linear solvers under situation that data cannot fit in memory.",
                    "label": 1
                },
                {
                    "sent": "We run a typical linear solve linear on a machine with only one GB memory, and here is the result where the X axis is the size data size in gigabytes and Y axis is the time in seconds.",
                    "label": 0
                },
                {
                    "sent": "And this vertical line indicate the real.",
                    "label": 0
                },
                {
                    "sent": "Available memory capacity and you can see that when the data size approaching the memory capacity, the curve becomes deeper.",
                    "label": 0
                },
                {
                    "sent": "This is because this what throw down the training speed.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our goal is to construct large linear classifiers for ordinary users on a single machine, such as your laptop.",
                    "label": 0
                },
                {
                    "sent": "We have some assumptions we assume that are larger than memory, but smaller than disk, and we also assume that submarine causes low accuracy and we have one requirement is that our solution must be simple so that we can support lots of functionality such as multiclass classification, parameter selection and so on.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our work we model the training time into two parts.",
                    "label": 0
                },
                {
                    "sent": "Time to check in memory data and time to access data from disk, and we pay more attention to the second part, actually decode the loading time at Dominator training procedure.",
                    "label": 1
                },
                {
                    "sent": "So even that half can fit in memory.",
                    "label": 0
                },
                {
                    "sent": "Second part is already an issue.",
                    "label": 0
                },
                {
                    "sent": "Here we see an example.",
                    "label": 0
                },
                {
                    "sent": "We run linear on a large data set as if you are, which contains hope more than half millions of.",
                    "label": 0
                },
                {
                    "sent": "Documents you can see that it takes larger than one minute to load or data, say while less than 5 seconds to computing the model.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So under our assumption and requirement for available method, it must satisfy the following conditions.",
                    "label": 0
                },
                {
                    "sent": "First, each optimization step is a continuous chunk of training data.",
                    "label": 1
                },
                {
                    "sent": "To avoid random access on a disk, second optimization procedure must converge.",
                    "label": 0
                },
                {
                    "sent": "Is even each time only a subset of data are available?",
                    "label": 1
                },
                {
                    "sent": "3rd, the number of optimizations that cannot be too large.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we may read data from disk too many times.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our work we can see the SVM as a linear classifier even though our method can be easily applied on logic regression.",
                    "label": 1
                },
                {
                    "sent": "In linear SVM we are given a set of training pair ysi.",
                    "label": 0
                },
                {
                    "sent": "Each XI is a visual vector in on dimensional space and each Y is the label could be plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "Where is the number of features and Elo is a number of data.",
                    "label": 1
                },
                {
                    "sent": "And there are two types of SVM, primal SVM, softly optimization problem and the variable of the primaries.",
                    "label": 0
                },
                {
                    "sent": "VM is W, which has the same dimensional as the number of features the other phone is still as VM itself.",
                    "label": 0
                },
                {
                    "sent": "This optimization method, where the East is a vector of ones and Q is the metrics we sell like this and the dual variable of Alpha as a dimensional hello.",
                    "label": 0
                },
                {
                    "sent": "Therefore, each do available after I is corresponding to a facial feature vector XI.",
                    "label": 0
                },
                {
                    "sent": "This is an important property we will use later.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is our blocking Edition framework for linear SVM, first with three data, one to allow to a partition.",
                    "label": 0
                },
                {
                    "sent": "We want to be at such that each block of data can fit in memory, and we store data into M file accordingly, who at the end is the number of blocks.",
                    "label": 1
                },
                {
                    "sent": "OK, then we set initial value for Alpha or W, then for each outer iteration we sequentially load data from box 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "OK, at each inner iteration.",
                    "label": 0
                },
                {
                    "sent": "Why is the product of the data are loaded?",
                    "label": 0
                },
                {
                    "sent": "We let conduct some operation on each.",
                    "label": 1
                },
                {
                    "sent": "An update of our double content and here we do not specify operation for each block here.",
                    "label": 0
                },
                {
                    "sent": "So we can apply another operation later.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we know, documentation is a classical optimization method which usually choose a block available to update at each time and is widely used in nonlinear SVM with recorded decomposition method.",
                    "label": 1
                },
                {
                    "sent": "However, in our framework we need a connection between a block of data and block of variables.",
                    "label": 1
                },
                {
                    "sent": "In the situation lab, data larger than memory.",
                    "label": 1
                },
                {
                    "sent": "To avoid random Ness and disk, we cannot use holistic method to select block variable because at each time only a block of data from a fixed partition is available in memory.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's some issue about our framework.",
                    "label": 0
                },
                {
                    "sent": "We need to decide the number of blocks.",
                    "label": 1
                },
                {
                    "sent": "Actually, if we assume oblock have similar size, the lender number of bugs is inversely proportional to the block size here.",
                    "label": 1
                },
                {
                    "sent": "So in terms of block size, it cannot be too large because each block must fit in memory.",
                    "label": 1
                },
                {
                    "sent": "But on the other hand it cannot be too small.",
                    "label": 1
                },
                {
                    "sent": "Actually, we think that should be as large as possible.",
                    "label": 0
                },
                {
                    "sent": "And our analysis is follow.",
                    "label": 0
                },
                {
                    "sent": "We first met the total time for an alteration is likely.",
                    "label": 0
                },
                {
                    "sent": "The T&B is a time cost of 1.",
                    "label": 1
                },
                {
                    "sent": "Email iteration in memory and a TDB is a tank cost of reading block over data from disk and both T&B&T DVR functions of block size.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a pest analysis, let usually consider T&B only there for the total time we can like this TN B * 8 / B.",
                    "label": 0
                },
                {
                    "sent": "Then because the growth rate over T&B is usually more linear to the block size, therefore previous SVN work suggests that smaller block sizes beta.",
                    "label": 0
                },
                {
                    "sent": "But now we take TDB into consideration and each file opening will cause some initial cost like this.",
                    "label": 1
                },
                {
                    "sent": "Therefore total reading time will become like the initial cost times at all over block size and a constant time.",
                    "label": 1
                },
                {
                    "sent": "Here you can clearly see that larger block size will be better, and actually we've done some some experiment to justify this translator.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's our implementation for linear.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, first we consider deal SVM.",
                    "label": 0
                },
                {
                    "sent": "Let FRB radio function like this, then recall the probability overdue SVM that each block available correspond to a block of data level one super block of data already in memory.",
                    "label": 1
                },
                {
                    "sent": "We can consult some problem like this, where the.",
                    "label": 0
                },
                {
                    "sent": "Bob Bob is a country Mantova Vijay and found this contract.",
                    "label": 1
                },
                {
                    "sent": "You can see that only variable corresponding to the block busy can be changed in this problem, and although the subproblem involves all data, but we can handle it by some technique and we omitted details here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now you see the implementation for Adidas VM at least.",
                    "label": 1
                },
                {
                    "sent": "Algorithm Two is a special case of algorithm one and only difference from algorithm.",
                    "label": 1
                },
                {
                    "sent": "Once this tab B as Dev C, once a block of data are loaded, we can approximate this software South problem an update and obtain a temporary solution.",
                    "label": 0
                },
                {
                    "sent": "Deep learning update Alpha accordingly.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The flock of data are in memory, so any bank contract can be applied to solve some problem.",
                    "label": 0
                },
                {
                    "sent": "And here we can see the Libor linear which is according a decent measure.",
                    "label": 0
                },
                {
                    "sent": "This made our implementation and two level block nization and the two labeled concept is used in some algorithms before, but ours might be the first.",
                    "label": 1
                },
                {
                    "sent": "Consider the inner level in memory while outer level in the disk.",
                    "label": 0
                },
                {
                    "sent": "But actually, in practice we usually obtain an approximate solution for each subproblem, therefore thus stopping criteria for each subproblem must be carefully designed to ensure the global convergence of the whole algorithms and we.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Opposed to stopping condition for sub problem.",
                    "label": 0
                },
                {
                    "sent": "That's why we can fix the number of passes to variable in the block.",
                    "label": 1
                },
                {
                    "sent": "Busy for example we can set a number of tests become 123 and so on.",
                    "label": 0
                },
                {
                    "sent": "The only problem is that we need to decide a proper number, the other.",
                    "label": 1
                },
                {
                    "sent": "Is gradient best having competition which made us easier to know how accurate the subproblems?",
                    "label": 1
                },
                {
                    "sent": "Solution is an in our implementation.",
                    "label": 0
                },
                {
                    "sent": "We used the default stopping condition in liver linear, which is a gradient best serving condition and we carefully check that the convergence holds for both conditions.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in addition to do SVM, our framework can be applied for a primal SVM.",
                    "label": 0
                },
                {
                    "sent": "Here we left.",
                    "label": 0
                },
                {
                    "sent": "Primary function like this and recall that the.",
                    "label": 0
                },
                {
                    "sent": "The prime available W here has some property that a block of a primal variable corresponding to a subset of features it does not have direct connection to the block of data, so we consider stochastic gradient descent approaches.",
                    "label": 1
                },
                {
                    "sent": "A good property of FGD is that for each update, only a block of data are needed.",
                    "label": 0
                },
                {
                    "sent": "So and now implementation we use packages updating rule.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the implementation that we apply pictures for each block.",
                    "label": 1
                },
                {
                    "sent": "Ann is still a special case of algorithm.",
                    "label": 1
                },
                {
                    "sent": "What and the difference here?",
                    "label": 0
                },
                {
                    "sent": "So once the block of.",
                    "label": 0
                },
                {
                    "sent": "Data are loaded in memory.",
                    "label": 0
                },
                {
                    "sent": "We can choose a private access update on the whole block in instead of that we can also split the block of data into a smaller partition then a price packs update on the smaller block.",
                    "label": 1
                },
                {
                    "sent": "For example, if the Alpha here.",
                    "label": 0
                },
                {
                    "sent": "Is equal to 1 plumbing.",
                    "label": 0
                },
                {
                    "sent": "We only apply one packet update on whole block.",
                    "label": 1
                },
                {
                    "sent": "If our bar is equal to the block size, then we applied the updates and one for each data instance in a block.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also developed some techniques to reduce the training time.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello techniques is data compilation.",
                    "label": 0
                },
                {
                    "sent": "If it's designed for this reading time.",
                    "label": 1
                },
                {
                    "sent": "PDB?",
                    "label": 0
                },
                {
                    "sent": "OK, so we know it's an initial cost.",
                    "label": 0
                },
                {
                    "sent": "TDB is proportional to the data size, therefore the data conversion can effectively reduce the Dicks reading time and there are some details here.",
                    "label": 1
                },
                {
                    "sent": "We omit it now.",
                    "label": 0
                },
                {
                    "sent": "OK, another technique is initially is applied in the initial speed of data.",
                    "label": 1
                },
                {
                    "sent": "Imagine the situation that data are ordered according to their labels, then lots of blocks are always the same.",
                    "label": 0
                },
                {
                    "sent": "Label is clear slowdown the training speed.",
                    "label": 0
                },
                {
                    "sent": "Therefore a random street in the initial is leaning needed.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Therefore, in the beginning we not.",
                    "label": 1
                },
                {
                    "sent": "We need to count down not only random speed, but also data compilation.",
                    "label": 0
                },
                {
                    "sent": "Consider that data larger than memory to avoid reading that out front.",
                    "label": 0
                },
                {
                    "sent": "This too many times we need to carry design mechanism such that we can conduct both tasks.",
                    "label": 0
                },
                {
                    "sent": "Actually we design algorithm can conduct both task by going data only once.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so function.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Due to the simplicity and block design of our framework, we can support large functionality.",
                    "label": 1
                },
                {
                    "sent": "Such cross validation, multiclass complication.",
                    "label": 0
                },
                {
                    "sent": "Incremental, decremental stating and will Mr detail here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Treatment and we run three data set Yahoo Korea web.",
                    "label": 0
                },
                {
                    "sent": "Spend an absolute the top two.",
                    "label": 0
                },
                {
                    "sent": "Then I said our document data set and we conduct all experiment on the machine with one GB memory and you can see that the larger data set web spend data size 20 times larger than memory.",
                    "label": 1
                },
                {
                    "sent": "Therefore we can simulate the real situation that data cannot fit in memory.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we compare several method where the prefix Black Star star means documentation method and the first one is blocked with solving the SVN and we apply liblinear for each subproblem and we go through each block of variable and runs and any could be 110 or 20 and brachetto D we applied liberally near default stopping condition for each block.",
                    "label": 1
                },
                {
                    "sent": "Fraga PB we.",
                    "label": 1
                },
                {
                    "sent": "Solving a primal SVM and we apply apply pictures on each on the whole block.",
                    "label": 0
                },
                {
                    "sent": "Let me for each block.",
                    "label": 0
                },
                {
                    "sent": "We only have 1 update.",
                    "label": 1
                },
                {
                    "sent": "Rock Pi we apply Pegasus update on each data instance.",
                    "label": 0
                },
                {
                    "sent": "Therefore we have be updates for each blocks.",
                    "label": 1
                },
                {
                    "sent": "I live linear.",
                    "label": 0
                },
                {
                    "sent": "We used and deliver linear without any modification.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is the result.",
                    "label": 0
                },
                {
                    "sent": "LeBron you spell Webspinner robot for alqueria.",
                    "label": 0
                },
                {
                    "sent": "The X axis is the time in local scale and Y axis is the relative function value reduction.",
                    "label": 1
                },
                {
                    "sent": "So the lower the bait.",
                    "label": 1
                },
                {
                    "sent": "And the vertical line here is the time for initial black spirit we mentioned the earlier and you can see that.",
                    "label": 0
                },
                {
                    "sent": "The right one is linear and left one is our block block measure you can.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You see that.",
                    "label": 0
                },
                {
                    "sent": "Proposed Method Office Libre Linear This because liberal linear southbound severe this swapping.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now pay attention to these two circle.",
                    "label": 0
                },
                {
                    "sent": "We enlarge it on the right.",
                    "label": 0
                },
                {
                    "sent": "OK so you can see that this one is the block PB and this one is a block.",
                    "label": 0
                },
                {
                    "sent": "PII was land block at All Star measure OK, especially for the.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fraga PB.",
                    "label": 0
                },
                {
                    "sent": "Apply only one update on each block, so the information of a block is underutilized.",
                    "label": 1
                },
                {
                    "sent": "Therefore in general.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consider the long reading time.",
                    "label": 0
                },
                {
                    "sent": "We search that we should put more efforts on each block.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here's another extreme.",
                    "label": 0
                },
                {
                    "sent": "And for random spread enroll where the upper curve is the law.",
                    "label": 0
                },
                {
                    "sent": "We mean that data are older according to labels.",
                    "label": 1
                },
                {
                    "sent": "Therefore lots of blocks always re sellable.",
                    "label": 0
                },
                {
                    "sent": "And the lower one we apply render, spreading the initial and from this figure.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And see that render speed is useful.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is another experience for a different block size and where N is the number blocks and you found this figure.",
                    "label": 1
                },
                {
                    "sent": "You can see that if an equal 1442 hundred and 400 and 1000, you can see that smaller M is better.",
                    "label": 0
                },
                {
                    "sent": "That is, we should use larger blocksize.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's justified analysis earlier.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now it's our conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have proposed method can handle large data set.",
                    "label": 1
                },
                {
                    "sent": "An experiment results show that we can handle effectively handle data 20 times larger than a memory and our implementation is available at least address.",
                    "label": 1
                },
                {
                    "sent": "And now you can change a pretty large data set on your laptop.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So we have time for questions.",
                    "label": 0
                },
                {
                    "sent": "Please, please could choose a microphone.",
                    "label": 0
                },
                {
                    "sent": "Yeah please.",
                    "label": 0
                },
                {
                    "sent": "Hi, it's very interesting work but I want to know have you evaluated if you do the subsampling of the training data and you do evaluations and you compare with this do not do the subsampling, how worth it is?",
                    "label": 0
                },
                {
                    "sent": "I mean how useful this compared to the data subsampling?",
                    "label": 0
                },
                {
                    "sent": "OK, an yes substantially will get us pretty Greece already, but in our work.",
                    "label": 0
                },
                {
                    "sent": "We consider to use hard data set which hope at least usage can get us a better improvement.",
                    "label": 0
                },
                {
                    "sent": "Even the information will be later.",
                    "label": 0
                },
                {
                    "sent": "So I think the my major concern is I think one thing, so the missing from your studies that you do not show the classification accuracy on testing data set now.",
                    "label": 0
                },
                {
                    "sent": "So actually this back to my orange.",
                    "label": 0
                },
                {
                    "sent": "My main question is, is actually any need at all to go through this data multiple times?",
                    "label": 0
                },
                {
                    "sent": "I mean if you follow standard theory of stochastic approximation there will be reduced on the rate of wild square root of an is the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "So I really don't know if there's any need at all to actually go through the data multiple times to get in so called accuracy results.",
                    "label": 0
                },
                {
                    "sent": "I don't know, but do you have any comments on that extra?",
                    "label": 0
                },
                {
                    "sent": "We have accuracy graph, we put it on a poster.",
                    "label": 0
                },
                {
                    "sent": "So OK now we have lot bigger but laughing so we can see that accuracy of pretty good when the number attention is not so large.",
                    "label": 0
                },
                {
                    "sent": "For example, we can run five or three.",
                    "label": 0
                },
                {
                    "sent": "Iteration accuracy is pretty good.",
                    "label": 0
                },
                {
                    "sent": "That's not my question.",
                    "label": 0
                },
                {
                    "sent": "My question is this.",
                    "label": 0
                },
                {
                    "sent": "Consider you just running this experience with very simple stochastic using the.",
                    "label": 0
                },
                {
                    "sent": "Resources, which is the one version of it.",
                    "label": 0
                },
                {
                    "sent": "But if you run this very simple stochastic approximation approach, just stream one run and we know that the error rate was reduced on the one of the square of the end.",
                    "label": 0
                },
                {
                    "sent": "I was just curious about your classification accuracy compared with this extremely simple strategy which goes through data exactly one single terms.",
                    "label": 0
                },
                {
                    "sent": "OK, well, well charity later, yeah?",
                    "label": 0
                },
                {
                    "sent": "Last one.",
                    "label": 0
                },
                {
                    "sent": "Question about there there's assumption here about the initial modeling about hard disk reading time.",
                    "label": 0
                },
                {
                    "sent": "And now we we have a data center.",
                    "label": 0
                },
                {
                    "sent": "We have all memory an entire hard disk was replaced with the SSD, so we do have some comments about algorithm info.",
                    "label": 0
                },
                {
                    "sent": "Instead of using traditional hard drive and we use the SSD.",
                    "label": 0
                },
                {
                    "sent": "IPad and I.",
                    "label": 0
                },
                {
                    "sent": "Basically with external storage is not hard.",
                    "label": 0
                },
                {
                    "sent": "Drive is not typical, hard drive is the solid state drive.",
                    "label": 0
                },
                {
                    "sent": "On yeah OK, actually we haven't tried Lattice earlier in the past in our.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, I think we have to postpone the other discussions offline.",
                    "label": 0
                },
                {
                    "sent": "So let's like speaker again.",
                    "label": 0
                }
            ]
        }
    }
}