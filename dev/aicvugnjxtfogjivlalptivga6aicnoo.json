{
    "id": "aicvugnjxtfogjivlalptivga6aicnoo",
    "title": "From Averaging to Acceleration, There is Only a Step-size",
    "info": {
        "author": [
            "Nicolas Flammarion, INRIA - SIERRA project-team"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_flammarion_averaging_acceleration/",
    "segmentation": [
        [
            "So when you want to."
        ],
        [
            "Major function with access potentially noisy access to its gradient.",
            "When the function F is moose, so when its graduates Lipschitz, there are two main modification of the gradient descent which have been studio lot.",
            "So is excellerated gradient descent described by Nesterov 83?",
            "Where you just do a gradient descent, but where your support point will be.",
            "Your current iterate plus the momentum term.",
            "Which will depend on the two previous Tier 8.",
            "And this modification.",
            "And Albert, you to go from the suboptimal rate of 1 / N to the optimal rate of 1 / N square.",
            "But it's not very robust and ice I should buy, for example, that promo Schmidt audible detail.",
            "OK."
        ],
        [
            "And on the other hand, we also have the average wage in dissent.",
            "Well, you just do an step of gradient descent and you take at the end the average of it, OK. And so.",
            "This modification is very good for stochastic approximation when your gradient is noisy.",
            "Because you will get the optimal optimal rates for stochastic approximation and it will be more abuse.",
            "The choice of stepsize set size.",
            "But for deterministic optimization.",
            "It's just not a lot of interest and even works for strongly convex function.",
            "You will go from a linear rate of conversions to a seminar rate.",
            "So.",
            "You have this.",
            "To Anchorage.",
            "Two modification of gradient descent.",
            "When is good for deterministic optimization, the other is good for stochastic approximation and at first sight they don't really look like each other OK?"
        ],
        [
            "But if you consider the average gradient descent.",
            "And you look at the average rate azzameen iterate.",
            "You can write the original online way.",
            "An by using the definition of the.",
            "Gradient descent.",
            "End the relationship between Supreme Editor at Siena.",
            "And the two average.",
            "Did the end end Italian minus one?",
            "You can rewrite it this algorithm in a way in dependently option.",
            "So you can rewrite the average gradient descent.",
            "Only in function of the average rate.",
            "And it will be.",
            "They signaled out there in time difference equation.",
            "So we see.",
            "That's the asker.",
            "It shares the same structure, then accelerated gradient descent."
        ],
        [
            "It will be a modification of the gradient descent with two support points.",
            "Which will be, I think, combination of the two previous iterate.",
            "And for some choice of stepchild Alpha and beta.",
            "You get back the average weight on this end and for another choice you will get the accelerated gradient descent OK."
        ],
        [
            "So this will be our main contribution for this paper.",
            "We only study quadratic function.",
            "And we deal with an ulcer in case.",
            "So we show that accelerated gradient descent and average gradient descent are at the edge of a unique framework, which will be a constant parameter, 2nd order difference equation.",
            "Then we will study the eigenvalues of this system as it pulled back down again.",
            "Can this?",
            "But in the normal case?",
            "And then in presence of noise.",
            "We will propose an alternative algorithm which will exhibit the good aspect of both of arranging an acceleration."
        ],
        [
            "So.",
            "We study a quadratic function where age will be the Asian.",
            "We can assume age to be invertible, but with arbitrarily small eigenvalues not or not to be strongly convex.",
            "And we can rewrite the previous algorithm function of Agent Estar, which will be the global optimum.",
            "And we first see that the tester will be a stationary point of this algorithm."
        ],
        [
            "And in fact.",
            "We see that.",
            "The algorithm only depend of N times data and my new status stars."
        ],
        [
            "So by denoting at the end, which will be ended, the end minus two testers.",
            "We can have formulated the previous algorithm.",
            "As an interactive system with constant them.",
            "So it just will be like a gradient descent.",
            "It would be the term identity minus Alpha H plus a moment to term, which will depend on Theta N in 10 -- 1, which will be weighted by.",
            "Metrics will depend on station.",
            "And so this is a constant term difference equation and this is very classic to study.",
            "But"
        ],
        [
            "By definition of it again.",
            "We go from a convention problem of F of the end to the solution at the Speedway, Dover and Square.",
            "Twist ability problem so.",
            "If we show that the iterate at the end is bounded, we directly have the convergence as a function value at the speed we want.",
            "OK.",
            "So not to study the convergence, we just have to project the iteration of difference eigenspace of Edge.",
            "So hi, will reduce the eigenvalue of H. And we get the different.",
            "Iteration system.",
            "And this will be like simple 2nd order difference equation.",
            "So we write it in a linear system and we just have to study the eigenvalue of the matrix FI.",
            "So the stability of our algorithm would be equivalent to the.",
            "Speak Colorados if I should be less than one mother.",
            "Spectral had uses.",
            "Hypertension values mortgages.",
            "So when we will compute the charis tickle polynomial of FI.",
            "In function of his indiscriminant we will have different behavior if the discriminate will be secure positive, we will have two real elegant values.",
            "If it will be negative, we have too complex and values in an oscillatory.",
            "Behavior.",
            "And we have a limit case when the discriminants will be 0."
        ],
        [
            "So in fact, for each eigenspace we will have a triangular value of Alpha and beta.",
            "For which.",
            "So iterate it again will be bonded, and so our algorithm will convert the speed one over square.",
            "And this young girl is separating two part, one part where the behavior will be complex and the other part where it will be real.",
            "And we.",
            "Can put on this figure like the classical algorithm.",
            "The average gradient descent.",
            "For Alpha equals 0.",
            "So accelerated gradient descent for Alpha equals beta.",
            "And the EV Bulger it of Polyak.",
            "So be thankful sale.",
            "In so, in order to have algorithms converge, we should have the D iterates.",
            "It and I to be bonded.",
            "And after that, for choice of Alpha and beta.",
            "Depending on the value of the eigenvalue hi, we will be either have for some.",
            "Complex behavior or real behavior.",
            "And it is important to note that the classical algorithm so average gradient descent oh so accelerated gradient descent.",
            "Choose for each eigenvalue the same behavior.",
            "So accelerated within decent all the eigenvalue would be complex if average gradient descent, they all will be real.",
            "OK, but like the direct computation of the eigenvalue.",
            "Will give you some rate of convergence.",
            "But they won't be efficient because they will depend on HI, which could be arbitrarily slow.",
            "So we have to derive some general bound.",
            "You definitely have hi."
        ],
        [
            "So this one will be the minimum of three terms in order to be valid in all the triangle for all the different value possible of Alpha L meta.",
            "So the first bunker I spoke to the acceleration when Alpha is non zero and if Alpha equals zero we will have to bound one.",
            "Bone is a classical rate of 1 / N for average gradient descent and the other one for 1 / N square for some starting point where chosen OK. And so invest independence.",
            "I'm not improbable Ann to prove this.",
            "We just define some different Lyapunov function of difference in pace, the pace of the triangle.",
            "So to conclude, the deterministic spark part, we just saw that in order to have 1 / N square rate of convergence, we should have Alpha strictly positive."
        ],
        [
            "So now.",
            "We will consider the situation when where there is some additive noise.",
            "So we have a noisy Oracle for the gradient.",
            "And we take a very simple, noise independent, uncorrelated your mean with abundant covariance.",
            "And we re write.",
            "Second parameter difference equation and we first see that.",
            "If Alpha is nonzero, so if we want to have some acceleration, we will have an important level of noise."
        ],
        [
            "So we wrote.",
            "We write it as a linear system.",
            "And we can compute the expected 2nd order moment, which will decompose as the sum of two term a bias term.",
            "Which will only depend.",
            "And the initial condition and which we have already studied before.",
            "And the value in STEM, which will depend on the noise and that we can compute."
        ],
        [
            "So this is our convergence result for the case of instructor noise.",
            "We see on the triangle the two classical algorithms and that we will explore the area in between them.",
            "So the bound.",
            "Will will be as before the minimum of two bound to be valid and all the area and will be the sum of the bias term plus the variance term.",
            "And we do not buy big N so reason of time because after we re label the step size Alpha and beta two dependent began."
        ],
        [
            "So for Alpha equals zero and beta equals 1 / L. This is the horizon gradient descent.",
            "We find again as in back in Berlin.",
            "That this algorithm is bounded but not converging.",
            "In fact, celebrated written dissent.",
            "Our bound is diverging and in we also see that in experiment.",
            "And for Alpha equals zero and beta equals 1 / sqrt N. This is a classical rate of four.",
            "Stochastic gradient descent is in a stronger case.",
            "And."
        ],
        [
            "If you know the value of the bias in the variance, you can choose the optimal value for Alpha and beta and you will get the optimal bias variance tradeoff.",
            "As in land, who will always yell.",
            "So.",
            "For this model of noise.",
            "We don't recover new results, but we just propose a new interpretation."
        ],
        [
            "But if we assume more on the noise.",
            "If you assume the noise is structured as in this clarification, so where the.",
            "Covariance of the nose will be less than the covariance of the output.",
            "We will be able to drive better bond.",
            "OK. Sonofa Alpha equals."
        ],
        [
            "So in beta equals 1 / N Thanks, average gradient descent, we have a convergence speed of 1 / N. For Alpha equals we take well 1 / N we will have accelerated gradient descent which will be bounded but not converging to 0.",
            "And for Alpha equals 1 / N power A we will be able to achieve a bias variance tradeoff by changing the value of Alpha.",
            "So if we you know that your problem, we will have important bias.",
            "You can take the right value of a.",
            "And the same for violence."
        ],
        [
            "And we also studied the minimax conversion, straight since we know that.",
            "For quadratic problem the conversion rate will be the same of the bias term plus the variance term.",
            "And we already know the minimax lower bound for the for both, so the bias times 1 / N square.",
            "Hey Mr Huff and for civilian stem it will be easier be 1 / sqrt N or 1 / N wait see back off."
        ],
        [
            "And we are able to attend this tool elbowing simultaneously for some situation, but not in all the cases that remain an open problem.",
            "So income."
        ],
        [
            "Asian.",
            "We propose a joint analysis of averaging and acceleration is the same framework.",
            "And we propose, in the presence of noise.",
            "A new choice of stepsize.",
            "Which will exhibit the bus aspect of both Android.",
            "That is how a lot of extension possible.",
            "First just a deeper, more complex noise independent.",
            "For example, a stochastic gradient descent.",
            "And to extend this results to non quadratic functions.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when you want to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Major function with access potentially noisy access to its gradient.",
                    "label": 1
                },
                {
                    "sent": "When the function F is moose, so when its graduates Lipschitz, there are two main modification of the gradient descent which have been studio lot.",
                    "label": 1
                },
                {
                    "sent": "So is excellerated gradient descent described by Nesterov 83?",
                    "label": 0
                },
                {
                    "sent": "Where you just do a gradient descent, but where your support point will be.",
                    "label": 0
                },
                {
                    "sent": "Your current iterate plus the momentum term.",
                    "label": 0
                },
                {
                    "sent": "Which will depend on the two previous Tier 8.",
                    "label": 0
                },
                {
                    "sent": "And this modification.",
                    "label": 0
                },
                {
                    "sent": "And Albert, you to go from the suboptimal rate of 1 / N to the optimal rate of 1 / N square.",
                    "label": 0
                },
                {
                    "sent": "But it's not very robust and ice I should buy, for example, that promo Schmidt audible detail.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And on the other hand, we also have the average wage in dissent.",
                    "label": 0
                },
                {
                    "sent": "Well, you just do an step of gradient descent and you take at the end the average of it, OK. And so.",
                    "label": 0
                },
                {
                    "sent": "This modification is very good for stochastic approximation when your gradient is noisy.",
                    "label": 0
                },
                {
                    "sent": "Because you will get the optimal optimal rates for stochastic approximation and it will be more abuse.",
                    "label": 1
                },
                {
                    "sent": "The choice of stepsize set size.",
                    "label": 0
                },
                {
                    "sent": "But for deterministic optimization.",
                    "label": 0
                },
                {
                    "sent": "It's just not a lot of interest and even works for strongly convex function.",
                    "label": 0
                },
                {
                    "sent": "You will go from a linear rate of conversions to a seminar rate.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You have this.",
                    "label": 0
                },
                {
                    "sent": "To Anchorage.",
                    "label": 1
                },
                {
                    "sent": "Two modification of gradient descent.",
                    "label": 0
                },
                {
                    "sent": "When is good for deterministic optimization, the other is good for stochastic approximation and at first sight they don't really look like each other OK?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if you consider the average gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And you look at the average rate azzameen iterate.",
                    "label": 0
                },
                {
                    "sent": "You can write the original online way.",
                    "label": 0
                },
                {
                    "sent": "An by using the definition of the.",
                    "label": 0
                },
                {
                    "sent": "Gradient descent.",
                    "label": 0
                },
                {
                    "sent": "End the relationship between Supreme Editor at Siena.",
                    "label": 0
                },
                {
                    "sent": "And the two average.",
                    "label": 0
                },
                {
                    "sent": "Did the end end Italian minus one?",
                    "label": 0
                },
                {
                    "sent": "You can rewrite it this algorithm in a way in dependently option.",
                    "label": 0
                },
                {
                    "sent": "So you can rewrite the average gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Only in function of the average rate.",
                    "label": 0
                },
                {
                    "sent": "And it will be.",
                    "label": 0
                },
                {
                    "sent": "They signaled out there in time difference equation.",
                    "label": 0
                },
                {
                    "sent": "So we see.",
                    "label": 0
                },
                {
                    "sent": "That's the asker.",
                    "label": 0
                },
                {
                    "sent": "It shares the same structure, then accelerated gradient descent.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It will be a modification of the gradient descent with two support points.",
                    "label": 0
                },
                {
                    "sent": "Which will be, I think, combination of the two previous iterate.",
                    "label": 0
                },
                {
                    "sent": "And for some choice of stepchild Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "You get back the average weight on this end and for another choice you will get the accelerated gradient descent OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this will be our main contribution for this paper.",
                    "label": 0
                },
                {
                    "sent": "We only study quadratic function.",
                    "label": 0
                },
                {
                    "sent": "And we deal with an ulcer in case.",
                    "label": 0
                },
                {
                    "sent": "So we show that accelerated gradient descent and average gradient descent are at the edge of a unique framework, which will be a constant parameter, 2nd order difference equation.",
                    "label": 1
                },
                {
                    "sent": "Then we will study the eigenvalues of this system as it pulled back down again.",
                    "label": 0
                },
                {
                    "sent": "Can this?",
                    "label": 1
                },
                {
                    "sent": "But in the normal case?",
                    "label": 1
                },
                {
                    "sent": "And then in presence of noise.",
                    "label": 0
                },
                {
                    "sent": "We will propose an alternative algorithm which will exhibit the good aspect of both of arranging an acceleration.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We study a quadratic function where age will be the Asian.",
                    "label": 0
                },
                {
                    "sent": "We can assume age to be invertible, but with arbitrarily small eigenvalues not or not to be strongly convex.",
                    "label": 0
                },
                {
                    "sent": "And we can rewrite the previous algorithm function of Agent Estar, which will be the global optimum.",
                    "label": 1
                },
                {
                    "sent": "And we first see that the tester will be a stationary point of this algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact.",
                    "label": 0
                },
                {
                    "sent": "We see that.",
                    "label": 0
                },
                {
                    "sent": "The algorithm only depend of N times data and my new status stars.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So by denoting at the end, which will be ended, the end minus two testers.",
                    "label": 0
                },
                {
                    "sent": "We can have formulated the previous algorithm.",
                    "label": 0
                },
                {
                    "sent": "As an interactive system with constant them.",
                    "label": 1
                },
                {
                    "sent": "So it just will be like a gradient descent.",
                    "label": 0
                },
                {
                    "sent": "It would be the term identity minus Alpha H plus a moment to term, which will depend on Theta N in 10 -- 1, which will be weighted by.",
                    "label": 0
                },
                {
                    "sent": "Metrics will depend on station.",
                    "label": 0
                },
                {
                    "sent": "And so this is a constant term difference equation and this is very classic to study.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By definition of it again.",
                    "label": 0
                },
                {
                    "sent": "We go from a convention problem of F of the end to the solution at the Speedway, Dover and Square.",
                    "label": 0
                },
                {
                    "sent": "Twist ability problem so.",
                    "label": 0
                },
                {
                    "sent": "If we show that the iterate at the end is bounded, we directly have the convergence as a function value at the speed we want.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So not to study the convergence, we just have to project the iteration of difference eigenspace of Edge.",
                    "label": 1
                },
                {
                    "sent": "So hi, will reduce the eigenvalue of H. And we get the different.",
                    "label": 0
                },
                {
                    "sent": "Iteration system.",
                    "label": 0
                },
                {
                    "sent": "And this will be like simple 2nd order difference equation.",
                    "label": 0
                },
                {
                    "sent": "So we write it in a linear system and we just have to study the eigenvalue of the matrix FI.",
                    "label": 1
                },
                {
                    "sent": "So the stability of our algorithm would be equivalent to the.",
                    "label": 0
                },
                {
                    "sent": "Speak Colorados if I should be less than one mother.",
                    "label": 0
                },
                {
                    "sent": "Spectral had uses.",
                    "label": 0
                },
                {
                    "sent": "Hypertension values mortgages.",
                    "label": 0
                },
                {
                    "sent": "So when we will compute the charis tickle polynomial of FI.",
                    "label": 0
                },
                {
                    "sent": "In function of his indiscriminant we will have different behavior if the discriminate will be secure positive, we will have two real elegant values.",
                    "label": 0
                },
                {
                    "sent": "If it will be negative, we have too complex and values in an oscillatory.",
                    "label": 0
                },
                {
                    "sent": "Behavior.",
                    "label": 0
                },
                {
                    "sent": "And we have a limit case when the discriminants will be 0.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in fact, for each eigenspace we will have a triangular value of Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "For which.",
                    "label": 0
                },
                {
                    "sent": "So iterate it again will be bonded, and so our algorithm will convert the speed one over square.",
                    "label": 0
                },
                {
                    "sent": "And this young girl is separating two part, one part where the behavior will be complex and the other part where it will be real.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                },
                {
                    "sent": "Can put on this figure like the classical algorithm.",
                    "label": 0
                },
                {
                    "sent": "The average gradient descent.",
                    "label": 0
                },
                {
                    "sent": "For Alpha equals 0.",
                    "label": 0
                },
                {
                    "sent": "So accelerated gradient descent for Alpha equals beta.",
                    "label": 0
                },
                {
                    "sent": "And the EV Bulger it of Polyak.",
                    "label": 0
                },
                {
                    "sent": "So be thankful sale.",
                    "label": 0
                },
                {
                    "sent": "In so, in order to have algorithms converge, we should have the D iterates.",
                    "label": 1
                },
                {
                    "sent": "It and I to be bonded.",
                    "label": 0
                },
                {
                    "sent": "And after that, for choice of Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "Depending on the value of the eigenvalue hi, we will be either have for some.",
                    "label": 1
                },
                {
                    "sent": "Complex behavior or real behavior.",
                    "label": 0
                },
                {
                    "sent": "And it is important to note that the classical algorithm so average gradient descent oh so accelerated gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Choose for each eigenvalue the same behavior.",
                    "label": 0
                },
                {
                    "sent": "So accelerated within decent all the eigenvalue would be complex if average gradient descent, they all will be real.",
                    "label": 0
                },
                {
                    "sent": "OK, but like the direct computation of the eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Will give you some rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "But they won't be efficient because they will depend on HI, which could be arbitrarily slow.",
                    "label": 0
                },
                {
                    "sent": "So we have to derive some general bound.",
                    "label": 0
                },
                {
                    "sent": "You definitely have hi.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this one will be the minimum of three terms in order to be valid in all the triangle for all the different value possible of Alpha L meta.",
                    "label": 0
                },
                {
                    "sent": "So the first bunker I spoke to the acceleration when Alpha is non zero and if Alpha equals zero we will have to bound one.",
                    "label": 0
                },
                {
                    "sent": "Bone is a classical rate of 1 / N for average gradient descent and the other one for 1 / N square for some starting point where chosen OK. And so invest independence.",
                    "label": 1
                },
                {
                    "sent": "I'm not improbable Ann to prove this.",
                    "label": 1
                },
                {
                    "sent": "We just define some different Lyapunov function of difference in pace, the pace of the triangle.",
                    "label": 0
                },
                {
                    "sent": "So to conclude, the deterministic spark part, we just saw that in order to have 1 / N square rate of convergence, we should have Alpha strictly positive.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "We will consider the situation when where there is some additive noise.",
                    "label": 0
                },
                {
                    "sent": "So we have a noisy Oracle for the gradient.",
                    "label": 1
                },
                {
                    "sent": "And we take a very simple, noise independent, uncorrelated your mean with abundant covariance.",
                    "label": 0
                },
                {
                    "sent": "And we re write.",
                    "label": 0
                },
                {
                    "sent": "Second parameter difference equation and we first see that.",
                    "label": 0
                },
                {
                    "sent": "If Alpha is nonzero, so if we want to have some acceleration, we will have an important level of noise.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we wrote.",
                    "label": 0
                },
                {
                    "sent": "We write it as a linear system.",
                    "label": 0
                },
                {
                    "sent": "And we can compute the expected 2nd order moment, which will decompose as the sum of two term a bias term.",
                    "label": 1
                },
                {
                    "sent": "Which will only depend.",
                    "label": 0
                },
                {
                    "sent": "And the initial condition and which we have already studied before.",
                    "label": 0
                },
                {
                    "sent": "And the value in STEM, which will depend on the noise and that we can compute.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is our convergence result for the case of instructor noise.",
                    "label": 0
                },
                {
                    "sent": "We see on the triangle the two classical algorithms and that we will explore the area in between them.",
                    "label": 0
                },
                {
                    "sent": "So the bound.",
                    "label": 0
                },
                {
                    "sent": "Will will be as before the minimum of two bound to be valid and all the area and will be the sum of the bias term plus the variance term.",
                    "label": 0
                },
                {
                    "sent": "And we do not buy big N so reason of time because after we re label the step size Alpha and beta two dependent began.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for Alpha equals zero and beta equals 1 / L. This is the horizon gradient descent.",
                    "label": 0
                },
                {
                    "sent": "We find again as in back in Berlin.",
                    "label": 0
                },
                {
                    "sent": "That this algorithm is bounded but not converging.",
                    "label": 1
                },
                {
                    "sent": "In fact, celebrated written dissent.",
                    "label": 0
                },
                {
                    "sent": "Our bound is diverging and in we also see that in experiment.",
                    "label": 0
                },
                {
                    "sent": "And for Alpha equals zero and beta equals 1 / sqrt N. This is a classical rate of four.",
                    "label": 1
                },
                {
                    "sent": "Stochastic gradient descent is in a stronger case.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you know the value of the bias in the variance, you can choose the optimal value for Alpha and beta and you will get the optimal bias variance tradeoff.",
                    "label": 0
                },
                {
                    "sent": "As in land, who will always yell.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For this model of noise.",
                    "label": 0
                },
                {
                    "sent": "We don't recover new results, but we just propose a new interpretation.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if we assume more on the noise.",
                    "label": 1
                },
                {
                    "sent": "If you assume the noise is structured as in this clarification, so where the.",
                    "label": 1
                },
                {
                    "sent": "Covariance of the nose will be less than the covariance of the output.",
                    "label": 0
                },
                {
                    "sent": "We will be able to drive better bond.",
                    "label": 0
                },
                {
                    "sent": "OK. Sonofa Alpha equals.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in beta equals 1 / N Thanks, average gradient descent, we have a convergence speed of 1 / N. For Alpha equals we take well 1 / N we will have accelerated gradient descent which will be bounded but not converging to 0.",
                    "label": 1
                },
                {
                    "sent": "And for Alpha equals 1 / N power A we will be able to achieve a bias variance tradeoff by changing the value of Alpha.",
                    "label": 1
                },
                {
                    "sent": "So if we you know that your problem, we will have important bias.",
                    "label": 1
                },
                {
                    "sent": "You can take the right value of a.",
                    "label": 0
                },
                {
                    "sent": "And the same for violence.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we also studied the minimax conversion, straight since we know that.",
                    "label": 0
                },
                {
                    "sent": "For quadratic problem the conversion rate will be the same of the bias term plus the variance term.",
                    "label": 1
                },
                {
                    "sent": "And we already know the minimax lower bound for the for both, so the bias times 1 / N square.",
                    "label": 0
                },
                {
                    "sent": "Hey Mr Huff and for civilian stem it will be easier be 1 / sqrt N or 1 / N wait see back off.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we are able to attend this tool elbowing simultaneously for some situation, but not in all the cases that remain an open problem.",
                    "label": 0
                },
                {
                    "sent": "So income.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asian.",
                    "label": 0
                },
                {
                    "sent": "We propose a joint analysis of averaging and acceleration is the same framework.",
                    "label": 1
                },
                {
                    "sent": "And we propose, in the presence of noise.",
                    "label": 0
                },
                {
                    "sent": "A new choice of stepsize.",
                    "label": 1
                },
                {
                    "sent": "Which will exhibit the bus aspect of both Android.",
                    "label": 0
                },
                {
                    "sent": "That is how a lot of extension possible.",
                    "label": 1
                },
                {
                    "sent": "First just a deeper, more complex noise independent.",
                    "label": 0
                },
                {
                    "sent": "For example, a stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And to extend this results to non quadratic functions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}