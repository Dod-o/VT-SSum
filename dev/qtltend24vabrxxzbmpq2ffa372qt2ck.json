{
    "id": "qtltend24vabrxxzbmpq2ffa372qt2ck",
    "title": "Grafting-Light: Fast, Incremental Feature Selection and Structure Learning of Markov Random Fields",
    "info": {
        "author": [
            "Jun Zhu, Department of Computer Science and Technology, Tsinghua University"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/kdd2010_zhu_glfi/",
    "segmentation": [
        [
            "OK, alright so so today I will talk about fast diagram for feature selection.",
            "Structure learning is a particular case of Marco Redfield."
        ],
        [
            "So here is the perfect introduction of my career field.",
            "So basically it's an anti aircraft model so it has some strong theoretical foundation in probability and graph theory.",
            "So you have been widely applied in many domains for some natural language processing, social network analysis and data mining and so on.",
            "So in this exists talk particular focus on conditional marker and field.",
            "So it's called CRF.",
            "So because because of its outstanding performance in many cases.",
            "So basically the model is like this.",
            "So here is a simple example for image understanding task.",
            "So basically our input axes images so you can pre segmented into small pages.",
            "Your output is represented here is replacing Y as Y.",
            "So you can define different level space for this particular task.",
            "For example if you do object detection you can define some object categories.",
            "This white space.",
            "So in this model you define your direct devices conditional probability which is log linear model.",
            "You have a set of feature functions which we evaluate on the.",
            "Our small piece called here called clearly so you have a set of parameters so the task of learning is to admit this pyramid so the the most commonly used criteria to learn is so called maximum condition like code, so you maximize the maximizing or minimizing the negative log likelihood.",
            "So the key thing here is if you do the dirt.",
            "If Techdirt, if you get the.",
            "The credit so you can see I basically it's content two terms.",
            "The first term is the protection of the function under the model distribution.",
            "In the second term is impure count in your training data.",
            "So here assumes that rain data fully observed, so you have a true label for each case.",
            "So.",
            "So you can't model the first term is very hard because you need to infer the marginal probability on each piece.",
            "So for example, for this great model, you need to infer the marginal probability for changing individual variable and Chappelle variable on one edge.",
            "So, so this is a pencil with subroutine for edge matching and also other tasks.",
            "For example structure learning.",
            "As I make I introduced later.",
            "So I I do care, I do algorithm for marker and fields should be.",
            "We prefer the performing inference on burst past graph, so that means the influence will be cheap.",
            "So also we prefer our algorithms that converge fast.",
            "That means you can you take a few steps of great design.",
            "So this is a.",
            "The basic motivation for our development of the algorithm.",
            "So."
        ],
        [
            "Why use a marker and field?",
            "Basically there are two problems need to pay special attention to, so the first one is feature selection because conditioner Enfield can in principle can use after feature.",
            "So you have overlaps feature space.",
            "So for example, is this NP chunking case you have?",
            "You can define most 3 million features so so basically here for example in Graham WordPress tech features.",
            "So.",
            "So I complicated or high dimensional feature space means it's more likely to overfit, so so we would like to select a subset of significant feature.",
            "So this is task of feature selection does so in this case as I will show you, so actually move in move in 99% features type be discarded without significantly reduce the performance.",
            "So another problem is the structure learning.",
            "So.",
            "So since the variety on the scale problem increase so.",
            "Kind of crafting this this kind of model featured model structures is is intractable, it I mean it's less applicable so.",
            "So structured learning is a task to automatically discover the structure of market render field."
        ],
        [
            "So let's formulate this problem.",
            "So first feature selection is in general is NP hard, so so the current approximate approximate approaches can be grouped into three categories of filter method, feature selection and learning two separate steps.",
            "So wrapping method learning as a black box, you just use the learning machine to evaluate the feature set and then select the good one.",
            "So embedding method is it's kind of the best strategy to integrate the feature selection learning together so you can you know you can get the information of learning machine.",
            "When you do feature selection, so this is probably the best choice you can."
        ],
        [
            "So one particular implementation of embedded feature selection is it's based on the regular optimization formulation.",
            "So basically you minimize the hybrid object function.",
            "So tradeoff between the.",
            "The goodness of fit, for example, the training error and model complexity.",
            "So you have one parameter, Lambda, two to control the trade off.",
            "So in the case of CRF, we are.",
            "We consider the loss function as the log loss, so basically it's convex and it's the 2nd order.",
            "Differentiable, so this is a nice property.",
            "So for the penalty term we choose their norm, which also convex and but it's simulator training to give you sparse at Matt.",
            "So this is this is again a nice property for the perimeter."
        ],
        [
            "So for the structured learning problem.",
            "So when traditional approaches based on local local search, so you have a scoring function to guide the search in the landscape of the object function.",
            "So basically it's similar to the wrapper feature selection method.",
            "It treats structure learning under the parameter dimension 2 step, so you take the primary measure subroutine and do the.",
            "I choose the optimal structure.",
            "So.",
            "Uh, I mean a better solution.",
            "Maybe it's.",
            "It's the little card approach, so it restructuring parameter estimation you join from work.",
            "So this is formula recently in the in this two papers."
        ],
        [
            "So we also focus on this integral approach."
        ],
        [
            "So here is the idea of the tree structure.",
            "Learning as recognized optimization problem.",
            "So basically you are sure.",
            "Associated to add it possible edge with a set of features.",
            "So then you do the feature selection as the user random regular optimization problem.",
            "So the idea is if the set of feature associated with one edge 0, so that means there's there are no contribution from this edge.",
            "Is a model distribution, so you can.",
            "You can basically ignore this.",
            "Edge is showing.",
            "This actually doesn't exist, so this is a key idea.",
            "So here I went.",
            "Important thing to notice if you consider all the possible features together you will get a complete graph.",
            "So because you will consider every pair of dependence."
        ],
        [
            "OK, so so we have a formula structure learning and feature selection is right regular optimization problem.",
            "So now the problem is how to efficiently solve this recognized optimization.",
            "So I reminded you, so an ideal algorithm.",
            "I should meet two requirements.",
            "So so here is, existing servers are basically.",
            "I mean there are two types of solutions for this, regular optimization.",
            "Always the better method and otherwise the incremental method here for better at least all the features are present during the whole process of feature selection.",
            "So for some great dissent or stochastic redesign or other versions so all they treat assumes all the features are present.",
            "At the very beginning of the feature selection procedure procedure.",
            "So this method can scale up to millions of features.",
            "For example, use the cost of neutering cause I Newton great design so.",
            "But the key thing is it's not applicable for structured learning because I as I said, if you consider all the features together you will have a complete graph.",
            "So in graph model literature it's well known that if you do inference on complete graph you will get there no guarantee on the accuracy and the convergence of your algorithm.",
            "So so maybe.",
            "Maybe you will have very accurate grading.",
            "OK, so for so here we were particular focus on incremental method.",
            "So we proposed faster working on this."
        ],
        [
            "So that the procedure were simple is 2 step iterative procedure.",
            "So one step is too great dissent.",
            "Overworking said we may tell works at so another that we choose select some features from the candidate.",
            "So the basic idea is for the great dissent is although the error norm is not differentiable at the origin, but but it's it's differentiable at each often.",
            "So, so there's basic idea.",
            "You can't choose security center particular subspace, so you can do it.",
            "You got to go to cents as your.",
            "So this is the basic idea.",
            "So the first for the second step we just select merge parameters called select unit features from the candidate that doesn't satisfy the Optima."
        ],
        [
            "Condition.",
            "So here are some analysis.",
            "It can be sure if the loss function is convex, bounded below and continuous differentiable, you can guarantee to get the global team, so there also Internet connection to other methods.",
            "So for example is the latest version of the incremental grafting and it's incremental version of the page.",
            "'cause I Newton method.",
            "So more details can be found in our papers."
        ],
        [
            "So, so let me share some impairments so that we will consider three tasks on synthetic data and real NP chunking and also structured learning of OCR characters.",
            "So here we compare with the incremental grafting and badge cause I nude and coding accordingly.",
            "Dissent best Goshen shared method.",
            "So all the implemented in C++ and all the other impairments are wrong standard piece."
        ],
        [
            "So here is synthetic results.",
            "So we generate basically generate the synthetic data, use linear chain CRF.",
            "So we sample the inputs from noise, Gaussian distribution, and we sample the true level of from from the conditional probability defined by random CRF.",
            "So then we use different to admit the primary as select the feature.",
            "So here at 6:30 area through the train time, the error rate and some others.",
            "So basically overall we can see the grafting light perform as good as the optimal.",
            "Best method for optimal?",
            "I mean it used the exact grading and it takes all the other information of the features into consideration.",
            "We went through each step of vertices so so so that's expected to be the fastest algorithm.",
            "So if we consider this to two blocks in the red box, you can see the graph light is efficient, more efficient than the grafting and coordinate.",
            "Mesa.",
            "So you can see these two you get during training.",
            "You can see some green light.",
            "Can't select any group pictures.",
            "If you will reduce this redundant features effectively so you would get or sparse solution.",
            "So for these three.",
            "You can see actually the grafting is greedy algorithm so all the like the coordinate method also created so you can see this electric field.",
            "It should be this one.",
            "They also had some.",
            "Worst performance on the average, so it's kind of on the undefeated."
        ],
        [
            "Data.",
            "So the similar story happens since this real case, so we basically have similar conclusions.",
            "So from this three fingers we can see obvious.",
            "If you are on fitting for this greedy graft and others also as I mentioned, so in this case you can still move that 99% of features can be discarded without significant reduced fees."
        ],
        [
            "The problem is.",
            "OK, so for this structure learning we do this on the OCR characters and the input at 2520 images.",
            "So the total feature is more than 8000.",
            "So here's the problem is on the train time and the lock light code.",
            "So basically we observe the graph light is consistent movie fiction graph and other method.",
            "So overall the incremental method perform better than the better method because better methods do inference on complete graph.",
            "It's very accurate."
        ],
        [
            "Secure though that the proposed change over the against the perimeter of select unit, so we have similar observation so.",
            "So here I show the non 0 features so you can see the page master doesn't choose pass structure because because of the."
        ],
        [
            "Inaccurate credit I also show that the everyday major generated from the model learn by different groups so you can see also the better method give you a kind of blurry results, so it's because of the accurate grading and on sparse not."
        ],
        [
            "Cost structure.",
            "OK, so here's the conclusion.",
            "So we present algorithm for structured learning and feature selection, marker and field.",
            "And we show that incremental or incremental method are better than better method for particular for the structured learning place and graphing lighting is generally more efficient than greedy grafting algorithm.",
            "For future workers, their cell directions, for example, do more analysis on the conversion rate and time complexity, and also to solve some nonconvex problem.",
            "For example, if you learn the structure with some variables.",
            "So here tomorrow is kind of interesting.",
            "So also you can compare with other organ and regular pest analysis so."
        ],
        [
            "Thank you so please come to our Post 11 for more details.",
            "Thank you.",
            "It is very specific to them.",
            "So do you have any idea if you're not doing very well selection reply or?",
            "No.",
            "Apologies, Madam.",
            "So you mean different object function or different algorithm, different objective function or different algorithms so so basically we show yes.",
            "Change.",
            "How are we doing here?",
            "Aerial.",
            "So, so I think the general message I want you to use so you can use the greatest.",
            "So here I talk about Curtis Ms Word for particular optimization problem, so you can choose different object function for different problem.",
            "So here is the general message I wanted to use.",
            "I use lazy version.",
            "So basically you can.",
            "So for grafting basically used to collect feature and do optimization until converted step.",
            "So here you choose a. Lazy version, I just do one step or step step of great design so you can see it can fast the procedure of converges, so that's the basic idea.",
            "I think it can be applied if you if you can.",
            "If your case.",
            "Maybe if you do great design or other coordinate descent method you can, you can consider this kind of lazy version.",
            "So maybe it's hard for you and I'm I'm not sure."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, alright so so today I will talk about fast diagram for feature selection.",
                    "label": 0
                },
                {
                    "sent": "Structure learning is a particular case of Marco Redfield.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the perfect introduction of my career field.",
                    "label": 0
                },
                {
                    "sent": "So basically it's an anti aircraft model so it has some strong theoretical foundation in probability and graph theory.",
                    "label": 1
                },
                {
                    "sent": "So you have been widely applied in many domains for some natural language processing, social network analysis and data mining and so on.",
                    "label": 1
                },
                {
                    "sent": "So in this exists talk particular focus on conditional marker and field.",
                    "label": 1
                },
                {
                    "sent": "So it's called CRF.",
                    "label": 1
                },
                {
                    "sent": "So because because of its outstanding performance in many cases.",
                    "label": 0
                },
                {
                    "sent": "So basically the model is like this.",
                    "label": 0
                },
                {
                    "sent": "So here is a simple example for image understanding task.",
                    "label": 0
                },
                {
                    "sent": "So basically our input axes images so you can pre segmented into small pages.",
                    "label": 0
                },
                {
                    "sent": "Your output is represented here is replacing Y as Y.",
                    "label": 0
                },
                {
                    "sent": "So you can define different level space for this particular task.",
                    "label": 0
                },
                {
                    "sent": "For example if you do object detection you can define some object categories.",
                    "label": 0
                },
                {
                    "sent": "This white space.",
                    "label": 0
                },
                {
                    "sent": "So in this model you define your direct devices conditional probability which is log linear model.",
                    "label": 0
                },
                {
                    "sent": "You have a set of feature functions which we evaluate on the.",
                    "label": 0
                },
                {
                    "sent": "Our small piece called here called clearly so you have a set of parameters so the task of learning is to admit this pyramid so the the most commonly used criteria to learn is so called maximum condition like code, so you maximize the maximizing or minimizing the negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So the key thing here is if you do the dirt.",
                    "label": 0
                },
                {
                    "sent": "If Techdirt, if you get the.",
                    "label": 0
                },
                {
                    "sent": "The credit so you can see I basically it's content two terms.",
                    "label": 0
                },
                {
                    "sent": "The first term is the protection of the function under the model distribution.",
                    "label": 0
                },
                {
                    "sent": "In the second term is impure count in your training data.",
                    "label": 0
                },
                {
                    "sent": "So here assumes that rain data fully observed, so you have a true label for each case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So you can't model the first term is very hard because you need to infer the marginal probability on each piece.",
                    "label": 0
                },
                {
                    "sent": "So for example, for this great model, you need to infer the marginal probability for changing individual variable and Chappelle variable on one edge.",
                    "label": 1
                },
                {
                    "sent": "So, so this is a pencil with subroutine for edge matching and also other tasks.",
                    "label": 0
                },
                {
                    "sent": "For example structure learning.",
                    "label": 0
                },
                {
                    "sent": "As I make I introduced later.",
                    "label": 0
                },
                {
                    "sent": "So I I do care, I do algorithm for marker and fields should be.",
                    "label": 0
                },
                {
                    "sent": "We prefer the performing inference on burst past graph, so that means the influence will be cheap.",
                    "label": 0
                },
                {
                    "sent": "So also we prefer our algorithms that converge fast.",
                    "label": 0
                },
                {
                    "sent": "That means you can you take a few steps of great design.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "The basic motivation for our development of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why use a marker and field?",
                    "label": 0
                },
                {
                    "sent": "Basically there are two problems need to pay special attention to, so the first one is feature selection because conditioner Enfield can in principle can use after feature.",
                    "label": 1
                },
                {
                    "sent": "So you have overlaps feature space.",
                    "label": 0
                },
                {
                    "sent": "So for example, is this NP chunking case you have?",
                    "label": 0
                },
                {
                    "sent": "You can define most 3 million features so so basically here for example in Graham WordPress tech features.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "So I complicated or high dimensional feature space means it's more likely to overfit, so so we would like to select a subset of significant feature.",
                    "label": 1
                },
                {
                    "sent": "So this is task of feature selection does so in this case as I will show you, so actually move in move in 99% features type be discarded without significantly reduce the performance.",
                    "label": 1
                },
                {
                    "sent": "So another problem is the structure learning.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So since the variety on the scale problem increase so.",
                    "label": 0
                },
                {
                    "sent": "Kind of crafting this this kind of model featured model structures is is intractable, it I mean it's less applicable so.",
                    "label": 0
                },
                {
                    "sent": "So structured learning is a task to automatically discover the structure of market render field.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's formulate this problem.",
                    "label": 0
                },
                {
                    "sent": "So first feature selection is in general is NP hard, so so the current approximate approximate approaches can be grouped into three categories of filter method, feature selection and learning two separate steps.",
                    "label": 1
                },
                {
                    "sent": "So wrapping method learning as a black box, you just use the learning machine to evaluate the feature set and then select the good one.",
                    "label": 1
                },
                {
                    "sent": "So embedding method is it's kind of the best strategy to integrate the feature selection learning together so you can you know you can get the information of learning machine.",
                    "label": 0
                },
                {
                    "sent": "When you do feature selection, so this is probably the best choice you can.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one particular implementation of embedded feature selection is it's based on the regular optimization formulation.",
                    "label": 0
                },
                {
                    "sent": "So basically you minimize the hybrid object function.",
                    "label": 0
                },
                {
                    "sent": "So tradeoff between the.",
                    "label": 0
                },
                {
                    "sent": "The goodness of fit, for example, the training error and model complexity.",
                    "label": 1
                },
                {
                    "sent": "So you have one parameter, Lambda, two to control the trade off.",
                    "label": 0
                },
                {
                    "sent": "So in the case of CRF, we are.",
                    "label": 1
                },
                {
                    "sent": "We consider the loss function as the log loss, so basically it's convex and it's the 2nd order.",
                    "label": 0
                },
                {
                    "sent": "Differentiable, so this is a nice property.",
                    "label": 0
                },
                {
                    "sent": "So for the penalty term we choose their norm, which also convex and but it's simulator training to give you sparse at Matt.",
                    "label": 0
                },
                {
                    "sent": "So this is this is again a nice property for the perimeter.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the structured learning problem.",
                    "label": 0
                },
                {
                    "sent": "So when traditional approaches based on local local search, so you have a scoring function to guide the search in the landscape of the object function.",
                    "label": 1
                },
                {
                    "sent": "So basically it's similar to the wrapper feature selection method.",
                    "label": 1
                },
                {
                    "sent": "It treats structure learning under the parameter dimension 2 step, so you take the primary measure subroutine and do the.",
                    "label": 0
                },
                {
                    "sent": "I choose the optimal structure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Uh, I mean a better solution.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's.",
                    "label": 1
                },
                {
                    "sent": "It's the little card approach, so it restructuring parameter estimation you join from work.",
                    "label": 0
                },
                {
                    "sent": "So this is formula recently in the in this two papers.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we also focus on this integral approach.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the idea of the tree structure.",
                    "label": 0
                },
                {
                    "sent": "Learning as recognized optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So basically you are sure.",
                    "label": 0
                },
                {
                    "sent": "Associated to add it possible edge with a set of features.",
                    "label": 0
                },
                {
                    "sent": "So then you do the feature selection as the user random regular optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So the idea is if the set of feature associated with one edge 0, so that means there's there are no contribution from this edge.",
                    "label": 0
                },
                {
                    "sent": "Is a model distribution, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can basically ignore this.",
                    "label": 0
                },
                {
                    "sent": "Edge is showing.",
                    "label": 0
                },
                {
                    "sent": "This actually doesn't exist, so this is a key idea.",
                    "label": 0
                },
                {
                    "sent": "So here I went.",
                    "label": 0
                },
                {
                    "sent": "Important thing to notice if you consider all the possible features together you will get a complete graph.",
                    "label": 0
                },
                {
                    "sent": "So because you will consider every pair of dependence.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so we have a formula structure learning and feature selection is right regular optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So now the problem is how to efficiently solve this recognized optimization.",
                    "label": 0
                },
                {
                    "sent": "So I reminded you, so an ideal algorithm.",
                    "label": 0
                },
                {
                    "sent": "I should meet two requirements.",
                    "label": 0
                },
                {
                    "sent": "So so here is, existing servers are basically.",
                    "label": 0
                },
                {
                    "sent": "I mean there are two types of solutions for this, regular optimization.",
                    "label": 0
                },
                {
                    "sent": "Always the better method and otherwise the incremental method here for better at least all the features are present during the whole process of feature selection.",
                    "label": 1
                },
                {
                    "sent": "So for some great dissent or stochastic redesign or other versions so all they treat assumes all the features are present.",
                    "label": 0
                },
                {
                    "sent": "At the very beginning of the feature selection procedure procedure.",
                    "label": 0
                },
                {
                    "sent": "So this method can scale up to millions of features.",
                    "label": 0
                },
                {
                    "sent": "For example, use the cost of neutering cause I Newton great design so.",
                    "label": 0
                },
                {
                    "sent": "But the key thing is it's not applicable for structured learning because I as I said, if you consider all the features together you will have a complete graph.",
                    "label": 1
                },
                {
                    "sent": "So in graph model literature it's well known that if you do inference on complete graph you will get there no guarantee on the accuracy and the convergence of your algorithm.",
                    "label": 0
                },
                {
                    "sent": "So so maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe you will have very accurate grading.",
                    "label": 0
                },
                {
                    "sent": "OK, so for so here we were particular focus on incremental method.",
                    "label": 0
                },
                {
                    "sent": "So we proposed faster working on this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that the procedure were simple is 2 step iterative procedure.",
                    "label": 0
                },
                {
                    "sent": "So one step is too great dissent.",
                    "label": 0
                },
                {
                    "sent": "Overworking said we may tell works at so another that we choose select some features from the candidate.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is for the great dissent is although the error norm is not differentiable at the origin, but but it's it's differentiable at each often.",
                    "label": 0
                },
                {
                    "sent": "So, so there's basic idea.",
                    "label": 0
                },
                {
                    "sent": "You can't choose security center particular subspace, so you can do it.",
                    "label": 0
                },
                {
                    "sent": "You got to go to cents as your.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic idea.",
                    "label": 0
                },
                {
                    "sent": "So the first for the second step we just select merge parameters called select unit features from the candidate that doesn't satisfy the Optima.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Condition.",
                    "label": 0
                },
                {
                    "sent": "So here are some analysis.",
                    "label": 0
                },
                {
                    "sent": "It can be sure if the loss function is convex, bounded below and continuous differentiable, you can guarantee to get the global team, so there also Internet connection to other methods.",
                    "label": 0
                },
                {
                    "sent": "So for example is the latest version of the incremental grafting and it's incremental version of the page.",
                    "label": 0
                },
                {
                    "sent": "'cause I Newton method.",
                    "label": 0
                },
                {
                    "sent": "So more details can be found in our papers.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so let me share some impairments so that we will consider three tasks on synthetic data and real NP chunking and also structured learning of OCR characters.",
                    "label": 0
                },
                {
                    "sent": "So here we compare with the incremental grafting and badge cause I nude and coding accordingly.",
                    "label": 1
                },
                {
                    "sent": "Dissent best Goshen shared method.",
                    "label": 0
                },
                {
                    "sent": "So all the implemented in C++ and all the other impairments are wrong standard piece.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is synthetic results.",
                    "label": 0
                },
                {
                    "sent": "So we generate basically generate the synthetic data, use linear chain CRF.",
                    "label": 0
                },
                {
                    "sent": "So we sample the inputs from noise, Gaussian distribution, and we sample the true level of from from the conditional probability defined by random CRF.",
                    "label": 0
                },
                {
                    "sent": "So then we use different to admit the primary as select the feature.",
                    "label": 0
                },
                {
                    "sent": "So here at 6:30 area through the train time, the error rate and some others.",
                    "label": 0
                },
                {
                    "sent": "So basically overall we can see the grafting light perform as good as the optimal.",
                    "label": 0
                },
                {
                    "sent": "Best method for optimal?",
                    "label": 0
                },
                {
                    "sent": "I mean it used the exact grading and it takes all the other information of the features into consideration.",
                    "label": 0
                },
                {
                    "sent": "We went through each step of vertices so so so that's expected to be the fastest algorithm.",
                    "label": 0
                },
                {
                    "sent": "So if we consider this to two blocks in the red box, you can see the graph light is efficient, more efficient than the grafting and coordinate.",
                    "label": 0
                },
                {
                    "sent": "Mesa.",
                    "label": 0
                },
                {
                    "sent": "So you can see these two you get during training.",
                    "label": 0
                },
                {
                    "sent": "You can see some green light.",
                    "label": 0
                },
                {
                    "sent": "Can't select any group pictures.",
                    "label": 0
                },
                {
                    "sent": "If you will reduce this redundant features effectively so you would get or sparse solution.",
                    "label": 0
                },
                {
                    "sent": "So for these three.",
                    "label": 0
                },
                {
                    "sent": "You can see actually the grafting is greedy algorithm so all the like the coordinate method also created so you can see this electric field.",
                    "label": 0
                },
                {
                    "sent": "It should be this one.",
                    "label": 0
                },
                {
                    "sent": "They also had some.",
                    "label": 0
                },
                {
                    "sent": "Worst performance on the average, so it's kind of on the undefeated.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data.",
                    "label": 0
                },
                {
                    "sent": "So the similar story happens since this real case, so we basically have similar conclusions.",
                    "label": 0
                },
                {
                    "sent": "So from this three fingers we can see obvious.",
                    "label": 0
                },
                {
                    "sent": "If you are on fitting for this greedy graft and others also as I mentioned, so in this case you can still move that 99% of features can be discarded without significant reduced fees.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is.",
                    "label": 0
                },
                {
                    "sent": "OK, so for this structure learning we do this on the OCR characters and the input at 2520 images.",
                    "label": 0
                },
                {
                    "sent": "So the total feature is more than 8000.",
                    "label": 0
                },
                {
                    "sent": "So here's the problem is on the train time and the lock light code.",
                    "label": 0
                },
                {
                    "sent": "So basically we observe the graph light is consistent movie fiction graph and other method.",
                    "label": 0
                },
                {
                    "sent": "So overall the incremental method perform better than the better method because better methods do inference on complete graph.",
                    "label": 0
                },
                {
                    "sent": "It's very accurate.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Secure though that the proposed change over the against the perimeter of select unit, so we have similar observation so.",
                    "label": 0
                },
                {
                    "sent": "So here I show the non 0 features so you can see the page master doesn't choose pass structure because because of the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inaccurate credit I also show that the everyday major generated from the model learn by different groups so you can see also the better method give you a kind of blurry results, so it's because of the accurate grading and on sparse not.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cost structure.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the conclusion.",
                    "label": 0
                },
                {
                    "sent": "So we present algorithm for structured learning and feature selection, marker and field.",
                    "label": 0
                },
                {
                    "sent": "And we show that incremental or incremental method are better than better method for particular for the structured learning place and graphing lighting is generally more efficient than greedy grafting algorithm.",
                    "label": 0
                },
                {
                    "sent": "For future workers, their cell directions, for example, do more analysis on the conversion rate and time complexity, and also to solve some nonconvex problem.",
                    "label": 0
                },
                {
                    "sent": "For example, if you learn the structure with some variables.",
                    "label": 0
                },
                {
                    "sent": "So here tomorrow is kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "So also you can compare with other organ and regular pest analysis so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you so please come to our Post 11 for more details.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "It is very specific to them.",
                    "label": 0
                },
                {
                    "sent": "So do you have any idea if you're not doing very well selection reply or?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Apologies, Madam.",
                    "label": 0
                },
                {
                    "sent": "So you mean different object function or different algorithm, different objective function or different algorithms so so basically we show yes.",
                    "label": 0
                },
                {
                    "sent": "Change.",
                    "label": 0
                },
                {
                    "sent": "How are we doing here?",
                    "label": 0
                },
                {
                    "sent": "Aerial.",
                    "label": 0
                },
                {
                    "sent": "So, so I think the general message I want you to use so you can use the greatest.",
                    "label": 0
                },
                {
                    "sent": "So here I talk about Curtis Ms Word for particular optimization problem, so you can choose different object function for different problem.",
                    "label": 0
                },
                {
                    "sent": "So here is the general message I wanted to use.",
                    "label": 0
                },
                {
                    "sent": "I use lazy version.",
                    "label": 0
                },
                {
                    "sent": "So basically you can.",
                    "label": 0
                },
                {
                    "sent": "So for grafting basically used to collect feature and do optimization until converted step.",
                    "label": 0
                },
                {
                    "sent": "So here you choose a. Lazy version, I just do one step or step step of great design so you can see it can fast the procedure of converges, so that's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "I think it can be applied if you if you can.",
                    "label": 0
                },
                {
                    "sent": "If your case.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you do great design or other coordinate descent method you can, you can consider this kind of lazy version.",
                    "label": 0
                },
                {
                    "sent": "So maybe it's hard for you and I'm I'm not sure.",
                    "label": 0
                }
            ]
        }
    }
}