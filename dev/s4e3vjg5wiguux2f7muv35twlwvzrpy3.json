{
    "id": "s4e3vjg5wiguux2f7muv35twlwvzrpy3",
    "title": "From Trees to Forests and Rule Sets - A Unified Overview of Ensemble Methods",
    "info": {
        "author": [
            "Giovanni Seni, Santa Clara University",
            "John Elder, Elder Research, Inc."
        ],
        "published": "Aug. 14, 2007",
        "recorded": "August 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Ensemble Methods",
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/kdd07_elder_seni_fttf/",
    "segmentation": [
        [
            "Welcome to tutorial on ensembles.",
            "I'm John Elder and this is Giovanni Seni an we are going to be speaking for an hour with him taking a 30 minute break and then an hour and a half to finish.",
            "The first time would be mostly an overview and then get into the meat of it.",
            "After that you should have notes in front of you and if you have questions please ask them along the way and we'll have a little time built into the schedule to allow for that.",
            "And if we can't get to it during the talk.",
            "We'll both be here the whole conference and and this is a topic that we're very excited about, so I would be happy to talk with you about it.",
            "I want to talk about ensembles mostly using two technologies, trees and regression, but it works with with basically any technology.",
            "So we want to do an overview and show the unified nature of the.",
            "Of the material that's been some exciting, recent theoretical developments.",
            "That kind of explain why this technique, which was invented a couple of different places, work so well."
        ],
        [
            "So our goals are to convey the essentials of what we find to be an exciting field and exciting sort of secret weapon that we as dataminers can have to get extra performance out of our models and two recent developments in particular, we want to focus on important sampling and rule ensembles and provide the foundation for further study and research.",
            "Furthermore, Giovanni has provided references in R so that which is a free.",
            "Code language, much like SRS plus so you can try it out tonight if you like using some of these ideas and some of these references and show a few real examples."
        ],
        [
            "So we have two slides that outline the way we're going to talk.",
            "We want to give the.",
            "The essence of ensembles in a nutshell, with a few illustrations and some examples, and then provide a little bit of a timeline.",
            "Talk about the problem of predictive learning and set up some notation that will be used consistently throughout and then describe decision trees which have received the bulk of the attention for ensemble.",
            "The major reason for that is decision trees are extremely popular, but of all the methods out there, the least accurate alone in their own.",
            "So I liken it to people reading a book.",
            "This got a beautiful cover and wonderful type size and fonts and has an interesting content.",
            "You know, I'm a practical person.",
            "I have a small company.",
            "We live or die off of our results in data mining and so decision trees are so tantalizing because they're so easy to use their powerful and so inaccurate but.",
            "When you ensemble, you make them competitive with other techniques.",
            "At the very least, and so on.",
            "Sampling is particularly powerful with decision trees.",
            "It's also particularly easy to understand that way, so we're going to focus on that.",
            "Talk about regression tree induction.",
            "The good things about trees and limitations, and then model selection.",
            "The tradeoff that we all face between bias and variance.",
            "How well you're fitting the model versus how well you're overfitting the model.",
            "Traditional way to control complexity through cost complexity, pruning and cross validation, and general terms.",
            "A regularization, some sort of restraint on the model parameters for the greater good down the road and the lasso, or the last Sue.",
            "It will be the.",
            "The technique we focus on there you'll notice that a lot of our work builds off of excellent work by Jerry Friedman, Trevor Hastie, Rob Tibshirani, Leo Breiman.",
            "So there's some Giants in the field that we're happy to follow and summarize some of their work and point out and pop."
        ],
        [
            "Arise.",
            "So ensemble methods will get into ensemble learning, an important sampling.",
            "And then you may have heard and used some of these particular ensemble techniques which can be looked at as special cases of the general techniques.",
            "So bagging, random forests, Adaboost, Mart, and so forth.",
            "Then we look at ensembles of rules and interpret them and show.",
            "Sometimes I don't think we'll have Giovanni's example from industry, but the paper in the back we have a section of references and he can provide that to anyone who asks us.",
            "We have our email on the cover, so if there's any follow up would be happy to provide."
        ],
        [
            "So ensembles are an algorithmic statistical procedure.",
            "A lot of the methods started out as as sort of heuristic.",
            "Ideas and the theory came along afterwards and sort of cleaned up after the parade, but now the theory is in great shape.",
            "The idea is to combine the fitted values from a number of fitting attempts, thinking of it as a committee of experts if you will and the question which a number of us might argue with in general is is a committee better than an individual.",
            "A camel has been called a committee, a horse designed by committee, you know, and a number of us I'm sure, have too many meetings in our lives and.",
            "Going to deal with other peoples opinions is just another.",
            "People are sometimes a real hindrance to getting our way, but committees can be a protective mechanism away that you don't do too many extreme things and so think of a portfolio of stocks in the sense you're not putting all your money in one.",
            "In one asset or one invention, one idea, but you're spreading it out amongst another.",
            "So there are a number of fields, finance, being foremost among them, where the whole portfolio of ideas is assumed wisdom.",
            "Instead of looking for the single best idea so.",
            "The.",
            "Ensemble methods are iterative and also borrow a lot from bootstrapping, which is an excellent way to assess the risk of a model or the spread of a model.",
            "The original idea was that if you had a weak learner, something that was only slightly better than chance, you could improve it by focusing either on its weakpoints or finding something else that was also a weak learner but disagreed with it in certain places, and perhaps their weak knowledge would combine to form something stronger."
        ],
        [
            "The reason they have gotten so much attention is they seem to work so well.",
            "So in a variety of situations and will show some examples.",
            "One can expect an increase in performance.",
            "Most of us at least early in our careers, have one method that we really, really like sort of.",
            "The saying that to a little boy with a hammer, all the world's a nail.",
            "You know there's a method, maybe not you, but perhaps you have a colleague.",
            "That one method is always the solution, and we've experienced where we reach, we reach a performance peak there.",
            "I think if anybody here is participating in the Netflix Prize, for instance, there was some pretty rapid improvement.",
            "And then there's been a.",
            "Very, very flat asymptotic approach and that seems to be the Case No matter what method 1 uses.",
            "So one hope for improvement beyond that is to incorporate some information from other methods.",
            "The theory came along after the experiments, you know, sort of the excitement of something over here is working.",
            "Why did they make these choices versus others?",
            "But the theory came along and now is in very good shape, but it couldn't have been said a few years ago.",
            "So we can interpret what, what and why they work."
        ],
        [
            "Well.",
            "We're a big fan of data mining products and there are a lot of 'em out there.",
            "There's a bewildering array of techniques to use, so these are some of the.",
            "Products that Elder Research has available.",
            "Ann's used occasionally on things, and they range in price from free to $150,000 and they all the largest ones have multiple different kinds of algorithms.",
            "I guess the good news is that the types of algorithms that are used inside the products are relatively fewer than the.",
            "The number of pro."
        ],
        [
            "So here's an example of the score surfaces or response surfaces that are created by 5 different techniques.",
            "Decision trees have piecewise constant decision surfaces and typically access parallel cuts, so they look sort of like a Manhattan cityscape skyline.",
            "On the far right, with the nearest neighbors they have.",
            "Also piecewise constant if you use the single nearest neighbor or but the region is the region for which that point is the closest to the known points, and so you can have polygonal Polygon shaped regions.",
            "One of the major problems with this is treason.",
            "Nearest neighbors are those sharp cut offs is as your input variable.",
            "As you're at latitude or longitude position here changes.",
            "You fall off a Cliff sometimes and in real life things tend to have warning signals that you tend to maybe in law when you turn 21 you can drink, and before that you can't.",
            "Or if you're going a certain speed limit or not, but actually I guess speed limit is a better example of more of an asymptotically transform.",
            "Your likelihood of being getting a ticket is.",
            "The law is broken when you cross over a sharp boundary, but the likelihood of having any consequences is probably more of a sigmoidal shape or so forth, so neural networks or for polynomial networks or those that have sort of differential differentiable transformations from one spot to the next tend to be more accurate, but then harder to fit than trees.",
            "Down here also some kernel services which are good ways in low dimensions to draw information, conditional expectation information from a smaller number of observations than is possible with other methods, and then a method that I invented sizing.",
            "Throw it up there just for fun where you do piecewise planar approximations to a score surface that allows also conditional expected that only has a conditional expectation, but a conditional variance depending on your location."
        ],
        [
            "So which technique is best?",
            "They're all, they're all out there, they're vying for our attention.",
            "We're going to hear about new algorithms and new twists and algorithms during the next few days.",
            "We did a study a few years back on some of the problems that are in the machine learning repository, so use five different algorithmic techniques.",
            "These raw in S at the time and on six different problems, some of which were real data like a Pima, Indians, diabetes data and some were made up with some Gaussian problems, but all are in the machine learning repository and what I've shown here is the relative performance, so lower is better, so the winner of the diabetes problem was a logistic regression and the loser was a decision tree.",
            "Now the spread between winner and loser wasn't very great.",
            "It was greatest for the problem on the far right and least for the problem on the far left.",
            "So the results have been normalized to emphasize the difference.",
            "But Interestingly, every technique out of those five was either first or second place on two of the six problems.",
            "So every dog has its day an I actually was in Chile and I mentioned that and we had a translator and it was something a little different than what I expected.",
            "So afterwards I asked and they said, oh, I said, tell the pig, Christmas is coming.",
            "So you know.",
            "If you're all up in the high on yourself pig, you know Christmas where you're going to be.",
            "The meal is coming, you know, so it had the same.",
            "Had the negative version of the same point, I wanted to make that, you know.",
            "Every technique has something to Crow about.",
            "There's a Miss Congeniality award for every algorithm at some point, so one can argue about which one is better.",
            "None of them are dominated in this experiment by any others, and they all have their points.",
            "And actually, there have been some interesting studies.",
            "In fact, there was a book that I liked a lot that was written that at the end of it had a decision tree for if your problem is like this, you should go this way in terms of which technique to use and so forth, but there's a better way than all of that.",
            "And of course that's ensembles, so we show."
        ],
        [
            "On the next slide, we show all of these methods applied to these same problems, but ensembled in four different ways.",
            "One of 'em very simply the average.",
            "So the yellow here is if you just take all five estimates from the five different techniques and average them together about whether it's diabetes or not, then your performance on the scale of the previous slide is here with.",
            "As you can see, the averaging does very well over the set of 6 problems, and other techniques like voting.",
            "Every.",
            "We don't average together your real valued estimate, but we just say what do you think?",
            "Yes or no?",
            "What do you think yes or no and take the majority vote and that also does well and then a couple of other techniques, advisor, perceptron and so forth that we were introducing did slightly better, but not not nothing to write home about.",
            "Nothing to write a Journal about.",
            "So the astonishing result to us at the time a decade ago.",
            "So on this experiment was every kind of every kind of reasonable ensemble.",
            "Method seems to work and the two things that seem to be important are that there's an edge for each of the learners, and that there's a variety of opinions amongst the learners so that they have something to contribute to the committee so."
        ],
        [
            "The difference between the individual performance.",
            "If you looked here and you had to choose a single method aside from not doing well in diabetes problem, the neural network looks pretty good.",
            "It wins, and four of the six, and ties.",
            "And so if you kind of think of that red line as your known ahead of time, that neuron that sort of work best on."
        ],
        [
            "Sweet you can get basically that performance or better from these ensembles, so it's a very good risk reduction technique."
        ],
        [
            "And I'll show another example.",
            "Those could be faulted because the data is pretty burned over.",
            "There have been more papers written using those sticks datasets and others in the machine learning repository.",
            "Then there are data points in the data set, so I wanted to I wanted to emphasize that this works in real applications as well.",
            "So we look at a credit scoring problem where you're trying to estimate the number of folks will default on their credit after a certain period of time.",
            "And we show 5 techniques that were used on this real data that took millions of dollars for the client to actually build this data because they were going after clients that were underserved by credit.",
            "Mystic way people who normally didn't even have a chance at credit so they had to build up a database by actually going out and pretty much randomly handing credit out to see how people would handle it and had to build a fresh new models to predict which ones would actually work out to be good credit risks or not.",
            "Well trees alone if we planted trees alone on here and these are people who are split into teams of people who are passionate about their algorithms.",
            "So another thing about comparative studies as I remember.",
            "A Masters student at University of Virginia, where I was spent a year on his decision tree algorithm and then he compared it against the neural net in one afternoon running on the default settings.",
            "So you know one had a lot more tender loving care than the other and they were.",
            "You know, roughly comparable.",
            "So you want when you do a comparative study to have people who are passionate about the algorithm behind it.",
            "So there was a contest here and people got free dinners and so forth that they did better, but trees alone would have been plotted up here.",
            "OK, now bundling the trees together just building a cross validation set of trees which will talk about brought it down into the competitive realm with the others, and so that's one of the lessons we want to take home.",
            "Is that the simple simple ensemble methods with trees?",
            "Take away the worst characteristic of trees.",
            "Stepwise regression didn't do poorly.",
            "It's closest to what the industry standard is scorecard method for.",
            "For this kind of problem, polynomial networks improved on a little bit.",
            "Neural networks further, and Mars, which is if you can.",
            "If it's one of the best single algorithms out there, did a little better.",
            "So when we ensemble then what we did is we ranked the rank the clients and then would average the ranks together a little bit more robust than averaging the estimates together.",
            "In case there's an outlier.",
            "And when we do that, you get this distribution of performance where Mars and polynomial networks did the best and then so forth going adding three at a time for the time in five at a time.",
            "So the end result if you put all five models together again, just averaging the ranks.",
            "And I think fancy no.",
            "None of the new stuff that Giovanni is going to get into the old stuff old school old school ensembles did better out of sample than the best of the individual ones.",
            "And again, you had no way of knowing what this ordering would have been.",
            "This is all out of sample results.",
            "So, and as you can notice, is the distribution, the mean and mean drops pretty much with each stage of ensemble.",
            "Now we took this out to 11 or 12 methods and reached an asymptotes.",
            "But this whole idea of ensemble is a secret weapon that we can we can boost the performance from of.",
            "Pretty much when we've reached the limit.",
            "What we can do with the individual."
        ],
        [
            "So I'm gonna talk about a little bit about the timeline and.",
            "Tree methods date back.",
            "20 three years or so 24 years to cart.",
            "Being actually go way beyond that because they've been invented in multiple fields, but probably the most influential and in many ways the best outline was by.",
            "Leo Breiman, Jerry Freeman and.",
            "Ocean stone and it's a commercial product now, which is a good one, but there are trees in pretty much every suite.",
            "Bagging by Leo Breiman was originally submitted to the Statistics Journal and was rejected and then he got it into the Machine Learning Journal.",
            "In November of 96, where it became a Citation Classic.",
            "Within a year or two, and made him a hero in that field so the statistics field, of which I'm very fan.",
            "I'm an engineer with Giovanni and our engineers, but statisticians were just want to be statisticians, but it has a habit sometimes of eating its own or.",
            "Not recognizing generating a lot of fields that become successful elsewhere, but the idea was to.",
            "To bootstrap aggregate, so a different model.",
            "Then this somewhat Bolt out of the blue Adaboost came out in 90, seven with a very strange and heuristic procedure that.",
            "That got that started to do very well and got a lot of people excited.",
            "And then sort of cleaned up and put into statistical frameworks a few years later.",
            "An important sampling is much more recent, but it's starting to make things clear."
        ],
        [
            "Giovanni, did you want to cover some of the details here, or do you want to do it with me?",
            "I'm not sure it's also John wanted me since I'm a little bit more familiar with this.",
            "He works a lot.",
            "So in parallel, in parallel to the in sample track that we had in the previous timeline, there was a regularization track being developed an and so 11 key paper that is distinguishable.",
            "Taylor Sue Ann where.",
            "A new form of regularization.",
            "Accented, we're going to talk a lot about regularization.",
            "Essentially the error function is supplemented or out meant with a term that measure how complex model is an so they lost over by tipsy.",
            "Ronnie had roots in also in work done by Raymond he he called his technique to gather up and I think it's still some people talk about it and then another important development came with the large.",
            "Everything which allow an iterative calculation of the assault solution by Efron at Stanford is Anthony.",
            "Just like we, he got the medical science from NSA.",
            "And then, more recently, Friedman published this technique or gradient director regularization that allows to extend the losu to our idea of loss functions.",
            "So they lost who was originally for least squares, and so that's also very important.",
            "Anna and I sat at a talk about elastic net by one PhD student of Hospi answer, lasting.",
            "Nate tries to combine the Losu penalty function with without rich regulation kind of penalty function.",
            "Trying to get the best of both of them.",
            "The other major so, so we're going to say a few things about the lawsuit.",
            "We're not going to go into details about these sort of techniques, but the references are there at the end.",
            "The order.",
            "Keep copy that we will cover his ruling samples, so we'll see some of the silent features of trees and why even in the latest code in August poll, despite the limitations, they remain very popular and one of the reasons is interpretability.",
            "It's so comforting to look at the model and get a sense out of it, as opposed to all my neural network told me not to give you credit.",
            "Anna.",
            "When we do an samples as young has been saying, we're going to fix the accuracy problem of trees, but we no longer have one tree that is nice to look at it, but we're going to have hundreds of trees, and so interpretability goes away.",
            "Ruling samples is an effort to get that interpretability back, and so so we will talk about rolling symbols and here.",
            "Just incidentally, I see a strong connection between this new work and pulling samples.",
            "And I worked on five years ago, by Professor Kleinberg, that is called stochastic discrimination.",
            "It's a paper that was published in Palm Beach five years ago, and for non mathematicians that I need, a math is a little hard to follow.",
            "But five years later, I think it's very, very cool to see the connection between these two very powerful techniques.",
            "So the reference to the paper we're not going to say anything more about it.",
            "Stochastic discrimination, but the reference is not painted.",
            "It's also at the end.",
            "Thank you."
        ],
        [
            "Alright, so we're going to do a little bit of overview of predictive learning and then decision trees."
        ],
        [
            "So predictive learning just to get our notation right, we have two independent variables or inputs.",
            "T&P in this example, Anna response.",
            "It's either good or bad.",
            "Good is blue and bad is green.",
            "You might, I don't know how it comes out in the grayscale on your notes.",
            "Sorry bout that.",
            "We couldn't.",
            "We didn't think ahead completely, so you may want to mark while looking at the green.",
            "You know there's a definitely a boundary.",
            "Then there's the one there in the middle.",
            "There's a question, mark, what's that one?",
            "Should that one be labeled?",
            "And depending on what estimation technique you have, you have a different label for that.",
            "Or you may even have an uncertainty value for your determination there.",
            "So we can either be descriptive, trying to summarize the data in some way that's understandable and actionable, or predicted which is more likely.",
            "What we focus on what is the response or the class of that new point."
        ],
        [
            "Metal.",
            "So ordinary linear regression will fit the data.",
            "You maybe would design A1 to the blue and a zero to the green and find your least squares fit, and then that would be a plane.",
            "The 3rd third dimension here would be the one or zero and you would have a plane depending on your threshold.",
            "If so, it was .5.",
            "That would provide a cut off point or decision boundary.",
            "If the two classes had different misclassification costs, that boundary might move, but it would still be the plane would provide the fit.",
            "So if it's a false alarm, false dismissal problem where false alarm was seven times more expensive than false, dismissal is 7 times more expensive than false alarm.",
            "Say you might move that decision boundary to minimize cost, but it will come from that single fit from the plane and the model would be.",
            "F Hat is a function of XA constant, an weighted linear terms.",
            "And here I guess you would have to label them one and minus one to have a decision boundary of zero that's mentioned in the notes.",
            "Now this straight line has it's has its value.",
            "People can do wonderful things with regression if you use the right inputs."
        ],
        [
            "But more flexibility would be very nice and decision trees provide almost the completely opposite way of looking at things where they first make a split and say I'm going to try to solve the whole problem with one single question, typically a single variable in a single threshold, and they try to see what split amongst all of the variables would best separate the two classes and so here AT at 5, where if it's greater than five.",
            "You would call it blue and less than five you would, you would say unknown and.",
            "So to read this, the yes goes to the left.",
            "To read this here.",
            "Mom.",
            "And then you continue iteratively looking at those two sides, sort of independently.",
            "So the data is a fresh look at a fresh look at all the variables in all the splits that are possible on the two sides of the tree, and you continue to carve the space out and classify it with different results.",
            "So you get at the end something that's equivalent to an expert system rule that if T is in the range 25 and P is one of the sets in one and two and three, and it's bad.",
            "Otherwise, it's good would be a good summary of that decision."
        ],
        [
            "So normally you'd be given training data cap in number of cases.",
            "Your output variable, Y inputs variables X form a vector there measured values of the attributes that why being response or output X being the predictors and your data.",
            "This is where non statisticians and statisticians disagree about how to look at things.",
            "The sessions think of there being a true underlying distribution out there and that you've somehow drawn a random sample from that distribution.",
            "And I think that's a useful way of looking at things.",
            "Computer scientists as a rule, don't have as much of an until they've worked with data, don't have as much respect for noise or uncertainty in the data number.",
            "A number of machine learning techniques.",
            "I remember early on would push their process their models, until they could exactly answer all the training data.",
            "But in statistics you know that that leads to overfit because you're fitting the noise as well as the signal.",
            "So there seems to be a much more common understanding of not wanting the model to be too complex at some point and needing regularization."
        ],
        [
            "So.",
            "The two, so the two methods that will try to minimize the risk where there's a loss function averaged over the the output variables and the fitted model.",
            "Ordinary at least ordinary linear regression uses the least squared error loss function.",
            "The minimum can be found instantly with olr with regression, so that's one reason is so popular is the math is so good.",
            "There's kind of a saying that well.",
            "Drunk is looking for his keys outside under the streetlamp and they ask him why says?",
            "You probably lost him in the bar, didn't she said, yeah, but the lights better out here so sometimes that's what happens with using regression.",
            "The math is so beautiful that it's used in places when it's not appropriate.",
            "The trees are neural Nets and clustering and so forth.",
            "Use heuristic algorithms that are iterative in nature.",
            "So they are much slower compared to."
        ],
        [
            "Aggression."
        ],
        [
            "So let's look at trees a little bit.",
            "Here's an example surface built by Decision Tree.",
            "You can think of it as the constants or these.",
            "Estimated constants are the means in the regions they see hats, and this is indicator function if check if your input is in this regions of M then it's a one else it's a zero and then multiply it by that constant.",
            "And that remodel can be thought of as the sum of all these regions.",
            "So these regions are defined by the different levels and their hyper rectangles."
        ],
        [
            "Now the trees allow for different scoring criteria.",
            "Is the two most the two most used R-squared error where the the ideal constant is the mean and the absolute error where the ideal constant is the median of the population?",
            "So one of these two loss functions then is averaged over the data and that is your risk.",
            "So you want to find the."
        ],
        [
            "Tree with the lowest prediction risk.",
            "So you're searching over the space of these constants an regions to minimize squared error, for instance.",
            "This is your tree model.",
            "And to do this in an unrestricted way is extraordinarily difficult.",
            "So one universal technique is to restrict the regions to be disjoint to cover the space and to be simple in some way.",
            "So here's one way you could define a region in an ordered way, and so another region behind it would show up.",
            "But the typical way is to.",
            "Is too.",
            "Provide splits that are independent and conditioned on the previous."
        ],
        [
            "Regional distribution.",
            "So just rest with stepwise linear regression, which is a greedy iterative procedure for deciding which inputs to use in a regression format.",
            "That's the typical way that decision trees are done.",
            "So you start with all the data and check each attribute in each possible split value and define a score function that tells how the quality of the split, how much you reduce the variance if you're using squared error, or how much you do see the least absolute deviation.",
            "If you're using absolute air.",
            "So you choose the input and the split that improve the fit the most.",
            "Again, greedily the single value that improves the most, with no regard to what's going to happen subsequently.",
            "And then you replace your original region with these two new split regions and do it recursively again.",
            "So the big question is when should we stop and if you go to tree talks and so forth.",
            "People will argue about whether we should use a chi squared to stop or if we should overfit and then use car.",
            "The cart technique, for instance, overfits intentionally and then decides how far back to prune and to reabsorb cuts that have been done so forth.",
            "So this simple algorithm can probably be coded up by when I coded up through my first tree algorithm is just one or two pages of C code is all it takes.",
            "But of course to handle real data."
        ],
        [
            "Categorical data and missing values and so forth would take hundreds and sometimes thousands of lines of code.",
            "So some of their great strengths they can deal with irrelevant inputs.",
            "Input either shows up in the tree or doesn't, so it naturally does a variable selection.",
            "And you can you can anything you can measure.",
            "You can use.",
            "You can use categorical variables, ordinal variables, numeric and so forth and you get a an important score.",
            "The better tree algorithms will tell you how useful the variables were.",
            "Monotonic transformations won't affect him, so you don't have any don't have problems with outliers.",
            "If a value is 3.14 or 3100, it's greater than three, and so the questions being asked are insensitive to outliers in your input variables, not your outputs."
        ],
        [
            "Now they're very fast compared to other iterative techniques.",
            "Missing values can be handled a couple of different ways.",
            "Either surrogate splits is how cart desert, where it comes up with substitute splits in case the first variables unknown.",
            "What variables will best replicate the splitting of the data that you might know?",
            "So it is sort of a stack of questions that you ask at any state.",
            "Very clever way of handling missing data, other techniques.",
            "Other algorithms create a new class called Missing.",
            "That may be, it may not be missing Amanda Random.",
            "There may be information and it's missing this and so in that case that might be a good way to split.",
            "Off the shelf there few trainable parameters you could you can kind of use them within minutes of learning about him and if you have a single tree it's very interpretable, at least a few levels.",
            "You can say the say it out as a question which is very useful when you're doing consulting to be able to interpret to your client where their problems are coming from or where the best clients are."
        ],
        [
            "Now they have severe limitations if you're trying to fit a trend.",
            "Piecewise constants are very poor way to do that, and so you also need a lot of data.",
            "It exponentially eats through your data by splitting, splitting the data up, and so in high dimensions, especially, your data is already very sparse."
        ],
        [
            "And if there's low interaction in your underlying data, in other words, just more of a trend than an A&B.",
            "Kind of a thing then.",
            "I have a very hard time feeding a plane where regression is very good, so you can already see an idea here where regression is terrable trees are good.",
            "Maybe we should combine them in some way.",
            "So if there's a dependence on many variables that also."
        ],
        [
            "I have trouble.",
            "Also, because of the greedy search strategy, there's a high variance.",
            "You can change one datapoint in your data set and get a completely different tree.",
            "I've seen that happen and so on that.",
            "Look like a completely different tree.",
            "It may have very very similar performance, but typically in your data.",
            "In real data, your variables are very correlated anyway, so the fact that it split on age instead of height, height might have been just barely second best, and therefore when you change one data point, it changes in that.",
            "That sort of the butterfly effect.",
            "The thing that happens here propagates, and the whole tree looks different.",
            "Now, the end end estimate values might not be as different.",
            "As the apparent difference by looking at the variables so.",
            "So early splits especially propagate their changes and.",
            "It's important to prune, so we should definitely use other methods whenever we can, but also ensembles can save the day with trees and maintain the advantages except for perhaps interpretability, while dramatically increasing."
        ],
        [
            "Accuracy.",
            "Alright, by the way, if there are any questions please feel free to ask him as we go and.",
            "It will also be time at the end.",
            "Wait, I'm sorry.",
            "This is the one I gave you the wrong one.",
            "So I'm going to hide a little bit behind the podium so that I can look at my notes if I need to.",
            "So.",
            "Um?",
            "So next, we're going to talk about model complexity, model selection, and regularization.",
            "Will try to get a intuition as to what bias invariances and this is important, because simple methods succeed by reducing bias, reducing variance, or finding a good tradeoff between the two.",
            "We'll revisit the equation of where do we stop growing the tree?",
            "Which which is particularly the technique using car that is called cost complexity pruning.",
            "And the reason we're going to do that is because it allows us to get an intuition of complexity based regularization.",
            "So before we jump into the law sueann regularization in assembles just looking at what's going on in the pruning.",
            "I think it really gets you the intuition of complexity ways regularization.",
            "Then we'll review cross validation.",
            "I'm sure most of you are familiar with it, but for those that aren't.",
            "Cost, complexity based regularization give rise to one or more meta parameters.",
            "So in addition to your original model parameters, there's going to be a couple of meta parameters, and those are generally estimated with cross validation, so it's important that.",
            "We see that I also cross validation is a very good example of getting a useful thing out of combining and so that's the reason that I wanted to make cross validation and then we will talk a little bit about the lawsuit.",
            "Which will be used as a post processing in the Indian symbol method.",
            "So later on Will will when?",
            "Will have this top level view of assembled, so instead of a bottom up view we're going to have this generic and simple method an the modern day recipe of ensembling calls for a post processing stage using regularization and so."
        ],
        [
            "So again, going back to the question of how big to grow the tree, what is what is the right size of the tree and what is the dilemma that we are facing?",
            "Well, essentially if the number of regions.",
            "Or the number of nodes.",
            "Terminal nodes in the tree is too small.",
            "Then your constant approximation is too crude.",
            "Ann, with this very simple example.",
            "So suppose the target function.",
            "This is our data.",
            "This is the truth, and we already know from the previous section that this is the worst kind of target functions for trees, but nonetheless, if we only have 1 split, namely, we have two terminal nodes, two constants and then the approximation is very crude, right?",
            "That?",
            "Intuitively it's what it's called bias, right?",
            "And it creates error if on the other end the tree is too large.",
            "We have two many terminal nodes, then the three you can grow a tree.",
            "All the way to having one terminal node for every single data point in your data and so so the three will have 0 error on your training data, and so that clearly it's overfitting the data.",
            "And if we get to get a new data set from the same phenomena like currently I work in manufacturing.",
            "So let's say you got some data this week and next from this manufacturing process next week you get another batch of data is the same process you expect the other distribution to be very similar.",
            "The three that you been last week would have won't have an had a 0 error last week.",
            "Won't have a cigarette or this week because the three simply overfit the data, and that's intuitively what we call variants and will will.",
            "Further develop that notion.",
            "And so.",
            "More formally."
        ],
        [
            "Let's let's assume we are in this scenario where?",
            "It's a very common assumption.",
            "A scenario for those of you that don't like the math notation at all.",
            "I'll try simply to walk through the through the intuitions of things.",
            "So all what this is saying, F is our target function that we're trying to model, but we don't know it.",
            "But because either we are not measuring everything that is relevant or we have problems with our measurement equipment.",
            "What we measure is this.",
            "So the response variable is the truth plus some error.",
            "I'm.",
            "Now consider an idealized aggregate estimator, so.",
            "Let's say that going back to my manufacturer example where you can take data week after week after week, so they F hat is going to be the model that you build on a particular data set.",
            "And let's assume that we can take many, many many data sets and create this F bar, which is an average of all those.",
            "Here, the expectation operator you can think of it as an average in operator.",
            "And so.",
            "Now let's look at the question of what is the error of one of these F had on one particular data point under one particular loss function that the square rollos loss that allows easy manipulation an you don't have to follow all of these.",
            "It follows from Trivial algebra an properties of the expectation operator.",
            "But so let's simply stuck here and then look at the end so the error at that particular point.",
            "Using these queries we know is this difference right?",
            "They expect a response minus the response of the F hat, and again through algebra properties of the expectation operator we get this expression here and the first term is what we call the square bias and the second term is what we call variants and the last term is what it's called data usable error.",
            "The error that was there since the beginning and there's nothing we can do about it unless we go and measure more stuff or.",
            "Improve our measurement equipment so.",
            "But what is this telling us the first term?",
            "So the first one is simply telling us So what is the squared bias?",
            "Is the amount by which our average estimator differs from the truth?",
            "Right, the F is a truth.",
            "An F hat is the average estimator.",
            "And then the second term is just the spread of RF had around there meant and so.",
            "And in the next, like there's, I think there's a graph that helps further get intuition for these concepts now.",
            "So what we have is the error is made of these two.",
            "Essentially of these two components.",
            "And so these two components are opposing forces.",
            "If one goes up the other one goes down.",
            "If the other one goes up, the other one goes down, and that's generally the situation we are in.",
            "So what happens is the more flexible your model is, the lower the values that is have, but the more body, and it is subject to it."
        ],
        [
            "So.",
            "Let's try to put graphically all that so.",
            "Think of.",
            "The truth, so we have the truth that we're trying to approximate, but we don't get a realization of the truth.",
            "What we get is something else that has error into it, right?",
            "So this is our Y.",
            "And this blue shaded area is indicative of the the.",
            "Sigma out there right?",
            "So next week I tried to get another realization of the truth and I might get it here and next week here and get it here.",
            "So that's what the circle means.",
            "Now we we have a model space, so we.",
            "And in one of the predictive learning overview slides at John presented.",
            "He said there are three elements essentially in a predictive learning situation.",
            "The moral whether it's a linear or polynomial or whatever an an.",
            "There is core function.",
            "How do we measure the errors and the search algorithm?",
            "How do we go around looking for the parameters of that particular model so?",
            "But the first part is the model and there's a model family, right?",
            "And the truth may or may not be part of the family, right?",
            "I. I'm not sure I was expecting this point to come first, but whatever, so we get this realization and we fit 1F had an.",
            "We're only taking models from the model space, so let's say that that's the closest to that I've had.",
            "Anne.",
            "So.",
            "OK.",
            "But again, if next week I get another realization here, I will fit another point here.",
            "And so on and so that's what this circle is represented.",
            "And then this point here is the average F bar.",
            "So remember, this is F had fit to a particular realization, an F bar is the average accuracy then.",
            "So this is model bias.",
            "Again, the amount by which the average estimator differs from the truth.",
            "And this circle is its variance, which is the spread of the F hats around their main."
        ],
        [
            "I.",
            "The next.",
            "The next idea about bias and variance is the so called.",
            "Tradeoff that I was hinting at, so let's assume that we have.",
            "That we have a way of measuring model complexity in the case of trees, model complexities, just the size of the tree.",
            "And so, so let's assume let's look at this horizontal axis as as a complexity access an so on the far left we have just a stump, namely a root node an.",
            "On the other extreme, we have a tree that has been grown all the way to having one terminal node pair up survey shun in the data.",
            "And so.",
            "Um?",
            "So two things here.",
            "The first one is as I mentioned.",
            "I can grow a tree all the way to having one terminal node per data, so the error the training error is going to be 0.",
            "And so that's why we say that training error is not a useful measurement of model quality.",
            "We need to look at a different kind of data, and that's what it stated here.",
            "If we measure the performance of each of those trees on separate data, the typical behavior is going to look like this right?",
            "And so somewhere there is M star, which is the most appropriate size of the tree."
        ],
        [
            "OK.",
            "So.",
            "How is this done in trees?",
            "So what trees do to find the optimal size of the tree to find the optimal complexity level of the tree is at the score criterion, so also join introduced before these are notation for risk.",
            "So statisticians call it like to call error we call error, they color risk and so I'm following their notation.",
            "So we have.",
            "Chris estimate whether it's base if.",
            "On square or laws of absolute deviation, whatever, we have a way of measuring the risk of the tree, the error of the tree.",
            "But we had this term.",
            "This Alpha absolute value T. It's not really absolute value 3 is this.",
            "That notation stands for the size of the tree.",
            "And so.",
            "Which is which is analogous to the number of degrees of freedom.",
            "So essentially we're going to have these two forces in the score.",
            "At the model can get more complex, but it better reduce their by a substantial amount to compensate for the increased penalty in complexity, and that's what complexity ways regularization is allowed is maintained.",
            "Your error function with something that looks like this.",
            "Um?",
            "Alpha, it's going to become a meta parameter of the procedure, and you can think of it as OK. Is the cost that you pay for every additional terminal node.",
            "Write an Inter parameters are needs to be determined.",
            "I'm.",
            "And KII said that already there the other thing to realize is that deep analyzation term is not data dependent, is deterministic, and that's what it's doing.",
            "This this counterbalancing to the data dependent part of the error function.",
            "Answer Output our goal.",
            "It's rephrase instead of finding the tree that has a minimum risk.",
            "Now we have we are looking for the tree that has the minimum regularize risk."
        ],
        [
            "I'm answering card if you go to the card book, the one published by the Four Titans, there will be a section of called Cost Complexity where where all this is described.",
            "Um so.",
            "Alpha as as we said, Alpha is a meta parameter of the procedure that is going to control the degree of stabilization of the regularize component of the error.",
            "And so at one extreme, if Alpha is 0, there's no regularization.",
            "We get back our original error, right?",
            "And so you get the least stable estimate.",
            "At the other extreme, Alpha is Infinity an it's completely deterministic no matter what the data is telling you the.",
            "Complexity penalty is so high that it always.",
            "Wind on and you get only a stump.",
            "Notary can be built an so in between.",
            "There's this continuum from Alpha equals 0 where you get the unregularized tree to Alpha much, much greater than 0 where you get a fully regularised solution.",
            "An something that happens very nice in trees that makes the.",
            "The the process work computationally very efficiently is that these trees are all nested.",
            "This can be set up in such a way that.",
            "Every tree is a subset of the previous tree.",
            "Um?",
            "And of course, they as we were looking for the optimal value of Alpha in this continuum, and choosing the optimal value of Alpha, is equivalent to choosing the optimal tree, right?",
            "Because once we find the optimal values Alpha, there is a three associated with that particular Alpha.",
            "And."
        ],
        [
            "So.",
            "I think maybe a couple of more slides and then we go on the break.",
            "I'm.",
            "So as we mentioned in the plot, where we have risk or error versus training set and test set, we mentioned that the error on the training set is not a useful estimator and we need a way to estimate what we call prediction trees or future risk or also test risk, right?",
            "So if you don't have that data available.",
            "The approach that that is uses to use cross validation and I was saying that cross validation is also an example of where we combine things in a useful way, so we are combining in cross validation is measures of fit.",
            "Right measures of prediction error.",
            "So.",
            "Very quickly suppose suppose that we have.",
            "If we were to do what is called 3 fold cross validation, we simply split the data into three groups.",
            "And what we're going to do is we take, say, the 1st two part, the 1st two groups, and we build the first 3 on that data, and then we're going to estimate the risk on this one.",
            "Anne.",
            "And so on.",
            "The third time, we're going to build the Third 3 build.",
            "Sorry on the last two partitions and we're going to evaluate it."
        ],
        [
            "That one.",
            "So copying, copying that picture over.",
            "So so.",
            "So we have those three trees everyone built on a fraction of the data, and we have this what it's called the indexing function like.",
            "If we look more carefully, every data point.",
            "So remember Big N is the number of observation.",
            "Every data point is a test point in one of the partitions, so that's what this function is telling us.",
            "The partitioning which.",
            "Observation I is.",
            "And so our cross validated estimate of risk, which we need to estimate Alpha and estimate is just computer like this.",
            "And so again.",
            "What we've done is we've taken an average of measures of fit over the previous split."
        ],
        [
            "Um?",
            "I think let's let's let's stop there and recommend so you guys can stretch coffee bathrooms, yeah?",
            "It's like."
        ],
        [
            "Another way to calculate the average statistic is to.",
            "Calculated for each of the test sets and then average.",
            "Here you're just building 1 set.",
            "By getting the predictions for the points by the model that did not see them in great right?",
            "Yes, just getting once at calculate error or whatever.",
            "Another possibility is to actually build the model on one, then calculate the error on that line and do this three times and then average these people who I believe that's what we're doing here but but, but let's take that offline yeah, so function.",
            "Let's one thing to do is to take that.",
            "Change the cross validation boundaries and do it again, so have multiple layers.",
            "Anyhow.",
            "I I still haven't gotten it.",
            "So, but if you have time now, let's.",
            "Right, so I was just thinking."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Welcome to tutorial on ensembles.",
                    "label": 0
                },
                {
                    "sent": "I'm John Elder and this is Giovanni Seni an we are going to be speaking for an hour with him taking a 30 minute break and then an hour and a half to finish.",
                    "label": 1
                },
                {
                    "sent": "The first time would be mostly an overview and then get into the meat of it.",
                    "label": 0
                },
                {
                    "sent": "After that you should have notes in front of you and if you have questions please ask them along the way and we'll have a little time built into the schedule to allow for that.",
                    "label": 0
                },
                {
                    "sent": "And if we can't get to it during the talk.",
                    "label": 0
                },
                {
                    "sent": "We'll both be here the whole conference and and this is a topic that we're very excited about, so I would be happy to talk with you about it.",
                    "label": 0
                },
                {
                    "sent": "I want to talk about ensembles mostly using two technologies, trees and regression, but it works with with basically any technology.",
                    "label": 0
                },
                {
                    "sent": "So we want to do an overview and show the unified nature of the.",
                    "label": 0
                },
                {
                    "sent": "Of the material that's been some exciting, recent theoretical developments.",
                    "label": 0
                },
                {
                    "sent": "That kind of explain why this technique, which was invented a couple of different places, work so well.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our goals are to convey the essentials of what we find to be an exciting field and exciting sort of secret weapon that we as dataminers can have to get extra performance out of our models and two recent developments in particular, we want to focus on important sampling and rule ensembles and provide the foundation for further study and research.",
                    "label": 1
                },
                {
                    "sent": "Furthermore, Giovanni has provided references in R so that which is a free.",
                    "label": 0
                },
                {
                    "sent": "Code language, much like SRS plus so you can try it out tonight if you like using some of these ideas and some of these references and show a few real examples.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have two slides that outline the way we're going to talk.",
                    "label": 0
                },
                {
                    "sent": "We want to give the.",
                    "label": 0
                },
                {
                    "sent": "The essence of ensembles in a nutshell, with a few illustrations and some examples, and then provide a little bit of a timeline.",
                    "label": 1
                },
                {
                    "sent": "Talk about the problem of predictive learning and set up some notation that will be used consistently throughout and then describe decision trees which have received the bulk of the attention for ensemble.",
                    "label": 0
                },
                {
                    "sent": "The major reason for that is decision trees are extremely popular, but of all the methods out there, the least accurate alone in their own.",
                    "label": 0
                },
                {
                    "sent": "So I liken it to people reading a book.",
                    "label": 0
                },
                {
                    "sent": "This got a beautiful cover and wonderful type size and fonts and has an interesting content.",
                    "label": 0
                },
                {
                    "sent": "You know, I'm a practical person.",
                    "label": 0
                },
                {
                    "sent": "I have a small company.",
                    "label": 0
                },
                {
                    "sent": "We live or die off of our results in data mining and so decision trees are so tantalizing because they're so easy to use their powerful and so inaccurate but.",
                    "label": 0
                },
                {
                    "sent": "When you ensemble, you make them competitive with other techniques.",
                    "label": 0
                },
                {
                    "sent": "At the very least, and so on.",
                    "label": 0
                },
                {
                    "sent": "Sampling is particularly powerful with decision trees.",
                    "label": 1
                },
                {
                    "sent": "It's also particularly easy to understand that way, so we're going to focus on that.",
                    "label": 0
                },
                {
                    "sent": "Talk about regression tree induction.",
                    "label": 1
                },
                {
                    "sent": "The good things about trees and limitations, and then model selection.",
                    "label": 0
                },
                {
                    "sent": "The tradeoff that we all face between bias and variance.",
                    "label": 0
                },
                {
                    "sent": "How well you're fitting the model versus how well you're overfitting the model.",
                    "label": 0
                },
                {
                    "sent": "Traditional way to control complexity through cost complexity, pruning and cross validation, and general terms.",
                    "label": 0
                },
                {
                    "sent": "A regularization, some sort of restraint on the model parameters for the greater good down the road and the lasso, or the last Sue.",
                    "label": 0
                },
                {
                    "sent": "It will be the.",
                    "label": 0
                },
                {
                    "sent": "The technique we focus on there you'll notice that a lot of our work builds off of excellent work by Jerry Friedman, Trevor Hastie, Rob Tibshirani, Leo Breiman.",
                    "label": 0
                },
                {
                    "sent": "So there's some Giants in the field that we're happy to follow and summarize some of their work and point out and pop.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arise.",
                    "label": 0
                },
                {
                    "sent": "So ensemble methods will get into ensemble learning, an important sampling.",
                    "label": 1
                },
                {
                    "sent": "And then you may have heard and used some of these particular ensemble techniques which can be looked at as special cases of the general techniques.",
                    "label": 0
                },
                {
                    "sent": "So bagging, random forests, Adaboost, Mart, and so forth.",
                    "label": 1
                },
                {
                    "sent": "Then we look at ensembles of rules and interpret them and show.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I don't think we'll have Giovanni's example from industry, but the paper in the back we have a section of references and he can provide that to anyone who asks us.",
                    "label": 0
                },
                {
                    "sent": "We have our email on the cover, so if there's any follow up would be happy to provide.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So ensembles are an algorithmic statistical procedure.",
                    "label": 1
                },
                {
                    "sent": "A lot of the methods started out as as sort of heuristic.",
                    "label": 0
                },
                {
                    "sent": "Ideas and the theory came along afterwards and sort of cleaned up after the parade, but now the theory is in great shape.",
                    "label": 0
                },
                {
                    "sent": "The idea is to combine the fitted values from a number of fitting attempts, thinking of it as a committee of experts if you will and the question which a number of us might argue with in general is is a committee better than an individual.",
                    "label": 1
                },
                {
                    "sent": "A camel has been called a committee, a horse designed by committee, you know, and a number of us I'm sure, have too many meetings in our lives and.",
                    "label": 0
                },
                {
                    "sent": "Going to deal with other peoples opinions is just another.",
                    "label": 0
                },
                {
                    "sent": "People are sometimes a real hindrance to getting our way, but committees can be a protective mechanism away that you don't do too many extreme things and so think of a portfolio of stocks in the sense you're not putting all your money in one.",
                    "label": 0
                },
                {
                    "sent": "In one asset or one invention, one idea, but you're spreading it out amongst another.",
                    "label": 0
                },
                {
                    "sent": "So there are a number of fields, finance, being foremost among them, where the whole portfolio of ideas is assumed wisdom.",
                    "label": 0
                },
                {
                    "sent": "Instead of looking for the single best idea so.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Ensemble methods are iterative and also borrow a lot from bootstrapping, which is an excellent way to assess the risk of a model or the spread of a model.",
                    "label": 0
                },
                {
                    "sent": "The original idea was that if you had a weak learner, something that was only slightly better than chance, you could improve it by focusing either on its weakpoints or finding something else that was also a weak learner but disagreed with it in certain places, and perhaps their weak knowledge would combine to form something stronger.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The reason they have gotten so much attention is they seem to work so well.",
                    "label": 0
                },
                {
                    "sent": "So in a variety of situations and will show some examples.",
                    "label": 1
                },
                {
                    "sent": "One can expect an increase in performance.",
                    "label": 0
                },
                {
                    "sent": "Most of us at least early in our careers, have one method that we really, really like sort of.",
                    "label": 0
                },
                {
                    "sent": "The saying that to a little boy with a hammer, all the world's a nail.",
                    "label": 0
                },
                {
                    "sent": "You know there's a method, maybe not you, but perhaps you have a colleague.",
                    "label": 0
                },
                {
                    "sent": "That one method is always the solution, and we've experienced where we reach, we reach a performance peak there.",
                    "label": 0
                },
                {
                    "sent": "I think if anybody here is participating in the Netflix Prize, for instance, there was some pretty rapid improvement.",
                    "label": 0
                },
                {
                    "sent": "And then there's been a.",
                    "label": 0
                },
                {
                    "sent": "Very, very flat asymptotic approach and that seems to be the Case No matter what method 1 uses.",
                    "label": 0
                },
                {
                    "sent": "So one hope for improvement beyond that is to incorporate some information from other methods.",
                    "label": 0
                },
                {
                    "sent": "The theory came along after the experiments, you know, sort of the excitement of something over here is working.",
                    "label": 0
                },
                {
                    "sent": "Why did they make these choices versus others?",
                    "label": 0
                },
                {
                    "sent": "But the theory came along and now is in very good shape, but it couldn't have been said a few years ago.",
                    "label": 0
                },
                {
                    "sent": "So we can interpret what, what and why they work.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "We're a big fan of data mining products and there are a lot of 'em out there.",
                    "label": 1
                },
                {
                    "sent": "There's a bewildering array of techniques to use, so these are some of the.",
                    "label": 0
                },
                {
                    "sent": "Products that Elder Research has available.",
                    "label": 0
                },
                {
                    "sent": "Ann's used occasionally on things, and they range in price from free to $150,000 and they all the largest ones have multiple different kinds of algorithms.",
                    "label": 0
                },
                {
                    "sent": "I guess the good news is that the types of algorithms that are used inside the products are relatively fewer than the.",
                    "label": 0
                },
                {
                    "sent": "The number of pro.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an example of the score surfaces or response surfaces that are created by 5 different techniques.",
                    "label": 1
                },
                {
                    "sent": "Decision trees have piecewise constant decision surfaces and typically access parallel cuts, so they look sort of like a Manhattan cityscape skyline.",
                    "label": 0
                },
                {
                    "sent": "On the far right, with the nearest neighbors they have.",
                    "label": 1
                },
                {
                    "sent": "Also piecewise constant if you use the single nearest neighbor or but the region is the region for which that point is the closest to the known points, and so you can have polygonal Polygon shaped regions.",
                    "label": 0
                },
                {
                    "sent": "One of the major problems with this is treason.",
                    "label": 0
                },
                {
                    "sent": "Nearest neighbors are those sharp cut offs is as your input variable.",
                    "label": 0
                },
                {
                    "sent": "As you're at latitude or longitude position here changes.",
                    "label": 0
                },
                {
                    "sent": "You fall off a Cliff sometimes and in real life things tend to have warning signals that you tend to maybe in law when you turn 21 you can drink, and before that you can't.",
                    "label": 0
                },
                {
                    "sent": "Or if you're going a certain speed limit or not, but actually I guess speed limit is a better example of more of an asymptotically transform.",
                    "label": 0
                },
                {
                    "sent": "Your likelihood of being getting a ticket is.",
                    "label": 0
                },
                {
                    "sent": "The law is broken when you cross over a sharp boundary, but the likelihood of having any consequences is probably more of a sigmoidal shape or so forth, so neural networks or for polynomial networks or those that have sort of differential differentiable transformations from one spot to the next tend to be more accurate, but then harder to fit than trees.",
                    "label": 0
                },
                {
                    "sent": "Down here also some kernel services which are good ways in low dimensions to draw information, conditional expectation information from a smaller number of observations than is possible with other methods, and then a method that I invented sizing.",
                    "label": 0
                },
                {
                    "sent": "Throw it up there just for fun where you do piecewise planar approximations to a score surface that allows also conditional expected that only has a conditional expectation, but a conditional variance depending on your location.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So which technique is best?",
                    "label": 0
                },
                {
                    "sent": "They're all, they're all out there, they're vying for our attention.",
                    "label": 0
                },
                {
                    "sent": "We're going to hear about new algorithms and new twists and algorithms during the next few days.",
                    "label": 0
                },
                {
                    "sent": "We did a study a few years back on some of the problems that are in the machine learning repository, so use five different algorithmic techniques.",
                    "label": 0
                },
                {
                    "sent": "These raw in S at the time and on six different problems, some of which were real data like a Pima, Indians, diabetes data and some were made up with some Gaussian problems, but all are in the machine learning repository and what I've shown here is the relative performance, so lower is better, so the winner of the diabetes problem was a logistic regression and the loser was a decision tree.",
                    "label": 1
                },
                {
                    "sent": "Now the spread between winner and loser wasn't very great.",
                    "label": 0
                },
                {
                    "sent": "It was greatest for the problem on the far right and least for the problem on the far left.",
                    "label": 0
                },
                {
                    "sent": "So the results have been normalized to emphasize the difference.",
                    "label": 0
                },
                {
                    "sent": "But Interestingly, every technique out of those five was either first or second place on two of the six problems.",
                    "label": 0
                },
                {
                    "sent": "So every dog has its day an I actually was in Chile and I mentioned that and we had a translator and it was something a little different than what I expected.",
                    "label": 0
                },
                {
                    "sent": "So afterwards I asked and they said, oh, I said, tell the pig, Christmas is coming.",
                    "label": 0
                },
                {
                    "sent": "So you know.",
                    "label": 0
                },
                {
                    "sent": "If you're all up in the high on yourself pig, you know Christmas where you're going to be.",
                    "label": 0
                },
                {
                    "sent": "The meal is coming, you know, so it had the same.",
                    "label": 0
                },
                {
                    "sent": "Had the negative version of the same point, I wanted to make that, you know.",
                    "label": 0
                },
                {
                    "sent": "Every technique has something to Crow about.",
                    "label": 0
                },
                {
                    "sent": "There's a Miss Congeniality award for every algorithm at some point, so one can argue about which one is better.",
                    "label": 0
                },
                {
                    "sent": "None of them are dominated in this experiment by any others, and they all have their points.",
                    "label": 0
                },
                {
                    "sent": "And actually, there have been some interesting studies.",
                    "label": 0
                },
                {
                    "sent": "In fact, there was a book that I liked a lot that was written that at the end of it had a decision tree for if your problem is like this, you should go this way in terms of which technique to use and so forth, but there's a better way than all of that.",
                    "label": 0
                },
                {
                    "sent": "And of course that's ensembles, so we show.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the next slide, we show all of these methods applied to these same problems, but ensembled in four different ways.",
                    "label": 0
                },
                {
                    "sent": "One of 'em very simply the average.",
                    "label": 0
                },
                {
                    "sent": "So the yellow here is if you just take all five estimates from the five different techniques and average them together about whether it's diabetes or not, then your performance on the scale of the previous slide is here with.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the averaging does very well over the set of 6 problems, and other techniques like voting.",
                    "label": 0
                },
                {
                    "sent": "Every.",
                    "label": 0
                },
                {
                    "sent": "We don't average together your real valued estimate, but we just say what do you think?",
                    "label": 0
                },
                {
                    "sent": "Yes or no?",
                    "label": 0
                },
                {
                    "sent": "What do you think yes or no and take the majority vote and that also does well and then a couple of other techniques, advisor, perceptron and so forth that we were introducing did slightly better, but not not nothing to write home about.",
                    "label": 0
                },
                {
                    "sent": "Nothing to write a Journal about.",
                    "label": 0
                },
                {
                    "sent": "So the astonishing result to us at the time a decade ago.",
                    "label": 0
                },
                {
                    "sent": "So on this experiment was every kind of every kind of reasonable ensemble.",
                    "label": 0
                },
                {
                    "sent": "Method seems to work and the two things that seem to be important are that there's an edge for each of the learners, and that there's a variety of opinions amongst the learners so that they have something to contribute to the committee so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The difference between the individual performance.",
                    "label": 0
                },
                {
                    "sent": "If you looked here and you had to choose a single method aside from not doing well in diabetes problem, the neural network looks pretty good.",
                    "label": 0
                },
                {
                    "sent": "It wins, and four of the six, and ties.",
                    "label": 0
                },
                {
                    "sent": "And so if you kind of think of that red line as your known ahead of time, that neuron that sort of work best on.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sweet you can get basically that performance or better from these ensembles, so it's a very good risk reduction technique.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'll show another example.",
                    "label": 0
                },
                {
                    "sent": "Those could be faulted because the data is pretty burned over.",
                    "label": 0
                },
                {
                    "sent": "There have been more papers written using those sticks datasets and others in the machine learning repository.",
                    "label": 0
                },
                {
                    "sent": "Then there are data points in the data set, so I wanted to I wanted to emphasize that this works in real applications as well.",
                    "label": 0
                },
                {
                    "sent": "So we look at a credit scoring problem where you're trying to estimate the number of folks will default on their credit after a certain period of time.",
                    "label": 0
                },
                {
                    "sent": "And we show 5 techniques that were used on this real data that took millions of dollars for the client to actually build this data because they were going after clients that were underserved by credit.",
                    "label": 0
                },
                {
                    "sent": "Mystic way people who normally didn't even have a chance at credit so they had to build up a database by actually going out and pretty much randomly handing credit out to see how people would handle it and had to build a fresh new models to predict which ones would actually work out to be good credit risks or not.",
                    "label": 0
                },
                {
                    "sent": "Well trees alone if we planted trees alone on here and these are people who are split into teams of people who are passionate about their algorithms.",
                    "label": 0
                },
                {
                    "sent": "So another thing about comparative studies as I remember.",
                    "label": 0
                },
                {
                    "sent": "A Masters student at University of Virginia, where I was spent a year on his decision tree algorithm and then he compared it against the neural net in one afternoon running on the default settings.",
                    "label": 0
                },
                {
                    "sent": "So you know one had a lot more tender loving care than the other and they were.",
                    "label": 0
                },
                {
                    "sent": "You know, roughly comparable.",
                    "label": 0
                },
                {
                    "sent": "So you want when you do a comparative study to have people who are passionate about the algorithm behind it.",
                    "label": 0
                },
                {
                    "sent": "So there was a contest here and people got free dinners and so forth that they did better, but trees alone would have been plotted up here.",
                    "label": 0
                },
                {
                    "sent": "OK, now bundling the trees together just building a cross validation set of trees which will talk about brought it down into the competitive realm with the others, and so that's one of the lessons we want to take home.",
                    "label": 0
                },
                {
                    "sent": "Is that the simple simple ensemble methods with trees?",
                    "label": 0
                },
                {
                    "sent": "Take away the worst characteristic of trees.",
                    "label": 0
                },
                {
                    "sent": "Stepwise regression didn't do poorly.",
                    "label": 0
                },
                {
                    "sent": "It's closest to what the industry standard is scorecard method for.",
                    "label": 0
                },
                {
                    "sent": "For this kind of problem, polynomial networks improved on a little bit.",
                    "label": 0
                },
                {
                    "sent": "Neural networks further, and Mars, which is if you can.",
                    "label": 0
                },
                {
                    "sent": "If it's one of the best single algorithms out there, did a little better.",
                    "label": 0
                },
                {
                    "sent": "So when we ensemble then what we did is we ranked the rank the clients and then would average the ranks together a little bit more robust than averaging the estimates together.",
                    "label": 0
                },
                {
                    "sent": "In case there's an outlier.",
                    "label": 0
                },
                {
                    "sent": "And when we do that, you get this distribution of performance where Mars and polynomial networks did the best and then so forth going adding three at a time for the time in five at a time.",
                    "label": 0
                },
                {
                    "sent": "So the end result if you put all five models together again, just averaging the ranks.",
                    "label": 0
                },
                {
                    "sent": "And I think fancy no.",
                    "label": 0
                },
                {
                    "sent": "None of the new stuff that Giovanni is going to get into the old stuff old school old school ensembles did better out of sample than the best of the individual ones.",
                    "label": 0
                },
                {
                    "sent": "And again, you had no way of knowing what this ordering would have been.",
                    "label": 0
                },
                {
                    "sent": "This is all out of sample results.",
                    "label": 0
                },
                {
                    "sent": "So, and as you can notice, is the distribution, the mean and mean drops pretty much with each stage of ensemble.",
                    "label": 0
                },
                {
                    "sent": "Now we took this out to 11 or 12 methods and reached an asymptotes.",
                    "label": 0
                },
                {
                    "sent": "But this whole idea of ensemble is a secret weapon that we can we can boost the performance from of.",
                    "label": 0
                },
                {
                    "sent": "Pretty much when we've reached the limit.",
                    "label": 0
                },
                {
                    "sent": "What we can do with the individual.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm gonna talk about a little bit about the timeline and.",
                    "label": 0
                },
                {
                    "sent": "Tree methods date back.",
                    "label": 0
                },
                {
                    "sent": "20 three years or so 24 years to cart.",
                    "label": 0
                },
                {
                    "sent": "Being actually go way beyond that because they've been invented in multiple fields, but probably the most influential and in many ways the best outline was by.",
                    "label": 0
                },
                {
                    "sent": "Leo Breiman, Jerry Freeman and.",
                    "label": 0
                },
                {
                    "sent": "Ocean stone and it's a commercial product now, which is a good one, but there are trees in pretty much every suite.",
                    "label": 0
                },
                {
                    "sent": "Bagging by Leo Breiman was originally submitted to the Statistics Journal and was rejected and then he got it into the Machine Learning Journal.",
                    "label": 0
                },
                {
                    "sent": "In November of 96, where it became a Citation Classic.",
                    "label": 0
                },
                {
                    "sent": "Within a year or two, and made him a hero in that field so the statistics field, of which I'm very fan.",
                    "label": 0
                },
                {
                    "sent": "I'm an engineer with Giovanni and our engineers, but statisticians were just want to be statisticians, but it has a habit sometimes of eating its own or.",
                    "label": 0
                },
                {
                    "sent": "Not recognizing generating a lot of fields that become successful elsewhere, but the idea was to.",
                    "label": 0
                },
                {
                    "sent": "To bootstrap aggregate, so a different model.",
                    "label": 0
                },
                {
                    "sent": "Then this somewhat Bolt out of the blue Adaboost came out in 90, seven with a very strange and heuristic procedure that.",
                    "label": 0
                },
                {
                    "sent": "That got that started to do very well and got a lot of people excited.",
                    "label": 0
                },
                {
                    "sent": "And then sort of cleaned up and put into statistical frameworks a few years later.",
                    "label": 0
                },
                {
                    "sent": "An important sampling is much more recent, but it's starting to make things clear.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Giovanni, did you want to cover some of the details here, or do you want to do it with me?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure it's also John wanted me since I'm a little bit more familiar with this.",
                    "label": 0
                },
                {
                    "sent": "He works a lot.",
                    "label": 0
                },
                {
                    "sent": "So in parallel, in parallel to the in sample track that we had in the previous timeline, there was a regularization track being developed an and so 11 key paper that is distinguishable.",
                    "label": 0
                },
                {
                    "sent": "Taylor Sue Ann where.",
                    "label": 0
                },
                {
                    "sent": "A new form of regularization.",
                    "label": 0
                },
                {
                    "sent": "Accented, we're going to talk a lot about regularization.",
                    "label": 0
                },
                {
                    "sent": "Essentially the error function is supplemented or out meant with a term that measure how complex model is an so they lost over by tipsy.",
                    "label": 0
                },
                {
                    "sent": "Ronnie had roots in also in work done by Raymond he he called his technique to gather up and I think it's still some people talk about it and then another important development came with the large.",
                    "label": 0
                },
                {
                    "sent": "Everything which allow an iterative calculation of the assault solution by Efron at Stanford is Anthony.",
                    "label": 0
                },
                {
                    "sent": "Just like we, he got the medical science from NSA.",
                    "label": 0
                },
                {
                    "sent": "And then, more recently, Friedman published this technique or gradient director regularization that allows to extend the losu to our idea of loss functions.",
                    "label": 0
                },
                {
                    "sent": "So they lost who was originally for least squares, and so that's also very important.",
                    "label": 0
                },
                {
                    "sent": "Anna and I sat at a talk about elastic net by one PhD student of Hospi answer, lasting.",
                    "label": 0
                },
                {
                    "sent": "Nate tries to combine the Losu penalty function with without rich regulation kind of penalty function.",
                    "label": 0
                },
                {
                    "sent": "Trying to get the best of both of them.",
                    "label": 0
                },
                {
                    "sent": "The other major so, so we're going to say a few things about the lawsuit.",
                    "label": 0
                },
                {
                    "sent": "We're not going to go into details about these sort of techniques, but the references are there at the end.",
                    "label": 0
                },
                {
                    "sent": "The order.",
                    "label": 0
                },
                {
                    "sent": "Keep copy that we will cover his ruling samples, so we'll see some of the silent features of trees and why even in the latest code in August poll, despite the limitations, they remain very popular and one of the reasons is interpretability.",
                    "label": 0
                },
                {
                    "sent": "It's so comforting to look at the model and get a sense out of it, as opposed to all my neural network told me not to give you credit.",
                    "label": 0
                },
                {
                    "sent": "Anna.",
                    "label": 0
                },
                {
                    "sent": "When we do an samples as young has been saying, we're going to fix the accuracy problem of trees, but we no longer have one tree that is nice to look at it, but we're going to have hundreds of trees, and so interpretability goes away.",
                    "label": 0
                },
                {
                    "sent": "Ruling samples is an effort to get that interpretability back, and so so we will talk about rolling symbols and here.",
                    "label": 0
                },
                {
                    "sent": "Just incidentally, I see a strong connection between this new work and pulling samples.",
                    "label": 0
                },
                {
                    "sent": "And I worked on five years ago, by Professor Kleinberg, that is called stochastic discrimination.",
                    "label": 0
                },
                {
                    "sent": "It's a paper that was published in Palm Beach five years ago, and for non mathematicians that I need, a math is a little hard to follow.",
                    "label": 0
                },
                {
                    "sent": "But five years later, I think it's very, very cool to see the connection between these two very powerful techniques.",
                    "label": 0
                },
                {
                    "sent": "So the reference to the paper we're not going to say anything more about it.",
                    "label": 0
                },
                {
                    "sent": "Stochastic discrimination, but the reference is not painted.",
                    "label": 0
                },
                {
                    "sent": "It's also at the end.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so we're going to do a little bit of overview of predictive learning and then decision trees.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So predictive learning just to get our notation right, we have two independent variables or inputs.",
                    "label": 0
                },
                {
                    "sent": "T&P in this example, Anna response.",
                    "label": 0
                },
                {
                    "sent": "It's either good or bad.",
                    "label": 0
                },
                {
                    "sent": "Good is blue and bad is green.",
                    "label": 0
                },
                {
                    "sent": "You might, I don't know how it comes out in the grayscale on your notes.",
                    "label": 0
                },
                {
                    "sent": "Sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "We couldn't.",
                    "label": 0
                },
                {
                    "sent": "We didn't think ahead completely, so you may want to mark while looking at the green.",
                    "label": 0
                },
                {
                    "sent": "You know there's a definitely a boundary.",
                    "label": 0
                },
                {
                    "sent": "Then there's the one there in the middle.",
                    "label": 0
                },
                {
                    "sent": "There's a question, mark, what's that one?",
                    "label": 0
                },
                {
                    "sent": "Should that one be labeled?",
                    "label": 0
                },
                {
                    "sent": "And depending on what estimation technique you have, you have a different label for that.",
                    "label": 0
                },
                {
                    "sent": "Or you may even have an uncertainty value for your determination there.",
                    "label": 0
                },
                {
                    "sent": "So we can either be descriptive, trying to summarize the data in some way that's understandable and actionable, or predicted which is more likely.",
                    "label": 1
                },
                {
                    "sent": "What we focus on what is the response or the class of that new point.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Metal.",
                    "label": 0
                },
                {
                    "sent": "So ordinary linear regression will fit the data.",
                    "label": 1
                },
                {
                    "sent": "You maybe would design A1 to the blue and a zero to the green and find your least squares fit, and then that would be a plane.",
                    "label": 0
                },
                {
                    "sent": "The 3rd third dimension here would be the one or zero and you would have a plane depending on your threshold.",
                    "label": 0
                },
                {
                    "sent": "If so, it was .5.",
                    "label": 0
                },
                {
                    "sent": "That would provide a cut off point or decision boundary.",
                    "label": 0
                },
                {
                    "sent": "If the two classes had different misclassification costs, that boundary might move, but it would still be the plane would provide the fit.",
                    "label": 0
                },
                {
                    "sent": "So if it's a false alarm, false dismissal problem where false alarm was seven times more expensive than false, dismissal is 7 times more expensive than false alarm.",
                    "label": 0
                },
                {
                    "sent": "Say you might move that decision boundary to minimize cost, but it will come from that single fit from the plane and the model would be.",
                    "label": 0
                },
                {
                    "sent": "F Hat is a function of XA constant, an weighted linear terms.",
                    "label": 0
                },
                {
                    "sent": "And here I guess you would have to label them one and minus one to have a decision boundary of zero that's mentioned in the notes.",
                    "label": 0
                },
                {
                    "sent": "Now this straight line has it's has its value.",
                    "label": 0
                },
                {
                    "sent": "People can do wonderful things with regression if you use the right inputs.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But more flexibility would be very nice and decision trees provide almost the completely opposite way of looking at things where they first make a split and say I'm going to try to solve the whole problem with one single question, typically a single variable in a single threshold, and they try to see what split amongst all of the variables would best separate the two classes and so here AT at 5, where if it's greater than five.",
                    "label": 0
                },
                {
                    "sent": "You would call it blue and less than five you would, you would say unknown and.",
                    "label": 0
                },
                {
                    "sent": "So to read this, the yes goes to the left.",
                    "label": 0
                },
                {
                    "sent": "To read this here.",
                    "label": 0
                },
                {
                    "sent": "Mom.",
                    "label": 0
                },
                {
                    "sent": "And then you continue iteratively looking at those two sides, sort of independently.",
                    "label": 0
                },
                {
                    "sent": "So the data is a fresh look at a fresh look at all the variables in all the splits that are possible on the two sides of the tree, and you continue to carve the space out and classify it with different results.",
                    "label": 0
                },
                {
                    "sent": "So you get at the end something that's equivalent to an expert system rule that if T is in the range 25 and P is one of the sets in one and two and three, and it's bad.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, it's good would be a good summary of that decision.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So normally you'd be given training data cap in number of cases.",
                    "label": 1
                },
                {
                    "sent": "Your output variable, Y inputs variables X form a vector there measured values of the attributes that why being response or output X being the predictors and your data.",
                    "label": 1
                },
                {
                    "sent": "This is where non statisticians and statisticians disagree about how to look at things.",
                    "label": 1
                },
                {
                    "sent": "The sessions think of there being a true underlying distribution out there and that you've somehow drawn a random sample from that distribution.",
                    "label": 0
                },
                {
                    "sent": "And I think that's a useful way of looking at things.",
                    "label": 0
                },
                {
                    "sent": "Computer scientists as a rule, don't have as much of an until they've worked with data, don't have as much respect for noise or uncertainty in the data number.",
                    "label": 0
                },
                {
                    "sent": "A number of machine learning techniques.",
                    "label": 0
                },
                {
                    "sent": "I remember early on would push their process their models, until they could exactly answer all the training data.",
                    "label": 0
                },
                {
                    "sent": "But in statistics you know that that leads to overfit because you're fitting the noise as well as the signal.",
                    "label": 0
                },
                {
                    "sent": "So there seems to be a much more common understanding of not wanting the model to be too complex at some point and needing regularization.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The two, so the two methods that will try to minimize the risk where there's a loss function averaged over the the output variables and the fitted model.",
                    "label": 0
                },
                {
                    "sent": "Ordinary at least ordinary linear regression uses the least squared error loss function.",
                    "label": 0
                },
                {
                    "sent": "The minimum can be found instantly with olr with regression, so that's one reason is so popular is the math is so good.",
                    "label": 0
                },
                {
                    "sent": "There's kind of a saying that well.",
                    "label": 0
                },
                {
                    "sent": "Drunk is looking for his keys outside under the streetlamp and they ask him why says?",
                    "label": 0
                },
                {
                    "sent": "You probably lost him in the bar, didn't she said, yeah, but the lights better out here so sometimes that's what happens with using regression.",
                    "label": 0
                },
                {
                    "sent": "The math is so beautiful that it's used in places when it's not appropriate.",
                    "label": 0
                },
                {
                    "sent": "The trees are neural Nets and clustering and so forth.",
                    "label": 0
                },
                {
                    "sent": "Use heuristic algorithms that are iterative in nature.",
                    "label": 0
                },
                {
                    "sent": "So they are much slower compared to.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Aggression.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at trees a little bit.",
                    "label": 0
                },
                {
                    "sent": "Here's an example surface built by Decision Tree.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as the constants or these.",
                    "label": 0
                },
                {
                    "sent": "Estimated constants are the means in the regions they see hats, and this is indicator function if check if your input is in this regions of M then it's a one else it's a zero and then multiply it by that constant.",
                    "label": 0
                },
                {
                    "sent": "And that remodel can be thought of as the sum of all these regions.",
                    "label": 0
                },
                {
                    "sent": "So these regions are defined by the different levels and their hyper rectangles.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the trees allow for different scoring criteria.",
                    "label": 0
                },
                {
                    "sent": "Is the two most the two most used R-squared error where the the ideal constant is the mean and the absolute error where the ideal constant is the median of the population?",
                    "label": 0
                },
                {
                    "sent": "So one of these two loss functions then is averaged over the data and that is your risk.",
                    "label": 0
                },
                {
                    "sent": "So you want to find the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tree with the lowest prediction risk.",
                    "label": 0
                },
                {
                    "sent": "So you're searching over the space of these constants an regions to minimize squared error, for instance.",
                    "label": 0
                },
                {
                    "sent": "This is your tree model.",
                    "label": 0
                },
                {
                    "sent": "And to do this in an unrestricted way is extraordinarily difficult.",
                    "label": 0
                },
                {
                    "sent": "So one universal technique is to restrict the regions to be disjoint to cover the space and to be simple in some way.",
                    "label": 0
                },
                {
                    "sent": "So here's one way you could define a region in an ordered way, and so another region behind it would show up.",
                    "label": 0
                },
                {
                    "sent": "But the typical way is to.",
                    "label": 0
                },
                {
                    "sent": "Is too.",
                    "label": 0
                },
                {
                    "sent": "Provide splits that are independent and conditioned on the previous.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regional distribution.",
                    "label": 0
                },
                {
                    "sent": "So just rest with stepwise linear regression, which is a greedy iterative procedure for deciding which inputs to use in a regression format.",
                    "label": 1
                },
                {
                    "sent": "That's the typical way that decision trees are done.",
                    "label": 1
                },
                {
                    "sent": "So you start with all the data and check each attribute in each possible split value and define a score function that tells how the quality of the split, how much you reduce the variance if you're using squared error, or how much you do see the least absolute deviation.",
                    "label": 0
                },
                {
                    "sent": "If you're using absolute air.",
                    "label": 0
                },
                {
                    "sent": "So you choose the input and the split that improve the fit the most.",
                    "label": 0
                },
                {
                    "sent": "Again, greedily the single value that improves the most, with no regard to what's going to happen subsequently.",
                    "label": 0
                },
                {
                    "sent": "And then you replace your original region with these two new split regions and do it recursively again.",
                    "label": 0
                },
                {
                    "sent": "So the big question is when should we stop and if you go to tree talks and so forth.",
                    "label": 1
                },
                {
                    "sent": "People will argue about whether we should use a chi squared to stop or if we should overfit and then use car.",
                    "label": 0
                },
                {
                    "sent": "The cart technique, for instance, overfits intentionally and then decides how far back to prune and to reabsorb cuts that have been done so forth.",
                    "label": 0
                },
                {
                    "sent": "So this simple algorithm can probably be coded up by when I coded up through my first tree algorithm is just one or two pages of C code is all it takes.",
                    "label": 0
                },
                {
                    "sent": "But of course to handle real data.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Categorical data and missing values and so forth would take hundreds and sometimes thousands of lines of code.",
                    "label": 0
                },
                {
                    "sent": "So some of their great strengths they can deal with irrelevant inputs.",
                    "label": 1
                },
                {
                    "sent": "Input either shows up in the tree or doesn't, so it naturally does a variable selection.",
                    "label": 0
                },
                {
                    "sent": "And you can you can anything you can measure.",
                    "label": 1
                },
                {
                    "sent": "You can use.",
                    "label": 0
                },
                {
                    "sent": "You can use categorical variables, ordinal variables, numeric and so forth and you get a an important score.",
                    "label": 0
                },
                {
                    "sent": "The better tree algorithms will tell you how useful the variables were.",
                    "label": 0
                },
                {
                    "sent": "Monotonic transformations won't affect him, so you don't have any don't have problems with outliers.",
                    "label": 0
                },
                {
                    "sent": "If a value is 3.14 or 3100, it's greater than three, and so the questions being asked are insensitive to outliers in your input variables, not your outputs.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now they're very fast compared to other iterative techniques.",
                    "label": 0
                },
                {
                    "sent": "Missing values can be handled a couple of different ways.",
                    "label": 0
                },
                {
                    "sent": "Either surrogate splits is how cart desert, where it comes up with substitute splits in case the first variables unknown.",
                    "label": 0
                },
                {
                    "sent": "What variables will best replicate the splitting of the data that you might know?",
                    "label": 0
                },
                {
                    "sent": "So it is sort of a stack of questions that you ask at any state.",
                    "label": 0
                },
                {
                    "sent": "Very clever way of handling missing data, other techniques.",
                    "label": 0
                },
                {
                    "sent": "Other algorithms create a new class called Missing.",
                    "label": 0
                },
                {
                    "sent": "That may be, it may not be missing Amanda Random.",
                    "label": 0
                },
                {
                    "sent": "There may be information and it's missing this and so in that case that might be a good way to split.",
                    "label": 0
                },
                {
                    "sent": "Off the shelf there few trainable parameters you could you can kind of use them within minutes of learning about him and if you have a single tree it's very interpretable, at least a few levels.",
                    "label": 0
                },
                {
                    "sent": "You can say the say it out as a question which is very useful when you're doing consulting to be able to interpret to your client where their problems are coming from or where the best clients are.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now they have severe limitations if you're trying to fit a trend.",
                    "label": 0
                },
                {
                    "sent": "Piecewise constants are very poor way to do that, and so you also need a lot of data.",
                    "label": 1
                },
                {
                    "sent": "It exponentially eats through your data by splitting, splitting the data up, and so in high dimensions, especially, your data is already very sparse.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if there's low interaction in your underlying data, in other words, just more of a trend than an A&B.",
                    "label": 1
                },
                {
                    "sent": "Kind of a thing then.",
                    "label": 0
                },
                {
                    "sent": "I have a very hard time feeding a plane where regression is very good, so you can already see an idea here where regression is terrable trees are good.",
                    "label": 0
                },
                {
                    "sent": "Maybe we should combine them in some way.",
                    "label": 0
                },
                {
                    "sent": "So if there's a dependence on many variables that also.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have trouble.",
                    "label": 0
                },
                {
                    "sent": "Also, because of the greedy search strategy, there's a high variance.",
                    "label": 1
                },
                {
                    "sent": "You can change one datapoint in your data set and get a completely different tree.",
                    "label": 0
                },
                {
                    "sent": "I've seen that happen and so on that.",
                    "label": 0
                },
                {
                    "sent": "Look like a completely different tree.",
                    "label": 0
                },
                {
                    "sent": "It may have very very similar performance, but typically in your data.",
                    "label": 0
                },
                {
                    "sent": "In real data, your variables are very correlated anyway, so the fact that it split on age instead of height, height might have been just barely second best, and therefore when you change one data point, it changes in that.",
                    "label": 0
                },
                {
                    "sent": "That sort of the butterfly effect.",
                    "label": 0
                },
                {
                    "sent": "The thing that happens here propagates, and the whole tree looks different.",
                    "label": 0
                },
                {
                    "sent": "Now, the end end estimate values might not be as different.",
                    "label": 0
                },
                {
                    "sent": "As the apparent difference by looking at the variables so.",
                    "label": 0
                },
                {
                    "sent": "So early splits especially propagate their changes and.",
                    "label": 0
                },
                {
                    "sent": "It's important to prune, so we should definitely use other methods whenever we can, but also ensembles can save the day with trees and maintain the advantages except for perhaps interpretability, while dramatically increasing.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Accuracy.",
                    "label": 0
                },
                {
                    "sent": "Alright, by the way, if there are any questions please feel free to ask him as we go and.",
                    "label": 0
                },
                {
                    "sent": "It will also be time at the end.",
                    "label": 0
                },
                {
                    "sent": "Wait, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "This is the one I gave you the wrong one.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to hide a little bit behind the podium so that I can look at my notes if I need to.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So next, we're going to talk about model complexity, model selection, and regularization.",
                    "label": 0
                },
                {
                    "sent": "Will try to get a intuition as to what bias invariances and this is important, because simple methods succeed by reducing bias, reducing variance, or finding a good tradeoff between the two.",
                    "label": 0
                },
                {
                    "sent": "We'll revisit the equation of where do we stop growing the tree?",
                    "label": 0
                },
                {
                    "sent": "Which which is particularly the technique using car that is called cost complexity pruning.",
                    "label": 0
                },
                {
                    "sent": "And the reason we're going to do that is because it allows us to get an intuition of complexity based regularization.",
                    "label": 0
                },
                {
                    "sent": "So before we jump into the law sueann regularization in assembles just looking at what's going on in the pruning.",
                    "label": 0
                },
                {
                    "sent": "I think it really gets you the intuition of complexity ways regularization.",
                    "label": 0
                },
                {
                    "sent": "Then we'll review cross validation.",
                    "label": 0
                },
                {
                    "sent": "I'm sure most of you are familiar with it, but for those that aren't.",
                    "label": 0
                },
                {
                    "sent": "Cost, complexity based regularization give rise to one or more meta parameters.",
                    "label": 0
                },
                {
                    "sent": "So in addition to your original model parameters, there's going to be a couple of meta parameters, and those are generally estimated with cross validation, so it's important that.",
                    "label": 0
                },
                {
                    "sent": "We see that I also cross validation is a very good example of getting a useful thing out of combining and so that's the reason that I wanted to make cross validation and then we will talk a little bit about the lawsuit.",
                    "label": 0
                },
                {
                    "sent": "Which will be used as a post processing in the Indian symbol method.",
                    "label": 0
                },
                {
                    "sent": "So later on Will will when?",
                    "label": 0
                },
                {
                    "sent": "Will have this top level view of assembled, so instead of a bottom up view we're going to have this generic and simple method an the modern day recipe of ensembling calls for a post processing stage using regularization and so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, going back to the question of how big to grow the tree, what is what is the right size of the tree and what is the dilemma that we are facing?",
                    "label": 1
                },
                {
                    "sent": "Well, essentially if the number of regions.",
                    "label": 0
                },
                {
                    "sent": "Or the number of nodes.",
                    "label": 1
                },
                {
                    "sent": "Terminal nodes in the tree is too small.",
                    "label": 1
                },
                {
                    "sent": "Then your constant approximation is too crude.",
                    "label": 0
                },
                {
                    "sent": "Ann, with this very simple example.",
                    "label": 0
                },
                {
                    "sent": "So suppose the target function.",
                    "label": 0
                },
                {
                    "sent": "This is our data.",
                    "label": 0
                },
                {
                    "sent": "This is the truth, and we already know from the previous section that this is the worst kind of target functions for trees, but nonetheless, if we only have 1 split, namely, we have two terminal nodes, two constants and then the approximation is very crude, right?",
                    "label": 1
                },
                {
                    "sent": "That?",
                    "label": 0
                },
                {
                    "sent": "Intuitively it's what it's called bias, right?",
                    "label": 0
                },
                {
                    "sent": "And it creates error if on the other end the tree is too large.",
                    "label": 0
                },
                {
                    "sent": "We have two many terminal nodes, then the three you can grow a tree.",
                    "label": 0
                },
                {
                    "sent": "All the way to having one terminal node for every single data point in your data and so so the three will have 0 error on your training data, and so that clearly it's overfitting the data.",
                    "label": 0
                },
                {
                    "sent": "And if we get to get a new data set from the same phenomena like currently I work in manufacturing.",
                    "label": 0
                },
                {
                    "sent": "So let's say you got some data this week and next from this manufacturing process next week you get another batch of data is the same process you expect the other distribution to be very similar.",
                    "label": 0
                },
                {
                    "sent": "The three that you been last week would have won't have an had a 0 error last week.",
                    "label": 0
                },
                {
                    "sent": "Won't have a cigarette or this week because the three simply overfit the data, and that's intuitively what we call variants and will will.",
                    "label": 0
                },
                {
                    "sent": "Further develop that notion.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "More formally.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's let's assume we are in this scenario where?",
                    "label": 0
                },
                {
                    "sent": "It's a very common assumption.",
                    "label": 0
                },
                {
                    "sent": "A scenario for those of you that don't like the math notation at all.",
                    "label": 0
                },
                {
                    "sent": "I'll try simply to walk through the through the intuitions of things.",
                    "label": 0
                },
                {
                    "sent": "So all what this is saying, F is our target function that we're trying to model, but we don't know it.",
                    "label": 0
                },
                {
                    "sent": "But because either we are not measuring everything that is relevant or we have problems with our measurement equipment.",
                    "label": 0
                },
                {
                    "sent": "What we measure is this.",
                    "label": 0
                },
                {
                    "sent": "So the response variable is the truth plus some error.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Now consider an idealized aggregate estimator, so.",
                    "label": 0
                },
                {
                    "sent": "Let's say that going back to my manufacturer example where you can take data week after week after week, so they F hat is going to be the model that you build on a particular data set.",
                    "label": 0
                },
                {
                    "sent": "And let's assume that we can take many, many many data sets and create this F bar, which is an average of all those.",
                    "label": 0
                },
                {
                    "sent": "Here, the expectation operator you can think of it as an average in operator.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at the question of what is the error of one of these F had on one particular data point under one particular loss function that the square rollos loss that allows easy manipulation an you don't have to follow all of these.",
                    "label": 0
                },
                {
                    "sent": "It follows from Trivial algebra an properties of the expectation operator.",
                    "label": 0
                },
                {
                    "sent": "But so let's simply stuck here and then look at the end so the error at that particular point.",
                    "label": 0
                },
                {
                    "sent": "Using these queries we know is this difference right?",
                    "label": 0
                },
                {
                    "sent": "They expect a response minus the response of the F hat, and again through algebra properties of the expectation operator we get this expression here and the first term is what we call the square bias and the second term is what we call variants and the last term is what it's called data usable error.",
                    "label": 0
                },
                {
                    "sent": "The error that was there since the beginning and there's nothing we can do about it unless we go and measure more stuff or.",
                    "label": 0
                },
                {
                    "sent": "Improve our measurement equipment so.",
                    "label": 0
                },
                {
                    "sent": "But what is this telling us the first term?",
                    "label": 0
                },
                {
                    "sent": "So the first one is simply telling us So what is the squared bias?",
                    "label": 0
                },
                {
                    "sent": "Is the amount by which our average estimator differs from the truth?",
                    "label": 0
                },
                {
                    "sent": "Right, the F is a truth.",
                    "label": 0
                },
                {
                    "sent": "An F hat is the average estimator.",
                    "label": 0
                },
                {
                    "sent": "And then the second term is just the spread of RF had around there meant and so.",
                    "label": 0
                },
                {
                    "sent": "And in the next, like there's, I think there's a graph that helps further get intuition for these concepts now.",
                    "label": 0
                },
                {
                    "sent": "So what we have is the error is made of these two.",
                    "label": 0
                },
                {
                    "sent": "Essentially of these two components.",
                    "label": 0
                },
                {
                    "sent": "And so these two components are opposing forces.",
                    "label": 0
                },
                {
                    "sent": "If one goes up the other one goes down.",
                    "label": 0
                },
                {
                    "sent": "If the other one goes up, the other one goes down, and that's generally the situation we are in.",
                    "label": 0
                },
                {
                    "sent": "So what happens is the more flexible your model is, the lower the values that is have, but the more body, and it is subject to it.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's try to put graphically all that so.",
                    "label": 0
                },
                {
                    "sent": "Think of.",
                    "label": 0
                },
                {
                    "sent": "The truth, so we have the truth that we're trying to approximate, but we don't get a realization of the truth.",
                    "label": 0
                },
                {
                    "sent": "What we get is something else that has error into it, right?",
                    "label": 0
                },
                {
                    "sent": "So this is our Y.",
                    "label": 0
                },
                {
                    "sent": "And this blue shaded area is indicative of the the.",
                    "label": 0
                },
                {
                    "sent": "Sigma out there right?",
                    "label": 0
                },
                {
                    "sent": "So next week I tried to get another realization of the truth and I might get it here and next week here and get it here.",
                    "label": 0
                },
                {
                    "sent": "So that's what the circle means.",
                    "label": 0
                },
                {
                    "sent": "Now we we have a model space, so we.",
                    "label": 1
                },
                {
                    "sent": "And in one of the predictive learning overview slides at John presented.",
                    "label": 0
                },
                {
                    "sent": "He said there are three elements essentially in a predictive learning situation.",
                    "label": 0
                },
                {
                    "sent": "The moral whether it's a linear or polynomial or whatever an an.",
                    "label": 0
                },
                {
                    "sent": "There is core function.",
                    "label": 0
                },
                {
                    "sent": "How do we measure the errors and the search algorithm?",
                    "label": 0
                },
                {
                    "sent": "How do we go around looking for the parameters of that particular model so?",
                    "label": 0
                },
                {
                    "sent": "But the first part is the model and there's a model family, right?",
                    "label": 0
                },
                {
                    "sent": "And the truth may or may not be part of the family, right?",
                    "label": 0
                },
                {
                    "sent": "I. I'm not sure I was expecting this point to come first, but whatever, so we get this realization and we fit 1F had an.",
                    "label": 0
                },
                {
                    "sent": "We're only taking models from the model space, so let's say that that's the closest to that I've had.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But again, if next week I get another realization here, I will fit another point here.",
                    "label": 0
                },
                {
                    "sent": "And so on and so that's what this circle is represented.",
                    "label": 0
                },
                {
                    "sent": "And then this point here is the average F bar.",
                    "label": 0
                },
                {
                    "sent": "So remember, this is F had fit to a particular realization, an F bar is the average accuracy then.",
                    "label": 0
                },
                {
                    "sent": "So this is model bias.",
                    "label": 1
                },
                {
                    "sent": "Again, the amount by which the average estimator differs from the truth.",
                    "label": 0
                },
                {
                    "sent": "And this circle is its variance, which is the spread of the F hats around their main.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "The next.",
                    "label": 0
                },
                {
                    "sent": "The next idea about bias and variance is the so called.",
                    "label": 0
                },
                {
                    "sent": "Tradeoff that I was hinting at, so let's assume that we have.",
                    "label": 0
                },
                {
                    "sent": "That we have a way of measuring model complexity in the case of trees, model complexities, just the size of the tree.",
                    "label": 1
                },
                {
                    "sent": "And so, so let's assume let's look at this horizontal axis as as a complexity access an so on the far left we have just a stump, namely a root node an.",
                    "label": 0
                },
                {
                    "sent": "On the other extreme, we have a tree that has been grown all the way to having one terminal node pair up survey shun in the data.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So two things here.",
                    "label": 0
                },
                {
                    "sent": "The first one is as I mentioned.",
                    "label": 0
                },
                {
                    "sent": "I can grow a tree all the way to having one terminal node per data, so the error the training error is going to be 0.",
                    "label": 0
                },
                {
                    "sent": "And so that's why we say that training error is not a useful measurement of model quality.",
                    "label": 1
                },
                {
                    "sent": "We need to look at a different kind of data, and that's what it stated here.",
                    "label": 1
                },
                {
                    "sent": "If we measure the performance of each of those trees on separate data, the typical behavior is going to look like this right?",
                    "label": 0
                },
                {
                    "sent": "And so somewhere there is M star, which is the most appropriate size of the tree.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How is this done in trees?",
                    "label": 1
                },
                {
                    "sent": "So what trees do to find the optimal size of the tree to find the optimal complexity level of the tree is at the score criterion, so also join introduced before these are notation for risk.",
                    "label": 0
                },
                {
                    "sent": "So statisticians call it like to call error we call error, they color risk and so I'm following their notation.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "Chris estimate whether it's base if.",
                    "label": 0
                },
                {
                    "sent": "On square or laws of absolute deviation, whatever, we have a way of measuring the risk of the tree, the error of the tree.",
                    "label": 0
                },
                {
                    "sent": "But we had this term.",
                    "label": 0
                },
                {
                    "sent": "This Alpha absolute value T. It's not really absolute value 3 is this.",
                    "label": 0
                },
                {
                    "sent": "That notation stands for the size of the tree.",
                    "label": 1
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Which is which is analogous to the number of degrees of freedom.",
                    "label": 1
                },
                {
                    "sent": "So essentially we're going to have these two forces in the score.",
                    "label": 1
                },
                {
                    "sent": "At the model can get more complex, but it better reduce their by a substantial amount to compensate for the increased penalty in complexity, and that's what complexity ways regularization is allowed is maintained.",
                    "label": 0
                },
                {
                    "sent": "Your error function with something that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Alpha, it's going to become a meta parameter of the procedure, and you can think of it as OK. Is the cost that you pay for every additional terminal node.",
                    "label": 0
                },
                {
                    "sent": "Write an Inter parameters are needs to be determined.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 1
                },
                {
                    "sent": "And KII said that already there the other thing to realize is that deep analyzation term is not data dependent, is deterministic, and that's what it's doing.",
                    "label": 0
                },
                {
                    "sent": "This this counterbalancing to the data dependent part of the error function.",
                    "label": 0
                },
                {
                    "sent": "Answer Output our goal.",
                    "label": 0
                },
                {
                    "sent": "It's rephrase instead of finding the tree that has a minimum risk.",
                    "label": 0
                },
                {
                    "sent": "Now we have we are looking for the tree that has the minimum regularize risk.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm answering card if you go to the card book, the one published by the Four Titans, there will be a section of called Cost Complexity where where all this is described.",
                    "label": 0
                },
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "Alpha as as we said, Alpha is a meta parameter of the procedure that is going to control the degree of stabilization of the regularize component of the error.",
                    "label": 1
                },
                {
                    "sent": "And so at one extreme, if Alpha is 0, there's no regularization.",
                    "label": 0
                },
                {
                    "sent": "We get back our original error, right?",
                    "label": 1
                },
                {
                    "sent": "And so you get the least stable estimate.",
                    "label": 0
                },
                {
                    "sent": "At the other extreme, Alpha is Infinity an it's completely deterministic no matter what the data is telling you the.",
                    "label": 0
                },
                {
                    "sent": "Complexity penalty is so high that it always.",
                    "label": 0
                },
                {
                    "sent": "Wind on and you get only a stump.",
                    "label": 0
                },
                {
                    "sent": "Notary can be built an so in between.",
                    "label": 0
                },
                {
                    "sent": "There's this continuum from Alpha equals 0 where you get the unregularized tree to Alpha much, much greater than 0 where you get a fully regularised solution.",
                    "label": 0
                },
                {
                    "sent": "An something that happens very nice in trees that makes the.",
                    "label": 0
                },
                {
                    "sent": "The the process work computationally very efficiently is that these trees are all nested.",
                    "label": 0
                },
                {
                    "sent": "This can be set up in such a way that.",
                    "label": 0
                },
                {
                    "sent": "Every tree is a subset of the previous tree.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And of course, they as we were looking for the optimal value of Alpha in this continuum, and choosing the optimal value of Alpha, is equivalent to choosing the optimal tree, right?",
                    "label": 0
                },
                {
                    "sent": "Because once we find the optimal values Alpha, there is a three associated with that particular Alpha.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think maybe a couple of more slides and then we go on the break.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So as we mentioned in the plot, where we have risk or error versus training set and test set, we mentioned that the error on the training set is not a useful estimator and we need a way to estimate what we call prediction trees or future risk or also test risk, right?",
                    "label": 1
                },
                {
                    "sent": "So if you don't have that data available.",
                    "label": 0
                },
                {
                    "sent": "The approach that that is uses to use cross validation and I was saying that cross validation is also an example of where we combine things in a useful way, so we are combining in cross validation is measures of fit.",
                    "label": 1
                },
                {
                    "sent": "Right measures of prediction error.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Very quickly suppose suppose that we have.",
                    "label": 0
                },
                {
                    "sent": "If we were to do what is called 3 fold cross validation, we simply split the data into three groups.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do is we take, say, the 1st two part, the 1st two groups, and we build the first 3 on that data, and then we're going to estimate the risk on this one.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "The third time, we're going to build the Third 3 build.",
                    "label": 0
                },
                {
                    "sent": "Sorry on the last two partitions and we're going to evaluate it.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That one.",
                    "label": 0
                },
                {
                    "sent": "So copying, copying that picture over.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "So we have those three trees everyone built on a fraction of the data, and we have this what it's called the indexing function like.",
                    "label": 1
                },
                {
                    "sent": "If we look more carefully, every data point.",
                    "label": 0
                },
                {
                    "sent": "So remember Big N is the number of observation.",
                    "label": 0
                },
                {
                    "sent": "Every data point is a test point in one of the partitions, so that's what this function is telling us.",
                    "label": 0
                },
                {
                    "sent": "The partitioning which.",
                    "label": 0
                },
                {
                    "sent": "Observation I is.",
                    "label": 1
                },
                {
                    "sent": "And so our cross validated estimate of risk, which we need to estimate Alpha and estimate is just computer like this.",
                    "label": 0
                },
                {
                    "sent": "And so again.",
                    "label": 1
                },
                {
                    "sent": "What we've done is we've taken an average of measures of fit over the previous split.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I think let's let's let's stop there and recommend so you guys can stretch coffee bathrooms, yeah?",
                    "label": 0
                },
                {
                    "sent": "It's like.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another way to calculate the average statistic is to.",
                    "label": 0
                },
                {
                    "sent": "Calculated for each of the test sets and then average.",
                    "label": 0
                },
                {
                    "sent": "Here you're just building 1 set.",
                    "label": 0
                },
                {
                    "sent": "By getting the predictions for the points by the model that did not see them in great right?",
                    "label": 0
                },
                {
                    "sent": "Yes, just getting once at calculate error or whatever.",
                    "label": 0
                },
                {
                    "sent": "Another possibility is to actually build the model on one, then calculate the error on that line and do this three times and then average these people who I believe that's what we're doing here but but, but let's take that offline yeah, so function.",
                    "label": 0
                },
                {
                    "sent": "Let's one thing to do is to take that.",
                    "label": 0
                },
                {
                    "sent": "Change the cross validation boundaries and do it again, so have multiple layers.",
                    "label": 0
                },
                {
                    "sent": "Anyhow.",
                    "label": 0
                },
                {
                    "sent": "I I still haven't gotten it.",
                    "label": 0
                },
                {
                    "sent": "So, but if you have time now, let's.",
                    "label": 0
                },
                {
                    "sent": "Right, so I was just thinking.",
                    "label": 0
                }
            ]
        }
    }
}