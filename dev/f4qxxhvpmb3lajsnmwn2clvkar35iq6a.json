{
    "id": "f4qxxhvpmb3lajsnmwn2clvkar35iq6a",
    "title": "Going forward with Probablistic Local Learning Approaches",
    "info": {
        "author": [
            "Jo-Anne Ting, Department of Computer Science, University of British Columbia"
        ],
        "published": "Aug. 3, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/rraa09_ting_gfpl/",
    "segmentation": [
        [
            "Ranting, I'm from the University of Edinburgh and initially we're going to have a panel discussion at this time, but we thought it best to shift it to the end of the workshop after everyone has talked.",
            "So we have actually more time to get into open discussion.",
            "So today I'm going to talk to you about probabilistic local approaches and ways of bringing these method."
        ],
        [
            "It's forward and the talk will be primarily.",
            "Little bit of an introduction or a survey of current methods out there, and then I'll end with an analysis of what's been done recently and what are the open questions and open problems that would be nice to look at.",
            "So to motivate the use of local methods in robotics.",
            "One can see examples from this morning that people have talked about.",
            "Typically these are situations where global methods can't necessarily always be used when either your memory or computation time is limited.",
            "So an example of this is when you want to do real time learning with adaptation as Ryan was talking about this morning.",
            "So here I have some examples of some of the systems out there that have been have been used and this includes the Aibo dog.",
            "Soccer playing dog.",
            "The logger platform for on line mapping and anthropomorphic arm for for real time drumming.",
            "So too."
        ],
        [
            "Really quickly give you a big overview.",
            "What do I mean by local versus global methods?",
            "If we take a look at the problem of nonlinear supervised learning, we can probably try to discretize it in roughly three categories on the left side.",
            "Here we have global methods such as Gaussian processes, efms neural Nets.",
            "These methods try to optimize a global cost function over the entire data set.",
            "On the far right we have local methods such as locally made it learning and different variations of local we are learning.",
            "Where they optimize the local cost function and use a subset of the training data.",
            "That is to say, a local distance metric.",
            "Then we have this sort of fuzzy air in between where it's semi local as I've turned it and this includes methods such as mixtures of experts.",
            "So for example, mixtures of GP experts or infinite mixtures of GP GP experts that Dan was talking about during the poster session.",
            "These methods fall in this sort of intermediate."
        ],
        [
            "Us so Christian at the very beginning in the morning, made a list of some of the challenges that we see in functional approximation.",
            "And here I tried to sort of visually at least put down all the challenges out there with regards to learning in robotics.",
            "So in blue are pieces of these puzzles related to modeling change challenges in green?",
            "Are these more algorithmic characteristics that we often like to have?",
            "Now the requirements for learning robotics are typically quite different than, say, learning in a traditional machine learning setting, where your data set might not be over so large.",
            "And batch analysis might be fine.",
            "So in robotics, real-time function approximation is quite crucial.",
            "So the things that challenges that we often face include sort of data where the dimensions are high.",
            "We have lots of datasets, lots of samples in our data set, we have input dimensions that are typically well, probably redundant and irrelevant.",
            "We have outliers, noise.",
            "We need to come up with algorithms that are quite quite fast in terms of performance time, but also algorithms that can adapt.",
            "In an online setting, so continual learning as some people have called it where.",
            "Depending on the data that's coming in, you might actually want to update the model and change the parameters.",
            "So to motivate this, typically these sort of data clouds or or date datasets.",
            "Are very hard to capture using traditional parametric family of functions simply because depending where you are in your data space, the characteristics of the underlying function could vary.",
            "So for this purpose allow."
        ],
        [
            "We start by doing a quick overview of nonparametric regression.",
            "I'm sure you guys know all about this since most of the speakers this morning were talking about these nonparametric techniques.",
            "And then I'll quickly touch again.",
            "Apone difference between local and global methods to motivate why we are interested in local methods.",
            "And then the latter part of the talk will be sort of focusing more on the current probabilistic local methods that have been recently developed."
        ],
        [
            "Sir, nonparametric aggression in a quickly skim over this is by people this morning.",
            "Ryan and Kristen talked about this as well as deter you want to approximate a function so you have some inputs X corresponding outputs.",
            "Why that you've observed an you want to estimate this underlying underlying function, but estimating estimating it not using some parametric family of functions.",
            "But instead using a data driven method to find the shape of this curve.",
            "So typically we can breakdown some of the category of methods into kernel regression methods, local polling them, polynomial regression methods and smoothing."
        ],
        [
            "Lines.",
            "So and, regression as rare was talking about this morning.",
            "You basically sort of take an average.",
            "Over the the samples in your data set.",
            "So you see here each data point in your training set has some sort of weighting function and depending on where your query point is, you would take me the average of the samples.",
            "And I've listed here a estimate for the weight for a particular point.",
            "So the Nadaraya Watson estimate is one of the popular estimators used Gaussian process regression.",
            "For example, an example is common method or common kernel regression method.",
            "And the second class of methods include."
        ],
        [
            "With polynomial fitting, so this is a little bit different than kernel regression in the sense that we're actually using weighted least squares to fit some degree polynomial, and so here we use.",
            "The notion of.",
            "Unlike kernel regression, it fits using local leaders, voter wait locally weighted regression, not by locally weighted averaging.",
            "One thing to note is as in in the previous case for kernel regression, you do have a sort of smoothing parameter H. If you remember from the previous slide to determine while to set when calculating your weight matrix.",
            "Here the WS."
        ],
        [
            "And splines are a sort of 3rd category of methods where you can actually find a function to minimize a explicit sort of pressure and incense.",
            "So here we have this penalize predicted residual sum of squares.",
            "And the eight shares this smoothing parameter that you saw before for local polynomials and also for kernel regression.",
            "And this is a smoothing parameter needs to be specified.",
            "Now for splines, this H is the roughness penalty.",
            "That sort of controls the amount of smoothing.",
            "So if HO are basically not smoothing at all and interpolating data if you set H Infinity, you're infinitely smoothing, so that similar to using a linear least squares estimate.",
            "So what this all boils down to is essentially."
        ],
        [
            "By bias variance tradeoff.",
            "Or, you know, terming the distance metric of GP or the length scale of the kernel, or an RBF.",
            "All these methods require that this parameter smoothing parameter H you specified in set.",
            "And there are many approaches that have been suggested in the statistics and machine learning literature, and these vary from minimizing some mean squared error prediction prediction error, either using cross validation or maximum likelihood estimate the parameter, the hyperparameter that would minimize this criterion.",
            "Alternatively, you could try to minimize the mean squared prediction error by optimizing volume of your local model.",
            "Or one other method proposed by Grace Wahba back in the 70s was to fix the bandwidth parameter, the smoothing parameter instead vary the degree of the polynomial when fitting local polynomials.",
            "So."
        ],
        [
            "This is some of the related work out there, just to quickly show you that there's nothing that has been done in the past and a lot of people who've worked on, say splines and kernel regression have sort of working the other two.",
            "Categories as well."
        ],
        [
            "So to quickly return back to the global versus local argument or our sort of view of methods."
        ],
        [
            "We have to get categories here.",
            "One where we can see include methods such as SVM and GPS makes use of GPS an on the left side on the right side.",
            "Sorry more.",
            "Locally weighted based methods.",
            "To chew.",
            "Just give a quick overview on.",
            "On well, we can see for global methods, use entire datasets.",
            "Local methods use a subset of the data set.",
            "Global methods tend to be computational pricey, as we know for GP, the version of AN by N currents matrix sucks up a lot of time.",
            "Local methods tend to be a little bit faster and more suitable for real time methods.",
            "There's a scalability issue with global methods, since the entire data set training data set is sort of needed and well, it's used in the global method.",
            "And when it comes to an adaptation when a new data point comes in such that you need to update your model in a global method, uses the entire data set, you have to recompute all over again.",
            "Using all the data, the new parameters corresponding to your model."
        ],
        [
            "So current local methods typically set the smoothing parameter, this bandwidth each of distance metric or hyperparameter.",
            "Primer using heuristic or ad hoc methods.",
            "And in the next.",
            "10 minutes to sort of the left next.",
            "Part of this talk I'm going to quickly give you an overview of the methods that have been proposed in the past year or two and go through different well do Internet quick analysis of what what's good about them and what still needs to be looked at.",
            "So we recently developed a basean treatment of local polynomial regression.",
            "Are local linear regression."
        ],
        [
            "Really, where we try to treat the smoothing parameter, each probabilistically, in order to learn it's optimal values.",
            "So instead of using heuristic techniques like, say, Oliver PR, does we try to give it a little bit more of a probabilistic treatment?",
            "So this involves treating the weights as well, putting in Bernoulli distribution over the weights, and this in effect kind of models for each point.",
            "A while we give each sample in the training data a membership sort of indicator.",
            "So because it's a Bernoulli distribution, the posterior value of the weights were between zero and one.",
            "So now you see the Bernoulli distribution has a parameter kernel where the H. Parameter the smoothing parameter.",
            "We then put a gamma distribution over it and infer the posterior.",
            "So this is in contrast to say, for example, our PR which avoids this problem entirely by imposing in hierarchy hierarchy on.",
            "Actually, sorry which.",
            "Basically, doesn't.",
            "Model H probabilistic, but instead uses local local version of partially squares to find the latent projections on the data space and.",
            "So here is a very convoluted model."
        ],
        [
            "Seems of of the.",
            "Of this probabilistic local regression method, what's interesting or I guess what's important to notice?",
            "H notes here.",
            "Nodes below are the smoothing parameters and they have their connects to the weights nodes in blue.",
            "So we can actually formulate this entire graph model as an Ant problem and find the parameters to maximize the likelihood of the data.",
            "Anne.",
            "One thing about this this sort of process through when we were trying to develop a probabilistic treatment of local local linear regression.",
            "One thing we found is that.",
            "We're able to learn the smoothing parameter quite well, but when we then move into spaces where the input dimensions are irrelevant and redundant, so this is sort of in a typical scenario for full time learning.",
            "Some amount of global information is needed in order to properly learn the bandwidth or the smoothing parameter.",
            "At the same time as your sparsity parameter.",
            "So this would be like the precision on your regression variable that specifies which input dimensions are relevant.",
            "So for those who are you who are aware?",
            "That would be sort of the ardhi prior put on to the regression variable, and then the precision variable would then be learned.",
            "So this is one thing which we hypothesize.",
            "In algebra PR, what's done is the.",
            "Relevant, I guess.",
            "Relevant dimensions are in Ferd.",
            "Given a particular distance metric, so this hierarchy is already sort of imposed versus in a probabilistic model, if we were to model both, the deeper the precision variable and the smoothing parameter both both probabilistically, it would be sort of a chicken egg problem in the sense that one parameter, one variable, will be dependent on the other.",
            "So intuitively, I guess you can try to think of it as if you were to decide or I guess OK depending on the window of samples you're looking into.",
            "So this window, I guess, is akin to the width of this local model or the distance of this kernel.",
            "The dimensions input dimensions that are relevant will depend on the size of window or the width of this kernel.",
            "So if you actually didn't know if you were trying to learn both probabilistic at the same time.",
            "You probably have a bit of trouble doing this, and that's what we found."
        ],
        [
            "So this is a quick summary using this puzzle analogy of what's missing."
        ],
        [
            "We're able to lend the smoothing parameter probabilistic Lee and automatically.",
            "That's very nice, but when we go to high dimensions where we have irrelevant redundant inputs, this approach fails."
        ],
        [
            "Now moving on to local Gaussian process regression, which is another popular passive local methods probabilistic local methods.",
            "Here's some recent work that has been done in the past year.",
            "And some of these examples well include applications where they're trying to estimate pose of human or dynamics of robot arm that data presented.",
            "There's also an interesting variant of what Dan was talking about.",
            "Infinite mixture of local GPs.",
            "So this is this, this last class of well, this last method out should be to separate section and address the 1st two categories together.",
            "Um?"
        ],
        [
            "So with the two implementations of local GPs, one for human pose estimation, the other one for learning the Dynamics robot arm.",
            "These Russian very similar in the in their implementation and hopefully this table will sort of tell you really quickly why.",
            "In terms of being able to capture spatially varying functions with spatially varying properties, so by this I mean functions where the density of the samples varies depending on where you are in space or whether noise might vary.",
            "So how does kudasik noise or where the curvature varies really quickly?",
            "So for these these properties, this one implementation by Artison in Darrell is able to capture these sort of.",
            "Non stationary, non stationary in space kualiti since they model the GP experts with different hyperparameters.",
            "Versus this method by Jan Peters in a student.",
            "Joy, they actually use one kernel while one hyperparameter value for all the GPS in their space.",
            "In terms of smoothing or determining the smoothing parameter H. Those methods optimize fine bioqual find this primer Valley by optimizing on a subset of the training data.",
            "So for this paper, the CPR paper they actually use a subset of the local models.",
            "Trained local models for the paper on the on the right.",
            "They actually use a subset of the training samples in the data.",
            "And and it's it's actually understandable to see why they don't use the entire data set, because they have too many data points, and it's a very large datasets.",
            "Specially.",
            "If you have.",
            "Many local experts.",
            "I'm one of these methods.",
            "Does not update the hyperparameter values in real time and the other one does using information gain.",
            "So the method on the right simply, but in new data point comes uses in maximal information gain to decide at which point to whether to add the point.",
            "And if you just get well, OK, so they kept the number of samples that each GP covers to some constant value, like 500.",
            "Something that's small enough to.",
            "A new computational for each local expert, quickly.",
            "An then it's Mr.",
            "Prediction.",
            "One of these methods actually uses the weighted averaging of the predicted posterior distribution predictive distributions from some subset of the local GPs and the other method simply does a weighted averaging over the means.",
            "Over again some T model.",
            "So as you can see here, what should T be?",
            "That's an open question and in the implementations these are done sort of heuristically or ad hoc manner."
        ],
        [
            "So really quickly.",
            "These well, this particular well.",
            "These particular implementations of local GPs are certainly more computationally efficient than a full GP, hence they can be applied to large datasets.",
            "But there are certain sort of caveats with the current implementations.",
            "And this is this includes the fact that the kernel hyperparameters.",
            "So this length scale distance metric is not updated at Test time.",
            "So for sort of real time adaptation this is not done at all as well.",
            "The number of training points that each local GP covers has to be set, and for both implementations there fixed cap to some value like while some values smaller than thousand since that is expressed well empirically, sort of the the.",
            "The barrier.",
            "Because these are GPS, the training samples need to be stored for each local model, so there's a memory requirement that's needed as well.",
            "And as I mentioned before, the smoothing parameter that controls the bias bias variance tradeoff problem are parameters are set using a subset of the data.",
            "So really quickly I'm going to go cover the last sort of.",
            "Um?",
            "Approach out there and you'll see really quickly why divided this like so for these infinite."
        ],
        [
            "Interested local GPs.",
            "The idea is very similar to.",
            "Sort of what you saw previously for the local GP implementations, with one caveat, and that is.",
            "Instead of partitioning the input space, sort of a priore.",
            "Say capping the number of samples each she covers to 500, this infinite mixture treatment tries to put a probabilistic treatment.",
            "Well, treatment on this partitioning so.",
            "Instead of, say one data point being covered by.",
            "1015, however, number of GPS.",
            "This model.",
            "This approach assumes that there is an infinite number of local models that could be covered.",
            "And so this is this this approach.",
            "It's a generative model an you get nice posterior distributions over your estimates.",
            "Up until late last year, only batch versions of this approach had been implemented.",
            "Since there is a pretty rigorous sampling procedure needed for inference.",
            "And in this recent implementation, which I think more details are given in dance poster, they've come up with a sort of incremental online version of this batch model with some variation.",
            "Some augmentations where a particle filtering type, sequential importance sampling approach is used to try to make this very heavy.",
            "Batch process a little bit, while really realizable in real time."
        ],
        [
            "However, there are caveats.",
            "In terms of the model, conceptually, it's nice because it's fully probabilistic, so everyone's been talking about probabilistic models and GPS and doing all that stuff.",
            "So in terms of partitioning the input space, this is a probabilistic treatment, since you're assuming you're not assuming at Prairie certain number of models, you're letting it sort of grow from the data.",
            "However, it's still not computationally as fast as the other local approaches out there.",
            "And so this particular version of.",
            "Of this incremental online infinite mixture model uses particle filtering sort of sampling.",
            "There is still a heavy storage requirement in terms of storing the data points, training points, and it's very hard to scale to large datasets.",
            "I should probably mention as well that I'm not sure if people are so familiar with the GP literature out there, but there are sparse variants of on line GPS, so there was a paper by Chato back from 2002.",
            "An where they try to take a sort of global approach, probabilistic global approach to incremental nonprimary regression.",
            "But not so much work has been done in that field, unfortunately due to very good reasons of computational limitations.",
            "So it's very quickly."
        ],
        [
            "Use my last two minutes.",
            "These are some of the questions."
        ],
        [
            "I sort of thought were interesting, or at least looking at the survey of methods out there.",
            "These are problems that, in my mind, are still not quite or that need to be looked at and rest.",
            "So first question, my mind is welcome this smoothing parameter H whatever you want to call it length scale parameter, distance metric, kernel weights.",
            "Can it be learned probabilistically in high dimensional spaces?",
            "And so questions include you need to pre set something, put some sort of information in this.",
            "How do you update these hyperparameters?",
            "So in a GP very often people don't talk about the updating of the hyperparameters, but for continual learning this this is necessary.",
            "How do we update a model or adaptive model online in a computationally efficient and principled manner?",
            "That's the hyperparameters was talking about an.",
            "With this second question also falls.",
            "I guess you could see it also as a online model selection problem.",
            "How you decide when data is coming into streaming sequential fashion?",
            "What how you should update and change the model?",
            "And last of all, how do we grow local models in a computational, efficient, principled, real time manner?",
            "So you saw that the infinite mixture of GP approach presents a very nice probabilistic way of sort of partitioning the space, but it's at the cost of not being as fast as some of the other locally weighted to purchase.",
            "So the question here is, is a fully probabilistic method necessarily the way to go, or is a partially probabilistic or something probabilistic?",
            "Shooting something to look into?",
            "And with that standard talking questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ranting, I'm from the University of Edinburgh and initially we're going to have a panel discussion at this time, but we thought it best to shift it to the end of the workshop after everyone has talked.",
                    "label": 1
                },
                {
                    "sent": "So we have actually more time to get into open discussion.",
                    "label": 0
                },
                {
                    "sent": "So today I'm going to talk to you about probabilistic local approaches and ways of bringing these method.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's forward and the talk will be primarily.",
                    "label": 0
                },
                {
                    "sent": "Little bit of an introduction or a survey of current methods out there, and then I'll end with an analysis of what's been done recently and what are the open questions and open problems that would be nice to look at.",
                    "label": 0
                },
                {
                    "sent": "So to motivate the use of local methods in robotics.",
                    "label": 1
                },
                {
                    "sent": "One can see examples from this morning that people have talked about.",
                    "label": 1
                },
                {
                    "sent": "Typically these are situations where global methods can't necessarily always be used when either your memory or computation time is limited.",
                    "label": 0
                },
                {
                    "sent": "So an example of this is when you want to do real time learning with adaptation as Ryan was talking about this morning.",
                    "label": 0
                },
                {
                    "sent": "So here I have some examples of some of the systems out there that have been have been used and this includes the Aibo dog.",
                    "label": 0
                },
                {
                    "sent": "Soccer playing dog.",
                    "label": 0
                },
                {
                    "sent": "The logger platform for on line mapping and anthropomorphic arm for for real time drumming.",
                    "label": 0
                },
                {
                    "sent": "So too.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really quickly give you a big overview.",
                    "label": 0
                },
                {
                    "sent": "What do I mean by local versus global methods?",
                    "label": 1
                },
                {
                    "sent": "If we take a look at the problem of nonlinear supervised learning, we can probably try to discretize it in roughly three categories on the left side.",
                    "label": 0
                },
                {
                    "sent": "Here we have global methods such as Gaussian processes, efms neural Nets.",
                    "label": 1
                },
                {
                    "sent": "These methods try to optimize a global cost function over the entire data set.",
                    "label": 1
                },
                {
                    "sent": "On the far right we have local methods such as locally made it learning and different variations of local we are learning.",
                    "label": 0
                },
                {
                    "sent": "Where they optimize the local cost function and use a subset of the training data.",
                    "label": 1
                },
                {
                    "sent": "That is to say, a local distance metric.",
                    "label": 0
                },
                {
                    "sent": "Then we have this sort of fuzzy air in between where it's semi local as I've turned it and this includes methods such as mixtures of experts.",
                    "label": 0
                },
                {
                    "sent": "So for example, mixtures of GP experts or infinite mixtures of GP GP experts that Dan was talking about during the poster session.",
                    "label": 0
                },
                {
                    "sent": "These methods fall in this sort of intermediate.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Us so Christian at the very beginning in the morning, made a list of some of the challenges that we see in functional approximation.",
                    "label": 0
                },
                {
                    "sent": "And here I tried to sort of visually at least put down all the challenges out there with regards to learning in robotics.",
                    "label": 1
                },
                {
                    "sent": "So in blue are pieces of these puzzles related to modeling change challenges in green?",
                    "label": 0
                },
                {
                    "sent": "Are these more algorithmic characteristics that we often like to have?",
                    "label": 0
                },
                {
                    "sent": "Now the requirements for learning robotics are typically quite different than, say, learning in a traditional machine learning setting, where your data set might not be over so large.",
                    "label": 0
                },
                {
                    "sent": "And batch analysis might be fine.",
                    "label": 0
                },
                {
                    "sent": "So in robotics, real-time function approximation is quite crucial.",
                    "label": 0
                },
                {
                    "sent": "So the things that challenges that we often face include sort of data where the dimensions are high.",
                    "label": 0
                },
                {
                    "sent": "We have lots of datasets, lots of samples in our data set, we have input dimensions that are typically well, probably redundant and irrelevant.",
                    "label": 0
                },
                {
                    "sent": "We have outliers, noise.",
                    "label": 0
                },
                {
                    "sent": "We need to come up with algorithms that are quite quite fast in terms of performance time, but also algorithms that can adapt.",
                    "label": 0
                },
                {
                    "sent": "In an online setting, so continual learning as some people have called it where.",
                    "label": 0
                },
                {
                    "sent": "Depending on the data that's coming in, you might actually want to update the model and change the parameters.",
                    "label": 0
                },
                {
                    "sent": "So to motivate this, typically these sort of data clouds or or date datasets.",
                    "label": 0
                },
                {
                    "sent": "Are very hard to capture using traditional parametric family of functions simply because depending where you are in your data space, the characteristics of the underlying function could vary.",
                    "label": 0
                },
                {
                    "sent": "So for this purpose allow.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We start by doing a quick overview of nonparametric regression.",
                    "label": 1
                },
                {
                    "sent": "I'm sure you guys know all about this since most of the speakers this morning were talking about these nonparametric techniques.",
                    "label": 0
                },
                {
                    "sent": "And then I'll quickly touch again.",
                    "label": 0
                },
                {
                    "sent": "Apone difference between local and global methods to motivate why we are interested in local methods.",
                    "label": 0
                },
                {
                    "sent": "And then the latter part of the talk will be sort of focusing more on the current probabilistic local methods that have been recently developed.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sir, nonparametric aggression in a quickly skim over this is by people this morning.",
                    "label": 0
                },
                {
                    "sent": "Ryan and Kristen talked about this as well as deter you want to approximate a function so you have some inputs X corresponding outputs.",
                    "label": 0
                },
                {
                    "sent": "Why that you've observed an you want to estimate this underlying underlying function, but estimating estimating it not using some parametric family of functions.",
                    "label": 0
                },
                {
                    "sent": "But instead using a data driven method to find the shape of this curve.",
                    "label": 0
                },
                {
                    "sent": "So typically we can breakdown some of the category of methods into kernel regression methods, local polling them, polynomial regression methods and smoothing.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lines.",
                    "label": 0
                },
                {
                    "sent": "So and, regression as rare was talking about this morning.",
                    "label": 0
                },
                {
                    "sent": "You basically sort of take an average.",
                    "label": 0
                },
                {
                    "sent": "Over the the samples in your data set.",
                    "label": 0
                },
                {
                    "sent": "So you see here each data point in your training set has some sort of weighting function and depending on where your query point is, you would take me the average of the samples.",
                    "label": 0
                },
                {
                    "sent": "And I've listed here a estimate for the weight for a particular point.",
                    "label": 0
                },
                {
                    "sent": "So the Nadaraya Watson estimate is one of the popular estimators used Gaussian process regression.",
                    "label": 0
                },
                {
                    "sent": "For example, an example is common method or common kernel regression method.",
                    "label": 1
                },
                {
                    "sent": "And the second class of methods include.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With polynomial fitting, so this is a little bit different than kernel regression in the sense that we're actually using weighted least squares to fit some degree polynomial, and so here we use.",
                    "label": 1
                },
                {
                    "sent": "The notion of.",
                    "label": 0
                },
                {
                    "sent": "Unlike kernel regression, it fits using local leaders, voter wait locally weighted regression, not by locally weighted averaging.",
                    "label": 0
                },
                {
                    "sent": "One thing to note is as in in the previous case for kernel regression, you do have a sort of smoothing parameter H. If you remember from the previous slide to determine while to set when calculating your weight matrix.",
                    "label": 0
                },
                {
                    "sent": "Here the WS.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And splines are a sort of 3rd category of methods where you can actually find a function to minimize a explicit sort of pressure and incense.",
                    "label": 1
                },
                {
                    "sent": "So here we have this penalize predicted residual sum of squares.",
                    "label": 0
                },
                {
                    "sent": "And the eight shares this smoothing parameter that you saw before for local polynomials and also for kernel regression.",
                    "label": 0
                },
                {
                    "sent": "And this is a smoothing parameter needs to be specified.",
                    "label": 1
                },
                {
                    "sent": "Now for splines, this H is the roughness penalty.",
                    "label": 0
                },
                {
                    "sent": "That sort of controls the amount of smoothing.",
                    "label": 0
                },
                {
                    "sent": "So if HO are basically not smoothing at all and interpolating data if you set H Infinity, you're infinitely smoothing, so that similar to using a linear least squares estimate.",
                    "label": 0
                },
                {
                    "sent": "So what this all boils down to is essentially.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By bias variance tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Or, you know, terming the distance metric of GP or the length scale of the kernel, or an RBF.",
                    "label": 0
                },
                {
                    "sent": "All these methods require that this parameter smoothing parameter H you specified in set.",
                    "label": 0
                },
                {
                    "sent": "And there are many approaches that have been suggested in the statistics and machine learning literature, and these vary from minimizing some mean squared error prediction prediction error, either using cross validation or maximum likelihood estimate the parameter, the hyperparameter that would minimize this criterion.",
                    "label": 0
                },
                {
                    "sent": "Alternatively, you could try to minimize the mean squared prediction error by optimizing volume of your local model.",
                    "label": 1
                },
                {
                    "sent": "Or one other method proposed by Grace Wahba back in the 70s was to fix the bandwidth parameter, the smoothing parameter instead vary the degree of the polynomial when fitting local polynomials.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is some of the related work out there, just to quickly show you that there's nothing that has been done in the past and a lot of people who've worked on, say splines and kernel regression have sort of working the other two.",
                    "label": 0
                },
                {
                    "sent": "Categories as well.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to quickly return back to the global versus local argument or our sort of view of methods.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have to get categories here.",
                    "label": 0
                },
                {
                    "sent": "One where we can see include methods such as SVM and GPS makes use of GPS an on the left side on the right side.",
                    "label": 0
                },
                {
                    "sent": "Sorry more.",
                    "label": 0
                },
                {
                    "sent": "Locally weighted based methods.",
                    "label": 0
                },
                {
                    "sent": "To chew.",
                    "label": 0
                },
                {
                    "sent": "Just give a quick overview on.",
                    "label": 0
                },
                {
                    "sent": "On well, we can see for global methods, use entire datasets.",
                    "label": 0
                },
                {
                    "sent": "Local methods use a subset of the data set.",
                    "label": 1
                },
                {
                    "sent": "Global methods tend to be computational pricey, as we know for GP, the version of AN by N currents matrix sucks up a lot of time.",
                    "label": 1
                },
                {
                    "sent": "Local methods tend to be a little bit faster and more suitable for real time methods.",
                    "label": 0
                },
                {
                    "sent": "There's a scalability issue with global methods, since the entire data set training data set is sort of needed and well, it's used in the global method.",
                    "label": 0
                },
                {
                    "sent": "And when it comes to an adaptation when a new data point comes in such that you need to update your model in a global method, uses the entire data set, you have to recompute all over again.",
                    "label": 0
                },
                {
                    "sent": "Using all the data, the new parameters corresponding to your model.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So current local methods typically set the smoothing parameter, this bandwidth each of distance metric or hyperparameter.",
                    "label": 1
                },
                {
                    "sent": "Primer using heuristic or ad hoc methods.",
                    "label": 0
                },
                {
                    "sent": "And in the next.",
                    "label": 0
                },
                {
                    "sent": "10 minutes to sort of the left next.",
                    "label": 0
                },
                {
                    "sent": "Part of this talk I'm going to quickly give you an overview of the methods that have been proposed in the past year or two and go through different well do Internet quick analysis of what what's good about them and what still needs to be looked at.",
                    "label": 0
                },
                {
                    "sent": "So we recently developed a basean treatment of local polynomial regression.",
                    "label": 1
                },
                {
                    "sent": "Are local linear regression.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really, where we try to treat the smoothing parameter, each probabilistically, in order to learn it's optimal values.",
                    "label": 1
                },
                {
                    "sent": "So instead of using heuristic techniques like, say, Oliver PR, does we try to give it a little bit more of a probabilistic treatment?",
                    "label": 0
                },
                {
                    "sent": "So this involves treating the weights as well, putting in Bernoulli distribution over the weights, and this in effect kind of models for each point.",
                    "label": 0
                },
                {
                    "sent": "A while we give each sample in the training data a membership sort of indicator.",
                    "label": 0
                },
                {
                    "sent": "So because it's a Bernoulli distribution, the posterior value of the weights were between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So now you see the Bernoulli distribution has a parameter kernel where the H. Parameter the smoothing parameter.",
                    "label": 0
                },
                {
                    "sent": "We then put a gamma distribution over it and infer the posterior.",
                    "label": 0
                },
                {
                    "sent": "So this is in contrast to say, for example, our PR which avoids this problem entirely by imposing in hierarchy hierarchy on.",
                    "label": 0
                },
                {
                    "sent": "Actually, sorry which.",
                    "label": 0
                },
                {
                    "sent": "Basically, doesn't.",
                    "label": 0
                },
                {
                    "sent": "Model H probabilistic, but instead uses local local version of partially squares to find the latent projections on the data space and.",
                    "label": 0
                },
                {
                    "sent": "So here is a very convoluted model.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seems of of the.",
                    "label": 0
                },
                {
                    "sent": "Of this probabilistic local regression method, what's interesting or I guess what's important to notice?",
                    "label": 0
                },
                {
                    "sent": "H notes here.",
                    "label": 0
                },
                {
                    "sent": "Nodes below are the smoothing parameters and they have their connects to the weights nodes in blue.",
                    "label": 0
                },
                {
                    "sent": "So we can actually formulate this entire graph model as an Ant problem and find the parameters to maximize the likelihood of the data.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "One thing about this this sort of process through when we were trying to develop a probabilistic treatment of local local linear regression.",
                    "label": 0
                },
                {
                    "sent": "One thing we found is that.",
                    "label": 0
                },
                {
                    "sent": "We're able to learn the smoothing parameter quite well, but when we then move into spaces where the input dimensions are irrelevant and redundant, so this is sort of in a typical scenario for full time learning.",
                    "label": 0
                },
                {
                    "sent": "Some amount of global information is needed in order to properly learn the bandwidth or the smoothing parameter.",
                    "label": 1
                },
                {
                    "sent": "At the same time as your sparsity parameter.",
                    "label": 0
                },
                {
                    "sent": "So this would be like the precision on your regression variable that specifies which input dimensions are relevant.",
                    "label": 0
                },
                {
                    "sent": "So for those who are you who are aware?",
                    "label": 0
                },
                {
                    "sent": "That would be sort of the ardhi prior put on to the regression variable, and then the precision variable would then be learned.",
                    "label": 0
                },
                {
                    "sent": "So this is one thing which we hypothesize.",
                    "label": 0
                },
                {
                    "sent": "In algebra PR, what's done is the.",
                    "label": 0
                },
                {
                    "sent": "Relevant, I guess.",
                    "label": 0
                },
                {
                    "sent": "Relevant dimensions are in Ferd.",
                    "label": 0
                },
                {
                    "sent": "Given a particular distance metric, so this hierarchy is already sort of imposed versus in a probabilistic model, if we were to model both, the deeper the precision variable and the smoothing parameter both both probabilistically, it would be sort of a chicken egg problem in the sense that one parameter, one variable, will be dependent on the other.",
                    "label": 0
                },
                {
                    "sent": "So intuitively, I guess you can try to think of it as if you were to decide or I guess OK depending on the window of samples you're looking into.",
                    "label": 0
                },
                {
                    "sent": "So this window, I guess, is akin to the width of this local model or the distance of this kernel.",
                    "label": 0
                },
                {
                    "sent": "The dimensions input dimensions that are relevant will depend on the size of window or the width of this kernel.",
                    "label": 1
                },
                {
                    "sent": "So if you actually didn't know if you were trying to learn both probabilistic at the same time.",
                    "label": 0
                },
                {
                    "sent": "You probably have a bit of trouble doing this, and that's what we found.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a quick summary using this puzzle analogy of what's missing.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're able to lend the smoothing parameter probabilistic Lee and automatically.",
                    "label": 0
                },
                {
                    "sent": "That's very nice, but when we go to high dimensions where we have irrelevant redundant inputs, this approach fails.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now moving on to local Gaussian process regression, which is another popular passive local methods probabilistic local methods.",
                    "label": 0
                },
                {
                    "sent": "Here's some recent work that has been done in the past year.",
                    "label": 0
                },
                {
                    "sent": "And some of these examples well include applications where they're trying to estimate pose of human or dynamics of robot arm that data presented.",
                    "label": 0
                },
                {
                    "sent": "There's also an interesting variant of what Dan was talking about.",
                    "label": 0
                },
                {
                    "sent": "Infinite mixture of local GPs.",
                    "label": 1
                },
                {
                    "sent": "So this is this, this last class of well, this last method out should be to separate section and address the 1st two categories together.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with the two implementations of local GPs, one for human pose estimation, the other one for learning the Dynamics robot arm.",
                    "label": 0
                },
                {
                    "sent": "These Russian very similar in the in their implementation and hopefully this table will sort of tell you really quickly why.",
                    "label": 0
                },
                {
                    "sent": "In terms of being able to capture spatially varying functions with spatially varying properties, so by this I mean functions where the density of the samples varies depending on where you are in space or whether noise might vary.",
                    "label": 0
                },
                {
                    "sent": "So how does kudasik noise or where the curvature varies really quickly?",
                    "label": 0
                },
                {
                    "sent": "So for these these properties, this one implementation by Artison in Darrell is able to capture these sort of.",
                    "label": 0
                },
                {
                    "sent": "Non stationary, non stationary in space kualiti since they model the GP experts with different hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Versus this method by Jan Peters in a student.",
                    "label": 0
                },
                {
                    "sent": "Joy, they actually use one kernel while one hyperparameter value for all the GPS in their space.",
                    "label": 0
                },
                {
                    "sent": "In terms of smoothing or determining the smoothing parameter H. Those methods optimize fine bioqual find this primer Valley by optimizing on a subset of the training data.",
                    "label": 0
                },
                {
                    "sent": "So for this paper, the CPR paper they actually use a subset of the local models.",
                    "label": 0
                },
                {
                    "sent": "Trained local models for the paper on the on the right.",
                    "label": 0
                },
                {
                    "sent": "They actually use a subset of the training samples in the data.",
                    "label": 0
                },
                {
                    "sent": "And and it's it's actually understandable to see why they don't use the entire data set, because they have too many data points, and it's a very large datasets.",
                    "label": 0
                },
                {
                    "sent": "Specially.",
                    "label": 0
                },
                {
                    "sent": "If you have.",
                    "label": 0
                },
                {
                    "sent": "Many local experts.",
                    "label": 0
                },
                {
                    "sent": "I'm one of these methods.",
                    "label": 0
                },
                {
                    "sent": "Does not update the hyperparameter values in real time and the other one does using information gain.",
                    "label": 0
                },
                {
                    "sent": "So the method on the right simply, but in new data point comes uses in maximal information gain to decide at which point to whether to add the point.",
                    "label": 0
                },
                {
                    "sent": "And if you just get well, OK, so they kept the number of samples that each GP covers to some constant value, like 500.",
                    "label": 0
                },
                {
                    "sent": "Something that's small enough to.",
                    "label": 0
                },
                {
                    "sent": "A new computational for each local expert, quickly.",
                    "label": 0
                },
                {
                    "sent": "An then it's Mr.",
                    "label": 0
                },
                {
                    "sent": "Prediction.",
                    "label": 0
                },
                {
                    "sent": "One of these methods actually uses the weighted averaging of the predicted posterior distribution predictive distributions from some subset of the local GPs and the other method simply does a weighted averaging over the means.",
                    "label": 1
                },
                {
                    "sent": "Over again some T model.",
                    "label": 0
                },
                {
                    "sent": "So as you can see here, what should T be?",
                    "label": 0
                },
                {
                    "sent": "That's an open question and in the implementations these are done sort of heuristically or ad hoc manner.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So really quickly.",
                    "label": 0
                },
                {
                    "sent": "These well, this particular well.",
                    "label": 0
                },
                {
                    "sent": "These particular implementations of local GPs are certainly more computationally efficient than a full GP, hence they can be applied to large datasets.",
                    "label": 1
                },
                {
                    "sent": "But there are certain sort of caveats with the current implementations.",
                    "label": 0
                },
                {
                    "sent": "And this is this includes the fact that the kernel hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So this length scale distance metric is not updated at Test time.",
                    "label": 0
                },
                {
                    "sent": "So for sort of real time adaptation this is not done at all as well.",
                    "label": 0
                },
                {
                    "sent": "The number of training points that each local GP covers has to be set, and for both implementations there fixed cap to some value like while some values smaller than thousand since that is expressed well empirically, sort of the the.",
                    "label": 0
                },
                {
                    "sent": "The barrier.",
                    "label": 0
                },
                {
                    "sent": "Because these are GPS, the training samples need to be stored for each local model, so there's a memory requirement that's needed as well.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned before, the smoothing parameter that controls the bias bias variance tradeoff problem are parameters are set using a subset of the data.",
                    "label": 0
                },
                {
                    "sent": "So really quickly I'm going to go cover the last sort of.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Approach out there and you'll see really quickly why divided this like so for these infinite.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interested local GPs.",
                    "label": 0
                },
                {
                    "sent": "The idea is very similar to.",
                    "label": 0
                },
                {
                    "sent": "Sort of what you saw previously for the local GP implementations, with one caveat, and that is.",
                    "label": 0
                },
                {
                    "sent": "Instead of partitioning the input space, sort of a priore.",
                    "label": 0
                },
                {
                    "sent": "Say capping the number of samples each she covers to 500, this infinite mixture treatment tries to put a probabilistic treatment.",
                    "label": 0
                },
                {
                    "sent": "Well, treatment on this partitioning so.",
                    "label": 0
                },
                {
                    "sent": "Instead of, say one data point being covered by.",
                    "label": 0
                },
                {
                    "sent": "1015, however, number of GPS.",
                    "label": 0
                },
                {
                    "sent": "This model.",
                    "label": 0
                },
                {
                    "sent": "This approach assumes that there is an infinite number of local models that could be covered.",
                    "label": 0
                },
                {
                    "sent": "And so this is this this approach.",
                    "label": 0
                },
                {
                    "sent": "It's a generative model an you get nice posterior distributions over your estimates.",
                    "label": 0
                },
                {
                    "sent": "Up until late last year, only batch versions of this approach had been implemented.",
                    "label": 0
                },
                {
                    "sent": "Since there is a pretty rigorous sampling procedure needed for inference.",
                    "label": 0
                },
                {
                    "sent": "And in this recent implementation, which I think more details are given in dance poster, they've come up with a sort of incremental online version of this batch model with some variation.",
                    "label": 0
                },
                {
                    "sent": "Some augmentations where a particle filtering type, sequential importance sampling approach is used to try to make this very heavy.",
                    "label": 0
                },
                {
                    "sent": "Batch process a little bit, while really realizable in real time.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, there are caveats.",
                    "label": 0
                },
                {
                    "sent": "In terms of the model, conceptually, it's nice because it's fully probabilistic, so everyone's been talking about probabilistic models and GPS and doing all that stuff.",
                    "label": 0
                },
                {
                    "sent": "So in terms of partitioning the input space, this is a probabilistic treatment, since you're assuming you're not assuming at Prairie certain number of models, you're letting it sort of grow from the data.",
                    "label": 0
                },
                {
                    "sent": "However, it's still not computationally as fast as the other local approaches out there.",
                    "label": 0
                },
                {
                    "sent": "And so this particular version of.",
                    "label": 0
                },
                {
                    "sent": "Of this incremental online infinite mixture model uses particle filtering sort of sampling.",
                    "label": 0
                },
                {
                    "sent": "There is still a heavy storage requirement in terms of storing the data points, training points, and it's very hard to scale to large datasets.",
                    "label": 1
                },
                {
                    "sent": "I should probably mention as well that I'm not sure if people are so familiar with the GP literature out there, but there are sparse variants of on line GPS, so there was a paper by Chato back from 2002.",
                    "label": 0
                },
                {
                    "sent": "An where they try to take a sort of global approach, probabilistic global approach to incremental nonprimary regression.",
                    "label": 0
                },
                {
                    "sent": "But not so much work has been done in that field, unfortunately due to very good reasons of computational limitations.",
                    "label": 0
                },
                {
                    "sent": "So it's very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use my last two minutes.",
                    "label": 0
                },
                {
                    "sent": "These are some of the questions.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I sort of thought were interesting, or at least looking at the survey of methods out there.",
                    "label": 0
                },
                {
                    "sent": "These are problems that, in my mind, are still not quite or that need to be looked at and rest.",
                    "label": 0
                },
                {
                    "sent": "So first question, my mind is welcome this smoothing parameter H whatever you want to call it length scale parameter, distance metric, kernel weights.",
                    "label": 0
                },
                {
                    "sent": "Can it be learned probabilistically in high dimensional spaces?",
                    "label": 0
                },
                {
                    "sent": "And so questions include you need to pre set something, put some sort of information in this.",
                    "label": 0
                },
                {
                    "sent": "How do you update these hyperparameters?",
                    "label": 0
                },
                {
                    "sent": "So in a GP very often people don't talk about the updating of the hyperparameters, but for continual learning this this is necessary.",
                    "label": 0
                },
                {
                    "sent": "How do we update a model or adaptive model online in a computationally efficient and principled manner?",
                    "label": 1
                },
                {
                    "sent": "That's the hyperparameters was talking about an.",
                    "label": 0
                },
                {
                    "sent": "With this second question also falls.",
                    "label": 0
                },
                {
                    "sent": "I guess you could see it also as a online model selection problem.",
                    "label": 0
                },
                {
                    "sent": "How you decide when data is coming into streaming sequential fashion?",
                    "label": 0
                },
                {
                    "sent": "What how you should update and change the model?",
                    "label": 1
                },
                {
                    "sent": "And last of all, how do we grow local models in a computational, efficient, principled, real time manner?",
                    "label": 0
                },
                {
                    "sent": "So you saw that the infinite mixture of GP approach presents a very nice probabilistic way of sort of partitioning the space, but it's at the cost of not being as fast as some of the other locally weighted to purchase.",
                    "label": 0
                },
                {
                    "sent": "So the question here is, is a fully probabilistic method necessarily the way to go, or is a partially probabilistic or something probabilistic?",
                    "label": 0
                },
                {
                    "sent": "Shooting something to look into?",
                    "label": 0
                },
                {
                    "sent": "And with that standard talking questions.",
                    "label": 0
                }
            ]
        }
    }
}