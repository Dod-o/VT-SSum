{
    "id": "4jeoam6adguw2asmo656k4wvdecx7avc",
    "title": "Look-Ahead Before You Leap: End-to-End Active Recognition by Forecasting the Effect of Motion",
    "info": {
        "author": [
            "Dinesh Jayaraman, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "Oct. 24, 2016",
        "recorded": "October 2016",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2016_jayaraman_active_recognition/",
    "segmentation": [
        [
            "My name is Dennis gentlemen, and this paper is going to be about active recognition and about how learning to predict the effects of your actions is a good auxiliary task for learning to perform active recognition.",
            "Alright."
        ],
        [
            "So today's standard recognition systems operate in the passive setting, so that means that at training time you have bags of manually labeled snapshots.",
            "Anna test time you."
        ],
        [
            "The name of the object of the scene category in new snapshots.",
            "Now this setting ignores the fact that one snapshot is usually not enough to fully observe an object or a scene.",
            "So take this scene for instance.",
            "If you only look at this one view."
        ],
        [
            "You might not notice that there's a herd of elephants near you.",
            "Or more importantly, that one particular member of that herd has taken a rather keen interest in you.",
            "Unless you move your camera around to look.",
            "Now this sort of action and emotional involved in our environment is actually something that we do daily as an integral part of the perception process.",
            "We pick up objects, we turn them around, we walk over to Windows, etc, and in particular an agent that's capable of this sort of action and motion becomes capable of active recognition."
        ],
        [
            "To understand what that means, suppose that."
        ],
        [
            "Robot picks up an object and observes one view of it and it's."
        ],
        [
            "Immediately, confident of the category of the object.",
            "Then it could choose to move or manipulate the object intelligently in such a way as to disambiguate."
        ],
        [
            "Could stop competing hypothesis and this kind of setting where an agent is actually able to select its own views."
        ],
        [
            "Called the active recognition setting, and that's the setting that we'll be dealing in.",
            "Alright."
        ],
        [
            "So at a high level, what is the difference from the standard passive setting where the first thing that you'll notice is?"
        ],
        [
            "But the kind of views that you will get will look nothing like the standard passive views that you get.",
            "That are, you know, well composed human images.",
            "Human uploaded images that are relatively well composed and so on.",
            "So that's going to make a recognition of those views harder right away.",
            "But when you think about it in the active set."
        ],
        [
            "You don't actually have to perform recognition from single views anymore.",
            "You can instead choose to look at multiple views and fuse the evidence across those multiple views, and more importantly, you have the ability to strategically select views in such a way as to maximize your recognition performance.",
            "Alright."
        ],
        [
            "So having understood the setting a little bit, let's start thinking about what an active recognition pipeline could look like.",
            "So given."
        ],
        [
            "Starting view, you pass it through a perception."
        ],
        [
            "Pipeline that does two things."
        ],
        [
            "Gives you its best guess about the category and it also feel."
        ],
        [
            "Is an action selection pipeline that controls an actuator."
        ],
        [
            "Which then feeds you to your next view."
        ],
        [
            "So now, once you have the next few you pass through the same perception pipeline as before, but this time because you have."
        ],
        [
            "2 views you instead choose to fuse the evidence from both views and now update your category hypoth."
        ],
        [
            "And this can go on and on.",
            "So this is how the whole pipeline will generally work.",
            "But we can stop the pipeline right here and observe that we've gotten these three basic sub tasks that are going to constitute our active recognition task percent."
        ],
        [
            "An action selection and evidence Fusion, and these are the subtasks, will have to solve to solve active recognition."
        ],
        [
            "Now the general idea in prior work is to come up with purist."
        ],
        [
            "Chicken, often more or less independent solutions to these three different components, and we see these as major shortcomings that we can improve upon.",
            "In particular, take for instance, the perception pipeline in prior work.",
            "Nearly all prior work treats the perception sub task is being entirely independent of the overall active recognition tasks, so they train a perception module purely for one view.",
            "Passive snapshot recognition.",
            "But this is clearly not the optimal thing to do.",
            "To see why, for instance, observed that the passive setting places a large emphasis on being being invariant to post changes, for example.",
            "While in the active setting you might not care so much because you can instead simply move your camera around.",
            "So this is just one example illustrating why it's important to design each of these components in this overall active recognition task to fit well with each other.",
            "So ideally you want to have a you want to have a way to kind of optimize each sub tasks performance for the overall active recognition task.",
            "So that is precisely what we."
        ],
        [
            "To do we account for the fact that these components of the active recognition problem are so closely intertwined and so we account for this by training them all jointly in an end to end manner.",
            "Now there are."
        ],
        [
            "Improvements that are being made here first.",
            "All of these components are being learned from data rather than relying on manual heuristics alone.",
            "And Secondly, the fact that they are being trained jointly means that they automatically play well together.",
            "Additionally, we jointly train our system."
        ],
        [
            "On the task of predicting the effects of its actions now this lookahead task is closely related to the target task of active recognition, but it can also be trained entirely without manual labels, only using internal supervision that comes from your internal proprioceptive signals about which actions you've performed, so that allows our method to more efficiently exploit any available manual supervision.",
            "So in a nutshell."
        ],
        [
            "What we're going to try to do is multi task training of active recognition components together with the look ahead task.",
            "Alright."
        ],
        [
            "Another important aspect of what we're doing is the nature scale and complexity of the task that we choose to tackle.",
            "Prior work in this area has typically been confined to dealing with."
        ],
        [
            "Simple proof of concept data sets, often with custom robotic setups and we instead choose to make the leap to targeting realistic."
        ],
        [
            "Can complex category recognition problems with real world imagery?",
            "So what you see on the left here represents a robot that's been rubbed into a scene and it has to direct its camera to observe the scene and name the category and on the right you see video of a robotic arm that's manipulating an object for object recognition, and these are both experimental settings that will eventually test on.",
            "But before that, let me describe the system itself.",
            "I'll briefly over."
        ],
        [
            "View the approach before walking through the details.",
            "So given the current view, the sensor module is going to embed it into a feature space and this feature."
        ],
        [
            "It is then passed to an aggregator which combines the evidence from this current view with evidence from past views to produce an aggregated feature vector.",
            "This aggregated feature vector is now further fed into the classifier, which predicts the class likelihoods for this current time step.",
            "Now, in preparation for the next time step, the actor module acts upon the same aggregated evidence and produces the action that is to be performed at the next time step and simultaneously the look ahead module predicts the effect of that action.",
            "So this is at the current time step.",
            "But to see what happens at the next time step, let's look at this unrolled version of the architecture instead.",
            "So now you'll notice that the active module at time step T produces the action that feeds the action sensor module at time T + 1, and now the whole process can continue all over again.",
            "So this is the high level flow that we will be using.",
            "Now let's start looking at some of the details of these MoD."
        ],
        [
            "Rules and their internal architectures.",
            "The sensor MoD."
        ],
        [
            "There will be a simple 2 stream feedforward neural network that takes in as input, both anima."
        ],
        [
            "Representing the current view and the."
        ],
        [
            "And camera pose and embed them jointly."
        ],
        [
            "Into a feature vector.",
            "This feature vector is now fed into the aggregate."
        ],
        [
            "Module which is a recurrent neural network.",
            "The aggregator now combines the evidence coming."
        ],
        [
            "This current view with evidence from past."
        ],
        [
            "Use that stored in its memory and produces an aggregated."
        ],
        [
            "Vector is output now in preparation for the next time step.",
            "It updates, it updates its internal memory."
        ],
        [
            "And now the look ahead."
        ],
        [
            "Takes in as input both the action and the aggregate feature vector at the previous time step, again stored in memory.",
            "And attempts to predict this."
        ],
        [
            "Date of the aggregate feature vector at the current time step, so its output is going to be the error in this prediction.",
            "And this task of predicting the effect."
        ],
        [
            "Of its own actions is essentially closely related to the target task of active recognition, and it can be trained entirely without manual labels.",
            "As you can see, since it only relies on internal proprioceptive signals.",
            "And this is all."
        ],
        [
            "So related to prior work in which we showed that training assistant training representations for for equal variance or predictable responses to observer actions or motions is useful for even static scene recognition tasks.",
            "Now."
        ],
        [
            "Armed with the aggregate feature representation and the current camera pose, the actor module produces a PDF over all legal actions and during training and action is sampled at random from this PDF and this is important because this makes the network stochastic and the stochasticity is necessary for training.",
            "Using the reinforcement learning scheme that will introduce shortly.",
            "And the classic."
        ],
        [
            "Our model is a simple module that acts upon the aggregated feature vector and produces the class likelihoods.",
            "Now where we put all these new."
        ],
        [
            "Other modules back into our original architecture.",
            "What we get is one large neural network that strained through a combination of three losses, softmax loss on the outputs of the classifier or classification reward, which is a reinforcement reward computed from the outputs of the classifier, but applied to the output nodes of the actor module.",
            "And then the look ahead loss that we were speaking about.",
            "So there is a combination of gradient descent and reinforce involved in this training procedure, very similar to previous methods proposed by Williams at all, and knee at all.",
            "Alright, so moving."
        ],
        [
            "The experiments remember how I introduced earlier that we were going to try to target realistic and complex category recognition problems in comparison to prior work?",
            "Another important thing is that in prior work, they've traditionally used custom robotic setups, which gets in the way of benchmarking and reproducibility.",
            "So we try to avoid this."
        ],
        [
            "And instead set up active recognition tasks using off the shelf datasets instead.",
            "So towards this we first set up an active scene recognition task using some 360 panoramas.",
            "Here the robot or the visual agent is essentially confined to use a 45 degree field of your camera and observe the scene and move its camera over the scene for scene recognition.",
            "Our second task is."
        ],
        [
            "Germs toy manipulation tasks where we use the germs data set which consists of robotic arms, manipulating toy objects, and now the agent's task is to select frames from the video and recognize the object.",
            "And finally we test on this."
        ],
        [
            "The CAD model data set for comparison comparison against some very recent work here the virtual agent has access to the virtual camera that's focused on the object and it can control the virtual camera.",
            "Alright."
        ],
        [
            "So moving on to results.",
            "We test our method against several baselines that have been reimplemented and upgraded with CNN features, ETC.",
            "For fairness, and for comparison for matching our experimental settings, I'm going to plot each methods performance as a function of the number of views.",
            "So first a passive neural network that's trained only for single snapshot recognition on the Sun 360 task gets to only about 40%.",
            "Or trans information baseline based on Sheila at all, gets to about 45% after 5 views.",
            "The sequential decision process baseline of denzler at all gets to about 47% and the combination of these two prior baseline gets baselines gets to about 48%.",
            "Now compare this with our method, which gets to about 65% after five years, which is a huge gain over these prior methods."
        ],
        [
            "And we observe similarly large gains on the germs of object manipulation task.",
            "And find."
        ],
        [
            "Leon, the model net 10 data set.",
            "We compare our method against two very recent methods.",
            "The shape Nets method of Wu at all 2015 and the pairwise method of Johns at all 2016.",
            "Both of these methods use RGB D data, while our method only uses RGB.",
            "And despite this we find that we strongly outperformed the shape method and perform on par with the pairwise method.",
            "Now this is despite the fact that the pairwise method not only uses additional depth information, but also uses a very complex training procedure involving training multiple, CNN's one after the other.",
            "So these are."
        ],
        [
            "According results and All in all, we take the message that our method strongly outperforms representative traditional approaches across a variety of tasks.",
            "Alright, so moving on to something."
        ],
        [
            "The results let me show you this example of a Plaza courtyard scene."
        ],
        [
            "What you see at the top right here is the view that the agent currently observes.",
            "And."
        ],
        [
            "The bottom you see the grid views created from the Panorama where the pink square represents the current view and the yellow squares represent potential next views.",
            "And right at the top, here you see."
        ],
        [
            "The system currently assigns a pretty low probability to this being a Plaza courtyard, which is not very good, but it's first guess."
        ],
        [
            "Is that this is a restaurant which I would say is a pretty good guess.",
            "Just going by this view."
        ],
        [
            "Now at the next time step, it observes a large number of people looking in the same direction, and it revises opinions and its best guess is now the."
        ],
        [
            "Category is now the theater category."
        ],
        [
            "Again, I would say this is a pretty good guess just going by this view."
        ],
        [
            "And finally, it does something interesting where it appears to follow the gaze direction of these these viewers in the second time, steppan looks at this brick building and because it this appears to be an outdoor scene, perhaps it revises opinions immediately and converges on Plaza Courtyard, which is the current correct, yes?",
            "Here's another example from the Germans data set.",
            "These are."
        ],
        [
            "First 2 views of it."
        ],
        [
            "Of an object of an object being manipulated by a robot and see since these views are oblique and poorly lit, the best guess of the robot at this stage is close to the correct guess, but it's not quite there."
        ],
        [
            "And finally, the robot manages to manipulate the object to a well lit and face frontal view of the object and gets to guess correctly."
        ],
        [
            "And these views are actually quite fun to look at, so I'd encourage you to take a look at more examples in my paper and my supplementary material."
        ],
        [
            "Alright, to sum things up, we proposed a method for end to end learning of an active recognition system which yielded large gains over prior work.",
            "The system improves still further when we jointly trained it on an unsupervised lookahead task of predicting the effects of its actions, and finally we showed results with experimental settings that are both realistic and complex, but also easily reproducible.",
            "Data and code will soon be shared on my web page and I really think that this is only the beginning of methods that deal with these kinds of problems, and they will only get more interesting as time goes on.",
            "So I encourage you to take a look.",
            "Thank you all for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Dennis gentlemen, and this paper is going to be about active recognition and about how learning to predict the effects of your actions is a good auxiliary task for learning to perform active recognition.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today's standard recognition systems operate in the passive setting, so that means that at training time you have bags of manually labeled snapshots.",
                    "label": 0
                },
                {
                    "sent": "Anna test time you.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The name of the object of the scene category in new snapshots.",
                    "label": 0
                },
                {
                    "sent": "Now this setting ignores the fact that one snapshot is usually not enough to fully observe an object or a scene.",
                    "label": 0
                },
                {
                    "sent": "So take this scene for instance.",
                    "label": 0
                },
                {
                    "sent": "If you only look at this one view.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You might not notice that there's a herd of elephants near you.",
                    "label": 0
                },
                {
                    "sent": "Or more importantly, that one particular member of that herd has taken a rather keen interest in you.",
                    "label": 0
                },
                {
                    "sent": "Unless you move your camera around to look.",
                    "label": 0
                },
                {
                    "sent": "Now this sort of action and emotional involved in our environment is actually something that we do daily as an integral part of the perception process.",
                    "label": 0
                },
                {
                    "sent": "We pick up objects, we turn them around, we walk over to Windows, etc, and in particular an agent that's capable of this sort of action and motion becomes capable of active recognition.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To understand what that means, suppose that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Robot picks up an object and observes one view of it and it's.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Immediately, confident of the category of the object.",
                    "label": 0
                },
                {
                    "sent": "Then it could choose to move or manipulate the object intelligently in such a way as to disambiguate.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Could stop competing hypothesis and this kind of setting where an agent is actually able to select its own views.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Called the active recognition setting, and that's the setting that we'll be dealing in.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at a high level, what is the difference from the standard passive setting where the first thing that you'll notice is?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the kind of views that you will get will look nothing like the standard passive views that you get.",
                    "label": 0
                },
                {
                    "sent": "That are, you know, well composed human images.",
                    "label": 0
                },
                {
                    "sent": "Human uploaded images that are relatively well composed and so on.",
                    "label": 0
                },
                {
                    "sent": "So that's going to make a recognition of those views harder right away.",
                    "label": 0
                },
                {
                    "sent": "But when you think about it in the active set.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You don't actually have to perform recognition from single views anymore.",
                    "label": 0
                },
                {
                    "sent": "You can instead choose to look at multiple views and fuse the evidence across those multiple views, and more importantly, you have the ability to strategically select views in such a way as to maximize your recognition performance.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So having understood the setting a little bit, let's start thinking about what an active recognition pipeline could look like.",
                    "label": 0
                },
                {
                    "sent": "So given.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Starting view, you pass it through a perception.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pipeline that does two things.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gives you its best guess about the category and it also feel.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is an action selection pipeline that controls an actuator.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which then feeds you to your next view.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now, once you have the next few you pass through the same perception pipeline as before, but this time because you have.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2 views you instead choose to fuse the evidence from both views and now update your category hypoth.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this can go on and on.",
                    "label": 0
                },
                {
                    "sent": "So this is how the whole pipeline will generally work.",
                    "label": 0
                },
                {
                    "sent": "But we can stop the pipeline right here and observe that we've gotten these three basic sub tasks that are going to constitute our active recognition task percent.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An action selection and evidence Fusion, and these are the subtasks, will have to solve to solve active recognition.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the general idea in prior work is to come up with purist.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chicken, often more or less independent solutions to these three different components, and we see these as major shortcomings that we can improve upon.",
                    "label": 0
                },
                {
                    "sent": "In particular, take for instance, the perception pipeline in prior work.",
                    "label": 0
                },
                {
                    "sent": "Nearly all prior work treats the perception sub task is being entirely independent of the overall active recognition tasks, so they train a perception module purely for one view.",
                    "label": 0
                },
                {
                    "sent": "Passive snapshot recognition.",
                    "label": 0
                },
                {
                    "sent": "But this is clearly not the optimal thing to do.",
                    "label": 0
                },
                {
                    "sent": "To see why, for instance, observed that the passive setting places a large emphasis on being being invariant to post changes, for example.",
                    "label": 0
                },
                {
                    "sent": "While in the active setting you might not care so much because you can instead simply move your camera around.",
                    "label": 0
                },
                {
                    "sent": "So this is just one example illustrating why it's important to design each of these components in this overall active recognition task to fit well with each other.",
                    "label": 0
                },
                {
                    "sent": "So ideally you want to have a you want to have a way to kind of optimize each sub tasks performance for the overall active recognition task.",
                    "label": 0
                },
                {
                    "sent": "So that is precisely what we.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do we account for the fact that these components of the active recognition problem are so closely intertwined and so we account for this by training them all jointly in an end to end manner.",
                    "label": 0
                },
                {
                    "sent": "Now there are.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Improvements that are being made here first.",
                    "label": 0
                },
                {
                    "sent": "All of these components are being learned from data rather than relying on manual heuristics alone.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, the fact that they are being trained jointly means that they automatically play well together.",
                    "label": 0
                },
                {
                    "sent": "Additionally, we jointly train our system.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the task of predicting the effects of its actions now this lookahead task is closely related to the target task of active recognition, but it can also be trained entirely without manual labels, only using internal supervision that comes from your internal proprioceptive signals about which actions you've performed, so that allows our method to more efficiently exploit any available manual supervision.",
                    "label": 0
                },
                {
                    "sent": "So in a nutshell.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we're going to try to do is multi task training of active recognition components together with the look ahead task.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another important aspect of what we're doing is the nature scale and complexity of the task that we choose to tackle.",
                    "label": 0
                },
                {
                    "sent": "Prior work in this area has typically been confined to dealing with.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple proof of concept data sets, often with custom robotic setups and we instead choose to make the leap to targeting realistic.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can complex category recognition problems with real world imagery?",
                    "label": 1
                },
                {
                    "sent": "So what you see on the left here represents a robot that's been rubbed into a scene and it has to direct its camera to observe the scene and name the category and on the right you see video of a robotic arm that's manipulating an object for object recognition, and these are both experimental settings that will eventually test on.",
                    "label": 0
                },
                {
                    "sent": "But before that, let me describe the system itself.",
                    "label": 0
                },
                {
                    "sent": "I'll briefly over.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "View the approach before walking through the details.",
                    "label": 0
                },
                {
                    "sent": "So given the current view, the sensor module is going to embed it into a feature space and this feature.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is then passed to an aggregator which combines the evidence from this current view with evidence from past views to produce an aggregated feature vector.",
                    "label": 0
                },
                {
                    "sent": "This aggregated feature vector is now further fed into the classifier, which predicts the class likelihoods for this current time step.",
                    "label": 0
                },
                {
                    "sent": "Now, in preparation for the next time step, the actor module acts upon the same aggregated evidence and produces the action that is to be performed at the next time step and simultaneously the look ahead module predicts the effect of that action.",
                    "label": 0
                },
                {
                    "sent": "So this is at the current time step.",
                    "label": 0
                },
                {
                    "sent": "But to see what happens at the next time step, let's look at this unrolled version of the architecture instead.",
                    "label": 0
                },
                {
                    "sent": "So now you'll notice that the active module at time step T produces the action that feeds the action sensor module at time T + 1, and now the whole process can continue all over again.",
                    "label": 0
                },
                {
                    "sent": "So this is the high level flow that we will be using.",
                    "label": 0
                },
                {
                    "sent": "Now let's start looking at some of the details of these MoD.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rules and their internal architectures.",
                    "label": 0
                },
                {
                    "sent": "The sensor MoD.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There will be a simple 2 stream feedforward neural network that takes in as input, both anima.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Representing the current view and the.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And camera pose and embed them jointly.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into a feature vector.",
                    "label": 0
                },
                {
                    "sent": "This feature vector is now fed into the aggregate.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Module which is a recurrent neural network.",
                    "label": 0
                },
                {
                    "sent": "The aggregator now combines the evidence coming.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This current view with evidence from past.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use that stored in its memory and produces an aggregated.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vector is output now in preparation for the next time step.",
                    "label": 0
                },
                {
                    "sent": "It updates, it updates its internal memory.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now the look ahead.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Takes in as input both the action and the aggregate feature vector at the previous time step, again stored in memory.",
                    "label": 0
                },
                {
                    "sent": "And attempts to predict this.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Date of the aggregate feature vector at the current time step, so its output is going to be the error in this prediction.",
                    "label": 0
                },
                {
                    "sent": "And this task of predicting the effect.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of its own actions is essentially closely related to the target task of active recognition, and it can be trained entirely without manual labels.",
                    "label": 0
                },
                {
                    "sent": "As you can see, since it only relies on internal proprioceptive signals.",
                    "label": 0
                },
                {
                    "sent": "And this is all.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So related to prior work in which we showed that training assistant training representations for for equal variance or predictable responses to observer actions or motions is useful for even static scene recognition tasks.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Armed with the aggregate feature representation and the current camera pose, the actor module produces a PDF over all legal actions and during training and action is sampled at random from this PDF and this is important because this makes the network stochastic and the stochasticity is necessary for training.",
                    "label": 0
                },
                {
                    "sent": "Using the reinforcement learning scheme that will introduce shortly.",
                    "label": 0
                },
                {
                    "sent": "And the classic.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our model is a simple module that acts upon the aggregated feature vector and produces the class likelihoods.",
                    "label": 0
                },
                {
                    "sent": "Now where we put all these new.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other modules back into our original architecture.",
                    "label": 0
                },
                {
                    "sent": "What we get is one large neural network that strained through a combination of three losses, softmax loss on the outputs of the classifier or classification reward, which is a reinforcement reward computed from the outputs of the classifier, but applied to the output nodes of the actor module.",
                    "label": 0
                },
                {
                    "sent": "And then the look ahead loss that we were speaking about.",
                    "label": 0
                },
                {
                    "sent": "So there is a combination of gradient descent and reinforce involved in this training procedure, very similar to previous methods proposed by Williams at all, and knee at all.",
                    "label": 1
                },
                {
                    "sent": "Alright, so moving.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The experiments remember how I introduced earlier that we were going to try to target realistic and complex category recognition problems in comparison to prior work?",
                    "label": 0
                },
                {
                    "sent": "Another important thing is that in prior work, they've traditionally used custom robotic setups, which gets in the way of benchmarking and reproducibility.",
                    "label": 0
                },
                {
                    "sent": "So we try to avoid this.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And instead set up active recognition tasks using off the shelf datasets instead.",
                    "label": 0
                },
                {
                    "sent": "So towards this we first set up an active scene recognition task using some 360 panoramas.",
                    "label": 1
                },
                {
                    "sent": "Here the robot or the visual agent is essentially confined to use a 45 degree field of your camera and observe the scene and move its camera over the scene for scene recognition.",
                    "label": 0
                },
                {
                    "sent": "Our second task is.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Germs toy manipulation tasks where we use the germs data set which consists of robotic arms, manipulating toy objects, and now the agent's task is to select frames from the video and recognize the object.",
                    "label": 0
                },
                {
                    "sent": "And finally we test on this.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The CAD model data set for comparison comparison against some very recent work here the virtual agent has access to the virtual camera that's focused on the object and it can control the virtual camera.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So moving on to results.",
                    "label": 0
                },
                {
                    "sent": "We test our method against several baselines that have been reimplemented and upgraded with CNN features, ETC.",
                    "label": 0
                },
                {
                    "sent": "For fairness, and for comparison for matching our experimental settings, I'm going to plot each methods performance as a function of the number of views.",
                    "label": 0
                },
                {
                    "sent": "So first a passive neural network that's trained only for single snapshot recognition on the Sun 360 task gets to only about 40%.",
                    "label": 1
                },
                {
                    "sent": "Or trans information baseline based on Sheila at all, gets to about 45% after 5 views.",
                    "label": 0
                },
                {
                    "sent": "The sequential decision process baseline of denzler at all gets to about 47% and the combination of these two prior baseline gets baselines gets to about 48%.",
                    "label": 0
                },
                {
                    "sent": "Now compare this with our method, which gets to about 65% after five years, which is a huge gain over these prior methods.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we observe similarly large gains on the germs of object manipulation task.",
                    "label": 0
                },
                {
                    "sent": "And find.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Leon, the model net 10 data set.",
                    "label": 0
                },
                {
                    "sent": "We compare our method against two very recent methods.",
                    "label": 0
                },
                {
                    "sent": "The shape Nets method of Wu at all 2015 and the pairwise method of Johns at all 2016.",
                    "label": 0
                },
                {
                    "sent": "Both of these methods use RGB D data, while our method only uses RGB.",
                    "label": 0
                },
                {
                    "sent": "And despite this we find that we strongly outperformed the shape method and perform on par with the pairwise method.",
                    "label": 0
                },
                {
                    "sent": "Now this is despite the fact that the pairwise method not only uses additional depth information, but also uses a very complex training procedure involving training multiple, CNN's one after the other.",
                    "label": 0
                },
                {
                    "sent": "So these are.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "According results and All in all, we take the message that our method strongly outperforms representative traditional approaches across a variety of tasks.",
                    "label": 0
                },
                {
                    "sent": "Alright, so moving on to something.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results let me show you this example of a Plaza courtyard scene.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you see at the top right here is the view that the agent currently observes.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bottom you see the grid views created from the Panorama where the pink square represents the current view and the yellow squares represent potential next views.",
                    "label": 0
                },
                {
                    "sent": "And right at the top, here you see.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The system currently assigns a pretty low probability to this being a Plaza courtyard, which is not very good, but it's first guess.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that this is a restaurant which I would say is a pretty good guess.",
                    "label": 0
                },
                {
                    "sent": "Just going by this view.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now at the next time step, it observes a large number of people looking in the same direction, and it revises opinions and its best guess is now the.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Category is now the theater category.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, I would say this is a pretty good guess just going by this view.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, it does something interesting where it appears to follow the gaze direction of these these viewers in the second time, steppan looks at this brick building and because it this appears to be an outdoor scene, perhaps it revises opinions immediately and converges on Plaza Courtyard, which is the current correct, yes?",
                    "label": 0
                },
                {
                    "sent": "Here's another example from the Germans data set.",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First 2 views of it.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of an object of an object being manipulated by a robot and see since these views are oblique and poorly lit, the best guess of the robot at this stage is close to the correct guess, but it's not quite there.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, the robot manages to manipulate the object to a well lit and face frontal view of the object and gets to guess correctly.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these views are actually quite fun to look at, so I'd encourage you to take a look at more examples in my paper and my supplementary material.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, to sum things up, we proposed a method for end to end learning of an active recognition system which yielded large gains over prior work.",
                    "label": 0
                },
                {
                    "sent": "The system improves still further when we jointly trained it on an unsupervised lookahead task of predicting the effects of its actions, and finally we showed results with experimental settings that are both realistic and complex, but also easily reproducible.",
                    "label": 1
                },
                {
                    "sent": "Data and code will soon be shared on my web page and I really think that this is only the beginning of methods that deal with these kinds of problems, and they will only get more interesting as time goes on.",
                    "label": 0
                },
                {
                    "sent": "So I encourage you to take a look.",
                    "label": 0
                },
                {
                    "sent": "Thank you all for your attention.",
                    "label": 0
                }
            ]
        }
    }
}