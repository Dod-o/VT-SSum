{
    "id": "lo7k7mlmmb5j2pmlrtw5ymi46fdg65ts",
    "title": "Contextual Bandit Algorithms with Supervised Learning Guarantees, incl. discussion by Brendan McMahan",
    "info": {
        "author": [
            "Brendan McMahan, Google, Inc.",
            "Lev Reyzin, Georgia Institute of Technology"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science"
        ]
    },
    "url": "http://videolectures.net/aistats2011_reyzin_bandit/",
    "segmentation": [
        [
            "OK, so let's begin.",
            "So let me give a motivating example for the model that we will be talking about.",
            "For you."
        ],
        [
            "Yahoo for example.",
            "Customers come to Yahoo's website and.",
            "Um?",
            "Yahoo gets to see their queries, their IP addresses, various browser properties."
        ],
        [
            "And Yahoo after seeing these.",
            "Various parameters gives users some result and after seeing whatever the result that Yahoo gives the users the."
        ],
        [
            "There's either click or not, and of course it's not just this guy who comes to Yahoo."
        ],
        [
            "It's also this guy and."
        ],
        [
            "And this other guy."
        ],
        [
            "Generally there are a lot of users and you can kind of abstract away and think about the users giving the advertiser a context and the advertiser taking some action and then the user giving the advertiser or reward, which is kind of a weird way to think about it, but indeed whether the user clicks are not on the content.",
            "In fact rewards search engines."
        ],
        [
            "Yeah.",
            "So let me first formally define the setting will be dealing with."
        ],
        [
            "So this is the contextual bandit setting.",
            "It's a generalization of the normal multi armed bandit setting.",
            "The contextual bandit setting for those of you who know the learning theory literature is also known as bandits with expert advice, and it goes like this.",
            "There are two rounds, K possible actions for.",
            "The learning algorithm or Yahoo to take and policy is in a policy class.",
            "And the policies map context to actions.",
            "And there's a game that happens.",
            "For T rounds you can think of each round as a user coming and your user coming, but for T rounds first the world commits to the rewards of the actions that the learner can take and then lender provides the context.",
            "Based on which the learner can take, make a decision.",
            "Now the learner has policies in its policy class that are recommend choices of actions.",
            "Like which advertisement to display.",
            "So the learner looking at these policies chooses an action and then receives the reward from the world or from the user.",
            "And the idea is we want to compete with following the best policy in hindsight.",
            "So it's not so in the traditional bandit setting.",
            "You just want to compete with taking the best fixed action.",
            "In hindsight, here we want a more difficult goal, and that is to compete with the best policy.",
            "So what is that?"
        ],
        [
            "In well, the reward of an algorithm is just the sum of the rewards it gets.",
            "Overall, the rounds.",
            "Unexpected reward is.",
            "The policy is the sum over, so Pi of X is the policy's choice times the reward.",
            "So, for example, I wrote it like this because it could actually the policy could actually impose a distribution of rewards over actions, but you could think of this pie vector as actually having just a 11 component of the ratios, and then it would just be a deterministic policy.",
            "And the regret is just competing.",
            "So the Max term is the best policy in the class and G of A is.",
            "The reward of the algorithm and the regret is the difference between your reward and the reward.",
            "You could have gotten had you followed the best policy in hindsight."
        ],
        [
            "Let me just add one thing that I should have stressed if I go back one slide and that is the learner receives only the reward of the action it chose to display.",
            "If you, for example have a lot of ads to show to user and you show run, you have no idea whether the user clicked or didn't click on an ad that you didn't show, so you only see the rewards."
        ],
        [
            "Of the actions you've taken."
        ],
        [
            "So the regret is defined as above.",
            "Now we can think about the expected regret of an algorithm, and that is just the best gain.",
            "The best experts are the best policy's gain.",
            "First came a little reward, minus your expected reward.",
            "Or you could think of a high probability bound and that is the difference between the Max and.",
            "The Max gain in your gain is going to be greater than some quantity epsilon with.",
            "So it should be greater than one minus Delta.",
            "But you could think of basically.",
            "The regret.",
            "You can measure regret, not just an expectation, but with high probabilities, and you want to be able to achieve a certain difference between your reward and the best reward with arbitrarily high probability."
        ],
        [
            "Now again, as I've mentioned, this is harder than supervised learning because in supervised learning we used to getting full feedback.",
            "But here we only get the feedback of the actions we've taken, but not of the actions we've not taken.",
            "And again, this is not the traditional K armed bandit setting.",
            "Because we have policies that we want to compete with."
        ],
        [
            "OK, so let me talk about what's known about this setting.",
            "So an algorithm like Exp 4.",
            "Um?",
            "Is is a known algorithm and it?",
            "Can give you regret that square root of Katie log the number of policies.",
            "So K is the number of actions to the number of policies.",
            "So tis the number of rounds and N is the number of policies you're competing with and.",
            "An algorithm like before will give you this regret, but it's not a high probability.",
            "Regret bound is just an expected regret bound, but it is contextual because you see that end there.",
            "The number of policies.",
            "Similarly.",
            "Epsilon greedy, which is one of the best known algorithms, will get you basically tied to the 2/3 order two to the 2/3 regret.",
            "It's high probability and contextual, but if you notice its dependence on T is worse.",
            "Then there's an algorithm Exp three dot P. Or UCB, there are two different algorithms.",
            "They both get you.",
            "Root Katie and they are high probability bounds, but they're not contextual.",
            "They don't compete with sets of policies."
        ],
        [
            "So in this work we're going to get Katie log and over Delta, where Delta is the probability with which we're willing to fail.",
            "It's both a high probability result and a contextual result.",
            "And note that there's also root Katie lower bound or root T log N lower bound, so this is basically the best you can get and that's why we call this optimal.",
            "Um?",
            "Now also we want to lug arhythmic dependence on N. The number of policies because you can imagine the number of policy is being really really huge."
        ],
        [
            "OK, so let me go over some ideas that are kind of the first things you think about, but but fail sometimes in subtle ways."
        ],
        [
            "So the first bad idea, and this is kind of the first thing everyone tries, is, well, let's maintain a set of possible hypothesis.",
            "Or policies and randomize uniformly over their predicted actions.",
            "And once we figured out a policy is bad, will stop considering it.",
            "Even if the data comes in IID, even though we made no assumption about this game.",
            "The context coming in IID, even if.",
            "This is the case."
        ],
        [
            "We could have something where the adversary has two actions, just two actions, one that always pays off one another piece of 0.",
            "Um and the hypothesis agree.",
            "Almost all agree on the correct action, except for one hypothesis.",
            "Defects on each round to the wrong action.",
            "Now, if you're randomizing among the actions of the non illuminated hypothesis, you're going to be choosing the wrong action with probability 1/2.",
            "For awhile until you illuminated everything until you get to the good ones, so that won't work."
        ],
        [
            "The other idea is you can maintain a set of possible hypothesis and just randomize uniformly among the hypothesis."
        ],
        [
            "This fails also because.",
            "If all but one hypothesis always predicts the wrong arm and one hypothesis predicts the right arm.",
            "It will take you awhile to discover which hypothesis it is.",
            "And again, if you fail."
        ],
        [
            "Now, the standard approach in this framework, and this is actually the best known high probability result for the contextual setting is epsilon greedy, an algorithm that's fairly old, and there are variants of it.",
            "Epsilon first epoch, greedy.",
            "The rough idea is you act randomly for epsilon rounds, and otherwise you go with the best action or policy.",
            "And even if you know the number of rounds in advance, it won't get you to the 1/2, which is the optimal learning rate we're looking for.",
            "Even in the non contextual setting, even if you just have to compete with fixed actions and the analysis is really easy even if you just have two arms or actions, these terms are used interchangeably.",
            "Um?",
            "You have to suffer regret well epsilon because that's how much you've been exploring uniformly at random and T over epsilon to the 1/2 because that's the accuracy to which you can measure.",
            "The performance of.",
            "Of an arm, if you only take epsilon samples and if you set epsilon to T to the 2/3, you get the optimal tradeoff and this gets regret T to the 2/3."
        ],
        [
            "OK, so I'm going to talk about what are the components of a good algorithm for this."
        ],
        [
            "Setting.",
            "Now I'll note that all of these components have actually been used in previous algorithms, but not combined into one algorithm.",
            "OK, one is exponential weights, but this is actually something that's present in.",
            "Very old algorithms like weighted majority.",
            "You just keep weights on each expert or policy so expert or policy are also used interchangeably, so you keep a weight on each policy and this weight drops exponentially in the policy is estimated performance.",
            "And you want to sample the policies or follow their recommendations.",
            "Proportional to their weight.",
            "So this basically forces you to put more credence in better policies, but never illuminate anything entirely, which is especially something that you need in adversarial setting Furthermore.",
            "Upper confidence bounds have been used and we use them in this paper in this work, and the idea is you don't just want to keep track of the estimated performance, but you want to.",
            "Keep track of the best policy could have been doing because there's air in your measure.",
            "Register in their measurement, but there's variance.",
            "Um, so you want to give?",
            "Give the weights some slack.",
            "You want to ensure exploration, so you want to make sure that each action that you could take is taken with some minimum probability because otherwise, well, you get into various issues.",
            "First of all, you could never detect a good action because somehow the weight on the experts isn't or in the policies isn't properly distributed.",
            "You could also.",
            "Get into variance problems because if you take an action with very very low probabilities then your variance and estimate is really high and then this goes to importance weighting.",
            "So if you're taking an action very rarely you have to give it more credence when you do take it right.",
            "So if you take an action one in a billion times and you see a reward of a high reward in that one in a billion times, you have to give this estimate.",
            "A lot of credence.",
            "You have to multiply by a billion.",
            "Basically in order to have an unbiased estimate.",
            "Of the reward."
        ],
        [
            "So this is XP4.",
            "So this is not our algorithm, but this is what our algorithm is based on.",
            "And this is a well known algorithm dating back to 95, and there have been various revisions of it.",
            "Now that the idea is actually probably step over here, the idea is that you.",
            "Set probabilities proportional of an accent proportional to this is an indicator of an expert taken an action.",
            "And this is the weight of the experts divided by the sum of the weights.",
            "So you're taking actions proportional to the weight of experts, but you also add in a minimum probability to ensure that each action is taken with some probability.",
            "And this is, you know, to normalize.",
            "And there's a optimal setting for this minimum probability.",
            "Then you draw your action from this probability distribution and then you update your weights basically.",
            "E to the reward.",
            "If the action is taken divided by the probability of having taken the action, that's the importance weighting and do update a policy's weight only if the action you took agreed with that policy's choice, and otherwise.",
            "You don't update anything.",
            "So in some sense, the if I write our algorithm.",
            "To confirm as close to this algorithm as possible, this is the only chain."
        ],
        [
            "And the only change appears here.",
            "Basically, this is a confidence bound.",
            "So one so this basically stays the same, so this is the indicator of having the taken action, and this is the important waiting.",
            "And this is one over the probability of whatever action the policy took.",
            "So for each policy we update its weight and we add in this term, so one over the probability of whatever the policy took times the square root of this quantity and all the next slide will hint at why this is set this way and will call this Y hat, which is the estimate of the reward in V. Had kind of a variance.",
            "And this is algorithm and we call it years before dot P. Pay for high probability."
        ],
        [
            "OK, so.",
            "Well, this analysis is based on two lemmas.",
            "One is basically if jihad is our estimated reward and Sigma had is basically standard deviation term that depends on our variance terms, then our estimate plus this function of Sigma hat.",
            "Is it going to?",
            "Um?",
            "Bound the true.",
            "Performance of a policy.",
            "So basically our estimated performance plus something is going to be bound on.",
            "On the true performance of a policy, and this we just use a Friedman style or Bernstein's inequality for martingales type inequality.",
            "Which is also new, but now."
        ],
        [
            "The thrust of her work in the second member we can prove is that we can bound the reward of Exp 4P.",
            "By this quantity which depends on you had which is the best measured game plus the standard deviation term.",
            "And because lemma one says that this bounds this empirical estimate bounds, the true estimate we can plug in the best policy is reward here and row will be, you know, something times the best policy minus some terms which will form our regret.",
            "And this basically lets us prove the main theorem, which is that.",
            "So we get to substitute gmax the best policy for you hacked, and basically we're off by root Katie log.",
            "Hanover, Delta and I don't have time to go into much more detail, but we can talk about it offline."
        ],
        [
            "So the problem is that this algorithm requires explicit waits on the policy.",
            "As you've noticed, I have to update the weights of every policy as I do this and this is OK for polynomially many policies and it's OK for some special cases, but it's not efficient in general because you see the regret scales as log of the number of policies, so this lets us handle a very rich policy class, but then the running time seems to run into this barrier.",
            "And we want an efficient algorithm that would perhaps have access to an empirical risk minimization Oracle.",
            "This is something that Langford and Django."
        ],
        [
            "But we don't have this.",
            "So in fact if we add another column.",
            "Whether algorithm is efficient, we see a problem.",
            "No, it's not.",
            "Efficient in the sense that.",
            "We have to be polynomial in and the number of policies.",
            "So because it retains this problem with Exp for well, epsilon, greedy for example can be made efficient assuming some.",
            "Some optimization, so the ability to do some optimization problems."
        ],
        [
            "Now let me quickly talk about how one deals with the VC set."
        ],
        [
            "So what if we have an infinite number of policies?",
            "OK, so we can't keep explicit waits on an infinite number of policies.",
            "Moreover, are bound becomes vacuous right?",
            "Because Loog of Infinity is.",
            "Still large so.",
            "If we assume a policy class has a finite VC dimension, we can still tackle this problem, but we need an idea assumption.",
            "Let's assume for this argument that the number of actions is equal to 2."
        ],
        [
            "Just illustrate the argument so the PC dimension of our hypothesis class captures its expressive power.",
            "It's the cardinality of the largest set that this class can shatter, and it just means to label in all possible config."
        ],
        [
            "Nations.",
            "So the way this algorithm works is you act uniformly.",
            "At random for towel rounds.",
            "Then you partition the policies into equivalence classes that have agreed and all that our rounds according to their predictions.",
            "Then you pick one representative from each equivalence class.",
            "This gives you Pi.",
            "What did I say, prime?",
            "BI Prime and then you run XP4P on \u03c0 prime and this gives you this algorithm we call VE.",
            "We all agreed in the acronym, but I don't remember if we agreed on what the acronym stood for."
        ],
        [
            "OK, well it's something in the paper.",
            "OK, so Sauer's lemma is a famous number.",
            "The balance, the number of equivalence classes you can have if you have small VC dimension.",
            "So the idea is that if your class has low VC dimension that has low expressive power and therefore if you take some number of samples it can't label them in too many ways, and in fact it's etal divided by D to the power of D. So these regret is going to be.",
            "Tao, because, well, you're sampling at random, so you're going to lose out there.",
            "Plus this term, which is just the regret from Exp for P. And then you can relate Pi Prime to like the regret of \u03c0 prime 2\u03c0 by saying that, well, look if we agreed.",
            "Um?",
            "For Tau steps, as in we formed these equivalence classes, what's the probability will disagree in the future, and that turns out to be pretty low.",
            "It's a very standard argument, but it gives a Route TD regret where D is the VC dimension.",
            "Of course, if you have more actions, then you get a K in there, so it's root KTD, which which is expected because of the root KT lower bound.",
            "But it's so inefficient.",
            "And that's because the class is the number of equivalence classes is very large."
        ],
        [
            "So let me talk about experiments we've done.",
            "With this algorithm."
        ],
        [
            "So well, the problem is of course we have to choose.",
            "A policy class for which we can efficiently keep track of the weights right, but we still want it to be contextual, so we went to large number of policy, so Luckily there are special cases where we can do this.",
            "So we created 5 clusters with users getting features based on their distances to the cluster.",
            "So we try to assign users to clusters then policy's map.",
            "What country and you're into article choices or action choices?",
            "And we run this on Yahoo's personalized news article recommendation for Yahoo Front page.",
            "Um?",
            "Now the setup we use is kind of a little different than the intrinsic setup for X.",
            "Before you would think you could just run this on a large data set, but that's not usually how these algorithms are implemented in practice, and I can get into some of the reasons why.",
            "So we use the learning bucket in which we ran the algorithms and the deployment bucket in which we ran their best policy."
        ],
        [
            "And these are estimated clickthrough rates.",
            "And they are normalized.",
            "So it's the ratio about how well our algorithms did compare to a random policy, and the reason we have to normalize us to protect anonymity of Yahoo data.",
            "We did this over 41 million.",
            "Or the privacy of Yahoo data?",
            "I don't know.",
            "I guess there's some data that that shouldn't be released exactly how many clicks are getting so?",
            "So we used 41 million visits, 250 three articles.",
            "That's a number of actions, so 41 million is your T 21 candidate articles per visit.",
            "Um?",
            "And so there's a learning bucket where we're learning and the deployment bucket where we're actually deploying the algorithm and the result of the algorithms learning choice.",
            "And, well, it's actually a very weird comparison right?",
            "Because Exp four and Exp four P. Their expected regret is very similar.",
            "It's just that XP4P has this high probability bound, right?",
            "So in essence, it's very hard to compare, but it turns out that Exp.",
            "Four P does.",
            "Quite a little better in deployment, even though it does a little worse in learning and this difference is even more accentuated with epsilon greedy.",
            "And I.",
            "In some sense you could see why because?",
            "Because we had this confidence bound.",
            "The the exploration.",
            "It's a software exploration so so as in we're not as focused on getting to the best algorithm.",
            "So you could imagine in learning you suffer because of this, but in deployment you win because of this and this is very accentuated by epsilon greedy which is fairly greedy.",
            "Which is fairly gradient in learning, but then doesn't do as well deploy."
        ],
        [
            "And so, in summary, I've described the 1st.",
            "High probability optimal algorithm for contextual bandit problem.",
            "Now why is it that I said in the title?",
            "Bandit algorithms with supervised learning guarantees well.",
            "It's not that the regret learning rate is the same, it's that we can be optimal, which is what we can be in supervised learning.",
            "We showed how to compete with the VC set.",
            "We showed experimental evidence of the effectiveness of our algorithm.",
            "The main drawback is efficiency.",
            "In fact, we have another well.",
            "Some subset of Co authors plus some other cultures have a different paper in here that was supposed to yesterday that focus, for example in linear.",
            "A nonlinear and linear policies and and there we can actually do opt be optimal and be efficient.",
            "So first or the OR for the algorithms we ran in our experiments.",
            "But in general efficiency is a big problem and the main open problem is to find an efficient optimal algorithm for this problem.",
            "And we've made quite a bit of progress on this, and that's all I'll say because I want you to go to John Langford stock at Snowbird.",
            "And that's it.",
            "Thank you."
        ],
        [
            "But since it was on contextual bandits, a good place to begin would be to try to put this work in a little bit of additional context, because we should all believe that context matters now.",
            "So I want to kind of think about a spectrum of formulations for learning problems and kind of on one end of this spectrum, you have supervised learning, so we've got a limited framework with a focus very much on prediction.",
            "You know, it could be classification.",
            "It could be regression, but the goal is to make accurate predictions.",
            "You pick a loss function that does that, so we've got a ton of tractable algorithms in this setting.",
            "It's been applied very successfully to many large scale real world problems.",
            "Kind of on the other end of the spectrum, I might put reinforcement learning and various variants thereof, so it's a very general framework right?",
            "The focus now is actually kind of an acting.",
            "Optimally, you can handle a bunch of kind of models of the world that don't really fit into supervised learning, in particular in particular stateful problems, problems with limited feedback, limited observations.",
            "There's been a lot of great work in that field, but for very large scale problems a lot of approaches there aren't tractable, and they may not even be appropriate because the level of generality might be much greater."
        ],
        [
            "You actually need for your particular particular problem, so I view contextual bandits.",
            "Is trying to kind of be somewhere on the spectrum in between these two, so it's maybe a richer formulation.",
            "It's focused more on decision making than supervised learning, but it doesn't have some of the same challenges that you need to address when you're going to tackle full reinforcement learning.",
            "So in particular, let's you optimize directly for making good decisions, not for prediction accuracy.",
            "You know if I care about money and somebody tells me this cost me $5, it's hard to translate that back to.",
            "Area under the RC curve or squared error, right?",
            "I'd like to be able to work with $5 and it also handles this partial feedback problem that you only get feedback about the actions you take."
        ],
        [
            "OK, so leverage did a good job of talking about kind of specific applications where contextual bandits work well.",
            "I wanted to just highlight a couple of points about what kind of domains these algorithms are particularly appropriate for and the first is you really want to have large quantities of data and many rounds of interaction, right?",
            "That kind of gives you the ability to do some useful learning here.",
            "The other place where I think applications are really promising is any where you have to make decisions in an automated fashion.",
            "A lot of statistics is about helping human decision makers, so you do some statistics.",
            "You can do kind of some fairly fine grained things, some things that really require interpretation an you have a person in the loop that then integrates all of that data and makes the decision an if you can afford to do that, we're not ready to replace human decision makers yet, but there's these applications where you need to make decisions.",
            "Extremely rapid fire like you know, serving ads on.google.com or a lot of website optimization where."
        ],
        [
            "You have to take the human out of the decision-making loop, and in that case you really do want to use a formulation that explicitly optimizes for decision making.",
            "Two kind of general approaches to this, and again left talked about this some.",
            "You can have kind of structure on the policy for taking actions that kind of gets at the efficiency thing, but you have to assume structure then the other class of approaches kind of don't assume anything about the policies, but the downside is you have to kind of enumerate the policy's and track how well each."
        ],
        [
            "One is doing.",
            "So if we think about the contributions of this paper, I kind of break them down into four contributions, the Middle 2 here, the being able to deal with these infinite policy classes that have finite VC dimension and also the example they have where they show how you can implement efficiently implement their algorithm, even though kind of in a naive implementation, you would have exponentially many experts.",
            "Both of those can be seen as kind of bridging.",
            "The gap between these two.",
            "You can take an algorithm that's really in this enumeration setting, and then if you have structure you can go from that.",
            "Back to something that lets you handle very large classes of policy."
        ],
        [
            "These and then the one other thing I wanted to talk a little bit about our experiments.",
            "I think it's still an open question about what's the right way to do empirical evaluation of contextual bandits algorithms.",
            "There's a number of problems that come up that don't really come up when you're doing supervised learning and supervised learning.",
            "We know how to do this right.",
            "You get your algorithm.",
            "You've got a whole battery of datasets you just chug through them and you look at standard accuracy metrics in the bandit setting.",
            "It's not even clear what a data set means.",
            "Because if it really is a real world bandit problem, you only got to take one action and so you don't know what would have happened if you take in a different action.",
            "So how do you evaluate an algorithm that might have done something different than the algorithm you actually ran in the real world to generate this data set?",
            "You can do some kind of statistical tricks to get around that, but it's you know it adds a level of complexity.",
            "There's also a practical constraint here.",
            "A lot of these applications, because they're very important.",
            "You know companies like Google and Yahoo Control a lot of data, and it can be difficult to get.",
            "Data released, I mean, I'm at Google and I'd like to see more data come out, but there's definitely there's definitely challenges with that.",
            "And then the other thing is, there's with all of these algorithms, there's going to be issues with tuning parameters, and again with things like cross validation and so on, we have ways of kind of dealing with parameter tuning and supervised learning.",
            "I don't think we have is mature of techniques in the contextual bandit setting, so I guess the.",
            "To conclude, I think this is a really nice paper.",
            "It makes some strong progress on it on a number of different parts of.",
            "The issue of making contextual bandits really applicable to large real world problems, but I think there's a lot of interesting work still to be to be done in this area.",
            "I think it's still really just getting started, so I'm excited to see I guess what other stuff you guys have been doing and John has been doing and hopefully all of you will get more interested in this as well."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's begin.",
                    "label": 0
                },
                {
                    "sent": "So let me give a motivating example for the model that we will be talking about.",
                    "label": 0
                },
                {
                    "sent": "For you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yahoo for example.",
                    "label": 0
                },
                {
                    "sent": "Customers come to Yahoo's website and.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yahoo gets to see their queries, their IP addresses, various browser properties.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Yahoo after seeing these.",
                    "label": 0
                },
                {
                    "sent": "Various parameters gives users some result and after seeing whatever the result that Yahoo gives the users the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's either click or not, and of course it's not just this guy who comes to Yahoo.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's also this guy and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this other guy.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generally there are a lot of users and you can kind of abstract away and think about the users giving the advertiser a context and the advertiser taking some action and then the user giving the advertiser or reward, which is kind of a weird way to think about it, but indeed whether the user clicks are not on the content.",
                    "label": 0
                },
                {
                    "sent": "In fact rewards search engines.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So let me first formally define the setting will be dealing with.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the contextual bandit setting.",
                    "label": 0
                },
                {
                    "sent": "It's a generalization of the normal multi armed bandit setting.",
                    "label": 0
                },
                {
                    "sent": "The contextual bandit setting for those of you who know the learning theory literature is also known as bandits with expert advice, and it goes like this.",
                    "label": 0
                },
                {
                    "sent": "There are two rounds, K possible actions for.",
                    "label": 1
                },
                {
                    "sent": "The learning algorithm or Yahoo to take and policy is in a policy class.",
                    "label": 0
                },
                {
                    "sent": "And the policies map context to actions.",
                    "label": 0
                },
                {
                    "sent": "And there's a game that happens.",
                    "label": 0
                },
                {
                    "sent": "For T rounds you can think of each round as a user coming and your user coming, but for T rounds first the world commits to the rewards of the actions that the learner can take and then lender provides the context.",
                    "label": 0
                },
                {
                    "sent": "Based on which the learner can take, make a decision.",
                    "label": 0
                },
                {
                    "sent": "Now the learner has policies in its policy class that are recommend choices of actions.",
                    "label": 0
                },
                {
                    "sent": "Like which advertisement to display.",
                    "label": 0
                },
                {
                    "sent": "So the learner looking at these policies chooses an action and then receives the reward from the world or from the user.",
                    "label": 0
                },
                {
                    "sent": "And the idea is we want to compete with following the best policy in hindsight.",
                    "label": 1
                },
                {
                    "sent": "So it's not so in the traditional bandit setting.",
                    "label": 0
                },
                {
                    "sent": "You just want to compete with taking the best fixed action.",
                    "label": 0
                },
                {
                    "sent": "In hindsight, here we want a more difficult goal, and that is to compete with the best policy.",
                    "label": 0
                },
                {
                    "sent": "So what is that?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In well, the reward of an algorithm is just the sum of the rewards it gets.",
                    "label": 1
                },
                {
                    "sent": "Overall, the rounds.",
                    "label": 0
                },
                {
                    "sent": "Unexpected reward is.",
                    "label": 0
                },
                {
                    "sent": "The policy is the sum over, so Pi of X is the policy's choice times the reward.",
                    "label": 0
                },
                {
                    "sent": "So, for example, I wrote it like this because it could actually the policy could actually impose a distribution of rewards over actions, but you could think of this pie vector as actually having just a 11 component of the ratios, and then it would just be a deterministic policy.",
                    "label": 0
                },
                {
                    "sent": "And the regret is just competing.",
                    "label": 0
                },
                {
                    "sent": "So the Max term is the best policy in the class and G of A is.",
                    "label": 1
                },
                {
                    "sent": "The reward of the algorithm and the regret is the difference between your reward and the reward.",
                    "label": 0
                },
                {
                    "sent": "You could have gotten had you followed the best policy in hindsight.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just add one thing that I should have stressed if I go back one slide and that is the learner receives only the reward of the action it chose to display.",
                    "label": 0
                },
                {
                    "sent": "If you, for example have a lot of ads to show to user and you show run, you have no idea whether the user clicked or didn't click on an ad that you didn't show, so you only see the rewards.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the actions you've taken.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the regret is defined as above.",
                    "label": 0
                },
                {
                    "sent": "Now we can think about the expected regret of an algorithm, and that is just the best gain.",
                    "label": 1
                },
                {
                    "sent": "The best experts are the best policy's gain.",
                    "label": 0
                },
                {
                    "sent": "First came a little reward, minus your expected reward.",
                    "label": 1
                },
                {
                    "sent": "Or you could think of a high probability bound and that is the difference between the Max and.",
                    "label": 0
                },
                {
                    "sent": "The Max gain in your gain is going to be greater than some quantity epsilon with.",
                    "label": 0
                },
                {
                    "sent": "So it should be greater than one minus Delta.",
                    "label": 0
                },
                {
                    "sent": "But you could think of basically.",
                    "label": 0
                },
                {
                    "sent": "The regret.",
                    "label": 0
                },
                {
                    "sent": "You can measure regret, not just an expectation, but with high probabilities, and you want to be able to achieve a certain difference between your reward and the best reward with arbitrarily high probability.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now again, as I've mentioned, this is harder than supervised learning because in supervised learning we used to getting full feedback.",
                    "label": 1
                },
                {
                    "sent": "But here we only get the feedback of the actions we've taken, but not of the actions we've not taken.",
                    "label": 1
                },
                {
                    "sent": "And again, this is not the traditional K armed bandit setting.",
                    "label": 1
                },
                {
                    "sent": "Because we have policies that we want to compete with.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me talk about what's known about this setting.",
                    "label": 0
                },
                {
                    "sent": "So an algorithm like Exp 4.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Is is a known algorithm and it?",
                    "label": 0
                },
                {
                    "sent": "Can give you regret that square root of Katie log the number of policies.",
                    "label": 0
                },
                {
                    "sent": "So K is the number of actions to the number of policies.",
                    "label": 0
                },
                {
                    "sent": "So tis the number of rounds and N is the number of policies you're competing with and.",
                    "label": 0
                },
                {
                    "sent": "An algorithm like before will give you this regret, but it's not a high probability.",
                    "label": 0
                },
                {
                    "sent": "Regret bound is just an expected regret bound, but it is contextual because you see that end there.",
                    "label": 0
                },
                {
                    "sent": "The number of policies.",
                    "label": 0
                },
                {
                    "sent": "Similarly.",
                    "label": 0
                },
                {
                    "sent": "Epsilon greedy, which is one of the best known algorithms, will get you basically tied to the 2/3 order two to the 2/3 regret.",
                    "label": 0
                },
                {
                    "sent": "It's high probability and contextual, but if you notice its dependence on T is worse.",
                    "label": 0
                },
                {
                    "sent": "Then there's an algorithm Exp three dot P. Or UCB, there are two different algorithms.",
                    "label": 0
                },
                {
                    "sent": "They both get you.",
                    "label": 0
                },
                {
                    "sent": "Root Katie and they are high probability bounds, but they're not contextual.",
                    "label": 0
                },
                {
                    "sent": "They don't compete with sets of policies.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this work we're going to get Katie log and over Delta, where Delta is the probability with which we're willing to fail.",
                    "label": 0
                },
                {
                    "sent": "It's both a high probability result and a contextual result.",
                    "label": 0
                },
                {
                    "sent": "And note that there's also root Katie lower bound or root T log N lower bound, so this is basically the best you can get and that's why we call this optimal.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now also we want to lug arhythmic dependence on N. The number of policies because you can imagine the number of policy is being really really huge.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me go over some ideas that are kind of the first things you think about, but but fail sometimes in subtle ways.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first bad idea, and this is kind of the first thing everyone tries, is, well, let's maintain a set of possible hypothesis.",
                    "label": 1
                },
                {
                    "sent": "Or policies and randomize uniformly over their predicted actions.",
                    "label": 1
                },
                {
                    "sent": "And once we figured out a policy is bad, will stop considering it.",
                    "label": 0
                },
                {
                    "sent": "Even if the data comes in IID, even though we made no assumption about this game.",
                    "label": 0
                },
                {
                    "sent": "The context coming in IID, even if.",
                    "label": 0
                },
                {
                    "sent": "This is the case.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We could have something where the adversary has two actions, just two actions, one that always pays off one another piece of 0.",
                    "label": 1
                },
                {
                    "sent": "Um and the hypothesis agree.",
                    "label": 1
                },
                {
                    "sent": "Almost all agree on the correct action, except for one hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Defects on each round to the wrong action.",
                    "label": 0
                },
                {
                    "sent": "Now, if you're randomizing among the actions of the non illuminated hypothesis, you're going to be choosing the wrong action with probability 1/2.",
                    "label": 0
                },
                {
                    "sent": "For awhile until you illuminated everything until you get to the good ones, so that won't work.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other idea is you can maintain a set of possible hypothesis and just randomize uniformly among the hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This fails also because.",
                    "label": 0
                },
                {
                    "sent": "If all but one hypothesis always predicts the wrong arm and one hypothesis predicts the right arm.",
                    "label": 1
                },
                {
                    "sent": "It will take you awhile to discover which hypothesis it is.",
                    "label": 0
                },
                {
                    "sent": "And again, if you fail.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, the standard approach in this framework, and this is actually the best known high probability result for the contextual setting is epsilon greedy, an algorithm that's fairly old, and there are variants of it.",
                    "label": 0
                },
                {
                    "sent": "Epsilon first epoch, greedy.",
                    "label": 0
                },
                {
                    "sent": "The rough idea is you act randomly for epsilon rounds, and otherwise you go with the best action or policy.",
                    "label": 1
                },
                {
                    "sent": "And even if you know the number of rounds in advance, it won't get you to the 1/2, which is the optimal learning rate we're looking for.",
                    "label": 1
                },
                {
                    "sent": "Even in the non contextual setting, even if you just have to compete with fixed actions and the analysis is really easy even if you just have two arms or actions, these terms are used interchangeably.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You have to suffer regret well epsilon because that's how much you've been exploring uniformly at random and T over epsilon to the 1/2 because that's the accuracy to which you can measure.",
                    "label": 0
                },
                {
                    "sent": "The performance of.",
                    "label": 0
                },
                {
                    "sent": "Of an arm, if you only take epsilon samples and if you set epsilon to T to the 2/3, you get the optimal tradeoff and this gets regret T to the 2/3.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk about what are the components of a good algorithm for this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Setting.",
                    "label": 0
                },
                {
                    "sent": "Now I'll note that all of these components have actually been used in previous algorithms, but not combined into one algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, one is exponential weights, but this is actually something that's present in.",
                    "label": 0
                },
                {
                    "sent": "Very old algorithms like weighted majority.",
                    "label": 0
                },
                {
                    "sent": "You just keep weights on each expert or policy so expert or policy are also used interchangeably, so you keep a weight on each policy and this weight drops exponentially in the policy is estimated performance.",
                    "label": 1
                },
                {
                    "sent": "And you want to sample the policies or follow their recommendations.",
                    "label": 0
                },
                {
                    "sent": "Proportional to their weight.",
                    "label": 0
                },
                {
                    "sent": "So this basically forces you to put more credence in better policies, but never illuminate anything entirely, which is especially something that you need in adversarial setting Furthermore.",
                    "label": 0
                },
                {
                    "sent": "Upper confidence bounds have been used and we use them in this paper in this work, and the idea is you don't just want to keep track of the estimated performance, but you want to.",
                    "label": 0
                },
                {
                    "sent": "Keep track of the best policy could have been doing because there's air in your measure.",
                    "label": 0
                },
                {
                    "sent": "Register in their measurement, but there's variance.",
                    "label": 0
                },
                {
                    "sent": "Um, so you want to give?",
                    "label": 0
                },
                {
                    "sent": "Give the weights some slack.",
                    "label": 1
                },
                {
                    "sent": "You want to ensure exploration, so you want to make sure that each action that you could take is taken with some minimum probability because otherwise, well, you get into various issues.",
                    "label": 0
                },
                {
                    "sent": "First of all, you could never detect a good action because somehow the weight on the experts isn't or in the policies isn't properly distributed.",
                    "label": 0
                },
                {
                    "sent": "You could also.",
                    "label": 0
                },
                {
                    "sent": "Get into variance problems because if you take an action with very very low probabilities then your variance and estimate is really high and then this goes to importance weighting.",
                    "label": 0
                },
                {
                    "sent": "So if you're taking an action very rarely you have to give it more credence when you do take it right.",
                    "label": 0
                },
                {
                    "sent": "So if you take an action one in a billion times and you see a reward of a high reward in that one in a billion times, you have to give this estimate.",
                    "label": 0
                },
                {
                    "sent": "A lot of credence.",
                    "label": 0
                },
                {
                    "sent": "You have to multiply by a billion.",
                    "label": 0
                },
                {
                    "sent": "Basically in order to have an unbiased estimate.",
                    "label": 0
                },
                {
                    "sent": "Of the reward.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is XP4.",
                    "label": 0
                },
                {
                    "sent": "So this is not our algorithm, but this is what our algorithm is based on.",
                    "label": 0
                },
                {
                    "sent": "And this is a well known algorithm dating back to 95, and there have been various revisions of it.",
                    "label": 0
                },
                {
                    "sent": "Now that the idea is actually probably step over here, the idea is that you.",
                    "label": 0
                },
                {
                    "sent": "Set probabilities proportional of an accent proportional to this is an indicator of an expert taken an action.",
                    "label": 0
                },
                {
                    "sent": "And this is the weight of the experts divided by the sum of the weights.",
                    "label": 0
                },
                {
                    "sent": "So you're taking actions proportional to the weight of experts, but you also add in a minimum probability to ensure that each action is taken with some probability.",
                    "label": 0
                },
                {
                    "sent": "And this is, you know, to normalize.",
                    "label": 0
                },
                {
                    "sent": "And there's a optimal setting for this minimum probability.",
                    "label": 0
                },
                {
                    "sent": "Then you draw your action from this probability distribution and then you update your weights basically.",
                    "label": 0
                },
                {
                    "sent": "E to the reward.",
                    "label": 0
                },
                {
                    "sent": "If the action is taken divided by the probability of having taken the action, that's the importance weighting and do update a policy's weight only if the action you took agreed with that policy's choice, and otherwise.",
                    "label": 0
                },
                {
                    "sent": "You don't update anything.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, the if I write our algorithm.",
                    "label": 0
                },
                {
                    "sent": "To confirm as close to this algorithm as possible, this is the only chain.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the only change appears here.",
                    "label": 0
                },
                {
                    "sent": "Basically, this is a confidence bound.",
                    "label": 0
                },
                {
                    "sent": "So one so this basically stays the same, so this is the indicator of having the taken action, and this is the important waiting.",
                    "label": 0
                },
                {
                    "sent": "And this is one over the probability of whatever action the policy took.",
                    "label": 0
                },
                {
                    "sent": "So for each policy we update its weight and we add in this term, so one over the probability of whatever the policy took times the square root of this quantity and all the next slide will hint at why this is set this way and will call this Y hat, which is the estimate of the reward in V. Had kind of a variance.",
                    "label": 0
                },
                {
                    "sent": "And this is algorithm and we call it years before dot P. Pay for high probability.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Well, this analysis is based on two lemmas.",
                    "label": 0
                },
                {
                    "sent": "One is basically if jihad is our estimated reward and Sigma had is basically standard deviation term that depends on our variance terms, then our estimate plus this function of Sigma hat.",
                    "label": 0
                },
                {
                    "sent": "Is it going to?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Bound the true.",
                    "label": 0
                },
                {
                    "sent": "Performance of a policy.",
                    "label": 0
                },
                {
                    "sent": "So basically our estimated performance plus something is going to be bound on.",
                    "label": 0
                },
                {
                    "sent": "On the true performance of a policy, and this we just use a Friedman style or Bernstein's inequality for martingales type inequality.",
                    "label": 0
                },
                {
                    "sent": "Which is also new, but now.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The thrust of her work in the second member we can prove is that we can bound the reward of Exp 4P.",
                    "label": 0
                },
                {
                    "sent": "By this quantity which depends on you had which is the best measured game plus the standard deviation term.",
                    "label": 0
                },
                {
                    "sent": "And because lemma one says that this bounds this empirical estimate bounds, the true estimate we can plug in the best policy is reward here and row will be, you know, something times the best policy minus some terms which will form our regret.",
                    "label": 0
                },
                {
                    "sent": "And this basically lets us prove the main theorem, which is that.",
                    "label": 0
                },
                {
                    "sent": "So we get to substitute gmax the best policy for you hacked, and basically we're off by root Katie log.",
                    "label": 0
                },
                {
                    "sent": "Hanover, Delta and I don't have time to go into much more detail, but we can talk about it offline.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem is that this algorithm requires explicit waits on the policy.",
                    "label": 1
                },
                {
                    "sent": "As you've noticed, I have to update the weights of every policy as I do this and this is OK for polynomially many policies and it's OK for some special cases, but it's not efficient in general because you see the regret scales as log of the number of policies, so this lets us handle a very rich policy class, but then the running time seems to run into this barrier.",
                    "label": 0
                },
                {
                    "sent": "And we want an efficient algorithm that would perhaps have access to an empirical risk minimization Oracle.",
                    "label": 1
                },
                {
                    "sent": "This is something that Langford and Django.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we don't have this.",
                    "label": 0
                },
                {
                    "sent": "So in fact if we add another column.",
                    "label": 0
                },
                {
                    "sent": "Whether algorithm is efficient, we see a problem.",
                    "label": 0
                },
                {
                    "sent": "No, it's not.",
                    "label": 0
                },
                {
                    "sent": "Efficient in the sense that.",
                    "label": 0
                },
                {
                    "sent": "We have to be polynomial in and the number of policies.",
                    "label": 0
                },
                {
                    "sent": "So because it retains this problem with Exp for well, epsilon, greedy for example can be made efficient assuming some.",
                    "label": 0
                },
                {
                    "sent": "Some optimization, so the ability to do some optimization problems.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let me quickly talk about how one deals with the VC set.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what if we have an infinite number of policies?",
                    "label": 1
                },
                {
                    "sent": "OK, so we can't keep explicit waits on an infinite number of policies.",
                    "label": 0
                },
                {
                    "sent": "Moreover, are bound becomes vacuous right?",
                    "label": 0
                },
                {
                    "sent": "Because Loog of Infinity is.",
                    "label": 0
                },
                {
                    "sent": "Still large so.",
                    "label": 0
                },
                {
                    "sent": "If we assume a policy class has a finite VC dimension, we can still tackle this problem, but we need an idea assumption.",
                    "label": 1
                },
                {
                    "sent": "Let's assume for this argument that the number of actions is equal to 2.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just illustrate the argument so the PC dimension of our hypothesis class captures its expressive power.",
                    "label": 0
                },
                {
                    "sent": "It's the cardinality of the largest set that this class can shatter, and it just means to label in all possible config.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nations.",
                    "label": 0
                },
                {
                    "sent": "So the way this algorithm works is you act uniformly.",
                    "label": 0
                },
                {
                    "sent": "At random for towel rounds.",
                    "label": 1
                },
                {
                    "sent": "Then you partition the policies into equivalence classes that have agreed and all that our rounds according to their predictions.",
                    "label": 0
                },
                {
                    "sent": "Then you pick one representative from each equivalence class.",
                    "label": 1
                },
                {
                    "sent": "This gives you Pi.",
                    "label": 0
                },
                {
                    "sent": "What did I say, prime?",
                    "label": 0
                },
                {
                    "sent": "BI Prime and then you run XP4P on \u03c0 prime and this gives you this algorithm we call VE.",
                    "label": 0
                },
                {
                    "sent": "We all agreed in the acronym, but I don't remember if we agreed on what the acronym stood for.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, well it's something in the paper.",
                    "label": 0
                },
                {
                    "sent": "OK, so Sauer's lemma is a famous number.",
                    "label": 1
                },
                {
                    "sent": "The balance, the number of equivalence classes you can have if you have small VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that if your class has low VC dimension that has low expressive power and therefore if you take some number of samples it can't label them in too many ways, and in fact it's etal divided by D to the power of D. So these regret is going to be.",
                    "label": 0
                },
                {
                    "sent": "Tao, because, well, you're sampling at random, so you're going to lose out there.",
                    "label": 0
                },
                {
                    "sent": "Plus this term, which is just the regret from Exp for P. And then you can relate Pi Prime to like the regret of \u03c0 prime 2\u03c0 by saying that, well, look if we agreed.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "For Tau steps, as in we formed these equivalence classes, what's the probability will disagree in the future, and that turns out to be pretty low.",
                    "label": 0
                },
                {
                    "sent": "It's a very standard argument, but it gives a Route TD regret where D is the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you have more actions, then you get a K in there, so it's root KTD, which which is expected because of the root KT lower bound.",
                    "label": 0
                },
                {
                    "sent": "But it's so inefficient.",
                    "label": 0
                },
                {
                    "sent": "And that's because the class is the number of equivalence classes is very large.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me talk about experiments we've done.",
                    "label": 0
                },
                {
                    "sent": "With this algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So well, the problem is of course we have to choose.",
                    "label": 0
                },
                {
                    "sent": "A policy class for which we can efficiently keep track of the weights right, but we still want it to be contextual, so we went to large number of policy, so Luckily there are special cases where we can do this.",
                    "label": 0
                },
                {
                    "sent": "So we created 5 clusters with users getting features based on their distances to the cluster.",
                    "label": 1
                },
                {
                    "sent": "So we try to assign users to clusters then policy's map.",
                    "label": 0
                },
                {
                    "sent": "What country and you're into article choices or action choices?",
                    "label": 1
                },
                {
                    "sent": "And we run this on Yahoo's personalized news article recommendation for Yahoo Front page.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now the setup we use is kind of a little different than the intrinsic setup for X.",
                    "label": 0
                },
                {
                    "sent": "Before you would think you could just run this on a large data set, but that's not usually how these algorithms are implemented in practice, and I can get into some of the reasons why.",
                    "label": 1
                },
                {
                    "sent": "So we use the learning bucket in which we ran the algorithms and the deployment bucket in which we ran their best policy.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these are estimated clickthrough rates.",
                    "label": 0
                },
                {
                    "sent": "And they are normalized.",
                    "label": 0
                },
                {
                    "sent": "So it's the ratio about how well our algorithms did compare to a random policy, and the reason we have to normalize us to protect anonymity of Yahoo data.",
                    "label": 0
                },
                {
                    "sent": "We did this over 41 million.",
                    "label": 0
                },
                {
                    "sent": "Or the privacy of Yahoo data?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I guess there's some data that that shouldn't be released exactly how many clicks are getting so?",
                    "label": 0
                },
                {
                    "sent": "So we used 41 million visits, 250 three articles.",
                    "label": 0
                },
                {
                    "sent": "That's a number of actions, so 41 million is your T 21 candidate articles per visit.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so there's a learning bucket where we're learning and the deployment bucket where we're actually deploying the algorithm and the result of the algorithms learning choice.",
                    "label": 0
                },
                {
                    "sent": "And, well, it's actually a very weird comparison right?",
                    "label": 0
                },
                {
                    "sent": "Because Exp four and Exp four P. Their expected regret is very similar.",
                    "label": 0
                },
                {
                    "sent": "It's just that XP4P has this high probability bound, right?",
                    "label": 0
                },
                {
                    "sent": "So in essence, it's very hard to compare, but it turns out that Exp.",
                    "label": 0
                },
                {
                    "sent": "Four P does.",
                    "label": 0
                },
                {
                    "sent": "Quite a little better in deployment, even though it does a little worse in learning and this difference is even more accentuated with epsilon greedy.",
                    "label": 0
                },
                {
                    "sent": "And I.",
                    "label": 0
                },
                {
                    "sent": "In some sense you could see why because?",
                    "label": 0
                },
                {
                    "sent": "Because we had this confidence bound.",
                    "label": 0
                },
                {
                    "sent": "The the exploration.",
                    "label": 0
                },
                {
                    "sent": "It's a software exploration so so as in we're not as focused on getting to the best algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you could imagine in learning you suffer because of this, but in deployment you win because of this and this is very accentuated by epsilon greedy which is fairly greedy.",
                    "label": 0
                },
                {
                    "sent": "Which is fairly gradient in learning, but then doesn't do as well deploy.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so, in summary, I've described the 1st.",
                    "label": 0
                },
                {
                    "sent": "High probability optimal algorithm for contextual bandit problem.",
                    "label": 1
                },
                {
                    "sent": "Now why is it that I said in the title?",
                    "label": 0
                },
                {
                    "sent": "Bandit algorithms with supervised learning guarantees well.",
                    "label": 0
                },
                {
                    "sent": "It's not that the regret learning rate is the same, it's that we can be optimal, which is what we can be in supervised learning.",
                    "label": 1
                },
                {
                    "sent": "We showed how to compete with the VC set.",
                    "label": 1
                },
                {
                    "sent": "We showed experimental evidence of the effectiveness of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "The main drawback is efficiency.",
                    "label": 0
                },
                {
                    "sent": "In fact, we have another well.",
                    "label": 0
                },
                {
                    "sent": "Some subset of Co authors plus some other cultures have a different paper in here that was supposed to yesterday that focus, for example in linear.",
                    "label": 0
                },
                {
                    "sent": "A nonlinear and linear policies and and there we can actually do opt be optimal and be efficient.",
                    "label": 0
                },
                {
                    "sent": "So first or the OR for the algorithms we ran in our experiments.",
                    "label": 0
                },
                {
                    "sent": "But in general efficiency is a big problem and the main open problem is to find an efficient optimal algorithm for this problem.",
                    "label": 1
                },
                {
                    "sent": "And we've made quite a bit of progress on this, and that's all I'll say because I want you to go to John Langford stock at Snowbird.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But since it was on contextual bandits, a good place to begin would be to try to put this work in a little bit of additional context, because we should all believe that context matters now.",
                    "label": 0
                },
                {
                    "sent": "So I want to kind of think about a spectrum of formulations for learning problems and kind of on one end of this spectrum, you have supervised learning, so we've got a limited framework with a focus very much on prediction.",
                    "label": 0
                },
                {
                    "sent": "You know, it could be classification.",
                    "label": 0
                },
                {
                    "sent": "It could be regression, but the goal is to make accurate predictions.",
                    "label": 0
                },
                {
                    "sent": "You pick a loss function that does that, so we've got a ton of tractable algorithms in this setting.",
                    "label": 0
                },
                {
                    "sent": "It's been applied very successfully to many large scale real world problems.",
                    "label": 1
                },
                {
                    "sent": "Kind of on the other end of the spectrum, I might put reinforcement learning and various variants thereof, so it's a very general framework right?",
                    "label": 1
                },
                {
                    "sent": "The focus now is actually kind of an acting.",
                    "label": 0
                },
                {
                    "sent": "Optimally, you can handle a bunch of kind of models of the world that don't really fit into supervised learning, in particular in particular stateful problems, problems with limited feedback, limited observations.",
                    "label": 1
                },
                {
                    "sent": "There's been a lot of great work in that field, but for very large scale problems a lot of approaches there aren't tractable, and they may not even be appropriate because the level of generality might be much greater.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You actually need for your particular particular problem, so I view contextual bandits.",
                    "label": 1
                },
                {
                    "sent": "Is trying to kind of be somewhere on the spectrum in between these two, so it's maybe a richer formulation.",
                    "label": 0
                },
                {
                    "sent": "It's focused more on decision making than supervised learning, but it doesn't have some of the same challenges that you need to address when you're going to tackle full reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "So in particular, let's you optimize directly for making good decisions, not for prediction accuracy.",
                    "label": 0
                },
                {
                    "sent": "You know if I care about money and somebody tells me this cost me $5, it's hard to translate that back to.",
                    "label": 0
                },
                {
                    "sent": "Area under the RC curve or squared error, right?",
                    "label": 0
                },
                {
                    "sent": "I'd like to be able to work with $5 and it also handles this partial feedback problem that you only get feedback about the actions you take.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so leverage did a good job of talking about kind of specific applications where contextual bandits work well.",
                    "label": 0
                },
                {
                    "sent": "I wanted to just highlight a couple of points about what kind of domains these algorithms are particularly appropriate for and the first is you really want to have large quantities of data and many rounds of interaction, right?",
                    "label": 1
                },
                {
                    "sent": "That kind of gives you the ability to do some useful learning here.",
                    "label": 1
                },
                {
                    "sent": "The other place where I think applications are really promising is any where you have to make decisions in an automated fashion.",
                    "label": 0
                },
                {
                    "sent": "A lot of statistics is about helping human decision makers, so you do some statistics.",
                    "label": 0
                },
                {
                    "sent": "You can do kind of some fairly fine grained things, some things that really require interpretation an you have a person in the loop that then integrates all of that data and makes the decision an if you can afford to do that, we're not ready to replace human decision makers yet, but there's these applications where you need to make decisions.",
                    "label": 0
                },
                {
                    "sent": "Extremely rapid fire like you know, serving ads on.google.com or a lot of website optimization where.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You have to take the human out of the decision-making loop, and in that case you really do want to use a formulation that explicitly optimizes for decision making.",
                    "label": 0
                },
                {
                    "sent": "Two kind of general approaches to this, and again left talked about this some.",
                    "label": 1
                },
                {
                    "sent": "You can have kind of structure on the policy for taking actions that kind of gets at the efficiency thing, but you have to assume structure then the other class of approaches kind of don't assume anything about the policies, but the downside is you have to kind of enumerate the policy's and track how well each.",
                    "label": 1
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One is doing.",
                    "label": 0
                },
                {
                    "sent": "So if we think about the contributions of this paper, I kind of break them down into four contributions, the Middle 2 here, the being able to deal with these infinite policy classes that have finite VC dimension and also the example they have where they show how you can implement efficiently implement their algorithm, even though kind of in a naive implementation, you would have exponentially many experts.",
                    "label": 1
                },
                {
                    "sent": "Both of those can be seen as kind of bridging.",
                    "label": 0
                },
                {
                    "sent": "The gap between these two.",
                    "label": 0
                },
                {
                    "sent": "You can take an algorithm that's really in this enumeration setting, and then if you have structure you can go from that.",
                    "label": 0
                },
                {
                    "sent": "Back to something that lets you handle very large classes of policy.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These and then the one other thing I wanted to talk a little bit about our experiments.",
                    "label": 0
                },
                {
                    "sent": "I think it's still an open question about what's the right way to do empirical evaluation of contextual bandits algorithms.",
                    "label": 0
                },
                {
                    "sent": "There's a number of problems that come up that don't really come up when you're doing supervised learning and supervised learning.",
                    "label": 0
                },
                {
                    "sent": "We know how to do this right.",
                    "label": 0
                },
                {
                    "sent": "You get your algorithm.",
                    "label": 0
                },
                {
                    "sent": "You've got a whole battery of datasets you just chug through them and you look at standard accuracy metrics in the bandit setting.",
                    "label": 0
                },
                {
                    "sent": "It's not even clear what a data set means.",
                    "label": 0
                },
                {
                    "sent": "Because if it really is a real world bandit problem, you only got to take one action and so you don't know what would have happened if you take in a different action.",
                    "label": 0
                },
                {
                    "sent": "So how do you evaluate an algorithm that might have done something different than the algorithm you actually ran in the real world to generate this data set?",
                    "label": 1
                },
                {
                    "sent": "You can do some kind of statistical tricks to get around that, but it's you know it adds a level of complexity.",
                    "label": 0
                },
                {
                    "sent": "There's also a practical constraint here.",
                    "label": 0
                },
                {
                    "sent": "A lot of these applications, because they're very important.",
                    "label": 0
                },
                {
                    "sent": "You know companies like Google and Yahoo Control a lot of data, and it can be difficult to get.",
                    "label": 0
                },
                {
                    "sent": "Data released, I mean, I'm at Google and I'd like to see more data come out, but there's definitely there's definitely challenges with that.",
                    "label": 0
                },
                {
                    "sent": "And then the other thing is, there's with all of these algorithms, there's going to be issues with tuning parameters, and again with things like cross validation and so on, we have ways of kind of dealing with parameter tuning and supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I don't think we have is mature of techniques in the contextual bandit setting, so I guess the.",
                    "label": 0
                },
                {
                    "sent": "To conclude, I think this is a really nice paper.",
                    "label": 0
                },
                {
                    "sent": "It makes some strong progress on it on a number of different parts of.",
                    "label": 0
                },
                {
                    "sent": "The issue of making contextual bandits really applicable to large real world problems, but I think there's a lot of interesting work still to be to be done in this area.",
                    "label": 0
                },
                {
                    "sent": "I think it's still really just getting started, so I'm excited to see I guess what other stuff you guys have been doing and John has been doing and hopefully all of you will get more interested in this as well.",
                    "label": 0
                }
            ]
        }
    }
}