{
    "id": "zgijhn7q7bndihjgo6nwrv76xx44dibn",
    "title": "Identifying Feature Relevance using a Random Forest",
    "info": {
        "author": [
            "Jeremy D. Rogers, University of Southampton"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "February 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Preprocessing",
            "Top->Computer Science->Machine Learning->Ensemble Methods"
        ]
    },
    "url": "http://videolectures.net/slsfs05_rogers_ifrur/",
    "segmentation": [
        [
            "Implication of feature relevance using random forests from some kind of device.",
            "OK, yeah, my name is Jeremy Rogers, one of Steves PhD students, and this is identifying feature relevance using a random forest.",
            "So."
        ],
        [
            "First of all, I'm just going to briefly go over the random forest algorithm.",
            "What it is, how it works.",
            "And also why useful for identifying feature relevance and also why it's an algorithm that benefits from some sort of feature selection scheme?",
            "I'm going to talk a little bit about relevance identification, some general techniques that we could use to identify feature relevance, and also our adopted measure using the random forest and how that ties in with these these general techniques.",
            "They're going to introduce a node complexity compensation.",
            "Which improves our measure of feature relevance.",
            "And I'm going to do some algorithms to actually employ these measures of feature relevance using random forest to improve classification accuracy and then finally I'm going to introduce a feature selection threshold which is applicable with this technique.",
            "OK, so round."
        ],
        [
            "Forest is an ensemble algorithm.",
            "Which uses cart based trees as the base learners.",
            "The combined using bagging so each learners trained on a bag set of the training data is the idea of this is to try and promote diversity within the base learners so that we get a good exploration of the possible hypothesis.",
            "And.",
            "Yeah, typically cuts will."
        ],
        [
            "It will optimize at each node, each split in the tree will find the optimal feature.",
            "And also the optimal split position in terms of information gain, which is basically the reduction in entropy, which is caused by splitting the data using this feature.",
            "So you get some idea of how well it separates the classes.",
            "Now with random forest we actually select these features randomly.",
            "The idea of this is to promote the diversity within the base learners to an even greater extent.",
            "So we get an even better search of possible hypothesis.",
            "Because it does this random selection of these features, you can see that actually forms these random.",
            "Gives a good random exploration of a feature subsets.",
            "So it's lends itself to feature selection very well.",
            "But we've lost the implicit feature selection that we have with the countries, because now we're not choosing the optimal feature which splits.",
            "We're just choosing them randomly.",
            "So you can see if you have a large number of irrelevant features will still be choosing them with a.",
            "A probability is equal to the relevant features.",
            "That makes no distinction between relevant features and irrelevant features, so.",
            "Random forests are really.",
            "His performance is degraded with the presence of these relevant features.",
            "OK, so relevant."
        ],
        [
            "Verification we can use a ranking style technique.",
            "Where we assign the school to each of the features.",
            "Um?",
            "And this can typically be done by analyzing the features individually using some sort of correlations of the target.",
            "And these correlation measures could be a standard linear correlation or.",
            "An information theoretic one, such as information gain gain ratio, symmetrical, uncertain, do things like that.",
            "But this is using the definition of feature relevance given here, where we have a target label Y.",
            "Feature XI would basically saying.",
            "But the feature is relevant if this inequality holds.",
            "So if some knowledge of feature.",
            "XI changes the distribution of the target.",
            "Then we deem it to be relevant.",
            "Now the problem with this is that we're assuming there's no feature interaction.",
            "Extreme case of this is with the parity problem where we have two binary features and the target function is given by the exclusive or appease.",
            "Now if you were to look at one of those features individually.",
            "Yeah, it was actually appear to be completely irrelevant because you need to know the value of the other feature.",
            "Trust me, yeah.",
            "Calculate the value of the target.",
            "OK, we can also use some."
        ],
        [
            "Tell method.",
            "Now with Decision Tree induction you could.",
            "Build a country.",
            "Selecting the feature but optimizing the feature of the splits and then use some sort of stopping criterions such as minimum description, length, principle so that you express the data in its simplest form and then all you need to do is just choose the features that have been selected to construct this tree.",
            "Also, wrapper methods where.",
            "Perform some sort of search through the.",
            "Feature subset space and analyze the goodness of each of these subsets using the learning algorithm.",
            "Or other subset search methods where.",
            "The goodness of each of the subsets is calculated with some function which is.",
            "Ideally computationally less expensive.",
            "OK, identify Michael blankets is another.",
            "Turning people tried to approximate these Markov blankets, which basically if you find a subset which makes the other features conditionally independent in the target, then this is obviously quite a beneficial thing.",
            "Pretty useful feature selection, but now we're using this definition feature relevance where we have still have our label Y.",
            "Son of a label wine feature, Exxon.",
            "Also now subservicing subset of all the features apart from Exxon, and we're now saying that the feature excise relevant if when we lose this feature we lose some information about the target.",
            "Now this if you go back to the interaction problems with parity problem, you find that actually does take into account.",
            "The feature extraction."
        ],
        [
            "OK, so.",
            "What we do is we build a random forest.",
            "And I'll measure feature relevance is the average information gain that's achieved by its feature over the entire fire, so we're splitting.",
            "The root node and partitions of space.",
            "We can split again using another random feature.",
            "This is good because we're now giving some sort of allowance for the feature interaction because we're testing the feature in different areas of the input space.",
            "So if you were to return to the parity problem Now if you were to.",
            "Attempt to split the data on one of the features.",
            "The feature relevance wouldn't be identified, but if you were then to split on the other feature, it would actually reveal the relevance of that feature.",
            "OK, the problem with this method is the reliability of each of the samples of information gain is dependent upon the node composition.",
            "Also, if you have smaller nodes and less data, they're splitting then.",
            "You could actually split it purely by chance, but then larger nodes with more data actually give you more reliable measure of the information gain.",
            "And also the relevant features will still give a non zero measure of relevance because in a worst case for each sample the information gain could be 0.",
            "So the average information is always going to mean of a non negative sample so.",
            "We're still going to have some some non zero major feature relevance for the relevant features."
        ],
        [
            "OK, no complexity compensation.",
            "We said that some nodes are easy to split.",
            "We're taking the average information games achieved by each feature.",
            "What we want to do is is wait it where each sample.",
            "So we have a weighted average.",
            "I want to waste it by some measure of the reliability of these nodes or the complexity.",
            "So what we do is we project the stage down to the one dimensional space.",
            "Is given by the feature and we're using a binary classification problem here with N examples in the node.",
            "I positive examples.",
            "Therefore we have N -- I negative examples and the number of possible arrangements when you project this data down to the one dimensional space is given by the combinatorial function."
        ],
        [
            "The next thing to consider is that some of these arrangements when you project it down to the space is actually that can be non unique such as here where we have.",
            "The node with two examples, one from each class.",
            "All we gotta do is reflect it and we have two possible arrangements, but they're basically the same, and if you to optimize the information gain, optimize the split to maximize information gain, you would actually achieve the same value for both.",
            "But then also, as other arrangements are symmetrical about the center and can be termed unique Now these will give.",
            "There are unique value of the optimized information gain.",
            "But then you also have to consider.",
            "Although these are the same arrangement, the probability of them occurring is doubled because there are two ways.",
            "For this arrangement to occur."
        ],
        [
            "OK, so the way we workout.",
            "The number of unique arrangements are given by these functions here, which is dependent upon whether NRI, odd or even.",
            "If you have an even number of examples in an odd number of positive examples, you can't form an arrangement which is symmetrical about its center, so therefore there are no unique arrangements in that case.",
            "And then using information theory, what we can do is you can say the probability of a unique arrangement is already given by one over the total number of possible arrangements and probability of the non unique arrangement is given by two over the total number of possible arrangements and then we set the negative base two logarithms of that to give us the information and then we wait them by the proportions of unique and non unique.",
            "Arrangements.",
            "And then we can simplify this down.",
            "We got our function here, which is our measure of node complexity for nodel.",
            "OK, wanted."
        ],
        [
            "The test to see how well this measure actually works.",
            "And what we wanted to do is look at the information game density functions for each feature.",
            "So if you imagine your.",
            "Sampling the information gain for each feature of different areas and using different nodes.",
            "And it should give us some sort of distribution.",
            "For these information, games for each of these features.",
            "So what we did is we built a random forest using 500 trees on an artificial data set.",
            "And we recorded each of the information gain values which is.",
            "And then use that to construct identity functions.",
            "And we used it one way using the unit weights for each sample and then another way we actually waited it by our measure of node complexity.",
            "And so this is what we have here."
        ],
        [
            "These are three different features.",
            "The two on the left are.",
            "Are relevant features and the third one is an irrelevant feature.",
            "Now the top plots are basically given by this unit waiting, so this is the normal distribution of the information gains.",
            "And here we have.",
            "When we wait it using our node complexity measure so these spikes that are occurring on the on the right of these plots, there actually caused by these smaller nodes they're being split.",
            "Just purely by chance by each of the features, and we find that we actually suppress these when we are.",
            "When we use our node complexity measure.",
            "So now you can see if you would take the average of these distributions, you can actually find we've got a much.",
            "A much more accurate measure of the feature relevance.",
            "OK, So what can we?"
        ],
        [
            "Do with these measures of feature relevance.",
            "Well, you could use some sort of feature selection scheme where we could.",
            "Rank them in order and select the K best or put.",
            "Some sort of threshold in or we could do some sort of feature waiting where we actually rely on the features to a different extent according to how relevant we think they are.",
            "And with random forest we can do this by altering the feature sampling distribution.",
            "Remember, I've said that if these features are chosen randomly split with equal probability, Now what we can do is we can actually change these probabilities so that more relevant features can be chosen with a greater probability.",
            "This can be done in two ways.",
            "We could do a parallel scheme.",
            "Where we actually built a forest and after each tree we could get some measure of the how relevant features are.",
            "Use that to update the feature sampling distribution.",
            "The problem with this is that in the initial stages of the algorithm you don't have very much information about the features.",
            "And you have to be careful not to.",
            "Overweight some of the features because then the algorithm will be able to recover from that as it proceeds.",
            "The other way we can do it is by using a two stage approach.",
            "Where we analyze the feature relevance.",
            "In the first stage, and then we use that to fix the feature sampling distribution prior to the forest construction.",
            "OK, so parallel scheme."
        ],
        [
            "We're going to do is try and control this update right?",
            "So the peace and precision we chose confidence intervals in this case.",
            "Now we can use this statistic, which is basically the average information game.",
            "Here the.",
            "All theoretical, true mean sample size and sample standard deviation.",
            "It has a students distribution with minus 1 degrees of freedom.",
            "So we can form.",
            "Working at confidence limits.",
            "On this statistic Q1 and Q2.",
            "Which gives us some level of confidence which we can set.",
            "So we use 95% in this case.",
            "And what we can do is rearrange it so we get these confidence bounds on our average information game for each feature.",
            "Now the idea here is that once we form these.",
            "Confidence intervals around each measure of feature relevance.",
            "We then use that to update the feature sampling distribution by maintaining the most uniform distribution.",
            "But that still lies within each of these confidence intervals.",
            "So that way the algorithm proceeds these companies to become smaller and starts alter the feature segmentation to greater extent.",
            "But in the initial stages.",
            "It should end on the side of caution."
        ],
        [
            "OK, so then we get the convergence rates.",
            "These plus these plots are showing the average confidence interval size averaged over all the features for each of the datasets.",
            "Now these convergence rates.",
            "Basically reflects how fast we are converging to our.",
            "Features happens to bution.",
            "And.",
            "They depend on the level of confidence you set, but they also depend on the number of features in the data set and on the average tree size.",
            "So you can see for the sonar data set it's got the slowest convergence because he's got the highest number of features and doesn't build particularly large trees, so not generating that many samples for each of the features and then on the other extreme we've got the Pima data set.",
            "Which converges the fastest simply because it's got the fewest number of features in the field to the largest trees."
        ],
        [
            "OK, So what we did here?",
            "We use 9% of the data training.",
            "We hold back 10% testing each time we build.",
            "100 trees for each of the forests.",
            "We average over 100 trials.",
            "These are the test error rates using the standard random forest.",
            "Using our confidence interval parallel technique and also using a two stage cart technique where we build a single country to begin with.",
            "We don't prune it.",
            "We build it as large as possible.",
            "And because at each stage it's optimizing over all the features, you get an optimal information gain value for each feature at each split in the tree.",
            "So we just build a single tree, take the average information game for that, and then use that to fix the feature sampling distribution and then build the forest from there.",
            "You can see it does perform.",
            "Actually very well in this case, but there's no guarantee on the reliability of this method.",
            "And I can't do math.",
            "You can see is actually.",
            "Particularly, the artificial data sets at the bottom.",
            "There are significantly improving performance and quite importantly is never significantly worse.",
            "So this problem of initially overweighting some of the features has been avoided in this case."
        ],
        [
            "OK, so irrelevant features.",
            "We said that the average information gain is the mean of a non negative sample.",
            "So we get the expected information gain of irrelevant feature is going to have some nonzero value.",
            "Now if you apply this to the feature sampling distribution and you have a high proportion of irrelevant features, although you may be giving them a small probability of being sampled when you actually.",
            "Combine these the.",
            "The total probability of choosing irrelevant feature is actually.",
            "Still significant and will still degrade the performance of random forests.",
            "So what we want to try and do."
        ],
        [
            "Is calculated this expected information gain value for these relevant features?",
            "So we got our parent node.",
            "We got the parent node and.",
            "When you split this, we create left sending rights and.",
            "We can say the NL examples and left descendants.",
            "I'll positive examples and then we know that there are any examples in the parent node.",
            "I positive examples in the parent mode, so from there we can workout.",
            "The corresponding numbers for the right descendant.",
            "Now what we do is for each possible arrangement.",
            "When you project the data, download the one dimensional space for each possible arrangement.",
            "You can try these different split values and they'll give you values for NL&IL and you want to choose the split which basically minimize this expression, which gives you the values of NLI which minimizes this expression.",
            "Reppies the parent entropy.",
            "And.",
            "We tried this for."
        ],
        [
            "Some values.",
            "Yeah, but no competition.",
            "So here we have for each node we got.",
            "This is the number of negative examples in the node number of positive examples.",
            "And so if you were to take a diagonal segment along here, we've got a fixed size N because we're basically just changing the class proportions within that node.",
            "And.",
            "We found that obviously doing in this manner's.",
            "Hugely expensive because you have to build each of the node or node arrangements, optimize the information gain than average.",
            "I roll them.",
            "But we did find that for a fixed value of N, that the highest value of expected information gain actually occurs when the class proportions are equal.",
            "So down using the center line here, we're actually increasing the size of the node, but it's actually half of each class.",
            "The minimum is occurring.",
            "When we have one example from one class.",
            "And all the other examples from the other class.",
            "OK, so."
        ],
        [
            "If we were to just take the value of N. So we just have the node of size.",
            "Then we can actually there's a lower bound is given when we have only one example from one class.",
            "And we can actually.",
            "Calculate this lower bound on the expected information gain of the relevant feature.",
            "We found that we could approximate the upper bound when the class proportions are equal.",
            "Basically, by plotting the the function on a log rhythmic scale, we found that we got an approximately straight line.",
            "And so these then give us the bounds on the expected information gain value for an irrelevant feature.",
            "So it says we just built."
        ],
        [
            "100 trees in artificial data.",
            "Set the two relevant features of features one and two.",
            "And the remaining seven are irrelevant.",
            "These stars represent the.",
            "Average information gain is achieved by each of the features, and then these bounds are bounds on our expected performance of an irrelevant feature.",
            "As you can see, the relevant features are within the bounds where we expect them to be and.",
            "Are two relevant features actually?",
            "Above these mounds."
        ],
        [
            "OK, So what we did then is just to see how well these work.",
            "We chose a feature selection threshold, which is basically the halfway point between these two bounds.",
            "We tried on the Freedmen data set, which is normally a regression, but we thresholded it too.",
            "Crates.",
            "A binary classification problem, which was.",
            "Pretty much equal balanced.",
            "We used our feature selection scheme using our feature selection threshold built 100 trees.",
            "OK, you got the bounds for the expected information gain.",
            "Use the threshold is the halfway point between these bounds and then selected the features which had an average information gain that was above this.",
            "And we found that over the 100 trials.",
            "Or I guess you picked out the five relevant features all the time.",
            "And picked some of the irrelevant features, but some of the time now CFS.",
            "Isn't arguing with competitors correlation based feature selection?",
            "Which uses symmetrical uncertainty to examine the correlation between.",
            "The.",
            "Feature on the class as well as the feature feature correlation to actually analyze redundancy in the data as well.",
            "And this algorithm.",
            "Doesn't choose any of the irrelevant features, but actually drops some of the relevant features as well."
        ],
        [
            "We found for artificial data set.",
            "Again, our algorithm is picking out the two relevant features all the time and still picks some of the irrelevant features some of the time CFS.",
            "Again.",
            "Ignores all of the irrelevant features.",
            "But also ignores one of the relevant features as well.",
            "Professor, if we look at the results for this, we try this again."
        ],
        [
            "It is noted that the date of a training Center for testing.",
            "Build 100 trees to actually analyze.",
            "The performance of each feature and use this to build.",
            "400 specification.",
            "And we have for the CFS algorithm.",
            "This is our feature selection technique.",
            "Also a feature weighting where after we build these hundred trees for evaluating the features, we use that to fix the feature sampling distribution to build the further 100 trees, and then the combination of the two techniques where we select our relevant features.",
            "And we weight them according to their measure of relevance.",
            "And as you can see, CFS.",
            "Doesn't form too well on the author states that particularly.",
            "Particularly this one.",
            "Simply because it's dropping that feature one all the time.",
            "And.",
            "Our room here is certainly doing better.",
            "One of the things CFS actually goes wrong.",
            "The vote states that.",
            "Which actually contains a large amount of redundancy.",
            "Which our algorithm isn't making any allowance for, it's all so our feature selection technique most of the time is just leaving all of the features in, but we still suppress some of the redundant features because they don't appear as as relevant.",
            "As relevant as the.",
            "As the.",
            "Most important features.",
            "And."
        ],
        [
            "OK, so.",
            "What we've done here?",
            "We've introduced this measure of node complexity, so we can actually.",
            "Improve our measure of feature relevance by examining the node composition.",
            "And creating a wait so we can actually wait this averaging process.",
            "We've also given some algorithms.",
            "For altering this feature sample distribution.",
            "The.",
            "Parallel scheme using confidence intervals actually controls the update rate that the feature sampling distribution.",
            "So that we avoid the initial overweighting stage and.",
            "We've also given a feature selection threshold.",
            "So we can actually try and remove some of these relevant features to improve the performance of a random forest.",
            "OK, so.",
            "Yeah, thank you so much.",
            "Questions.",
            "Mutual Information or information gain if you multiply it by the number of instances, it has a high square distribution.",
            "So is there a good reason why you didn't use this in the computation of the confidence intervals?",
            "And the confidence intervals.",
            "You're saying, what do you feel if you compute the confidence interval of mutual information?",
            "You can sort of use this fact that it has the high squared.",
            "Distribution.",
            "OK, I haven't thought about that.",
            "An idea?",
            "Using.",
            "Yeah, so you.",
            "So you're using the T is.",
            "Almost as far as.",
            "Visual information Brian.",
            "Yeah.",
            "Coming.",
            "OK thanks man."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Implication of feature relevance using random forests from some kind of device.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, my name is Jeremy Rogers, one of Steves PhD students, and this is identifying feature relevance using a random forest.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, I'm just going to briefly go over the random forest algorithm.",
                    "label": 0
                },
                {
                    "sent": "What it is, how it works.",
                    "label": 0
                },
                {
                    "sent": "And also why useful for identifying feature relevance and also why it's an algorithm that benefits from some sort of feature selection scheme?",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk a little bit about relevance identification, some general techniques that we could use to identify feature relevance, and also our adopted measure using the random forest and how that ties in with these these general techniques.",
                    "label": 0
                },
                {
                    "sent": "They're going to introduce a node complexity compensation.",
                    "label": 1
                },
                {
                    "sent": "Which improves our measure of feature relevance.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to do some algorithms to actually employ these measures of feature relevance using random forest to improve classification accuracy and then finally I'm going to introduce a feature selection threshold which is applicable with this technique.",
                    "label": 0
                },
                {
                    "sent": "OK, so round.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Forest is an ensemble algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which uses cart based trees as the base learners.",
                    "label": 1
                },
                {
                    "sent": "The combined using bagging so each learners trained on a bag set of the training data is the idea of this is to try and promote diversity within the base learners so that we get a good exploration of the possible hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, typically cuts will.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It will optimize at each node, each split in the tree will find the optimal feature.",
                    "label": 1
                },
                {
                    "sent": "And also the optimal split position in terms of information gain, which is basically the reduction in entropy, which is caused by splitting the data using this feature.",
                    "label": 0
                },
                {
                    "sent": "So you get some idea of how well it separates the classes.",
                    "label": 0
                },
                {
                    "sent": "Now with random forest we actually select these features randomly.",
                    "label": 1
                },
                {
                    "sent": "The idea of this is to promote the diversity within the base learners to an even greater extent.",
                    "label": 1
                },
                {
                    "sent": "So we get an even better search of possible hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Because it does this random selection of these features, you can see that actually forms these random.",
                    "label": 0
                },
                {
                    "sent": "Gives a good random exploration of a feature subsets.",
                    "label": 0
                },
                {
                    "sent": "So it's lends itself to feature selection very well.",
                    "label": 0
                },
                {
                    "sent": "But we've lost the implicit feature selection that we have with the countries, because now we're not choosing the optimal feature which splits.",
                    "label": 1
                },
                {
                    "sent": "We're just choosing them randomly.",
                    "label": 0
                },
                {
                    "sent": "So you can see if you have a large number of irrelevant features will still be choosing them with a.",
                    "label": 0
                },
                {
                    "sent": "A probability is equal to the relevant features.",
                    "label": 0
                },
                {
                    "sent": "That makes no distinction between relevant features and irrelevant features, so.",
                    "label": 0
                },
                {
                    "sent": "Random forests are really.",
                    "label": 0
                },
                {
                    "sent": "His performance is degraded with the presence of these relevant features.",
                    "label": 0
                },
                {
                    "sent": "OK, so relevant.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Verification we can use a ranking style technique.",
                    "label": 0
                },
                {
                    "sent": "Where we assign the school to each of the features.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And this can typically be done by analyzing the features individually using some sort of correlations of the target.",
                    "label": 0
                },
                {
                    "sent": "And these correlation measures could be a standard linear correlation or.",
                    "label": 0
                },
                {
                    "sent": "An information theoretic one, such as information gain gain ratio, symmetrical, uncertain, do things like that.",
                    "label": 0
                },
                {
                    "sent": "But this is using the definition of feature relevance given here, where we have a target label Y.",
                    "label": 0
                },
                {
                    "sent": "Feature XI would basically saying.",
                    "label": 1
                },
                {
                    "sent": "But the feature is relevant if this inequality holds.",
                    "label": 1
                },
                {
                    "sent": "So if some knowledge of feature.",
                    "label": 0
                },
                {
                    "sent": "XI changes the distribution of the target.",
                    "label": 0
                },
                {
                    "sent": "Then we deem it to be relevant.",
                    "label": 1
                },
                {
                    "sent": "Now the problem with this is that we're assuming there's no feature interaction.",
                    "label": 0
                },
                {
                    "sent": "Extreme case of this is with the parity problem where we have two binary features and the target function is given by the exclusive or appease.",
                    "label": 0
                },
                {
                    "sent": "Now if you were to look at one of those features individually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it was actually appear to be completely irrelevant because you need to know the value of the other feature.",
                    "label": 1
                },
                {
                    "sent": "Trust me, yeah.",
                    "label": 0
                },
                {
                    "sent": "Calculate the value of the target.",
                    "label": 0
                },
                {
                    "sent": "OK, we can also use some.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tell method.",
                    "label": 0
                },
                {
                    "sent": "Now with Decision Tree induction you could.",
                    "label": 1
                },
                {
                    "sent": "Build a country.",
                    "label": 0
                },
                {
                    "sent": "Selecting the feature but optimizing the feature of the splits and then use some sort of stopping criterions such as minimum description, length, principle so that you express the data in its simplest form and then all you need to do is just choose the features that have been selected to construct this tree.",
                    "label": 0
                },
                {
                    "sent": "Also, wrapper methods where.",
                    "label": 0
                },
                {
                    "sent": "Perform some sort of search through the.",
                    "label": 0
                },
                {
                    "sent": "Feature subset space and analyze the goodness of each of these subsets using the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Or other subset search methods where.",
                    "label": 1
                },
                {
                    "sent": "The goodness of each of the subsets is calculated with some function which is.",
                    "label": 0
                },
                {
                    "sent": "Ideally computationally less expensive.",
                    "label": 0
                },
                {
                    "sent": "OK, identify Michael blankets is another.",
                    "label": 0
                },
                {
                    "sent": "Turning people tried to approximate these Markov blankets, which basically if you find a subset which makes the other features conditionally independent in the target, then this is obviously quite a beneficial thing.",
                    "label": 0
                },
                {
                    "sent": "Pretty useful feature selection, but now we're using this definition feature relevance where we have still have our label Y.",
                    "label": 0
                },
                {
                    "sent": "Son of a label wine feature, Exxon.",
                    "label": 0
                },
                {
                    "sent": "Also now subservicing subset of all the features apart from Exxon, and we're now saying that the feature excise relevant if when we lose this feature we lose some information about the target.",
                    "label": 0
                },
                {
                    "sent": "Now this if you go back to the interaction problems with parity problem, you find that actually does take into account.",
                    "label": 0
                },
                {
                    "sent": "The feature extraction.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What we do is we build a random forest.",
                    "label": 0
                },
                {
                    "sent": "And I'll measure feature relevance is the average information gain that's achieved by its feature over the entire fire, so we're splitting.",
                    "label": 1
                },
                {
                    "sent": "The root node and partitions of space.",
                    "label": 0
                },
                {
                    "sent": "We can split again using another random feature.",
                    "label": 0
                },
                {
                    "sent": "This is good because we're now giving some sort of allowance for the feature interaction because we're testing the feature in different areas of the input space.",
                    "label": 0
                },
                {
                    "sent": "So if you were to return to the parity problem Now if you were to.",
                    "label": 0
                },
                {
                    "sent": "Attempt to split the data on one of the features.",
                    "label": 0
                },
                {
                    "sent": "The feature relevance wouldn't be identified, but if you were then to split on the other feature, it would actually reveal the relevance of that feature.",
                    "label": 0
                },
                {
                    "sent": "OK, the problem with this method is the reliability of each of the samples of information gain is dependent upon the node composition.",
                    "label": 0
                },
                {
                    "sent": "Also, if you have smaller nodes and less data, they're splitting then.",
                    "label": 0
                },
                {
                    "sent": "You could actually split it purely by chance, but then larger nodes with more data actually give you more reliable measure of the information gain.",
                    "label": 0
                },
                {
                    "sent": "And also the relevant features will still give a non zero measure of relevance because in a worst case for each sample the information gain could be 0.",
                    "label": 0
                },
                {
                    "sent": "So the average information is always going to mean of a non negative sample so.",
                    "label": 0
                },
                {
                    "sent": "We're still going to have some some non zero major feature relevance for the relevant features.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, no complexity compensation.",
                    "label": 0
                },
                {
                    "sent": "We said that some nodes are easy to split.",
                    "label": 1
                },
                {
                    "sent": "We're taking the average information games achieved by each feature.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is is wait it where each sample.",
                    "label": 0
                },
                {
                    "sent": "So we have a weighted average.",
                    "label": 1
                },
                {
                    "sent": "I want to waste it by some measure of the reliability of these nodes or the complexity.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we project the stage down to the one dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Is given by the feature and we're using a binary classification problem here with N examples in the node.",
                    "label": 0
                },
                {
                    "sent": "I positive examples.",
                    "label": 0
                },
                {
                    "sent": "Therefore we have N -- I negative examples and the number of possible arrangements when you project this data down to the one dimensional space is given by the combinatorial function.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next thing to consider is that some of these arrangements when you project it down to the space is actually that can be non unique such as here where we have.",
                    "label": 0
                },
                {
                    "sent": "The node with two examples, one from each class.",
                    "label": 0
                },
                {
                    "sent": "All we gotta do is reflect it and we have two possible arrangements, but they're basically the same, and if you to optimize the information gain, optimize the split to maximize information gain, you would actually achieve the same value for both.",
                    "label": 0
                },
                {
                    "sent": "But then also, as other arrangements are symmetrical about the center and can be termed unique Now these will give.",
                    "label": 1
                },
                {
                    "sent": "There are unique value of the optimized information gain.",
                    "label": 0
                },
                {
                    "sent": "But then you also have to consider.",
                    "label": 0
                },
                {
                    "sent": "Although these are the same arrangement, the probability of them occurring is doubled because there are two ways.",
                    "label": 0
                },
                {
                    "sent": "For this arrangement to occur.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the way we workout.",
                    "label": 0
                },
                {
                    "sent": "The number of unique arrangements are given by these functions here, which is dependent upon whether NRI, odd or even.",
                    "label": 0
                },
                {
                    "sent": "If you have an even number of examples in an odd number of positive examples, you can't form an arrangement which is symmetrical about its center, so therefore there are no unique arrangements in that case.",
                    "label": 0
                },
                {
                    "sent": "And then using information theory, what we can do is you can say the probability of a unique arrangement is already given by one over the total number of possible arrangements and probability of the non unique arrangement is given by two over the total number of possible arrangements and then we set the negative base two logarithms of that to give us the information and then we wait them by the proportions of unique and non unique.",
                    "label": 0
                },
                {
                    "sent": "Arrangements.",
                    "label": 0
                },
                {
                    "sent": "And then we can simplify this down.",
                    "label": 0
                },
                {
                    "sent": "We got our function here, which is our measure of node complexity for nodel.",
                    "label": 0
                },
                {
                    "sent": "OK, wanted.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The test to see how well this measure actually works.",
                    "label": 0
                },
                {
                    "sent": "And what we wanted to do is look at the information game density functions for each feature.",
                    "label": 0
                },
                {
                    "sent": "So if you imagine your.",
                    "label": 0
                },
                {
                    "sent": "Sampling the information gain for each feature of different areas and using different nodes.",
                    "label": 0
                },
                {
                    "sent": "And it should give us some sort of distribution.",
                    "label": 0
                },
                {
                    "sent": "For these information, games for each of these features.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we built a random forest using 500 trees on an artificial data set.",
                    "label": 0
                },
                {
                    "sent": "And we recorded each of the information gain values which is.",
                    "label": 0
                },
                {
                    "sent": "And then use that to construct identity functions.",
                    "label": 0
                },
                {
                    "sent": "And we used it one way using the unit weights for each sample and then another way we actually waited it by our measure of node complexity.",
                    "label": 0
                },
                {
                    "sent": "And so this is what we have here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are three different features.",
                    "label": 0
                },
                {
                    "sent": "The two on the left are.",
                    "label": 0
                },
                {
                    "sent": "Are relevant features and the third one is an irrelevant feature.",
                    "label": 0
                },
                {
                    "sent": "Now the top plots are basically given by this unit waiting, so this is the normal distribution of the information gains.",
                    "label": 0
                },
                {
                    "sent": "And here we have.",
                    "label": 0
                },
                {
                    "sent": "When we wait it using our node complexity measure so these spikes that are occurring on the on the right of these plots, there actually caused by these smaller nodes they're being split.",
                    "label": 0
                },
                {
                    "sent": "Just purely by chance by each of the features, and we find that we actually suppress these when we are.",
                    "label": 0
                },
                {
                    "sent": "When we use our node complexity measure.",
                    "label": 0
                },
                {
                    "sent": "So now you can see if you would take the average of these distributions, you can actually find we've got a much.",
                    "label": 0
                },
                {
                    "sent": "A much more accurate measure of the feature relevance.",
                    "label": 0
                },
                {
                    "sent": "OK, So what can we?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do with these measures of feature relevance.",
                    "label": 0
                },
                {
                    "sent": "Well, you could use some sort of feature selection scheme where we could.",
                    "label": 0
                },
                {
                    "sent": "Rank them in order and select the K best or put.",
                    "label": 0
                },
                {
                    "sent": "Some sort of threshold in or we could do some sort of feature waiting where we actually rely on the features to a different extent according to how relevant we think they are.",
                    "label": 0
                },
                {
                    "sent": "And with random forest we can do this by altering the feature sampling distribution.",
                    "label": 0
                },
                {
                    "sent": "Remember, I've said that if these features are chosen randomly split with equal probability, Now what we can do is we can actually change these probabilities so that more relevant features can be chosen with a greater probability.",
                    "label": 0
                },
                {
                    "sent": "This can be done in two ways.",
                    "label": 1
                },
                {
                    "sent": "We could do a parallel scheme.",
                    "label": 0
                },
                {
                    "sent": "Where we actually built a forest and after each tree we could get some measure of the how relevant features are.",
                    "label": 0
                },
                {
                    "sent": "Use that to update the feature sampling distribution.",
                    "label": 0
                },
                {
                    "sent": "The problem with this is that in the initial stages of the algorithm you don't have very much information about the features.",
                    "label": 0
                },
                {
                    "sent": "And you have to be careful not to.",
                    "label": 0
                },
                {
                    "sent": "Overweight some of the features because then the algorithm will be able to recover from that as it proceeds.",
                    "label": 0
                },
                {
                    "sent": "The other way we can do it is by using a two stage approach.",
                    "label": 1
                },
                {
                    "sent": "Where we analyze the feature relevance.",
                    "label": 0
                },
                {
                    "sent": "In the first stage, and then we use that to fix the feature sampling distribution prior to the forest construction.",
                    "label": 1
                },
                {
                    "sent": "OK, so parallel scheme.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to do is try and control this update right?",
                    "label": 0
                },
                {
                    "sent": "So the peace and precision we chose confidence intervals in this case.",
                    "label": 0
                },
                {
                    "sent": "Now we can use this statistic, which is basically the average information game.",
                    "label": 0
                },
                {
                    "sent": "Here the.",
                    "label": 0
                },
                {
                    "sent": "All theoretical, true mean sample size and sample standard deviation.",
                    "label": 0
                },
                {
                    "sent": "It has a students distribution with minus 1 degrees of freedom.",
                    "label": 1
                },
                {
                    "sent": "So we can form.",
                    "label": 0
                },
                {
                    "sent": "Working at confidence limits.",
                    "label": 0
                },
                {
                    "sent": "On this statistic Q1 and Q2.",
                    "label": 0
                },
                {
                    "sent": "Which gives us some level of confidence which we can set.",
                    "label": 0
                },
                {
                    "sent": "So we use 95% in this case.",
                    "label": 0
                },
                {
                    "sent": "And what we can do is rearrange it so we get these confidence bounds on our average information game for each feature.",
                    "label": 0
                },
                {
                    "sent": "Now the idea here is that once we form these.",
                    "label": 1
                },
                {
                    "sent": "Confidence intervals around each measure of feature relevance.",
                    "label": 1
                },
                {
                    "sent": "We then use that to update the feature sampling distribution by maintaining the most uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "But that still lies within each of these confidence intervals.",
                    "label": 0
                },
                {
                    "sent": "So that way the algorithm proceeds these companies to become smaller and starts alter the feature segmentation to greater extent.",
                    "label": 0
                },
                {
                    "sent": "But in the initial stages.",
                    "label": 0
                },
                {
                    "sent": "It should end on the side of caution.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so then we get the convergence rates.",
                    "label": 1
                },
                {
                    "sent": "These plus these plots are showing the average confidence interval size averaged over all the features for each of the datasets.",
                    "label": 1
                },
                {
                    "sent": "Now these convergence rates.",
                    "label": 0
                },
                {
                    "sent": "Basically reflects how fast we are converging to our.",
                    "label": 0
                },
                {
                    "sent": "Features happens to bution.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "They depend on the level of confidence you set, but they also depend on the number of features in the data set and on the average tree size.",
                    "label": 0
                },
                {
                    "sent": "So you can see for the sonar data set it's got the slowest convergence because he's got the highest number of features and doesn't build particularly large trees, so not generating that many samples for each of the features and then on the other extreme we've got the Pima data set.",
                    "label": 0
                },
                {
                    "sent": "Which converges the fastest simply because it's got the fewest number of features in the field to the largest trees.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what we did here?",
                    "label": 0
                },
                {
                    "sent": "We use 9% of the data training.",
                    "label": 0
                },
                {
                    "sent": "We hold back 10% testing each time we build.",
                    "label": 0
                },
                {
                    "sent": "100 trees for each of the forests.",
                    "label": 1
                },
                {
                    "sent": "We average over 100 trials.",
                    "label": 1
                },
                {
                    "sent": "These are the test error rates using the standard random forest.",
                    "label": 0
                },
                {
                    "sent": "Using our confidence interval parallel technique and also using a two stage cart technique where we build a single country to begin with.",
                    "label": 0
                },
                {
                    "sent": "We don't prune it.",
                    "label": 0
                },
                {
                    "sent": "We build it as large as possible.",
                    "label": 0
                },
                {
                    "sent": "And because at each stage it's optimizing over all the features, you get an optimal information gain value for each feature at each split in the tree.",
                    "label": 0
                },
                {
                    "sent": "So we just build a single tree, take the average information game for that, and then use that to fix the feature sampling distribution and then build the forest from there.",
                    "label": 0
                },
                {
                    "sent": "You can see it does perform.",
                    "label": 0
                },
                {
                    "sent": "Actually very well in this case, but there's no guarantee on the reliability of this method.",
                    "label": 0
                },
                {
                    "sent": "And I can't do math.",
                    "label": 0
                },
                {
                    "sent": "You can see is actually.",
                    "label": 0
                },
                {
                    "sent": "Particularly, the artificial data sets at the bottom.",
                    "label": 0
                },
                {
                    "sent": "There are significantly improving performance and quite importantly is never significantly worse.",
                    "label": 0
                },
                {
                    "sent": "So this problem of initially overweighting some of the features has been avoided in this case.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so irrelevant features.",
                    "label": 0
                },
                {
                    "sent": "We said that the average information gain is the mean of a non negative sample.",
                    "label": 1
                },
                {
                    "sent": "So we get the expected information gain of irrelevant feature is going to have some nonzero value.",
                    "label": 1
                },
                {
                    "sent": "Now if you apply this to the feature sampling distribution and you have a high proportion of irrelevant features, although you may be giving them a small probability of being sampled when you actually.",
                    "label": 1
                },
                {
                    "sent": "Combine these the.",
                    "label": 0
                },
                {
                    "sent": "The total probability of choosing irrelevant feature is actually.",
                    "label": 0
                },
                {
                    "sent": "Still significant and will still degrade the performance of random forests.",
                    "label": 0
                },
                {
                    "sent": "So what we want to try and do.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is calculated this expected information gain value for these relevant features?",
                    "label": 1
                },
                {
                    "sent": "So we got our parent node.",
                    "label": 0
                },
                {
                    "sent": "We got the parent node and.",
                    "label": 0
                },
                {
                    "sent": "When you split this, we create left sending rights and.",
                    "label": 0
                },
                {
                    "sent": "We can say the NL examples and left descendants.",
                    "label": 0
                },
                {
                    "sent": "I'll positive examples and then we know that there are any examples in the parent node.",
                    "label": 0
                },
                {
                    "sent": "I positive examples in the parent mode, so from there we can workout.",
                    "label": 0
                },
                {
                    "sent": "The corresponding numbers for the right descendant.",
                    "label": 0
                },
                {
                    "sent": "Now what we do is for each possible arrangement.",
                    "label": 0
                },
                {
                    "sent": "When you project the data, download the one dimensional space for each possible arrangement.",
                    "label": 0
                },
                {
                    "sent": "You can try these different split values and they'll give you values for NL&IL and you want to choose the split which basically minimize this expression, which gives you the values of NLI which minimizes this expression.",
                    "label": 0
                },
                {
                    "sent": "Reppies the parent entropy.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We tried this for.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some values.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but no competition.",
                    "label": 0
                },
                {
                    "sent": "So here we have for each node we got.",
                    "label": 0
                },
                {
                    "sent": "This is the number of negative examples in the node number of positive examples.",
                    "label": 1
                },
                {
                    "sent": "And so if you were to take a diagonal segment along here, we've got a fixed size N because we're basically just changing the class proportions within that node.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We found that obviously doing in this manner's.",
                    "label": 0
                },
                {
                    "sent": "Hugely expensive because you have to build each of the node or node arrangements, optimize the information gain than average.",
                    "label": 0
                },
                {
                    "sent": "I roll them.",
                    "label": 0
                },
                {
                    "sent": "But we did find that for a fixed value of N, that the highest value of expected information gain actually occurs when the class proportions are equal.",
                    "label": 0
                },
                {
                    "sent": "So down using the center line here, we're actually increasing the size of the node, but it's actually half of each class.",
                    "label": 0
                },
                {
                    "sent": "The minimum is occurring.",
                    "label": 0
                },
                {
                    "sent": "When we have one example from one class.",
                    "label": 0
                },
                {
                    "sent": "And all the other examples from the other class.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we were to just take the value of N. So we just have the node of size.",
                    "label": 0
                },
                {
                    "sent": "Then we can actually there's a lower bound is given when we have only one example from one class.",
                    "label": 1
                },
                {
                    "sent": "And we can actually.",
                    "label": 0
                },
                {
                    "sent": "Calculate this lower bound on the expected information gain of the relevant feature.",
                    "label": 1
                },
                {
                    "sent": "We found that we could approximate the upper bound when the class proportions are equal.",
                    "label": 0
                },
                {
                    "sent": "Basically, by plotting the the function on a log rhythmic scale, we found that we got an approximately straight line.",
                    "label": 0
                },
                {
                    "sent": "And so these then give us the bounds on the expected information gain value for an irrelevant feature.",
                    "label": 0
                },
                {
                    "sent": "So it says we just built.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "100 trees in artificial data.",
                    "label": 0
                },
                {
                    "sent": "Set the two relevant features of features one and two.",
                    "label": 0
                },
                {
                    "sent": "And the remaining seven are irrelevant.",
                    "label": 0
                },
                {
                    "sent": "These stars represent the.",
                    "label": 0
                },
                {
                    "sent": "Average information gain is achieved by each of the features, and then these bounds are bounds on our expected performance of an irrelevant feature.",
                    "label": 1
                },
                {
                    "sent": "As you can see, the relevant features are within the bounds where we expect them to be and.",
                    "label": 0
                },
                {
                    "sent": "Are two relevant features actually?",
                    "label": 0
                },
                {
                    "sent": "Above these mounds.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what we did then is just to see how well these work.",
                    "label": 0
                },
                {
                    "sent": "We chose a feature selection threshold, which is basically the halfway point between these two bounds.",
                    "label": 0
                },
                {
                    "sent": "We tried on the Freedmen data set, which is normally a regression, but we thresholded it too.",
                    "label": 0
                },
                {
                    "sent": "Crates.",
                    "label": 0
                },
                {
                    "sent": "A binary classification problem, which was.",
                    "label": 0
                },
                {
                    "sent": "Pretty much equal balanced.",
                    "label": 0
                },
                {
                    "sent": "We used our feature selection scheme using our feature selection threshold built 100 trees.",
                    "label": 0
                },
                {
                    "sent": "OK, you got the bounds for the expected information gain.",
                    "label": 0
                },
                {
                    "sent": "Use the threshold is the halfway point between these bounds and then selected the features which had an average information gain that was above this.",
                    "label": 0
                },
                {
                    "sent": "And we found that over the 100 trials.",
                    "label": 0
                },
                {
                    "sent": "Or I guess you picked out the five relevant features all the time.",
                    "label": 0
                },
                {
                    "sent": "And picked some of the irrelevant features, but some of the time now CFS.",
                    "label": 0
                },
                {
                    "sent": "Isn't arguing with competitors correlation based feature selection?",
                    "label": 0
                },
                {
                    "sent": "Which uses symmetrical uncertainty to examine the correlation between.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Feature on the class as well as the feature feature correlation to actually analyze redundancy in the data as well.",
                    "label": 0
                },
                {
                    "sent": "And this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Doesn't choose any of the irrelevant features, but actually drops some of the relevant features as well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We found for artificial data set.",
                    "label": 0
                },
                {
                    "sent": "Again, our algorithm is picking out the two relevant features all the time and still picks some of the irrelevant features some of the time CFS.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "Ignores all of the irrelevant features.",
                    "label": 0
                },
                {
                    "sent": "But also ignores one of the relevant features as well.",
                    "label": 0
                },
                {
                    "sent": "Professor, if we look at the results for this, we try this again.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is noted that the date of a training Center for testing.",
                    "label": 0
                },
                {
                    "sent": "Build 100 trees to actually analyze.",
                    "label": 0
                },
                {
                    "sent": "The performance of each feature and use this to build.",
                    "label": 0
                },
                {
                    "sent": "400 specification.",
                    "label": 0
                },
                {
                    "sent": "And we have for the CFS algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is our feature selection technique.",
                    "label": 0
                },
                {
                    "sent": "Also a feature weighting where after we build these hundred trees for evaluating the features, we use that to fix the feature sampling distribution to build the further 100 trees, and then the combination of the two techniques where we select our relevant features.",
                    "label": 0
                },
                {
                    "sent": "And we weight them according to their measure of relevance.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, CFS.",
                    "label": 0
                },
                {
                    "sent": "Doesn't form too well on the author states that particularly.",
                    "label": 0
                },
                {
                    "sent": "Particularly this one.",
                    "label": 0
                },
                {
                    "sent": "Simply because it's dropping that feature one all the time.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Our room here is certainly doing better.",
                    "label": 0
                },
                {
                    "sent": "One of the things CFS actually goes wrong.",
                    "label": 0
                },
                {
                    "sent": "The vote states that.",
                    "label": 0
                },
                {
                    "sent": "Which actually contains a large amount of redundancy.",
                    "label": 0
                },
                {
                    "sent": "Which our algorithm isn't making any allowance for, it's all so our feature selection technique most of the time is just leaving all of the features in, but we still suppress some of the redundant features because they don't appear as as relevant.",
                    "label": 0
                },
                {
                    "sent": "As relevant as the.",
                    "label": 0
                },
                {
                    "sent": "As the.",
                    "label": 0
                },
                {
                    "sent": "Most important features.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What we've done here?",
                    "label": 0
                },
                {
                    "sent": "We've introduced this measure of node complexity, so we can actually.",
                    "label": 0
                },
                {
                    "sent": "Improve our measure of feature relevance by examining the node composition.",
                    "label": 1
                },
                {
                    "sent": "And creating a wait so we can actually wait this averaging process.",
                    "label": 0
                },
                {
                    "sent": "We've also given some algorithms.",
                    "label": 0
                },
                {
                    "sent": "For altering this feature sample distribution.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 1
                },
                {
                    "sent": "Parallel scheme using confidence intervals actually controls the update rate that the feature sampling distribution.",
                    "label": 0
                },
                {
                    "sent": "So that we avoid the initial overweighting stage and.",
                    "label": 0
                },
                {
                    "sent": "We've also given a feature selection threshold.",
                    "label": 0
                },
                {
                    "sent": "So we can actually try and remove some of these relevant features to improve the performance of a random forest.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you so much.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Mutual Information or information gain if you multiply it by the number of instances, it has a high square distribution.",
                    "label": 0
                },
                {
                    "sent": "So is there a good reason why you didn't use this in the computation of the confidence intervals?",
                    "label": 0
                },
                {
                    "sent": "And the confidence intervals.",
                    "label": 0
                },
                {
                    "sent": "You're saying, what do you feel if you compute the confidence interval of mutual information?",
                    "label": 0
                },
                {
                    "sent": "You can sort of use this fact that it has the high squared.",
                    "label": 0
                },
                {
                    "sent": "Distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, I haven't thought about that.",
                    "label": 0
                },
                {
                    "sent": "An idea?",
                    "label": 0
                },
                {
                    "sent": "Using.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you.",
                    "label": 0
                },
                {
                    "sent": "So you're using the T is.",
                    "label": 0
                },
                {
                    "sent": "Almost as far as.",
                    "label": 0
                },
                {
                    "sent": "Visual information Brian.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Coming.",
                    "label": 0
                },
                {
                    "sent": "OK thanks man.",
                    "label": 0
                }
            ]
        }
    }
}