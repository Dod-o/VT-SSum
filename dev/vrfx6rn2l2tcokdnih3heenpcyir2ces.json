{
    "id": "vrfx6rn2l2tcokdnih3heenpcyir2ces",
    "title": "Optimal Reverse Prediction: A Unified Perspective on Supervised, Unsupervised and Semi-Supervised Learning",
    "info": {
        "author": [
            "Linli Xu, Department of Computing Science, University of Alberta"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_xu_orp/",
    "segmentation": [
        [
            "OK, so today I'm going to talk about optimal reverse prediction unified perspective on supervised unsupervised and semi supervised learning and this is joint work with Martha White and their Sherman."
        ],
        [
            "So the underlying motivation of this work is that supervised learning and unsupervised learning being two foundational problem problems in machine learning have usually been looked at in a separate manner, but unfortunately but unfortunately in semi supervised learning actually one we need to consider the both of the problems at the same time.",
            "Therefore it would be beneficial for us if we could unify these two principles into the same framework.",
            "So."
        ],
        [
            "This is actually a man contribution of our work where we proposed a unified view of classical supervised learning an unsupervised learning.",
            "So throughout this talk will be focused on the least squared loss function, which is a very simple loss function, but we will show later that even in the in this simple setting there are many powerful an interesting properties that we can exploit.",
            "So here we have a unified view of supervised least squares learning and several unsupervised.",
            "Unsupervised learning principles, including principle component analysis K means clustering, an normalized graph cut, and the only difference is that is in the assumption on the labels whether the labels are given or not, whether the labels are continuous or discrete.",
            "And once these unified frameworks build, we are not ready to develop new algorithms.",
            "For Semi supervised learning."
        ],
        [
            "So to set up, we need to establish establish the background.",
            "So I'm going to give a brief introduction to the supervised learning in this worst case.",
            "And here I'm supposed."
        ],
        [
            "Given the data matrix in supervised learning were given a data matrix and a label matrix and the data matrix for each column, the data matrix in is an N dimensional feature vector and the each each each each row of data matrix is an end dimensional data feature vector an each row of the label matrix is a K dimensional target vector and here we are assuming that the both the data matrix and the.",
            "Label matrix is full rank and we're also assuming that the rank of the data matrix is bigger than the rank of the label matrix.",
            "That is, K smaller than N. And the goal here is that we want to learn linear model.",
            "Want to learn model.",
            "Such that sought to predict the predict the output from the input, and this is parameterized by the double matrix, and the problem can be solved very in a very simple way.",
            "Just solving this quadratic program and it can be computed.",
            "The matrix can be computed by the multiplying the pseudo inverse of the X matrix in the wild matrix OK. And that's the simplest least square loss function, and we can add a Reg."
        ],
        [
            "Authorization term to the objective function to address the problem of overfitting, and we can also reformulate the objective with the inner product with only the inner products of the inputs such that we can introduce kernels an also we can add instance waiting for each example, and this can be done by just multiplying the objective, multiplying diagonal matrix with the object function function here, and the diagnosis.",
            "These matrix are the.",
            "Weights on the instances."
        ],
        [
            "So that's given all these extensions.",
            "We have different problem types and these problem types depending depends on the constraints on the labels.",
            "For example, if we if we have continuous wire labels then the we have a regression problem and in the regression problem the testing procedure is pretty simple.",
            "It's just using this prediction rule and on the on the other hand if we have the constraints on the Y. Labels such that white takes value from zero and one an each row of the yr. Each role of the wine matrix there can be only one entry that is not zero.",
            "That is the sum of each of the wine matrix one.",
            "In this way we have discrete classification constraints on the wire labels for classification problems.",
            "Ann for classification problems we if we are given new data points, if we want to testing these data points, we need to discrete discretize the outputs.",
            "So this can be done well in a simple way.",
            "Just we can just threshold the outputs of these prediction outputs.",
            "So that's basically the end."
        ],
        [
            "Reduction to the supervised list list.",
            "Square learning weather is regression or its classification?",
            "Now we're going to look at different way to do that, which which may seem like less intuitive, but it's actually equivalent to what we have seen so far.",
            "So basically"
        ],
        [
            "What we are doing here is that instead, instead of predicting labels from inputs as it's done in the forward least squares, we're.",
            "We're doing the opposite, so we're predicting the inputs from the labels.",
            "That is, we are.",
            "Suppose we are given the Y outputs.",
            "We are reconstructing the input X with this term.",
            "And we're trying to minimize this difference of the of the inputs and the reconstruction OK, and this problem can be solved as in the same manner using this using this equation."
        ],
        [
            "So.",
            "This may sound counter intuitive to what we have seen so far, but the key point here is that we can actually recover the forward solution from the reverse solution.",
            "Basically, if X has rank one, the forward and reverse optimization problems have the same equilibrium condition and therefore the forward solution can be exactly exactly recovered from the reverse solution by this equation.",
            "OK now, um."
        ],
        [
            "We can similarly add regularization or kernels or instance weighting to the same product to the problem and recover the forward solution in a similar manner."
        ],
        [
            "So that's basically the rivers least squares formulation that we proposed, and we will see that in supervised learning.",
            "Well, under supervised least squares formulation, the forward and reverse views are equivalent to each other and also they can be recovered exact from the other.",
            "But we should notice that the forward and reverse losses are not identical because they are measured in different units.",
            "So that's basically the."
        ],
        [
            "Supervised case, So what about the unsupervised case?",
            "In unsupervised case where we are, there are no training labels given."
        ],
        [
            "And here we treat the unsupervised problem using the principle of optimism.",
            "That is, where are treating the gases of the missing labels as a variable.",
            "That is that and we also we are optimizing the problem in terms of that as well as the parameters W. Or the you anyway?",
            "So this is the forward formulation of the unsupervised discourse learning.",
            "And this is the reverse way to do that.",
            "So here we can see that in the forward model we're reconstructing the outputs using the inputs and in the reverse model we are reconstructing the inputs using these outputs.",
            "K. But"
        ],
        [
            "We can immediately notice that the forwardly squares in the unsupervised setting is actually vacuous, because for each parameter W we can just choose the output that equals to XW, and therefore we can have zero objective function and therefore there is no way for us to distinguish good model from bad ones in the forward.",
            "In the forward formulation.",
            "On the other hand."
        ],
        [
            "It's interesting to see that.",
            "The reverse list.",
            "Worse in the unsupervised setting, the solution is not vacuous.",
            "Therefore we can have nontrivial results, and it enables us to unify the classical training principles.",
            "As we will see later."
        ],
        [
            "In this talk.",
            "So the first thing that we can show is that the optimistic reversely squares in the unsupervised setting is equivalent to principal component analysis, so the proof can be just.",
            "The proof can be just simply."
        ],
        [
            "Showed in this way, so we first minimize the problem over here and have this solution.",
            "Just plug this solution back into into the objective.",
            "Now with just several steps we can see that this problem is just exactly the same as principle component analysis."
        ],
        [
            "And to solve that we can just take the first top K eigenvectors up here.",
            "XX transpose matrix, which is also a kernel matrix and after we solve for zatan you we can now recover the forward model using this equation.",
            "Now, after recovering the forward model when if we are given new data points, we can just embed those new data points using using the forward model.",
            "So that."
        ],
        [
            "Basically, the regression case where the wild labels are continuous.",
            "So if we now apply the classification that discrete classification constraints on the outputs, that is, the these classification constraints, then we will show our 2nd result, which is that the.",
            "Optimistic reverse prediction in the unsupervised case, with the classification constraints is actually equivalent to K means clustering.",
            "So the proof is the same."
        ],
        [
            "And in that first we solve for you first and we plug you back into the objective function and the object function turns into this formulation.",
            "And this difference is actually the difference of each data point between."
        ],
        [
            "In the mean of the class that this data point belongs to.",
            "Therefore, the object function is exactly exactly the."
        ],
        [
            "Love.",
            "The sum of the squared distance between each data point and the meaning of the class that this data point belongs to.",
            "So this this problem is actually equivalent to K means.",
            "So that's our.",
            "Proof to work through our 2nd result and we can also."
        ],
        [
            "Easily add a kernels and instance ways to the K means clustering and what we can show here now is that.",
            "And the normalized graph cut is equivalent."
        ],
        [
            "2K means clustering with kernels.",
            "An instance weights and here we have, you assume that case Dublin, Dublin, negative, that is, K is the entries of K is now negative and K is a semi definite positive matrix.",
            "And here the Lambda here is the sum of the kernel matrix, and we're, uh, you're multiplying the objective function with this with this Lambda as as as as a way to.",
            "As a way for normalization in graph cut.",
            "So so far we have seen supervised learning and unsupervised learning.",
            "And to summarize we can see that we are looking at the problem in a reverse list least squares formulation and we have we have shown that the reverse List square prediction."
        ],
        [
            "In the supervised cases equivalent to least squares learning, it can be least squares regression or least squares classification, and we have shown that in the unsupervised case a whole stream of unsupervised principles can be fitted into into this system.",
            "It can be like it includes principle, component, last analysis and K means clustering.",
            "An normalized cut clustering.",
            "So now we have a unified framework for both supervised learning and unsupervised learning.",
            "Now we are ready to it would be natural for us just to combine these principles together and to develop new semi supervised algorithms."
        ],
        [
            "So now before I just show our algorithms, I want to 1st go through the another nice and interesting property of the reverse least squares loss.",
            "So the reverse list least squares loss can be.",
            "Can be looked at in two different settings.",
            "The supervised setting and the unsupervised setting.",
            "And in the in the supervised setting, suppose we are given a group of data points and we have.",
            "We have a predictor we have a which which is which correspond to a linear space, right?",
            "So the supervised and supervised setting what we want to do is that given a given outputs, we want to reconstruct the inputs.",
            "This is this is done by projecting the outputs onto this linear hyperplane and what we want to do is to end the supervised reverse loss can actually measure the as the squared distance between these data points and the reconstruction right?",
            "So basically the sum of the squared distance of this.",
            "These segments are just the supervised least supervised least squared loss.",
            "And in the unsupervised case where the awhile labels are actually unknown to us, therefore we need to take guesses of the white labels and for each guest we projected unto these linear hyperplane and we want to find one of the guests that minimize this squared distance.",
            "The sum of the squared distance.",
            "So basically it turns out that the reconstruction in the unsupervised case of the data points are actually the orthogonal projection of these data points until the hyperplane.",
            "So basically the unsupervised reverse loss is just the sum of these orthogonal projections.",
            "Now if we are looking at the."
        ],
        [
            "Example of X3.",
            "The data point here using the Pythagorean theorem will see that the supervised reverse loss is actually the sum of the unsupervised reverse loss and B squared distance between the supervise reconstruction and the unsupervised reconstruction.",
            "Right now using these."
        ],
        [
            "Using this fact, we can now decompose the supervised loss into two terms.",
            "The first one is the unsupervised loss, which only depends on the input, so it has nothing to do with the output.",
            "It has nothing to do with the labels right and the second term is the squared distance between the supervised reconstruction and the unsupervised reconstruction, so.",
            "This is the.",
            "This is the way that we we can decompose the reverse least squares."
        ],
        [
            "Lost now given this decomposition decomposition, we can now get an estimate of the supervised loss.",
            "Here we can see that this we can decompose the supervised loss into these two terms, and if we look at the second term, which only depends on the inputs, it doesn't have anything to do with the outputs.",
            "Therefore we can get an unsupervised loss estimate.",
            "And if we introduce large amount of unlabeled data, then we will have a better estimate of these unsupervised lossett estimate.",
            "Because the variance can be reduced in general, so that's basically a very nice property of."
        ],
        [
            "About the reverse least squares loss function and now we can switch to the semi supervised algorithms that we develop.",
            "So basically in semi supervised learning.",
            "Natural way to do is to combine supervised loss with unsupervised loss.",
            "And what is she really been done?",
            "Is that the unsupervised loss is in is in the reverse formulation because as we have seen, the unsupervised forward loss is vacuous.",
            "So and what is really being done is that the.",
            "The unsupervised losses in the reverse formulation and the supervised Lawson is in the forward formulation, so this is the little lack of principle because forward loss is usually not in the same units at the reverse loss.",
            "Therefore, idea here is that we want to combine the supervised loss and the unsupervised loss both from the reverse perspective.",
            "So."
        ],
        [
            "This can be done here if we look at the regression problem first, where we have no constraints on the white labels, then what we what we did is just a very obvious way to just combine the supervised and unsupervised objective.",
            "So this is the supervised loss and this is unsupervised loss.",
            "And what we can do is just this problem is.",
            "Not joint convex, so we just did the very simple thing is to using alternating minimization to solve this problem.",
            "And after the problem is solved, we can just recover the forward solution using this equation and in the forward model after after the forward model is filled, we can if we are given new data points, we can just predict the label predicted labels in this in this forward model.",
            "So this algorithm is very straightforward, but actually it performs very pretty well in practice and.",
            "Here we have we can see some results on the regression."
        ],
        [
            "Problems be compared.",
            "The semi supervised regression problem.",
            "Regression algorithms to these supervised regression algorithms and we can see that the improvement is is very significant.",
            "And we also compared our algorithm with the transductive kernel regression algorithm, which is the middle column.",
            "We can see that on these three regression datasets, the.",
            "The advantage of the semi supervised regression algorithms is quite significant.",
            "Also, so that's basically the regression, semi supervised regression algorithms and what we can do next is that we can apply the classification."
        ],
        [
            "Strains on the wild on the output labels, and then we can have a semi supervised classification algorithm.",
            "So to do that we just need to combine the least squares with the K means principle and we just add the supervised loss at with the unsupervised loss and.",
            "Again, this is hard optimization problem because because these constraints are hard and what we can do is that we just simply used alternating minimization.",
            "And again in this, after we solve this problem, we can recover the forward solution using using this formulation.",
            "And in the forward model we can now classify more data, more data.",
            "If you ever get more data data points.",
            "So."
        ],
        [
            "That's basically.",
            "That's basically semi supervised classification.",
            "Another thing to try is to combine these squares with normalized cut which can give us another semi supervised classification algorithm.",
            "So similarly we have a supervised loss, an unsupervised loss and the algorithm is done again using alternating minimization and the forward solution can be recovered again here.",
            "OK, so we can look."
        ],
        [
            "Some of our class semi supervised classification results.",
            "Here we are the first.",
            "The first row here is the supervised K means and supervised normalized cuts and this the 2nd three algorithms are just the spectral graph transducer and the Laplacian SVM and the Laplacian regularizer least squares algorithms and the bottom two algorithms are semi supervised algorithms.",
            "So we perform.",
            "All these are all these algorithms.",
            "Some datasets that have been widely used in semi supervised learning and here we can see that although our algorithm is are very simple, it can still achieve like state of the art performance even though on some datasets like this one data set is not the best but it still is very close to the state of the art."
        ],
        [
            "So in conclusion, here we are.",
            "Here we have developed a unified framework for supervised and unsupervised learning and based on this framework we are ready to to develop novel, semi supervised learning algorithms and in the future one natural thing to do is to consider other loss functions other than the least squares.",
            "But one thing that we should notice that the least squares loss function give us some nice properties like the equivalence between the forward model and the reverse model.",
            "And also.",
            "In in the least squares formulation we we also have this nice decomposition of the loss function, right?",
            "So it's not obvious that this can be extended to the other loss functions, but it's worthwhile to investigate into.",
            "And also we can consider other more complex data such as structured outputs.",
            "And another thing to do is that as we have mentioned, we we basically we're using alternating minimization to solve our problems.",
            "So what we can do is we can develop convex relaxation and get some convex algorithms that will guarantee us some global solutions.",
            "So that's basically my talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so today I'm going to talk about optimal reverse prediction unified perspective on supervised unsupervised and semi supervised learning and this is joint work with Martha White and their Sherman.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the underlying motivation of this work is that supervised learning and unsupervised learning being two foundational problem problems in machine learning have usually been looked at in a separate manner, but unfortunately but unfortunately in semi supervised learning actually one we need to consider the both of the problems at the same time.",
                    "label": 1
                },
                {
                    "sent": "Therefore it would be beneficial for us if we could unify these two principles into the same framework.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is actually a man contribution of our work where we proposed a unified view of classical supervised learning an unsupervised learning.",
                    "label": 1
                },
                {
                    "sent": "So throughout this talk will be focused on the least squared loss function, which is a very simple loss function, but we will show later that even in the in this simple setting there are many powerful an interesting properties that we can exploit.",
                    "label": 0
                },
                {
                    "sent": "So here we have a unified view of supervised least squares learning and several unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised learning principles, including principle component analysis K means clustering, an normalized graph cut, and the only difference is that is in the assumption on the labels whether the labels are given or not, whether the labels are continuous or discrete.",
                    "label": 1
                },
                {
                    "sent": "And once these unified frameworks build, we are not ready to develop new algorithms.",
                    "label": 0
                },
                {
                    "sent": "For Semi supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to set up, we need to establish establish the background.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give a brief introduction to the supervised learning in this worst case.",
                    "label": 0
                },
                {
                    "sent": "And here I'm supposed.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given the data matrix in supervised learning were given a data matrix and a label matrix and the data matrix for each column, the data matrix in is an N dimensional feature vector and the each each each each row of data matrix is an end dimensional data feature vector an each row of the label matrix is a K dimensional target vector and here we are assuming that the both the data matrix and the.",
                    "label": 0
                },
                {
                    "sent": "Label matrix is full rank and we're also assuming that the rank of the data matrix is bigger than the rank of the label matrix.",
                    "label": 1
                },
                {
                    "sent": "That is, K smaller than N. And the goal here is that we want to learn linear model.",
                    "label": 0
                },
                {
                    "sent": "Want to learn model.",
                    "label": 0
                },
                {
                    "sent": "Such that sought to predict the predict the output from the input, and this is parameterized by the double matrix, and the problem can be solved very in a very simple way.",
                    "label": 0
                },
                {
                    "sent": "Just solving this quadratic program and it can be computed.",
                    "label": 0
                },
                {
                    "sent": "The matrix can be computed by the multiplying the pseudo inverse of the X matrix in the wild matrix OK. And that's the simplest least square loss function, and we can add a Reg.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Authorization term to the objective function to address the problem of overfitting, and we can also reformulate the objective with the inner product with only the inner products of the inputs such that we can introduce kernels an also we can add instance waiting for each example, and this can be done by just multiplying the objective, multiplying diagonal matrix with the object function function here, and the diagnosis.",
                    "label": 0
                },
                {
                    "sent": "These matrix are the.",
                    "label": 0
                },
                {
                    "sent": "Weights on the instances.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's given all these extensions.",
                    "label": 0
                },
                {
                    "sent": "We have different problem types and these problem types depending depends on the constraints on the labels.",
                    "label": 1
                },
                {
                    "sent": "For example, if we if we have continuous wire labels then the we have a regression problem and in the regression problem the testing procedure is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "It's just using this prediction rule and on the on the other hand if we have the constraints on the Y. Labels such that white takes value from zero and one an each row of the yr. Each role of the wine matrix there can be only one entry that is not zero.",
                    "label": 0
                },
                {
                    "sent": "That is the sum of each of the wine matrix one.",
                    "label": 0
                },
                {
                    "sent": "In this way we have discrete classification constraints on the wire labels for classification problems.",
                    "label": 0
                },
                {
                    "sent": "Ann for classification problems we if we are given new data points, if we want to testing these data points, we need to discrete discretize the outputs.",
                    "label": 0
                },
                {
                    "sent": "So this can be done well in a simple way.",
                    "label": 0
                },
                {
                    "sent": "Just we can just threshold the outputs of these prediction outputs.",
                    "label": 1
                },
                {
                    "sent": "So that's basically the end.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reduction to the supervised list list.",
                    "label": 0
                },
                {
                    "sent": "Square learning weather is regression or its classification?",
                    "label": 0
                },
                {
                    "sent": "Now we're going to look at different way to do that, which which may seem like less intuitive, but it's actually equivalent to what we have seen so far.",
                    "label": 0
                },
                {
                    "sent": "So basically",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we are doing here is that instead, instead of predicting labels from inputs as it's done in the forward least squares, we're.",
                    "label": 1
                },
                {
                    "sent": "We're doing the opposite, so we're predicting the inputs from the labels.",
                    "label": 1
                },
                {
                    "sent": "That is, we are.",
                    "label": 0
                },
                {
                    "sent": "Suppose we are given the Y outputs.",
                    "label": 0
                },
                {
                    "sent": "We are reconstructing the input X with this term.",
                    "label": 0
                },
                {
                    "sent": "And we're trying to minimize this difference of the of the inputs and the reconstruction OK, and this problem can be solved as in the same manner using this using this equation.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This may sound counter intuitive to what we have seen so far, but the key point here is that we can actually recover the forward solution from the reverse solution.",
                    "label": 0
                },
                {
                    "sent": "Basically, if X has rank one, the forward and reverse optimization problems have the same equilibrium condition and therefore the forward solution can be exactly exactly recovered from the reverse solution by this equation.",
                    "label": 1
                },
                {
                    "sent": "OK now, um.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can similarly add regularization or kernels or instance weighting to the same product to the problem and recover the forward solution in a similar manner.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's basically the rivers least squares formulation that we proposed, and we will see that in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Well, under supervised least squares formulation, the forward and reverse views are equivalent to each other and also they can be recovered exact from the other.",
                    "label": 1
                },
                {
                    "sent": "But we should notice that the forward and reverse losses are not identical because they are measured in different units.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Supervised case, So what about the unsupervised case?",
                    "label": 0
                },
                {
                    "sent": "In unsupervised case where we are, there are no training labels given.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here we treat the unsupervised problem using the principle of optimism.",
                    "label": 1
                },
                {
                    "sent": "That is, where are treating the gases of the missing labels as a variable.",
                    "label": 1
                },
                {
                    "sent": "That is that and we also we are optimizing the problem in terms of that as well as the parameters W. Or the you anyway?",
                    "label": 0
                },
                {
                    "sent": "So this is the forward formulation of the unsupervised discourse learning.",
                    "label": 0
                },
                {
                    "sent": "And this is the reverse way to do that.",
                    "label": 0
                },
                {
                    "sent": "So here we can see that in the forward model we're reconstructing the outputs using the inputs and in the reverse model we are reconstructing the inputs using these outputs.",
                    "label": 0
                },
                {
                    "sent": "K. But",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can immediately notice that the forwardly squares in the unsupervised setting is actually vacuous, because for each parameter W we can just choose the output that equals to XW, and therefore we can have zero objective function and therefore there is no way for us to distinguish good model from bad ones in the forward.",
                    "label": 0
                },
                {
                    "sent": "In the forward formulation.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's interesting to see that.",
                    "label": 0
                },
                {
                    "sent": "The reverse list.",
                    "label": 0
                },
                {
                    "sent": "Worse in the unsupervised setting, the solution is not vacuous.",
                    "label": 1
                },
                {
                    "sent": "Therefore we can have nontrivial results, and it enables us to unify the classical training principles.",
                    "label": 1
                },
                {
                    "sent": "As we will see later.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this talk.",
                    "label": 0
                },
                {
                    "sent": "So the first thing that we can show is that the optimistic reversely squares in the unsupervised setting is equivalent to principal component analysis, so the proof can be just.",
                    "label": 1
                },
                {
                    "sent": "The proof can be just simply.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Showed in this way, so we first minimize the problem over here and have this solution.",
                    "label": 0
                },
                {
                    "sent": "Just plug this solution back into into the objective.",
                    "label": 0
                },
                {
                    "sent": "Now with just several steps we can see that this problem is just exactly the same as principle component analysis.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to solve that we can just take the first top K eigenvectors up here.",
                    "label": 0
                },
                {
                    "sent": "XX transpose matrix, which is also a kernel matrix and after we solve for zatan you we can now recover the forward model using this equation.",
                    "label": 0
                },
                {
                    "sent": "Now, after recovering the forward model when if we are given new data points, we can just embed those new data points using using the forward model.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, the regression case where the wild labels are continuous.",
                    "label": 0
                },
                {
                    "sent": "So if we now apply the classification that discrete classification constraints on the outputs, that is, the these classification constraints, then we will show our 2nd result, which is that the.",
                    "label": 0
                },
                {
                    "sent": "Optimistic reverse prediction in the unsupervised case, with the classification constraints is actually equivalent to K means clustering.",
                    "label": 1
                },
                {
                    "sent": "So the proof is the same.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in that first we solve for you first and we plug you back into the objective function and the object function turns into this formulation.",
                    "label": 0
                },
                {
                    "sent": "And this difference is actually the difference of each data point between.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the mean of the class that this data point belongs to.",
                    "label": 0
                },
                {
                    "sent": "Therefore, the object function is exactly exactly the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Love.",
                    "label": 0
                },
                {
                    "sent": "The sum of the squared distance between each data point and the meaning of the class that this data point belongs to.",
                    "label": 1
                },
                {
                    "sent": "So this this problem is actually equivalent to K means.",
                    "label": 0
                },
                {
                    "sent": "So that's our.",
                    "label": 0
                },
                {
                    "sent": "Proof to work through our 2nd result and we can also.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Easily add a kernels and instance ways to the K means clustering and what we can show here now is that.",
                    "label": 0
                },
                {
                    "sent": "And the normalized graph cut is equivalent.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2K means clustering with kernels.",
                    "label": 0
                },
                {
                    "sent": "An instance weights and here we have, you assume that case Dublin, Dublin, negative, that is, K is the entries of K is now negative and K is a semi definite positive matrix.",
                    "label": 0
                },
                {
                    "sent": "And here the Lambda here is the sum of the kernel matrix, and we're, uh, you're multiplying the objective function with this with this Lambda as as as as a way to.",
                    "label": 0
                },
                {
                    "sent": "As a way for normalization in graph cut.",
                    "label": 0
                },
                {
                    "sent": "So so far we have seen supervised learning and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "And to summarize we can see that we are looking at the problem in a reverse list least squares formulation and we have we have shown that the reverse List square prediction.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the supervised cases equivalent to least squares learning, it can be least squares regression or least squares classification, and we have shown that in the unsupervised case a whole stream of unsupervised principles can be fitted into into this system.",
                    "label": 0
                },
                {
                    "sent": "It can be like it includes principle, component, last analysis and K means clustering.",
                    "label": 1
                },
                {
                    "sent": "An normalized cut clustering.",
                    "label": 0
                },
                {
                    "sent": "So now we have a unified framework for both supervised learning and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Now we are ready to it would be natural for us just to combine these principles together and to develop new semi supervised algorithms.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now before I just show our algorithms, I want to 1st go through the another nice and interesting property of the reverse least squares loss.",
                    "label": 0
                },
                {
                    "sent": "So the reverse list least squares loss can be.",
                    "label": 0
                },
                {
                    "sent": "Can be looked at in two different settings.",
                    "label": 0
                },
                {
                    "sent": "The supervised setting and the unsupervised setting.",
                    "label": 0
                },
                {
                    "sent": "And in the in the supervised setting, suppose we are given a group of data points and we have.",
                    "label": 0
                },
                {
                    "sent": "We have a predictor we have a which which is which correspond to a linear space, right?",
                    "label": 0
                },
                {
                    "sent": "So the supervised and supervised setting what we want to do is that given a given outputs, we want to reconstruct the inputs.",
                    "label": 0
                },
                {
                    "sent": "This is this is done by projecting the outputs onto this linear hyperplane and what we want to do is to end the supervised reverse loss can actually measure the as the squared distance between these data points and the reconstruction right?",
                    "label": 0
                },
                {
                    "sent": "So basically the sum of the squared distance of this.",
                    "label": 0
                },
                {
                    "sent": "These segments are just the supervised least supervised least squared loss.",
                    "label": 0
                },
                {
                    "sent": "And in the unsupervised case where the awhile labels are actually unknown to us, therefore we need to take guesses of the white labels and for each guest we projected unto these linear hyperplane and we want to find one of the guests that minimize this squared distance.",
                    "label": 0
                },
                {
                    "sent": "The sum of the squared distance.",
                    "label": 0
                },
                {
                    "sent": "So basically it turns out that the reconstruction in the unsupervised case of the data points are actually the orthogonal projection of these data points until the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So basically the unsupervised reverse loss is just the sum of these orthogonal projections.",
                    "label": 1
                },
                {
                    "sent": "Now if we are looking at the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example of X3.",
                    "label": 0
                },
                {
                    "sent": "The data point here using the Pythagorean theorem will see that the supervised reverse loss is actually the sum of the unsupervised reverse loss and B squared distance between the supervise reconstruction and the unsupervised reconstruction.",
                    "label": 1
                },
                {
                    "sent": "Right now using these.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using this fact, we can now decompose the supervised loss into two terms.",
                    "label": 1
                },
                {
                    "sent": "The first one is the unsupervised loss, which only depends on the input, so it has nothing to do with the output.",
                    "label": 0
                },
                {
                    "sent": "It has nothing to do with the labels right and the second term is the squared distance between the supervised reconstruction and the unsupervised reconstruction, so.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is the way that we we can decompose the reverse least squares.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lost now given this decomposition decomposition, we can now get an estimate of the supervised loss.",
                    "label": 1
                },
                {
                    "sent": "Here we can see that this we can decompose the supervised loss into these two terms, and if we look at the second term, which only depends on the inputs, it doesn't have anything to do with the outputs.",
                    "label": 0
                },
                {
                    "sent": "Therefore we can get an unsupervised loss estimate.",
                    "label": 1
                },
                {
                    "sent": "And if we introduce large amount of unlabeled data, then we will have a better estimate of these unsupervised lossett estimate.",
                    "label": 0
                },
                {
                    "sent": "Because the variance can be reduced in general, so that's basically a very nice property of.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About the reverse least squares loss function and now we can switch to the semi supervised algorithms that we develop.",
                    "label": 0
                },
                {
                    "sent": "So basically in semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Natural way to do is to combine supervised loss with unsupervised loss.",
                    "label": 1
                },
                {
                    "sent": "And what is she really been done?",
                    "label": 0
                },
                {
                    "sent": "Is that the unsupervised loss is in is in the reverse formulation because as we have seen, the unsupervised forward loss is vacuous.",
                    "label": 0
                },
                {
                    "sent": "So and what is really being done is that the.",
                    "label": 0
                },
                {
                    "sent": "The unsupervised losses in the reverse formulation and the supervised Lawson is in the forward formulation, so this is the little lack of principle because forward loss is usually not in the same units at the reverse loss.",
                    "label": 1
                },
                {
                    "sent": "Therefore, idea here is that we want to combine the supervised loss and the unsupervised loss both from the reverse perspective.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This can be done here if we look at the regression problem first, where we have no constraints on the white labels, then what we what we did is just a very obvious way to just combine the supervised and unsupervised objective.",
                    "label": 0
                },
                {
                    "sent": "So this is the supervised loss and this is unsupervised loss.",
                    "label": 0
                },
                {
                    "sent": "And what we can do is just this problem is.",
                    "label": 0
                },
                {
                    "sent": "Not joint convex, so we just did the very simple thing is to using alternating minimization to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "And after the problem is solved, we can just recover the forward solution using this equation and in the forward model after after the forward model is filled, we can if we are given new data points, we can just predict the label predicted labels in this in this forward model.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm is very straightforward, but actually it performs very pretty well in practice and.",
                    "label": 0
                },
                {
                    "sent": "Here we have we can see some results on the regression.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problems be compared.",
                    "label": 0
                },
                {
                    "sent": "The semi supervised regression problem.",
                    "label": 0
                },
                {
                    "sent": "Regression algorithms to these supervised regression algorithms and we can see that the improvement is is very significant.",
                    "label": 0
                },
                {
                    "sent": "And we also compared our algorithm with the transductive kernel regression algorithm, which is the middle column.",
                    "label": 0
                },
                {
                    "sent": "We can see that on these three regression datasets, the.",
                    "label": 0
                },
                {
                    "sent": "The advantage of the semi supervised regression algorithms is quite significant.",
                    "label": 0
                },
                {
                    "sent": "Also, so that's basically the regression, semi supervised regression algorithms and what we can do next is that we can apply the classification.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Strains on the wild on the output labels, and then we can have a semi supervised classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "So to do that we just need to combine the least squares with the K means principle and we just add the supervised loss at with the unsupervised loss and.",
                    "label": 1
                },
                {
                    "sent": "Again, this is hard optimization problem because because these constraints are hard and what we can do is that we just simply used alternating minimization.",
                    "label": 1
                },
                {
                    "sent": "And again in this, after we solve this problem, we can recover the forward solution using using this formulation.",
                    "label": 0
                },
                {
                    "sent": "And in the forward model we can now classify more data, more data.",
                    "label": 0
                },
                {
                    "sent": "If you ever get more data data points.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's basically.",
                    "label": 0
                },
                {
                    "sent": "That's basically semi supervised classification.",
                    "label": 0
                },
                {
                    "sent": "Another thing to try is to combine these squares with normalized cut which can give us another semi supervised classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "So similarly we have a supervised loss, an unsupervised loss and the algorithm is done again using alternating minimization and the forward solution can be recovered again here.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can look.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of our class semi supervised classification results.",
                    "label": 0
                },
                {
                    "sent": "Here we are the first.",
                    "label": 0
                },
                {
                    "sent": "The first row here is the supervised K means and supervised normalized cuts and this the 2nd three algorithms are just the spectral graph transducer and the Laplacian SVM and the Laplacian regularizer least squares algorithms and the bottom two algorithms are semi supervised algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we perform.",
                    "label": 0
                },
                {
                    "sent": "All these are all these algorithms.",
                    "label": 0
                },
                {
                    "sent": "Some datasets that have been widely used in semi supervised learning and here we can see that although our algorithm is are very simple, it can still achieve like state of the art performance even though on some datasets like this one data set is not the best but it still is very close to the state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, here we are.",
                    "label": 0
                },
                {
                    "sent": "Here we have developed a unified framework for supervised and unsupervised learning and based on this framework we are ready to to develop novel, semi supervised learning algorithms and in the future one natural thing to do is to consider other loss functions other than the least squares.",
                    "label": 1
                },
                {
                    "sent": "But one thing that we should notice that the least squares loss function give us some nice properties like the equivalence between the forward model and the reverse model.",
                    "label": 0
                },
                {
                    "sent": "And also.",
                    "label": 0
                },
                {
                    "sent": "In in the least squares formulation we we also have this nice decomposition of the loss function, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not obvious that this can be extended to the other loss functions, but it's worthwhile to investigate into.",
                    "label": 1
                },
                {
                    "sent": "And also we can consider other more complex data such as structured outputs.",
                    "label": 0
                },
                {
                    "sent": "And another thing to do is that as we have mentioned, we we basically we're using alternating minimization to solve our problems.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can develop convex relaxation and get some convex algorithms that will guarantee us some global solutions.",
                    "label": 0
                },
                {
                    "sent": "So that's basically my talk.",
                    "label": 0
                }
            ]
        }
    }
}