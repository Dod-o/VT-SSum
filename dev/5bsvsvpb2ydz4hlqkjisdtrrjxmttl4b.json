{
    "id": "5bsvsvpb2ydz4hlqkjisdtrrjxmttl4b",
    "title": "Deep RL",
    "info": {
        "author": [
            "Marc G. Bellemare, Google, Inc."
        ],
        "published": "Oct. 11, 2018",
        "recorded": "August 2018",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/DLRLsummerschool2018_bellemare_deep_RL/",
    "segmentation": [
        [
            "Well, thank you first time Masood Doina and Joel for the invitation.",
            "This is real treat.",
            "I think that summer school has been fantastic.",
            "I've certainly enjoyed it.",
            "So deep reinforcement learning."
        ],
        [
            "Is a term that's fairly recent.",
            "The history of AI statics.",
            "So what I thought I'd do today is, rather than cover the whole gamut of deep reinforcement learning, which is actually quite big now give you a flavor of what I think it is and where I think the challenges are in the coming years.",
            "And well, to get started deep."
        ],
        [
            "And reinforcement learning, right?",
            "This is the theme of the summer school.",
            "Deep learning on one end, neural networks and then a reinforcement learning.",
            "Trying to learn to make the right decisions in an environment.",
            "Putting neural networks inside an RL agent is not something that's necessary in you, and I'm only going to go back now to the 1990s, but really it dates back from even longer than that.",
            "People have been using deep Nets to learn value functions, return policies for quite a long time.",
            "There was backgammon with Jerry.",
            "Pterosaurs work there was the elevator control paper by Christ in Bartow.",
            "There was flying helicopter that also involved in your network.",
            "I actually early in Mycareer work on transfer learning in Tic Tac toe using these networks called cascade correlation neural networks.",
            "This was really fun.",
            "And of course, since then, well, you know we've had some major successes with using neural networks, which reinforcement learning.",
            "So if it's not new, then you know what does the term mean in in modern terms, right?",
            "What?",
            "What's new in the context of what's happening today?",
            "And so."
        ],
        [
            "What I want to say is I think actually Deep RL is more than just a combination of putting a network inside an environment.",
            "It's really about the kind of domains that we now look at by the kind of challenge domains that we might be interested in solving, and it's also the algorithms that we develop to make these.",
            "Networks operate smoothly or robustly inside the enforcement learning environments, and I do think that those two things are fairly new.",
            "But if you go back, for example, divorce paper.",
            "It was TD Lambda.",
            "It was very much the what I would might say.",
            "The vanilla algorithm, you know, plug it into a network and it was no replay memory.",
            "There was no target network per say.",
            "So big big difference has been to really think hard as to what the challenges are when we combine neural networks with our environments and how do we address these challenges."
        ],
        [
            "So this list here is by no means complete, but here's a sample of the kind of challenges that arise in deep reinforcement learning.",
            "So first of all, the training data is not ID, so we know how to train deep Nets when data is drawn from a data set or from a stream of ID data.",
            "But now when we're dealing with reinforcement learning as their policy changes, we're going to see different parts of the state space, and that's actually quite difficult to handle with classical training methods for deep networks.",
            "Another thing that is dear to my heart is in talked about this and Emma talked about.",
            "This is the difficulty to get confidence intervals.",
            "So how good is there actually prediction?",
            "This matters too deep learning this matters to DRL because we're going to want to exploration or maybe want to take action safely, right?",
            "So that's also a big challenge.",
            "And then there's there's a variety of a certain more ordered me questions.",
            "Example the We know that combining our methods with deep networks might lead to divergent.",
            "Often though, the state space is unknown, so we don't even know what that state space looks like.",
            "If you're playing, for example, an Atari game.",
            "And the last one actually is actually very interesting, but I think so far what we've seen is mostly model free methods working really well and often requiring simulation.",
            "So there's clearly a big challenge in making model based methods or sample efficient methods work in deeper, and it's very exciting to see all the work that's happening in meta PRL.",
            "Few shot learning."
        ],
        [
            "OK, so what's the challenge that I've been the most involved in?",
            "It's the arcade learning environment, the arcade learning environment is actually 10 years old.",
            "This year it started on the beaches of Sunny Barbados at the Reinforcement Workshop in 2008 there was a paper by a researcher called Colours colours Duke, who said we should use pitfall as a domain for testing our agents and this populated Michael bowling and you are not as master student.",
            "I'm started a project.",
            "I came in a bit later in 2011 and then eventually this led to the release of a Journal paper.",
            "The release of an open source.",
            "That piece of software called the Arcade Learning Environment and then from the famous Nature Paper Dicul Nature paper."
        ],
        [
            "OK, so if you've never seen if you've never seen an Atari game who has never seen an Atari game, obviously entire game.",
            "Maybe that's a better question.",
            "There we go.",
            "Well, you've seen this before, so this is a game called Pong Pong is very, very simple game that you can play on the Atari 2600 you have a green paddle on the right hand side, and you're trying to prevent the ball from going to the net at the same time.",
            "You're also trying to score points on the.",
            "Orange paddle by, as you saw, just getting the ball across across the net.",
            "OK, so this is probably one of the simplest games we think about, but as you'll see, it already has some complexities baked in."
        ],
        [
            "So when we talk about the Ark alone environment, it's talking about the Atari 2600.",
            "So what's the the basic setup here?",
            "Is we have this console which is actually from 1977, which we're going to be emulating and there are in fact 15 actions you can take.",
            "So up left, down, right and the fire button.",
            "If you think the cross product of these you get 18.",
            "And then there's actually the Atari memory is actually 128 bytes, so it's actually a very small machine by comparison to most modern computers, right?",
            "But the big challenges that we have.",
            "Well, first of all, even 128 bytes if you were to consider every state as a possible bit configuration, that's still to 1000, right?",
            "So there's still quite a bit.",
            "Quite a few states that you could be in.",
            "But if you instead look at the image that you're getting from the emulator, then that image is roughly 33,000 pixels, and it's actually been given to you at 60 Hertz.",
            "So that in comparison to many other oral environments at the time, was quite daunting."
        ],
        [
            "And So what is the Ellie D Aily is actually it's a rapper or an interface between the Stella Emulator which emulates Atari 2600 games and what we would consider Classic RL interface where you have an agent in an environment receiving observations, rewards, taking actions.",
            "Anne Anne, I know that many of you of course are using German to, but that's a gem you could think of it as an extra layer, an extra RL layer.",
            "Alright, so formally speaking, we have this observation.",
            "We've really looked at two different kinds of observations.",
            "Chiefly, it's been images because images are very appealing, but also the 80 provides you with the RAM state, and in fact, to this day I think image based methods perform much better at playing Atari games and RAM based methods.",
            "And when you think about it, sort of makes sense because the information is highly compacted.",
            "If you look at the ram right, every bit matters, whereas maybe you have a bit more fudge if you're looking at images.",
            "As I said, the action is joystick motions and."
        ],
        [
            "I think she did most interesting thing about Haley in many ways is the reward function.",
            "And.",
            "At the time it was I think customer to design A reward function that would say you look at every game you look at a game and say my goal is to achieve XL define reward function to be to be why.",
            "But we said well we have this Atari system.",
            "There will be a lot of games that we can actually play with.",
            "And we want to be ignostic to how to how the actual game plays and how humans would play this game.",
            "So we're going to define the reward function to be discord differential.",
            "So basically you have almost all of these games ever score, and in fact all the games that daily supports have a score and we need to find the reward to be the change from one time step to the next, and your reward in your school.",
            "That means you could have negative Warden games where you lose points and you will have positive rewards in games where you always make points.",
            "OK."
        ],
        [
            "So as I sort of hinted at already, the other interesting aspect of the early is that it provides you not just with one environment, but really provides you with a ton of environment.",
            "I think there's something like 900 recorded games on Wikipedia that you could play through the guitar.",
            "Now early on we filtered these games down to roughly 60 now.",
            "And these are the 60 Canonical games.",
            "If you will, of the early where, where daily provides you with with an interface to these games and that was very exciting at the time, I think still is today, but it's rare to have a platform that will provide you with such diversity.",
            "For free or out of the box."
        ],
        [
            "So what does it matter?",
            "It matters because one thing we wanted to address then was this idea of general competency.",
            "It's actually interesting that when I was preparing this talk, I realized how how obvious this ideas become now.",
            "But you know, at the time it wasn't.",
            "No.",
            "That's not good.",
            "Are you guys seeing things Flash?",
            "No, OK, good.",
            "It's just a monitor.",
            "So what is general competency?",
            "Well, let me give you an example with.",
            "You know with something like a self driving car, it might be that we want.",
            "The same agent running the same piece of code to operate under different conditions OK?",
            "So maybe you know, maybe you want your agent to perform well in mountainous terrains.",
            "Maybe you want to perform well in snowy terrains, maybe in crowded city environments.",
            "OK, with the idea that there's something appealing saying we have a learning algorithm, it should be baked into a single piece of code."
        ],
        [
            "As a position to see narrow competency where we would have a specific piece of code for each environment, right?",
            "So now imagine if we had 60 pieces of code for 60 different environments.",
            "That would be a bit more disappointing, right?",
            "So this was this was a goal of the project and this is what the aliens all about.",
            "So."
        ],
        [
            "Why is the alien interesting challenge domain for deeper, well, well, it's exactly, I think because it pushes for that idea.",
            "General competency and there's really three key characteristics that make it interesting beyond the perceptual level.",
            "It's as I said, it's a diverse set.",
            "It's interesting, and it's also independent.",
            "OK, so let me go very quickly over what these three key words mean."
        ],
        [
            "Reverse is is sort of the most important one in many ways.",
            "Say 10 years ago and you know to this day we used the mountain current environment to evaluate our reinforcement learning agents.",
            "And in many ways, Mountain car is a bit of the M list of RL.",
            "It's really supported a lot of good results, but it's also a fairly simple domain.",
            "It's 2 dimensional continuous continuous space domain OK?",
            "And So what?",
            "You know what you might imagine is that overtime researchers developed methods for mountain car.",
            "The first method was so, so in the second method came around was slightly better and then suddenly you have many many many agents that have been optimized for mountain power.",
            "And this was actually flagged in a paper based human rights and colleagues where they said we review that reinforcement learning is pretty vulnerable to environment over fitting and then they have the solution, which was to generalize these these environments and I think this is actually incredibly relevant today where we are.",
            "I think the DRL community is again facing the same kind of questions, but at the time it was very clear that say if we evaluate our agents and mountain car.",
            "We might be missing out, or it may be overfitting tonight.",
            "And so being there."
        ],
        [
            "So having a large set of environment so we could play with was wasn't in fact at a sort of a solution to this this question.",
            "And actually we went to separate and we said we should in fact have."
        ],
        [
            "Five training games.",
            "And then the rest would be test, right?",
            "So you don't get the optimizing hyperparameters on all games, but only a subset and spirit of general competency.",
            "Now you can carry over that code and those hyperparameters to to the rest of the game to 55 remaining games, and then report your testing school.",
            "And I think this is one of the things that has made the early so successful.",
            "Is that really people have played by these rules and reported if you will generalization scores?",
            "OK, OK, so that's that's that's one part of ITL is interesting 50 parallel.",
            "Another aspect is that the."
        ],
        [
            "Domain itself is interesting to people.",
            "People play and have played these games.",
            "So for example, let me show you at the bottom you have a game called Pitfall another game called Seaquest, and both both of these games are.",
            "Highly.",
            "Looking for there are highly entertaining.",
            "They're very humans, enjoy playing these games.",
            "They enjoy finding out where the interactions are in these games, exploring the world, you know, catching the fish, saving the swimmers.",
            "There's really a sense that humans find it interesting to watch to watch agents involved in this in this scenario, and also to play the game themselves.",
            "And that's really not true of every benchmark.",
            "It's very hard to get humans to play mountain car, for example.",
            "Another thing that's important."
        ],
        [
            "Is this this idea of independence and the way I can best describe this is to say that these games were designed by people and four people as opposed to design for researchers with a specific research agenda and that actually mattered quite a bit in the design of the design of the early, because we were able to say, let's take, let's take games that.",
            "That were in this case design 30 years ago 40 years ago and treat them as an apartment.",
            "Where this independence means is that we have to deal with whatever.",
            "Biases are quirks of the environment where put in OK.",
            "So one question often get is where is the XY position of the agent in this in this domain?",
            "And my answer is, well, this is not provided to humans, so why should we be using this XY coordinate, right?",
            "And so this independence was very important, because it it really forces us to deal with with problems that maybe.",
            "As researchers, we haven't even think of.",
            "Yeah, that's going to be a theme through the rest of this talk.",
            "Also so diverse, interesting and independent.",
            "OK."
        ],
        [
            "So just to give you a sense for early attempts before I move into the deep networks part early on, some of the some of the results we had, we tried something called a basic features which were basically.",
            "Discretizing the screen and encoding the presence of covers and this was done actually mostly in in three bit color modes, because there is not enough memory for the 8 bit color, so 3 bit color mode encode the presence of colors and then define a feature set with which we could do linear function approximation.",
            "We did the same thing with with Randy to.",
            "We did the same thing by something called locally sensitive hashing on pixels, and last but not least actually there was there was a significant portion of the.",
            "Even the devs research that was actually spent detecting object, so this album is called Disco OK and the idea was simple.",
            "You would subtract the background you would extract using a vision technique.",
            "You would extract all of the shapes that were present at a contiguous blobs.",
            "After extracting the background and then you would try to match these two existing labels.",
            "You see I've seen a yellow submarine like this before.",
            "This must be an object of class.",
            "Class zero, you've seen a fish like this is a Class 1 fish.",
            "It turns out that's incredibly hard to do, or at least if you if you don't spend too much time on it.",
            "And in."
        ],
        [
            "When we measured the scores of this disco algorithm, it was pretty dismal and it's sort of telling sign that.",
            "Mapping perception to send object representation is not a trivial task.",
            "We had we.",
            "We sort of value these different algorithms.",
            "One thing I want to point out is we have this thing called the score distribution, which effectively you're going to.",
            "It's like a community distribution function for scores and.",
            "Just last, I think six months ago or so, Ben Rector post talking about performance profiles, which is exactly what's a better version of this idea, so it's quite nice to see that in fact this idea score distribution is not too crazy.",
            "It's about impossible to Barstow, but early on we said, well, if you want to compare 60 different games, what can you do?",
            "You can plug them in that score distribution and then lower curve would correspond to a better agent or the other way round.",
            "Higher curve is a better age.",
            "Another interesting thing that happens is if you have 60 different games you need to aggregate the numbers and this is what's been shown here.",
            "You could take an average.",
            "You could take a median.",
            "The scores are actually on different scales, so you might have to normalize first right?",
            "So for example, if you're playing, you see the score in this game called Seaquest Now and the score goes up by 20 points.",
            "Every time you do something, if instead you're playing phone, discord goes up by one point.",
            "So how do you compare with 60 different games?",
            "How do you compare how well these games are?",
            "There's been a few solutions proposed, unsurprisingly.",
            "Of course, the way you normalize is going to change the way you report your results.",
            "And then you could take the average with the median.",
            "It turns out the average as we all know is heavily dominated by outliers, and it remains true today.",
            "So median scores are definitely much better measure of performance and average scores.",
            "OK."
        ],
        [
            "OK, so this was this was the ellee up to about 2014 and then the deep Q networks idea came around.",
            "OK, so what's what's the Q&Q and is taking a deep network and using it as the value function for a value based reinforcement learning agent.",
            "OK, so this is where the network actually looks like.",
            "It's going to feed in a stack of four images.",
            "To the to the network and then run them through a series of convolutional layers and produce a hidden layer which is now treated like a linear representation and we do.",
            "The last layer is linear layer and we have put the Q value so actually."
        ],
        [
            "What's interesting is document is not just this network.",
            "Decline is really an agent architecture, so in the middle you have the network, but then of course you have a preprocessing stack that takes in the raw raw frames if you will given to you by the early and then you there's a series of preprocessing operations that are applied.",
            "There's Gray scaling, there's frame pooling and then finally downsampling to stacking part itself happens.",
            "And then you feed this.",
            "You feed this to the network so you don't even need to network right away, so 50 take actions you're going to remember all of these frames you're going to put them in a replay memory which stores old trend transitions.",
            "And then what's going to happen is that the network outputs build the policy in this train by this replay memory.",
            "So the another part.",
            "Another important part of the when is the learning algorithm.",
            "How do you actually make use of that experience to training network?",
            "And a big part of the original agent which has somehow it's almost vestigal has stuck around.",
            "Is this target network?",
            "The Target Network does is it sort of.",
            "It stabilizes the learning by giving you a nicer loss.",
            "So really, I like to think of DQ and in the context of deeper L as also having brought this around this idea of the whole agent architecture as opposed to value function that produces a policy."
        ],
        [
            "OK, so how do you actually train this network?",
            "It's very simple.",
            "You have declines.",
            "Learning algorithm was basically minimizing the squared TD error.",
            "With a subtlety and you can see it on the screen.",
            "There's a tilt on the Q value at the Max and what it's saying is we're going to use the Q value from the target network.",
            "In Tensorflow term there would be almost like doing a subgradient, but from the previous network and we're going to use that next value because it stabilizes learning.",
            "And it's also because it's like doing a TD update as opposed to doing a bellman visual minimization.",
            "And so then you define this loss function to squared loss, the spreadsheet here, and then you just do gradient descent.",
            "What's your favorite auto differentiation tool to learn to minimize that PDF?",
            "Basically try to match.",
            "There the value function Lipsy.",
            "And so this was a."
        ],
        [
            "Surprisingly successful, this is work.",
            "This is a video of a game called Space Invaders that is actually slightly later work.",
            "On on something called persistent algorithms, but just to give you a sense for how how impressive these agents are playing the game, there's really a sense of of smooth control.",
            "But if you look at these agents and you watch them play this game, there's a sense that they know what's going on.",
            "They can predict the motion in this case of the invaders, and then they can prevent prevent these invaders were coming down.",
            "And so there was it.",
            "He can Dodge the bullets also.",
            "So really this technique.",
            "Led to significant improvements in performance on daily."
        ],
        [
            "So this is the agent architecture, yes?",
            "What's the 80 behind grey scaling?",
            "Um?",
            "I think it makes your input vector smaller.",
            "Anne.",
            "It turns out so it may be piece of anecdote.",
            "It turns out that.",
            "Atari is $128.00.",
            "It has underneath it called.",
            "It's about 7 bits but the because it was designed for both color TV's and black and white TV's the same bit sets will allow you to play the game in black and white to play the game in color.",
            "So it turns out that is a mapping is a direct mapping between color pixels and luminance.",
            "You lose a few things, but as a human you would play these games with these easy so the Gray scaling is just a way to reduce the input space without losing anything.",
            "Yes.",
            "Right, so you've probably seen in some of these games.",
            "Imagine you're playing Palm right and you have the ball coming at you.",
            "If you just look at the last frame, you don't actually know which way the ball is coming.",
            "Is it going towards you?",
            "Is it leaving?",
            "To go towards the other so by by stacking these frames, what you're doing is effectively giving yourself.",
            "We're learning about the dynamics first, so you can learn now about dynamic so you can see the ball was one pixel away the previous time step, so I can learn the direction of the model.",
            "I mean, and more generally, we could say this is a way to get around some of the partial observability that we see in Atari games, and in fact, if you just use one frame, the performance is much worse.",
            "Now I don't know if anybody is using two or three.",
            "Four seems like a convenient number.",
            "Maybe I should have repeated questions.",
            "The first question was why Grayscale and a second question was why stack these frames?",
            "Any other questions?",
            "Yes, that effect.",
            "So the agent the question was when the agent is playing Space Invaders.",
            "It seems to shoot through the wall, which is clearly the wrong thing to do.",
            "This might have to do with the fact that the agent isn't penalized to do this.",
            "It might not be optimal, but it doesn't have an impact on its ultimate return, so the agent just doesn't see the difference between taking a random action in this case and not.",
            "OK."
        ],
        [
            "So what's nice about having an architecture is you can actually look.",
            "We can start looking at its pieces and a large part of the parallel research, especially right after the document paper came out was about studying different aspects of this architecture and so you can look at the network and say how would I make this a better network.",
            "Maybe I changed the frame stacking by a recurrent layer.",
            "Or maybe I did this with a single SCM layer, maybe for recurrent network."
        ],
        [
            "Other thing we can do is we can look at the policy and we can say how do we actually change the policy or how we output the Q values to lead to again to other better performance or more stable and more robust agents.",
            "So some of the work in that space there was the dualing networks by ON doing networks splits the Q function into value V depending on the policy and advantage function A and of course Q will then be equal to V + A and that allows you to recover the Q value.",
            "Of the agent, but you only need A to choose actions.",
            "Synchronous actor critic is a name says it's an actor critic method, which means there's a parameterized policy that the agent follows and then uses AQ function to RV function to predict and learn about this policy.",
            "And then really fun piece of work by Ian Husband and colleagues and Bootstrap Q functions.",
            "Instead of having a single value function, we now have a collection of value functions and we were going to randomly train these Q functions on a subset of the data to.",
            "This is the bootstrap part.",
            "To get different estimates.",
            "Trying to get at a confidence interval over the Q functions and then an act according to some function of this assembled of Q functions to incentivize exploration.",
            "Similarly, more recently, there's been.",
            "There's been two work.",
            "One work by Mary Fortune ET al.",
            "And also another piece of work by opening high on noisy networks or parameterized noise to go ahead and change the actual parameters of the network to change the Q function or the stochastic policy and eventually drive better exploration.",
            "Another"
        ],
        [
            "Of the architecture that's been quite looked at is the replay memory itself, so we have this store of data and now we have to train on this data, and in fact I didn't say this, But the memory is actually bounded.",
            "For practical reasons, people typically keep a small subset of the data.",
            "And so I watch.",
            "I want to just briefly mention.",
            "Prize replay, which which was which.",
            "Reweigh the experience too.",
            "Reweigh the experience, to train on samples where we have a high TD error with the 80, that being that these are the samples where we need to learn about them right now.",
            "Samples with low TD error we already know how to predict them.",
            "We should not be training on them.",
            "There's also work by Bruce Lee's and some of my colleagues at Deep Mind on applying important sampling to get better estimates of the replay data and Calculator has actually been working with me.",
            "We're looking into off policy learning.",
            "For similar similar questions."
        ],
        [
            "OK, the part I've been the most involved in is actually the learning algorithm itself, and this this has actually been the subject of a lot of work.",
            "For example, double Q learning by heart Vanasselt looked at this idea of reducing the statistical bias that's induced when we take the Max in Q learning by choosing the action of that Max according to the first network.",
            "So it's actually a very simple trick that was shown to reduce statistical bias at a time and actually significantly improved performance roughly around the same time we looked at.",
            "This idea of advantage learning advantage learning is actually an idea from Lehman Baird in the mid 90s we looked at.",
            "Different algorithms that would be what we called action gap increasing.",
            "So if you know that action A is the best action you think it's the best action, then you're going to try to increase.",
            "The Q value for that action, or rather keep it the same and decrease the Q value for older sub optimal actions so it's easier for your network.",
            "To make the distinction between good action in a bad action, and again, if you do this, there's a number of ways you can do this.",
            "Encourage you to look into the paper, curious many ways to do this, but in all cases this actually improves learning.",
            "Yeah, sure.",
            "So that's.",
            "So that's a good question.",
            "So if we have a softmax, can't we get the same effect?",
            "Yes, now you're leaving it to the learning algorithm to slowly, overtime accrete evidence that the higher action is better.",
            "If you're dealing with Q value.",
            "So if you just did a softmax, for example over the Q values, but you learn the Q values using Q learning, you're stuck with the values you have, so you don't have when you could play with the temperature parameter, but that's a bit fiddly.",
            "Here I think the idea is more to say, let's make the job easier for the network to just distinguish these two states.",
            "Well, that makes sense.",
            "Maybe rather we need to revisit this work.",
            "Um?",
            "Any other questions?",
            "Yes.",
            "Right, right?",
            "So the comment is that if you suck Max 2 very close values and of course you get a uniform policy, that's true.",
            "I think your point was more than now you can change the temperature to compensate for this.",
            "Is something worth looking at here.",
            "So very briefly, let me just point out there's been work on some off policy learning algorithm Q, Lambda, retrace and then most recently this Society of distributional reinforcement.",
            "Is these distribution distributional methods, which have been quite involved with and that's actually been quite exciting because we've seen a lot of progress and new interesting avenues for research coming out of these.",
            "So I thought I would actually tell you about these distributional methods.",
            "So let's actually get into into this part of the talk."
        ],
        [
            "In distribution or reinforcement learning, we're going to look at the full distribution of possible returns, so let me give you an example for what that means."
        ],
        [
            "Suppose you're playing a game of Monopoly.",
            "And you're the wheelbarrow.",
            "And you're sitting on Park Place and you're about to roll the dice.",
            "An if you roll it too, and you're very unlucky, roll it to uni, Lennon, Boardwalk and the hotel is owned by your opponent.",
            "You're going to lose $2000.",
            "But if you roll anything else than a 2, then you would actually go over Boardwalk, an win $200.",
            "So in classic reinforcement learning terms, we would.",
            "We would say you know there's a state, just Park Place.",
            "This is the location of your wheelbarrow.",
            "Possibly also the location of the hotels.",
            "And then as a reward are of X.",
            "Right and we care about the expected reward, so the expected reward is we can do the math.",
            "It comes out to be $139 on average.",
            "Yeah.",
            "No, this is a purely prediction problem, so there's no actions being taken here."
        ],
        [
            "OK, now if you think about this problem.",
            "If you think about what's going to happen, you can really say I'm going to roll the dice and then have two possible outcomes.",
            "Now this is this is RL.",
            "This is a sequential decision making problem or sequential problem.",
            "So there's going to be more decisions or more choices.",
            "Random Rolls made afterwards, and we're going to keep rolling these forward.",
            "Whether we have some policy involved, enough.",
            "Either way we have this this whole tree of possible outcomes based.",
            "Under the roll of the dice.",
            "And So what we can say is we have this sequence of rewards which are actually random.",
            "OK, and we're going to discount them and look at the value function.",
            "The expected summer discounted rewards, right?",
            "So that's what the moment equation tells us.",
            "It tells us that the value, for example, of a policy at state X is going to be expected reward plus the expected next state value function.",
            "OK."
        ],
        [
            "So what's very interesting about this picture is that the ground truth, the ground truth is you can think of the tree of all possible outcomes.",
            "You can weigh them by their probability, but if you look at the Bellman equation.",
            "That talks about expectations then.",
            "Then really, the model, the mental picture of the Bellman equation, if you will, is a straight line going through when you take all of the randomness and you can collapse it down into just going from from one mean state to another mean state where you get the mean reward whenever you transition from one step to the next.",
            "And this was actually.",
            "Pointed out already in the in the mid 2000s by number of authors that we're looking at learning linear models of the world.",
            "So if you want to predict the next set of features, present occurrence data features.",
            "It turns out that you end up with one of these flat flat models.",
            "These flat expected models.",
            "Parrot"
        ],
        [
            "So.",
            "That makes sense because the value function is an expected sum of discounted rewards by linearity of expectation, like Tour was telling us yesterday, you can break this down in two different time steps and predict each of these time steps separately."
        ],
        [
            "OK, so now this is all good, but if we go back to this this full picture of the all possible outcomes, we could actually think about.",
            "Well when I play out a specific game I observe."
        ],
        [
            "One of these trajectories, one very specific trajectory.",
            "OK, so I can look at this value function citizen expectation, but now what if I look at each trajectory as an individual?",
            "Simple outcome in a bigger random variable.",
            "OK, so really the."
        ],
        [
            "Value function you can think of it as breaking down into the individual timestamps, or think of it as the expectation of a anatomique random variable.",
            "When I call it zed, \u03c0 and that random variable is the random return that you're going to get from start state X going forward, and it's really accounting for all the randomness along the way.",
            "OK.",
            "So we there's actually, this idea is already present in a paper from 2015 with my colleague Joel Veness, where we did this with Monte Carlo estimation methods now.",
            "Then the new part here is to say, OK, so we have the Bellman equation.",
            "That relates the expectation of that random return to the expectation of a random return at the next state.",
            "So I've taken the value functions and just replace them by expectations."
        ],
        [
            "And so the distributional method is to say, let's actually just remove these expectation signs and see and see let's them make sense and see if that.",
            "If that happens then OK."
        ],
        [
            "So it turns out that there is a similar equation which you can write down, which is now going to be the value distribution at one state is going to be the random reward.",
            "Plus the next date value distribution.",
            "Now the big jump years that we're no longer talking about expectations were talking about random variables, or more specifically, we're talking about their distribution.",
            "So we're saying that the probability distribution of the random variable zed by X is displays the same distribution as the sum of these two random variables in the right hand side.",
            "And so this this way this is called the distributional Bellman equation.",
            "So it's not.",
            "It's not the only.",
            "Equation of its kind OK.",
            "There's a number of other equations like it have been proposed.",
            "What's maybe.",
            "New about this one is that we really looking at the whole distribution instead of looking at just a few of the moments and the picture looks something like this.",
            "You have again whole distribution at the current State X, and if you're satisfying this distributional equation then it must mean that when you transition to next states in proportion to how often a transition you observe, these other distributions that are next dates.",
            "So you have a.",
            "You could think of it as a distribution mixture of distributions matching your current distribution.",
            "OK. Alright."
        ],
        [
            "So.",
            "How would you actually go about with this?",
            "You know this.",
            "I've shown you a Bellman equation.",
            "How will you actually design A learning rule out of this idea?",
            "Well, first of all, let's go back to regular expected reinforcement learning.",
            "If you're doing expected reinforcement learning, you're going to observe a transition.",
            "You start in Park Place, you roll the dice and you land on go.",
            "You land somewhere and you collect 200.",
            "Dollars.",
            "Now suppose that your current value prediction is.",
            "This should be worth 150.",
            "Yeah.",
            "And the state you land that you say this would be worth 300.",
            "So how do we do this in RL?",
            "We add up the next state prediction.",
            "Discounted and add the reward.",
            "And then we."
        ],
        [
            "We change our prediction of the value of the current state to reflect what we've actually observed in a stochastic environment.",
            "Will have a step size and maybe we only move towards that prediction.",
            "But I."
        ],
        [
            "Either way, that's that's the rough picture."
        ],
        [
            "So it turns out we can do the same thing with distributions.",
            "OK, so we're going to start with a prediction about what the distribution should look like.",
            "This is really a guess about the whole.",
            "The entirety of all the possible outcomes you could see.",
            "You make the prediction you observe the next state, and you also observe the reward at the next state.",
            "You also have a predicted distribution.",
            "And now we're going to do is you're going to add if you will.",
            "Those two distributions and what that means is effectively it's a convolution.",
            "If your reward is the terministic, then you can just shift your whole distribution left or right on the X axis.",
            "And then you can you discount your distribution so you have the whole distribution of all possible returns you could see, and then you squash when you multiply the discount factor you effectively squashing your distribution towards 0.",
            "Yep.",
            "So we do both of these things and then you're going to learn about the distribution.",
            "So change, you can have a new target distribution you want to learn about the change in distribution to look more like what you've just observed.",
            "So we've gone from predicting a single scalar to predicting a whole distribution, and this is how these two updates relate."
        ],
        [
            "So what does that mean in the context of designing a new learning rule for a deep agent that plays Atari games?",
            "Well, first of all, we go back to the network and this network is called the C 51 Network.",
            "The network and one thing you can do is you can actually replace the Q value being output at the last layer of your network by effectively, except where would you like to be?",
            "A set of probabilities so would like to predict the probabilities of a set of events.",
            "Based on your input.",
            "It turns out we have to.",
            "We can't predict the whole distribution, so will make a number of these predictions.",
            "In fact, the word the name see 51 means we make 51 predictions.",
            "And So what we do is we actually define a histogram distribution evenly spaced on the interval minus 10 return to 10 return.",
            "Which sort of makes sense for Atari games, and each of these buckets now we're going to use softmax to predict to predict the distribution, so weather network is going to output the network is going to output the lodge.",
            "It's of a softmax which then gives us the probability distribution, and that's true for each action.",
            "OK.",
            "So we replace that last Q function layer by logic."
        ],
        [
            "Now the other thing that happens in this network that learning rule changes.",
            "But also we now have a distribution.",
            "So how do we act according to this distribution?",
            "Well, very simple thing.",
            "We're actually going to be a greedy where epsilon greedy with respect to the expected value that distribution.",
            "Um?",
            "And I'll come back to this point in just a little bit."
        ],
        [
            "So now if we make an approximation where we have a softmax, then we're going to run into a problem where after we shift our distribution and scale it down, we might not actually line up with the histogram we started with, and that can be a big issue."
        ],
        [
            "So in this case we are going to define a projection step which takes an arbitrary distribution.",
            "Let's keep it simple and say it's also histogram distribution and we're going to project it back into the support of the thing that our network can actually output.",
            "So projected back into 51 atoms that we can actually predict.",
            "So that lets us define actually a full learning algorithm.",
            "Under so you have the you have the operator notation at the top.",
            "Basically all it's saying is again repeating what I said earlier.",
            "You look at the next state distribution, you squash it, you shift it OK and that would give you a target.",
            "But then the next step is you need to actually project it back into your space that you can predict it.",
            "So it's a very simple algorithm.",
            "You sample a transition, you compute your sample backup, which eventually give you something like a TD error and eventually project back into the support.",
            "Now you have these, you have a new distribution you want to predict we have large.",
            "It's the network outputs to the natural thing to do is to update our predictions towards that projected Bellman target.",
            "And we do this by minimizing the you know the cross entropy loss.",
            "Alright, so let me show you yes quick question.",
            "If I understand correctly, the question is, is this going to converge to a fixed distribution eventually?",
            "Yes, the short answer is that under certain conditions you're guaranteed to actually converge to a fixed point.",
            "So the equation in fact the equation was showing you earlier is that fixed point there is there is a stationary point.",
            "Maybe before.",
            "I don't know.",
            "It may be what you mean is this so?",
            "Does it add variance?",
            "You set the policy gradient maybe does it add more variance in the optimization process, right?",
            "Because you're predicting more things, the short answer is that is still to be determined.",
            "It doesn't seem too.",
            "Doesn't seem to hurt.",
            "Do we have a Bellman optimality operator?",
            "Yes, it's true.",
            "This is all the prediction setting now you can define in the same way to take the Max you take the Max over a distribution.",
            "So now we have a problem which is how do you think?",
            "How do you compare distributions?",
            "So again what we do is we actually just choose to.",
            "The one with the highest expectation and propagate this back.",
            "That's right, so in fact you could do something different if you have.",
            "If you give me any set of distributions corresponding to different actions that could take in the next state, I can define a selection rule.",
            "What we've shown is that if your selection rule is greedy with respect to the expectation.",
            "Then you'll converge to the value distribution of an optimal policy.",
            "And actually have to quantify this.",
            "You might converge to something.",
            "Its expectation will be will be que star the optimal Q value, but it might not look in many correspond to any stationary policy.",
            "So some funny things happen, but you do get convergence.",
            "You could also decide to have Max rule that doesn't choose just the expected value, but maybe says maximize expectation but minimize variance in brick ties by minimizing variance.",
            "So or maybe maximize some combination of the two that would define a different kind of selection rule.",
            "So why learn this whole distribution?",
            "In the end?",
            "We just need the expectation that is actually a very good question and this is one that we've been answering that the last 2 1/2 years.",
            "Come back to that in a second, but the short answer is you know this is the million dollar question.",
            "This is this is a high value question to to answer.",
            "It's going to work well in practice is a short answer."
        ],
        [
            "So let me show you very briefly what it looks like, and I'll give you some learning curves and then the question will become relevant.",
            "So let me just show you what these agents are learning to predict.",
            "OK, so this is first of all, the pong agent in Pong you've seen this game before.",
            "Now you'll notice that there's three colors corresponding to three different actions you could be taking up down and stay still OK.",
            "Weather interesting is when the ball is about to get into the into the green petals net.",
            "You can see the distribution separating, so they typically on top of each other because his most actions are about the same.",
            "But then when it becomes critical you can see these distributions clearly separating.",
            "So that's a pretty simple example."
        ],
        [
            "My favorite example is actually from Space Invaders, where so you have this agent playing and 11 particularity of Space Invaders is you can lose a life if the enemy bullets hit you, but you can also lose the game right if the invaders successfully invade and reached the bottom role of the screen and what you're going to see now is that the agent is predicting it's predicting the distribution, so they should say the X axis is all the possible returns.",
            "the Y axis is the probability of that return, so you're seeing the distribution of possible returns.",
            "OK, and the agent and now you see it now on 0 there's there's a probability that the agent will get zero return.",
            "Now what is that?",
            "That's the agent saying if I make the wrong choices or if I'm just uncertain about my state, there's a chance that they will lose the game altogether.",
            "And that's actually early on.",
            "One of the reasons we thought this work pretty well is because predicting the distribution really gives you a finer grain prediction about the state of the world.",
            "If you will, right?",
            "If you were predicting expected value, you would just say.",
            "0 * 10% plus the rest times 9 * 80%, and we wouldn't have such a fine grained prediction about them.",
            "So we think that matters."
        ],
        [
            "So we can of course compare our agent under 60 games of the attorneys.",
            "So we trained in five games and report its course on all the games.",
            "If you look what was very surprising was very surprising is that the C 51 agent actually performs better than at a time the all the other state of the art methods.",
            "So both are higher mean score.",
            "But more importantly, higher median score.",
            "And it was actually also hire.",
            "These percentages are in terms of human normalized scores which remind users.",
            "So high also in human baseline and much, much better than the original DQN algorithm.",
            "OK, so this was very satisfying.",
            "One of my."
        ],
        [
            "Great results from a game called Sea Quest where you know this is quite cherry picked, but I love this learning curve where the orange is C 51 in DQ and is just at the bottom here and the previous method in dotted line to previous best was about half the score so there was really something you know shift in paradigm in terms of the learning here.",
            "OK.",
            "Yes, my name is Sue.",
            "What is stochastic in this game?",
            "I think it would be great to have a more complete study of that.",
            "First of all, first of all the agents policy is stochastic.",
            "Second of all, because you only have an imperfect view in the world, but there might be some remaining partial observability there might be.",
            "There may be a few states which you haven't learned to distinguish yet.",
            "There's there's implicit stochasticity, so the game itself, the inverted emulator, is a deterministic but.",
            "From the agent's perspective, in many ways it appears stochastic.",
            "And so we saw this actually actually with the display Sadeghi Space Invaders example, if the agent had an optimal policy, it should never lose the game, so it should never predict that's about to lose the game.",
            "But somehow through a combination of not quite understanding its state to a full extent, and maybe that it just keeps making mistakes.",
            "Then it learns to predict this.",
            "So how calibrated is the distribution?",
            "The short answer is, it isn't an one of the first things is that you saw this Bell shaped curve.",
            "The Bell shaped curve actually is an artifact of doing having histogram prediction and then repeatedly applying a discount factor.",
            "That's what a process called diffusion.",
            "So in that sense it's not calibrated, but I think in terms of predicting the outcomes, it probably is close to calibrate.",
            "It would be an interesting question.",
            "And anymore questions.",
            "OK.",
            "I'm actually quite short on time, but I will show you this video because it's quite neat."
        ],
        [
            "And it made me think the point this is work on D4 PG by my colleagues Gabriel Berkmar and Matt Hoffman and some others in Deep Mind.",
            "And I'll show you the agent first.",
            "This is one of these control tasks where the agent has to learn to cross the maze as fast as possible in the process.",
            "It doesn't need to be graceful, but it doesn't across domains very quickly and you'll notice that sometimes bumps into into walls.",
            "And so, why?",
            "Why is it happening and what does the distribution look like?",
            "Well, if we re."
        ],
        [
            "Play this video, but now predicting the looking at the actual distribution that's predicted by the agent.",
            "In this case, it's a.",
            "It's a V distribution if you will.",
            "So just a value.",
            "You'll see what happens whenever it reaches a wall, right?",
            "So do you see that there's multiple modes in their distribution?",
            "So the agent again knows that there's a chance it's going to fail, and we're talking about a trained agent, so it really should know it should have happened dissent.",
            "But somehow maybe because the agent is constrained in its representation, it's still.",
            "It's still unable to.",
            "To fail to run through walls.",
            "So that was quite neat.",
            "OK, so very brief."
        ],
        [
            "Since then, there's actually been a lot of work on either improving the network that's the left branch of this graph, quantile regression, some really need work.",
            "I will Damien implicit quantum networks and also digital networks idea, so there's been one branch of actually improving the agent architecture for these games.",
            "The other branch is actually going back closer to more traditional reinforcement learning is saying how do we come to understand distributional?",
            "Distributional reinforcement learning.",
            "In the context of this RL in general, and so actually one of the students are working with Claire had a workshop paper acnl where we found a surprising result that if you were to just use linear approximation then in fact.",
            "Nothing should be different, so this suggests that is something to do with the interaction between the deep networks and the distribution of learning.",
            "And that's actually very exciting because it gives us a sense of where to look next as to why, why it might be useful to predict the distribution even if we take the expectation at the end of the day.",
            "So this was."
        ],
        [
            "Actually, the first thing I was going to tell you about, maybe I'll very briefly talk about this other part and leave some room for questions so."
        ],
        [
            "So far I've talked about games where where we've actually made a lot of progress.",
            "In fact, this graph is a bit out of date.",
            "We've made progress in all the games now.",
            "But if you, if you were to look at different Atari games that are available through the alley, and you plot overtime, the roughly the performance we've attained on these different games and very early on the game, like Pong, we knew how to play well, and that remains true today.",
            "If you, if your agent, if you deep Agent doesn't play pong very well, then you know then you should check for bugs.",
            "The seaquest actually was a hard game until roughly until the distributional method came around.",
            "But then there's been other games actually have received a lot of attention in last 6 to 8 months, and one of them is called Montezuma's Revenge.",
            "It's actually it was designed by Robert Yeager when he was 16, and it's an adventure game.",
            "So let me show you what it looks like.",
            "I hope my sound is."
        ],
        [
            "OK, good, and so you're this.",
            "This adventure navigating this maze and unlike, say, policy Quest.",
            "There's clearly a higher level order of cognition that's happening here where you have to go and navigate and pick up a key and move from room to room, opening doors and you'll see the score.",
            "You're only actually changing in score when you achieve certain events, right?",
            "So, unlike, say, seaquest, we collecting points all the time.",
            "Now this agent can go on for minutes without collecting a new piece of score.",
            "So what we would say is that the reward function is actually sparse.",
            "OK, so this is me playing the game where you can get a flavor for where where this game is going OK Now it turns out that for."
        ],
        [
            "For an agent like Gwen or your distributional agents, that's an incredibly hard game to play.",
            "So I was already illustrating the fact that you know you started this first room and and the actions you need to take right you playing this game in this case at 15 Hertz.",
            "The actions you need to take our there's many of them.",
            "You think we get any reward, OK?",
            "There's also partial observability, so for example there's an object in the game called the Torch, and if you haven't picked up the torch and the whole screen is black, you haven't actually can't see anything, so you can't.",
            "You don't know what you're actually looking at, and of course you know there's traps so you can fall in quicksand, and you can.",
            "You can jump in a skull, and that's bad too."
        ],
        [
            "And then it's actually these barriers.",
            "You seldom, I think, these blue barriers if you run into those blue barriers, then you actually lose them.",
            "So putting this differently, right?",
            "It's a, it's a.",
            "It's a lot to coordinate your agent to get the key, OK?",
            "And actually, the one thing I want to focus on here is this Blue Barrier business.",
            "So if you think about a classic reinforcement learning agent that needs to go and learn to play this game Now, it turns out in this game you have to cross this four sequence of double barriers to eventually make your way to the goal.",
            "And you know, typically we would use an epsilon greedy policy to do this, but in that case the agent has very little chance of crossing those four double barriers by epsilon, greedy, and every time do you lose a life.",
            "Actually will start on the right hand side, so it's almost as infamous as one of the problems that was showing us yesterday where just randomly choosing actions will get you nowhere so.",
            "The key here is that if we want these agents to be able to succeed at these games.",
            "We need them to be incentivized beyond just the reward function to try to see what's on the other side and formulate a plan to actually get to that other side.",
            "At."
        ],
        [
            "So.",
            "Yesterday we heard about exploration, the classic way to do exploration in table MVP's would be one of the ways to do it is to add an exploration bonus.",
            "So we're going to have an empirical model of the world right, and then we're going to add this.",
            "This bonus which decays according to the square root of the number of times you've seen a state.",
            "It turns out this is not the best algorithms class, but it turns out it's good enough.",
            "You could learn to.",
            "You would then start exploring everywhere in body would be incentivized eventually.",
            "Try to make your way across to the other side of these four crackers."
        ],
        [
            "Now this makes sense if your environment is a grid world where each you know each state is a node in the graph, and you can count how many times we've been in each of these states.",
            "Except the problem in is in most games on the Eli, you never experienced the same observation twice, so counting doesn't really make sense."
        ],
        [
            "And as this great number from a Charles Manson hotel, and 90% of singletons in most in most Atari games.",
            "And that's certainly true for Montezuma's Revenge, right?",
            "So counting.",
            "Unique observations is hopeless."
        ],
        [
            "So now that approach would be to say, well, we could try to get a model which would approximately count.",
            "So one way to do this is to start actually with a generative model, right agent of model is one that you can train on images, and then you can use it to generate more images."
        ],
        [
            "Now you can do this.",
            "You could actually if you ask of your model, that also gives you the probability I'm going to call us a density model.",
            "So if you have a density model you can train it and then later say what's the probability of that of that frame here that you're showing me and that is going to give us the probability we call it roll Rolex or roll N of X where N is the number of steps you've seen so.",
            "And the key idea here is to say we're going to try to count using this density model and how we're going to do this.",
            "We're going to assume that a higher density implies that you've seen something."
        ],
        [
            "Often so high density correlate with frequency.",
            "And we're also going to assume that you have a that if your model is a good generative model or good density model, then also it's generalizing in the right way for this domain.",
            "So these are sort of two reasonable assumptions to make.",
            "A density model should generate things that are frequent more often, and it should also produce similar things the right way.",
            "OK, so.",
            "The way we're going to use this density model to count as we're going to say the probability that the the model the general model assigns to specific observation.",
            "Is given to us by a ratio of account and called the pseudo count and a total count total pseudocounts sort of total count.",
            "Total count.",
            "So that's great, but that's two unknowns and one and one equation.",
            "So now we're going to need a second equation to try to get rid of that second order and the key here is to talk about equality, called the recording probability, right row prime of X, and it says row prime of X is going to be the same counts, but we add 1 to each of these and roll prime is going to say is going to be the probability.",
            "Of X after we've trained on it.",
            "So if we have these two equations then we can two equations, two unknowns we can solve for the pseudo count for big in hand to get out a notion of account, and so that's again I'm going to start with a density model and we're going to use it to extract account to know how often we've been to a specific state.",
            "OK.",
            "Yes.",
            "So density model is.",
            "We will train it on images.",
            "And then later you can show it in different image and it says this image is probability.",
            "I'm going to leave it as big as that, but so a general model that auto regressive general model would be a density model.",
            "Pixel CN is a good general density model.",
            "Yes.",
            "Yes, So what is a pseudo path so the pseudo count is the quantity I'm defining by these two equations, so it's the thing specifically the pseudo count.",
            "Is this capital in hand?",
            "Is.",
            "Is the thing that you can imagine that the model is is assigning probability to a frame.",
            "And it's in fact saying I'm assignment probabilities as if I was counting, so it's a pseudo cabinets and that it's not a real account, but it behaves like I'll show you an example.",
            "Maybe they'll make more sense.",
            "Yes.",
            "Why am I doing all of this instead of just multiplying the probability by N?",
            "Right, so it turns out that that doesn't work, so if you were to take right, the natural way would be to say I've trained a trained my model.",
            "It gives you the probability of an image and I want to multiply by the total number of things that I've ever seen.",
            "It turns out that if you do it this way.",
            "Your model generalizes to very large spaces then.",
            "Then the probabilities you'll see will be very small.",
            "They might be over the order two to the minus 20 for example, so you can't directly go right if you look at the general model that assigns Nats, typically the numbers will report for sameness.",
            "We might be, you know, 60 minutes.",
            "That means it's either the minus 60 is the probability of any specific image.",
            "Yes yes sure.",
            "So the first question is, why do we not use the log likelihood and the second one is?",
            "Why do we bother or how do we actually train so the training vital answered with the next slide?",
            "The first one.",
            "What I'll say is if you were just to look at the log likelihood.",
            "This would correspond to.",
            "Well, eventually we want to use this quantity too as account.",
            "Now what we want to do is we want to incentivize the agent to go places where it still learning about the world.",
            "If you were to use the log likelihood, it would fever very rare places where the user likelihood order log likelihood it would favor going back to places that are very rare, which is not actually what we're trying to achieve for.",
            "But as a reader question.",
            "So how do we train and why would we train this thing?",
            "The way to think?"
        ],
        [
            "But it is that before you take an action, you're going to predict you're going to ask your model for what's the probability of this state, and now answering your question actually can do this online.",
            "So now you actually act upon the world, OK?",
            "An annual train and then you so you've actually seen that frame.",
            "You are in that state right now.",
            "You train on your frame and then you query the recording probability and so online.",
            "When you experience A-frame, it makes sense to compute that value at that time because you're going to be training a density model.",
            "As you're learning about the world.",
            "It is effectively a second order gradient step, and if it's a network that's right.",
            "So in fact, so we view this with multiple models.",
            "If the model is not the deep net then then you would be doing something different.",
            "Does it make sense?",
            "So."
        ],
        [
            "In fact, the first thing we've used is in multiple CTS model, which is not a deep net.",
            "All the deep Nets actually perform better at this task, so the CDs file is very simple.",
            "It's an auto regressive model based on.",
            "Variable order Markov predictor.",
            "If you're curious, we can talk more about this after after the talk."
        ],
        [
            "I just want to give you a sense for what you can do with this technique.",
            "So first of all, this is a game called Freeway where the chickens trying to cross the road.",
            "OK, that's very, very simple game.",
            "You get a point every time you go up, and it's a fun game for it's a simple game for general modeling and counting, because there's actually a limited number of states you can be in.",
            "Let me play this again so the chicken actually moves up and down and the cars actually rotate on a Taurus if you will.",
            "So I can give you exactly the number of states that are in this game, it's.",
            "The 20 three I think is how many states there are in this game.",
            "It's great, it's also a lower bound if you ever do general modeling.",
            "It's a.",
            "It's a.",
            "It's an upper bound for it's a lower bound entropy that you can achieve if you were to sample frames uniform.",
            "So one thing we did is we actually."
        ],
        [
            "We ran our this technique just to look at the word pseudocounts look like under game Freeway where we were basically looking at two things that accounts go up linearly.",
            "So if you for example counting whether you're at the start position.",
            "Then it turns out that accounts for sort of progressively the X axis is time for training frames.",
            "the Y axis is the pseudo count that you models actually giving you another experiment we did is.",
            "We said I want you to count whenever the agent reaches the top right when it crosses the road.",
            "But now I'm actually going to make the policy sometimes stop.",
            "And stay at the bottom and sometimes go forward.",
            "And so for a while.",
            "You're not actually increasing these counts and it also shows up in the model that the being at the top has lower pseudocounts than being at the bottom, which is exactly what you want, right?",
            "So now we have this measure.",
            "We can say this state.",
            "We've seen less frequently than this other state.",
            "So."
        ],
        [
            "So what can you do with this very simple you can take.",
            "You can take this exploration bonus algorithm and you can replace the real count of a state, which would be really hard to get under Terry by a pseudo components.",
            "So we've done all this work.",
            "To get a quantity which we now finally turned into just a reward, an extra award for going places that you haven't seen in the past.",
            "OK, and it turns out that if you know cross our fingers, we may be able to explore better and incentivize our agent to reach hard to reach places with this kind of bonus."
        ],
        [
            "So rather than show you learning curves, I want to actually show you what this agencies OK, so I'm going to show you a bit of a command center view of the agent.",
            "This is actually not.",
            "This is the agent with another CD S model, but the later pixels and model behaviors very qualitatively similar under top left you have the actual frame that's experienced by the agent.",
            "Bottom Left is the value function.",
            "If you haven't seen a value function before as the agent is playing the game, you'll see what it looks like then.",
            "Top right is actually this.",
            "This intrinsic reward, that trained agent is receiving.",
            "It turns out with the Pixel stand model, these rewards don't go to 0.",
            "But you'll see that they are higher in places that are further away.",
            "And finally I want to show you the bottom.",
            "The bottom is actually bottom right is fewer pixel by pixel.",
            "Where the agent is finding novel things.",
            "So, So what is actually unknown to the agent?",
            "So in this work we could actually get this down to the pixel level, so let me get started red and the bottom right graph is novel.",
            "Blue is actually empty novel because you could have negative.",
            "Use the probability and So what you'll see is the agent.",
            "The agent is now.",
            "This is an agent that's trying to play this game and it gets spikes of reward and places where maybe it hasn't been as often.",
            "It learns to do these kind of strange behaviors where it's very excited by room transitions because these are a bit a bit more rare.",
            "But eventually it's going to make its way.",
            "It's going to make its way to the to the through these blue barriers and then go down.",
            "And then successfully play this game and I'll show you one more thing about these agents.",
            "I mentioned the torch earlier these SRL agents, so they don't actually need to see to play these games.",
            "So the agent has learned that if you go if you go into that dark room on the right hand side, there's some gems you can collect and you can collect the gems where you can see whether gems or not, and so the agent decided.",
            "Well, I'll just.",
            "I'll just go and collect these gems, and it looks at this court.",
            "You know, when it's done collecting the gems.",
            "Maybe that's an example of overfitting.",
            "To a specific game you tell me.",
            "OK, so."
        ],
        [
            "Another way we can visualize with this agent is doing is to actually look at the map of the environment.",
            "We start actually turns out in Montezuma's Revenge.",
            "It's a pyramid shape map you started atop, and you're trying to reach the bottom left corner of the pyramid, and so I just want to show you what happens if you use DQN.",
            "OK or decline with this pseudo camp owners.",
            "Camp.",
            "And we're going to show you is going to highlight the rooms at the agent has visited in the bottom video, and they're going to be grayed out if the agent is no longer visiting these rooms.",
            "But we're going to remain highlighted, so let me start a video.",
            "You'll see.",
            "It's sort of very clear that this bonus based agent.",
            "It's very rapidly person goes left.",
            "It checks out the left area, it says OK, there's actually no reward on the left hand side that can reach.",
            "Now it moves to the right hand side and very quickly discovers to maximize its reward.",
            "And so at the counter is actually a number of millions of frames.",
            "I don't have the numbers with me.",
            "I think I think 200 million frames is 38 days of game playing, so really, that's a lot of time for the Q and Agent to learn to play this game.",
            "And still it really can't get anywhere.",
            "So of course we can."
        ],
        [
            "Blood learning curves.",
            "It turns out that.",
            "Pseudocode algorithm performs very well compared to DQM, as you might expect.",
            "If you measure it in terms of score.",
            "And that's that, was also better than another method called the optimistic initialization method.",
            "OK.",
            "So."
        ],
        [
            "One thing I want to point out before wrap this up is very briefly to say that is really two.",
            "I really talked about the pseudo count part, but there's another aspect of this project which which is very interesting, which is that you miss comes up all the time in reinforcement learning.",
            "We need to assign credit quickly.",
            "OK, so I'm sure which told you about TD, Lambda and Q&A on.",
            "On Wednesday, right?",
            "So with these three, these Lambda methods were trying to propagate far away to the errors back to the current state.",
            "So what we did here is we actually had a poor man's version of this where we mixed in the one Step TD error with the Monte Carlo updates and we call this a mixed Monte Carlo update and the combination of these two things.",
            "Actually leads to much faster credit assignment and is actually necessary to learn well and these games.",
            "So for example, if you look at other games from the Terrace suite."
        ],
        [
            "You can find out that that this mix Montecarlo update is sometimes the sole responsible for better performance, but then there are games for which you really need both.",
            "You need sudo count bonus and you need the money intermix medical update or some sort of longer term credit assignment.",
            "One of these games is called Private I.",
            "It's an adventure style game.",
            "One man."
        ],
        [
            "Thing I'll say is this is a full report with your coast hrobsky where we actually removed the extrinsic rewards altogether from the equation.",
            "So remember that equation with the with the pseudo can bonus.",
            "So we also remove these bonuses and found that if you up the gain high enough on your on your intrinsic bonus, then you can actually play Montezuma's Revenge with no reward.",
            "But you still get the learning curve which is pretty neat.",
            "So we can talk more about this at lunchtime if you want."
        ],
        [
            "OK, so that's going to be the closer the end of my talk.",
            "I actually want to go back to this idea of challenge domains and serve as the question where are we at now with with the Eli and the context of deeper methods?",
            "Now this this slide is a bit.",
            "I made it in September.",
            "Things actually moving rapidly, but I think some of the take home message is still still correct.",
            "So I'm going to take the timeline from 1977, which is when the target 2600 was released to 1989, which is the last game we have in the daily.",
            "Sweet and."
        ],
        [
            "I can actually plot all of the games with them, all of them on this axis, and in September 2017 I could label these according to three categories.",
            "Games where we were superhuman games where we had was going to call the scoring exploit.",
            "So we collect a lot of points, but we're not really playing this game the right way.",
            "And also games where the agents were subhuman and that's where in the last almost year.",
            "Now things have moved very rapidly.",
            "But now if you if you lead us down, this is giving you a sense of the progress of our best of our best Atari DRL agents.",
            "And what's very interesting is if you look at the first part of that graph of the 1980 were really, you know, I think I think the last game is actually falling now, so we're really superhuman in all of these games, we can play these games really, really well.",
            "And now you move into the 19 in the early 1980s, and already that proportion drops and there's there's more games also, where we're collecting a lot of points, but Nancy playing the game the way human would have played it.",
            "And then finally female entity link or mid mid 80s.",
            "There's a lot of less data, but still, there's a clear trend that there are fewer games or superhuman.",
            "So one way to think about it is as we moving forward in time with these video games, we're seeing our agents performance become less and less.",
            "Good for the same level of architecture, the same level of compute and all of these anatomy is very telling that in some sense you know the agents that we have that we had in September 2017.",
            "Maybe the single GPU agents were sort of stuck in 1983.",
            "And so it begs the question also of saying, well, what next for deeper out and then you know what's the next thing after the alley.",
            "So on this note, I'm going to close and get leave some time for questions.",
            "Thank you very much.",
            "How would Google Self driving cars play these games or compared?",
            "I know very little about the self driving cars so I couldn't even begin to answer that question.",
            "Generally speaking, I think we're still far from general competency, and in the sense that we want to design algorithms that work as well as they can on a specific domain.",
            "Applied to yes.",
            "The results you showed with Montezuma's Revenge are those still with the architecture that takes in only the four previous frames.",
            "Yes, and it's actually a very good question, so those results except for the learning rule and the bonus.",
            "It was exactly the same architecture as before.",
            "And so it's been a common theme throughout to say, well, whenever we have this agent architecture, I just change one piece at a time and see what happens.",
            "Now maybe what you're asking is, could we do better if we change that part too?",
            "Well, maybe that's something you should look into.",
            "So if I'm not wrong, the first equation trying to solve both for the expectation right?",
            "Imagine for expectation of reward in the future and then you can put it into an equation for the distributions.",
            "No.",
            "If all that is the expectation value that I'm going to have more freedom for my distribution, so can I add another distribution which has the means 0 for instance, but helps me with variance or what not gonna do that extra freedom.",
            "Help me so I'm not sure if I understand the question.",
            "If you saying could we instead of predicting the whole distribution, predict the mean and then also predict the distribution with mean zero to add on top of it?",
            "So I have this extra stochastic variable I could add right, but this sooner or later you have, so I don't want to change them in because I need that, but I could use something that helps me build something later during the training, 'cause that is extra degree freedom, right?",
            "Would that be useful or would it be useful to look at the rest of the distribution of what you're asking?",
            "It should be if you wanted to be risk aware, for example, so there's this good paper by by actually implicit quantum networks paper looks at this question, where they looked at different risk profiles.",
            "Now the field of Risk Aware RL is actually quite rich and purposefully.",
            "We weren't really looking at this with the original paper.",
            "Risk Aware RL is.",
            "Brings in extra complications.",
            "So for example, if you want to be.",
            "You know, if you want to add variance constraints, there might not be an easy solution.",
            "You might be hard actually an NP hard to solve for the maximize expectation.",
            "Subjective variance constraints.",
            "Hi so.",
            "I have a question like does the idea of incentivising your agent to explore hurt performance in the earlier games, in which you had like more dense rewards?",
            "Like what is there a trade off and?",
            "Is there a trade off to explore more?",
            "The answer has to be, yes, right?",
            "If you can take a single step and get maximum reward or that other agent is going to go off in a tangent and never get it reward.",
            "In many ways we could hope is an adaptive algorithm that says if after a while I haven't explored, maybe I go back to this first.",
            "This first piece of reward.",
            "We're going to be able to release to have some sort of regret bound that says you have to trade off unless you have more information about the world.",
            "Certainly in the case of Atari, if you add a very high intrinsic reward to palm, it might not actually learn to play the game.",
            "So regarding the question about your learning distribution, but you are only using the expectation, so do you think that this advantage that you seen performances the artifact of using this neural network and deep neural networks?",
            "Meaning that if you take this out and replace it with something else, do we still see the same improvement?",
            "Has somebody tried this?",
            "So the answers I so I can actually give you the answer?",
            "The answer is no.",
            "If you look at a tablet setting I can we have a proof Now that shows that.",
            "And set up a coupling experiment where side-by-side you're learning the distribution or the Q function with the same random events, and then there is absolutely.",
            "I mean, it's obviously indicates expectations remain the same.",
            "More surprising is, even if you have something like linear approximation.",
            "It should not.",
            "Actually, it shouldn't give you an advantage, and it might even hurt.",
            "So so so the theoretical side of that work.",
            "It was published what was presented in a workshop paper at ACL.",
            "I can I can send it to you.",
            "So basically the magic is this Norton network that I at least I would say 95% of the magic is inside the neural network, thank you.",
            "We're out of time, so thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, thank you first time Masood Doina and Joel for the invitation.",
                    "label": 0
                },
                {
                    "sent": "This is real treat.",
                    "label": 0
                },
                {
                    "sent": "I think that summer school has been fantastic.",
                    "label": 0
                },
                {
                    "sent": "I've certainly enjoyed it.",
                    "label": 0
                },
                {
                    "sent": "So deep reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a term that's fairly recent.",
                    "label": 0
                },
                {
                    "sent": "The history of AI statics.",
                    "label": 0
                },
                {
                    "sent": "So what I thought I'd do today is, rather than cover the whole gamut of deep reinforcement learning, which is actually quite big now give you a flavor of what I think it is and where I think the challenges are in the coming years.",
                    "label": 0
                },
                {
                    "sent": "And well, to get started deep.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And reinforcement learning, right?",
                    "label": 0
                },
                {
                    "sent": "This is the theme of the summer school.",
                    "label": 0
                },
                {
                    "sent": "Deep learning on one end, neural networks and then a reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Trying to learn to make the right decisions in an environment.",
                    "label": 0
                },
                {
                    "sent": "Putting neural networks inside an RL agent is not something that's necessary in you, and I'm only going to go back now to the 1990s, but really it dates back from even longer than that.",
                    "label": 0
                },
                {
                    "sent": "People have been using deep Nets to learn value functions, return policies for quite a long time.",
                    "label": 0
                },
                {
                    "sent": "There was backgammon with Jerry.",
                    "label": 0
                },
                {
                    "sent": "Pterosaurs work there was the elevator control paper by Christ in Bartow.",
                    "label": 0
                },
                {
                    "sent": "There was flying helicopter that also involved in your network.",
                    "label": 0
                },
                {
                    "sent": "I actually early in Mycareer work on transfer learning in Tic Tac toe using these networks called cascade correlation neural networks.",
                    "label": 0
                },
                {
                    "sent": "This was really fun.",
                    "label": 0
                },
                {
                    "sent": "And of course, since then, well, you know we've had some major successes with using neural networks, which reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So if it's not new, then you know what does the term mean in in modern terms, right?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What's new in the context of what's happening today?",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I want to say is I think actually Deep RL is more than just a combination of putting a network inside an environment.",
                    "label": 0
                },
                {
                    "sent": "It's really about the kind of domains that we now look at by the kind of challenge domains that we might be interested in solving, and it's also the algorithms that we develop to make these.",
                    "label": 0
                },
                {
                    "sent": "Networks operate smoothly or robustly inside the enforcement learning environments, and I do think that those two things are fairly new.",
                    "label": 0
                },
                {
                    "sent": "But if you go back, for example, divorce paper.",
                    "label": 0
                },
                {
                    "sent": "It was TD Lambda.",
                    "label": 0
                },
                {
                    "sent": "It was very much the what I would might say.",
                    "label": 0
                },
                {
                    "sent": "The vanilla algorithm, you know, plug it into a network and it was no replay memory.",
                    "label": 0
                },
                {
                    "sent": "There was no target network per say.",
                    "label": 0
                },
                {
                    "sent": "So big big difference has been to really think hard as to what the challenges are when we combine neural networks with our environments and how do we address these challenges.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this list here is by no means complete, but here's a sample of the kind of challenges that arise in deep reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the training data is not ID, so we know how to train deep Nets when data is drawn from a data set or from a stream of ID data.",
                    "label": 0
                },
                {
                    "sent": "But now when we're dealing with reinforcement learning as their policy changes, we're going to see different parts of the state space, and that's actually quite difficult to handle with classical training methods for deep networks.",
                    "label": 0
                },
                {
                    "sent": "Another thing that is dear to my heart is in talked about this and Emma talked about.",
                    "label": 0
                },
                {
                    "sent": "This is the difficulty to get confidence intervals.",
                    "label": 0
                },
                {
                    "sent": "So how good is there actually prediction?",
                    "label": 0
                },
                {
                    "sent": "This matters too deep learning this matters to DRL because we're going to want to exploration or maybe want to take action safely, right?",
                    "label": 0
                },
                {
                    "sent": "So that's also a big challenge.",
                    "label": 0
                },
                {
                    "sent": "And then there's there's a variety of a certain more ordered me questions.",
                    "label": 0
                },
                {
                    "sent": "Example the We know that combining our methods with deep networks might lead to divergent.",
                    "label": 0
                },
                {
                    "sent": "Often though, the state space is unknown, so we don't even know what that state space looks like.",
                    "label": 0
                },
                {
                    "sent": "If you're playing, for example, an Atari game.",
                    "label": 0
                },
                {
                    "sent": "And the last one actually is actually very interesting, but I think so far what we've seen is mostly model free methods working really well and often requiring simulation.",
                    "label": 0
                },
                {
                    "sent": "So there's clearly a big challenge in making model based methods or sample efficient methods work in deeper, and it's very exciting to see all the work that's happening in meta PRL.",
                    "label": 0
                },
                {
                    "sent": "Few shot learning.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so what's the challenge that I've been the most involved in?",
                    "label": 0
                },
                {
                    "sent": "It's the arcade learning environment, the arcade learning environment is actually 10 years old.",
                    "label": 0
                },
                {
                    "sent": "This year it started on the beaches of Sunny Barbados at the Reinforcement Workshop in 2008 there was a paper by a researcher called Colours colours Duke, who said we should use pitfall as a domain for testing our agents and this populated Michael bowling and you are not as master student.",
                    "label": 0
                },
                {
                    "sent": "I'm started a project.",
                    "label": 0
                },
                {
                    "sent": "I came in a bit later in 2011 and then eventually this led to the release of a Journal paper.",
                    "label": 0
                },
                {
                    "sent": "The release of an open source.",
                    "label": 0
                },
                {
                    "sent": "That piece of software called the Arcade Learning Environment and then from the famous Nature Paper Dicul Nature paper.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if you've never seen if you've never seen an Atari game who has never seen an Atari game, obviously entire game.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's a better question.",
                    "label": 0
                },
                {
                    "sent": "There we go.",
                    "label": 0
                },
                {
                    "sent": "Well, you've seen this before, so this is a game called Pong Pong is very, very simple game that you can play on the Atari 2600 you have a green paddle on the right hand side, and you're trying to prevent the ball from going to the net at the same time.",
                    "label": 0
                },
                {
                    "sent": "You're also trying to score points on the.",
                    "label": 0
                },
                {
                    "sent": "Orange paddle by, as you saw, just getting the ball across across the net.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is probably one of the simplest games we think about, but as you'll see, it already has some complexities baked in.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we talk about the Ark alone environment, it's talking about the Atari 2600.",
                    "label": 0
                },
                {
                    "sent": "So what's the the basic setup here?",
                    "label": 0
                },
                {
                    "sent": "Is we have this console which is actually from 1977, which we're going to be emulating and there are in fact 15 actions you can take.",
                    "label": 0
                },
                {
                    "sent": "So up left, down, right and the fire button.",
                    "label": 0
                },
                {
                    "sent": "If you think the cross product of these you get 18.",
                    "label": 0
                },
                {
                    "sent": "And then there's actually the Atari memory is actually 128 bytes, so it's actually a very small machine by comparison to most modern computers, right?",
                    "label": 0
                },
                {
                    "sent": "But the big challenges that we have.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, even 128 bytes if you were to consider every state as a possible bit configuration, that's still to 1000, right?",
                    "label": 0
                },
                {
                    "sent": "So there's still quite a bit.",
                    "label": 0
                },
                {
                    "sent": "Quite a few states that you could be in.",
                    "label": 0
                },
                {
                    "sent": "But if you instead look at the image that you're getting from the emulator, then that image is roughly 33,000 pixels, and it's actually been given to you at 60 Hertz.",
                    "label": 0
                },
                {
                    "sent": "So that in comparison to many other oral environments at the time, was quite daunting.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what is the Ellie D Aily is actually it's a rapper or an interface between the Stella Emulator which emulates Atari 2600 games and what we would consider Classic RL interface where you have an agent in an environment receiving observations, rewards, taking actions.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne, I know that many of you of course are using German to, but that's a gem you could think of it as an extra layer, an extra RL layer.",
                    "label": 0
                },
                {
                    "sent": "Alright, so formally speaking, we have this observation.",
                    "label": 0
                },
                {
                    "sent": "We've really looked at two different kinds of observations.",
                    "label": 0
                },
                {
                    "sent": "Chiefly, it's been images because images are very appealing, but also the 80 provides you with the RAM state, and in fact, to this day I think image based methods perform much better at playing Atari games and RAM based methods.",
                    "label": 0
                },
                {
                    "sent": "And when you think about it, sort of makes sense because the information is highly compacted.",
                    "label": 0
                },
                {
                    "sent": "If you look at the ram right, every bit matters, whereas maybe you have a bit more fudge if you're looking at images.",
                    "label": 0
                },
                {
                    "sent": "As I said, the action is joystick motions and.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think she did most interesting thing about Haley in many ways is the reward function.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "At the time it was I think customer to design A reward function that would say you look at every game you look at a game and say my goal is to achieve XL define reward function to be to be why.",
                    "label": 0
                },
                {
                    "sent": "But we said well we have this Atari system.",
                    "label": 0
                },
                {
                    "sent": "There will be a lot of games that we can actually play with.",
                    "label": 0
                },
                {
                    "sent": "And we want to be ignostic to how to how the actual game plays and how humans would play this game.",
                    "label": 0
                },
                {
                    "sent": "So we're going to define the reward function to be discord differential.",
                    "label": 0
                },
                {
                    "sent": "So basically you have almost all of these games ever score, and in fact all the games that daily supports have a score and we need to find the reward to be the change from one time step to the next, and your reward in your school.",
                    "label": 0
                },
                {
                    "sent": "That means you could have negative Warden games where you lose points and you will have positive rewards in games where you always make points.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I sort of hinted at already, the other interesting aspect of the early is that it provides you not just with one environment, but really provides you with a ton of environment.",
                    "label": 0
                },
                {
                    "sent": "I think there's something like 900 recorded games on Wikipedia that you could play through the guitar.",
                    "label": 0
                },
                {
                    "sent": "Now early on we filtered these games down to roughly 60 now.",
                    "label": 0
                },
                {
                    "sent": "And these are the 60 Canonical games.",
                    "label": 0
                },
                {
                    "sent": "If you will, of the early where, where daily provides you with with an interface to these games and that was very exciting at the time, I think still is today, but it's rare to have a platform that will provide you with such diversity.",
                    "label": 0
                },
                {
                    "sent": "For free or out of the box.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what does it matter?",
                    "label": 0
                },
                {
                    "sent": "It matters because one thing we wanted to address then was this idea of general competency.",
                    "label": 0
                },
                {
                    "sent": "It's actually interesting that when I was preparing this talk, I realized how how obvious this ideas become now.",
                    "label": 0
                },
                {
                    "sent": "But you know, at the time it wasn't.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "That's not good.",
                    "label": 0
                },
                {
                    "sent": "Are you guys seeing things Flash?",
                    "label": 0
                },
                {
                    "sent": "No, OK, good.",
                    "label": 0
                },
                {
                    "sent": "It's just a monitor.",
                    "label": 0
                },
                {
                    "sent": "So what is general competency?",
                    "label": 0
                },
                {
                    "sent": "Well, let me give you an example with.",
                    "label": 0
                },
                {
                    "sent": "You know with something like a self driving car, it might be that we want.",
                    "label": 0
                },
                {
                    "sent": "The same agent running the same piece of code to operate under different conditions OK?",
                    "label": 0
                },
                {
                    "sent": "So maybe you know, maybe you want your agent to perform well in mountainous terrains.",
                    "label": 0
                },
                {
                    "sent": "Maybe you want to perform well in snowy terrains, maybe in crowded city environments.",
                    "label": 0
                },
                {
                    "sent": "OK, with the idea that there's something appealing saying we have a learning algorithm, it should be baked into a single piece of code.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a position to see narrow competency where we would have a specific piece of code for each environment, right?",
                    "label": 0
                },
                {
                    "sent": "So now imagine if we had 60 pieces of code for 60 different environments.",
                    "label": 0
                },
                {
                    "sent": "That would be a bit more disappointing, right?",
                    "label": 0
                },
                {
                    "sent": "So this was this was a goal of the project and this is what the aliens all about.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why is the alien interesting challenge domain for deeper, well, well, it's exactly, I think because it pushes for that idea.",
                    "label": 0
                },
                {
                    "sent": "General competency and there's really three key characteristics that make it interesting beyond the perceptual level.",
                    "label": 0
                },
                {
                    "sent": "It's as I said, it's a diverse set.",
                    "label": 0
                },
                {
                    "sent": "It's interesting, and it's also independent.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me go very quickly over what these three key words mean.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reverse is is sort of the most important one in many ways.",
                    "label": 0
                },
                {
                    "sent": "Say 10 years ago and you know to this day we used the mountain current environment to evaluate our reinforcement learning agents.",
                    "label": 0
                },
                {
                    "sent": "And in many ways, Mountain car is a bit of the M list of RL.",
                    "label": 0
                },
                {
                    "sent": "It's really supported a lot of good results, but it's also a fairly simple domain.",
                    "label": 0
                },
                {
                    "sent": "It's 2 dimensional continuous continuous space domain OK?",
                    "label": 0
                },
                {
                    "sent": "And So what?",
                    "label": 0
                },
                {
                    "sent": "You know what you might imagine is that overtime researchers developed methods for mountain car.",
                    "label": 0
                },
                {
                    "sent": "The first method was so, so in the second method came around was slightly better and then suddenly you have many many many agents that have been optimized for mountain power.",
                    "label": 0
                },
                {
                    "sent": "And this was actually flagged in a paper based human rights and colleagues where they said we review that reinforcement learning is pretty vulnerable to environment over fitting and then they have the solution, which was to generalize these these environments and I think this is actually incredibly relevant today where we are.",
                    "label": 0
                },
                {
                    "sent": "I think the DRL community is again facing the same kind of questions, but at the time it was very clear that say if we evaluate our agents and mountain car.",
                    "label": 0
                },
                {
                    "sent": "We might be missing out, or it may be overfitting tonight.",
                    "label": 0
                },
                {
                    "sent": "And so being there.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So having a large set of environment so we could play with was wasn't in fact at a sort of a solution to this this question.",
                    "label": 0
                },
                {
                    "sent": "And actually we went to separate and we said we should in fact have.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Five training games.",
                    "label": 0
                },
                {
                    "sent": "And then the rest would be test, right?",
                    "label": 0
                },
                {
                    "sent": "So you don't get the optimizing hyperparameters on all games, but only a subset and spirit of general competency.",
                    "label": 0
                },
                {
                    "sent": "Now you can carry over that code and those hyperparameters to to the rest of the game to 55 remaining games, and then report your testing school.",
                    "label": 0
                },
                {
                    "sent": "And I think this is one of the things that has made the early so successful.",
                    "label": 0
                },
                {
                    "sent": "Is that really people have played by these rules and reported if you will generalization scores?",
                    "label": 0
                },
                {
                    "sent": "OK, OK, so that's that's that's one part of ITL is interesting 50 parallel.",
                    "label": 0
                },
                {
                    "sent": "Another aspect is that the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Domain itself is interesting to people.",
                    "label": 0
                },
                {
                    "sent": "People play and have played these games.",
                    "label": 0
                },
                {
                    "sent": "So for example, let me show you at the bottom you have a game called Pitfall another game called Seaquest, and both both of these games are.",
                    "label": 0
                },
                {
                    "sent": "Highly.",
                    "label": 0
                },
                {
                    "sent": "Looking for there are highly entertaining.",
                    "label": 0
                },
                {
                    "sent": "They're very humans, enjoy playing these games.",
                    "label": 0
                },
                {
                    "sent": "They enjoy finding out where the interactions are in these games, exploring the world, you know, catching the fish, saving the swimmers.",
                    "label": 0
                },
                {
                    "sent": "There's really a sense that humans find it interesting to watch to watch agents involved in this in this scenario, and also to play the game themselves.",
                    "label": 0
                },
                {
                    "sent": "And that's really not true of every benchmark.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to get humans to play mountain car, for example.",
                    "label": 0
                },
                {
                    "sent": "Another thing that's important.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this this idea of independence and the way I can best describe this is to say that these games were designed by people and four people as opposed to design for researchers with a specific research agenda and that actually mattered quite a bit in the design of the design of the early, because we were able to say, let's take, let's take games that.",
                    "label": 0
                },
                {
                    "sent": "That were in this case design 30 years ago 40 years ago and treat them as an apartment.",
                    "label": 0
                },
                {
                    "sent": "Where this independence means is that we have to deal with whatever.",
                    "label": 0
                },
                {
                    "sent": "Biases are quirks of the environment where put in OK.",
                    "label": 0
                },
                {
                    "sent": "So one question often get is where is the XY position of the agent in this in this domain?",
                    "label": 0
                },
                {
                    "sent": "And my answer is, well, this is not provided to humans, so why should we be using this XY coordinate, right?",
                    "label": 0
                },
                {
                    "sent": "And so this independence was very important, because it it really forces us to deal with with problems that maybe.",
                    "label": 0
                },
                {
                    "sent": "As researchers, we haven't even think of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's going to be a theme through the rest of this talk.",
                    "label": 0
                },
                {
                    "sent": "Also so diverse, interesting and independent.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give you a sense for early attempts before I move into the deep networks part early on, some of the some of the results we had, we tried something called a basic features which were basically.",
                    "label": 0
                },
                {
                    "sent": "Discretizing the screen and encoding the presence of covers and this was done actually mostly in in three bit color modes, because there is not enough memory for the 8 bit color, so 3 bit color mode encode the presence of colors and then define a feature set with which we could do linear function approximation.",
                    "label": 0
                },
                {
                    "sent": "We did the same thing with with Randy to.",
                    "label": 0
                },
                {
                    "sent": "We did the same thing by something called locally sensitive hashing on pixels, and last but not least actually there was there was a significant portion of the.",
                    "label": 0
                },
                {
                    "sent": "Even the devs research that was actually spent detecting object, so this album is called Disco OK and the idea was simple.",
                    "label": 0
                },
                {
                    "sent": "You would subtract the background you would extract using a vision technique.",
                    "label": 0
                },
                {
                    "sent": "You would extract all of the shapes that were present at a contiguous blobs.",
                    "label": 0
                },
                {
                    "sent": "After extracting the background and then you would try to match these two existing labels.",
                    "label": 0
                },
                {
                    "sent": "You see I've seen a yellow submarine like this before.",
                    "label": 0
                },
                {
                    "sent": "This must be an object of class.",
                    "label": 0
                },
                {
                    "sent": "Class zero, you've seen a fish like this is a Class 1 fish.",
                    "label": 0
                },
                {
                    "sent": "It turns out that's incredibly hard to do, or at least if you if you don't spend too much time on it.",
                    "label": 0
                },
                {
                    "sent": "And in.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we measured the scores of this disco algorithm, it was pretty dismal and it's sort of telling sign that.",
                    "label": 0
                },
                {
                    "sent": "Mapping perception to send object representation is not a trivial task.",
                    "label": 0
                },
                {
                    "sent": "We had we.",
                    "label": 0
                },
                {
                    "sent": "We sort of value these different algorithms.",
                    "label": 0
                },
                {
                    "sent": "One thing I want to point out is we have this thing called the score distribution, which effectively you're going to.",
                    "label": 0
                },
                {
                    "sent": "It's like a community distribution function for scores and.",
                    "label": 0
                },
                {
                    "sent": "Just last, I think six months ago or so, Ben Rector post talking about performance profiles, which is exactly what's a better version of this idea, so it's quite nice to see that in fact this idea score distribution is not too crazy.",
                    "label": 0
                },
                {
                    "sent": "It's about impossible to Barstow, but early on we said, well, if you want to compare 60 different games, what can you do?",
                    "label": 0
                },
                {
                    "sent": "You can plug them in that score distribution and then lower curve would correspond to a better agent or the other way round.",
                    "label": 0
                },
                {
                    "sent": "Higher curve is a better age.",
                    "label": 0
                },
                {
                    "sent": "Another interesting thing that happens is if you have 60 different games you need to aggregate the numbers and this is what's been shown here.",
                    "label": 0
                },
                {
                    "sent": "You could take an average.",
                    "label": 0
                },
                {
                    "sent": "You could take a median.",
                    "label": 0
                },
                {
                    "sent": "The scores are actually on different scales, so you might have to normalize first right?",
                    "label": 0
                },
                {
                    "sent": "So for example, if you're playing, you see the score in this game called Seaquest Now and the score goes up by 20 points.",
                    "label": 0
                },
                {
                    "sent": "Every time you do something, if instead you're playing phone, discord goes up by one point.",
                    "label": 0
                },
                {
                    "sent": "So how do you compare with 60 different games?",
                    "label": 0
                },
                {
                    "sent": "How do you compare how well these games are?",
                    "label": 0
                },
                {
                    "sent": "There's been a few solutions proposed, unsurprisingly.",
                    "label": 0
                },
                {
                    "sent": "Of course, the way you normalize is going to change the way you report your results.",
                    "label": 0
                },
                {
                    "sent": "And then you could take the average with the median.",
                    "label": 0
                },
                {
                    "sent": "It turns out the average as we all know is heavily dominated by outliers, and it remains true today.",
                    "label": 0
                },
                {
                    "sent": "So median scores are definitely much better measure of performance and average scores.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this was this was the ellee up to about 2014 and then the deep Q networks idea came around.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's what's the Q&Q and is taking a deep network and using it as the value function for a value based reinforcement learning agent.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is where the network actually looks like.",
                    "label": 0
                },
                {
                    "sent": "It's going to feed in a stack of four images.",
                    "label": 0
                },
                {
                    "sent": "To the to the network and then run them through a series of convolutional layers and produce a hidden layer which is now treated like a linear representation and we do.",
                    "label": 0
                },
                {
                    "sent": "The last layer is linear layer and we have put the Q value so actually.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's interesting is document is not just this network.",
                    "label": 0
                },
                {
                    "sent": "Decline is really an agent architecture, so in the middle you have the network, but then of course you have a preprocessing stack that takes in the raw raw frames if you will given to you by the early and then you there's a series of preprocessing operations that are applied.",
                    "label": 0
                },
                {
                    "sent": "There's Gray scaling, there's frame pooling and then finally downsampling to stacking part itself happens.",
                    "label": 0
                },
                {
                    "sent": "And then you feed this.",
                    "label": 0
                },
                {
                    "sent": "You feed this to the network so you don't even need to network right away, so 50 take actions you're going to remember all of these frames you're going to put them in a replay memory which stores old trend transitions.",
                    "label": 0
                },
                {
                    "sent": "And then what's going to happen is that the network outputs build the policy in this train by this replay memory.",
                    "label": 0
                },
                {
                    "sent": "So the another part.",
                    "label": 0
                },
                {
                    "sent": "Another important part of the when is the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "How do you actually make use of that experience to training network?",
                    "label": 0
                },
                {
                    "sent": "And a big part of the original agent which has somehow it's almost vestigal has stuck around.",
                    "label": 0
                },
                {
                    "sent": "Is this target network?",
                    "label": 0
                },
                {
                    "sent": "The Target Network does is it sort of.",
                    "label": 0
                },
                {
                    "sent": "It stabilizes the learning by giving you a nicer loss.",
                    "label": 0
                },
                {
                    "sent": "So really, I like to think of DQ and in the context of deeper L as also having brought this around this idea of the whole agent architecture as opposed to value function that produces a policy.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do you actually train this network?",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "You have declines.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithm was basically minimizing the squared TD error.",
                    "label": 0
                },
                {
                    "sent": "With a subtlety and you can see it on the screen.",
                    "label": 0
                },
                {
                    "sent": "There's a tilt on the Q value at the Max and what it's saying is we're going to use the Q value from the target network.",
                    "label": 0
                },
                {
                    "sent": "In Tensorflow term there would be almost like doing a subgradient, but from the previous network and we're going to use that next value because it stabilizes learning.",
                    "label": 0
                },
                {
                    "sent": "And it's also because it's like doing a TD update as opposed to doing a bellman visual minimization.",
                    "label": 0
                },
                {
                    "sent": "And so then you define this loss function to squared loss, the spreadsheet here, and then you just do gradient descent.",
                    "label": 0
                },
                {
                    "sent": "What's your favorite auto differentiation tool to learn to minimize that PDF?",
                    "label": 0
                },
                {
                    "sent": "Basically try to match.",
                    "label": 0
                },
                {
                    "sent": "There the value function Lipsy.",
                    "label": 0
                },
                {
                    "sent": "And so this was a.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Surprisingly successful, this is work.",
                    "label": 0
                },
                {
                    "sent": "This is a video of a game called Space Invaders that is actually slightly later work.",
                    "label": 0
                },
                {
                    "sent": "On on something called persistent algorithms, but just to give you a sense for how how impressive these agents are playing the game, there's really a sense of of smooth control.",
                    "label": 0
                },
                {
                    "sent": "But if you look at these agents and you watch them play this game, there's a sense that they know what's going on.",
                    "label": 0
                },
                {
                    "sent": "They can predict the motion in this case of the invaders, and then they can prevent prevent these invaders were coming down.",
                    "label": 0
                },
                {
                    "sent": "And so there was it.",
                    "label": 0
                },
                {
                    "sent": "He can Dodge the bullets also.",
                    "label": 0
                },
                {
                    "sent": "So really this technique.",
                    "label": 0
                },
                {
                    "sent": "Led to significant improvements in performance on daily.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the agent architecture, yes?",
                    "label": 0
                },
                {
                    "sent": "What's the 80 behind grey scaling?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I think it makes your input vector smaller.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "It turns out so it may be piece of anecdote.",
                    "label": 0
                },
                {
                    "sent": "It turns out that.",
                    "label": 0
                },
                {
                    "sent": "Atari is $128.00.",
                    "label": 0
                },
                {
                    "sent": "It has underneath it called.",
                    "label": 0
                },
                {
                    "sent": "It's about 7 bits but the because it was designed for both color TV's and black and white TV's the same bit sets will allow you to play the game in black and white to play the game in color.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that is a mapping is a direct mapping between color pixels and luminance.",
                    "label": 0
                },
                {
                    "sent": "You lose a few things, but as a human you would play these games with these easy so the Gray scaling is just a way to reduce the input space without losing anything.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right, so you've probably seen in some of these games.",
                    "label": 0
                },
                {
                    "sent": "Imagine you're playing Palm right and you have the ball coming at you.",
                    "label": 0
                },
                {
                    "sent": "If you just look at the last frame, you don't actually know which way the ball is coming.",
                    "label": 0
                },
                {
                    "sent": "Is it going towards you?",
                    "label": 0
                },
                {
                    "sent": "Is it leaving?",
                    "label": 0
                },
                {
                    "sent": "To go towards the other so by by stacking these frames, what you're doing is effectively giving yourself.",
                    "label": 0
                },
                {
                    "sent": "We're learning about the dynamics first, so you can learn now about dynamic so you can see the ball was one pixel away the previous time step, so I can learn the direction of the model.",
                    "label": 0
                },
                {
                    "sent": "I mean, and more generally, we could say this is a way to get around some of the partial observability that we see in Atari games, and in fact, if you just use one frame, the performance is much worse.",
                    "label": 0
                },
                {
                    "sent": "Now I don't know if anybody is using two or three.",
                    "label": 0
                },
                {
                    "sent": "Four seems like a convenient number.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should have repeated questions.",
                    "label": 0
                },
                {
                    "sent": "The first question was why Grayscale and a second question was why stack these frames?",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yes, that effect.",
                    "label": 0
                },
                {
                    "sent": "So the agent the question was when the agent is playing Space Invaders.",
                    "label": 0
                },
                {
                    "sent": "It seems to shoot through the wall, which is clearly the wrong thing to do.",
                    "label": 0
                },
                {
                    "sent": "This might have to do with the fact that the agent isn't penalized to do this.",
                    "label": 0
                },
                {
                    "sent": "It might not be optimal, but it doesn't have an impact on its ultimate return, so the agent just doesn't see the difference between taking a random action in this case and not.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's nice about having an architecture is you can actually look.",
                    "label": 0
                },
                {
                    "sent": "We can start looking at its pieces and a large part of the parallel research, especially right after the document paper came out was about studying different aspects of this architecture and so you can look at the network and say how would I make this a better network.",
                    "label": 0
                },
                {
                    "sent": "Maybe I changed the frame stacking by a recurrent layer.",
                    "label": 0
                },
                {
                    "sent": "Or maybe I did this with a single SCM layer, maybe for recurrent network.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other thing we can do is we can look at the policy and we can say how do we actually change the policy or how we output the Q values to lead to again to other better performance or more stable and more robust agents.",
                    "label": 0
                },
                {
                    "sent": "So some of the work in that space there was the dualing networks by ON doing networks splits the Q function into value V depending on the policy and advantage function A and of course Q will then be equal to V + A and that allows you to recover the Q value.",
                    "label": 0
                },
                {
                    "sent": "Of the agent, but you only need A to choose actions.",
                    "label": 0
                },
                {
                    "sent": "Synchronous actor critic is a name says it's an actor critic method, which means there's a parameterized policy that the agent follows and then uses AQ function to RV function to predict and learn about this policy.",
                    "label": 0
                },
                {
                    "sent": "And then really fun piece of work by Ian Husband and colleagues and Bootstrap Q functions.",
                    "label": 0
                },
                {
                    "sent": "Instead of having a single value function, we now have a collection of value functions and we were going to randomly train these Q functions on a subset of the data to.",
                    "label": 0
                },
                {
                    "sent": "This is the bootstrap part.",
                    "label": 0
                },
                {
                    "sent": "To get different estimates.",
                    "label": 0
                },
                {
                    "sent": "Trying to get at a confidence interval over the Q functions and then an act according to some function of this assembled of Q functions to incentivize exploration.",
                    "label": 0
                },
                {
                    "sent": "Similarly, more recently, there's been.",
                    "label": 0
                },
                {
                    "sent": "There's been two work.",
                    "label": 0
                },
                {
                    "sent": "One work by Mary Fortune ET al.",
                    "label": 0
                },
                {
                    "sent": "And also another piece of work by opening high on noisy networks or parameterized noise to go ahead and change the actual parameters of the network to change the Q function or the stochastic policy and eventually drive better exploration.",
                    "label": 0
                },
                {
                    "sent": "Another",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the architecture that's been quite looked at is the replay memory itself, so we have this store of data and now we have to train on this data, and in fact I didn't say this, But the memory is actually bounded.",
                    "label": 0
                },
                {
                    "sent": "For practical reasons, people typically keep a small subset of the data.",
                    "label": 0
                },
                {
                    "sent": "And so I watch.",
                    "label": 0
                },
                {
                    "sent": "I want to just briefly mention.",
                    "label": 0
                },
                {
                    "sent": "Prize replay, which which was which.",
                    "label": 0
                },
                {
                    "sent": "Reweigh the experience too.",
                    "label": 0
                },
                {
                    "sent": "Reweigh the experience, to train on samples where we have a high TD error with the 80, that being that these are the samples where we need to learn about them right now.",
                    "label": 0
                },
                {
                    "sent": "Samples with low TD error we already know how to predict them.",
                    "label": 0
                },
                {
                    "sent": "We should not be training on them.",
                    "label": 0
                },
                {
                    "sent": "There's also work by Bruce Lee's and some of my colleagues at Deep Mind on applying important sampling to get better estimates of the replay data and Calculator has actually been working with me.",
                    "label": 0
                },
                {
                    "sent": "We're looking into off policy learning.",
                    "label": 0
                },
                {
                    "sent": "For similar similar questions.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the part I've been the most involved in is actually the learning algorithm itself, and this this has actually been the subject of a lot of work.",
                    "label": 0
                },
                {
                    "sent": "For example, double Q learning by heart Vanasselt looked at this idea of reducing the statistical bias that's induced when we take the Max in Q learning by choosing the action of that Max according to the first network.",
                    "label": 0
                },
                {
                    "sent": "So it's actually a very simple trick that was shown to reduce statistical bias at a time and actually significantly improved performance roughly around the same time we looked at.",
                    "label": 0
                },
                {
                    "sent": "This idea of advantage learning advantage learning is actually an idea from Lehman Baird in the mid 90s we looked at.",
                    "label": 0
                },
                {
                    "sent": "Different algorithms that would be what we called action gap increasing.",
                    "label": 0
                },
                {
                    "sent": "So if you know that action A is the best action you think it's the best action, then you're going to try to increase.",
                    "label": 0
                },
                {
                    "sent": "The Q value for that action, or rather keep it the same and decrease the Q value for older sub optimal actions so it's easier for your network.",
                    "label": 0
                },
                {
                    "sent": "To make the distinction between good action in a bad action, and again, if you do this, there's a number of ways you can do this.",
                    "label": 0
                },
                {
                    "sent": "Encourage you to look into the paper, curious many ways to do this, but in all cases this actually improves learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "So that's a good question.",
                    "label": 0
                },
                {
                    "sent": "So if we have a softmax, can't we get the same effect?",
                    "label": 0
                },
                {
                    "sent": "Yes, now you're leaving it to the learning algorithm to slowly, overtime accrete evidence that the higher action is better.",
                    "label": 0
                },
                {
                    "sent": "If you're dealing with Q value.",
                    "label": 0
                },
                {
                    "sent": "So if you just did a softmax, for example over the Q values, but you learn the Q values using Q learning, you're stuck with the values you have, so you don't have when you could play with the temperature parameter, but that's a bit fiddly.",
                    "label": 0
                },
                {
                    "sent": "Here I think the idea is more to say, let's make the job easier for the network to just distinguish these two states.",
                    "label": 0
                },
                {
                    "sent": "Well, that makes sense.",
                    "label": 0
                },
                {
                    "sent": "Maybe rather we need to revisit this work.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "So the comment is that if you suck Max 2 very close values and of course you get a uniform policy, that's true.",
                    "label": 0
                },
                {
                    "sent": "I think your point was more than now you can change the temperature to compensate for this.",
                    "label": 0
                },
                {
                    "sent": "Is something worth looking at here.",
                    "label": 0
                },
                {
                    "sent": "So very briefly, let me just point out there's been work on some off policy learning algorithm Q, Lambda, retrace and then most recently this Society of distributional reinforcement.",
                    "label": 1
                },
                {
                    "sent": "Is these distribution distributional methods, which have been quite involved with and that's actually been quite exciting because we've seen a lot of progress and new interesting avenues for research coming out of these.",
                    "label": 0
                },
                {
                    "sent": "So I thought I would actually tell you about these distributional methods.",
                    "label": 0
                },
                {
                    "sent": "So let's actually get into into this part of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In distribution or reinforcement learning, we're going to look at the full distribution of possible returns, so let me give you an example for what that means.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose you're playing a game of Monopoly.",
                    "label": 0
                },
                {
                    "sent": "And you're the wheelbarrow.",
                    "label": 0
                },
                {
                    "sent": "And you're sitting on Park Place and you're about to roll the dice.",
                    "label": 0
                },
                {
                    "sent": "An if you roll it too, and you're very unlucky, roll it to uni, Lennon, Boardwalk and the hotel is owned by your opponent.",
                    "label": 0
                },
                {
                    "sent": "You're going to lose $2000.",
                    "label": 0
                },
                {
                    "sent": "But if you roll anything else than a 2, then you would actually go over Boardwalk, an win $200.",
                    "label": 0
                },
                {
                    "sent": "So in classic reinforcement learning terms, we would.",
                    "label": 0
                },
                {
                    "sent": "We would say you know there's a state, just Park Place.",
                    "label": 0
                },
                {
                    "sent": "This is the location of your wheelbarrow.",
                    "label": 0
                },
                {
                    "sent": "Possibly also the location of the hotels.",
                    "label": 0
                },
                {
                    "sent": "And then as a reward are of X.",
                    "label": 0
                },
                {
                    "sent": "Right and we care about the expected reward, so the expected reward is we can do the math.",
                    "label": 0
                },
                {
                    "sent": "It comes out to be $139 on average.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, this is a purely prediction problem, so there's no actions being taken here.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now if you think about this problem.",
                    "label": 0
                },
                {
                    "sent": "If you think about what's going to happen, you can really say I'm going to roll the dice and then have two possible outcomes.",
                    "label": 0
                },
                {
                    "sent": "Now this is this is RL.",
                    "label": 0
                },
                {
                    "sent": "This is a sequential decision making problem or sequential problem.",
                    "label": 0
                },
                {
                    "sent": "So there's going to be more decisions or more choices.",
                    "label": 0
                },
                {
                    "sent": "Random Rolls made afterwards, and we're going to keep rolling these forward.",
                    "label": 0
                },
                {
                    "sent": "Whether we have some policy involved, enough.",
                    "label": 0
                },
                {
                    "sent": "Either way we have this this whole tree of possible outcomes based.",
                    "label": 0
                },
                {
                    "sent": "Under the roll of the dice.",
                    "label": 0
                },
                {
                    "sent": "And So what we can say is we have this sequence of rewards which are actually random.",
                    "label": 0
                },
                {
                    "sent": "OK, and we're going to discount them and look at the value function.",
                    "label": 0
                },
                {
                    "sent": "The expected summer discounted rewards, right?",
                    "label": 0
                },
                {
                    "sent": "So that's what the moment equation tells us.",
                    "label": 0
                },
                {
                    "sent": "It tells us that the value, for example, of a policy at state X is going to be expected reward plus the expected next state value function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's very interesting about this picture is that the ground truth, the ground truth is you can think of the tree of all possible outcomes.",
                    "label": 0
                },
                {
                    "sent": "You can weigh them by their probability, but if you look at the Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "That talks about expectations then.",
                    "label": 0
                },
                {
                    "sent": "Then really, the model, the mental picture of the Bellman equation, if you will, is a straight line going through when you take all of the randomness and you can collapse it down into just going from from one mean state to another mean state where you get the mean reward whenever you transition from one step to the next.",
                    "label": 0
                },
                {
                    "sent": "And this was actually.",
                    "label": 0
                },
                {
                    "sent": "Pointed out already in the in the mid 2000s by number of authors that we're looking at learning linear models of the world.",
                    "label": 0
                },
                {
                    "sent": "So if you want to predict the next set of features, present occurrence data features.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you end up with one of these flat flat models.",
                    "label": 0
                },
                {
                    "sent": "These flat expected models.",
                    "label": 0
                },
                {
                    "sent": "Parrot",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That makes sense because the value function is an expected sum of discounted rewards by linearity of expectation, like Tour was telling us yesterday, you can break this down in two different time steps and predict each of these time steps separately.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now this is all good, but if we go back to this this full picture of the all possible outcomes, we could actually think about.",
                    "label": 0
                },
                {
                    "sent": "Well when I play out a specific game I observe.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of these trajectories, one very specific trajectory.",
                    "label": 0
                },
                {
                    "sent": "OK, so I can look at this value function citizen expectation, but now what if I look at each trajectory as an individual?",
                    "label": 0
                },
                {
                    "sent": "Simple outcome in a bigger random variable.",
                    "label": 0
                },
                {
                    "sent": "OK, so really the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Value function you can think of it as breaking down into the individual timestamps, or think of it as the expectation of a anatomique random variable.",
                    "label": 0
                },
                {
                    "sent": "When I call it zed, \u03c0 and that random variable is the random return that you're going to get from start state X going forward, and it's really accounting for all the randomness along the way.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we there's actually, this idea is already present in a paper from 2015 with my colleague Joel Veness, where we did this with Monte Carlo estimation methods now.",
                    "label": 0
                },
                {
                    "sent": "Then the new part here is to say, OK, so we have the Bellman equation.",
                    "label": 1
                },
                {
                    "sent": "That relates the expectation of that random return to the expectation of a random return at the next state.",
                    "label": 0
                },
                {
                    "sent": "So I've taken the value functions and just replace them by expectations.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the distributional method is to say, let's actually just remove these expectation signs and see and see let's them make sense and see if that.",
                    "label": 0
                },
                {
                    "sent": "If that happens then OK.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out that there is a similar equation which you can write down, which is now going to be the value distribution at one state is going to be the random reward.",
                    "label": 0
                },
                {
                    "sent": "Plus the next date value distribution.",
                    "label": 0
                },
                {
                    "sent": "Now the big jump years that we're no longer talking about expectations were talking about random variables, or more specifically, we're talking about their distribution.",
                    "label": 0
                },
                {
                    "sent": "So we're saying that the probability distribution of the random variable zed by X is displays the same distribution as the sum of these two random variables in the right hand side.",
                    "label": 0
                },
                {
                    "sent": "And so this this way this is called the distributional Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "So it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not the only.",
                    "label": 0
                },
                {
                    "sent": "Equation of its kind OK.",
                    "label": 0
                },
                {
                    "sent": "There's a number of other equations like it have been proposed.",
                    "label": 0
                },
                {
                    "sent": "What's maybe.",
                    "label": 0
                },
                {
                    "sent": "New about this one is that we really looking at the whole distribution instead of looking at just a few of the moments and the picture looks something like this.",
                    "label": 0
                },
                {
                    "sent": "You have again whole distribution at the current State X, and if you're satisfying this distributional equation then it must mean that when you transition to next states in proportion to how often a transition you observe, these other distributions that are next dates.",
                    "label": 0
                },
                {
                    "sent": "So you have a.",
                    "label": 0
                },
                {
                    "sent": "You could think of it as a distribution mixture of distributions matching your current distribution.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How would you actually go about with this?",
                    "label": 0
                },
                {
                    "sent": "You know this.",
                    "label": 0
                },
                {
                    "sent": "I've shown you a Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "How will you actually design A learning rule out of this idea?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, let's go back to regular expected reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "If you're doing expected reinforcement learning, you're going to observe a transition.",
                    "label": 0
                },
                {
                    "sent": "You start in Park Place, you roll the dice and you land on go.",
                    "label": 0
                },
                {
                    "sent": "You land somewhere and you collect 200.",
                    "label": 0
                },
                {
                    "sent": "Dollars.",
                    "label": 0
                },
                {
                    "sent": "Now suppose that your current value prediction is.",
                    "label": 0
                },
                {
                    "sent": "This should be worth 150.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And the state you land that you say this would be worth 300.",
                    "label": 0
                },
                {
                    "sent": "So how do we do this in RL?",
                    "label": 0
                },
                {
                    "sent": "We add up the next state prediction.",
                    "label": 0
                },
                {
                    "sent": "Discounted and add the reward.",
                    "label": 0
                },
                {
                    "sent": "And then we.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We change our prediction of the value of the current state to reflect what we've actually observed in a stochastic environment.",
                    "label": 0
                },
                {
                    "sent": "Will have a step size and maybe we only move towards that prediction.",
                    "label": 0
                },
                {
                    "sent": "But I.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Either way, that's that's the rough picture.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out we can do the same thing with distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to start with a prediction about what the distribution should look like.",
                    "label": 0
                },
                {
                    "sent": "This is really a guess about the whole.",
                    "label": 0
                },
                {
                    "sent": "The entirety of all the possible outcomes you could see.",
                    "label": 0
                },
                {
                    "sent": "You make the prediction you observe the next state, and you also observe the reward at the next state.",
                    "label": 0
                },
                {
                    "sent": "You also have a predicted distribution.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to do is you're going to add if you will.",
                    "label": 0
                },
                {
                    "sent": "Those two distributions and what that means is effectively it's a convolution.",
                    "label": 0
                },
                {
                    "sent": "If your reward is the terministic, then you can just shift your whole distribution left or right on the X axis.",
                    "label": 0
                },
                {
                    "sent": "And then you can you discount your distribution so you have the whole distribution of all possible returns you could see, and then you squash when you multiply the discount factor you effectively squashing your distribution towards 0.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So we do both of these things and then you're going to learn about the distribution.",
                    "label": 0
                },
                {
                    "sent": "So change, you can have a new target distribution you want to learn about the change in distribution to look more like what you've just observed.",
                    "label": 0
                },
                {
                    "sent": "So we've gone from predicting a single scalar to predicting a whole distribution, and this is how these two updates relate.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what does that mean in the context of designing a new learning rule for a deep agent that plays Atari games?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, we go back to the network and this network is called the C 51 Network.",
                    "label": 0
                },
                {
                    "sent": "The network and one thing you can do is you can actually replace the Q value being output at the last layer of your network by effectively, except where would you like to be?",
                    "label": 0
                },
                {
                    "sent": "A set of probabilities so would like to predict the probabilities of a set of events.",
                    "label": 0
                },
                {
                    "sent": "Based on your input.",
                    "label": 0
                },
                {
                    "sent": "It turns out we have to.",
                    "label": 0
                },
                {
                    "sent": "We can't predict the whole distribution, so will make a number of these predictions.",
                    "label": 0
                },
                {
                    "sent": "In fact, the word the name see 51 means we make 51 predictions.",
                    "label": 0
                },
                {
                    "sent": "And So what we do is we actually define a histogram distribution evenly spaced on the interval minus 10 return to 10 return.",
                    "label": 0
                },
                {
                    "sent": "Which sort of makes sense for Atari games, and each of these buckets now we're going to use softmax to predict to predict the distribution, so weather network is going to output the network is going to output the lodge.",
                    "label": 0
                },
                {
                    "sent": "It's of a softmax which then gives us the probability distribution, and that's true for each action.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we replace that last Q function layer by logic.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the other thing that happens in this network that learning rule changes.",
                    "label": 0
                },
                {
                    "sent": "But also we now have a distribution.",
                    "label": 0
                },
                {
                    "sent": "So how do we act according to this distribution?",
                    "label": 0
                },
                {
                    "sent": "Well, very simple thing.",
                    "label": 0
                },
                {
                    "sent": "We're actually going to be a greedy where epsilon greedy with respect to the expected value that distribution.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And I'll come back to this point in just a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now if we make an approximation where we have a softmax, then we're going to run into a problem where after we shift our distribution and scale it down, we might not actually line up with the histogram we started with, and that can be a big issue.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case we are going to define a projection step which takes an arbitrary distribution.",
                    "label": 0
                },
                {
                    "sent": "Let's keep it simple and say it's also histogram distribution and we're going to project it back into the support of the thing that our network can actually output.",
                    "label": 0
                },
                {
                    "sent": "So projected back into 51 atoms that we can actually predict.",
                    "label": 0
                },
                {
                    "sent": "So that lets us define actually a full learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Under so you have the you have the operator notation at the top.",
                    "label": 0
                },
                {
                    "sent": "Basically all it's saying is again repeating what I said earlier.",
                    "label": 0
                },
                {
                    "sent": "You look at the next state distribution, you squash it, you shift it OK and that would give you a target.",
                    "label": 0
                },
                {
                    "sent": "But then the next step is you need to actually project it back into your space that you can predict it.",
                    "label": 0
                },
                {
                    "sent": "So it's a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "You sample a transition, you compute your sample backup, which eventually give you something like a TD error and eventually project back into the support.",
                    "label": 0
                },
                {
                    "sent": "Now you have these, you have a new distribution you want to predict we have large.",
                    "label": 0
                },
                {
                    "sent": "It's the network outputs to the natural thing to do is to update our predictions towards that projected Bellman target.",
                    "label": 0
                },
                {
                    "sent": "And we do this by minimizing the you know the cross entropy loss.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let me show you yes quick question.",
                    "label": 0
                },
                {
                    "sent": "If I understand correctly, the question is, is this going to converge to a fixed distribution eventually?",
                    "label": 0
                },
                {
                    "sent": "Yes, the short answer is that under certain conditions you're guaranteed to actually converge to a fixed point.",
                    "label": 0
                },
                {
                    "sent": "So the equation in fact the equation was showing you earlier is that fixed point there is there is a stationary point.",
                    "label": 0
                },
                {
                    "sent": "Maybe before.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "It may be what you mean is this so?",
                    "label": 0
                },
                {
                    "sent": "Does it add variance?",
                    "label": 0
                },
                {
                    "sent": "You set the policy gradient maybe does it add more variance in the optimization process, right?",
                    "label": 0
                },
                {
                    "sent": "Because you're predicting more things, the short answer is that is still to be determined.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem too.",
                    "label": 0
                },
                {
                    "sent": "Doesn't seem to hurt.",
                    "label": 0
                },
                {
                    "sent": "Do we have a Bellman optimality operator?",
                    "label": 0
                },
                {
                    "sent": "Yes, it's true.",
                    "label": 0
                },
                {
                    "sent": "This is all the prediction setting now you can define in the same way to take the Max you take the Max over a distribution.",
                    "label": 0
                },
                {
                    "sent": "So now we have a problem which is how do you think?",
                    "label": 0
                },
                {
                    "sent": "How do you compare distributions?",
                    "label": 0
                },
                {
                    "sent": "So again what we do is we actually just choose to.",
                    "label": 0
                },
                {
                    "sent": "The one with the highest expectation and propagate this back.",
                    "label": 0
                },
                {
                    "sent": "That's right, so in fact you could do something different if you have.",
                    "label": 0
                },
                {
                    "sent": "If you give me any set of distributions corresponding to different actions that could take in the next state, I can define a selection rule.",
                    "label": 0
                },
                {
                    "sent": "What we've shown is that if your selection rule is greedy with respect to the expectation.",
                    "label": 0
                },
                {
                    "sent": "Then you'll converge to the value distribution of an optimal policy.",
                    "label": 0
                },
                {
                    "sent": "And actually have to quantify this.",
                    "label": 0
                },
                {
                    "sent": "You might converge to something.",
                    "label": 0
                },
                {
                    "sent": "Its expectation will be will be que star the optimal Q value, but it might not look in many correspond to any stationary policy.",
                    "label": 0
                },
                {
                    "sent": "So some funny things happen, but you do get convergence.",
                    "label": 0
                },
                {
                    "sent": "You could also decide to have Max rule that doesn't choose just the expected value, but maybe says maximize expectation but minimize variance in brick ties by minimizing variance.",
                    "label": 0
                },
                {
                    "sent": "So or maybe maximize some combination of the two that would define a different kind of selection rule.",
                    "label": 0
                },
                {
                    "sent": "So why learn this whole distribution?",
                    "label": 0
                },
                {
                    "sent": "In the end?",
                    "label": 0
                },
                {
                    "sent": "We just need the expectation that is actually a very good question and this is one that we've been answering that the last 2 1/2 years.",
                    "label": 0
                },
                {
                    "sent": "Come back to that in a second, but the short answer is you know this is the million dollar question.",
                    "label": 0
                },
                {
                    "sent": "This is this is a high value question to to answer.",
                    "label": 0
                },
                {
                    "sent": "It's going to work well in practice is a short answer.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me show you very briefly what it looks like, and I'll give you some learning curves and then the question will become relevant.",
                    "label": 0
                },
                {
                    "sent": "So let me just show you what these agents are learning to predict.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is first of all, the pong agent in Pong you've seen this game before.",
                    "label": 0
                },
                {
                    "sent": "Now you'll notice that there's three colors corresponding to three different actions you could be taking up down and stay still OK.",
                    "label": 0
                },
                {
                    "sent": "Weather interesting is when the ball is about to get into the into the green petals net.",
                    "label": 0
                },
                {
                    "sent": "You can see the distribution separating, so they typically on top of each other because his most actions are about the same.",
                    "label": 0
                },
                {
                    "sent": "But then when it becomes critical you can see these distributions clearly separating.",
                    "label": 0
                },
                {
                    "sent": "So that's a pretty simple example.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My favorite example is actually from Space Invaders, where so you have this agent playing and 11 particularity of Space Invaders is you can lose a life if the enemy bullets hit you, but you can also lose the game right if the invaders successfully invade and reached the bottom role of the screen and what you're going to see now is that the agent is predicting it's predicting the distribution, so they should say the X axis is all the possible returns.",
                    "label": 0
                },
                {
                    "sent": "the Y axis is the probability of that return, so you're seeing the distribution of possible returns.",
                    "label": 0
                },
                {
                    "sent": "OK, and the agent and now you see it now on 0 there's there's a probability that the agent will get zero return.",
                    "label": 0
                },
                {
                    "sent": "Now what is that?",
                    "label": 0
                },
                {
                    "sent": "That's the agent saying if I make the wrong choices or if I'm just uncertain about my state, there's a chance that they will lose the game altogether.",
                    "label": 0
                },
                {
                    "sent": "And that's actually early on.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons we thought this work pretty well is because predicting the distribution really gives you a finer grain prediction about the state of the world.",
                    "label": 0
                },
                {
                    "sent": "If you will, right?",
                    "label": 0
                },
                {
                    "sent": "If you were predicting expected value, you would just say.",
                    "label": 0
                },
                {
                    "sent": "0 * 10% plus the rest times 9 * 80%, and we wouldn't have such a fine grained prediction about them.",
                    "label": 0
                },
                {
                    "sent": "So we think that matters.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can of course compare our agent under 60 games of the attorneys.",
                    "label": 0
                },
                {
                    "sent": "So we trained in five games and report its course on all the games.",
                    "label": 0
                },
                {
                    "sent": "If you look what was very surprising was very surprising is that the C 51 agent actually performs better than at a time the all the other state of the art methods.",
                    "label": 0
                },
                {
                    "sent": "So both are higher mean score.",
                    "label": 0
                },
                {
                    "sent": "But more importantly, higher median score.",
                    "label": 0
                },
                {
                    "sent": "And it was actually also hire.",
                    "label": 0
                },
                {
                    "sent": "These percentages are in terms of human normalized scores which remind users.",
                    "label": 0
                },
                {
                    "sent": "So high also in human baseline and much, much better than the original DQN algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was very satisfying.",
                    "label": 0
                },
                {
                    "sent": "One of my.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great results from a game called Sea Quest where you know this is quite cherry picked, but I love this learning curve where the orange is C 51 in DQ and is just at the bottom here and the previous method in dotted line to previous best was about half the score so there was really something you know shift in paradigm in terms of the learning here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, my name is Sue.",
                    "label": 0
                },
                {
                    "sent": "What is stochastic in this game?",
                    "label": 0
                },
                {
                    "sent": "I think it would be great to have a more complete study of that.",
                    "label": 0
                },
                {
                    "sent": "First of all, first of all the agents policy is stochastic.",
                    "label": 0
                },
                {
                    "sent": "Second of all, because you only have an imperfect view in the world, but there might be some remaining partial observability there might be.",
                    "label": 0
                },
                {
                    "sent": "There may be a few states which you haven't learned to distinguish yet.",
                    "label": 0
                },
                {
                    "sent": "There's there's implicit stochasticity, so the game itself, the inverted emulator, is a deterministic but.",
                    "label": 0
                },
                {
                    "sent": "From the agent's perspective, in many ways it appears stochastic.",
                    "label": 0
                },
                {
                    "sent": "And so we saw this actually actually with the display Sadeghi Space Invaders example, if the agent had an optimal policy, it should never lose the game, so it should never predict that's about to lose the game.",
                    "label": 0
                },
                {
                    "sent": "But somehow through a combination of not quite understanding its state to a full extent, and maybe that it just keeps making mistakes.",
                    "label": 0
                },
                {
                    "sent": "Then it learns to predict this.",
                    "label": 0
                },
                {
                    "sent": "So how calibrated is the distribution?",
                    "label": 0
                },
                {
                    "sent": "The short answer is, it isn't an one of the first things is that you saw this Bell shaped curve.",
                    "label": 0
                },
                {
                    "sent": "The Bell shaped curve actually is an artifact of doing having histogram prediction and then repeatedly applying a discount factor.",
                    "label": 0
                },
                {
                    "sent": "That's what a process called diffusion.",
                    "label": 0
                },
                {
                    "sent": "So in that sense it's not calibrated, but I think in terms of predicting the outcomes, it probably is close to calibrate.",
                    "label": 0
                },
                {
                    "sent": "It would be an interesting question.",
                    "label": 0
                },
                {
                    "sent": "And anymore questions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm actually quite short on time, but I will show you this video because it's quite neat.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it made me think the point this is work on D4 PG by my colleagues Gabriel Berkmar and Matt Hoffman and some others in Deep Mind.",
                    "label": 0
                },
                {
                    "sent": "And I'll show you the agent first.",
                    "label": 0
                },
                {
                    "sent": "This is one of these control tasks where the agent has to learn to cross the maze as fast as possible in the process.",
                    "label": 0
                },
                {
                    "sent": "It doesn't need to be graceful, but it doesn't across domains very quickly and you'll notice that sometimes bumps into into walls.",
                    "label": 0
                },
                {
                    "sent": "And so, why?",
                    "label": 0
                },
                {
                    "sent": "Why is it happening and what does the distribution look like?",
                    "label": 0
                },
                {
                    "sent": "Well, if we re.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Play this video, but now predicting the looking at the actual distribution that's predicted by the agent.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a V distribution if you will.",
                    "label": 0
                },
                {
                    "sent": "So just a value.",
                    "label": 0
                },
                {
                    "sent": "You'll see what happens whenever it reaches a wall, right?",
                    "label": 0
                },
                {
                    "sent": "So do you see that there's multiple modes in their distribution?",
                    "label": 0
                },
                {
                    "sent": "So the agent again knows that there's a chance it's going to fail, and we're talking about a trained agent, so it really should know it should have happened dissent.",
                    "label": 0
                },
                {
                    "sent": "But somehow maybe because the agent is constrained in its representation, it's still.",
                    "label": 0
                },
                {
                    "sent": "It's still unable to.",
                    "label": 0
                },
                {
                    "sent": "To fail to run through walls.",
                    "label": 0
                },
                {
                    "sent": "So that was quite neat.",
                    "label": 0
                },
                {
                    "sent": "OK, so very brief.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since then, there's actually been a lot of work on either improving the network that's the left branch of this graph, quantile regression, some really need work.",
                    "label": 0
                },
                {
                    "sent": "I will Damien implicit quantum networks and also digital networks idea, so there's been one branch of actually improving the agent architecture for these games.",
                    "label": 0
                },
                {
                    "sent": "The other branch is actually going back closer to more traditional reinforcement learning is saying how do we come to understand distributional?",
                    "label": 1
                },
                {
                    "sent": "Distributional reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "In the context of this RL in general, and so actually one of the students are working with Claire had a workshop paper acnl where we found a surprising result that if you were to just use linear approximation then in fact.",
                    "label": 0
                },
                {
                    "sent": "Nothing should be different, so this suggests that is something to do with the interaction between the deep networks and the distribution of learning.",
                    "label": 0
                },
                {
                    "sent": "And that's actually very exciting because it gives us a sense of where to look next as to why, why it might be useful to predict the distribution even if we take the expectation at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "So this was.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, the first thing I was going to tell you about, maybe I'll very briefly talk about this other part and leave some room for questions so.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So far I've talked about games where where we've actually made a lot of progress.",
                    "label": 0
                },
                {
                    "sent": "In fact, this graph is a bit out of date.",
                    "label": 0
                },
                {
                    "sent": "We've made progress in all the games now.",
                    "label": 0
                },
                {
                    "sent": "But if you, if you were to look at different Atari games that are available through the alley, and you plot overtime, the roughly the performance we've attained on these different games and very early on the game, like Pong, we knew how to play well, and that remains true today.",
                    "label": 0
                },
                {
                    "sent": "If you, if your agent, if you deep Agent doesn't play pong very well, then you know then you should check for bugs.",
                    "label": 0
                },
                {
                    "sent": "The seaquest actually was a hard game until roughly until the distributional method came around.",
                    "label": 0
                },
                {
                    "sent": "But then there's been other games actually have received a lot of attention in last 6 to 8 months, and one of them is called Montezuma's Revenge.",
                    "label": 0
                },
                {
                    "sent": "It's actually it was designed by Robert Yeager when he was 16, and it's an adventure game.",
                    "label": 0
                },
                {
                    "sent": "So let me show you what it looks like.",
                    "label": 0
                },
                {
                    "sent": "I hope my sound is.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, good, and so you're this.",
                    "label": 0
                },
                {
                    "sent": "This adventure navigating this maze and unlike, say, policy Quest.",
                    "label": 0
                },
                {
                    "sent": "There's clearly a higher level order of cognition that's happening here where you have to go and navigate and pick up a key and move from room to room, opening doors and you'll see the score.",
                    "label": 0
                },
                {
                    "sent": "You're only actually changing in score when you achieve certain events, right?",
                    "label": 0
                },
                {
                    "sent": "So, unlike, say, seaquest, we collecting points all the time.",
                    "label": 0
                },
                {
                    "sent": "Now this agent can go on for minutes without collecting a new piece of score.",
                    "label": 0
                },
                {
                    "sent": "So what we would say is that the reward function is actually sparse.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is me playing the game where you can get a flavor for where where this game is going OK Now it turns out that for.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For an agent like Gwen or your distributional agents, that's an incredibly hard game to play.",
                    "label": 0
                },
                {
                    "sent": "So I was already illustrating the fact that you know you started this first room and and the actions you need to take right you playing this game in this case at 15 Hertz.",
                    "label": 0
                },
                {
                    "sent": "The actions you need to take our there's many of them.",
                    "label": 0
                },
                {
                    "sent": "You think we get any reward, OK?",
                    "label": 0
                },
                {
                    "sent": "There's also partial observability, so for example there's an object in the game called the Torch, and if you haven't picked up the torch and the whole screen is black, you haven't actually can't see anything, so you can't.",
                    "label": 0
                },
                {
                    "sent": "You don't know what you're actually looking at, and of course you know there's traps so you can fall in quicksand, and you can.",
                    "label": 0
                },
                {
                    "sent": "You can jump in a skull, and that's bad too.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then it's actually these barriers.",
                    "label": 0
                },
                {
                    "sent": "You seldom, I think, these blue barriers if you run into those blue barriers, then you actually lose them.",
                    "label": 0
                },
                {
                    "sent": "So putting this differently, right?",
                    "label": 0
                },
                {
                    "sent": "It's a, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a lot to coordinate your agent to get the key, OK?",
                    "label": 0
                },
                {
                    "sent": "And actually, the one thing I want to focus on here is this Blue Barrier business.",
                    "label": 0
                },
                {
                    "sent": "So if you think about a classic reinforcement learning agent that needs to go and learn to play this game Now, it turns out in this game you have to cross this four sequence of double barriers to eventually make your way to the goal.",
                    "label": 0
                },
                {
                    "sent": "And you know, typically we would use an epsilon greedy policy to do this, but in that case the agent has very little chance of crossing those four double barriers by epsilon, greedy, and every time do you lose a life.",
                    "label": 0
                },
                {
                    "sent": "Actually will start on the right hand side, so it's almost as infamous as one of the problems that was showing us yesterday where just randomly choosing actions will get you nowhere so.",
                    "label": 0
                },
                {
                    "sent": "The key here is that if we want these agents to be able to succeed at these games.",
                    "label": 0
                },
                {
                    "sent": "We need them to be incentivized beyond just the reward function to try to see what's on the other side and formulate a plan to actually get to that other side.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yesterday we heard about exploration, the classic way to do exploration in table MVP's would be one of the ways to do it is to add an exploration bonus.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have an empirical model of the world right, and then we're going to add this.",
                    "label": 0
                },
                {
                    "sent": "This bonus which decays according to the square root of the number of times you've seen a state.",
                    "label": 0
                },
                {
                    "sent": "It turns out this is not the best algorithms class, but it turns out it's good enough.",
                    "label": 0
                },
                {
                    "sent": "You could learn to.",
                    "label": 0
                },
                {
                    "sent": "You would then start exploring everywhere in body would be incentivized eventually.",
                    "label": 0
                },
                {
                    "sent": "Try to make your way across to the other side of these four crackers.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this makes sense if your environment is a grid world where each you know each state is a node in the graph, and you can count how many times we've been in each of these states.",
                    "label": 0
                },
                {
                    "sent": "Except the problem in is in most games on the Eli, you never experienced the same observation twice, so counting doesn't really make sense.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as this great number from a Charles Manson hotel, and 90% of singletons in most in most Atari games.",
                    "label": 0
                },
                {
                    "sent": "And that's certainly true for Montezuma's Revenge, right?",
                    "label": 0
                },
                {
                    "sent": "So counting.",
                    "label": 0
                },
                {
                    "sent": "Unique observations is hopeless.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now that approach would be to say, well, we could try to get a model which would approximately count.",
                    "label": 0
                },
                {
                    "sent": "So one way to do this is to start actually with a generative model, right agent of model is one that you can train on images, and then you can use it to generate more images.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you can do this.",
                    "label": 0
                },
                {
                    "sent": "You could actually if you ask of your model, that also gives you the probability I'm going to call us a density model.",
                    "label": 0
                },
                {
                    "sent": "So if you have a density model you can train it and then later say what's the probability of that of that frame here that you're showing me and that is going to give us the probability we call it roll Rolex or roll N of X where N is the number of steps you've seen so.",
                    "label": 0
                },
                {
                    "sent": "And the key idea here is to say we're going to try to count using this density model and how we're going to do this.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that a higher density implies that you've seen something.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Often so high density correlate with frequency.",
                    "label": 0
                },
                {
                    "sent": "And we're also going to assume that you have a that if your model is a good generative model or good density model, then also it's generalizing in the right way for this domain.",
                    "label": 0
                },
                {
                    "sent": "So these are sort of two reasonable assumptions to make.",
                    "label": 0
                },
                {
                    "sent": "A density model should generate things that are frequent more often, and it should also produce similar things the right way.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The way we're going to use this density model to count as we're going to say the probability that the the model the general model assigns to specific observation.",
                    "label": 0
                },
                {
                    "sent": "Is given to us by a ratio of account and called the pseudo count and a total count total pseudocounts sort of total count.",
                    "label": 0
                },
                {
                    "sent": "Total count.",
                    "label": 0
                },
                {
                    "sent": "So that's great, but that's two unknowns and one and one equation.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to need a second equation to try to get rid of that second order and the key here is to talk about equality, called the recording probability, right row prime of X, and it says row prime of X is going to be the same counts, but we add 1 to each of these and roll prime is going to say is going to be the probability.",
                    "label": 0
                },
                {
                    "sent": "Of X after we've trained on it.",
                    "label": 0
                },
                {
                    "sent": "So if we have these two equations then we can two equations, two unknowns we can solve for the pseudo count for big in hand to get out a notion of account, and so that's again I'm going to start with a density model and we're going to use it to extract account to know how often we've been to a specific state.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So density model is.",
                    "label": 0
                },
                {
                    "sent": "We will train it on images.",
                    "label": 0
                },
                {
                    "sent": "And then later you can show it in different image and it says this image is probability.",
                    "label": 0
                },
                {
                    "sent": "I'm going to leave it as big as that, but so a general model that auto regressive general model would be a density model.",
                    "label": 0
                },
                {
                    "sent": "Pixel CN is a good general density model.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, So what is a pseudo path so the pseudo count is the quantity I'm defining by these two equations, so it's the thing specifically the pseudo count.",
                    "label": 0
                },
                {
                    "sent": "Is this capital in hand?",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Is the thing that you can imagine that the model is is assigning probability to a frame.",
                    "label": 0
                },
                {
                    "sent": "And it's in fact saying I'm assignment probabilities as if I was counting, so it's a pseudo cabinets and that it's not a real account, but it behaves like I'll show you an example.",
                    "label": 0
                },
                {
                    "sent": "Maybe they'll make more sense.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Why am I doing all of this instead of just multiplying the probability by N?",
                    "label": 0
                },
                {
                    "sent": "Right, so it turns out that that doesn't work, so if you were to take right, the natural way would be to say I've trained a trained my model.",
                    "label": 0
                },
                {
                    "sent": "It gives you the probability of an image and I want to multiply by the total number of things that I've ever seen.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if you do it this way.",
                    "label": 0
                },
                {
                    "sent": "Your model generalizes to very large spaces then.",
                    "label": 0
                },
                {
                    "sent": "Then the probabilities you'll see will be very small.",
                    "label": 0
                },
                {
                    "sent": "They might be over the order two to the minus 20 for example, so you can't directly go right if you look at the general model that assigns Nats, typically the numbers will report for sameness.",
                    "label": 0
                },
                {
                    "sent": "We might be, you know, 60 minutes.",
                    "label": 0
                },
                {
                    "sent": "That means it's either the minus 60 is the probability of any specific image.",
                    "label": 0
                },
                {
                    "sent": "Yes yes sure.",
                    "label": 0
                },
                {
                    "sent": "So the first question is, why do we not use the log likelihood and the second one is?",
                    "label": 0
                },
                {
                    "sent": "Why do we bother or how do we actually train so the training vital answered with the next slide?",
                    "label": 0
                },
                {
                    "sent": "The first one.",
                    "label": 0
                },
                {
                    "sent": "What I'll say is if you were just to look at the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "This would correspond to.",
                    "label": 0
                },
                {
                    "sent": "Well, eventually we want to use this quantity too as account.",
                    "label": 0
                },
                {
                    "sent": "Now what we want to do is we want to incentivize the agent to go places where it still learning about the world.",
                    "label": 0
                },
                {
                    "sent": "If you were to use the log likelihood, it would fever very rare places where the user likelihood order log likelihood it would favor going back to places that are very rare, which is not actually what we're trying to achieve for.",
                    "label": 0
                },
                {
                    "sent": "But as a reader question.",
                    "label": 0
                },
                {
                    "sent": "So how do we train and why would we train this thing?",
                    "label": 0
                },
                {
                    "sent": "The way to think?",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it is that before you take an action, you're going to predict you're going to ask your model for what's the probability of this state, and now answering your question actually can do this online.",
                    "label": 0
                },
                {
                    "sent": "So now you actually act upon the world, OK?",
                    "label": 0
                },
                {
                    "sent": "An annual train and then you so you've actually seen that frame.",
                    "label": 0
                },
                {
                    "sent": "You are in that state right now.",
                    "label": 0
                },
                {
                    "sent": "You train on your frame and then you query the recording probability and so online.",
                    "label": 0
                },
                {
                    "sent": "When you experience A-frame, it makes sense to compute that value at that time because you're going to be training a density model.",
                    "label": 0
                },
                {
                    "sent": "As you're learning about the world.",
                    "label": 0
                },
                {
                    "sent": "It is effectively a second order gradient step, and if it's a network that's right.",
                    "label": 0
                },
                {
                    "sent": "So in fact, so we view this with multiple models.",
                    "label": 0
                },
                {
                    "sent": "If the model is not the deep net then then you would be doing something different.",
                    "label": 0
                },
                {
                    "sent": "Does it make sense?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, the first thing we've used is in multiple CTS model, which is not a deep net.",
                    "label": 0
                },
                {
                    "sent": "All the deep Nets actually perform better at this task, so the CDs file is very simple.",
                    "label": 0
                },
                {
                    "sent": "It's an auto regressive model based on.",
                    "label": 0
                },
                {
                    "sent": "Variable order Markov predictor.",
                    "label": 0
                },
                {
                    "sent": "If you're curious, we can talk more about this after after the talk.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just want to give you a sense for what you can do with this technique.",
                    "label": 0
                },
                {
                    "sent": "So first of all, this is a game called Freeway where the chickens trying to cross the road.",
                    "label": 0
                },
                {
                    "sent": "OK, that's very, very simple game.",
                    "label": 0
                },
                {
                    "sent": "You get a point every time you go up, and it's a fun game for it's a simple game for general modeling and counting, because there's actually a limited number of states you can be in.",
                    "label": 0
                },
                {
                    "sent": "Let me play this again so the chicken actually moves up and down and the cars actually rotate on a Taurus if you will.",
                    "label": 0
                },
                {
                    "sent": "So I can give you exactly the number of states that are in this game, it's.",
                    "label": 0
                },
                {
                    "sent": "The 20 three I think is how many states there are in this game.",
                    "label": 0
                },
                {
                    "sent": "It's great, it's also a lower bound if you ever do general modeling.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's an upper bound for it's a lower bound entropy that you can achieve if you were to sample frames uniform.",
                    "label": 0
                },
                {
                    "sent": "So one thing we did is we actually.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We ran our this technique just to look at the word pseudocounts look like under game Freeway where we were basically looking at two things that accounts go up linearly.",
                    "label": 0
                },
                {
                    "sent": "So if you for example counting whether you're at the start position.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out that accounts for sort of progressively the X axis is time for training frames.",
                    "label": 0
                },
                {
                    "sent": "the Y axis is the pseudo count that you models actually giving you another experiment we did is.",
                    "label": 0
                },
                {
                    "sent": "We said I want you to count whenever the agent reaches the top right when it crosses the road.",
                    "label": 0
                },
                {
                    "sent": "But now I'm actually going to make the policy sometimes stop.",
                    "label": 0
                },
                {
                    "sent": "And stay at the bottom and sometimes go forward.",
                    "label": 0
                },
                {
                    "sent": "And so for a while.",
                    "label": 0
                },
                {
                    "sent": "You're not actually increasing these counts and it also shows up in the model that the being at the top has lower pseudocounts than being at the bottom, which is exactly what you want, right?",
                    "label": 0
                },
                {
                    "sent": "So now we have this measure.",
                    "label": 0
                },
                {
                    "sent": "We can say this state.",
                    "label": 0
                },
                {
                    "sent": "We've seen less frequently than this other state.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what can you do with this very simple you can take.",
                    "label": 0
                },
                {
                    "sent": "You can take this exploration bonus algorithm and you can replace the real count of a state, which would be really hard to get under Terry by a pseudo components.",
                    "label": 0
                },
                {
                    "sent": "So we've done all this work.",
                    "label": 0
                },
                {
                    "sent": "To get a quantity which we now finally turned into just a reward, an extra award for going places that you haven't seen in the past.",
                    "label": 0
                },
                {
                    "sent": "OK, and it turns out that if you know cross our fingers, we may be able to explore better and incentivize our agent to reach hard to reach places with this kind of bonus.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So rather than show you learning curves, I want to actually show you what this agencies OK, so I'm going to show you a bit of a command center view of the agent.",
                    "label": 0
                },
                {
                    "sent": "This is actually not.",
                    "label": 0
                },
                {
                    "sent": "This is the agent with another CD S model, but the later pixels and model behaviors very qualitatively similar under top left you have the actual frame that's experienced by the agent.",
                    "label": 0
                },
                {
                    "sent": "Bottom Left is the value function.",
                    "label": 0
                },
                {
                    "sent": "If you haven't seen a value function before as the agent is playing the game, you'll see what it looks like then.",
                    "label": 0
                },
                {
                    "sent": "Top right is actually this.",
                    "label": 0
                },
                {
                    "sent": "This intrinsic reward, that trained agent is receiving.",
                    "label": 0
                },
                {
                    "sent": "It turns out with the Pixel stand model, these rewards don't go to 0.",
                    "label": 0
                },
                {
                    "sent": "But you'll see that they are higher in places that are further away.",
                    "label": 0
                },
                {
                    "sent": "And finally I want to show you the bottom.",
                    "label": 0
                },
                {
                    "sent": "The bottom is actually bottom right is fewer pixel by pixel.",
                    "label": 0
                },
                {
                    "sent": "Where the agent is finding novel things.",
                    "label": 0
                },
                {
                    "sent": "So, So what is actually unknown to the agent?",
                    "label": 0
                },
                {
                    "sent": "So in this work we could actually get this down to the pixel level, so let me get started red and the bottom right graph is novel.",
                    "label": 0
                },
                {
                    "sent": "Blue is actually empty novel because you could have negative.",
                    "label": 0
                },
                {
                    "sent": "Use the probability and So what you'll see is the agent.",
                    "label": 0
                },
                {
                    "sent": "The agent is now.",
                    "label": 0
                },
                {
                    "sent": "This is an agent that's trying to play this game and it gets spikes of reward and places where maybe it hasn't been as often.",
                    "label": 0
                },
                {
                    "sent": "It learns to do these kind of strange behaviors where it's very excited by room transitions because these are a bit a bit more rare.",
                    "label": 0
                },
                {
                    "sent": "But eventually it's going to make its way.",
                    "label": 0
                },
                {
                    "sent": "It's going to make its way to the to the through these blue barriers and then go down.",
                    "label": 0
                },
                {
                    "sent": "And then successfully play this game and I'll show you one more thing about these agents.",
                    "label": 0
                },
                {
                    "sent": "I mentioned the torch earlier these SRL agents, so they don't actually need to see to play these games.",
                    "label": 0
                },
                {
                    "sent": "So the agent has learned that if you go if you go into that dark room on the right hand side, there's some gems you can collect and you can collect the gems where you can see whether gems or not, and so the agent decided.",
                    "label": 0
                },
                {
                    "sent": "Well, I'll just.",
                    "label": 0
                },
                {
                    "sent": "I'll just go and collect these gems, and it looks at this court.",
                    "label": 0
                },
                {
                    "sent": "You know, when it's done collecting the gems.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's an example of overfitting.",
                    "label": 0
                },
                {
                    "sent": "To a specific game you tell me.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another way we can visualize with this agent is doing is to actually look at the map of the environment.",
                    "label": 0
                },
                {
                    "sent": "We start actually turns out in Montezuma's Revenge.",
                    "label": 0
                },
                {
                    "sent": "It's a pyramid shape map you started atop, and you're trying to reach the bottom left corner of the pyramid, and so I just want to show you what happens if you use DQN.",
                    "label": 0
                },
                {
                    "sent": "OK or decline with this pseudo camp owners.",
                    "label": 0
                },
                {
                    "sent": "Camp.",
                    "label": 0
                },
                {
                    "sent": "And we're going to show you is going to highlight the rooms at the agent has visited in the bottom video, and they're going to be grayed out if the agent is no longer visiting these rooms.",
                    "label": 0
                },
                {
                    "sent": "But we're going to remain highlighted, so let me start a video.",
                    "label": 0
                },
                {
                    "sent": "You'll see.",
                    "label": 0
                },
                {
                    "sent": "It's sort of very clear that this bonus based agent.",
                    "label": 0
                },
                {
                    "sent": "It's very rapidly person goes left.",
                    "label": 0
                },
                {
                    "sent": "It checks out the left area, it says OK, there's actually no reward on the left hand side that can reach.",
                    "label": 0
                },
                {
                    "sent": "Now it moves to the right hand side and very quickly discovers to maximize its reward.",
                    "label": 0
                },
                {
                    "sent": "And so at the counter is actually a number of millions of frames.",
                    "label": 0
                },
                {
                    "sent": "I don't have the numbers with me.",
                    "label": 0
                },
                {
                    "sent": "I think I think 200 million frames is 38 days of game playing, so really, that's a lot of time for the Q and Agent to learn to play this game.",
                    "label": 0
                },
                {
                    "sent": "And still it really can't get anywhere.",
                    "label": 0
                },
                {
                    "sent": "So of course we can.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Blood learning curves.",
                    "label": 0
                },
                {
                    "sent": "It turns out that.",
                    "label": 0
                },
                {
                    "sent": "Pseudocode algorithm performs very well compared to DQM, as you might expect.",
                    "label": 0
                },
                {
                    "sent": "If you measure it in terms of score.",
                    "label": 0
                },
                {
                    "sent": "And that's that, was also better than another method called the optimistic initialization method.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One thing I want to point out before wrap this up is very briefly to say that is really two.",
                    "label": 0
                },
                {
                    "sent": "I really talked about the pseudo count part, but there's another aspect of this project which which is very interesting, which is that you miss comes up all the time in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "We need to assign credit quickly.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm sure which told you about TD, Lambda and Q&A on.",
                    "label": 0
                },
                {
                    "sent": "On Wednesday, right?",
                    "label": 0
                },
                {
                    "sent": "So with these three, these Lambda methods were trying to propagate far away to the errors back to the current state.",
                    "label": 0
                },
                {
                    "sent": "So what we did here is we actually had a poor man's version of this where we mixed in the one Step TD error with the Monte Carlo updates and we call this a mixed Monte Carlo update and the combination of these two things.",
                    "label": 1
                },
                {
                    "sent": "Actually leads to much faster credit assignment and is actually necessary to learn well and these games.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you look at other games from the Terrace suite.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can find out that that this mix Montecarlo update is sometimes the sole responsible for better performance, but then there are games for which you really need both.",
                    "label": 0
                },
                {
                    "sent": "You need sudo count bonus and you need the money intermix medical update or some sort of longer term credit assignment.",
                    "label": 0
                },
                {
                    "sent": "One of these games is called Private I.",
                    "label": 0
                },
                {
                    "sent": "It's an adventure style game.",
                    "label": 0
                },
                {
                    "sent": "One man.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing I'll say is this is a full report with your coast hrobsky where we actually removed the extrinsic rewards altogether from the equation.",
                    "label": 0
                },
                {
                    "sent": "So remember that equation with the with the pseudo can bonus.",
                    "label": 0
                },
                {
                    "sent": "So we also remove these bonuses and found that if you up the gain high enough on your on your intrinsic bonus, then you can actually play Montezuma's Revenge with no reward.",
                    "label": 0
                },
                {
                    "sent": "But you still get the learning curve which is pretty neat.",
                    "label": 0
                },
                {
                    "sent": "So we can talk more about this at lunchtime if you want.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's going to be the closer the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "I actually want to go back to this idea of challenge domains and serve as the question where are we at now with with the Eli and the context of deeper methods?",
                    "label": 0
                },
                {
                    "sent": "Now this this slide is a bit.",
                    "label": 0
                },
                {
                    "sent": "I made it in September.",
                    "label": 0
                },
                {
                    "sent": "Things actually moving rapidly, but I think some of the take home message is still still correct.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to take the timeline from 1977, which is when the target 2600 was released to 1989, which is the last game we have in the daily.",
                    "label": 0
                },
                {
                    "sent": "Sweet and.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I can actually plot all of the games with them, all of them on this axis, and in September 2017 I could label these according to three categories.",
                    "label": 0
                },
                {
                    "sent": "Games where we were superhuman games where we had was going to call the scoring exploit.",
                    "label": 0
                },
                {
                    "sent": "So we collect a lot of points, but we're not really playing this game the right way.",
                    "label": 0
                },
                {
                    "sent": "And also games where the agents were subhuman and that's where in the last almost year.",
                    "label": 0
                },
                {
                    "sent": "Now things have moved very rapidly.",
                    "label": 0
                },
                {
                    "sent": "But now if you if you lead us down, this is giving you a sense of the progress of our best of our best Atari DRL agents.",
                    "label": 0
                },
                {
                    "sent": "And what's very interesting is if you look at the first part of that graph of the 1980 were really, you know, I think I think the last game is actually falling now, so we're really superhuman in all of these games, we can play these games really, really well.",
                    "label": 0
                },
                {
                    "sent": "And now you move into the 19 in the early 1980s, and already that proportion drops and there's there's more games also, where we're collecting a lot of points, but Nancy playing the game the way human would have played it.",
                    "label": 0
                },
                {
                    "sent": "And then finally female entity link or mid mid 80s.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of less data, but still, there's a clear trend that there are fewer games or superhuman.",
                    "label": 0
                },
                {
                    "sent": "So one way to think about it is as we moving forward in time with these video games, we're seeing our agents performance become less and less.",
                    "label": 0
                },
                {
                    "sent": "Good for the same level of architecture, the same level of compute and all of these anatomy is very telling that in some sense you know the agents that we have that we had in September 2017.",
                    "label": 0
                },
                {
                    "sent": "Maybe the single GPU agents were sort of stuck in 1983.",
                    "label": 0
                },
                {
                    "sent": "And so it begs the question also of saying, well, what next for deeper out and then you know what's the next thing after the alley.",
                    "label": 0
                },
                {
                    "sent": "So on this note, I'm going to close and get leave some time for questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "How would Google Self driving cars play these games or compared?",
                    "label": 0
                },
                {
                    "sent": "I know very little about the self driving cars so I couldn't even begin to answer that question.",
                    "label": 0
                },
                {
                    "sent": "Generally speaking, I think we're still far from general competency, and in the sense that we want to design algorithms that work as well as they can on a specific domain.",
                    "label": 0
                },
                {
                    "sent": "Applied to yes.",
                    "label": 0
                },
                {
                    "sent": "The results you showed with Montezuma's Revenge are those still with the architecture that takes in only the four previous frames.",
                    "label": 0
                },
                {
                    "sent": "Yes, and it's actually a very good question, so those results except for the learning rule and the bonus.",
                    "label": 1
                },
                {
                    "sent": "It was exactly the same architecture as before.",
                    "label": 0
                },
                {
                    "sent": "And so it's been a common theme throughout to say, well, whenever we have this agent architecture, I just change one piece at a time and see what happens.",
                    "label": 0
                },
                {
                    "sent": "Now maybe what you're asking is, could we do better if we change that part too?",
                    "label": 0
                },
                {
                    "sent": "Well, maybe that's something you should look into.",
                    "label": 0
                },
                {
                    "sent": "So if I'm not wrong, the first equation trying to solve both for the expectation right?",
                    "label": 0
                },
                {
                    "sent": "Imagine for expectation of reward in the future and then you can put it into an equation for the distributions.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "If all that is the expectation value that I'm going to have more freedom for my distribution, so can I add another distribution which has the means 0 for instance, but helps me with variance or what not gonna do that extra freedom.",
                    "label": 0
                },
                {
                    "sent": "Help me so I'm not sure if I understand the question.",
                    "label": 0
                },
                {
                    "sent": "If you saying could we instead of predicting the whole distribution, predict the mean and then also predict the distribution with mean zero to add on top of it?",
                    "label": 0
                },
                {
                    "sent": "So I have this extra stochastic variable I could add right, but this sooner or later you have, so I don't want to change them in because I need that, but I could use something that helps me build something later during the training, 'cause that is extra degree freedom, right?",
                    "label": 0
                },
                {
                    "sent": "Would that be useful or would it be useful to look at the rest of the distribution of what you're asking?",
                    "label": 0
                },
                {
                    "sent": "It should be if you wanted to be risk aware, for example, so there's this good paper by by actually implicit quantum networks paper looks at this question, where they looked at different risk profiles.",
                    "label": 0
                },
                {
                    "sent": "Now the field of Risk Aware RL is actually quite rich and purposefully.",
                    "label": 0
                },
                {
                    "sent": "We weren't really looking at this with the original paper.",
                    "label": 0
                },
                {
                    "sent": "Risk Aware RL is.",
                    "label": 0
                },
                {
                    "sent": "Brings in extra complications.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you want to be.",
                    "label": 0
                },
                {
                    "sent": "You know, if you want to add variance constraints, there might not be an easy solution.",
                    "label": 0
                },
                {
                    "sent": "You might be hard actually an NP hard to solve for the maximize expectation.",
                    "label": 0
                },
                {
                    "sent": "Subjective variance constraints.",
                    "label": 0
                },
                {
                    "sent": "Hi so.",
                    "label": 0
                },
                {
                    "sent": "I have a question like does the idea of incentivising your agent to explore hurt performance in the earlier games, in which you had like more dense rewards?",
                    "label": 0
                },
                {
                    "sent": "Like what is there a trade off and?",
                    "label": 0
                },
                {
                    "sent": "Is there a trade off to explore more?",
                    "label": 0
                },
                {
                    "sent": "The answer has to be, yes, right?",
                    "label": 0
                },
                {
                    "sent": "If you can take a single step and get maximum reward or that other agent is going to go off in a tangent and never get it reward.",
                    "label": 0
                },
                {
                    "sent": "In many ways we could hope is an adaptive algorithm that says if after a while I haven't explored, maybe I go back to this first.",
                    "label": 0
                },
                {
                    "sent": "This first piece of reward.",
                    "label": 0
                },
                {
                    "sent": "We're going to be able to release to have some sort of regret bound that says you have to trade off unless you have more information about the world.",
                    "label": 0
                },
                {
                    "sent": "Certainly in the case of Atari, if you add a very high intrinsic reward to palm, it might not actually learn to play the game.",
                    "label": 0
                },
                {
                    "sent": "So regarding the question about your learning distribution, but you are only using the expectation, so do you think that this advantage that you seen performances the artifact of using this neural network and deep neural networks?",
                    "label": 0
                },
                {
                    "sent": "Meaning that if you take this out and replace it with something else, do we still see the same improvement?",
                    "label": 0
                },
                {
                    "sent": "Has somebody tried this?",
                    "label": 0
                },
                {
                    "sent": "So the answers I so I can actually give you the answer?",
                    "label": 0
                },
                {
                    "sent": "The answer is no.",
                    "label": 0
                },
                {
                    "sent": "If you look at a tablet setting I can we have a proof Now that shows that.",
                    "label": 0
                },
                {
                    "sent": "And set up a coupling experiment where side-by-side you're learning the distribution or the Q function with the same random events, and then there is absolutely.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's obviously indicates expectations remain the same.",
                    "label": 0
                },
                {
                    "sent": "More surprising is, even if you have something like linear approximation.",
                    "label": 0
                },
                {
                    "sent": "It should not.",
                    "label": 0
                },
                {
                    "sent": "Actually, it shouldn't give you an advantage, and it might even hurt.",
                    "label": 0
                },
                {
                    "sent": "So so so the theoretical side of that work.",
                    "label": 0
                },
                {
                    "sent": "It was published what was presented in a workshop paper at ACL.",
                    "label": 0
                },
                {
                    "sent": "I can I can send it to you.",
                    "label": 0
                },
                {
                    "sent": "So basically the magic is this Norton network that I at least I would say 95% of the magic is inside the neural network, thank you.",
                    "label": 0
                },
                {
                    "sent": "We're out of time, so thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}