{
    "id": "wttpxlulqjwznf36ehcqc3kznnmqtwil",
    "title": "Online Markov Decision Processes under Bandit Feedback",
    "info": {
        "author": [
            "Gergely Neu, SequeL, INRIA Lille - Nord Europe"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/nips2010_neu_omd/",
    "segmentation": [
        [
            "Thanks for the introduction.",
            "There was a pretty good job so.",
            "So this is about online MDP's under bandit feedback, so our framework is related to regular MDP's where we have a player or an agent."
        ],
        [
            "To interact with environment so it selects action based on the opposite state from the environment and we have a good stochastic model about this environment, but how our framework is different from regular MVP's is that we don't have a fixed reward process here, but instead what we have is a sequence of reward functions are of D which is an individual sequence.",
            "So this is the mode or cases where we have two parts of this state.",
            "One part is is fully observed and we have a good model about it.",
            "This is represented by the environment and we have another part of the state that we can't observe and we don't have any model about it whatsoever, and this is represented by this individual sequence of rewards.",
            "So now this is an online learning problem where we want to minimize regret.",
            "This regret is defined with respect to the best fixed stationary policy, so the regret is essentially just the expected total reward of of this best fixed policy and our.",
            "Total expected regret.",
            "So we're not the first ones to consider this problem.",
            "Actually, Evendarr monster in Cocody have considered this problem first.",
            "Under the following information assumption, which is a pretty strong assumption that after each time step this reward function is revealed in its entirety.",
            "So we know the rewards, even in states that we have not visited.",
            "But they're pretty nice analysts that we borrowed some ideas from them and they prove a nice square root bound.",
            "Yeah, we want to do something similar, but under bandit assumption, so we don't want to assume that we observe the whole remote function after each time step, but only the rewards that we receive in indeed.",
            "So this is the bandit case, human or inching.",
            "Shimkin have considered this problem, and they have proposed an algorithm that has sub linearly yes up in their regret.",
            "And basically this is the result that we want to improve.",
            "And this year's cold, we have considered the simply create case of this problem for episodic MDP's, and we have proved rude man.",
            "For the regret there.",
            "So this is what we do.",
            "We define some estimates of rewards which are."
        ],
        [
            "Usual estimates for adversarial bandits.",
            "It's just the observed rewards divided by the probability of observing that reward.",
            "That's the probability of visiting state action.",
            "Pair X and A and the trick here.",
            "The tricky part is that we want these estimates to be well defined so that all these probabilities that we are dividing by.",
            "ARA are positive, so the trick that we do here is fix our policies for the next end time steps to allow some mixing and so these probabilities will become positive.",
            "The estimates well defined and as unbiased as well.",
            "It can be shown.",
            "So we use these unbiased estimates of rewards to define unbiased estimates of the actual values of the policy that we follow by solving the Bellman equations for our current policy and these estimates of the rewards.",
            "And we use these estimates of the action values to feed an instance of XP3 in each state X.",
            "So this is the algorithm."
        ],
        [
            "That we use, so we need some assumptions for this to work.",
            "For example, we need every policy to generate the stationary distribution over the state space, and we need these stationary distributions be uniformly bounded away from zero by some constant Alpha, and we managed to prove regret band that scales with the 2/3 power of the time horizon T, which is the first explicit regret bound for this problem.",
            "So yeah, here's the proof.",
            "It looks like it.",
            "Just one line if you know if you want to know how to stretch this out for 15 pages and come to see our poster, it's 95, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks for the introduction.",
                    "label": 0
                },
                {
                    "sent": "There was a pretty good job so.",
                    "label": 0
                },
                {
                    "sent": "So this is about online MDP's under bandit feedback, so our framework is related to regular MDP's where we have a player or an agent.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To interact with environment so it selects action based on the opposite state from the environment and we have a good stochastic model about this environment, but how our framework is different from regular MVP's is that we don't have a fixed reward process here, but instead what we have is a sequence of reward functions are of D which is an individual sequence.",
                    "label": 1
                },
                {
                    "sent": "So this is the mode or cases where we have two parts of this state.",
                    "label": 0
                },
                {
                    "sent": "One part is is fully observed and we have a good model about it.",
                    "label": 0
                },
                {
                    "sent": "This is represented by the environment and we have another part of the state that we can't observe and we don't have any model about it whatsoever, and this is represented by this individual sequence of rewards.",
                    "label": 0
                },
                {
                    "sent": "So now this is an online learning problem where we want to minimize regret.",
                    "label": 1
                },
                {
                    "sent": "This regret is defined with respect to the best fixed stationary policy, so the regret is essentially just the expected total reward of of this best fixed policy and our.",
                    "label": 1
                },
                {
                    "sent": "Total expected regret.",
                    "label": 0
                },
                {
                    "sent": "So we're not the first ones to consider this problem.",
                    "label": 0
                },
                {
                    "sent": "Actually, Evendarr monster in Cocody have considered this problem first.",
                    "label": 0
                },
                {
                    "sent": "Under the following information assumption, which is a pretty strong assumption that after each time step this reward function is revealed in its entirety.",
                    "label": 0
                },
                {
                    "sent": "So we know the rewards, even in states that we have not visited.",
                    "label": 0
                },
                {
                    "sent": "But they're pretty nice analysts that we borrowed some ideas from them and they prove a nice square root bound.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we want to do something similar, but under bandit assumption, so we don't want to assume that we observe the whole remote function after each time step, but only the rewards that we receive in indeed.",
                    "label": 0
                },
                {
                    "sent": "So this is the bandit case, human or inching.",
                    "label": 1
                },
                {
                    "sent": "Shimkin have considered this problem, and they have proposed an algorithm that has sub linearly yes up in their regret.",
                    "label": 0
                },
                {
                    "sent": "And basically this is the result that we want to improve.",
                    "label": 0
                },
                {
                    "sent": "And this year's cold, we have considered the simply create case of this problem for episodic MDP's, and we have proved rude man.",
                    "label": 0
                },
                {
                    "sent": "For the regret there.",
                    "label": 0
                },
                {
                    "sent": "So this is what we do.",
                    "label": 0
                },
                {
                    "sent": "We define some estimates of rewards which are.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Usual estimates for adversarial bandits.",
                    "label": 0
                },
                {
                    "sent": "It's just the observed rewards divided by the probability of observing that reward.",
                    "label": 0
                },
                {
                    "sent": "That's the probability of visiting state action.",
                    "label": 0
                },
                {
                    "sent": "Pair X and A and the trick here.",
                    "label": 0
                },
                {
                    "sent": "The tricky part is that we want these estimates to be well defined so that all these probabilities that we are dividing by.",
                    "label": 0
                },
                {
                    "sent": "ARA are positive, so the trick that we do here is fix our policies for the next end time steps to allow some mixing and so these probabilities will become positive.",
                    "label": 0
                },
                {
                    "sent": "The estimates well defined and as unbiased as well.",
                    "label": 0
                },
                {
                    "sent": "It can be shown.",
                    "label": 0
                },
                {
                    "sent": "So we use these unbiased estimates of rewards to define unbiased estimates of the actual values of the policy that we follow by solving the Bellman equations for our current policy and these estimates of the rewards.",
                    "label": 1
                },
                {
                    "sent": "And we use these estimates of the action values to feed an instance of XP3 in each state X.",
                    "label": 1
                },
                {
                    "sent": "So this is the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we use, so we need some assumptions for this to work.",
                    "label": 0
                },
                {
                    "sent": "For example, we need every policy to generate the stationary distribution over the state space, and we need these stationary distributions be uniformly bounded away from zero by some constant Alpha, and we managed to prove regret band that scales with the 2/3 power of the time horizon T, which is the first explicit regret bound for this problem.",
                    "label": 0
                },
                {
                    "sent": "So yeah, here's the proof.",
                    "label": 0
                },
                {
                    "sent": "It looks like it.",
                    "label": 0
                },
                {
                    "sent": "Just one line if you know if you want to know how to stretch this out for 15 pages and come to see our poster, it's 95, thanks.",
                    "label": 0
                }
            ]
        }
    }
}