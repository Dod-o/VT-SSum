{
    "id": "64ykaxuagzna3mi5wlzlnszg3izl3k5b",
    "title": "Mistake bounds and risk bounds for on-line learning algorithms",
    "info": {
        "author": [
            "Nicol\u00f2 Cesa-Bianchi, University of Milan"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "October 2005",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/mcslw04_bianchi_mbrbl/",
    "segmentation": [
        [
            "Online.",
            "Thank you.",
            "So.",
            "Can you see?",
            "Maybe we can switch off this one?",
            "Quit pretending.",
            "Right, this is better?",
            "Or is this too much?",
            "Maybe the periphery is bad.",
            "OK, so.",
            "Yeah.",
            "More like.",
            "OK. OK yeah yeah, no, it's this is.",
            "This is the best I think because it doesn't get to the OK.",
            "Thanks.",
            "So this is this is Richard Research is part of a program that I'm carrying out together with cloud Agent Elam.",
            "Proving that analysis done in the area of online learning that are typically based on adversarial assumptions on the generation of the data can be.",
            "Transform it via reduction, let's say 2.",
            "At risk bounds.",
            "Tail risk bounds in the traditional statistical learning theory setting.",
            "And this reduction from the same mistake bounds to risk bounds is actually very easy to do.",
            "An involves very little statistics and the reason why it involves very latest statistics, because the mistake bounds are proven in the adversarial framework are already very strong.",
            "So actually the conversion into tail bounds is essentially a weakening of the result in some sense.",
            "And.",
            "So.",
            "OK."
        ],
        [
            "So I will now set to the terminology that I'll be using.",
            "This is the statistical traditional statistical learning theory set up and which we have pairs of.",
            "Of instances and labels that are drawn IID according to some fixed area known probability distribution from.",
            "At condition product of two spaces that are generic spaces, we're not making any specific assumption, just like John in the previous talk.",
            "We will have a.",
            "An outcome space O space.",
            "Sorry, I'm now switching notation with respect to Johns.",
            "So in my case Y is the space of labels or values that are you're observing associated with your axis in the training set.",
            "And the D is the decision space or the space in which you from which you take your predictions, OK?",
            "So now learning algorithm is is a mapping from a training set of ID examples to a function that Maps instances into predictions.",
            "OK, so this is exactly like John described and you have a loss.",
            "The notion of a loss Sir, which is just a way of measuring the discrepancy of your prediction.",
            "With respect to a certain outcome.",
            "And this is real valued in this script to be positive.",
            "And the only assumption that we will make.",
            "Concerning the loss is that the loss will be bounded.",
            "Essentially, we will take this this value.",
            "The value is taken by the image of this function will be like a bounded interval from zero to some number.",
            "And in statistical learning theory, our measure of performance in general is the risk and which is defined like.",
            "At the risk of a specific function or hypothesis, age is expected loss on a random example drawn from the same distribution from the same underlying distribution and as a proxy from the risk is it is traditional to look at the empirical risk of some hypothesis, which is just the, let's say, the training error.",
            "What is commonly known as a training error?",
            "OK, so.",
            "Usually the goal of statistical learning theory is to design learning algorithms that generate with moderately large training sets generate with high probability.",
            "I pothys H hat that have a low risk.",
            "Alright, see.",
            "Rulau relatively to some hypothesis space or model class from which they extract the OR each hat.",
            "OK, I give."
        ],
        [
            "If your examples of this is very well known, but just to fix ideas, so this is a general set up with which you can deal.",
            "And we wish you can describe problems like regression with squared loss.",
            "And in this case you have the outcome space, decision space or the real line and your loss is just the square loss.",
            "Square the difference or you can handle binary classification.",
            "Deterministic binary classification, in which your outcome decision space are just the banner in values minus 1 + 1 and the loss indicator function of the fact that your prediction is wrong with respect to the correct label.",
            "Or you can have a case in which the outcomes based decision spaces differences like with classification with absolute loss, which is something we usually statistician don't like, but you can justify it by saying that this is the expected loss.",
            "Of a randomized binary classifier.",
            "In this case you have the outcomes are binary, but your predictions are real elements from this interval.",
            "Real numbers from this interval, and you play with the absolute value of the difference.",
            "OK.",
            "So a very general setup."
        ],
        [
            "And again.",
            "The idea is that is that you want to show that for your learning algorithm, the risk is more with high probability where probability is with respect to that drawing to the random draw of the training set as usual, and the only thing we need to assume at this moment is the fact that the loss function that uses using is.",
            "Bounded the note that we don't insist on the convexity in particular.",
            "Neither neither on convex."
        ],
        [
            "Ignore on differential differential ability everywhere, so this function is convex differentiable.",
            "This is convex, but not differentiable.",
            "This is not convex, so anything is fine as long as it is dismounted.",
            "OK, a very successful approach."
        ],
        [
            "To the analysis of risk in statistical learning theory is the vast nature monkeys theory, and especially the latest developments that are commonly called data dependent VC theory.",
            "So in in this theory I just give up.",
            "I'm not now introducing the theory, I'm just giving an idea of the flavor of the results.",
            "I'm going to describe this result, which is not the tightest possible, but this is something that I want to relate to with my own theory.",
            "So the idea is that you consider a class of models, so it's the set of function from which your algorithm is going to take your His.",
            "I hypothesis is classifier or aggressor function.",
            "OK, and you bound the risk.",
            "You prove a statements that involve the larger deviations of the empirical risk with respect to expected value, which is the risk.",
            "So you you prove a bound of this form that holds simultaneously for every element of your model class of age, or I partisi space which is called in this Kalina learning computational learning theory.",
            "This is called the office space, and these things are called a prothesis, which is confusing for statisticians.",
            "That's the way it is, and so yeah, you would be back until next year.",
            "You can prove that.",
            "With high probability for any element of your model class, the risk of this this guy is upper bounded by its empirical risk on the training set of size N. Plus a term which sum of two terms, let's say, which depends on again an empirical risk and this is what and on quantities like back nature and kiss dimension of suitable generalization of that that are measures of global properties of your Class H and these bounds are particularly nice becausw.",
            "They involve all the they involve statistics that are that are really real statistics that are measurable on your on your training set, and in particular in this case.",
            "Here you see that if you are lucky enough to find.",
            "An elements in hypothesis or classifiers or whatever in your mother class, whose empirical risk is small enough then basically the this term will vanish.",
            "So this is close to zero and dominating term get dominating.",
            "Additional term will be this one.",
            "So here you see you have the this kind of bounds show the interplane between the usual rate 1 / sqrt N of your convergence of means to expectations and fast rate one over North, which is takes which kicks in when you're.",
            "Let's say your Bayes optimal classifier is in your model class, for instance, or when you're able.",
            "Anyway, when you were able to find.",
            "And a classifier with a banishing empirical training error.",
            "OK.",
            "I published.",
            "With respect to any distribution on the.",
            "Right?",
            "Yeah, this I mean, the nice thing is that you have this.",
            "This of course depends on the distribution.",
            "So if you have a nice distribution then you can observe a faster rate.",
            "So I want now to that.",
            "So these bands are obtained via via sophisticated statistical and at least for me sophisticated statistical analysis.",
            "Anne and.",
            "I I want to show you now that with a very simple with just one application of a simple statistical inequality you cannot taste.",
            "You can obtain similar bounds.",
            "For arbitrary learning algorithms, so note that this this bound is doesn't doesn't involve any statement on the learning algorithm that you're using.",
            "This is a strength, but is also a weakness.",
            "Is I mean this?",
            "Is this a property of the class?",
            "Is not the property or the property of the algorithm they're using to pick elements from the class?",
            "But it might be the case that if you look at.",
            "Joint properties of the algorithm and the class from which the model class from each of the algorithm which the algorithm is working.",
            "You might be able to get better bounds or at least different bounds, and this is the way I want to the path I want to take an.",
            "We're going to look at small subclasses of H. Hopefully small subclasses of H that are generated by the action of the algorithm on the training data.",
            "So we.",
            "We will be considering specific algorithms and the way.",
            "The behavior of the algorithm that we are considering on the training data will define a specific subclass of age, which will be the class on.",
            "That will replace this this big age.",
            "In this analysis somehow so.",
            "Really we are aiming at algorithmic dependent risk analysis rather than class model class dependent or a model.",
            "Whatever independent analysis which is the case of the document theory OK?",
            "So.",
            "So now."
        ],
        [
            "We do this to create this small subclass of age which is generated between interaction of the algorithm data.",
            "We we proceed in this way.",
            "OK so we generate an ensemble of hypothesis or functions which are obtained by feeding to the algorithm 8A which is a fixed learning algorithm of your choice.",
            "Increasing prefixes of your training set so.",
            "You look at the training set as a sequence of ordered sequence of examples and you feed it to the algorithm at the initial.",
            "Initially, you don't feed anything, you just get the default hypothesis.",
            "Then you feed the first example you get, passes each one you feed the first examples.",
            "If you get, it passes through H2 and so on so forth until you feed all of them.",
            "And that by doing that you get what we what I call an ensemble of functions, which is generated by a on this ordered sequence.",
            "Not that OK. Of course this ensemble will depend on the specific ordering.",
            "Of the sequence, but this is.",
            "What we want for our analysis OK?",
            "And of course you can do this.",
            "You can run this schema.",
            "This schema looks looks like a sort of a strange kind of cross validation, right?",
            "But it's sort of like this kind of progressive because.",
            "You're giving longer and longer pieces of the training data to the algorithm, and of course you can do this with any learning algorithm, but usually it takes a lot of time.",
            "There are specific algorithms for which this is very.",
            "This is very natural to do this very natural thing to do, and these are the.",
            "So called it online or implement incremental learning algorithms like the perceptron and all the variance of this algorithm.",
            "OK or Ridge regression for instance in for regression and many other algorithms.",
            "These algorithms work really incrementally, so they basically they look at the data, the training data one by one and then adjust their H. A little bit a little bit each time a little bit.",
            "If, for instance, in the classification case, if a mistake is made on the new element.",
            "New example that has been added to the to the data.",
            "Once this fed.",
            "OK, so.",
            "Again it.",
            "This is a general scheme and that can be applied to any to any learning algorithm, but this is particularly natural and effective for incremental learning algorithms.",
            "So the balance will all the bounce.",
            "That will prove it will hold for any A.",
            "But keep in mind that for efficiency reasons that it might be sensible to use this.",
            "This kind of of.",
            "I'll make the algorithm only for incremental learners.",
            "OK, looking at properties of this ensemble of functions that are generated and this is sample and functions contains what you will expect to choose, which is the hypothesis each subnet has seen the whole set of data.",
            "This is the guy that you would expect to.",
            "Choose as your yes.",
            "Sorry.",
            "Yeah, if you have a batch algorithm like a support vector machine that's super linear in the number of samples the running time.",
            "Yes, rice can do is instead of thinking about feeding an individual examples one at a time.",
            "You could feed feed batches of examples right?",
            "First 10 in the 1st right, right, right, right, right, right.",
            "I say yeah, yeah, yeah yeah yeah yeah yeah OK OK thanks.",
            "Thanks, sorry I mean yes, I just say that this is more natural.",
            "But yeah, then there are ways to, I mean.",
            "To make it efficient, also even for arguments like SVM that are typically match.",
            "OK, so now.",
            "As I said, the teacher in this ensemble, because this is the hypothesis that you have generated.",
            "Once you've seen the whole training set, but I.",
            "You know, building a theory you can use vectoring theory to bound the, for instance, for SVM you can use battering theory to bound the risk of this guy.",
            "I want to use now a different theory which will prove statements about the average risk of the elements of disassemble and proving a statement about the average risk of the elements of disassemble will be much easier than proving a statement for the last guy.",
            "OK, because I because of some intuitively there there's some natural averaging argument that you can apply, which actually will connect very well with the with the process that has generated disassemble of a prothesis question so far.",
            "Everything is clear."
        ],
        [
            "OK, so now the goals that I want to set up and I will cover in the rest of the talk, the following.",
            "First of all, I want to bound the average risk of being sample in terms of statistics, which we pray, replace the replaces the training error of the back nature vectoring theory and these statistics has to do with the incremental performance of the algorithm, the data and that will define it in a moment.",
            "It's a very simple quantity which is very easy to compute.",
            "And then I want to go further and the show away you're constructing.",
            "Wait for finding an element of the assemble, whose risk is close to the assemble average.",
            "So while this is useful if your losses convex and your decision space is convex, then you can you can just take."
        ],
        [
            "You can just take the average out the prediction.",
            "You can average the predictions of these guys in the assemble and you will get a valid prediction because you're.",
            "Decisional space is convex and by instance inequality."
        ],
        [
            "The risk of your average prediction will be smaller than the average risk of the prediction in this sample, because the loss is convex.",
            "So for convex losses we are just just fine to bound the average risk of the example, but if you want to do classification then this doesn't work anymore, so you have to exhibit a specific guidance sample that achieves nearly achieves the average risk of the sample.",
            "To make it effective and we will do that this way.",
            "And then.",
            "Also, you would like to relate.",
            "These results with the with what is usually your your.",
            "Your term reference, which is the risk of the best class, the best model in your class.",
            "So the smallest risk of any element in X, so we will.",
            "And for doing that will be.",
            "We will have to take assumptions on this specific algorithm learning algorithm that will be using up to here.",
            "It's completely it's.",
            "It will hold for any algorithm A here will have to take.",
            "Make us we will.",
            "We will have to look at specific algorithms."
        ],
        [
            "OK, so bounding the average risk is really simple becausw the way with which constructed the our assemble together with the fact that the training set has been drawn IID.",
            "Makes the process.",
            "I mean it creates a simple stochastic process that corresponds to a martingale difference sequence.",
            "So precisely, if you remember each hypothesis within index, let's say T -- 1 has been generated there only using the first T -- 1 elements of the training set as you can."
        ],
        [
            "See in this picture.",
            "So each two is generated using only the first 2 elements of the training set, OK?"
        ],
        [
            "So if I look now I can compare the risk of this guy with this deterministic quantity.",
            "Sorry, it's not me, it's the random quantity which is the loss of the same function on the next example on the next random example this has been generating using the first T -- 1 examples and measuring its loss on the next upcoming example.",
            "Now this thing if you take the expectation of this difference conditioned on the first team.",
            "I know T -- 1 examples drawn.",
            "This is 0 because by definition.",
            "If for personal, if I fix this sequence then this is deterministic element because it is defined by.",
            "The first T -- 1 elements of the training set, so this is deterministic hypothesis, deterministic function and then by definition this is via is an iid pair and this by definition is the risk the spectation, sorry this spectation of this.",
            "Is the risk of HT minus one?",
            "This is 0.",
            "OK.",
            "So now.",
            "We can look at the associated Martingale if we sum up over T. OK, this becomes a mountain GAIL and we can split this difference into two sums.",
            "And divide by N. And now we look at this two elements and we see that.",
            "This is the average risk of the elements in this new sample.",
            "There's a little thing here that you might notice.",
            "It's a little caveat that an this is something this is a statistic.",
            "It's really depends.",
            "I can measure once I've given the sample, I can generate the ensemble and measure this quantity, and this is just this is what we call the mistake.",
            "The let's say.",
            "The online loss of an algorithm, so it's the sum of the losses generated by made by each function on the next example of the training set.",
            "So I am again."
        ],
        [
            "An example in this schema here.",
            "So I'm generating a guy.",
            "Which depends only on the 1st example.",
            "And now I'm testing this guy on the next one and then measuring the loss.",
            "Then I'm again feeling this back.",
            "Getting the second guy and I'm testing this second guy on the third example and measuring the loss and I'm summing up this loss.",
            "Is this?",
            "This is something that if you are able to run efficiently in the algorithm on in this way, then you can measure these statistics this statistic efficiently.",
            "There's no, there's no problem."
        ],
        [
            "OK, so the little trick is that I'm throwing away the last hypothesis here.",
            "You see, because.",
            "This this sum goes up to N -- 1.",
            "And there is a.",
            "There is a reason why I'm throwing away this guy.",
            "Somehow I'll try.",
            "I mean it's.",
            "I'm not using.",
            "I'm not using this guy here in the analysis, which is which is the guy who has seen the most of the data.",
            "But just I'm just stopping here.",
            "So it doesn't make a lot of difference, right?",
            "In denies it will be used.",
            "OK.",
            "Smell.",
            "Once I haven't monticola different sequence and.",
            "The elements, I mean that the increments of the martingale are bounded, because this loss is bounded.",
            "Now I can really hope to show that with high probability this quantity is close to this, so I can relay to statistic that I can easily measure on my training set two and interesting.",
            "Interesting.",
            "Think which I can't observe, which is the average risk of the sample.",
            "OK, so the tool so far is no snow statistics, it's just the definitions.",
            "So the only piece of statistical theory I'll be using is."
        ],
        [
            "First time bound.",
            "Which is a very basic basic but useful tool.",
            "And this is very well known.",
            "Basically education tool.",
            "It says that they've Z1Z2 blah blah is a martingale difference sequence with increments bounded by one.",
            "Let's see.",
            "Just cause we assume that we assume now that our loss is bounded between 01 to just two.",
            "To simplify things.",
            "And the.",
            "And the sum of conditional variances of our matching the different sequence which is called in statistics, the quadratic variation, the total quadratic variation of the multi Gale.",
            "OK, this is visa ban.",
            "Then it can be the case.",
            "That so.",
            "Let's say it will happen with very little probability that.",
            "Will have simultaneously.",
            "A large value of the martingale will be large.",
            "And the value of the total quadratic variation will be small.",
            "This can happen at the same time or will happen with exponentially small probability.",
            "Where?",
            "This depends on both S&K.",
            "So for any two bounds.",
            "On the Martin Gale and the total projected variations, it can be the case that one is bigger, there is more.",
            "Alright.",
            "We talked for a little bit.",
            "OK.",
            "So this is.",
            "It's very.",
            "It's perfect for our purposes because we have a magical different sequence with increments bounded 'cause we look at lower losses bounded so we can.",
            "Black this.",
            "Anne."
        ],
        [
            "Into our but this to our mountain girl and now it takes a little algebra to carry out the computations becausw.",
            "OK, first of all.",
            "You have a certain number of things that you want to take care of.",
            "You do know a value of the variance, so you have to stratify over the possible values of the variance.",
            "Which will bring in a logarithmic additional factor, which is this an then at the end that you can't keep the variance inside that you have to use the fact that the variance is the loss is bounded, variance is trivially the variance of the losses tribute bounded by the expected value of the risk.",
            "Trivially, here this is a crude bound, but you have to.",
            "You have to use this.",
            "So if you do this approximation, you first stratify your analysis over possible various of the variance, and then you plug this crude upper bound and you do it a little bit of calculations.",
            "You end up with a bound.",
            "It looks like this.",
            "So this hold again with high probability.",
            "There's no I.",
            "This is not spelled out in the bound, but just to to make it simpler.",
            "You have the average risk of this sample is at most base business.",
            "This MCV is is what I is what I called.",
            "The cumulative loss.",
            "Of our incremental learning algorithm.",
            "So it's again the sum of the losses that each each function generated by the algorithm is made on.",
            "The next example in the training sequence averaged.",
            "OK. And so this is the hour.",
            "What this is what replaces the training error or the empirical risk in the dimension analysis?",
            "OK. And this quantity, I mean the.",
            "The sum of the community loss because the loss is bounded varies between a constant and something that is linear in the training set size.",
            "So this is if you look at this bound you can say that OK, you can express this bound by saying that these two these two guys are close to each other.",
            "So this is a good proxy.",
            "This statistics is a good proxy for the average risk of the sample, and the slack is something again that has the same behavior.",
            "Like I mean the same flavor the same.",
            "Qualitative appearance as the one which was gotten using VC theory because you see again you have a 1 / N term where instead of the VC dimension where you have the log of the number of let's say number of mistakes in a classification problem just to make it concrete, that is in a classification problem.",
            "This is just the number of mistakes.",
            "Computing this way.",
            "And here you have a term which expresses the same tradeoff that we observed in the VC theory.",
            "Because if you bring in the 1 / N into the square you have here you have.",
            "We have this term and some bent over N and then you have something which is.",
            "Slightly more than slightly super slightly above the constant, it's going slowly.",
            "Divided by square root of N. So if if this divided by N, so this term is Spanish ING, then again you have a rate of 1 / N otherwise you have intermediate rates between 1 / N and 1 / sqrt N. Again, it's the same kind of tradeoff we observed before.",
            "OK.",
            "So this is the first 1st result, yes?",
            "Tumse turn off style balance work for.",
            "Martin gives it in the last call.",
            "He had some paper.",
            "Yeah, he had he had, I mean the same topics of his results is the same of hours.",
            "He was claiming that you can't use them.",
            "Bernstein for doing this kind of analysis.",
            "And here we proved everything from scratch, which was quite good.",
            "But I think that when you work through the implications, C can become one and the log of M. Sabin can also become one.",
            "No, I mean in we did it.",
            "We compared the two.",
            "I mean, these synthetics were exactly the same.",
            "I don't think you can, Constance.",
            "Oh, the constants.",
            "Yeah, I don't know about it.",
            "I don't know about it.",
            "Yeah yeah yeah I don't know about it, but I mean the main one of the points we would like to make is that you don't need a lot of.",
            "I mean you can use standard off the shelf statistical tools.",
            "Very simple and get these bounds.",
            "Maybe you can, you can work on the constants by reproving everything from scratch.",
            "Yes, I can come from the bound on the right because we ask.",
            "We include some it's come from running the loss, but it's also it is also dependent on the fact that you have.",
            "Yeah you are.",
            "Right and and you have algebra.",
            "I mean it's not like you have a series of inequality just to stratify over the values of the variance.",
            "So this brings in a little bit of you know additional constants.",
            "But in our case I mean we have this is this is like.",
            "In this way, for this term is like 30 an for this term is like 4 or something like that.",
            "We can do better, but I'm not after just kind of.",
            "Just do better straight out of tongues.",
            "Yes.",
            "Yes, in terms of constants, that's very possible.",
            "That's very possible.",
            "I think you can also avoid stratifying on the.",
            "I don't think so.",
            "Now we look at that.",
            "We look at that.",
            "Are you on the various?",
            "Doing these VC to step down.",
            "It's not necessary.",
            "I think it's right here.",
            "Is not necessary in the same sense?",
            "I'm not sure about it.",
            "I mean, I mean looking at his paper, we didn't find a way of avoiding that, and he at the end I mean at the end he gets insane.",
            "He gets the same as us, possibly with better constants, yes?",
            "I mean, these tests from a tighter bound which involves the KL divergent.",
            "But then if you want to get a readable bound then you end up with this.",
            "Oh, OK."
        ],
        [
            "Game now.",
            "Second part, so this Mount you can apply it.",
            "Directly."
        ],
        [
            "Two regression with squared loss, for instance cause.",
            "This would be the square loss and then you can.",
            "You can just say OK bye yes inequality.",
            "If I take the average hypothesis of my sample.",
            "It's basically high probability bounded by this statistic.",
            "It's.",
            "That I can do."
        ],
        [
            "Now, if you do classification instead, then you need to work out a little to work a little bit more and to really show that some hypothesis is.",
            "I mean, you know that there exists at least some one classifier in your ansambl whose risk is more than the average, of course, so you want to.",
            "Like this guy or some guy close to it and the way to do that is to use a sequence simple sequential testing procedure as you go along, generating your ensemble of hypothesis.",
            "So there is that each time you generate, generate the new hypothesis you test it on the remaining part of the training set.",
            "OK, and then you pick at the end of the sweep over the training set you pick the guy that minimizes.",
            "Which would be a guy from the sample which minimizes penalized the risk estimate, which takes into account the fact that which has to turn this penalize the risk estimate as two terms.",
            "One is the.",
            "Average loss.",
            "In curd by H. Subte on the remaining part of the sample and the other is a penalization term that takes into account the fact that we are comparing empirical risks over different portions of the training set, because each guy will be tested on smaller and smaller portion of the training set.",
            "So this is by the way.",
            "If you are running a kernel based at the.",
            "Algorithm like kernel perceptron or Kernel Ridge regression.",
            "This doesn't cost you anything in terms of computational overhead.",
            "This testing thing becausw.",
            "And because the kernel based algorithms like Perceptrons are built in such a way that to evaluate.",
            "Some hypothesis you need to evaluate all day prothesis were previously constructed, the incrementally up to that point, so it's somehow it's built in.",
            "This testing is built in for free in the evaluation mechanism for kind of based perceptions, or.",
            "Ridge regression.",
            "OK, so you lose a little bit.",
            "Indeed.",
            "You retain the main properties of the old inequality's, but you lose a little bit becausw here you have now a log squared N where N is the sample side.",
            "Here you have a line instead of having L of msub.",
            "And that's OK. That's what you lose.",
            "OK, now.",
            "We can, so far this holds.",
            "This holds for any algorithm.",
            "This does only assumption is bounded loss.",
            "Nor their assumptions.",
            "Game.",
            "This in particular this bound holds for non convex loss functions like in the classification case.",
            "Now suppose now you want to make."
        ],
        [
            "Concrete examples for specific learners A.",
            "And then in particular, you want to relate your online statistic.",
            "To the best risk of any any.",
            "Any element of your in your mother class.",
            "So we can, for instance, use Bosc, an aggregating forecaster for regression with squared loss, which is a nice variation on Ridge regression.",
            "Which achieves.",
            "On any individual sequence of any individual data sequence bound.",
            "On the cumulative squared loss which is on the online statistics, let's say which is better than the bound of the Ridge regression.",
            "So we can define each star Subban.",
            "And this is the.",
            "Optimal a penalized element of your over in our model class.",
            "OK. Where the penalization?",
            "Vanish is as the sample size increases.",
            "OK. And that we can show now.",
            "That we hype by combining the Vox analysis for the aggregating forecaster with simple.",
            "Large deviation analysis, same same flavor as before.",
            "We can prove that this online statistic will be related.",
            "To the up the penalized risk of the optimal penalized risk in the class for any reproducing kernel.",
            "Sorry, we're working here in any reproducing kernel Hilbert space of functions, so we're.",
            "We're using so H will be elements of this reproducing kernel Hilbert space of functions.",
            "And we can prove that basically again.",
            "This online statistics is related to the optimal risk in the class.",
            "Cluster one term which depends square root of the risk divided by N. And this other term, which depends on why which is the.",
            "Size the absolute.",
            "Why is the absolute largest absolute value of of an outcome in our data sequence?",
            "So let's say is.",
            "Is the right so this is a regression problem, so we are giving our labels are real values, so this is the largest norm.",
            "The largest models of the values that you can we may observe.",
            "Ann, this is the sum of the logs of 1 plus the eigenvalues of the Canon of the kernel matrix.",
            "How many eigenvalues away?",
            "If you want to current space you want to have a more eigenvalues.",
            "Definitely more than the number of examples that we've seen so far.",
            "This can you can this summer will run, definitely not for at most over N elements.",
            "'cause these are the number of examples that you've added in your kernel matrix.",
            "Is this song?",
            "Yes, if you are working in a finite dimensional space.",
            "This is bounded by.",
            "The same, so this was the.",
            "The sum of the logs is bounded.",
            "So Sam.",
            "I learned one plus Lambda.",
            "I is bounded by D. Ln N + 1, where D is the.",
            "D is the dimension.",
            "Of the space in which the axis leave.",
            "So if in a finite dimensional space.",
            "If.",
            "You can you can you can use this bound.",
            "OK, four.",
            "Space.",
            "Not in a no.",
            "I mean in this case they're they're hoping you're betting on the fact that your kernel will have the fastly decreasing eigenvalues like caching kernels.",
            "So this effectively this summer will only take into account few terms.",
            "Few terms of this time will be.",
            "AC will be heavy.",
            "By trace.",
            "Oh yeah, but that's that's.",
            "Yeah, that's kind of accrued.",
            "So perception perception styles are going.",
            "Algorithms will give you that race, but then so here the bound is logarithmic in their eyes and and perception is not.",
            "We do half, let's say for regression with your half is not.",
            "More examples in classifications in classification OK."
        ],
        [
            "So now.",
            "So this is a."
        ],
        [
            "Found that.",
            "This is about the relates directly.",
            "The online statistic with the.",
            "A risk of the best guy in your class.",
            "Now in the classification, you can't quite do that because.",
            "You don't.",
            "And it uses this bound users.",
            "This bound comes from.",
            "An analysis for individual sequences.",
            "Of of this quantity here.",
            "So you start for from a pointwise bound that says OK, this quantity is not larger for any individual sequence that a certain quantity which is related to this and then you take expectations and a little bit of logic creations here and you get the band like this.",
            "For a classification you can't quite do that so."
        ],
        [
            "You can take the second step.",
            "You can take it to expectations, so you just look at the pointwise bound.",
            "But still you might be interested in getting getting idea of how this quantity is related to properties of the data sequence of the year training sequence.",
            "OK, so this online analysis of online algorithms tell you that for.",
            "Any sequence of of data with binary labels.",
            "So we are embedding classification example set up.",
            "Let's let's limit ourselves to for simplicity, to the linearly separable case.",
            "Still we are working in reproducing kernel Hilbert space.",
            "Now.",
            "So you know that for kernel perceptron, OK, so suppose that we're nearly separable space, and then there is some linear separator F which has a margin gamma.",
            "K and Lambda one Lambda, 2 on the eigenvalues of the kernel metrics.",
            "But this time this eigen values are only compute the kernel matrix only includes the instances that correspond to mistaken examples.",
            "Would you mean mistaken examples?",
            "That is taken examples are.",
            "Are those examples in your training sequence on which the current hypothesis made a mistake?",
            "So in which?"
        ],
        [
            "On which this quantity.",
            "Was 1 four o'clock for classification.",
            "OK, so for classification this this is the 01 loss can be 01 at each time HT minus one makes a mistake on the next example you include the XT in the kernel matrix.",
            "You build your kind of metrics all including these guys and you're bound will depend."
        ],
        [
            "On the eigenvalues of this kind of metrics.",
            "So this it typically is much smaller, hopefully is much smaller than the whole kernel metrics that you build from the whole train set.",
            "OK, so in this case for the perception, the bound depends on the trace of the econometrics.",
            "And so there's some sort again values, and you can have some more sophisticated versions like the 2nd order perception in which you have a subtler dependence between the whole spectrum.",
            "So I mean you have the sum of the locks.",
            "Of the eigenvalues and you also have the terms that take into account the interaction of the linear separator on the specific elements of the data sequence.",
            "Examples of this?",
            "I mean, I'm not going into details on this, just to show you that for many algorithms you can get pointwise bounds on this quantity on your online statistic that can be plugged in to get final bound on.",
            "On the on your on your risk, so again, you can use this once.",
            "And you can plug them in."
        ],
        [
            "Here.",
            "To get right more general bound that depends on on several quantity.",
            "Several properties of the data signals OK.",
            "This is basically.",
            "What I wanted to say."
        ],
        [
            "So.",
            "This is a different view on the study of risk in statistical learning theory, which is based on.",
            "Not on the study of the properties of your money class, but on the study of the properties of the interaction of your arguing with the model and data.",
            "So which I think is a simple, nice, easy to understand.",
            "And you get data dependent bounds for any learning that are not in terms of training error, but in terms of a different statistic, which I called online statistic.",
            "And you can even get bounced for point is related to this, with some penalization, for instance for specific learners, and the main one nice thing of this approach is that you don't really need any fancy statistical tool, you don't need the red marker.",
            "Rademacher, Ann An average is.",
            "It's it's, it's really.",
            "It's really easy, and the reason why it's easy because you start from the game theoretic analysis of the online statistic.",
            "Which.",
            "Provide you, I mean.",
            "No, sorry, it's really easy because the you exploit specific way of running your learning algorithm on the data that generates a very simple stochastic process, and Martindale that it's very easy to analyze this way.",
            "Maybe the way to explain it, and if you want to get detailed bounds then you can use the online theory theory of online algorithms to get ahold.",
            "Like in this case.",
            "Sorry to get ahold groups.",
            "OK, to get ahold of."
        ],
        [
            "Yeah, to get an idea of how these quantities behave.",
            "So there are theories that explains for different tags will give you an idea of these quantities.",
            "OK, this is about it.",
            "So I'd like to add that this really works in practice.",
            "Money code and I just had a bunch of these progressive validation bounds on a bunch of empirical data sets.",
            "Works quite well.",
            "Thanks because I have experience here that can.",
            "I mean, this is really like simple experiments on.",
            "On a textual data."
        ],
        [
            "So maybe just want to say very very just one minute remark on this.",
            "We did experience.",
            "We tried these bounds on the real data and so this is the."
        ],
        [
            "The risk estimated on the test set on a separated test set.",
            "This is the estimate using the online statistic using the theory an you see that qualitatively the behavior will fit a little bit with constants, so it's again it don't take it as gold, but you see that the behavior is nice, and I mean the absolute difference is not big, and then we also."
        ],
        [
            "Broke up this into.",
            "These are averages over over 50 different binary classification problems and then you can see the breakup of the different averages.",
            "So this is the estimate and this is the actual risk.",
            "And if you move if you feed more and more."
        ],
        [
            "Exam."
        ],
        [
            "'cause you see that that."
        ],
        [
            "It's converged nicely apart."
        ],
        [
            "From the few glitches, but more or less, it's really nice.",
            "I mean, even in here in the tail where you have some very small risk, it follows pretty nicely.",
            "OK, so give you some some idea of how things go in practice.",
            "Yeah.",
            "Shameless plug, shameless plug.",
            "Second theorem I presented yesterday was actually very similar in the reasoning stably also taken after you have practice sequential prediction algorithm.",
            "And in the end, what you look at is what is the average of its performance overtime and that is.",
            "Something which is apparently in general easy to analyze so many papers recently where Varonis, also several papers, very states that, well, exactly this quantity, which your context turns out to be.",
            "Interest.",
            "OK, so let's start with further questions then.",
            "535"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Online.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Can you see?",
                    "label": 0
                },
                {
                    "sent": "Maybe we can switch off this one?",
                    "label": 0
                },
                {
                    "sent": "Quit pretending.",
                    "label": 0
                },
                {
                    "sent": "Right, this is better?",
                    "label": 0
                },
                {
                    "sent": "Or is this too much?",
                    "label": 0
                },
                {
                    "sent": "Maybe the periphery is bad.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "More like.",
                    "label": 0
                },
                {
                    "sent": "OK. OK yeah yeah, no, it's this is.",
                    "label": 0
                },
                {
                    "sent": "This is the best I think because it doesn't get to the OK.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So this is this is Richard Research is part of a program that I'm carrying out together with cloud Agent Elam.",
                    "label": 0
                },
                {
                    "sent": "Proving that analysis done in the area of online learning that are typically based on adversarial assumptions on the generation of the data can be.",
                    "label": 0
                },
                {
                    "sent": "Transform it via reduction, let's say 2.",
                    "label": 0
                },
                {
                    "sent": "At risk bounds.",
                    "label": 0
                },
                {
                    "sent": "Tail risk bounds in the traditional statistical learning theory setting.",
                    "label": 1
                },
                {
                    "sent": "And this reduction from the same mistake bounds to risk bounds is actually very easy to do.",
                    "label": 0
                },
                {
                    "sent": "An involves very little statistics and the reason why it involves very latest statistics, because the mistake bounds are proven in the adversarial framework are already very strong.",
                    "label": 0
                },
                {
                    "sent": "So actually the conversion into tail bounds is essentially a weakening of the result in some sense.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will now set to the terminology that I'll be using.",
                    "label": 0
                },
                {
                    "sent": "This is the statistical traditional statistical learning theory set up and which we have pairs of.",
                    "label": 1
                },
                {
                    "sent": "Of instances and labels that are drawn IID according to some fixed area known probability distribution from.",
                    "label": 1
                },
                {
                    "sent": "At condition product of two spaces that are generic spaces, we're not making any specific assumption, just like John in the previous talk.",
                    "label": 0
                },
                {
                    "sent": "We will have a.",
                    "label": 0
                },
                {
                    "sent": "An outcome space O space.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I'm now switching notation with respect to Johns.",
                    "label": 1
                },
                {
                    "sent": "So in my case Y is the space of labels or values that are you're observing associated with your axis in the training set.",
                    "label": 0
                },
                {
                    "sent": "And the D is the decision space or the space in which you from which you take your predictions, OK?",
                    "label": 0
                },
                {
                    "sent": "So now learning algorithm is is a mapping from a training set of ID examples to a function that Maps instances into predictions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is exactly like John described and you have a loss.",
                    "label": 0
                },
                {
                    "sent": "The notion of a loss Sir, which is just a way of measuring the discrepancy of your prediction.",
                    "label": 0
                },
                {
                    "sent": "With respect to a certain outcome.",
                    "label": 0
                },
                {
                    "sent": "And this is real valued in this script to be positive.",
                    "label": 0
                },
                {
                    "sent": "And the only assumption that we will make.",
                    "label": 0
                },
                {
                    "sent": "Concerning the loss is that the loss will be bounded.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we will take this this value.",
                    "label": 0
                },
                {
                    "sent": "The value is taken by the image of this function will be like a bounded interval from zero to some number.",
                    "label": 0
                },
                {
                    "sent": "And in statistical learning theory, our measure of performance in general is the risk and which is defined like.",
                    "label": 0
                },
                {
                    "sent": "At the risk of a specific function or hypothesis, age is expected loss on a random example drawn from the same distribution from the same underlying distribution and as a proxy from the risk is it is traditional to look at the empirical risk of some hypothesis, which is just the, let's say, the training error.",
                    "label": 0
                },
                {
                    "sent": "What is commonly known as a training error?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Usually the goal of statistical learning theory is to design learning algorithms that generate with moderately large training sets generate with high probability.",
                    "label": 0
                },
                {
                    "sent": "I pothys H hat that have a low risk.",
                    "label": 0
                },
                {
                    "sent": "Alright, see.",
                    "label": 0
                },
                {
                    "sent": "Rulau relatively to some hypothesis space or model class from which they extract the OR each hat.",
                    "label": 0
                },
                {
                    "sent": "OK, I give.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If your examples of this is very well known, but just to fix ideas, so this is a general set up with which you can deal.",
                    "label": 0
                },
                {
                    "sent": "And we wish you can describe problems like regression with squared loss.",
                    "label": 1
                },
                {
                    "sent": "And in this case you have the outcome space, decision space or the real line and your loss is just the square loss.",
                    "label": 0
                },
                {
                    "sent": "Square the difference or you can handle binary classification.",
                    "label": 1
                },
                {
                    "sent": "Deterministic binary classification, in which your outcome decision space are just the banner in values minus 1 + 1 and the loss indicator function of the fact that your prediction is wrong with respect to the correct label.",
                    "label": 0
                },
                {
                    "sent": "Or you can have a case in which the outcomes based decision spaces differences like with classification with absolute loss, which is something we usually statistician don't like, but you can justify it by saying that this is the expected loss.",
                    "label": 1
                },
                {
                    "sent": "Of a randomized binary classifier.",
                    "label": 0
                },
                {
                    "sent": "In this case you have the outcomes are binary, but your predictions are real elements from this interval.",
                    "label": 0
                },
                {
                    "sent": "Real numbers from this interval, and you play with the absolute value of the difference.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So a very general setup.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And again.",
                    "label": 0
                },
                {
                    "sent": "The idea is that is that you want to show that for your learning algorithm, the risk is more with high probability where probability is with respect to that drawing to the random draw of the training set as usual, and the only thing we need to assume at this moment is the fact that the loss function that uses using is.",
                    "label": 1
                },
                {
                    "sent": "Bounded the note that we don't insist on the convexity in particular.",
                    "label": 0
                },
                {
                    "sent": "Neither neither on convex.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ignore on differential differential ability everywhere, so this function is convex differentiable.",
                    "label": 0
                },
                {
                    "sent": "This is convex, but not differentiable.",
                    "label": 0
                },
                {
                    "sent": "This is not convex, so anything is fine as long as it is dismounted.",
                    "label": 0
                },
                {
                    "sent": "OK, a very successful approach.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the analysis of risk in statistical learning theory is the vast nature monkeys theory, and especially the latest developments that are commonly called data dependent VC theory.",
                    "label": 0
                },
                {
                    "sent": "So in in this theory I just give up.",
                    "label": 0
                },
                {
                    "sent": "I'm not now introducing the theory, I'm just giving an idea of the flavor of the results.",
                    "label": 0
                },
                {
                    "sent": "I'm going to describe this result, which is not the tightest possible, but this is something that I want to relate to with my own theory.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that you consider a class of models, so it's the set of function from which your algorithm is going to take your His.",
                    "label": 0
                },
                {
                    "sent": "I hypothesis is classifier or aggressor function.",
                    "label": 0
                },
                {
                    "sent": "OK, and you bound the risk.",
                    "label": 0
                },
                {
                    "sent": "You prove a statements that involve the larger deviations of the empirical risk with respect to expected value, which is the risk.",
                    "label": 0
                },
                {
                    "sent": "So you you prove a bound of this form that holds simultaneously for every element of your model class of age, or I partisi space which is called in this Kalina learning computational learning theory.",
                    "label": 0
                },
                {
                    "sent": "This is called the office space, and these things are called a prothesis, which is confusing for statisticians.",
                    "label": 0
                },
                {
                    "sent": "That's the way it is, and so yeah, you would be back until next year.",
                    "label": 0
                },
                {
                    "sent": "You can prove that.",
                    "label": 0
                },
                {
                    "sent": "With high probability for any element of your model class, the risk of this this guy is upper bounded by its empirical risk on the training set of size N. Plus a term which sum of two terms, let's say, which depends on again an empirical risk and this is what and on quantities like back nature and kiss dimension of suitable generalization of that that are measures of global properties of your Class H and these bounds are particularly nice becausw.",
                    "label": 0
                },
                {
                    "sent": "They involve all the they involve statistics that are that are really real statistics that are measurable on your on your training set, and in particular in this case.",
                    "label": 0
                },
                {
                    "sent": "Here you see that if you are lucky enough to find.",
                    "label": 0
                },
                {
                    "sent": "An elements in hypothesis or classifiers or whatever in your mother class, whose empirical risk is small enough then basically the this term will vanish.",
                    "label": 0
                },
                {
                    "sent": "So this is close to zero and dominating term get dominating.",
                    "label": 0
                },
                {
                    "sent": "Additional term will be this one.",
                    "label": 0
                },
                {
                    "sent": "So here you see you have the this kind of bounds show the interplane between the usual rate 1 / sqrt N of your convergence of means to expectations and fast rate one over North, which is takes which kicks in when you're.",
                    "label": 0
                },
                {
                    "sent": "Let's say your Bayes optimal classifier is in your model class, for instance, or when you're able.",
                    "label": 0
                },
                {
                    "sent": "Anyway, when you were able to find.",
                    "label": 0
                },
                {
                    "sent": "And a classifier with a banishing empirical training error.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I published.",
                    "label": 0
                },
                {
                    "sent": "With respect to any distribution on the.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, this I mean, the nice thing is that you have this.",
                    "label": 0
                },
                {
                    "sent": "This of course depends on the distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you have a nice distribution then you can observe a faster rate.",
                    "label": 0
                },
                {
                    "sent": "So I want now to that.",
                    "label": 0
                },
                {
                    "sent": "So these bands are obtained via via sophisticated statistical and at least for me sophisticated statistical analysis.",
                    "label": 0
                },
                {
                    "sent": "Anne and.",
                    "label": 0
                },
                {
                    "sent": "I I want to show you now that with a very simple with just one application of a simple statistical inequality you cannot taste.",
                    "label": 0
                },
                {
                    "sent": "You can obtain similar bounds.",
                    "label": 0
                },
                {
                    "sent": "For arbitrary learning algorithms, so note that this this bound is doesn't doesn't involve any statement on the learning algorithm that you're using.",
                    "label": 0
                },
                {
                    "sent": "This is a strength, but is also a weakness.",
                    "label": 0
                },
                {
                    "sent": "Is I mean this?",
                    "label": 0
                },
                {
                    "sent": "Is this a property of the class?",
                    "label": 0
                },
                {
                    "sent": "Is not the property or the property of the algorithm they're using to pick elements from the class?",
                    "label": 0
                },
                {
                    "sent": "But it might be the case that if you look at.",
                    "label": 0
                },
                {
                    "sent": "Joint properties of the algorithm and the class from which the model class from each of the algorithm which the algorithm is working.",
                    "label": 0
                },
                {
                    "sent": "You might be able to get better bounds or at least different bounds, and this is the way I want to the path I want to take an.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at small subclasses of H. Hopefully small subclasses of H that are generated by the action of the algorithm on the training data.",
                    "label": 1
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "We will be considering specific algorithms and the way.",
                    "label": 0
                },
                {
                    "sent": "The behavior of the algorithm that we are considering on the training data will define a specific subclass of age, which will be the class on.",
                    "label": 0
                },
                {
                    "sent": "That will replace this this big age.",
                    "label": 0
                },
                {
                    "sent": "In this analysis somehow so.",
                    "label": 0
                },
                {
                    "sent": "Really we are aiming at algorithmic dependent risk analysis rather than class model class dependent or a model.",
                    "label": 0
                },
                {
                    "sent": "Whatever independent analysis which is the case of the document theory OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We do this to create this small subclass of age which is generated between interaction of the algorithm data.",
                    "label": 0
                },
                {
                    "sent": "We we proceed in this way.",
                    "label": 0
                },
                {
                    "sent": "OK so we generate an ensemble of hypothesis or functions which are obtained by feeding to the algorithm 8A which is a fixed learning algorithm of your choice.",
                    "label": 0
                },
                {
                    "sent": "Increasing prefixes of your training set so.",
                    "label": 0
                },
                {
                    "sent": "You look at the training set as a sequence of ordered sequence of examples and you feed it to the algorithm at the initial.",
                    "label": 0
                },
                {
                    "sent": "Initially, you don't feed anything, you just get the default hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Then you feed the first example you get, passes each one you feed the first examples.",
                    "label": 0
                },
                {
                    "sent": "If you get, it passes through H2 and so on so forth until you feed all of them.",
                    "label": 0
                },
                {
                    "sent": "And that by doing that you get what we what I call an ensemble of functions, which is generated by a on this ordered sequence.",
                    "label": 1
                },
                {
                    "sent": "Not that OK. Of course this ensemble will depend on the specific ordering.",
                    "label": 0
                },
                {
                    "sent": "Of the sequence, but this is.",
                    "label": 0
                },
                {
                    "sent": "What we want for our analysis OK?",
                    "label": 0
                },
                {
                    "sent": "And of course you can do this.",
                    "label": 0
                },
                {
                    "sent": "You can run this schema.",
                    "label": 0
                },
                {
                    "sent": "This schema looks looks like a sort of a strange kind of cross validation, right?",
                    "label": 0
                },
                {
                    "sent": "But it's sort of like this kind of progressive because.",
                    "label": 0
                },
                {
                    "sent": "You're giving longer and longer pieces of the training data to the algorithm, and of course you can do this with any learning algorithm, but usually it takes a lot of time.",
                    "label": 0
                },
                {
                    "sent": "There are specific algorithms for which this is very.",
                    "label": 0
                },
                {
                    "sent": "This is very natural to do this very natural thing to do, and these are the.",
                    "label": 0
                },
                {
                    "sent": "So called it online or implement incremental learning algorithms like the perceptron and all the variance of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK or Ridge regression for instance in for regression and many other algorithms.",
                    "label": 0
                },
                {
                    "sent": "These algorithms work really incrementally, so they basically they look at the data, the training data one by one and then adjust their H. A little bit a little bit each time a little bit.",
                    "label": 0
                },
                {
                    "sent": "If, for instance, in the classification case, if a mistake is made on the new element.",
                    "label": 0
                },
                {
                    "sent": "New example that has been added to the to the data.",
                    "label": 0
                },
                {
                    "sent": "Once this fed.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Again it.",
                    "label": 0
                },
                {
                    "sent": "This is a general scheme and that can be applied to any to any learning algorithm, but this is particularly natural and effective for incremental learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So the balance will all the bounce.",
                    "label": 0
                },
                {
                    "sent": "That will prove it will hold for any A.",
                    "label": 0
                },
                {
                    "sent": "But keep in mind that for efficiency reasons that it might be sensible to use this.",
                    "label": 0
                },
                {
                    "sent": "This kind of of.",
                    "label": 0
                },
                {
                    "sent": "I'll make the algorithm only for incremental learners.",
                    "label": 0
                },
                {
                    "sent": "OK, looking at properties of this ensemble of functions that are generated and this is sample and functions contains what you will expect to choose, which is the hypothesis each subnet has seen the whole set of data.",
                    "label": 0
                },
                {
                    "sent": "This is the guy that you would expect to.",
                    "label": 0
                },
                {
                    "sent": "Choose as your yes.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you have a batch algorithm like a support vector machine that's super linear in the number of samples the running time.",
                    "label": 0
                },
                {
                    "sent": "Yes, rice can do is instead of thinking about feeding an individual examples one at a time.",
                    "label": 0
                },
                {
                    "sent": "You could feed feed batches of examples right?",
                    "label": 0
                },
                {
                    "sent": "First 10 in the 1st right, right, right, right, right, right.",
                    "label": 0
                },
                {
                    "sent": "I say yeah, yeah, yeah yeah yeah yeah yeah OK OK thanks.",
                    "label": 0
                },
                {
                    "sent": "Thanks, sorry I mean yes, I just say that this is more natural.",
                    "label": 0
                },
                {
                    "sent": "But yeah, then there are ways to, I mean.",
                    "label": 0
                },
                {
                    "sent": "To make it efficient, also even for arguments like SVM that are typically match.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                },
                {
                    "sent": "As I said, the teacher in this ensemble, because this is the hypothesis that you have generated.",
                    "label": 0
                },
                {
                    "sent": "Once you've seen the whole training set, but I.",
                    "label": 0
                },
                {
                    "sent": "You know, building a theory you can use vectoring theory to bound the, for instance, for SVM you can use battering theory to bound the risk of this guy.",
                    "label": 0
                },
                {
                    "sent": "I want to use now a different theory which will prove statements about the average risk of the elements of disassemble and proving a statement about the average risk of the elements of disassemble will be much easier than proving a statement for the last guy.",
                    "label": 0
                },
                {
                    "sent": "OK, because I because of some intuitively there there's some natural averaging argument that you can apply, which actually will connect very well with the with the process that has generated disassemble of a prothesis question so far.",
                    "label": 0
                },
                {
                    "sent": "Everything is clear.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now the goals that I want to set up and I will cover in the rest of the talk, the following.",
                    "label": 0
                },
                {
                    "sent": "First of all, I want to bound the average risk of being sample in terms of statistics, which we pray, replace the replaces the training error of the back nature vectoring theory and these statistics has to do with the incremental performance of the algorithm, the data and that will define it in a moment.",
                    "label": 1
                },
                {
                    "sent": "It's a very simple quantity which is very easy to compute.",
                    "label": 0
                },
                {
                    "sent": "And then I want to go further and the show away you're constructing.",
                    "label": 0
                },
                {
                    "sent": "Wait for finding an element of the assemble, whose risk is close to the assemble average.",
                    "label": 1
                },
                {
                    "sent": "So while this is useful if your losses convex and your decision space is convex, then you can you can just take.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can just take the average out the prediction.",
                    "label": 0
                },
                {
                    "sent": "You can average the predictions of these guys in the assemble and you will get a valid prediction because you're.",
                    "label": 0
                },
                {
                    "sent": "Decisional space is convex and by instance inequality.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The risk of your average prediction will be smaller than the average risk of the prediction in this sample, because the loss is convex.",
                    "label": 0
                },
                {
                    "sent": "So for convex losses we are just just fine to bound the average risk of the example, but if you want to do classification then this doesn't work anymore, so you have to exhibit a specific guidance sample that achieves nearly achieves the average risk of the sample.",
                    "label": 1
                },
                {
                    "sent": "To make it effective and we will do that this way.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Also, you would like to relate.",
                    "label": 0
                },
                {
                    "sent": "These results with the with what is usually your your.",
                    "label": 0
                },
                {
                    "sent": "Your term reference, which is the risk of the best class, the best model in your class.",
                    "label": 0
                },
                {
                    "sent": "So the smallest risk of any element in X, so we will.",
                    "label": 0
                },
                {
                    "sent": "And for doing that will be.",
                    "label": 0
                },
                {
                    "sent": "We will have to take assumptions on this specific algorithm learning algorithm that will be using up to here.",
                    "label": 0
                },
                {
                    "sent": "It's completely it's.",
                    "label": 0
                },
                {
                    "sent": "It will hold for any algorithm A here will have to take.",
                    "label": 0
                },
                {
                    "sent": "Make us we will.",
                    "label": 0
                },
                {
                    "sent": "We will have to look at specific algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so bounding the average risk is really simple becausw the way with which constructed the our assemble together with the fact that the training set has been drawn IID.",
                    "label": 1
                },
                {
                    "sent": "Makes the process.",
                    "label": 0
                },
                {
                    "sent": "I mean it creates a simple stochastic process that corresponds to a martingale difference sequence.",
                    "label": 1
                },
                {
                    "sent": "So precisely, if you remember each hypothesis within index, let's say T -- 1 has been generated there only using the first T -- 1 elements of the training set as you can.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See in this picture.",
                    "label": 0
                },
                {
                    "sent": "So each two is generated using only the first 2 elements of the training set, OK?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if I look now I can compare the risk of this guy with this deterministic quantity.",
                    "label": 0
                },
                {
                    "sent": "Sorry, it's not me, it's the random quantity which is the loss of the same function on the next example on the next random example this has been generating using the first T -- 1 examples and measuring its loss on the next upcoming example.",
                    "label": 0
                },
                {
                    "sent": "Now this thing if you take the expectation of this difference conditioned on the first team.",
                    "label": 0
                },
                {
                    "sent": "I know T -- 1 examples drawn.",
                    "label": 0
                },
                {
                    "sent": "This is 0 because by definition.",
                    "label": 0
                },
                {
                    "sent": "If for personal, if I fix this sequence then this is deterministic element because it is defined by.",
                    "label": 0
                },
                {
                    "sent": "The first T -- 1 elements of the training set, so this is deterministic hypothesis, deterministic function and then by definition this is via is an iid pair and this by definition is the risk the spectation, sorry this spectation of this.",
                    "label": 0
                },
                {
                    "sent": "Is the risk of HT minus one?",
                    "label": 0
                },
                {
                    "sent": "This is 0.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "We can look at the associated Martingale if we sum up over T. OK, this becomes a mountain GAIL and we can split this difference into two sums.",
                    "label": 0
                },
                {
                    "sent": "And divide by N. And now we look at this two elements and we see that.",
                    "label": 0
                },
                {
                    "sent": "This is the average risk of the elements in this new sample.",
                    "label": 1
                },
                {
                    "sent": "There's a little thing here that you might notice.",
                    "label": 1
                },
                {
                    "sent": "It's a little caveat that an this is something this is a statistic.",
                    "label": 0
                },
                {
                    "sent": "It's really depends.",
                    "label": 0
                },
                {
                    "sent": "I can measure once I've given the sample, I can generate the ensemble and measure this quantity, and this is just this is what we call the mistake.",
                    "label": 0
                },
                {
                    "sent": "The let's say.",
                    "label": 0
                },
                {
                    "sent": "The online loss of an algorithm, so it's the sum of the losses generated by made by each function on the next example of the training set.",
                    "label": 0
                },
                {
                    "sent": "So I am again.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An example in this schema here.",
                    "label": 0
                },
                {
                    "sent": "So I'm generating a guy.",
                    "label": 0
                },
                {
                    "sent": "Which depends only on the 1st example.",
                    "label": 0
                },
                {
                    "sent": "And now I'm testing this guy on the next one and then measuring the loss.",
                    "label": 0
                },
                {
                    "sent": "Then I'm again feeling this back.",
                    "label": 0
                },
                {
                    "sent": "Getting the second guy and I'm testing this second guy on the third example and measuring the loss and I'm summing up this loss.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "This is something that if you are able to run efficiently in the algorithm on in this way, then you can measure these statistics this statistic efficiently.",
                    "label": 0
                },
                {
                    "sent": "There's no, there's no problem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the little trick is that I'm throwing away the last hypothesis here.",
                    "label": 0
                },
                {
                    "sent": "You see, because.",
                    "label": 0
                },
                {
                    "sent": "This this sum goes up to N -- 1.",
                    "label": 0
                },
                {
                    "sent": "And there is a.",
                    "label": 0
                },
                {
                    "sent": "There is a reason why I'm throwing away this guy.",
                    "label": 1
                },
                {
                    "sent": "Somehow I'll try.",
                    "label": 0
                },
                {
                    "sent": "I mean it's.",
                    "label": 0
                },
                {
                    "sent": "I'm not using.",
                    "label": 0
                },
                {
                    "sent": "I'm not using this guy here in the analysis, which is which is the guy who has seen the most of the data.",
                    "label": 0
                },
                {
                    "sent": "But just I'm just stopping here.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't make a lot of difference, right?",
                    "label": 0
                },
                {
                    "sent": "In denies it will be used.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Smell.",
                    "label": 0
                },
                {
                    "sent": "Once I haven't monticola different sequence and.",
                    "label": 0
                },
                {
                    "sent": "The elements, I mean that the increments of the martingale are bounded, because this loss is bounded.",
                    "label": 0
                },
                {
                    "sent": "Now I can really hope to show that with high probability this quantity is close to this, so I can relay to statistic that I can easily measure on my training set two and interesting.",
                    "label": 0
                },
                {
                    "sent": "Interesting.",
                    "label": 0
                },
                {
                    "sent": "Think which I can't observe, which is the average risk of the sample.",
                    "label": 1
                },
                {
                    "sent": "OK, so the tool so far is no snow statistics, it's just the definitions.",
                    "label": 0
                },
                {
                    "sent": "So the only piece of statistical theory I'll be using is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First time bound.",
                    "label": 0
                },
                {
                    "sent": "Which is a very basic basic but useful tool.",
                    "label": 0
                },
                {
                    "sent": "And this is very well known.",
                    "label": 0
                },
                {
                    "sent": "Basically education tool.",
                    "label": 0
                },
                {
                    "sent": "It says that they've Z1Z2 blah blah is a martingale difference sequence with increments bounded by one.",
                    "label": 1
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "Just cause we assume that we assume now that our loss is bounded between 01 to just two.",
                    "label": 0
                },
                {
                    "sent": "To simplify things.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "And the sum of conditional variances of our matching the different sequence which is called in statistics, the quadratic variation, the total quadratic variation of the multi Gale.",
                    "label": 0
                },
                {
                    "sent": "OK, this is visa ban.",
                    "label": 0
                },
                {
                    "sent": "Then it can be the case.",
                    "label": 0
                },
                {
                    "sent": "That so.",
                    "label": 0
                },
                {
                    "sent": "Let's say it will happen with very little probability that.",
                    "label": 0
                },
                {
                    "sent": "Will have simultaneously.",
                    "label": 0
                },
                {
                    "sent": "A large value of the martingale will be large.",
                    "label": 0
                },
                {
                    "sent": "And the value of the total quadratic variation will be small.",
                    "label": 0
                },
                {
                    "sent": "This can happen at the same time or will happen with exponentially small probability.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "This depends on both S&K.",
                    "label": 0
                },
                {
                    "sent": "So for any two bounds.",
                    "label": 0
                },
                {
                    "sent": "On the Martin Gale and the total projected variations, it can be the case that one is bigger, there is more.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "We talked for a little bit.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "It's very.",
                    "label": 0
                },
                {
                    "sent": "It's perfect for our purposes because we have a magical different sequence with increments bounded 'cause we look at lower losses bounded so we can.",
                    "label": 0
                },
                {
                    "sent": "Black this.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into our but this to our mountain girl and now it takes a little algebra to carry out the computations becausw.",
                    "label": 0
                },
                {
                    "sent": "OK, first of all.",
                    "label": 0
                },
                {
                    "sent": "You have a certain number of things that you want to take care of.",
                    "label": 0
                },
                {
                    "sent": "You do know a value of the variance, so you have to stratify over the possible values of the variance.",
                    "label": 0
                },
                {
                    "sent": "Which will bring in a logarithmic additional factor, which is this an then at the end that you can't keep the variance inside that you have to use the fact that the variance is the loss is bounded, variance is trivially the variance of the losses tribute bounded by the expected value of the risk.",
                    "label": 0
                },
                {
                    "sent": "Trivially, here this is a crude bound, but you have to.",
                    "label": 0
                },
                {
                    "sent": "You have to use this.",
                    "label": 0
                },
                {
                    "sent": "So if you do this approximation, you first stratify your analysis over possible various of the variance, and then you plug this crude upper bound and you do it a little bit of calculations.",
                    "label": 0
                },
                {
                    "sent": "You end up with a bound.",
                    "label": 0
                },
                {
                    "sent": "It looks like this.",
                    "label": 0
                },
                {
                    "sent": "So this hold again with high probability.",
                    "label": 0
                },
                {
                    "sent": "There's no I.",
                    "label": 0
                },
                {
                    "sent": "This is not spelled out in the bound, but just to to make it simpler.",
                    "label": 0
                },
                {
                    "sent": "You have the average risk of this sample is at most base business.",
                    "label": 0
                },
                {
                    "sent": "This MCV is is what I is what I called.",
                    "label": 0
                },
                {
                    "sent": "The cumulative loss.",
                    "label": 0
                },
                {
                    "sent": "Of our incremental learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it's again the sum of the losses that each each function generated by the algorithm is made on.",
                    "label": 0
                },
                {
                    "sent": "The next example in the training sequence averaged.",
                    "label": 0
                },
                {
                    "sent": "OK. And so this is the hour.",
                    "label": 0
                },
                {
                    "sent": "What this is what replaces the training error or the empirical risk in the dimension analysis?",
                    "label": 0
                },
                {
                    "sent": "OK. And this quantity, I mean the.",
                    "label": 0
                },
                {
                    "sent": "The sum of the community loss because the loss is bounded varies between a constant and something that is linear in the training set size.",
                    "label": 0
                },
                {
                    "sent": "So this is if you look at this bound you can say that OK, you can express this bound by saying that these two these two guys are close to each other.",
                    "label": 0
                },
                {
                    "sent": "So this is a good proxy.",
                    "label": 0
                },
                {
                    "sent": "This statistics is a good proxy for the average risk of the sample, and the slack is something again that has the same behavior.",
                    "label": 0
                },
                {
                    "sent": "Like I mean the same flavor the same.",
                    "label": 0
                },
                {
                    "sent": "Qualitative appearance as the one which was gotten using VC theory because you see again you have a 1 / N term where instead of the VC dimension where you have the log of the number of let's say number of mistakes in a classification problem just to make it concrete, that is in a classification problem.",
                    "label": 0
                },
                {
                    "sent": "This is just the number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "Computing this way.",
                    "label": 0
                },
                {
                    "sent": "And here you have a term which expresses the same tradeoff that we observed in the VC theory.",
                    "label": 0
                },
                {
                    "sent": "Because if you bring in the 1 / N into the square you have here you have.",
                    "label": 0
                },
                {
                    "sent": "We have this term and some bent over N and then you have something which is.",
                    "label": 0
                },
                {
                    "sent": "Slightly more than slightly super slightly above the constant, it's going slowly.",
                    "label": 0
                },
                {
                    "sent": "Divided by square root of N. So if if this divided by N, so this term is Spanish ING, then again you have a rate of 1 / N otherwise you have intermediate rates between 1 / N and 1 / sqrt N. Again, it's the same kind of tradeoff we observed before.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the first 1st result, yes?",
                    "label": 0
                },
                {
                    "sent": "Tumse turn off style balance work for.",
                    "label": 0
                },
                {
                    "sent": "Martin gives it in the last call.",
                    "label": 0
                },
                {
                    "sent": "He had some paper.",
                    "label": 0
                },
                {
                    "sent": "Yeah, he had he had, I mean the same topics of his results is the same of hours.",
                    "label": 0
                },
                {
                    "sent": "He was claiming that you can't use them.",
                    "label": 0
                },
                {
                    "sent": "Bernstein for doing this kind of analysis.",
                    "label": 0
                },
                {
                    "sent": "And here we proved everything from scratch, which was quite good.",
                    "label": 0
                },
                {
                    "sent": "But I think that when you work through the implications, C can become one and the log of M. Sabin can also become one.",
                    "label": 0
                },
                {
                    "sent": "No, I mean in we did it.",
                    "label": 0
                },
                {
                    "sent": "We compared the two.",
                    "label": 0
                },
                {
                    "sent": "I mean, these synthetics were exactly the same.",
                    "label": 0
                },
                {
                    "sent": "I don't think you can, Constance.",
                    "label": 0
                },
                {
                    "sent": "Oh, the constants.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know about it.",
                    "label": 0
                },
                {
                    "sent": "I don't know about it.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah I don't know about it, but I mean the main one of the points we would like to make is that you don't need a lot of.",
                    "label": 0
                },
                {
                    "sent": "I mean you can use standard off the shelf statistical tools.",
                    "label": 0
                },
                {
                    "sent": "Very simple and get these bounds.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can, you can work on the constants by reproving everything from scratch.",
                    "label": 0
                },
                {
                    "sent": "Yes, I can come from the bound on the right because we ask.",
                    "label": 0
                },
                {
                    "sent": "We include some it's come from running the loss, but it's also it is also dependent on the fact that you have.",
                    "label": 0
                },
                {
                    "sent": "Yeah you are.",
                    "label": 0
                },
                {
                    "sent": "Right and and you have algebra.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not like you have a series of inequality just to stratify over the values of the variance.",
                    "label": 0
                },
                {
                    "sent": "So this brings in a little bit of you know additional constants.",
                    "label": 0
                },
                {
                    "sent": "But in our case I mean we have this is this is like.",
                    "label": 0
                },
                {
                    "sent": "In this way, for this term is like 30 an for this term is like 4 or something like that.",
                    "label": 0
                },
                {
                    "sent": "We can do better, but I'm not after just kind of.",
                    "label": 0
                },
                {
                    "sent": "Just do better straight out of tongues.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, in terms of constants, that's very possible.",
                    "label": 0
                },
                {
                    "sent": "That's very possible.",
                    "label": 0
                },
                {
                    "sent": "I think you can also avoid stratifying on the.",
                    "label": 0
                },
                {
                    "sent": "I don't think so.",
                    "label": 0
                },
                {
                    "sent": "Now we look at that.",
                    "label": 0
                },
                {
                    "sent": "We look at that.",
                    "label": 0
                },
                {
                    "sent": "Are you on the various?",
                    "label": 0
                },
                {
                    "sent": "Doing these VC to step down.",
                    "label": 0
                },
                {
                    "sent": "It's not necessary.",
                    "label": 0
                },
                {
                    "sent": "I think it's right here.",
                    "label": 0
                },
                {
                    "sent": "Is not necessary in the same sense?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure about it.",
                    "label": 0
                },
                {
                    "sent": "I mean, I mean looking at his paper, we didn't find a way of avoiding that, and he at the end I mean at the end he gets insane.",
                    "label": 0
                },
                {
                    "sent": "He gets the same as us, possibly with better constants, yes?",
                    "label": 0
                },
                {
                    "sent": "I mean, these tests from a tighter bound which involves the KL divergent.",
                    "label": 0
                },
                {
                    "sent": "But then if you want to get a readable bound then you end up with this.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Game now.",
                    "label": 0
                },
                {
                    "sent": "Second part, so this Mount you can apply it.",
                    "label": 0
                },
                {
                    "sent": "Directly.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two regression with squared loss, for instance cause.",
                    "label": 0
                },
                {
                    "sent": "This would be the square loss and then you can.",
                    "label": 0
                },
                {
                    "sent": "You can just say OK bye yes inequality.",
                    "label": 0
                },
                {
                    "sent": "If I take the average hypothesis of my sample.",
                    "label": 0
                },
                {
                    "sent": "It's basically high probability bounded by this statistic.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "That I can do.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, if you do classification instead, then you need to work out a little to work a little bit more and to really show that some hypothesis is.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know that there exists at least some one classifier in your ansambl whose risk is more than the average, of course, so you want to.",
                    "label": 0
                },
                {
                    "sent": "Like this guy or some guy close to it and the way to do that is to use a sequence simple sequential testing procedure as you go along, generating your ensemble of hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So there is that each time you generate, generate the new hypothesis you test it on the remaining part of the training set.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you pick at the end of the sweep over the training set you pick the guy that minimizes.",
                    "label": 0
                },
                {
                    "sent": "Which would be a guy from the sample which minimizes penalized the risk estimate, which takes into account the fact that which has to turn this penalize the risk estimate as two terms.",
                    "label": 0
                },
                {
                    "sent": "One is the.",
                    "label": 0
                },
                {
                    "sent": "Average loss.",
                    "label": 0
                },
                {
                    "sent": "In curd by H. Subte on the remaining part of the sample and the other is a penalization term that takes into account the fact that we are comparing empirical risks over different portions of the training set, because each guy will be tested on smaller and smaller portion of the training set.",
                    "label": 0
                },
                {
                    "sent": "So this is by the way.",
                    "label": 0
                },
                {
                    "sent": "If you are running a kernel based at the.",
                    "label": 0
                },
                {
                    "sent": "Algorithm like kernel perceptron or Kernel Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "This doesn't cost you anything in terms of computational overhead.",
                    "label": 0
                },
                {
                    "sent": "This testing thing becausw.",
                    "label": 0
                },
                {
                    "sent": "And because the kernel based algorithms like Perceptrons are built in such a way that to evaluate.",
                    "label": 0
                },
                {
                    "sent": "Some hypothesis you need to evaluate all day prothesis were previously constructed, the incrementally up to that point, so it's somehow it's built in.",
                    "label": 0
                },
                {
                    "sent": "This testing is built in for free in the evaluation mechanism for kind of based perceptions, or.",
                    "label": 0
                },
                {
                    "sent": "Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "OK, so you lose a little bit.",
                    "label": 0
                },
                {
                    "sent": "Indeed.",
                    "label": 0
                },
                {
                    "sent": "You retain the main properties of the old inequality's, but you lose a little bit becausw here you have now a log squared N where N is the sample side.",
                    "label": 0
                },
                {
                    "sent": "Here you have a line instead of having L of msub.",
                    "label": 0
                },
                {
                    "sent": "And that's OK. That's what you lose.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "We can, so far this holds.",
                    "label": 0
                },
                {
                    "sent": "This holds for any algorithm.",
                    "label": 0
                },
                {
                    "sent": "This does only assumption is bounded loss.",
                    "label": 0
                },
                {
                    "sent": "Nor their assumptions.",
                    "label": 0
                },
                {
                    "sent": "Game.",
                    "label": 0
                },
                {
                    "sent": "This in particular this bound holds for non convex loss functions like in the classification case.",
                    "label": 0
                },
                {
                    "sent": "Now suppose now you want to make.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Concrete examples for specific learners A.",
                    "label": 0
                },
                {
                    "sent": "And then in particular, you want to relate your online statistic.",
                    "label": 0
                },
                {
                    "sent": "To the best risk of any any.",
                    "label": 1
                },
                {
                    "sent": "Any element of your in your mother class.",
                    "label": 0
                },
                {
                    "sent": "So we can, for instance, use Bosc, an aggregating forecaster for regression with squared loss, which is a nice variation on Ridge regression.",
                    "label": 1
                },
                {
                    "sent": "Which achieves.",
                    "label": 0
                },
                {
                    "sent": "On any individual sequence of any individual data sequence bound.",
                    "label": 0
                },
                {
                    "sent": "On the cumulative squared loss which is on the online statistics, let's say which is better than the bound of the Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "So we can define each star Subban.",
                    "label": 0
                },
                {
                    "sent": "And this is the.",
                    "label": 0
                },
                {
                    "sent": "Optimal a penalized element of your over in our model class.",
                    "label": 0
                },
                {
                    "sent": "OK. Where the penalization?",
                    "label": 0
                },
                {
                    "sent": "Vanish is as the sample size increases.",
                    "label": 0
                },
                {
                    "sent": "OK. And that we can show now.",
                    "label": 0
                },
                {
                    "sent": "That we hype by combining the Vox analysis for the aggregating forecaster with simple.",
                    "label": 0
                },
                {
                    "sent": "Large deviation analysis, same same flavor as before.",
                    "label": 0
                },
                {
                    "sent": "We can prove that this online statistic will be related.",
                    "label": 0
                },
                {
                    "sent": "To the up the penalized risk of the optimal penalized risk in the class for any reproducing kernel.",
                    "label": 0
                },
                {
                    "sent": "Sorry, we're working here in any reproducing kernel Hilbert space of functions, so we're.",
                    "label": 0
                },
                {
                    "sent": "We're using so H will be elements of this reproducing kernel Hilbert space of functions.",
                    "label": 0
                },
                {
                    "sent": "And we can prove that basically again.",
                    "label": 1
                },
                {
                    "sent": "This online statistics is related to the optimal risk in the class.",
                    "label": 0
                },
                {
                    "sent": "Cluster one term which depends square root of the risk divided by N. And this other term, which depends on why which is the.",
                    "label": 0
                },
                {
                    "sent": "Size the absolute.",
                    "label": 0
                },
                {
                    "sent": "Why is the absolute largest absolute value of of an outcome in our data sequence?",
                    "label": 0
                },
                {
                    "sent": "So let's say is.",
                    "label": 0
                },
                {
                    "sent": "Is the right so this is a regression problem, so we are giving our labels are real values, so this is the largest norm.",
                    "label": 0
                },
                {
                    "sent": "The largest models of the values that you can we may observe.",
                    "label": 0
                },
                {
                    "sent": "Ann, this is the sum of the logs of 1 plus the eigenvalues of the Canon of the kernel matrix.",
                    "label": 1
                },
                {
                    "sent": "How many eigenvalues away?",
                    "label": 0
                },
                {
                    "sent": "If you want to current space you want to have a more eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Definitely more than the number of examples that we've seen so far.",
                    "label": 0
                },
                {
                    "sent": "This can you can this summer will run, definitely not for at most over N elements.",
                    "label": 0
                },
                {
                    "sent": "'cause these are the number of examples that you've added in your kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Is this song?",
                    "label": 0
                },
                {
                    "sent": "Yes, if you are working in a finite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "This is bounded by.",
                    "label": 0
                },
                {
                    "sent": "The same, so this was the.",
                    "label": 0
                },
                {
                    "sent": "The sum of the logs is bounded.",
                    "label": 0
                },
                {
                    "sent": "So Sam.",
                    "label": 0
                },
                {
                    "sent": "I learned one plus Lambda.",
                    "label": 0
                },
                {
                    "sent": "I is bounded by D. Ln N + 1, where D is the.",
                    "label": 0
                },
                {
                    "sent": "D is the dimension.",
                    "label": 0
                },
                {
                    "sent": "Of the space in which the axis leave.",
                    "label": 0
                },
                {
                    "sent": "So if in a finite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "You can you can you can use this bound.",
                    "label": 0
                },
                {
                    "sent": "OK, four.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "Not in a no.",
                    "label": 0
                },
                {
                    "sent": "I mean in this case they're they're hoping you're betting on the fact that your kernel will have the fastly decreasing eigenvalues like caching kernels.",
                    "label": 0
                },
                {
                    "sent": "So this effectively this summer will only take into account few terms.",
                    "label": 0
                },
                {
                    "sent": "Few terms of this time will be.",
                    "label": 0
                },
                {
                    "sent": "AC will be heavy.",
                    "label": 0
                },
                {
                    "sent": "By trace.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, but that's that's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's kind of accrued.",
                    "label": 0
                },
                {
                    "sent": "So perception perception styles are going.",
                    "label": 0
                },
                {
                    "sent": "Algorithms will give you that race, but then so here the bound is logarithmic in their eyes and and perception is not.",
                    "label": 0
                },
                {
                    "sent": "We do half, let's say for regression with your half is not.",
                    "label": 0
                },
                {
                    "sent": "More examples in classifications in classification OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Found that.",
                    "label": 0
                },
                {
                    "sent": "This is about the relates directly.",
                    "label": 0
                },
                {
                    "sent": "The online statistic with the.",
                    "label": 0
                },
                {
                    "sent": "A risk of the best guy in your class.",
                    "label": 0
                },
                {
                    "sent": "Now in the classification, you can't quite do that because.",
                    "label": 0
                },
                {
                    "sent": "You don't.",
                    "label": 0
                },
                {
                    "sent": "And it uses this bound users.",
                    "label": 0
                },
                {
                    "sent": "This bound comes from.",
                    "label": 0
                },
                {
                    "sent": "An analysis for individual sequences.",
                    "label": 0
                },
                {
                    "sent": "Of of this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So you start for from a pointwise bound that says OK, this quantity is not larger for any individual sequence that a certain quantity which is related to this and then you take expectations and a little bit of logic creations here and you get the band like this.",
                    "label": 0
                },
                {
                    "sent": "For a classification you can't quite do that so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can take the second step.",
                    "label": 0
                },
                {
                    "sent": "You can take it to expectations, so you just look at the pointwise bound.",
                    "label": 0
                },
                {
                    "sent": "But still you might be interested in getting getting idea of how this quantity is related to properties of the data sequence of the year training sequence.",
                    "label": 0
                },
                {
                    "sent": "OK, so this online analysis of online algorithms tell you that for.",
                    "label": 0
                },
                {
                    "sent": "Any sequence of of data with binary labels.",
                    "label": 0
                },
                {
                    "sent": "So we are embedding classification example set up.",
                    "label": 0
                },
                {
                    "sent": "Let's let's limit ourselves to for simplicity, to the linearly separable case.",
                    "label": 1
                },
                {
                    "sent": "Still we are working in reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 1
                },
                {
                    "sent": "So you know that for kernel perceptron, OK, so suppose that we're nearly separable space, and then there is some linear separator F which has a margin gamma.",
                    "label": 0
                },
                {
                    "sent": "K and Lambda one Lambda, 2 on the eigenvalues of the kernel metrics.",
                    "label": 1
                },
                {
                    "sent": "But this time this eigen values are only compute the kernel matrix only includes the instances that correspond to mistaken examples.",
                    "label": 0
                },
                {
                    "sent": "Would you mean mistaken examples?",
                    "label": 0
                },
                {
                    "sent": "That is taken examples are.",
                    "label": 0
                },
                {
                    "sent": "Are those examples in your training sequence on which the current hypothesis made a mistake?",
                    "label": 0
                },
                {
                    "sent": "So in which?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On which this quantity.",
                    "label": 0
                },
                {
                    "sent": "Was 1 four o'clock for classification.",
                    "label": 0
                },
                {
                    "sent": "OK, so for classification this this is the 01 loss can be 01 at each time HT minus one makes a mistake on the next example you include the XT in the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "You build your kind of metrics all including these guys and you're bound will depend.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the eigenvalues of this kind of metrics.",
                    "label": 1
                },
                {
                    "sent": "So this it typically is much smaller, hopefully is much smaller than the whole kernel metrics that you build from the whole train set.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case for the perception, the bound depends on the trace of the econometrics.",
                    "label": 0
                },
                {
                    "sent": "And so there's some sort again values, and you can have some more sophisticated versions like the 2nd order perception in which you have a subtler dependence between the whole spectrum.",
                    "label": 0
                },
                {
                    "sent": "So I mean you have the sum of the locks.",
                    "label": 0
                },
                {
                    "sent": "Of the eigenvalues and you also have the terms that take into account the interaction of the linear separator on the specific elements of the data sequence.",
                    "label": 1
                },
                {
                    "sent": "Examples of this?",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not going into details on this, just to show you that for many algorithms you can get pointwise bounds on this quantity on your online statistic that can be plugged in to get final bound on.",
                    "label": 0
                },
                {
                    "sent": "On the on your on your risk, so again, you can use this once.",
                    "label": 0
                },
                {
                    "sent": "And you can plug them in.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "To get right more general bound that depends on on several quantity.",
                    "label": 0
                },
                {
                    "sent": "Several properties of the data signals OK.",
                    "label": 0
                },
                {
                    "sent": "This is basically.",
                    "label": 0
                },
                {
                    "sent": "What I wanted to say.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is a different view on the study of risk in statistical learning theory, which is based on.",
                    "label": 0
                },
                {
                    "sent": "Not on the study of the properties of your money class, but on the study of the properties of the interaction of your arguing with the model and data.",
                    "label": 0
                },
                {
                    "sent": "So which I think is a simple, nice, easy to understand.",
                    "label": 0
                },
                {
                    "sent": "And you get data dependent bounds for any learning that are not in terms of training error, but in terms of a different statistic, which I called online statistic.",
                    "label": 1
                },
                {
                    "sent": "And you can even get bounced for point is related to this, with some penalization, for instance for specific learners, and the main one nice thing of this approach is that you don't really need any fancy statistical tool, you don't need the red marker.",
                    "label": 0
                },
                {
                    "sent": "Rademacher, Ann An average is.",
                    "label": 0
                },
                {
                    "sent": "It's it's, it's really.",
                    "label": 0
                },
                {
                    "sent": "It's really easy, and the reason why it's easy because you start from the game theoretic analysis of the online statistic.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Provide you, I mean.",
                    "label": 0
                },
                {
                    "sent": "No, sorry, it's really easy because the you exploit specific way of running your learning algorithm on the data that generates a very simple stochastic process, and Martindale that it's very easy to analyze this way.",
                    "label": 0
                },
                {
                    "sent": "Maybe the way to explain it, and if you want to get detailed bounds then you can use the online theory theory of online algorithms to get ahold.",
                    "label": 0
                },
                {
                    "sent": "Like in this case.",
                    "label": 0
                },
                {
                    "sent": "Sorry to get ahold groups.",
                    "label": 0
                },
                {
                    "sent": "OK, to get ahold of.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, to get an idea of how these quantities behave.",
                    "label": 0
                },
                {
                    "sent": "So there are theories that explains for different tags will give you an idea of these quantities.",
                    "label": 0
                },
                {
                    "sent": "OK, this is about it.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to add that this really works in practice.",
                    "label": 0
                },
                {
                    "sent": "Money code and I just had a bunch of these progressive validation bounds on a bunch of empirical data sets.",
                    "label": 0
                },
                {
                    "sent": "Works quite well.",
                    "label": 0
                },
                {
                    "sent": "Thanks because I have experience here that can.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is really like simple experiments on.",
                    "label": 0
                },
                {
                    "sent": "On a textual data.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So maybe just want to say very very just one minute remark on this.",
                    "label": 0
                },
                {
                    "sent": "We did experience.",
                    "label": 0
                },
                {
                    "sent": "We tried these bounds on the real data and so this is the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The risk estimated on the test set on a separated test set.",
                    "label": 0
                },
                {
                    "sent": "This is the estimate using the online statistic using the theory an you see that qualitatively the behavior will fit a little bit with constants, so it's again it don't take it as gold, but you see that the behavior is nice, and I mean the absolute difference is not big, and then we also.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Broke up this into.",
                    "label": 0
                },
                {
                    "sent": "These are averages over over 50 different binary classification problems and then you can see the breakup of the different averages.",
                    "label": 0
                },
                {
                    "sent": "So this is the estimate and this is the actual risk.",
                    "label": 0
                },
                {
                    "sent": "And if you move if you feed more and more.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exam.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause you see that that.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's converged nicely apart.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the few glitches, but more or less, it's really nice.",
                    "label": 0
                },
                {
                    "sent": "I mean, even in here in the tail where you have some very small risk, it follows pretty nicely.",
                    "label": 0
                },
                {
                    "sent": "OK, so give you some some idea of how things go in practice.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Shameless plug, shameless plug.",
                    "label": 0
                },
                {
                    "sent": "Second theorem I presented yesterday was actually very similar in the reasoning stably also taken after you have practice sequential prediction algorithm.",
                    "label": 0
                },
                {
                    "sent": "And in the end, what you look at is what is the average of its performance overtime and that is.",
                    "label": 0
                },
                {
                    "sent": "Something which is apparently in general easy to analyze so many papers recently where Varonis, also several papers, very states that, well, exactly this quantity, which your context turns out to be.",
                    "label": 0
                },
                {
                    "sent": "Interest.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's start with further questions then.",
                    "label": 0
                },
                {
                    "sent": "535",
                    "label": 0
                }
            ]
        }
    }
}