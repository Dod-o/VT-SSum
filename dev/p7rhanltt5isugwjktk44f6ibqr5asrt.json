{
    "id": "p7rhanltt5isugwjktk44f6ibqr5asrt",
    "title": "Pixel GAN autoencoder",
    "info": {
        "author": [
            "Alireza Makhzani, Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_makhzani_pixel_gan/",
    "segmentation": [
        [
            "And so hi everyone, my name is Alireza, and I'm a PhD student from the University of Toronto.",
            "Today I'm going to talk about pixel gun autoencoders.",
            "This is a joint work with my advisor, Brendan Frey."
        ],
        [
            "So first I'm going to talk a little bit about pixel sinzan variational autoencoders, that, I guess most of you guys are familiar with.",
            "Then I'm going to talk about the adversarial autoencoder, which is a generative autoencoder, in which the recognition path users a generative adversarial network to impose a prior distribution on the latent code.",
            "Then I'm going to introduce the pixel gun autoencoder, which is a generative autoencoder, in which the genitive files uses pixels in conditional pixels in and that conditions on a latent vector and the recognition path, just like the adversarial autoencoder, uses a generative adversarial network to impose a prior distribution on the latent code show that imposing different distributions on the latent code will result in different decomposition of information between the latent code and autoregressive decoder, for example by imposing a Gaussian distribution we can achieve.",
            "What versus local decompositional information or by imposing a categorical distribution on the latent code, we can achieve a discrete versus continuous decomposition of information between latent code and autoregressive decoder.",
            "Finally, I'll show how the pixel gun autoencoder with categorical prior can be used in clustering and semi supervised learning applications."
        ],
        [
            "So the Pixel CNN is an auto regressive neural networks that captures the image statistics directly at the pixel level.",
            "They are very good at modeling though level pixel statistics and local structures.",
            "But they generated samples often like the global structure.",
            "So one of the drawback is that they don't learn a hierarchical latent representation, so it's not very straightforward to use them in downstream tasks such as English verbs learning.",
            "So here we can see the architecture of a conditional pixel CNN that can learn conditional densities.",
            "Using adaptive bias in the convolutional layers."
        ],
        [
            "So the variational autoencoder is a generative autoencoder which characterizes the posterior distribution using the encoder network and pair it with the top down generative network.",
            "Both networks are trained jointly to maximize the variational lower bound on the Datalog.",
            "Cute so variational autoencoders do learn hierarchal representation that is useful for downstream tasks such as semi supervised learning.",
            "So as you can see, these latent variable models such as variational autoencoders kind of complements the auto regressive architecture in the sense that one is good at capturing the global statistics and the other is good at capturing the local statistiques.",
            "So it kind of makes sense to combine them.",
            "There has been some attempts at combining them, such as the Pixel, the work from the Middle Group here in Montreal, and the variational lossy autoencoder from Open AI.",
            "So this pixel gun auto encoder that I'm going to introduce can be considered as another attempt at combining these architectures."
        ],
        [
            "So let's talk a little bit about the adversarial autoencoder.",
            "Before that, let me define the aggregated posterior distribution directed poster distribution.",
            "QFC is essentially the average posterior distribution over the data distribution.",
            "So in adversarial autoencoders, in addition to the standard reconstruction costs of the auto encoder, we have an adversarial cost as a regularization term that tries to match the aggregated closer distribution to an arbitrary prior distribution.",
            "So it's kind of like replacing the KL term.",
            "In the variation autoencoder objective with an adversarial cost.",
            "So the quality of the quality of samples of the adversarial autoencoder is very much like the variational autoencoder, but its main advantage is that you can impose any arbitrary or on the latent code just from the samples of the prior.",
            "For example here you can see the enemies codespace off the pixel gun autoencoder where we have imposed a Gaussian or a mixture of Gaussian distribution on the latent code."
        ],
        [
            "So let's talk about the pixel gun autoencoder."
        ],
        [
            "So the latent viral models that I just talked about, such as variational, autoencoder, or adverse aerial autoencoder, do have some limitations in order to sample from the first sample from the priority of Z and then we use the conditional P of X currency to sample from the model distribution.",
            "But in these models the conditional piece of excrement Z is typically a factor is Gaussian distribution, which has very limited stochastic city.",
            "So in these architectures, essentially we have a single latent vector that is capturing all the underlying factors of variation, such as the label information, destroying information, the global structure, or the local structure."
        ],
        [
            "The Pixel Gun auto encoder addresses this issue by using a powerful stochastically coder such as pixel CNN.",
            "So in pixel gun autoencoders, the statistics of the data distribution is jointly captured by the latent code and autoregressive decoder."
        ],
        [
            "The inference path uses a generative adversarial network to impose a prior distribution on the latent code."
        ],
        [
            "So I'll show that imposing different distribution on the latent code enables us to specify the type of their statistics that we care about and capture it by the Latin representation, while the remaining structure of the image is captured by the auto regressive decoder.",
            "For example by imposing a Gaussian distribution we can achieve a global versus local decomposition of information and disentangled the low frequency and high frequency content of the images, or by imposing a categorical distribution supplier we can achieve a discrete versus continuous decomposition and disentangled the label and the style of images.",
            "In a purely unsupervised fashion, so this discrete versus continue."
        ],
        [
            "The composition is particularly interesting because it can be directly used for semi supervised learning tasks."
        ],
        [
            "So here's the architecture of the adversarial autoencoder.",
            "As I explained, the genetic path is a pixel center.",
            "Conditions on the latent vector and the recognition path users.",
            "A generative adversarial network to impose a prior distribution on the latent code.",
            "So the cost function of the pixel gun autoencoder is there construction cost plus adverse aerial cost."
        ],
        [
            "So let's see what happens if we impose a Gaussian distribution on the latent code."
        ],
        [
            "So the left figure shows the samples of the pixel gun.",
            "Autoencoder trained on this data set for the purpose of matter illustrating the decomposition of information.",
            "We have chosen a limited code size of two 2D Gaussian code and the limited receptive field size.",
            "The middle figure shows the samples of the pixel CNN that uses the same limited receptive field size and the right figure shows the samples of an adversarial autoencoder that uses limited code size of two.",
            "So you can see pixel CNN.",
            "Is capturing the local statistics very well, but fails to generate the global structure because of the limited receptive field size.",
            "And adversarial autoencoder, on the other hand, is capturing the global statistics, but fails to generate the details of the image due to the limited code size.",
            "But the Pixel Gan autoencoder with the same limited code size, limited receptive field, can kind of combine the best of both and generate short images with global statistics."
        ],
        [
            "So here you can see the enemies codespace of ethics again, autoencoder, where we have imposed a Gaussian distribution on the latent code.",
            "So in pixel gun autoencoders the latent code is encouraged to capture the global structure or the what information as opposed to the very information.",
            "For example, in this figure you can see the network is using one of the access of the two D Gaussian code to explicitly encode the label information even though we have imposed a continuous prior such as the Gaussian program.",
            "In this case we can potentially get a much better separation in the codespace if we impose a discrete prior.",
            "Also, you can see in this figure that the latent code is capturing all the label information.",
            "What is also capturing some of their study information while the remaining style information is captured by the auto regressive decoder?",
            "So the question is whether it is possible to achieve a complete decomposition of style and content in the sense that the label the latent variable only captures the label information and all the style information is separately captured by the auto regressive decoder.",
            "So it turns out it is."
        ],
        [
            "Possible if we impose a categorical prior on the latent code."
        ],
        [
            "So here's the architecture of the pixel gun autoencoder with categorical prior.",
            "The categorical prior is imposed directly on the continuous output probabilities of the softmax softmax layer of the encoder.",
            "In this case, the discrete latent code of the network learns discrete factors of variations, such as the label information and.",
            "And the auto regressive decoder learns the study information of the digit condition on the label information of the digits."
        ],
        [
            "So in order to better understand the effect of the adverse aerial training, we train a pixel gun.",
            "Autoencoder only on the 1st 3 digits of MNIST and we choose the code size to be tree.",
            "So here in this figure we are projecting that really softmax simplex of the encoder onto a 2D triangle and we are visualizing the codespace."
        ],
        [
            "Left figure shows the emnace codespace off the pixel gun autoencoder where no distribution is imposed on the latent code.",
            "So you can see that the network has learned to use the surface of the softmax simplex to encode summer study information and therefore the three corners of the softmax simplex do not really have any meaningful interpretation."
        ],
        [
            "The right figure shows the code is in discord.",
            "This place off the pixel gun autoencoder where a categorical priorities impose on the latent code so you can see that in this case the network is using the tree corners of the softmax simplex to encode the label information and all their study in formation is separately captured by the auto regressive decoder."
        ],
        [
            "In this case, we can get an almost perfect error rate of .3% even though the network is trained in a purely unsupervised fashion."
        ],
        [
            "So once the pixel gun autoencoder is trained, we can use this encoder to cluster new points and we can use its decoder to generate conditional samples from each of the learning clusters."
        ],
        [
            "Here you can see the conditional samples of the pixel gun autoencoder trained on the full amnesty data set.",
            "You can see from.",
            "So each row of this figure shows basically the conditional samples from one of the learned clusters, so you can see from this figure that the discrete latent code off the network has learned discrete factors of variation, such as the label information and some discrete design information.",
            "For example, the network has assigned two different clusters to digital tools based on whether they are written with a loop at the bottom or it has assigned like different clusters to digit 7 based on whether they have a dash in the middle."
        ],
        [
            "So in this case we can achieve 5% error rate on the full in this data set in an unsupervised fashion just by matching each cluster to a digit type."
        ],
        [
            "So it turns out it's very straightforward to use these pixel gun architectures in the semi supervised learning settings."
        ],
        [
            "In order to do so, after optimizing the reconstruction costs and the and the adversarial costs on and only, we simply train the weights of the encoder networks on labeled mini batch to minimize the cross entropy at the end of the last layer of the encoder."
        ],
        [
            "So this figure shows the conditional samples of a semi supervised pixel gun autoencoder trained on the MSDN and or data set.",
            "So each column shows the conditional samples given each class label.",
            "So you can see that the pixel gun autoencoder can achieve a rather clean separation of content under slide with very few labeled data."
        ],
        [
            "So this table reports the semi supervised classification results of the pixel gun.",
            "Autoencoder trained on the Omni switch analog data set on.",
            "Then this data set we have been using 100 total labels, 50 labels, 20 labels and labels on a switch and use thousand labels on 500 labels on North datasets used $1000 and you can see that our classification results are highly competitive with their state of the art."
        ],
        [
            "Thank you and now you have to take your questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so hi everyone, my name is Alireza, and I'm a PhD student from the University of Toronto.",
                    "label": 0
                },
                {
                    "sent": "Today I'm going to talk about pixel gun autoencoders.",
                    "label": 1
                },
                {
                    "sent": "This is a joint work with my advisor, Brendan Frey.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first I'm going to talk a little bit about pixel sinzan variational autoencoders, that, I guess most of you guys are familiar with.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk about the adversarial autoencoder, which is a generative autoencoder, in which the recognition path users a generative adversarial network to impose a prior distribution on the latent code.",
                    "label": 1
                },
                {
                    "sent": "Then I'm going to introduce the pixel gun autoencoder, which is a generative autoencoder, in which the genitive files uses pixels in conditional pixels in and that conditions on a latent vector and the recognition path, just like the adversarial autoencoder, uses a generative adversarial network to impose a prior distribution on the latent code show that imposing different distributions on the latent code will result in different decomposition of information between the latent code and autoregressive decoder, for example by imposing a Gaussian distribution we can achieve.",
                    "label": 0
                },
                {
                    "sent": "What versus local decompositional information or by imposing a categorical distribution on the latent code, we can achieve a discrete versus continuous decomposition of information between latent code and autoregressive decoder.",
                    "label": 1
                },
                {
                    "sent": "Finally, I'll show how the pixel gun autoencoder with categorical prior can be used in clustering and semi supervised learning applications.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the Pixel CNN is an auto regressive neural networks that captures the image statistics directly at the pixel level.",
                    "label": 1
                },
                {
                    "sent": "They are very good at modeling though level pixel statistics and local structures.",
                    "label": 0
                },
                {
                    "sent": "But they generated samples often like the global structure.",
                    "label": 0
                },
                {
                    "sent": "So one of the drawback is that they don't learn a hierarchical latent representation, so it's not very straightforward to use them in downstream tasks such as English verbs learning.",
                    "label": 1
                },
                {
                    "sent": "So here we can see the architecture of a conditional pixel CNN that can learn conditional densities.",
                    "label": 0
                },
                {
                    "sent": "Using adaptive bias in the convolutional layers.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the variational autoencoder is a generative autoencoder which characterizes the posterior distribution using the encoder network and pair it with the top down generative network.",
                    "label": 0
                },
                {
                    "sent": "Both networks are trained jointly to maximize the variational lower bound on the Datalog.",
                    "label": 0
                },
                {
                    "sent": "Cute so variational autoencoders do learn hierarchal representation that is useful for downstream tasks such as semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, these latent variable models such as variational autoencoders kind of complements the auto regressive architecture in the sense that one is good at capturing the global statistics and the other is good at capturing the local statistiques.",
                    "label": 1
                },
                {
                    "sent": "So it kind of makes sense to combine them.",
                    "label": 0
                },
                {
                    "sent": "There has been some attempts at combining them, such as the Pixel, the work from the Middle Group here in Montreal, and the variational lossy autoencoder from Open AI.",
                    "label": 0
                },
                {
                    "sent": "So this pixel gun auto encoder that I'm going to introduce can be considered as another attempt at combining these architectures.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's talk a little bit about the adversarial autoencoder.",
                    "label": 1
                },
                {
                    "sent": "Before that, let me define the aggregated posterior distribution directed poster distribution.",
                    "label": 0
                },
                {
                    "sent": "QFC is essentially the average posterior distribution over the data distribution.",
                    "label": 0
                },
                {
                    "sent": "So in adversarial autoencoders, in addition to the standard reconstruction costs of the auto encoder, we have an adversarial cost as a regularization term that tries to match the aggregated closer distribution to an arbitrary prior distribution.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of like replacing the KL term.",
                    "label": 0
                },
                {
                    "sent": "In the variation autoencoder objective with an adversarial cost.",
                    "label": 0
                },
                {
                    "sent": "So the quality of the quality of samples of the adversarial autoencoder is very much like the variational autoencoder, but its main advantage is that you can impose any arbitrary or on the latent code just from the samples of the prior.",
                    "label": 1
                },
                {
                    "sent": "For example here you can see the enemies codespace off the pixel gun autoencoder where we have imposed a Gaussian or a mixture of Gaussian distribution on the latent code.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's talk about the pixel gun autoencoder.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the latent viral models that I just talked about, such as variational, autoencoder, or adverse aerial autoencoder, do have some limitations in order to sample from the first sample from the priority of Z and then we use the conditional P of X currency to sample from the model distribution.",
                    "label": 1
                },
                {
                    "sent": "But in these models the conditional piece of excrement Z is typically a factor is Gaussian distribution, which has very limited stochastic city.",
                    "label": 0
                },
                {
                    "sent": "So in these architectures, essentially we have a single latent vector that is capturing all the underlying factors of variation, such as the label information, destroying information, the global structure, or the local structure.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Pixel Gun auto encoder addresses this issue by using a powerful stochastically coder such as pixel CNN.",
                    "label": 0
                },
                {
                    "sent": "So in pixel gun autoencoders, the statistics of the data distribution is jointly captured by the latent code and autoregressive decoder.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The inference path uses a generative adversarial network to impose a prior distribution on the latent code.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll show that imposing different distribution on the latent code enables us to specify the type of their statistics that we care about and capture it by the Latin representation, while the remaining structure of the image is captured by the auto regressive decoder.",
                    "label": 1
                },
                {
                    "sent": "For example by imposing a Gaussian distribution we can achieve a global versus local decomposition of information and disentangled the low frequency and high frequency content of the images, or by imposing a categorical distribution supplier we can achieve a discrete versus continuous decomposition and disentangled the label and the style of images.",
                    "label": 0
                },
                {
                    "sent": "In a purely unsupervised fashion, so this discrete versus continue.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The composition is particularly interesting because it can be directly used for semi supervised learning tasks.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the architecture of the adversarial autoencoder.",
                    "label": 1
                },
                {
                    "sent": "As I explained, the genetic path is a pixel center.",
                    "label": 0
                },
                {
                    "sent": "Conditions on the latent vector and the recognition path users.",
                    "label": 0
                },
                {
                    "sent": "A generative adversarial network to impose a prior distribution on the latent code.",
                    "label": 0
                },
                {
                    "sent": "So the cost function of the pixel gun autoencoder is there construction cost plus adverse aerial cost.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see what happens if we impose a Gaussian distribution on the latent code.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the left figure shows the samples of the pixel gun.",
                    "label": 0
                },
                {
                    "sent": "Autoencoder trained on this data set for the purpose of matter illustrating the decomposition of information.",
                    "label": 0
                },
                {
                    "sent": "We have chosen a limited code size of two 2D Gaussian code and the limited receptive field size.",
                    "label": 0
                },
                {
                    "sent": "The middle figure shows the samples of the pixel CNN that uses the same limited receptive field size and the right figure shows the samples of an adversarial autoencoder that uses limited code size of two.",
                    "label": 0
                },
                {
                    "sent": "So you can see pixel CNN.",
                    "label": 0
                },
                {
                    "sent": "Is capturing the local statistics very well, but fails to generate the global structure because of the limited receptive field size.",
                    "label": 0
                },
                {
                    "sent": "And adversarial autoencoder, on the other hand, is capturing the global statistics, but fails to generate the details of the image due to the limited code size.",
                    "label": 0
                },
                {
                    "sent": "But the Pixel Gan autoencoder with the same limited code size, limited receptive field, can kind of combine the best of both and generate short images with global statistics.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here you can see the enemies codespace of ethics again, autoencoder, where we have imposed a Gaussian distribution on the latent code.",
                    "label": 1
                },
                {
                    "sent": "So in pixel gun autoencoders the latent code is encouraged to capture the global structure or the what information as opposed to the very information.",
                    "label": 0
                },
                {
                    "sent": "For example, in this figure you can see the network is using one of the access of the two D Gaussian code to explicitly encode the label information even though we have imposed a continuous prior such as the Gaussian program.",
                    "label": 0
                },
                {
                    "sent": "In this case we can potentially get a much better separation in the codespace if we impose a discrete prior.",
                    "label": 0
                },
                {
                    "sent": "Also, you can see in this figure that the latent code is capturing all the label information.",
                    "label": 0
                },
                {
                    "sent": "What is also capturing some of their study information while the remaining style information is captured by the auto regressive decoder?",
                    "label": 0
                },
                {
                    "sent": "So the question is whether it is possible to achieve a complete decomposition of style and content in the sense that the label the latent variable only captures the label information and all the style information is separately captured by the auto regressive decoder.",
                    "label": 0
                },
                {
                    "sent": "So it turns out it is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Possible if we impose a categorical prior on the latent code.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the architecture of the pixel gun autoencoder with categorical prior.",
                    "label": 1
                },
                {
                    "sent": "The categorical prior is imposed directly on the continuous output probabilities of the softmax softmax layer of the encoder.",
                    "label": 0
                },
                {
                    "sent": "In this case, the discrete latent code of the network learns discrete factors of variations, such as the label information and.",
                    "label": 0
                },
                {
                    "sent": "And the auto regressive decoder learns the study information of the digit condition on the label information of the digits.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to better understand the effect of the adverse aerial training, we train a pixel gun.",
                    "label": 0
                },
                {
                    "sent": "Autoencoder only on the 1st 3 digits of MNIST and we choose the code size to be tree.",
                    "label": 1
                },
                {
                    "sent": "So here in this figure we are projecting that really softmax simplex of the encoder onto a 2D triangle and we are visualizing the codespace.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Left figure shows the emnace codespace off the pixel gun autoencoder where no distribution is imposed on the latent code.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the network has learned to use the surface of the softmax simplex to encode summer study information and therefore the three corners of the softmax simplex do not really have any meaningful interpretation.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The right figure shows the code is in discord.",
                    "label": 0
                },
                {
                    "sent": "This place off the pixel gun autoencoder where a categorical priorities impose on the latent code so you can see that in this case the network is using the tree corners of the softmax simplex to encode the label information and all their study in formation is separately captured by the auto regressive decoder.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case, we can get an almost perfect error rate of .3% even though the network is trained in a purely unsupervised fashion.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once the pixel gun autoencoder is trained, we can use this encoder to cluster new points and we can use its decoder to generate conditional samples from each of the learning clusters.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here you can see the conditional samples of the pixel gun autoencoder trained on the full amnesty data set.",
                    "label": 0
                },
                {
                    "sent": "You can see from.",
                    "label": 0
                },
                {
                    "sent": "So each row of this figure shows basically the conditional samples from one of the learned clusters, so you can see from this figure that the discrete latent code off the network has learned discrete factors of variation, such as the label information and some discrete design information.",
                    "label": 0
                },
                {
                    "sent": "For example, the network has assigned two different clusters to digital tools based on whether they are written with a loop at the bottom or it has assigned like different clusters to digit 7 based on whether they have a dash in the middle.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case we can achieve 5% error rate on the full in this data set in an unsupervised fashion just by matching each cluster to a digit type.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out it's very straightforward to use these pixel gun architectures in the semi supervised learning settings.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to do so, after optimizing the reconstruction costs and the and the adversarial costs on and only, we simply train the weights of the encoder networks on labeled mini batch to minimize the cross entropy at the end of the last layer of the encoder.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this figure shows the conditional samples of a semi supervised pixel gun autoencoder trained on the MSDN and or data set.",
                    "label": 0
                },
                {
                    "sent": "So each column shows the conditional samples given each class label.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the pixel gun autoencoder can achieve a rather clean separation of content under slide with very few labeled data.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this table reports the semi supervised classification results of the pixel gun.",
                    "label": 0
                },
                {
                    "sent": "Autoencoder trained on the Omni switch analog data set on.",
                    "label": 0
                },
                {
                    "sent": "Then this data set we have been using 100 total labels, 50 labels, 20 labels and labels on a switch and use thousand labels on 500 labels on North datasets used $1000 and you can see that our classification results are highly competitive with their state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you and now you have to take your questions.",
                    "label": 0
                }
            ]
        }
    }
}