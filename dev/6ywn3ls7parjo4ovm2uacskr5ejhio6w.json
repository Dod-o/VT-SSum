{
    "id": "6ywn3ls7parjo4ovm2uacskr5ejhio6w",
    "title": "Asymptotic Theory for Linear-Chain Conditional Random Fields",
    "info": {
        "author": [
            "Mathieu Sinn, University of Waterloo"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Algorithmic Information Theory",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2011_sinn_asymptotic/",
    "segmentation": [
        [
            "Yes, the title suggests this is a more theoretical paper.",
            "Yeah, and in this talk I will omit some of the technical details and instead try to illustrate the main ideas and at some point I can promise you that I'm even going to show you a short animation.",
            "But yeah, I will give you notice in time."
        ],
        [
            "So let us start by recalling the definition of linear chain, conditional random fields or else refs for short sonar CRF models.",
            "The conditional distribution of a sequence of labels Y one up to YN given sequence of observations X0 up to XM.",
            "So for example, think of our theorists in the context of human activity recognition, where XN represents a stream of sensor measurements and why in the sequence of human activities.",
            "And the basic assumption when using an out CRF set the labels conditional on the observations for Markov chain.",
            "And then by a famous result from the theory of undirected graphical models, the probability of given sequences Y an condition on XN can be written as such a product of what is called potentials and where.",
            "This factor here is a normalizing constant and is common in the literature.",
            "Here we also assume that the potential can be written as log linear combinations of weights Lambda and features F. And so those of you who are more familiar with conditional random fields will realize that here we are making certain restrictions.",
            "The most important is that the weights and the features are shared across the potentials, which means that the dynamics of the model doesn't change with time."
        ],
        [
            "Now let us consider the following problem.",
            "So suppose we were given training data samples X&Y and where the distribution of the sequence XN is unknown and the distribution of Y and condition on X and follows in LCR F where we know the features F. But we don't know the true weights Lambda star.",
            "So the question is, how can we estimate Lambda star from the data and the most widely used approach is to maximize conditional.",
            "Look like you that as we compute the arc Max of this function of Lambda, which is the log of the conditional probability of Y given X if the parameter of the CRF is Lambda.",
            "And practice.",
            "There are, there are simple closed form expressions for the gradient of the end, so we can find the artworks by simple gradient based methods."
        ],
        [
            "Now, intuitively, we will expect that larger N. Of course, the closer our estimate Lambda head and will be to the true value Lambda star in our paper.",
            "Our goal is to establish that this fact formally or more precisely we want to establish conditions under which the estimator Lambda head and is consistent, which means converges to the true value Lambda star as the sample size N goes to Infinity.",
            "But at this point we are facing a problem.",
            "We don't have a model for infinite sequences, so conventional L. CRF specified distributions of finite sequences.",
            "But here we need a model for sequences of arbitrary length.",
            "The way we're going to handle this is the following.",
            "We will define linear chain conditional random fields for double sight infinite sequences X&Y.",
            "And then consider the samples to be finite subsets sequences of these infinite sequences."
        ],
        [
            "So before going into the theory, let me show you very simple toy example.",
            "So here we suppose the observations of the CRF are real value.",
            "There are only two labels, one and two.",
            "And suppose the features and weights have this following simple form.",
            "So in particular this means that if the observations are positive, it's more likely the label at the same time point is equal to 1.",
            "If the observation is negative, it's more likely the labels two and these two bottom features ensure that there are some persistence of the labeled sequence.",
            "So for example, here we have a sample of observations of 200 time points generated by an auto regressive model with standard normal innovations."
        ],
        [
            "And now we are.",
            "We consider why conditional on X following the LCR F with the above features and wait and hear the grey regions represent the time points at which the labels equal to 1 and the white regions the time points at which the labels equal to 0 and you can see the correspondence between the grey regions and the parts where the observations are positive and correspondence between regions where labels equal to two.",
            "And the observations are negative and."
        ],
        [
            "What happens if we apply our maximum likelihood estimator to the data?",
            "So based on this fairly short sequence, we obtain this estimate here, which are not surprisingly, is not too accurate.",
            "But as these lower plot show as the sample size increases, the maximum likelihood estimates indeed converge to the true value Lambda star.",
            "So here in these four plots is shown separately for each component, the X axis is the log of the sample size, the black lines.",
            "Show the mean of the estimates and the blue lines.",
            "The upper and lower 5th percentiles.",
            "And in our paper, we're going to prove that under certain conditions, indeed, the maximum likelihood estimates always converge to the true value Lambda star."
        ],
        [
            "As a final introductory mark, of course, in practice we do not only need to estimate Lambda star, but also to estimate the features.",
            "So in fact, the problem then is to learn the energy functions of the LTR F, which is a very difficult problem.",
            "Uh, so common approaches and practice are then to either use prior knowledge to obtain some reasonable gas or to resort to some parametric approximations.",
            "For example using linear features or features that are piecewise constant.",
            "And of course it would be interesting to know what can we say about the limit of the maximum likelihood estimates in those cases and our present work.",
            "OK, we start showing that in the vanilla case where F is known, everything works fine, provides a framework tool study.",
            "Also these modificate cases, in particular by.",
            "Defining these else Rs for infinite sequences in establishing some useful link to the theories of week ago, Decety and Markov chain and random environm."
        ],
        [
            "It's."
        ],
        [
            "So.",
            "Let's see how to define the actors for infinite sequences.",
            "So the goal is given a sequence of observations to define the distribution of why conditional on X?",
            "And our starting point is a well known representation for the model and distribution of conventional LCF.",
            "So here M is a matrix which contains the potentials of all labels IJ given the current observation is X.",
            "Then the ice component of the vector Alpha is the total potential of any labeled sequence between time S&T, which ends with I and similarly the ice component of the vector beater is the total potential of label sequences which start and I, and then it's a well known fact for conventional CRF's.",
            "For finite sequences, the marginal probabilities can be written in this form.",
            "And here so this expression here in the bottom, it only depends on the observations from time zero to N. So which is indicated here by the superscript and.",
            "It's assumed that the time indices T is greater than zero ante plus K is smaller than N."
        ],
        [
            "Now to define the answer is for infinite sequences we use the same formula, but we take into account a larger observation.",
            "ULL context, namely from observation X -- 1 up to XN.",
            "And so, again, this is emphasized by the superscripts, and here the time indices can be arbitrary, fixed but arbitrary.",
            "So and St when, when sorry, is large enough, then this properly defines a probability distribution.",
            "And now the idea of defining the answer for infinite sequences to let the size of this observation of context go to Infinity, as here.",
            "In the statement of the theorem.",
            "But of course the question is.",
            "Under what conditions does does this limit here exist?",
            "So it could very well happen that these probabilities oscillate anywhere in between zero and one as N goes to Infinity.",
            "But as a theorem shows, if we assume that the features are bounded and the weights Lambda, find it, then this limit in fact does exist.",
            "Moreover, we can show that the rate of convergence is geometrical.",
            "And to show you only."
        ],
        [
            "One detail of the proof.",
            "So in order to show that that this expression here converges, we need to analyze the asymptotical behavior of these matrix products which occur in the definition of the vectors Alpha and beta, and using the theory of weak echo density, we can show that the roles of these matrices tend to proportionality as N goes to Infinity.",
            "That allows us then to show that the ratios of components of Alpha and beta.",
            "Converge to constants.",
            "And it turns out that this these results from the theory of weak at this city are quite useful also in the following and then using Theorem one we obtained the marginal distributions of the sequence.",
            "Why conditional on X and using Kolmogorov extension theorem that specifies a unique distribution for the whole sequence Y?"
        ],
        [
            "So let me illustrate through in one.",
            "So this is going to be the animation I was talking about at the beginning.",
            "So let's suppose we are given such a sequence of observations and we want to consider the distribution of the label at time .0.",
            "So the LCS has the same features and weights SB4.",
            "If we condition YT only on X0.",
            "Then the distribution is the following the probability that YT is equal to 1 is 62% probability that it's equal to two is 38%."
        ],
        [
            "Now if we increase the size of the observational contracts context, which is this Gray region here, then the following happens the observations at time minus one and plus one are both negative and that increases the probability that the label is equal to 2."
        ],
        [
            "If you further increase the size for the next 2 observations, also negative, then the probability that YT is equal to do further increases."
        ],
        [
            "Then more positive observations again come into play and then we see when N is equal to five and larger distribution stabilizes and."
        ],
        [
            "Actually converges pretty fast."
        ],
        [
            "Now."
        ],
        [
            "In our paper we then."
        ],
        [
            "Derive properties of the."
        ],
        [
            "The Lt Rs were infinite sequences, so first."
        ],
        [
            "Same S4, find it LCRFY conditional on X is a Markov chain with the transition probabilities given by this expression and we can show that these transition probabilities are strictly bounded away from zero.",
            "Furthermore, if the distribution of the observation sequences stationary, then also the joint sequence X&Y stationary so the distribution doesn't vary with time.",
            "And with a little bit more effort, we can show that if X is agatic then also the joint sequence X&Y psychotic and this is particularly useful in the following because it implies that any such time average of a function G converges to the expected value provided that expected value exists and is finite."
        ],
        [
            "Now let us apply these results."
        ],
        [
            "To analyze consistency of the maximum likelihood estimates.",
            "So this is again the setting from the introduction.",
            "So we observe a finite sequences XN of observations, and why, and of labels, which we now suppose a subsequence is of infinite answer F. Can of the distribution of X is unknown to us and we're also not primarily interested in it, and the distribution of Y conditional on X follows in Elsie.",
            "Referee infinite sequences with the known features F and unknown weights.",
            "Lambda star again in order to estimate these unknown weights, we compute the maximum likelihood estimate.",
            "That is, we maximize the log likelihood function Ellen of Lambda.",
            "And now we're going to look at formal conditions under which this maximum liked it.",
            "Estimates converge to the true parameter value.",
            "So first we establish the following lemma.",
            "We're making three assumptions, 1st that the features are bounded, and the weights Lambda.",
            "Find it so that the LCL CRF infinite sequences, well defined.",
            "Second, that process access agatic so that the drawing process X&Y psychotic and served at the parameter space data over which we are maximizing is compact, and then we control the following.",
            "So first the limit of the empirical likelihood function.",
            "Is well defined pointwise, so in fact, if we're going back to the definition, this mainly follows by their God theory, which shows that such averages will go to converge to some constant second, we can show that the convergence is uniform on the parameter set, and 3rd, we can show that the limit of the Hessian of the empirical likelihood function is finite and given by this infinite sum over covariances between.",
            "At different time points, for example, to show that this is fine, it we can again use theory of weak city too.",
            "In fact, true that these covariances decay with a geometric rate so that this sum is finite."
        ],
        [
            "And then based on that lemma we can establish conditions for consistency.",
            "We need to assume Additionally that the limit of the of the Hessian is nonsingular on the parameter space.",
            "Then it follows that the limit function is strictly concave.",
            "With a unique maximum in Lambda Star and the maximum likelihood estimates indeed converges to this maximum value.",
            "Now, this additional condition that the Hessian is nonsingular maybe not so nice because it's not always easy to verify, so let's see what actually happens if the limit of the Hessian singular.",
            "So simple example is if the vector features contains 2 identical features FI&FJ, then in fact there are infinitely many equivalent solutions to lumber star, namely any vector Lambda which.",
            "Ice and JS components have the same sum.",
            "Then I suggest components of Lambda star and or other components are equal to those of Lambda star yields exactly the same model.",
            "But one could think now that OK by simply ruling out such a linear dependences between features, maybe that's enough, but it's actually possible to construct a counterexample, so I won't go into details here, but OK, this counterexample shows.",
            "So it's it's not enough to simply out rule such linear dependencies."
        ],
        [
            "And that.",
            "Leads me directly to."
        ],
        [
            "All the discussion and open questions for future future research.",
            "So one question is what are sufficient conditions for the limit of the Hessian to be nonsingular and hence the Lambda star to be identifiable?",
            "Then another question is can weaken the assumption that the features are bounded?",
            "So maybe you've noticed that in the example I've shown you at the beginning with the linear feature functions, this assumption is actually not satisfied, so there is some evidence that that we can we can that assumption.",
            "What should be straightforward to do is to general generalize all these results to LC refs of higher orders, and, as I mentioned earlier, what later on will be really interesting.",
            "But of course much very, very hard questions is what can we say about how the limit of the maximum likelihood estimates, if we miss, specify the model.",
            "So if we, for example, we approximate unknown energy functions using linear features and in.",
            "In the context of human activity recognition where we're applying LCF, we're interested ultimately in guidelines which tell us how we should choose features to obtain models that are robust towards differences between individuals."
        ],
        [
            "And that was my last slide.",
            "Here are my main references and thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, the title suggests this is a more theoretical paper.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and in this talk I will omit some of the technical details and instead try to illustrate the main ideas and at some point I can promise you that I'm even going to show you a short animation.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I will give you notice in time.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let us start by recalling the definition of linear chain, conditional random fields or else refs for short sonar CRF models.",
                    "label": 0
                },
                {
                    "sent": "The conditional distribution of a sequence of labels Y one up to YN given sequence of observations X0 up to XM.",
                    "label": 0
                },
                {
                    "sent": "So for example, think of our theorists in the context of human activity recognition, where XN represents a stream of sensor measurements and why in the sequence of human activities.",
                    "label": 0
                },
                {
                    "sent": "And the basic assumption when using an out CRF set the labels conditional on the observations for Markov chain.",
                    "label": 1
                },
                {
                    "sent": "And then by a famous result from the theory of undirected graphical models, the probability of given sequences Y an condition on XN can be written as such a product of what is called potentials and where.",
                    "label": 0
                },
                {
                    "sent": "This factor here is a normalizing constant and is common in the literature.",
                    "label": 0
                },
                {
                    "sent": "Here we also assume that the potential can be written as log linear combinations of weights Lambda and features F. And so those of you who are more familiar with conditional random fields will realize that here we are making certain restrictions.",
                    "label": 0
                },
                {
                    "sent": "The most important is that the weights and the features are shared across the potentials, which means that the dynamics of the model doesn't change with time.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let us consider the following problem.",
                    "label": 0
                },
                {
                    "sent": "So suppose we were given training data samples X&Y and where the distribution of the sequence XN is unknown and the distribution of Y and condition on X and follows in LCR F where we know the features F. But we don't know the true weights Lambda star.",
                    "label": 1
                },
                {
                    "sent": "So the question is, how can we estimate Lambda star from the data and the most widely used approach is to maximize conditional.",
                    "label": 1
                },
                {
                    "sent": "Look like you that as we compute the arc Max of this function of Lambda, which is the log of the conditional probability of Y given X if the parameter of the CRF is Lambda.",
                    "label": 0
                },
                {
                    "sent": "And practice.",
                    "label": 0
                },
                {
                    "sent": "There are, there are simple closed form expressions for the gradient of the end, so we can find the artworks by simple gradient based methods.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, intuitively, we will expect that larger N. Of course, the closer our estimate Lambda head and will be to the true value Lambda star in our paper.",
                    "label": 1
                },
                {
                    "sent": "Our goal is to establish that this fact formally or more precisely we want to establish conditions under which the estimator Lambda head and is consistent, which means converges to the true value Lambda star as the sample size N goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "But at this point we are facing a problem.",
                    "label": 0
                },
                {
                    "sent": "We don't have a model for infinite sequences, so conventional L. CRF specified distributions of finite sequences.",
                    "label": 1
                },
                {
                    "sent": "But here we need a model for sequences of arbitrary length.",
                    "label": 0
                },
                {
                    "sent": "The way we're going to handle this is the following.",
                    "label": 0
                },
                {
                    "sent": "We will define linear chain conditional random fields for double sight infinite sequences X&Y.",
                    "label": 1
                },
                {
                    "sent": "And then consider the samples to be finite subsets sequences of these infinite sequences.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before going into the theory, let me show you very simple toy example.",
                    "label": 0
                },
                {
                    "sent": "So here we suppose the observations of the CRF are real value.",
                    "label": 0
                },
                {
                    "sent": "There are only two labels, one and two.",
                    "label": 0
                },
                {
                    "sent": "And suppose the features and weights have this following simple form.",
                    "label": 0
                },
                {
                    "sent": "So in particular this means that if the observations are positive, it's more likely the label at the same time point is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "If the observation is negative, it's more likely the labels two and these two bottom features ensure that there are some persistence of the labeled sequence.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we have a sample of observations of 200 time points generated by an auto regressive model with standard normal innovations.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we are.",
                    "label": 0
                },
                {
                    "sent": "We consider why conditional on X following the LCR F with the above features and wait and hear the grey regions represent the time points at which the labels equal to 1 and the white regions the time points at which the labels equal to 0 and you can see the correspondence between the grey regions and the parts where the observations are positive and correspondence between regions where labels equal to two.",
                    "label": 0
                },
                {
                    "sent": "And the observations are negative and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What happens if we apply our maximum likelihood estimator to the data?",
                    "label": 0
                },
                {
                    "sent": "So based on this fairly short sequence, we obtain this estimate here, which are not surprisingly, is not too accurate.",
                    "label": 1
                },
                {
                    "sent": "But as these lower plot show as the sample size increases, the maximum likelihood estimates indeed converge to the true value Lambda star.",
                    "label": 0
                },
                {
                    "sent": "So here in these four plots is shown separately for each component, the X axis is the log of the sample size, the black lines.",
                    "label": 0
                },
                {
                    "sent": "Show the mean of the estimates and the blue lines.",
                    "label": 0
                },
                {
                    "sent": "The upper and lower 5th percentiles.",
                    "label": 0
                },
                {
                    "sent": "And in our paper, we're going to prove that under certain conditions, indeed, the maximum likelihood estimates always converge to the true value Lambda star.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a final introductory mark, of course, in practice we do not only need to estimate Lambda star, but also to estimate the features.",
                    "label": 0
                },
                {
                    "sent": "So in fact, the problem then is to learn the energy functions of the LTR F, which is a very difficult problem.",
                    "label": 0
                },
                {
                    "sent": "Uh, so common approaches and practice are then to either use prior knowledge to obtain some reasonable gas or to resort to some parametric approximations.",
                    "label": 1
                },
                {
                    "sent": "For example using linear features or features that are piecewise constant.",
                    "label": 0
                },
                {
                    "sent": "And of course it would be interesting to know what can we say about the limit of the maximum likelihood estimates in those cases and our present work.",
                    "label": 1
                },
                {
                    "sent": "OK, we start showing that in the vanilla case where F is known, everything works fine, provides a framework tool study.",
                    "label": 0
                },
                {
                    "sent": "Also these modificate cases, in particular by.",
                    "label": 0
                },
                {
                    "sent": "Defining these else Rs for infinite sequences in establishing some useful link to the theories of week ago, Decety and Markov chain and random environm.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's see how to define the actors for infinite sequences.",
                    "label": 1
                },
                {
                    "sent": "So the goal is given a sequence of observations to define the distribution of why conditional on X?",
                    "label": 1
                },
                {
                    "sent": "And our starting point is a well known representation for the model and distribution of conventional LCF.",
                    "label": 0
                },
                {
                    "sent": "So here M is a matrix which contains the potentials of all labels IJ given the current observation is X.",
                    "label": 0
                },
                {
                    "sent": "Then the ice component of the vector Alpha is the total potential of any labeled sequence between time S&T, which ends with I and similarly the ice component of the vector beater is the total potential of label sequences which start and I, and then it's a well known fact for conventional CRF's.",
                    "label": 0
                },
                {
                    "sent": "For finite sequences, the marginal probabilities can be written in this form.",
                    "label": 0
                },
                {
                    "sent": "And here so this expression here in the bottom, it only depends on the observations from time zero to N. So which is indicated here by the superscript and.",
                    "label": 0
                },
                {
                    "sent": "It's assumed that the time indices T is greater than zero ante plus K is smaller than N.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now to define the answer is for infinite sequences we use the same formula, but we take into account a larger observation.",
                    "label": 1
                },
                {
                    "sent": "ULL context, namely from observation X -- 1 up to XN.",
                    "label": 0
                },
                {
                    "sent": "And so, again, this is emphasized by the superscripts, and here the time indices can be arbitrary, fixed but arbitrary.",
                    "label": 0
                },
                {
                    "sent": "So and St when, when sorry, is large enough, then this properly defines a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And now the idea of defining the answer for infinite sequences to let the size of this observation of context go to Infinity, as here.",
                    "label": 0
                },
                {
                    "sent": "In the statement of the theorem.",
                    "label": 0
                },
                {
                    "sent": "But of course the question is.",
                    "label": 0
                },
                {
                    "sent": "Under what conditions does does this limit here exist?",
                    "label": 0
                },
                {
                    "sent": "So it could very well happen that these probabilities oscillate anywhere in between zero and one as N goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "But as a theorem shows, if we assume that the features are bounded and the weights Lambda, find it, then this limit in fact does exist.",
                    "label": 1
                },
                {
                    "sent": "Moreover, we can show that the rate of convergence is geometrical.",
                    "label": 0
                },
                {
                    "sent": "And to show you only.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One detail of the proof.",
                    "label": 0
                },
                {
                    "sent": "So in order to show that that this expression here converges, we need to analyze the asymptotical behavior of these matrix products which occur in the definition of the vectors Alpha and beta, and using the theory of weak echo density, we can show that the roles of these matrices tend to proportionality as N goes to Infinity.",
                    "label": 1
                },
                {
                    "sent": "That allows us then to show that the ratios of components of Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "Converge to constants.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this these results from the theory of weak at this city are quite useful also in the following and then using Theorem one we obtained the marginal distributions of the sequence.",
                    "label": 1
                },
                {
                    "sent": "Why conditional on X and using Kolmogorov extension theorem that specifies a unique distribution for the whole sequence Y?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me illustrate through in one.",
                    "label": 0
                },
                {
                    "sent": "So this is going to be the animation I was talking about at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose we are given such a sequence of observations and we want to consider the distribution of the label at time .0.",
                    "label": 1
                },
                {
                    "sent": "So the LCS has the same features and weights SB4.",
                    "label": 0
                },
                {
                    "sent": "If we condition YT only on X0.",
                    "label": 1
                },
                {
                    "sent": "Then the distribution is the following the probability that YT is equal to 1 is 62% probability that it's equal to two is 38%.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if we increase the size of the observational contracts context, which is this Gray region here, then the following happens the observations at time minus one and plus one are both negative and that increases the probability that the label is equal to 2.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you further increase the size for the next 2 observations, also negative, then the probability that YT is equal to do further increases.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then more positive observations again come into play and then we see when N is equal to five and larger distribution stabilizes and.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually converges pretty fast.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our paper we then.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Derive properties of the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Lt Rs were infinite sequences, so first.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same S4, find it LCRFY conditional on X is a Markov chain with the transition probabilities given by this expression and we can show that these transition probabilities are strictly bounded away from zero.",
                    "label": 1
                },
                {
                    "sent": "Furthermore, if the distribution of the observation sequences stationary, then also the joint sequence X&Y stationary so the distribution doesn't vary with time.",
                    "label": 0
                },
                {
                    "sent": "And with a little bit more effort, we can show that if X is agatic then also the joint sequence X&Y psychotic and this is particularly useful in the following because it implies that any such time average of a function G converges to the expected value provided that expected value exists and is finite.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let us apply these results.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To analyze consistency of the maximum likelihood estimates.",
                    "label": 0
                },
                {
                    "sent": "So this is again the setting from the introduction.",
                    "label": 0
                },
                {
                    "sent": "So we observe a finite sequences XN of observations, and why, and of labels, which we now suppose a subsequence is of infinite answer F. Can of the distribution of X is unknown to us and we're also not primarily interested in it, and the distribution of Y conditional on X follows in Elsie.",
                    "label": 1
                },
                {
                    "sent": "Referee infinite sequences with the known features F and unknown weights.",
                    "label": 0
                },
                {
                    "sent": "Lambda star again in order to estimate these unknown weights, we compute the maximum likelihood estimate.",
                    "label": 0
                },
                {
                    "sent": "That is, we maximize the log likelihood function Ellen of Lambda.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to look at formal conditions under which this maximum liked it.",
                    "label": 0
                },
                {
                    "sent": "Estimates converge to the true parameter value.",
                    "label": 0
                },
                {
                    "sent": "So first we establish the following lemma.",
                    "label": 0
                },
                {
                    "sent": "We're making three assumptions, 1st that the features are bounded, and the weights Lambda.",
                    "label": 0
                },
                {
                    "sent": "Find it so that the LCL CRF infinite sequences, well defined.",
                    "label": 0
                },
                {
                    "sent": "Second, that process access agatic so that the drawing process X&Y psychotic and served at the parameter space data over which we are maximizing is compact, and then we control the following.",
                    "label": 0
                },
                {
                    "sent": "So first the limit of the empirical likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Is well defined pointwise, so in fact, if we're going back to the definition, this mainly follows by their God theory, which shows that such averages will go to converge to some constant second, we can show that the convergence is uniform on the parameter set, and 3rd, we can show that the limit of the Hessian of the empirical likelihood function is finite and given by this infinite sum over covariances between.",
                    "label": 0
                },
                {
                    "sent": "At different time points, for example, to show that this is fine, it we can again use theory of weak city too.",
                    "label": 0
                },
                {
                    "sent": "In fact, true that these covariances decay with a geometric rate so that this sum is finite.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then based on that lemma we can establish conditions for consistency.",
                    "label": 0
                },
                {
                    "sent": "We need to assume Additionally that the limit of the of the Hessian is nonsingular on the parameter space.",
                    "label": 1
                },
                {
                    "sent": "Then it follows that the limit function is strictly concave.",
                    "label": 1
                },
                {
                    "sent": "With a unique maximum in Lambda Star and the maximum likelihood estimates indeed converges to this maximum value.",
                    "label": 0
                },
                {
                    "sent": "Now, this additional condition that the Hessian is nonsingular maybe not so nice because it's not always easy to verify, so let's see what actually happens if the limit of the Hessian singular.",
                    "label": 0
                },
                {
                    "sent": "So simple example is if the vector features contains 2 identical features FI&FJ, then in fact there are infinitely many equivalent solutions to lumber star, namely any vector Lambda which.",
                    "label": 0
                },
                {
                    "sent": "Ice and JS components have the same sum.",
                    "label": 0
                },
                {
                    "sent": "Then I suggest components of Lambda star and or other components are equal to those of Lambda star yields exactly the same model.",
                    "label": 0
                },
                {
                    "sent": "But one could think now that OK by simply ruling out such a linear dependences between features, maybe that's enough, but it's actually possible to construct a counterexample, so I won't go into details here, but OK, this counterexample shows.",
                    "label": 0
                },
                {
                    "sent": "So it's it's not enough to simply out rule such linear dependencies.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that.",
                    "label": 0
                },
                {
                    "sent": "Leads me directly to.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the discussion and open questions for future future research.",
                    "label": 0
                },
                {
                    "sent": "So one question is what are sufficient conditions for the limit of the Hessian to be nonsingular and hence the Lambda star to be identifiable?",
                    "label": 0
                },
                {
                    "sent": "Then another question is can weaken the assumption that the features are bounded?",
                    "label": 0
                },
                {
                    "sent": "So maybe you've noticed that in the example I've shown you at the beginning with the linear feature functions, this assumption is actually not satisfied, so there is some evidence that that we can we can that assumption.",
                    "label": 0
                },
                {
                    "sent": "What should be straightforward to do is to general generalize all these results to LC refs of higher orders, and, as I mentioned earlier, what later on will be really interesting.",
                    "label": 0
                },
                {
                    "sent": "But of course much very, very hard questions is what can we say about how the limit of the maximum likelihood estimates, if we miss, specify the model.",
                    "label": 0
                },
                {
                    "sent": "So if we, for example, we approximate unknown energy functions using linear features and in.",
                    "label": 0
                },
                {
                    "sent": "In the context of human activity recognition where we're applying LCF, we're interested ultimately in guidelines which tell us how we should choose features to obtain models that are robust towards differences between individuals.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that was my last slide.",
                    "label": 0
                },
                {
                    "sent": "Here are my main references and thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}