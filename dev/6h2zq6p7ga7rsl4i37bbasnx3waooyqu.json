{
    "id": "6h2zq6p7ga7rsl4i37bbasnx3waooyqu",
    "title": "Dirichlet Aggregation: Unsupervised Learning towards an Optimal Metric for Proportional Data",
    "info": {
        "author": [
            "Hua-Yan Wang, National Laboratory On Machine Perception, Peking University"
        ],
        "published": "June 23, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Bayesian Learning->Dirichlet Processes"
        ]
    },
    "url": "http://videolectures.net/icml07_wang_daul/",
    "segmentation": [
        [
            "I sincerely thank you guys for not leaving.",
            "OK, our work is an unsupervised machine learning which is a model named Dark Cloud Aggregation and we have a comparison of number of papers in this metric learning session and the metric learning session two.",
            "There are seven and we are the only one that is unsupervised, so you may want to pay special attention."
        ],
        [
            "OK, here is the outline of my presentation.",
            "First I will give a general introduction to the metric learning problem and some and one related work on unsupervised metric learning.",
            "And then I will present our model including.",
            "This basic idea is setting and our approach.",
            "Finally I will show some experiment results."
        ],
        [
            "Gather with a conclusion.",
            "We have seen this several times.",
            "We need a metric or dissimilarity measure in a lot of tasks in machine learning, including classification, clustering, and information retrieval.",
            "Uh, so basically why we have to learn a metric.",
            "We already have a lot of simple ways to define a metric in a vector space, such as the Euclidean metric or LP metric.",
            "Also, I think a possible explanation to this question is a simple metric such as Euclidean metric does not work well due to sophisticated underlying latent structure of the data space.",
            "And these sophisticated, sophisticated structures are arising from generative process from some latent variable space, and this general to process, also known as a generative model, is sometimes very hard to describe.",
            "So metric learning could be viewed as trying to find some evidence about the latent structure directly in the data space without saying anything about latent variables."
        ],
        [
            "We have two different settings in metric learning, in semi supervised or supervised metric learning, relative comparison information or category labels of some data points given as constraints.",
            "That target metric should satisfy.",
            "And in our case which is unsupervised metric learning.",
            "We explore distribution patterns of unlabeled data points.",
            "So from this perspective is highly related to.",
            "Nonlinear dimensionality reduction also."
        ],
        [
            "Known as manifold learning.",
            "Here is a well known related work on unsupervised metric learning.",
            "As we know, for learning for learning the metric we first need to specify a family of candidate metrics so we can choose one metric from this family according to some criterion.",
            "For for defining this candidate matrix is also of this work.",
            "First defined parametric family of Maps that from.",
            "For most simplex two itself on these Maps are scaling of individual of individual dimensions.",
            "As you can see in this formula.",
            "So given this map we can define pullback metrics of the Fisher information metric so it's parametric family of metrics parameterized by the same parameter Lambda.",
            "By the way, in this way we define a parametric family of metrics that that change the metric in the simple simplex by scaling individual dimensions.",
            "In contrast, in our work, as we can see later, we define parametric family of metrics that is parameterized by affinity's, or distances between vertices of the simplex or dimensions of the vector space so that.",
            "Give us two more flex."
        ],
        [
            "Ability.",
            "Here's a basic idea of unsupervised metric learning and nonlinear dimensionality reduction.",
            "That is, geodesics should go through dense data points or have a simple example to explain this.",
            "First, we have a latent variable space with objects in it and this red arrow indicates the indicates our generative process, which results in data and data in the data space."
        ],
        [
            "Geodesics between two points, for example A&B in the latent variable space, is approximately the straight line connecting them.",
            "Becausw latent variable space often could be assumed as quasi Euclid."
        ],
        [
            "And distance measure could be viewed as a transition process moving from one point to another, which means changing one object into another object.",
            "The point is intermediate states on that straight line are also in the latent variable space, which means they represent meaningful objects in the same domain.",
            "So this transition process."
        ],
        [
            "This is meaningful.",
            "Usually the image of the latent variable space is a sparse subset in the data space with respect to the generative process, which is a projection.",
            "Or sometimes we assume that the the image of the latent variable spaces the manifold.",
            "This is be cause the latent variable space is usually low dimensional and data space usually is usually high dimensional.",
            "So in the data space, using a straight line to measure distance between two points is problematic.",
            "'cause was this in some intermediate?"
        ],
        [
            "States because some intermediate states might have no preimage in the in the latent variable space, which means they do not represent meaningful objects.",
            "And this transition.",
            "So this transition process along the straight line is."
        ],
        [
            "Meaningless.",
            "So as we know, the geodesics should go like this.",
            "It's a well known result in manifold learning.",
            "Such that all intermediate states have preimage in latent variable space.",
            "That is a they represent many for intermediate objects."
        ],
        [
            "Of the transition process.",
            "So in other words, the real length of the blue path.",
            "This is blue.",
            "This is relevant, is not.",
            "Of the real length of the blue path could be much longer than the red path when projected back to the latent variable space, and some points in the blue path.",
            "Maybe a million lus.",
            "So, however, finding the real geodesics such as red Pass in the data space is difficult in most cases.",
            "Actually, in manifold learning methods we are trying to find such real geodesic.",
            "However, the data are sometimes noisy and manifold assumption is sometimes violated, so funding such geodesics is hard job.",
            "So in our approach we are top, we adopt more flexibel version of this basic idea, that is path going through dense data points should be relatively shorter than Pascal into sparse data points."
        ],
        [
            "OK, here is a basic setting of our work.",
            "We can't we consider unlabeled data points in a simplex.",
            "For example normalized histograms.",
            "And each vertex of the simplex corresponds to one dimension in the original vector space.",
            "As you can see from this projection."
        ],
        [
            "OK, we consider a simple example of bag of words representation with forwards economy, market terenin geography.",
            "So in this setting, documents can be represented as normalized histograms, which are points in the three simplex.",
            "Intuitively, or we could expect many documents mention both economy and market a lot, and we also expect many documents mentioned both terenin geography alot.",
            "However, a relatively few documents would mention both economy and Terran alot, so the distribution of.",
            "Of data points in the simplex will be like this."
        ],
        [
            "Consider documents a BNC.",
            "So maybe the straight line AB should be shorter than BC, because according to the basic idea we have discussed, maybe it goes through dense data points and BC it goes through sparse data points, so this should be.",
            "Longer in this speech should be."
        ],
        [
            "Shorter.",
            "So in other words, the simplex should be warped like this.",
            "Now the following problem is how to implement such warping.",
            "In our approach we first in Furaffinity is between simplex vertices.",
            "This is by fitting a distribution parameterized by vertex affinity's.",
            "Then a metric on the simplex is induced from these infinities which is defined, defined as a smaller distance.",
            "Always ground distance is derived from Vertex finished."
        ],
        [
            "Explain this later.",
            "As well as we know, a common commonly used distribution on the simplex is a Democrat distribution.",
            "Which has an important property that expectation on on one dimension is proportional to.",
            "To the parameter associated with that dimension.",
            "So each parameter is associated with one vertex vertex of the simplex.",
            "Here's some examples of Democratic distribution, and it shows."
        ],
        [
            "2 lines.",
            "To get more flexibility, Orbu cause apparently the Democrat distribution cannot model this pattern.",
            "Becauses is, 4 words are equally important.",
            "So so if you if you use a direct distribution, the four parameters would be equal.",
            "So that gives rise to a symmetric distribution.",
            "So we have to get more flexibility.",
            "We use Dirichlet mixture model with an equally weighted components.",
            "And so we have also the parameters of this mixture distribution constitute metrics and we impose additional constraints on these parameters.",
            "So the.",
            "So the parameter matrix is a symmetric matrix."
        ],
        [
            "Always with diagonal elements.",
            "B1 off diagonal elements between zero and one 2C structure of the distribution with device.",
            "We use a fourth dimensional example for visualization.",
            "So as we can see, the diagonal elements represent affinity of.",
            "Affinity of a simplex two itself, so they are one and off that no elements corresponds to edges of the simplex, which indicates finishes between vertices."
        ],
        [
            "So we consider toy example like this.",
            "Are the parameters associated with H12 and three 4 exibits higher value than other off diagonal elements?"
        ],
        [
            "And we sampled 1000 points from the Dirichlet aggregation distribution with these parameters.",
            "So."
        ],
        [
            "So we can see that the distribution with device to use the pattern that just resembles the imaginary distribution we have discussed.",
            "So this is desirable.",
            "Also, suppose that these parameters are not given by hand, but are estimated from the observed distribution in the simplex.",
            "So we can readily conclude that the affinity between economy and market is not .7 and the affinity between geography and Ontarian is not .6.",
            "Parameter of the distribution now estimated by a very simple iterative algorithm.",
            "As a hamster Easter Banam step in the step, each data point is associated is assigned with the wait with respect to each dark cloud component and then stop parameters of each direct component are RE estimated with."
        ],
        [
            "Weighted data points.",
            "So now we explain how to defend a metric on the simplex.",
            "They are derived from these finishes.",
            "The metric is defined as the smallest distance with minus log of the learned vertex affinity's as ground distances.",
            "Their smallest distance between two histograms is defined as the effort needed to make them identical by transporting mass.",
            "Among these histogram bins.",
            "And the distance between histogram bins are called ground distance.",
            "For example, if if ground distance between being one and B2 is larger than that between being one bin three, so the left histogram would be closer to the top right histogram, then to the bottom right histogram if measured by MD."
        ],
        [
            "And now I will show some experiment results.",
            "We did experiments on the Reuters Corpus and Caltech image database.",
            "In the Reuters covers, we use the topic histogram representation of documents obtained by LDA.",
            "And for the image database we first extract sift descriptors, which is 128 dimensional and we cluster these descriptors into 2000 clusters and which we call visual words and we also reduce the dimensionality of this representation by LDA and we have a visual topic histogram representation of images.",
            "And we have 1/3 representation which is obtained by a cluster.",
            "The descriptors into 100.",
            "We show worse and directly use the visual world histogram to represent image."
        ],
        [
            "This.",
            "We compare the metric learned by our method and another simple metrics such as L1 metric and Jeffrey Divergance in Information retrieval task and different number of topics in LDA.",
            "So we see a stable is superior."
        ],
        [
            "Performance on this.",
            "This is a similar experiment on the Caltech image database.",
            "And we see similar superior result."
        ],
        [
            "And this is a different data representation of.",
            "Official word histogram.",
            "So our our method also works well in this situation."
        ],
        [
            "Now this is where we come to a conclusion of this work.",
            "It has several advantages, including this unsupervised, so we can easily collect the large training set.",
            "And this flexibel in that it handles correlations among dimensions.",
            "And it's also a global solution because we fit a unified distribution.",
            "We use a unified distribution to feed all observations.",
            "And it has several limitations, including that it's intuitive but need more solid theoretical support.",
            "So as we can see we the whole method is very intuitive but relatively simple, and it's time consuming in parameter estimation and calculating of those movers distance.",
            "By the way, calculating the smallest distance we need to solve linear programming programming.",
            "Problem.",
            "So that's that's relative.",
            "Become time consuming comparing to calculating other simple metrics such as."
        ],
        [
            "Cleaning metric.",
            "Oh, thanks for your attention.",
            "Distance when the points don't lie on the simplex.",
            "Points in the gradient space, so so you you mean the points not innocent Blacks?",
            "I was wondering is could you apply your same methodology points that just lie in Euclidean space?",
            "Yeah, that's possible.",
            "Yeah, that's possible, but one problem is that if we are not in a simple were innocent Blacks or or histograms have a total mass of 1.",
            "So so if we are not, we do not use this constraint.",
            "The smallest distance is not well defined metric.",
            "You would have to use some other metric with a similar flavor to do a movers, but but something more general for you.",
            "That's a it's a possible direction."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I sincerely thank you guys for not leaving.",
                    "label": 0
                },
                {
                    "sent": "OK, our work is an unsupervised machine learning which is a model named Dark Cloud Aggregation and we have a comparison of number of papers in this metric learning session and the metric learning session two.",
                    "label": 0
                },
                {
                    "sent": "There are seven and we are the only one that is unsupervised, so you may want to pay special attention.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is the outline of my presentation.",
                    "label": 0
                },
                {
                    "sent": "First I will give a general introduction to the metric learning problem and some and one related work on unsupervised metric learning.",
                    "label": 1
                },
                {
                    "sent": "And then I will present our model including.",
                    "label": 1
                },
                {
                    "sent": "This basic idea is setting and our approach.",
                    "label": 1
                },
                {
                    "sent": "Finally I will show some experiment results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gather with a conclusion.",
                    "label": 0
                },
                {
                    "sent": "We have seen this several times.",
                    "label": 0
                },
                {
                    "sent": "We need a metric or dissimilarity measure in a lot of tasks in machine learning, including classification, clustering, and information retrieval.",
                    "label": 1
                },
                {
                    "sent": "Uh, so basically why we have to learn a metric.",
                    "label": 1
                },
                {
                    "sent": "We already have a lot of simple ways to define a metric in a vector space, such as the Euclidean metric or LP metric.",
                    "label": 0
                },
                {
                    "sent": "Also, I think a possible explanation to this question is a simple metric such as Euclidean metric does not work well due to sophisticated underlying latent structure of the data space.",
                    "label": 1
                },
                {
                    "sent": "And these sophisticated, sophisticated structures are arising from generative process from some latent variable space, and this general to process, also known as a generative model, is sometimes very hard to describe.",
                    "label": 0
                },
                {
                    "sent": "So metric learning could be viewed as trying to find some evidence about the latent structure directly in the data space without saying anything about latent variables.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have two different settings in metric learning, in semi supervised or supervised metric learning, relative comparison information or category labels of some data points given as constraints.",
                    "label": 1
                },
                {
                    "sent": "That target metric should satisfy.",
                    "label": 1
                },
                {
                    "sent": "And in our case which is unsupervised metric learning.",
                    "label": 0
                },
                {
                    "sent": "We explore distribution patterns of unlabeled data points.",
                    "label": 0
                },
                {
                    "sent": "So from this perspective is highly related to.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear dimensionality reduction also.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Known as manifold learning.",
                    "label": 0
                },
                {
                    "sent": "Here is a well known related work on unsupervised metric learning.",
                    "label": 0
                },
                {
                    "sent": "As we know, for learning for learning the metric we first need to specify a family of candidate metrics so we can choose one metric from this family according to some criterion.",
                    "label": 0
                },
                {
                    "sent": "For for defining this candidate matrix is also of this work.",
                    "label": 0
                },
                {
                    "sent": "First defined parametric family of Maps that from.",
                    "label": 0
                },
                {
                    "sent": "For most simplex two itself on these Maps are scaling of individual of individual dimensions.",
                    "label": 1
                },
                {
                    "sent": "As you can see in this formula.",
                    "label": 0
                },
                {
                    "sent": "So given this map we can define pullback metrics of the Fisher information metric so it's parametric family of metrics parameterized by the same parameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "By the way, in this way we define a parametric family of metrics that that change the metric in the simple simplex by scaling individual dimensions.",
                    "label": 0
                },
                {
                    "sent": "In contrast, in our work, as we can see later, we define parametric family of metrics that is parameterized by affinity's, or distances between vertices of the simplex or dimensions of the vector space so that.",
                    "label": 0
                },
                {
                    "sent": "Give us two more flex.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ability.",
                    "label": 0
                },
                {
                    "sent": "Here's a basic idea of unsupervised metric learning and nonlinear dimensionality reduction.",
                    "label": 1
                },
                {
                    "sent": "That is, geodesics should go through dense data points or have a simple example to explain this.",
                    "label": 0
                },
                {
                    "sent": "First, we have a latent variable space with objects in it and this red arrow indicates the indicates our generative process, which results in data and data in the data space.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Geodesics between two points, for example A&B in the latent variable space, is approximately the straight line connecting them.",
                    "label": 0
                },
                {
                    "sent": "Becausw latent variable space often could be assumed as quasi Euclid.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And distance measure could be viewed as a transition process moving from one point to another, which means changing one object into another object.",
                    "label": 1
                },
                {
                    "sent": "The point is intermediate states on that straight line are also in the latent variable space, which means they represent meaningful objects in the same domain.",
                    "label": 0
                },
                {
                    "sent": "So this transition process.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is meaningful.",
                    "label": 0
                },
                {
                    "sent": "Usually the image of the latent variable space is a sparse subset in the data space with respect to the generative process, which is a projection.",
                    "label": 1
                },
                {
                    "sent": "Or sometimes we assume that the the image of the latent variable spaces the manifold.",
                    "label": 0
                },
                {
                    "sent": "This is be cause the latent variable space is usually low dimensional and data space usually is usually high dimensional.",
                    "label": 1
                },
                {
                    "sent": "So in the data space, using a straight line to measure distance between two points is problematic.",
                    "label": 0
                },
                {
                    "sent": "'cause was this in some intermediate?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "States because some intermediate states might have no preimage in the in the latent variable space, which means they do not represent meaningful objects.",
                    "label": 1
                },
                {
                    "sent": "And this transition.",
                    "label": 1
                },
                {
                    "sent": "So this transition process along the straight line is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Meaningless.",
                    "label": 0
                },
                {
                    "sent": "So as we know, the geodesics should go like this.",
                    "label": 1
                },
                {
                    "sent": "It's a well known result in manifold learning.",
                    "label": 0
                },
                {
                    "sent": "Such that all intermediate states have preimage in latent variable space.",
                    "label": 1
                },
                {
                    "sent": "That is a they represent many for intermediate objects.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the transition process.",
                    "label": 0
                },
                {
                    "sent": "So in other words, the real length of the blue path.",
                    "label": 1
                },
                {
                    "sent": "This is blue.",
                    "label": 0
                },
                {
                    "sent": "This is relevant, is not.",
                    "label": 0
                },
                {
                    "sent": "Of the real length of the blue path could be much longer than the red path when projected back to the latent variable space, and some points in the blue path.",
                    "label": 1
                },
                {
                    "sent": "Maybe a million lus.",
                    "label": 1
                },
                {
                    "sent": "So, however, finding the real geodesics such as red Pass in the data space is difficult in most cases.",
                    "label": 0
                },
                {
                    "sent": "Actually, in manifold learning methods we are trying to find such real geodesic.",
                    "label": 0
                },
                {
                    "sent": "However, the data are sometimes noisy and manifold assumption is sometimes violated, so funding such geodesics is hard job.",
                    "label": 1
                },
                {
                    "sent": "So in our approach we are top, we adopt more flexibel version of this basic idea, that is path going through dense data points should be relatively shorter than Pascal into sparse data points.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is a basic setting of our work.",
                    "label": 0
                },
                {
                    "sent": "We can't we consider unlabeled data points in a simplex.",
                    "label": 1
                },
                {
                    "sent": "For example normalized histograms.",
                    "label": 0
                },
                {
                    "sent": "And each vertex of the simplex corresponds to one dimension in the original vector space.",
                    "label": 1
                },
                {
                    "sent": "As you can see from this projection.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we consider a simple example of bag of words representation with forwards economy, market terenin geography.",
                    "label": 0
                },
                {
                    "sent": "So in this setting, documents can be represented as normalized histograms, which are points in the three simplex.",
                    "label": 1
                },
                {
                    "sent": "Intuitively, or we could expect many documents mention both economy and market a lot, and we also expect many documents mentioned both terenin geography alot.",
                    "label": 1
                },
                {
                    "sent": "However, a relatively few documents would mention both economy and Terran alot, so the distribution of.",
                    "label": 0
                },
                {
                    "sent": "Of data points in the simplex will be like this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Consider documents a BNC.",
                    "label": 0
                },
                {
                    "sent": "So maybe the straight line AB should be shorter than BC, because according to the basic idea we have discussed, maybe it goes through dense data points and BC it goes through sparse data points, so this should be.",
                    "label": 1
                },
                {
                    "sent": "Longer in this speech should be.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Shorter.",
                    "label": 0
                },
                {
                    "sent": "So in other words, the simplex should be warped like this.",
                    "label": 1
                },
                {
                    "sent": "Now the following problem is how to implement such warping.",
                    "label": 1
                },
                {
                    "sent": "In our approach we first in Furaffinity is between simplex vertices.",
                    "label": 1
                },
                {
                    "sent": "This is by fitting a distribution parameterized by vertex affinity's.",
                    "label": 1
                },
                {
                    "sent": "Then a metric on the simplex is induced from these infinities which is defined, defined as a smaller distance.",
                    "label": 0
                },
                {
                    "sent": "Always ground distance is derived from Vertex finished.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Explain this later.",
                    "label": 0
                },
                {
                    "sent": "As well as we know, a common commonly used distribution on the simplex is a Democrat distribution.",
                    "label": 0
                },
                {
                    "sent": "Which has an important property that expectation on on one dimension is proportional to.",
                    "label": 0
                },
                {
                    "sent": "To the parameter associated with that dimension.",
                    "label": 0
                },
                {
                    "sent": "So each parameter is associated with one vertex vertex of the simplex.",
                    "label": 1
                },
                {
                    "sent": "Here's some examples of Democratic distribution, and it shows.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "2 lines.",
                    "label": 0
                },
                {
                    "sent": "To get more flexibility, Orbu cause apparently the Democrat distribution cannot model this pattern.",
                    "label": 0
                },
                {
                    "sent": "Becauses is, 4 words are equally important.",
                    "label": 0
                },
                {
                    "sent": "So so if you if you use a direct distribution, the four parameters would be equal.",
                    "label": 0
                },
                {
                    "sent": "So that gives rise to a symmetric distribution.",
                    "label": 0
                },
                {
                    "sent": "So we have to get more flexibility.",
                    "label": 1
                },
                {
                    "sent": "We use Dirichlet mixture model with an equally weighted components.",
                    "label": 1
                },
                {
                    "sent": "And so we have also the parameters of this mixture distribution constitute metrics and we impose additional constraints on these parameters.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 1
                },
                {
                    "sent": "So the parameter matrix is a symmetric matrix.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Always with diagonal elements.",
                    "label": 0
                },
                {
                    "sent": "B1 off diagonal elements between zero and one 2C structure of the distribution with device.",
                    "label": 0
                },
                {
                    "sent": "We use a fourth dimensional example for visualization.",
                    "label": 0
                },
                {
                    "sent": "So as we can see, the diagonal elements represent affinity of.",
                    "label": 0
                },
                {
                    "sent": "Affinity of a simplex two itself, so they are one and off that no elements corresponds to edges of the simplex, which indicates finishes between vertices.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we consider toy example like this.",
                    "label": 0
                },
                {
                    "sent": "Are the parameters associated with H12 and three 4 exibits higher value than other off diagonal elements?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we sampled 1000 points from the Dirichlet aggregation distribution with these parameters.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can see that the distribution with device to use the pattern that just resembles the imaginary distribution we have discussed.",
                    "label": 1
                },
                {
                    "sent": "So this is desirable.",
                    "label": 0
                },
                {
                    "sent": "Also, suppose that these parameters are not given by hand, but are estimated from the observed distribution in the simplex.",
                    "label": 0
                },
                {
                    "sent": "So we can readily conclude that the affinity between economy and market is not .7 and the affinity between geography and Ontarian is not .6.",
                    "label": 1
                },
                {
                    "sent": "Parameter of the distribution now estimated by a very simple iterative algorithm.",
                    "label": 0
                },
                {
                    "sent": "As a hamster Easter Banam step in the step, each data point is associated is assigned with the wait with respect to each dark cloud component and then stop parameters of each direct component are RE estimated with.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Weighted data points.",
                    "label": 0
                },
                {
                    "sent": "So now we explain how to defend a metric on the simplex.",
                    "label": 1
                },
                {
                    "sent": "They are derived from these finishes.",
                    "label": 0
                },
                {
                    "sent": "The metric is defined as the smallest distance with minus log of the learned vertex affinity's as ground distances.",
                    "label": 1
                },
                {
                    "sent": "Their smallest distance between two histograms is defined as the effort needed to make them identical by transporting mass.",
                    "label": 1
                },
                {
                    "sent": "Among these histogram bins.",
                    "label": 0
                },
                {
                    "sent": "And the distance between histogram bins are called ground distance.",
                    "label": 0
                },
                {
                    "sent": "For example, if if ground distance between being one and B2 is larger than that between being one bin three, so the left histogram would be closer to the top right histogram, then to the bottom right histogram if measured by MD.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now I will show some experiment results.",
                    "label": 0
                },
                {
                    "sent": "We did experiments on the Reuters Corpus and Caltech image database.",
                    "label": 1
                },
                {
                    "sent": "In the Reuters covers, we use the topic histogram representation of documents obtained by LDA.",
                    "label": 1
                },
                {
                    "sent": "And for the image database we first extract sift descriptors, which is 128 dimensional and we cluster these descriptors into 2000 clusters and which we call visual words and we also reduce the dimensionality of this representation by LDA and we have a visual topic histogram representation of images.",
                    "label": 1
                },
                {
                    "sent": "And we have 1/3 representation which is obtained by a cluster.",
                    "label": 0
                },
                {
                    "sent": "The descriptors into 100.",
                    "label": 0
                },
                {
                    "sent": "We show worse and directly use the visual world histogram to represent image.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "We compare the metric learned by our method and another simple metrics such as L1 metric and Jeffrey Divergance in Information retrieval task and different number of topics in LDA.",
                    "label": 0
                },
                {
                    "sent": "So we see a stable is superior.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Performance on this.",
                    "label": 0
                },
                {
                    "sent": "This is a similar experiment on the Caltech image database.",
                    "label": 1
                },
                {
                    "sent": "And we see similar superior result.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a different data representation of.",
                    "label": 0
                },
                {
                    "sent": "Official word histogram.",
                    "label": 0
                },
                {
                    "sent": "So our our method also works well in this situation.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this is where we come to a conclusion of this work.",
                    "label": 0
                },
                {
                    "sent": "It has several advantages, including this unsupervised, so we can easily collect the large training set.",
                    "label": 1
                },
                {
                    "sent": "And this flexibel in that it handles correlations among dimensions.",
                    "label": 1
                },
                {
                    "sent": "And it's also a global solution because we fit a unified distribution.",
                    "label": 0
                },
                {
                    "sent": "We use a unified distribution to feed all observations.",
                    "label": 1
                },
                {
                    "sent": "And it has several limitations, including that it's intuitive but need more solid theoretical support.",
                    "label": 1
                },
                {
                    "sent": "So as we can see we the whole method is very intuitive but relatively simple, and it's time consuming in parameter estimation and calculating of those movers distance.",
                    "label": 0
                },
                {
                    "sent": "By the way, calculating the smallest distance we need to solve linear programming programming.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "So that's that's relative.",
                    "label": 0
                },
                {
                    "sent": "Become time consuming comparing to calculating other simple metrics such as.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cleaning metric.",
                    "label": 0
                },
                {
                    "sent": "Oh, thanks for your attention.",
                    "label": 1
                },
                {
                    "sent": "Distance when the points don't lie on the simplex.",
                    "label": 0
                },
                {
                    "sent": "Points in the gradient space, so so you you mean the points not innocent Blacks?",
                    "label": 0
                },
                {
                    "sent": "I was wondering is could you apply your same methodology points that just lie in Euclidean space?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's possible.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's possible, but one problem is that if we are not in a simple were innocent Blacks or or histograms have a total mass of 1.",
                    "label": 0
                },
                {
                    "sent": "So so if we are not, we do not use this constraint.",
                    "label": 0
                },
                {
                    "sent": "The smallest distance is not well defined metric.",
                    "label": 0
                },
                {
                    "sent": "You would have to use some other metric with a similar flavor to do a movers, but but something more general for you.",
                    "label": 0
                },
                {
                    "sent": "That's a it's a possible direction.",
                    "label": 0
                }
            ]
        }
    }
}