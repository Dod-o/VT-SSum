{
    "id": "puidkdagcr4pq3i52adx76iskubajjze",
    "title": "Active Learning for Reward Estimation in Inverse Reinforcement Learning",
    "info": {
        "author": [
            "Francisco S. Melo, INESC- Instituto de Engenharia de Sistemas e Computadores"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_melo_alreirl/",
    "segmentation": [
        [
            "Today I'm going to talk a little bit about some work that we've been doing on active learning for Rewards, Dimension, Universe reinforcement learning.",
            "It's a rather large title, but as you will see, it's not very complex.",
            "Situation that it's not very complex work and the main idea behind this work is that in our Institute we have been doing some work on learning by them."
        ],
        [
            "Stration there have?",
            "They they they are there the setting in which we are working on this is on is for in robotics and for the particular classes of problems that we have been working with.",
            "Learning from demonstration is a particularly intuitive way of a programmer to have a robot do some complex task without having to spend too much time with a very detailed programming.",
            "And this is useful if we want to have for example robots in.",
            "More common scenarios where the people using the robots may not be experts in programming the robots so.",
            "In in the work that I will talk to you about, we address learning from demonstration from an inverse reinforcement learning point."
        ],
        [
            "I don't know if you're familiar with the reinforcement learning paradigm you several.",
            "If you probably are basically what happens in reinforcement learning, usually is that we have an agent that interacts with the world and as part of that interaction the agent is fed with a reward that innocence encodes the test that the agent should perform.",
            "So from this information, the agent outputs a policy which is basically a way of choosing actions in the inverse reinforcement.",
            "Learning what happens is kind of the opposite, so we have an expert that demonstrates to the agent how he should should choose the actions and the purpose of the agent is to.",
            "Output a test description in the form, in this case, in particular form of the reward.",
            "There are other approaches that also deal with this problem of learning from an expert in which they are more focused, perhaps in outputing.",
            "A policy here, but in this context we are more interested in having the task descripted described as a reward because it has got several interesting features.",
            "In particular, it allows some separation between the agents and the expert.",
            "I won't go into that if you want me to talk a little bit about that weekend.",
            "Afterwards, discuss a little bit.",
            "There are, however, several difficulties in the inverse reinforcement learning scenarios that it's Neal defined problem in the sense that if the agent is."
        ],
        [
            "Is given a policy, then usually there are several rewards that make that policy optimal and the other way around also is true.",
            "So usually for one reward there are more than one policy that there are optimal.",
            "And for from the point of view of the particular problem that we are interested in addressing here, having an expert demonstrate what action the agent should do in every possible situation may be a little bit impractical, particularly in complex tasks.",
            "And So what we want to do in this particular work is to use.",
            "Active learning basically allows the agent to query the demonstrator about which situations he still needs to learn, and hopefully with that.",
            "Sorry, hopefully with that the agent will require the demonstrator to demonstrate not as many situations to somehow simplify the task of the demonstrator.",
            "OK, so."
        ],
        [
            "This is a brief outline of the talk.",
            "I'll quickly go over some background on reinforcement learning and the purpose of this background review is just to introduce notation and then I'll give an overview of our algorithm.",
            "I'll show you some results that we have some preliminary results and then.",
            "I'll conclude so."
        ],
        [
            "The basic formalism used to describe this kind of situations is that of Markov decision processes, in which the world or the situation of the agent in the world is described by a set of states, which here we will focus on finite set of States and there is also a set of actions that the agent can from which the agent can choose to interact with the world, and these actions caused the state of the world to change according to some transition probabilities.",
            "OK, so this basically.",
            "This PAXY just tells the probability of moving from state X to state why when the agent chooses a given action A and then there's this reward.",
            "Here this reward function which is I already mentioned, somehow encodes the task of the agent OK."
        ],
        [
            "So let me give you a quick example.",
            "If we have a little mouse that wants to reach the cheese and avoid the trap.",
            "The state space here could be all the possible squares in this maze.",
            "The actions would be moving up, down, left, and right, and the transition probabilities correspond to the effects somehow describe the effects of these actions.",
            "So for example, these that move the mouse against the wall keep the state and change then the others just so on.",
            "The reward could somehow describe how desirable which state is.",
            "So in this case.",
            "The trap is not very desirable.",
            "The cheese is desirable and all other states are indifferent, and then the goal of the agent in this case is of course to get cheese in the Vita trap, and this is somehow what we would expect the agent to output if we gave him this reward.",
            "Now."
        ],
        [
            "How?",
            "How do we move from having the information about the reward to actually choosing the actions?",
            "Basically we define we look for what we call a policy, which basically describes the probability of choosing an action in a given state.",
            "OK, so I'll just denote this as Pi of XNA, OK, and the goal of the agent is to choose the actions so that throughout his lifetime he gathers as much reward as possible.",
            "So this is the sum over all.",
            "Of all the rewards for all time instance and there's this discount here that just assigns more important to rewards that come sooner than those that come later.",
            "So this is all standard things and there is one policy that has the largest value for all States and that's what we actually look for, and this can be solved using dynamic programming.",
            "Now all these things that I think is not working very well.",
            "All these things to get to this point here.",
            "So this function here Q star of X and A provides for every state X rank of the actions.",
            "According to.",
            "A rank of the actions a according to how useful they are for the particular goal that the agent has.",
            "OK, So what I would like you to keep from this slide is that.",
            "If the agent has this function here, he just needs in every state to look for the action that has the maximal Q star value.",
            "OK, so in a sense this Q star function somehow summarizes all that the agent needs to know."
        ],
        [
            "OK, so in inverse reinforcement learning what we try to do is precisely the opposite.",
            "Now we don't have the reward, but we do have a policy and So what we want to do is to extract the reward, meaning try to infer what task that policy is trying to illustrate.",
            "So these are the problems that I already mentioned.",
            "There are many rewards that have that lead to the same policy and there are more than one policy.",
            "Typically for every war.",
            "So if you think about it for the reward in which all states are equal, it doesn't really matter.",
            "All policies are optimal.",
            "OK, so in particular this is a solution for all inverse reinforcement learning problems, but it's not a very interesting one.",
            "I will not go too much into this, I will just say that.",
            "One way of solving inverse reinforcement learning is somehow to invert the Bellman equation, which was that relation that appeared with that Q star function in the previous slide.",
            "Is it more or less clear so far what?",
            "What all these things do?",
            "OK, so.",
            "What did the way that we address in verse 3 forcement learn?"
        ],
        [
            "Here is a is more of A is a probabilistic way that has been proposed two years ago in each guy and basically it considers that the demonstration appears as a data set.",
            "Basically where we have pairs of States and actions and these pairs basically correspond.",
            "To the actions that the agents should do in each of the states.",
            "And in this probabilistic view, we do not assume that these pairs are generated from the exactly optimal policy, but we admit that the demonstrator is not perfect.",
            "Sometimes he can make a mistake, and in particular we assume that those mistakes have this nice form in which that basically translates that.",
            "Actions that are better are more likely to be demonstrated than actions that are worse.",
            "OK, so.",
            "Even if the demonstrator makes a mistake, it's not a completely random mistake.",
            "It's kind of a structured mistake.",
            "And this Q function here that appears here is that optimal Q star function that I mentioned a couple of slides ago for the reward that the demonstrator is trying to optimize and that the agent is trying to learn or trying to infer.",
            "OK, is this clear to everyone?",
            "So basically now if we have a demonstration, we can think of this as giving the likelihood of a pair, and so the likely."
        ],
        [
            "We have demonstration is just.",
            "The product of all these individual likelihoods.",
            "And.",
            "Let me make a cure.",
            "A side note that if we, for example, if we depart from this likelihood and we just want to maximum likelihood solution for this, we can choose just use for example a gradient approach to this.",
            "OK, there are some little issues with computing this gradient, but general is not too complicated.",
            "OK, so enough."
        ],
        [
            "The background.",
            "So now what we want to do is to actually have the agent ask demonstrator for the right action in whatever states may be more useful.",
            "So we need to come up with a good way of of figuring out how useful it is for the agent to see the action in a given state.",
            "OK, and it's important here to to.",
            "To to call your attention for something that so far if we just find the problem like this.",
            "This is like any active learning setting, so we basically we have a function which in this case is a policy that we want to to learn and we just want to use the instances that are more more useful.",
            "The interesting thing about this problem is that.",
            "The demonstration of an action in a given state does not only give information about that state, but it also gives information about other states becausw.",
            "Of the underlying structure of the Markov decision process OK?",
            "So.",
            "In particular, even if we have samples of a demonstration, since that demonstrate sorry if.",
            "Even if we are given samples of a policy, that policy is translated in terms of reward in a nonlinear way by by means of that distribution that I showed you, and by means of the relation between the Q star function and the reward function, and so when the agent observes an action in a given state, it has to propagate that information to the reward level and not just focus it on the policy level.",
            "And this is the main difference between a standard active learning setting and.",
            "Active learning this particular scenario.",
            "Every other things works more or less the same.",
            "So now we have some uncertainty in terms of the reward and then what we have to do is to propagate that uncertainty back for for the policy.",
            "OK.",
            "So general algorithm that we propose is pretty step."
        ],
        [
            "Did we have we estimate?",
            "A distributor distribution over policy is given the demonstration using some kind of Monte Carlo for example.",
            "Then we compute this H of X that I'll I'll explain in a minute and then just choose the actions that sorry.",
            "Choose the states that maximize this H. So in a sense, this edge function somehow measures the uncertainty in the in any given state X that the agent has with respect to the task that the demonstrator is trying to demonstrate.",
            "OK. And we have this new sample.",
            "The demonstrator then demonstrates an action for this state and we have this sample to the to our data set OK.",
            "So let me just explain how we compute this age function.",
            "So if we have a positive distribution over evil."
        ],
        [
            "Let's give him the demonstration.",
            "We can use this too.",
            "To estimate the distribution over policy here, did a distribution over policy where this capital \u03c0 means the set of all possible policy OK and then?",
            "For every state action pair, what we have is.",
            "Some uncertainty on the.",
            "The probability of choosing each of the actions.",
            "OK, so it's like for any state we have some uncertainty regarding the exact value of this probability.",
            "OK, so if you think of this barplot as.",
            "The policy at one given State X, this bar being the probability of choosing a one this choosing A2 and so on so forth.",
            "And these little plots here.",
            "R represents somehow the distribution of this particular value, and so on for all of these.",
            "So we can measure the uncertainty in these small distributions and then average over all the actions and this is what we call the per state entropy OK?",
            "So we average over all the actions.",
            "The entropy of this distributions of these little distributions here OK.",
            "This clear.",
            "OK. And then basically we have the criterion that we need.",
            "So if we think about these little distributions as.",
            "Of the if you think about this, entropy of these little distributions as measuring the uncertainty that the agent has with respect to the probability of choosing a particular action in a particular state, then this just this H of X just measures the overall uncertainty in State X. OK, so let me move on.",
            "To the results."
        ],
        [
            "So these are results in very simple scenarios that we wanted to have a better understanding on how reasonable this is.",
            "And the 1st result that we have is a very simple scenario where an agent moves in the real line between minus one and one we have discretized this into 21 intervals.",
            "I think of one of one each an in each state.",
            "The agent has two actions moving right or moving left OK and this just moves the agent to the corresponding next interval in the real line and the reward function that we have is just a function.",
            "Afetr wanna see to do Sir, there's a square missing here.",
            "The square it, so it's a square function of X and we have these two parameters, Theta one and Theta two.",
            "Basically what it's important to keep in mind here is that this data two determines the point where there is a maximum or minimum depending on the sign of data.",
            "One in particular we tested it with feta 1 -- 1 and three to two point 15.",
            "So basically this is just.",
            "2nd order function with a maximum in point feed 15 and we have demonstrated the actions.",
            "On the extreme state, so in minus 1 -- .9 -- .8 point 8.9 and one.",
            "And if you if you look at this, this immediately tells you if what the agent is looking is for a maximum in the borders, which would mean that the function would be something.",
            "With a positive fetal one, or if it's a maximum in the somewhere in the interval, which would correspond to a negative fetal one?",
            "OK, so let me."
        ],
        [
            "Just show you some results, so these are a bunch of samples from the distribution.",
            "What it's important to notice is that.",
            "In the first iteration, the agent knows that in so this one corresponds to the action move right and zero corresponds to the action move left.",
            "And in this.",
            "In this first iteration, he already knows with probability one that he should move right on these three States and left on these three states, but he's uncertain as to what happens in the middle, and this can be seen from this messy graph, because these are all possible reward functions, and if you notice, the maximum of these rewards can be anywhere here because you have samples anywhere, and what the algorithm does, it first chooses a sample in zero.",
            "And then it's sample in 0, three and then a sample in 01 and then a sample is 02 and finally after five iterations we have this thing here which basically tells that the maximum is somewhere in here.",
            "And that's exactly the truth.",
            "And if you notice all these functions have the maximum more or less in the same place, but the scale factor which corresponds to the exact value of Theta one.",
            "The agent cannot figure that out because it doesn't change the policy.",
            "Now the interesting thing that I want to call your attention to is that what the algorithm is doing here is basically doing it by section.",
            "OK, so it first starts in the middle of the interval, and then it's again divides in middle and then in middle and then in middle.",
            "And this is clearly better than just doing some random sampling.",
            "OK, it's a very simple example, but it illustrates well how method work and this error bars somehow give you the uncertainty in the policy.",
            "Now to some."
        ],
        [
            "Other examples we have here are slightly more complicated, where the agent needs to navigate in discontinuous world.",
            "There's a reward area and the penalty area, but the agent doesn't know exactly where they are.",
            "And this is the function that he should be trying to optimize, and the parameters here are just the the.",
            "The coordinates of where each of these two areas are of the center.",
            "Basically, this is what happens."
        ],
        [
            "After a good initialization, the agent is able to guess that the goal area is somewhere in that corner and that the.",
            "That area is somewhere over there and as it gets more samples, it is finally able to pinpoint quite accurately.",
            "Where where the two areas are.",
            "And finally we also tried with the general grid.",
            "First we used a parameterized reward where basically the agent just was just trying."
        ],
        [
            "Look for a goal state, and in this case we got very good."
        ],
        [
            "Results if we use a general reward function.",
            "Basically can be any vector, then basically the algorithm behaves more or less like random.",
            "And."
        ],
        [
            "So from this, what we concluded is that.",
            "This approach appears to experimentally indicate that it has interesting results.",
            "It seems, at least, to never be worse than random.",
            "However, from this these preliminary results we still haven't got a clear notion of in which situations may or may not be advantage to use this active sampling, and so we are currently trying to get more theoretical understanding of the properties of this algorithm.",
            "Thank you for."
        ],
        [
            "Your attention, I'll, I'll take your questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Today I'm going to talk a little bit about some work that we've been doing on active learning for Rewards, Dimension, Universe reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "It's a rather large title, but as you will see, it's not very complex.",
                    "label": 0
                },
                {
                    "sent": "Situation that it's not very complex work and the main idea behind this work is that in our Institute we have been doing some work on learning by them.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stration there have?",
                    "label": 0
                },
                {
                    "sent": "They they they are there the setting in which we are working on this is on is for in robotics and for the particular classes of problems that we have been working with.",
                    "label": 0
                },
                {
                    "sent": "Learning from demonstration is a particularly intuitive way of a programmer to have a robot do some complex task without having to spend too much time with a very detailed programming.",
                    "label": 0
                },
                {
                    "sent": "And this is useful if we want to have for example robots in.",
                    "label": 0
                },
                {
                    "sent": "More common scenarios where the people using the robots may not be experts in programming the robots so.",
                    "label": 0
                },
                {
                    "sent": "In in the work that I will talk to you about, we address learning from demonstration from an inverse reinforcement learning point.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I don't know if you're familiar with the reinforcement learning paradigm you several.",
                    "label": 1
                },
                {
                    "sent": "If you probably are basically what happens in reinforcement learning, usually is that we have an agent that interacts with the world and as part of that interaction the agent is fed with a reward that innocence encodes the test that the agent should perform.",
                    "label": 0
                },
                {
                    "sent": "So from this information, the agent outputs a policy which is basically a way of choosing actions in the inverse reinforcement.",
                    "label": 1
                },
                {
                    "sent": "Learning what happens is kind of the opposite, so we have an expert that demonstrates to the agent how he should should choose the actions and the purpose of the agent is to.",
                    "label": 0
                },
                {
                    "sent": "Output a test description in the form, in this case, in particular form of the reward.",
                    "label": 0
                },
                {
                    "sent": "There are other approaches that also deal with this problem of learning from an expert in which they are more focused, perhaps in outputing.",
                    "label": 0
                },
                {
                    "sent": "A policy here, but in this context we are more interested in having the task descripted described as a reward because it has got several interesting features.",
                    "label": 0
                },
                {
                    "sent": "In particular, it allows some separation between the agents and the expert.",
                    "label": 0
                },
                {
                    "sent": "I won't go into that if you want me to talk a little bit about that weekend.",
                    "label": 0
                },
                {
                    "sent": "Afterwards, discuss a little bit.",
                    "label": 1
                },
                {
                    "sent": "There are, however, several difficulties in the inverse reinforcement learning scenarios that it's Neal defined problem in the sense that if the agent is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is given a policy, then usually there are several rewards that make that policy optimal and the other way around also is true.",
                    "label": 0
                },
                {
                    "sent": "So usually for one reward there are more than one policy that there are optimal.",
                    "label": 1
                },
                {
                    "sent": "And for from the point of view of the particular problem that we are interested in addressing here, having an expert demonstrate what action the agent should do in every possible situation may be a little bit impractical, particularly in complex tasks.",
                    "label": 0
                },
                {
                    "sent": "And So what we want to do in this particular work is to use.",
                    "label": 0
                },
                {
                    "sent": "Active learning basically allows the agent to query the demonstrator about which situations he still needs to learn, and hopefully with that.",
                    "label": 0
                },
                {
                    "sent": "Sorry, hopefully with that the agent will require the demonstrator to demonstrate not as many situations to somehow simplify the task of the demonstrator.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a brief outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'll quickly go over some background on reinforcement learning and the purpose of this background review is just to introduce notation and then I'll give an overview of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "I'll show you some results that we have some preliminary results and then.",
                    "label": 0
                },
                {
                    "sent": "I'll conclude so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic formalism used to describe this kind of situations is that of Markov decision processes, in which the world or the situation of the agent in the world is described by a set of states, which here we will focus on finite set of States and there is also a set of actions that the agent can from which the agent can choose to interact with the world, and these actions caused the state of the world to change according to some transition probabilities.",
                    "label": 1
                },
                {
                    "sent": "OK, so this basically.",
                    "label": 0
                },
                {
                    "sent": "This PAXY just tells the probability of moving from state X to state why when the agent chooses a given action A and then there's this reward.",
                    "label": 1
                },
                {
                    "sent": "Here this reward function which is I already mentioned, somehow encodes the task of the agent OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me give you a quick example.",
                    "label": 0
                },
                {
                    "sent": "If we have a little mouse that wants to reach the cheese and avoid the trap.",
                    "label": 1
                },
                {
                    "sent": "The state space here could be all the possible squares in this maze.",
                    "label": 1
                },
                {
                    "sent": "The actions would be moving up, down, left, and right, and the transition probabilities correspond to the effects somehow describe the effects of these actions.",
                    "label": 0
                },
                {
                    "sent": "So for example, these that move the mouse against the wall keep the state and change then the others just so on.",
                    "label": 0
                },
                {
                    "sent": "The reward could somehow describe how desirable which state is.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "The trap is not very desirable.",
                    "label": 0
                },
                {
                    "sent": "The cheese is desirable and all other states are indifferent, and then the goal of the agent in this case is of course to get cheese in the Vita trap, and this is somehow what we would expect the agent to output if we gave him this reward.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "How do we move from having the information about the reward to actually choosing the actions?",
                    "label": 0
                },
                {
                    "sent": "Basically we define we look for what we call a policy, which basically describes the probability of choosing an action in a given state.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll just denote this as Pi of XNA, OK, and the goal of the agent is to choose the actions so that throughout his lifetime he gathers as much reward as possible.",
                    "label": 1
                },
                {
                    "sent": "So this is the sum over all.",
                    "label": 0
                },
                {
                    "sent": "Of all the rewards for all time instance and there's this discount here that just assigns more important to rewards that come sooner than those that come later.",
                    "label": 1
                },
                {
                    "sent": "So this is all standard things and there is one policy that has the largest value for all States and that's what we actually look for, and this can be solved using dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "Now all these things that I think is not working very well.",
                    "label": 0
                },
                {
                    "sent": "All these things to get to this point here.",
                    "label": 0
                },
                {
                    "sent": "So this function here Q star of X and A provides for every state X rank of the actions.",
                    "label": 1
                },
                {
                    "sent": "According to.",
                    "label": 0
                },
                {
                    "sent": "A rank of the actions a according to how useful they are for the particular goal that the agent has.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I would like you to keep from this slide is that.",
                    "label": 0
                },
                {
                    "sent": "If the agent has this function here, he just needs in every state to look for the action that has the maximal Q star value.",
                    "label": 0
                },
                {
                    "sent": "OK, so in a sense this Q star function somehow summarizes all that the agent needs to know.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in inverse reinforcement learning what we try to do is precisely the opposite.",
                    "label": 1
                },
                {
                    "sent": "Now we don't have the reward, but we do have a policy and So what we want to do is to extract the reward, meaning try to infer what task that policy is trying to illustrate.",
                    "label": 0
                },
                {
                    "sent": "So these are the problems that I already mentioned.",
                    "label": 0
                },
                {
                    "sent": "There are many rewards that have that lead to the same policy and there are more than one policy.",
                    "label": 1
                },
                {
                    "sent": "Typically for every war.",
                    "label": 0
                },
                {
                    "sent": "So if you think about it for the reward in which all states are equal, it doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "All policies are optimal.",
                    "label": 0
                },
                {
                    "sent": "OK, so in particular this is a solution for all inverse reinforcement learning problems, but it's not a very interesting one.",
                    "label": 0
                },
                {
                    "sent": "I will not go too much into this, I will just say that.",
                    "label": 1
                },
                {
                    "sent": "One way of solving inverse reinforcement learning is somehow to invert the Bellman equation, which was that relation that appeared with that Q star function in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "Is it more or less clear so far what?",
                    "label": 0
                },
                {
                    "sent": "What all these things do?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What did the way that we address in verse 3 forcement learn?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a is more of A is a probabilistic way that has been proposed two years ago in each guy and basically it considers that the demonstration appears as a data set.",
                    "label": 0
                },
                {
                    "sent": "Basically where we have pairs of States and actions and these pairs basically correspond.",
                    "label": 0
                },
                {
                    "sent": "To the actions that the agents should do in each of the states.",
                    "label": 0
                },
                {
                    "sent": "And in this probabilistic view, we do not assume that these pairs are generated from the exactly optimal policy, but we admit that the demonstrator is not perfect.",
                    "label": 1
                },
                {
                    "sent": "Sometimes he can make a mistake, and in particular we assume that those mistakes have this nice form in which that basically translates that.",
                    "label": 0
                },
                {
                    "sent": "Actions that are better are more likely to be demonstrated than actions that are worse.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Even if the demonstrator makes a mistake, it's not a completely random mistake.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a structured mistake.",
                    "label": 0
                },
                {
                    "sent": "And this Q function here that appears here is that optimal Q star function that I mentioned a couple of slides ago for the reward that the demonstrator is trying to optimize and that the agent is trying to learn or trying to infer.",
                    "label": 0
                },
                {
                    "sent": "OK, is this clear to everyone?",
                    "label": 1
                },
                {
                    "sent": "So basically now if we have a demonstration, we can think of this as giving the likelihood of a pair, and so the likely.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have demonstration is just.",
                    "label": 0
                },
                {
                    "sent": "The product of all these individual likelihoods.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Let me make a cure.",
                    "label": 0
                },
                {
                    "sent": "A side note that if we, for example, if we depart from this likelihood and we just want to maximum likelihood solution for this, we can choose just use for example a gradient approach to this.",
                    "label": 1
                },
                {
                    "sent": "OK, there are some little issues with computing this gradient, but general is not too complicated.",
                    "label": 0
                },
                {
                    "sent": "OK, so enough.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The background.",
                    "label": 0
                },
                {
                    "sent": "So now what we want to do is to actually have the agent ask demonstrator for the right action in whatever states may be more useful.",
                    "label": 0
                },
                {
                    "sent": "So we need to come up with a good way of of figuring out how useful it is for the agent to see the action in a given state.",
                    "label": 0
                },
                {
                    "sent": "OK, and it's important here to to.",
                    "label": 0
                },
                {
                    "sent": "To to call your attention for something that so far if we just find the problem like this.",
                    "label": 0
                },
                {
                    "sent": "This is like any active learning setting, so we basically we have a function which in this case is a policy that we want to to learn and we just want to use the instances that are more more useful.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing about this problem is that.",
                    "label": 0
                },
                {
                    "sent": "The demonstration of an action in a given state does not only give information about that state, but it also gives information about other states becausw.",
                    "label": 0
                },
                {
                    "sent": "Of the underlying structure of the Markov decision process OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In particular, even if we have samples of a demonstration, since that demonstrate sorry if.",
                    "label": 0
                },
                {
                    "sent": "Even if we are given samples of a policy, that policy is translated in terms of reward in a nonlinear way by by means of that distribution that I showed you, and by means of the relation between the Q star function and the reward function, and so when the agent observes an action in a given state, it has to propagate that information to the reward level and not just focus it on the policy level.",
                    "label": 0
                },
                {
                    "sent": "And this is the main difference between a standard active learning setting and.",
                    "label": 0
                },
                {
                    "sent": "Active learning this particular scenario.",
                    "label": 1
                },
                {
                    "sent": "Every other things works more or less the same.",
                    "label": 0
                },
                {
                    "sent": "So now we have some uncertainty in terms of the reward and then what we have to do is to propagate that uncertainty back for for the policy.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So general algorithm that we propose is pretty step.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did we have we estimate?",
                    "label": 0
                },
                {
                    "sent": "A distributor distribution over policy is given the demonstration using some kind of Monte Carlo for example.",
                    "label": 0
                },
                {
                    "sent": "Then we compute this H of X that I'll I'll explain in a minute and then just choose the actions that sorry.",
                    "label": 0
                },
                {
                    "sent": "Choose the states that maximize this H. So in a sense, this edge function somehow measures the uncertainty in the in any given state X that the agent has with respect to the task that the demonstrator is trying to demonstrate.",
                    "label": 0
                },
                {
                    "sent": "OK. And we have this new sample.",
                    "label": 1
                },
                {
                    "sent": "The demonstrator then demonstrates an action for this state and we have this sample to the to our data set OK.",
                    "label": 1
                },
                {
                    "sent": "So let me just explain how we compute this age function.",
                    "label": 0
                },
                {
                    "sent": "So if we have a positive distribution over evil.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's give him the demonstration.",
                    "label": 0
                },
                {
                    "sent": "We can use this too.",
                    "label": 0
                },
                {
                    "sent": "To estimate the distribution over policy here, did a distribution over policy where this capital \u03c0 means the set of all possible policy OK and then?",
                    "label": 0
                },
                {
                    "sent": "For every state action pair, what we have is.",
                    "label": 0
                },
                {
                    "sent": "Some uncertainty on the.",
                    "label": 0
                },
                {
                    "sent": "The probability of choosing each of the actions.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's like for any state we have some uncertainty regarding the exact value of this probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you think of this barplot as.",
                    "label": 0
                },
                {
                    "sent": "The policy at one given State X, this bar being the probability of choosing a one this choosing A2 and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "And these little plots here.",
                    "label": 0
                },
                {
                    "sent": "R represents somehow the distribution of this particular value, and so on for all of these.",
                    "label": 0
                },
                {
                    "sent": "So we can measure the uncertainty in these small distributions and then average over all the actions and this is what we call the per state entropy OK?",
                    "label": 0
                },
                {
                    "sent": "So we average over all the actions.",
                    "label": 0
                },
                {
                    "sent": "The entropy of this distributions of these little distributions here OK.",
                    "label": 0
                },
                {
                    "sent": "This clear.",
                    "label": 0
                },
                {
                    "sent": "OK. And then basically we have the criterion that we need.",
                    "label": 0
                },
                {
                    "sent": "So if we think about these little distributions as.",
                    "label": 0
                },
                {
                    "sent": "Of the if you think about this, entropy of these little distributions as measuring the uncertainty that the agent has with respect to the probability of choosing a particular action in a particular state, then this just this H of X just measures the overall uncertainty in State X. OK, so let me move on.",
                    "label": 0
                },
                {
                    "sent": "To the results.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are results in very simple scenarios that we wanted to have a better understanding on how reasonable this is.",
                    "label": 0
                },
                {
                    "sent": "And the 1st result that we have is a very simple scenario where an agent moves in the real line between minus one and one we have discretized this into 21 intervals.",
                    "label": 0
                },
                {
                    "sent": "I think of one of one each an in each state.",
                    "label": 0
                },
                {
                    "sent": "The agent has two actions moving right or moving left OK and this just moves the agent to the corresponding next interval in the real line and the reward function that we have is just a function.",
                    "label": 1
                },
                {
                    "sent": "Afetr wanna see to do Sir, there's a square missing here.",
                    "label": 0
                },
                {
                    "sent": "The square it, so it's a square function of X and we have these two parameters, Theta one and Theta two.",
                    "label": 0
                },
                {
                    "sent": "Basically what it's important to keep in mind here is that this data two determines the point where there is a maximum or minimum depending on the sign of data.",
                    "label": 0
                },
                {
                    "sent": "One in particular we tested it with feta 1 -- 1 and three to two point 15.",
                    "label": 0
                },
                {
                    "sent": "So basically this is just.",
                    "label": 0
                },
                {
                    "sent": "2nd order function with a maximum in point feed 15 and we have demonstrated the actions.",
                    "label": 0
                },
                {
                    "sent": "On the extreme state, so in minus 1 -- .9 -- .8 point 8.9 and one.",
                    "label": 0
                },
                {
                    "sent": "And if you if you look at this, this immediately tells you if what the agent is looking is for a maximum in the borders, which would mean that the function would be something.",
                    "label": 0
                },
                {
                    "sent": "With a positive fetal one, or if it's a maximum in the somewhere in the interval, which would correspond to a negative fetal one?",
                    "label": 0
                },
                {
                    "sent": "OK, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just show you some results, so these are a bunch of samples from the distribution.",
                    "label": 0
                },
                {
                    "sent": "What it's important to notice is that.",
                    "label": 0
                },
                {
                    "sent": "In the first iteration, the agent knows that in so this one corresponds to the action move right and zero corresponds to the action move left.",
                    "label": 0
                },
                {
                    "sent": "And in this.",
                    "label": 0
                },
                {
                    "sent": "In this first iteration, he already knows with probability one that he should move right on these three States and left on these three states, but he's uncertain as to what happens in the middle, and this can be seen from this messy graph, because these are all possible reward functions, and if you notice, the maximum of these rewards can be anywhere here because you have samples anywhere, and what the algorithm does, it first chooses a sample in zero.",
                    "label": 0
                },
                {
                    "sent": "And then it's sample in 0, three and then a sample in 01 and then a sample is 02 and finally after five iterations we have this thing here which basically tells that the maximum is somewhere in here.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly the truth.",
                    "label": 0
                },
                {
                    "sent": "And if you notice all these functions have the maximum more or less in the same place, but the scale factor which corresponds to the exact value of Theta one.",
                    "label": 0
                },
                {
                    "sent": "The agent cannot figure that out because it doesn't change the policy.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing that I want to call your attention to is that what the algorithm is doing here is basically doing it by section.",
                    "label": 0
                },
                {
                    "sent": "OK, so it first starts in the middle of the interval, and then it's again divides in middle and then in middle and then in middle.",
                    "label": 0
                },
                {
                    "sent": "And this is clearly better than just doing some random sampling.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a very simple example, but it illustrates well how method work and this error bars somehow give you the uncertainty in the policy.",
                    "label": 0
                },
                {
                    "sent": "Now to some.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other examples we have here are slightly more complicated, where the agent needs to navigate in discontinuous world.",
                    "label": 0
                },
                {
                    "sent": "There's a reward area and the penalty area, but the agent doesn't know exactly where they are.",
                    "label": 0
                },
                {
                    "sent": "And this is the function that he should be trying to optimize, and the parameters here are just the the.",
                    "label": 0
                },
                {
                    "sent": "The coordinates of where each of these two areas are of the center.",
                    "label": 0
                },
                {
                    "sent": "Basically, this is what happens.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After a good initialization, the agent is able to guess that the goal area is somewhere in that corner and that the.",
                    "label": 0
                },
                {
                    "sent": "That area is somewhere over there and as it gets more samples, it is finally able to pinpoint quite accurately.",
                    "label": 0
                },
                {
                    "sent": "Where where the two areas are.",
                    "label": 0
                },
                {
                    "sent": "And finally we also tried with the general grid.",
                    "label": 0
                },
                {
                    "sent": "First we used a parameterized reward where basically the agent just was just trying.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look for a goal state, and in this case we got very good.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results if we use a general reward function.",
                    "label": 0
                },
                {
                    "sent": "Basically can be any vector, then basically the algorithm behaves more or less like random.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So from this, what we concluded is that.",
                    "label": 0
                },
                {
                    "sent": "This approach appears to experimentally indicate that it has interesting results.",
                    "label": 1
                },
                {
                    "sent": "It seems, at least, to never be worse than random.",
                    "label": 1
                },
                {
                    "sent": "However, from this these preliminary results we still haven't got a clear notion of in which situations may or may not be advantage to use this active sampling, and so we are currently trying to get more theoretical understanding of the properties of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Thank you for.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your attention, I'll, I'll take your questions.",
                    "label": 0
                }
            ]
        }
    }
}