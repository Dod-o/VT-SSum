{
    "id": "uvgolqhepsucuwndfkfo4wivo6wdlwbp",
    "title": "Introduction to Machine Learning",
    "info": {
        "author": [
            "Pascal Vincent, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_vincent_machine_learning/",
    "segmentation": [
        [
            "So I'm glad to be here with you this morning.",
            "I'm going to do something that some of you might find a little boring.",
            "If you've already been exposed to that material, that's an introduction to machine learning, so machine learning is quite broad.",
            "I try to focus on what I feel is really essential.",
            "The core that kind of binds all of this together, maybe beyond the specific algorithms, so I won't go into much details into specific algorithms.",
            "But I try to cover more principles.",
            "Can I do that, then wait to give you a more of an intuitive feeling than an informal, very formal mathematical presentation of those concepts?",
            "Although there be a couple of formulas but?",
            "OK, so let me start with the."
        ],
        [
            "Very, very brief historical perspective on what machine learning is so historically.",
            "Machine Learning was born from that very ambitious goal of creating an artificial intelligence.",
            "At that time, that was called research on that was called cybernetics, I think, so some of you might have notice time already.",
            "Half Cyborg that's happened like 3.",
            "Few weeks ago, and apparently I still have to do that for another three weeks with a bad experiment on Montreal pavements.",
            "But that's about it.",
            "The founding projects that's often thought of being the founder for this field is Frank Rosenblatt, who invented an algorithm with Perceptron in 1957.",
            "That was the first artificial neuron that learned from examples.",
            "Cape and historically, there's been two opposed approaches to artificial intelligence.",
            "One is a neuroscience is inspired.",
            "And that's what we're talking about here, where we have these neural Nets that learning from examples used for artificial artificial perception and the other is what we call classical symbolic artificial intelligence, which primary focus was on the logical reasoning capabilities.",
            "So what characterized that is that there was no learning.",
            "Initially, it was mostly humans coding rules, and it had a poor handling of uncertainty.",
            "So now overtime this kind of got fixed from that field.",
            "We can trace back the event event of a Bayes net which are able probabilistic forms of reasoning and that got merged into.",
            "Basically what we now call machine learning.",
            "But what we can say from today is that learning approve astic models and pricing models, largely one those kind of battle and machine learning has now is not pervasive in all.",
            "Walks almost walks of artificial intentions, let's say."
        ],
        [
            "OK.",
            "The view in the artificial intelligence in the 60s.",
            "We could summarize it like this.",
            "There was a computer science where artificial intelligence, which was largely a symbolic and neuroscience.",
            "Inspired by the brain and at the interface between the two, we had artificial neural networks.",
            "If I was to draw my current view of the founding disciplines of what is machine learning, you still have these two now in the middle.",
            "This has a little bit split between artificial neural networks, which are more on the artificial intelligence side and computational neuroscience, which use try to simulate more of the real networks, but."
        ],
        [
            "Now we found that a lot of what's been discovered had already been formalized in a nice mathematical way in statistics, and there's also been a lot of influence from physics, and most specifically at the interface of statistical physics.",
            "That's why you sometimes hear about energy functions and.",
            "Boltzmann machines and these things from statistical physics.",
            "Related to physics and information theory and optimization control.",
            "An at the interface of all this you have this field.",
            "That's machine learning.",
            "So that's a very exciting field, right?",
            "It has all these influences.",
            "I am."
        ],
        [
            "OK, that's my partial view.",
            "Now I'm going to try to answer what is machine learning up from her house or historical perspective.",
            "Other kind of a little bit of crazy users.",
            "Professor perspective.",
            "OK, and here I'll take on their user.",
            "SPECT Evanthe perspective, and they had hypnotized users perspective so.",
            "What I say is machine learning is really a scientific field, but really it's witchcraft.",
            "Believe me, it's witchcraft.",
            "It researches fundamental principles of what they really do is potions, and it develops magical algorithm.",
            "Believe me, it spells to invoke capable of leveraging collected data to automatically produce accurate predictive functions applicable to similar data in the future.",
            "Wow.",
            "OK, so that's a crazy.",
            "The user specific perspective you're happy with that."
        ],
        [
            "So from that the question given this, what is the key ingredient to machine learning?",
            "Going to you.",
            "Oh, I heard it.",
            "Thank you data.",
            "Yes, that's what gets into the cauldron.",
            "The magical cauldron, right?",
            "And data is collected from nature or from industrial processes.",
            "It comes in many forms and many formats.",
            "Sometimes clean, if you're lucky, sometimes structured, unstructured, often messy, messy.",
            "An in machine learning we like to view that data as a list of examples, and if it's not like that, trust us, we'll turn it into one.",
            "Ideally, many examples of the same source and preferably with each example, a vector of numbers and again will first turn it into one if it isn't already."
        ],
        [
            "Hey.",
            "So here's what we work with.",
            "We work with the training data, so training data.",
            "It is made of several things.",
            "A list of examples here we have N small and the number of examples.",
            "And each example here.",
            "That's a case of supervised learning.",
            "I'll get back to that is a pair of input.",
            "And target input is what we observe here.",
            "It's really visual images and target is what we'd like to predict, right?",
            "Remember, we're building this crystal ball automatically automatically.",
            "Same for a cat, same force.",
            "OK, and that's our training data set, OK call IDN.",
            "Here, because it has an examples.",
            "So we like we first turn it into a nice datamatrix.",
            "OK, that's the role of preprocessing or feature extraction, where will represent each row will now be vector, so there's part of that vector that corresponds to the input image.",
            "An another little part here that corresponds to the target that want to predict.",
            "Here is a two class problems classification problem, horse versus cat.",
            "So we have plus one through to note the class of horse and minus one to denote the class of cat.",
            "OK, so now it's a nice little matrix.",
            "And the goal.",
            "The goal of machine learning.",
            "Is to learn from that, or rather from that, 'cause that's what the computer gets to see.",
            "Such in such a way that when you get shown a new test point that you've never seen never part of your training set initially.",
            "You can predict the right class.",
            "So how is this?",
            "Use the test time you take that represented as a vector and hopefully you've learned a function here called F. Theta F is for function.",
            "Theater is usually for its per meters, 'cause it's parameterized function.",
            "That is going to map an input vector to hear a class at Target, so there's a dimension that important here.",
            "An is what I call the number examples, and D is the input dimensionality.",
            "That's the number of dimensions of your input vector.",
            "OK."
        ],
        [
            "Alright, so these are very important dimension ladies.",
            "Number of examples.",
            "Sometimes you have several millions input dimensionality the between 100,000, sometimes more and the target dimensionality, here here for classification classes.",
            "It's the number of classes that's relevant.",
            "So data will often be in a matrix and times D + 1 or end times 2 plus.",
            "OK."
        ],
        [
            "That's it.",
            "OK, now turning data into a nice list of examples, turning messy data into a nice list of examples.",
            "That's more my practical side slide, so if you're ever in the situation where you have this wonderful Lee Overengineered database that you need to extract some data to learn something on OK, and it looks a little bit like that, so it's kind of a hopeless situation, really, that's the realm of data plumbing.",
            "So you can have your favorite date of lumbar.",
            "There's a name for that.",
            "Is Dayton plumbing OK?",
            "An turn it into a nice little list of examples like that.",
            "So if you're ever in a situation to where you have to do that.",
            "Try to get someone else to do it.",
            "But if you can't, or if you have to guide that person, there's two questions that you should really be asking yourself, because it's really not trivial to turn this into that.",
            "OK, the key question that you need to ask yourself is to decide what exactly what an example should be.",
            "And there's a question regarding the input, and the input is ask yourself what is all the potentially relevant information that you will have at your disposal about the case when you will have to make a prediction about it.",
            "At Test time for the future cases, now you have a test case.",
            "What is all the information that's available at that time?",
            "That should be your inputs over your examples and for target the important question target is what you want to predict is can you get your hands on many such examples that are actually labeled with prediction targets?",
            "Always the case?"
        ],
        [
            "OK, now the second part, so that's how to turn our thing into a list of examples.",
            "The second part is turning an example into a vector.",
            "OK, so there we can use what we call raw input.",
            "Representation is an example of a of a bitmap image of a digit, so it's a list of grayscale values, light or light values between zero and 2:55, so it's naturally a vector.",
            "It's almost probably already like that.",
            "Or something close to that when it's stored in the computer.",
            "Similarly here for sound file sound file, you can extract the magnitude here at every time step.",
            "Or we can use some preprocessor presentation.",
            "Here's an example of of a spectrogram of that sound file, so that gives you another vector.",
            "Here's an example for how we could represent a small sentence the cat jumped.",
            "In what we call a bag of words or presentation.",
            "So bag of water presentation is you take that whatever text and make it into a huge vector.",
            "That's the size of the vocabulary that's filled with zeros but has once at the places where there was such a word in the sentence for the cat.",
            "Or you can use hand engineered features or handcrafted features, in which case you have some way to extract some features that you think are relevant using some prior knowledge and you build your factor vector as a list of such features.",
            "OK. Site.",
            "So far so good."
        ],
        [
            "So far so good.",
            "So here's this view.",
            "We have this data set.",
            "That we now have into a nice matrix.",
            "OK, an examples.",
            "Each input has D dimension.",
            "Here in this example we have 5 dimensions, so each X is 5 components.",
            "Target the label Y.",
            "Here is one of zero decided to use 1071 plus one.",
            "So each example each row is a D + 1 dimensional vector here.",
            "OK, input lost are yet, and that's one way to view your data.",
            "Another very interesting way is to view it as points in a high dimensional vector space.",
            "So that's an equivalent view concept of that same data matrix, right?",
            "So here we've colored the points according to our target.",
            "Label 01 and each point is in a small D dimensional space, right?",
            "Each input is a point in a D dimensional vector space.",
            "So this is your data.",
            "This is another view of your data.",
            "Of course we can't display dimension or even imagine them in our head, but conceptually we can think that we're dealing with points in the D dimensional space.",
            "OK, we can plot two.",
            "That's nice two or three or."
        ],
        [
            "So that's an important view because it gives us a lot of intuition about some algorithms.",
            "OK, so this geometric view of the data and other problems.",
            "So here is an example of the nearest neighbor classifier.",
            "So nearest neighbor classifier is very simple.",
            "We have our training data set here.",
            "The blue and red points and our input space.",
            "And were shown in you test point.",
            "Remember the goal is to predict for a new test point.",
            "So what is the nearest neighbor algorithm, very simple.",
            "Four test point X here in green.",
            "Find the nearest neighbor of X among their training set.",
            "That would be that guy according to some distance measure, so that's to be specified.",
            "And simply predict that X is the same class as its neighbor will predict blue class one of the simplest algorithm based on this geometric view of things."
        ],
        [
            "Now, machine learning is traditionally viewed as a different set of tasks.",
            "We have supervised learning.",
            "And enterprise learning and reinforcement learning.",
            "OK, so supervised learning.",
            "And there's also this thing called semi supervised learning which is kind of a.",
            "Improvement on supervisor in school.",
            "It this way predicts the goal is to predict the target Y from input X, so we have an explicit target.",
            "Was an unsupervised learning.",
            "We have no explicit prediction target.",
            "That's the key difference.",
            "And reinforcement learning.",
            "I won't cover it at all here.",
            "It's a more general framework and more.",
            "Complicated.",
            "More sophisticated say.",
            "That's users that can use all of this to good effect.",
            "So they the most classical tasks in supervised learning are.",
            "Called classification and regression classification is when Y the prediction target.",
            "We want to predict represents a category of class and regression is when Y is a real number.",
            "So in classification will further distinguish between binary classification where you have to discriminate between two classes, just two classes.",
            "So often the class labels will be represented either as minus 1 + 1 or 01.",
            "And the multiclass case in which your class label might be between one and M, 'cause you have M classes or zero and minus one, and we have regression where trica single real number or several real numbers multiple regression.",
            "And so, in supervised learning.",
            "I mean, I'd see I'd say the mother of all unsupervised learning problems, or maybe of all machine learning problems will be probably density estimation.",
            "We want to model the probability distribution X.",
            "That's very hard, but if we can do that, we can do a lot of other things or discovering underlying structure of data.",
            "So that will be clustering in groups, dimensionality reduction and an supervisor representation.",
            "Learning that you'll hear a lot about this summer school.",
            "So this is it this supervised learning sometimes called predictive modeling.",
            "An unsupervised learning, descriptive modeling."
        ],
        [
            "OK, now in learning there is typically a two faces.",
            "There is what we call the training phase, where we learn a predictive function F Theta.",
            "By optimizing it so that it predicts well on the training set.",
            "OK. And there is using for prediction where we can use the F data on new test inputs.",
            "That were not part of the training set.",
            "So the goal of learning is not to learn perfectly, memorize the training set.",
            "That's not our goal.",
            "The goal is what's important is the ability to generalize well on new future cases on you test cases so.",
            "As I said in the beginning, I suppose that a lot of what I presented so far is pretty basic and get a little more involved in awhile, but it's probably things that you're familiar with already, but it's good to see."
        ],
        [
            "I think so.",
            "Here's our an example of 1D regression.",
            "Here we have our inputs.",
            "That's going to be 1D, so just a scalar real value.",
            "And a target that's another scalar real value, so we're given so the first step is to collect some training data.",
            "So we find some training data.",
            "So that's set of data that associate's to a number of input locations.",
            "Here a target value OK target the label value.",
            "So that's all training sets here.",
            "For 1D regression problem.",
            "Basically it's all these these 12345 examples.",
            "Of input target pairs.",
            "2nd, we learn a function that's from this data that's going to predict input to target.",
            "So the input to target mapping.",
            "See learn that function.",
            "And third, once we've learned the function well, usually we can.",
            "We have called FT to.",
            "Usually we can get rid of the training points, right?",
            "Don't need them anymore.",
            "We have a function, will learn based on them.",
            "It summarizes all the knowledge, all the relevant nodes, all the nodes that we deemed relevant about it.",
            "And now we can use it so that the first phase was a training.",
            "Now we can use it user learn function on new inputs.",
            "That's going to be our test points are probably our new cases in the in the in the field.",
            "So these are new points that were not necessary part, probably not part of the training data and that."
        ],
        [
            "We can now use our function.",
            "Sorry oh that's going to be long.",
            "Oh no, it's OK. Hey, you got the idea.",
            "So here's another way to to picture what's happening.",
            "How are we going to learn that function?",
            "We have our training set here.",
            "And the function we we can represent it as being trained such that when we give it inputs from the training set.",
            "It's going to map that input to some output F of X and that output.",
            "Is going to be compared with the target the prediction target in the training set and this comparison is done with something that we call a loss function or or cost.",
            "That we call that we write L here.",
            "That's going to measure the discrepancy between our prediction FT devex and the true target that we would like it to predict and learning will adapt.",
            "Find the function or adapt its parameters in such a way that this is minimized."
        ],
        [
            "OK.",
            "So now a machine learning algorithm an you can view almost all machine learning algorithms this way.",
            "They typically composed of the following three elements.",
            "The choice of a specific function family F. Often it's a parameterized family.",
            "A way to evaluate the quality of a function small F in that family.",
            "So that's going typically to use a cost or loss function as just showed, measuring how wrongly F predicts.",
            "And a way to search for the best function F in that family.",
            "So that's searching for the best typically optimization problem.",
            "OK, so optimization of function parameters if we have a permit rest function.",
            "Oh, sorry so.",
            "OK.",
            "Sorry, I use the keys, it works better.",
            "OK, so don't want to get get back to.",
            "I'm going in turn a look at these two aspects about how to evaluate the quality of function and search for the best function, and then I'll spend a serious amount of time on specifying the function family F."
        ],
        [
            "OK, so evaluating the quality and searching for the best function."
        ],
        [
            "So when we evaluate the predictor function F, the full performance is often evaluated using several different evaluation metrics.",
            "And that's important among those evaluation metrics, there are the often the true quantities of interest, and these are often ultimately expressed in dollars saved or number of life saved or this kind of quantity of real world interest when using the predictor inside the more complicated system in which it's designed to work.",
            "So that's what we ideally want to optimize, right?",
            "Not always easy.",
            "There's a standard evaluation metrics in this specific field.",
            "If you're working in.",
            "Machine translation, for instance, there's a blue scores.",
            "That may or may not be a super accurate for other purposes, but it's standard in the field, so you have to compare to it.",
            "There's things like misclassification error rate for a classifier, so that's a much lower thing, but you have other choices like precision, recall, or F score.",
            "How to evaluate a classifier?",
            "And then there's this other thing.",
            "That's the loss that you're actually optimizing.",
            "OK, by the machine learning algorithm, which may often be very different from all the above."
        ],
        [
            "I'll give a short example in a minute.",
            "So here are the standard loss functions for the tasks that I just described.",
            "Most important task for density estimation F will represent the probability density function or sometimes a probability mass function, and then the loss that will use is negative log likelihood loss.",
            "So that's simply taking minus the log of F and minimizing that.",
            "That's one way for regression tasks.",
            "That's probably the easiest and.",
            "The simplest, the squared error loss is simply.",
            "Taking the difference between your prediction target Y you label Y.",
            "And your whatever your function predicted.",
            "To the square, to the power of 2.",
            "So the further your prediction is from Y, the more you pay as a cost.",
            "And for classification tasks for classification task, remember we map some our input or D dimensional input vector to a class number.",
            "Well, the misclassification error rate.",
            "Miss gas station error loss as you pay one.",
            "If your predictor F of X didn't predict the correct class.",
            "OK, and you pay 0 otherwise that's easy."
        ],
        [
            "OK. Now here is a leader for classification tasks.",
            "This simple misclassification error loss OK, well, it's not super usable.",
            "Actually, it's hard to optimize directly becausw first if you try to find a gradient, you have a gradient of zero everywhere.",
            "And even if you think of linear classifiers and actually NP hard problem, if it's not linearly separable, so such a simple thing, right?",
            "OK, can't optimize.",
            "So what we do is we must use a surrogate loss, so that's not necessarily the real thing that we want, right?",
            "But it's something that's hopefully correlated to what we want.",
            "So here's some further distinctions.",
            "I maybe I won't spend much too much time on that.",
            "I again there's this binary classifier, multiclass classifier.",
            "Depending on how you represent your classes.",
            "And then there's the second probabilistic classifiers on non probabilistic classifiers.",
            "So for binary classifier that's probabilistic usually.",
            "I will use a binary cross entropy loss that the formula is given here, and that's nice when it's promising classifier by that what I mean by posting binary classifier is what you actually get as output function is G. Here that gives you the probability for class 0 or class one, probably that your label is 1 given your input and three days the other classes 1 -- G. Right, and then the decision function.",
            "The F. Actually, if you wanted to compute this misclassification error loss, you can get from G by simply looking is the probability of having class one greater than 0.5, right?",
            "So that's indicator of G of X greater than 0.5.",
            "Is that clear question?",
            "OK, now if we look at binary classifiers not not, not all boundaries affairs are probabilistic.",
            "One notable example is for instance the support vector machine is not a probabilistic classifier, at least not initially in its initial formulation.",
            "So here what you get is not a probability.",
            "G doesn't give you a probability for class one.",
            "It gives you kind of score a real valued score, right?",
            "And the score for the other classes.",
            "The negation of it.",
            "And what's used in an SVM is the hinge loss.",
            "So it's a little different here.",
            "LA rectifier of sorts.",
            "Um?",
            "And similarly for multiclass classifiers, well, you have the probabilistic classifier case, and you have the non probability classifier caseware multiclass margin losses generalization of the hinge loss.",
            "But my point here, they take home messages.",
            "There's several loss functions that you may want to to monitor.",
            "OK, and there's some that you're learning algorithm will actually be able to optimize, but they're not necessarily the ones that you're ultimately interested in, maybe not even shortly interested, and the ones that you're really ultimately interested in is probably dollars or number of life saved.",
            "So the better closer you can optimize, that better."
        ],
        [
            "OK, now I get into.",
            "Expected risk versus empirical risk.",
            "So.",
            "From now on, we'll suppose that are examples that we get in our training set are drawn identically and independently, identically distributed from an unknown true distribution P of XY.",
            "It's unknown as a distribution in nature.",
            "The distribution of images of cats and of cats and horses, or from some industrial process.",
            "The distribution of whatever characteristics your customers have OK or whatever web pages they browse are.",
            "Um?",
            "OK, so the idea assumption is very strong, it's it's can be.",
            "It's not necessary, but it makes our exposition simple, simpler here.",
            "So this is what we call the generalization error.",
            "Another name for it will be the expected risk, or sometimes just risk.",
            "And here's a layman's way of understanding it is how poorly we will do our predictor will do on average on the Infinity of future examples from that unknown distribution.",
            "Remember our goal is generalization, right?",
            "So as the future of the future example to Infinity of future examples drawn from that same distribution, how poor you are going to do on them, and so that's expressed like this.",
            "This risk of the predictor function.",
            "F is going to be the expectation for X / X and Y is drawn from the.",
            "Probably distribution P off this loss discrepancy between our prediction an the true label little target.",
            "OK, generalization error.",
            "Expected risk.",
            "Now there's another notion that's highly rated as empirical risk.",
            "Empirical risk will be the average loss on a finite data set.",
            "So in layman terms at how poorly we're doing on average, so it's not an expectation, is an average that's closely related, but not the same.",
            "So how poorly we getting doing on average on this finite data set?",
            "So that's a difference.",
            "Here we have a finite data set here.",
            "It's over the distribution or another way to say is overall the infinite number of examples that you get in the future.",
            "So here is just simply an average over of the loss.",
            "Over a given data set.",
            "Any questions?"
        ],
        [
            "Expect risk and picaresque now.",
            "There's a big principle as the principle of empirical risk minimization.",
            "So the examples here again are supposed to draw from their unknown true distribution.",
            "And would love to find a predictor that minimizes the generalization error.",
            "OK, that's our goal is to generalize well, so we'd like to find the function our predictor that minimize the generalization error that minimizes the expected risk.",
            "But we can't even compute that expected risk.",
            "Remember, it's an expectation over an unknown distribution, we can't compute it exactly.",
            "So we can do something that's kind of closer to it.",
            "It's the use the empirical risk.",
            "So the principle that we're going to use to actually.",
            "Learn.",
            "A function, find the function, find the parameters of a function.",
            "Is the empirical risk minimization principle.",
            "And layman terms is final predictor that minimizes the average loss over a specific data set that we're going to call the training except.",
            "So here's the equation.",
            "Remember our heart is our empirical risk, so we're going to search for the function small F in our family function.",
            "That's going to minimize the empirical risk over the training set, so that's a function that was going to make the smallest number of errors in average on the training set.",
            "OK, and that's what we call F hat of the train.",
            "OK, so that's the optimal function when that's doing the least errors on the training set.",
            "So that's the training phase in machine learning."
        ],
        [
            "Now, once we have found the predictor by training it, we will also want to evaluate how good it is, right?",
            "So the problem and how good it is?",
            "Again, it's a generalization error that we'd like to have, so we can't compute again.",
            "Their expected risk of a function F, but our empirical risk here of F. / A set D is a good estimate.",
            "Would say it's an unbiased estimate of our of F of the generalization error of the expected risk.",
            "Empirical risk over the good estimate of.",
            "Um?",
            "Expected risk OK?",
            "But the conditions for that provided D was not used to find the choose F. In any way OK?",
            "Otherwise our estimate would be biased, so we can't be using the training set here to estimate how well a function that was trained on the training set does.",
            "And provide also Dr. Our set that we use here is large enough, otherwise their estimate will be too noisy.",
            "OK, and on from P obviously.",
            "So the logical conclusion of that is that we must keep a separate test set that's different from the training set to probably estimate the generalization error of F of the train.",
            "So here's a very imprecise approximate.",
            "OK, this can be made more precise.",
            "Actually, it's it's an unbiased estimate.",
            "But the risks, the expected risk, so the generalization error.",
            "Of your function, F hat that you optimize on your training set.",
            "This expected risk.",
            "You can get a good unbiased estimate of it by.",
            "Running it and measuring its empirical risk on the test set, yes.",
            "OK, it means.",
            "It means that if you sample.",
            "Lots of such test sets.",
            "OK. And there's two ways to see it.",
            "If you were to use a test set that whose number of examples would go to Infinity that you would have an equal here.",
            "OK, as in R as a test set grows, you have an equal.",
            "So as a test set grows, this converges to the.",
            "So the empirical risk converges to the expected risk.",
            "Another way to see it is as if you were to repeat that by sampling.",
            "If you have.",
            "If you use a finite test, sets A of 10 examples.",
            "OK, but you repeat that by sampling new test set of 10 example every time and you average that you'll get to the expected risk as well.",
            "That's what's unbiased here.",
            "Answer question.",
            "I can read more.",
            "More more, more more formal with a limit than everything expression, but OK.",
            "The practical use here is remember, I'm trying to demystify the mystifier hypnotised users perspective on things and getting you the intuition.",
            "So the point here is you don't want to use the D train here.",
            "OK, that's otherwise you won't get a good estimate.",
            "So this here is really the test phase in machine learning."
        ],
        [
            "OK, and then this gives us a simple train test procedure.",
            "Which provided large enough data, set D drawing from that distribution.",
            "First you need to make sure that your examples are in a random order.",
            "Otherwise you're going to have problems.",
            "That's EII did represent the ID thing and split that data set into a training set part, an test set.",
            "So these are two subsets of your original data.",
            "Use a D train to optimize.",
            "Find the best predictor F = F hat of the train.",
            "That's a training phase.",
            "Optimizing to find the function that makes the fewest errors according to our loss on the training set.",
            "And once you've done that, you found that optimal predictor use the test set to evaluate generalization performance other predictor.",
            "OK."
        ],
        [
            "Now I'm coming to what is probably the most important.",
            "Whatever the most important aspect of that that unites all machine learning.",
            "Enough of this presentation is how to choose a specific function family F. OK, I remember there was our Third Point.",
            "We had three things we needed a way to evaluate a predicting predictor function.",
            "We needed a way to find the best predictor function among a family, but we haven't talked about family yet.",
            "So how to choose a specific function?",
            "Family F."
        ],
        [
            "To do that?",
            "First, let me give you well.",
            "First, let me ask you a question.",
            "What is the simple simplest predictor FET events that you can think of?",
            "OK, I heard it.",
            "I heard constant, so the simplest is really constant, right constant predictor.",
            "So I call this family F constant constant predictor is a predictor that always give you the same answer regardless of X.",
            "It's always going to answer B.",
            "So it is a permit, parameterized function family.",
            "It has a parimeter.",
            "It's what constant should it answer?",
            "You can learn that parameter right.",
            "You can learn it.",
            "So if you have a classification problem, say what should a constant with the optimal concentrate to be?",
            "What would answer?",
            "The majority class which class appears more often in your training set, right?",
            "That's the predictor which is going to make the fewest errors on your training set.",
            "Among that family.",
            "So that's a constant predictor.",
            "If we have a regression problem, what should be the constant predictor?",
            "The average of the targets of all your training set points.",
            "Which other targets labels.",
            "OK, so we have this family.",
            "It's a pretty small family.",
            "We have slightly bigger family that I also heard.",
            "That's called F linear.",
            "It's actually a slight abuse because I also include the affine in their off and put them together.",
            "Linear and affine collinear is usually a fine fine predictor.",
            "So in one dimension there will be a function F. That computer W * X + B so X is a scalar.",
            "Multiply it by some weight W an add this bias B and the parameters of that function family RWNBW is the same dimension as your input and be a real.",
            "So indeed I mentioned you have a dot product here.",
            "OK, so the constant is included in that family, right?",
            "If you put W 0 you get the constant predictor.",
            "So slightly bigger than that.",
            "Logic follows the logic.",
            "Polynomial thank you.",
            "OK, so linear is a special case of a polynomial predictor.",
            "Well, number predictor of degree P. OK, here I write it in one dimension so you still have your BUX.",
            "Is is just a scalar and you have all these parameters?",
            "A 18283 that are the coefficients with the exponent of of X?",
            "So I.",
            "Here.",
            "How many scalars do I have in one dimension in the parameters?",
            "P + 1 OK.",
            "Here how many I had.",
            "I had just two, right?",
            "WNB and the one dimension.",
            "Now if you move to two dimensions, OK, give me an order of the number of.",
            "Of four meters.",
            "I say degree P in D dimension.",
            "I don't want to precise answer.",
            "K. PC P2D OKP, 2D.",
            "So if you're if you're in 100 dimensions, your D OK, and even the polynomial of degree two is grows pretty fast, right?",
            "Do you want to know what degree three you get lots and lots of parameters?",
            "Just again, I guess that's one of the problems with problem predictor and we see some consequences of that later.",
            "Sorry, wrong key.",
            "OK yeah, yeah just here.",
            "Just wanted to show the examples so forget the blue curve.",
            "The blue curve is actually what the examples were kind of drawn from so the examples is X target Y OK. And here's what a linear predictor the function they will learn it can learn.",
            "Wow.",
            "That thing is not working very well.",
            "Sorry bout that.",
            "OK, and here here you have a polynomial predictor degree two or three.",
            "OK."
        ],
        [
            "OK, so this brings me to the key notion of this talk, and that's the notion of the capacity of a learning algorithm.",
            "Please raise your hand who has heard of the notion of capacity.",
            "I don't see all hands race, so my presentation won't be totally useless OK?",
            "On so, the notion of capacity.",
            "Is a following when you choose a specific machine learning algorithm OK?",
            "That means that you're choosing a specific function family F. Remember that one of the constituents of learning algorithm is a specific function family, family, F. Now we can say that this family, as I showed some example here.",
            "You can you can picture this informal notion, but you can imagine formally there's a size to that family function as we solve for the constant predicted linear predicted the polynomial predictor.",
            "So how big that family is, we can also see how rich how flexible, how expressive, how complex it is defines what we informally call the capacity of the machine learning algorithm.",
            "OK.",
            "So here in our example, the capacity of a polynomial of degree three or predictor would be greater, larger than the capacity of a linear predictor.",
            "OK.",
            "So what can actually come up with several formal measures of capacity for a function family learning algorithm?",
            "One such formal measure is for classic classifiers.",
            "Is the VC dimension vector analysis dimension, but that you can come up with many others.",
            "So what's important here is to get the notion of the kind of informal notion of capacity.",
            "So one rule of thumb to estimate the capacity of a function family F or learning algorithm, which is kind of the same, will also call that the model is the number of adaptable para meters, the number of scalars it has insight.",
            "That's a little bit the exercise that we did.",
            "You know we're looking at the concept is just one.",
            "The linear has well in the dimension as D + 1 and the polynomial has depending on the grill promo.",
            "Many minutes expansion of degree.",
            "OK, so the number of adaptable para meters, but that's just the rule of thumb of proxy.",
            "It's kind of very imprecise.",
            "So how many skill levels are continuing into?",
            "So there's one notable notable exception to that.",
            "Rule of thumb.",
            "Can someone tell me?",
            "I wouldn't say news name, so in my view nearest neighbors has many, many parameters because it's para meters are the training set.",
            "That's what it learns that what it needs to make a prediction.",
            "The whole training set so it has many.",
            "Anne.",
            "OK.",
            "Sorry, So what I'm hinting at here is simply, if you're familiar with neural networks or layered architectures.",
            "If you compose multiple linear transformations, OK, you get what.",
            "A linear transformation.",
            "So if I take 1000 matrices OK, an multiply them together and that's my composition, I have 1000 times more parameter, but is still a linear model.",
            "OK, so I haven't increased that.",
            "So just a real thump.",
            "OK.",
            "So this notion of capacity now introduced some even more fuzzier notion.",
            "The notion of effective capacity."
        ],
        [
            "And it's related also to what I call capacity control hyperparameters.",
            "So we have this notion of capacity, that's the size.",
            "If we want off of our function family F. But the effective capacity of machine learning is the controlled by some couple of other things.",
            "So there's a choice of machine learning algorithm or.",
            "That essentially determines the big family Big F. But there's other things.",
            "There's hyperparameters that further specify F. So a simple example in what we saw is the degree P of a polynomial, right?",
            "We said we have a family.",
            "That's the polynomials.",
            "OK, fine.",
            "But when you say you haven't specified degree yet, right?",
            "So you can specify the degree of the polynomial.",
            "OK, that will further restrict your effective.",
            "Your function family F. Similarly, if you support vector machines, the choice of the kernel.",
            "Can further specify the family F or in a neural network that we in a number of layers, the architecture, the number of neurons, etc that will specify.",
            "The function family.",
            "So far so good.",
            "Yep.",
            "There's other things, there's what we call hyperparameters for regularization schemes, regularization, hyperparameters.",
            "So I give a couple of examples here.",
            "You can imagine, say, a linear classifier or polynomial crossfire neural network.",
            "OK, but you can add some constraint on its weights, say the normal weights is no bigger than three.",
            "OK, well if you do that, you've effectively restricted the set of parameters that you could get.",
            "The thing can use, so you've restricted your function, your function family.",
            "Right?",
            "So often it's not phrased as a constraint on the norm, but rather a penalty on the norm.",
            "But that's kind of that's equivalent if you're penalizing the norm, it's equivalent to putting a constraint on the norm.",
            "So that gives you if you take linear regression and you add that kind of constraint or penalty that gives you Ridge regression.",
            "OK, annual networks.",
            "That's called weight decay.",
            "Is everybody heard about weight decay?",
            "Yeah, OK.",
            "So we're in neural network decade.",
            "Anne.",
            "So um, sometimes often, well.",
            "I say in some cases these trade off.",
            "Well, try to favor some values per meters versus others and this can building tube Asian prior print para meters.",
            "So that can also be the strength of that Bayesian prior or that vision prior itself can also be seen as a regularization scheme.",
            "There's other kind of more recent than strange realisation schemes like noise and noise injection.",
            "You can control the degree of noise where you inject the noise, etc.",
            "All these things will have an effect.",
            "Maybe not on the bigger family F. If you view it strictly but effectively on what part of that space of F that you're effectively going to explore.",
            "OK, and there's a final one here.",
            "That's early stopping, and that's something that you can use in any iterative search optimization procedure.",
            "OK, whether it be gradient descent or boosting, you're familiar with boosting, right?",
            "It's also iterative, right?",
            "Can also be seen as gradient descent, but.",
            "Although it's not implemented strictly this way, so when you choose to stop that exploration before convergence, you choose to stop it before convergence.",
            "This iterative search optimization procedure.",
            "Well, if you did have a question to stop it before convergence, that means that you that you won't explore as far from the original point as if you had run it to convergence so ineffectively, you're not exploring the whole space F, so that's also an effective restricting the effective capacity of your argument.",
            "Any questions?",
            "OK, yeah.",
            "So all regularization schemes are suboptimal.",
            "If your goal is to find a minimum on your training set.",
            "But that's not your goal.",
            "Your goal is to find the best generalization.",
            "That's why we have we have.",
            "Capacity control regularization schemes.",
            "If your goal is to find the minimum error on your training set.",
            "All these things are going to hurt you.",
            "All of them.",
            "You want to be hurt, but I get to that, yes.",
            "Yes.",
            "I hope.",
            "I know, I know, it's what do you appetite?",
            "Yes, sorry.",
            "OK, that's too, that's my next slide, wonderful."
        ],
        [
            "So I'm making a an important distinction.",
            "I called hyperparameters.",
            "Some people call them later para meters OK. That's to distinguish them from power meters.",
            "And per meters are what is being optimized on the training set by your optimizer or your learning algorithm.",
            "OK, so take the example of the polynomial classifier.",
            "Which isn't there OK?",
            "OK, it's still take that one.",
            "OK, the polynomial classifier you're going to learn the coefficient of your polynome, let's call them W and be OK to make it simple.",
            "I'm going to learn those the algorithm is going to optimize those on the training set.",
            "OK, it's going to find the values of those parameters such that your model makes the fewest error on the training set, but there's things that you fixed ahead of time before that training, and that will be the degree of your polynomial, so the degree of your polynomial is a parimeter that you fixed ahead of time for your learning algorithm.",
            "That's called, that's what we call a hyperparameter that's to be distinguished from the parameters of the model that the algorithm is actually going to learn, so these.",
            "Specify the family.",
            "A function.",
            "These allow you specify which function in the family you're going to choose.",
            "So the distinction is very important.",
            "They specify the family of the function big F. These are the parameters of the small F that's part of Big F. Yes.",
            "Yes, I automatically automagically.",
            "Yes automatically sure I will get to that a little bit.",
            "I'll give the high level principle and then maybe up into some.",
            "Hopeful future directions.",
            "OK, so just quickly here, just to give a set of examples.",
            "Some of you may be familiar with is I picked up a couple of very popular learning algorithms, so for class popular.",
            "Sorry classifiers more specifically, so there's classic algorithm for classification.",
            "So one of the simple that's a linear classifiers logistic regression and the other is a linear SVM.",
            "These are two linear classifiers.",
            "They don't optimize the same function exactly.",
            "OK, none of them optimize the misclassification loss.",
            "Remember why, why?",
            "Because we can't OK so.",
            "So they they use a slightly different proxy in both.",
            "That's a probabilistic classifier.",
            "That's a non probabilistic classifier.",
            "And the linear SVM has a hyperparameter.",
            "If you use the package, you might have encountered AC and annoying, see OK. And it learns WND, similar dogic regression if you add a regularizer, because I want put in power with linear SVM, and you have the strength of this regularizer to set ahead of time before training it with the training set.",
            "Similarly, if you have kernel support vector machines and you still have that C, But you have other hyperparameters, that's the choice of your kernel and the parameters hidden inside your kernel.",
            "So for popular kernels like their RBF or Gaussian kernel, it's a Sigma the width of the Gaussians for polynomial kernel, it's the degree of the polynomial.",
            "And what it learns the parameters.",
            "So once you've set dizzy ahead of time and you run your SVM, it will learn support vector weights.",
            "Neural networks you specify layer sizes or stopping criteria.",
            "Lots of other things.",
            "So again, these will specify the capacity, the effective capacity, the family function, and how well you're going to explore it are restricted.",
            "Realistically, you're going to explore it.",
            "Of your model.",
            "And what you get after training it on the.",
            "On the training set is the weight matrix the weights in the weight matrices decision tree.",
            "The primary capacity control parameter is the depth of the tree.",
            "An in Kenya's neighbor may see you have two you have.",
            "KK is essential hyperparameter and the choice of the metrics that you're using and what it learns.",
            "I answered that question before.",
            "In my view, it less parameters, it memorizes the training set that sits para meters.",
            "OK, that's why it's needed.",
            "That's what it requires to make a decision.",
            "OK."
        ],
        [
            "So tuning the capacity, how do you tune that capacity?",
            "We have this notion of capacity.",
            "We have ways to control it, and all this lever to control it.",
            "So how do we?",
            "Make it best optimal, right?",
            "So it must be optimally tuned to ensure good generalization, OK?",
            "And that's done by tuning all these levels.",
            "So by choosing the right algorithm or.",
            "One among several an choosing it's right hyperparameters OK, and the goal of that is to avoid underfitting and overfitting.",
            "OK, so here's an here's again our simple example.",
            "Anne.",
            "See, we have these points here, so it's a regression problem.",
            "OK, so we have X here, that's our input.",
            "An why that's what we want the predictor to predict.",
            "Apples in the training set, that's that are the black dots.",
            "And basically, those black dots were drawn with a little noise from that blue curve so that blue curve with a little noise is essentially that distribution from well.",
            "In this case, it's not nature, it's artificial, but that's the unknown distribution, right?",
            "That's famous, unknown distribution.",
            "OK, now we know it because it's an artificial experiment, but usually we don't, so it's just to show where they actually come from.",
            "Now, if we fit a linear predictor to that OK, we're going to fit that little line here, so you can see the error that is making on the training set.",
            "So what is the error on the training set?",
            "We have regression problem so squared error.",
            "So the squared errors with distance.",
            "Here how far each for each training point we are from the projection on the red.",
            "Right, so it's this distances squared the sum of all these distances squared, or the average of all this distance squared, right?",
            "That's our training error.",
            "That's our actually that's our going to be our empirical.",
            "Um?",
            "Risk on the training set and we can get have an idea of the generalization error.",
            "The generalization error.",
            "Well, that would be if we drew other points from that same distribution.",
            "How far the red line with it you see in this here?",
            "It will be pretty far.",
            "So here it's a linear classifieds printer locally Facetti, so we have fenneman.",
            "That's underfitting the function.",
            "We chose the family found with functional family is not flexible enough to adapt.",
            "Optimally, to the training data.",
            "Here's what you'd get if you take a higher degree polynomials that would be like, I know, degree 15.",
            "Something like that.",
            "OK, take a bit of degree 15.",
            "What do you see here?",
            "Well, you see it learn the function in red that passes through all the training points.",
            "Wonderful, you may think that means that you have.",
            "Zero training error, right?",
            "It learned the training set my heart perfectly.",
            "OK.",
            "But how is it going to do on new points on new examples?",
            "OK, if you draw new examples from the from the blue curve, well if you draw some some around here is going to predict something that's pretty far off.",
            "OK, so here that's the phenomenon of overfitting.",
            "You've chosen 2 high capacity.",
            "Your function is too flexible.",
            "It learns very well on training set, but generalizes very poorly.",
            "OK. Now what would be the optimal capacity so that would be maybe a polynomial of degree three site?",
            "23 So that would give you this red curve, so you see, it doesn't do a perfect job at true learning the training set, it's not passing through all the training points.",
            "The training error is not zero.",
            "But it will generalize well if you look how far the blue the red curve here is from the blue one.",
            "OK, there's nowhere.",
            "It's super far.",
            "OK, so that would be the optimal capacity.",
            "Again, here's an example to choose what to choose.",
            "The degree of a polynomial that's a hyperparameter that controls the family function effectively, what degree polynomial?",
            "And that controls the capacity.",
            "OK. And the whole message here is that performance on the training set here.",
            "OK, so the performance on the on the data that you.",
            "Learned your chosen your function on.",
            "Learn this family design is not a good estimate of generalization here.",
            "It's zero.",
            "It says, OK, I'm perfect.",
            "I learned that training set perfectly.",
            "Very bad realization that's overfitting any questions."
        ],
        [
            "Right, I'll here to quickly another example in classification.",
            "Here we have two classes, the red versus the.",
            "Black, so it's a 2 dimensional input, OK. Luminosity and.",
            "Anne, what?",
            "So here's an example.",
            "Is 2D classification with a linear classifier you get there.",
            "You'll get a hyperplane separating hyperplane.",
            "That's what you get.",
            "OK, so maybe here the capacity is little too low for this problem.",
            "Relative to the number examples you have an underfitting phenomenon."
        ],
        [
            "At the other side, if you take a more flexible predictor, it may learn something like that, say here it predicted perfectly.",
            "It makes absolutely no error on the training set.",
            "OK, but maybe it's a little too convoluted for what that really represents, so it's likely to have overfitting."
        ],
        [
            "And here is probably something that would have with optimal capacity.",
            "We have something that's nice and smooth.",
            "That's more likely to generalize well.",
            "Just picture for the same."
        ],
        [
            "OK, so what's happening here?",
            "I'll use this little decomposition of generalization error as a.",
            "An explanation of what's going on.",
            "OK, basically you can think of this green box here as the set of all possible functions in the universe.",
            "OK among all those best all those powerful possible functions universe.",
            "Let's say there's one that's the best possible predictor F star.",
            "OK, it's here.",
            "Best product.",
            "Possible predictor in the universe.",
            "OK so.",
            "Now what we've done is we've that restricted ourselves to a specific function family.",
            "Big F, yes.",
            "That's our whole point here.",
            "OK, so inside that function family big F you have the best possible function for the task that has the lowest generalization error.",
            "We talk about generalization error here among that family of functions, let's call it F star F to say that it's in the family function.",
            "And then we have a third guy.",
            "That third guy is whatever our learning algorithm learned using the training set.",
            "OK, that's called F hat of the train.",
            "It was trained on your training set.",
            "So actually we can decompose the generalization error that this guy is going to make.",
            "Anne.",
            "In 2 two important terms, basically at measure how far it is from the best possible function and one is adding how far it is from All Star.",
            "The functional lumbar algorithm is from the best function in the set in the function family that we chose.",
            "And the second term is how far that best function in F is far from the best possible function.",
            "OK. Anne.",
            "So these two terms have a name, that big name one is called a bias.",
            "That's not to be confused with the bias that you have in linear models or in neural networks, OK?",
            "And another term here is variance.",
            "Is all related to a notion that best approximation, error and variance is an estimation error.",
            "So can someone tell me what is responsible for the bias?",
            "Why do we have a bias?",
            "Sorry.",
            "Yeah, so so.",
            "Basically the bias is due primarily to the fact that we are considering unrestricted family function.",
            "OK, so that's the choice of algorithm.",
            "That's restricting that, and the choice of hyperparameters OK?",
            "So that's why we have a bias.",
            "That's basically how far the best function in that family is.",
            "You're stuck in that family, right?",
            "So that's advice.",
            "How can we reduce the bias?",
            "Expanding the family.",
            "Yes, I get back to that.",
            "So that's what the bias term is do is due to having a restricted family.",
            "Anne.",
            "That's the capacity of the family, it's restricted.",
            "How is the variance term due to?",
            "Yes.",
            "Yes, so there's different factors, but let's suppose for a minute that we have a wonderful optimization algorithm.",
            "That is really able to optimize that perfectly.",
            "So given the training set, it's finding the optimal for that training set, it is OK, it's magical, it works.",
            "No training, no.",
            "Optimization operators know learning rate to do nothing.",
            "OK, it works.",
            "It finds it.",
            "So then there's still remaining source of variability.",
            "What is it?",
            "Yeah, so these two are related.",
            "Is this the fact that we have a limited finite training set and the variance is really that?",
            "If you were to sample if you if you were to?",
            "I mean you got that training set from the true nature distribution, but you could have gotten others right?",
            "You could have got another training sets actually have a distribution over training sets.",
            "If you take a training set of size N OK, you have a whole distribution of training sets of size and that you could have gotten.",
            "And for each of these training set.",
            "Training set one during set 2 drinks at three, well, you're super optimal for that training set will be at some slightly different place in there, so that's responsible for the variance here.",
            "OK."
        ],
        [
            "Right?",
            "So now we have the bias variance dilemma.",
            "OK, so as you answered rightly.",
            "If we want to.",
            "Reduce the bias.",
            "We can simply choose a larger family.",
            "Right then will be closer, so will increase the capacity.",
            "Choosing richer family will increase the capacity so, but that's what that's going to do is is going to decrease the bias, sure, but it's also going to increase the variance.",
            "So just to understand, I mean informed way to understand why Richard family means more parameters to tune, but you still have the same number of training examples, OK?",
            "So your estimate of the parameter is going to be even more noisy.",
            "You can do the other way round.",
            "You can reduce the capacity.",
            "Take a smaller function.",
            "Family OK, in which case your variance will decrease.",
            "OK, I mean, if you take the constant predictor.",
            "The van's gonna be pretty low because it's pretty easy to estimate an average using a couple of points right there.",
            "It's going to be pretty low, but the bias can be is going to be huge.",
            "OK.",
            "So, um.",
            "We have to find an optimal compromise between the bias and variance.",
            "That's called what that's called the bias variance dilemma.",
            "What I showed and bias variance tradeoff training one for the other.",
            "There will be some optimal compromise, but that compromise will depend on the number of examples.",
            "OK, the bigger your number of examples.",
            "If you don't touch your model family, you don't touch your F. If you have larger, larger training set, well, your variance will automatically decrease.",
            "OK, you don't have to change the function family.",
            "The variance will decrease.",
            "So if you have bigger, if you have more data then you can afford to increase the capacity in order to lower the bias.",
            "OK, you can use more expressive models with bigger datasets.",
            "Anne.",
            "And I'll conclude from that side is that the best regularizer is more data.",
            "Please advise.",
            "Alright."
        ],
        [
            "But you don't have more data unfortunately, so this is my heart, yes?",
            "Yes.",
            "That's right, yeah, exactly I. I gave that example.",
            "Oh, you have very small capacity so that case will be.",
            "You can view this image here.",
            "You have a tiny, tiny, tiny little family F. It has just one parameter in it.",
            "It's very small ish.",
            "Tiny family F. So that means that the variance is going to be very small, but the bias is really going to be super high unless your true best function is also constant.",
            "OK, it could be the optimal in some specific cases.",
            "Yeah.",
            "So that's what's happening here.",
            "Yes, it you're going to have a very small capacity.",
            "Thus small variance and a high bias.",
            "Yeah.",
            "No, it depends on the size of family.",
            "The capacity is the size of family function.",
            "Predicting predicting the mean.",
            "I mean, the mean is just one parameter.",
            "OK, it's a constant predictor.",
            "It's very it has very small capacity.",
            "So very few parameters to adapt.",
            "That means that it will have very low variance.",
            "It's very it will be very.",
            "It won't be very noisy to estimate it given given a training points, but it's going to be very far from OK. Let me show you the example.",
            "I didn't show it in here, but it could have."
        ],
        [
            "OK, constant predictor here will be a straight line.",
            "OK, that's.",
            "The variance.",
            "Will be low, but the bias how far it is.",
            "From how far that family function is from containing the actual true function is very high.",
            "Sorry.",
            "OK, so model selection.",
            "This is my how to slide.",
            "How to do it?",
            "So remember, initially we split our data set between train and test set.",
            "Now we're going to split it in three, so that's supposing you have plenty of data, OK.",
            "If not, this gets more complicated.",
            "I won't get into it, but suppose it will split it into three trees, three datasets that we call the training set validation set.",
            "And a test.",
            "So with better test set and validation entry.",
            "And then here is a super generic mid algorithm.",
            "For model selection, so for selecting your family of function, OK, you're F. That includes selecting the actual learning algorithm.",
            "OK, is it a polynomial?",
            "Is it linear?",
            "Is it a?",
            "Is it a kernel support vector machine?",
            "Is it whatever your machine learning algorithm and its hyperparameters?",
            "So instead of hypermedia, quite Lambda.",
            "So once you can do, that's a principle algorithm.",
            "OK, it's not practical.",
            "For that you can choose for each considered model, so that includes you put put all in the back OK support vector machine, linear kernel, or kind of kernels, neural networks, decision trees, put them all in the back, all of them OK, they're all in that back.",
            "You're going to loop over them here in that loop, and then for each of those algorithms you take all their hyper hyper parameters that control capacity.",
            "You put them all in the bag.",
            "OK, and then you're going to loop over all possible configurations of values of those parameters.",
            "That means that for neural networks, you try all layer sizes or whatever for Kernel 4.",
            "For support vector machines, account support vector machine, say RBF kernels, you try all widths of kernels for nearest neighbor.",
            "You try all K for nearest neighbor etc.",
            "That's so far all algorithms.",
            "Then for all consider hyperparameter values.",
            "And that algorithm?",
            "That's a huge space.",
            "OK, that's a problem, but it's a principle.",
            "So for each such algorithm and specific complication of hyperparameter value, you train that model with that configuration of hyperparameter value on your training set.",
            "This training procedure I write a Lambda is a learning algorithm specified with a hyper hyper parameters.",
            "Lambda OK. And you control it on the training set you train it on the training set and what that gives you is the optimal function for that algorithm.",
            "That parameter under of that function function space.",
            "On the training set, and that's what I call F hat a Lambda.",
            "OK, so for each algorithm A are each Lambda you're going to have a different hat alenda different predictor.",
            "Then you evaluate that predictor on the validation set, don't evaluate it on the training set.",
            "I mean you can, but don't base your decisions.",
            "Don't base your model selection on.",
            "That will get to that in a minute.",
            "So with your preferred evaluation metric, not that the evaluation here might be different from what the algorithm actually optimized, that's OK. What you want, for example, here is customary to train a, say, a a classifier probabilistic classifier with binary cross entropy back to evaluate it on misclassification error.",
            "OK, that's common.",
            "Alright, so we're going to evaluate on the preferred evaluation metric of interest.",
            "So that you take the risk according to that loss function off your just trained algorithm on the validation set an we call the resulting error EA Lambda.",
            "So for each algorithm a each hyper printer combination Lambda you have an EA London.",
            "So among all of them remember you had many areas and many hypermedia commendation you find which one is.",
            "The best.",
            "Which one you like the best?",
            "And we call that algorithm a star.",
            "Ann hyperedges landstar.",
            "And then you simply return that the predictor that was obtained with that algorithm.",
            "That's your model selection you've effectively selected among all the set of models as VM's, whatever all those set of hyperparameters Lander with kernel, etc, you selected the one that performed.",
            "Best on validation set and you return that optionally what you can do is you can actually take those hyperparameters.",
            "It doesn't always work, but sometimes helps take those hyperparameters and that algorithm and retrain it on the union of the training set and the validation set.",
            "That's fair, yes.",
            "If I recommend it so I just said it doesn't always work, it will work.",
            "If your optimal algorithm hyperparameters Lander and algorithm scale relatively well when you increase or give relatively well, well being unchanged as you increase your data, your data set.",
            "Yeah so.",
            "We could actually make a test for that bit complicated OK, and so that's the model selection and training.",
            "OK, so that's that's a kind of meta algorithm, because you see inside it uses basic learning algorithms.",
            "So that's the higher level.",
            "Which Toons chooses it algorithm and the hyperparameter values.",
            "And finally you have this winning algorithm F star here.",
            "Oh no, the wonder of this thing here.",
            "This algorithm at last is now going without any hyperparameters to tune.",
            "Isn't that wonderful, OK?",
            "Yeah, it's just you you need.",
            "You need clusters the size of the universe to actually implement it, but.",
            "And finally, you compute the unbiased estimate authorization performance using our test set.",
            "OK, I'll test set.",
            "We still haven't touched it.",
            "We haven't used it.",
            "We must never have used it.",
            "Actually during training or model selection to select, learn or tune anything, you can look at it.",
            "You're allowed to look at it.",
            "Yeah wow, some people say you're not allowed to look at it, but everybody looks at it, OK?",
            "But you're not allowed to select anything based on it tune anything based on it.",
            "Optimize anything based on anything you see from it.",
            "OK. You can look at it.",
            "So yes.",
            "Oh sorry, yeah yeah it it.",
            "It will appear in time.",
            "Yes.",
            "So, so that's complicated.",
            "Usually when you're when you're when you're caught with doing K fold cross validation is because you don't have enough data to begin with, right?",
            "So it depends, it depends on your.",
            "How your thing is going to be evaluated if you have a proper evaluation test set that you can touch, that's big enough and reliable enough, then you can use that.",
            "OK, but often the problem is if you're caught and then you can do.",
            "You can do careful cross validation to find the best hyperparameters.",
            "Those that on on average well will do best on your moving violation.",
            "Set OK, but then then use that, use that.",
            "Retrain on the whole training set.",
            "Full complete training and validation.",
            "Set and use your separate test set now.",
            "If you really have very, very small data, then maybe you need to do something else called double cross validation.",
            "So you need to do a cross validation.",
            "So say, for example, we've run out on your test data, OK?",
            "And and inside you take what's remaining as your training set, and you do another.",
            "For example, leave one out OK, fold to choose a hyperparameter.",
            "When you do that, what you will evaluate is not.",
            "You won't get a single algorithm in hyperparameter out of that.",
            "You'll get an evaluation actually off on what you're going to evaluate on your test set.",
            "Is your model selection meter.",
            "That meet algorithm that has no hyperparameter.",
            "That's the version of that that's going to be unbiased on your test set.",
            "It's a little more complicated procedure with a double cross validation.",
            "Yes, sorry.",
            "Nope.",
            "Sorry, did the bigger the better.",
            "I know it's saying it's customary to take like 110th for training set for a test set.",
            "110 Fender validation set.",
            "That's about the same size and training set, but.",
            "Really, it again, it's a tradeoff, right?",
            "If you put more examples in your test set.",
            "Your your evaluation of your generalization error will be less noisy, but you have few examples to train it on, so yeah.",
            "Size.",
            "They are OK. That was given the first thing, fixed their other hyperparameters actually is.",
            "The choice of which albums you're going to consider.",
            "And.",
            "Yeah, but did that.",
            "You shouldn't do that.",
            "Beijing is what they say is you consider all the algorithm within the universe OK, but you put a flyer over them right?",
            "You put it prior and then you integrate them all.",
            "Suppose you're able to do that for the answer, but we still have a high performance.",
            "That's your prior.",
            "OK, so there there's some priors that are kind of universal.",
            "Supposedly I don't know.",
            "Sorry I don't have to advance it.",
            "Oh, so here's."
        ],
        [
            "There an example of Moe hyperparameter selection procedure so.",
            "OK, I'm almost up.",
            "Example hyperparameters, so we're running that algorithm, but for a simple case, we chose just a single algorithm and a single hyperparameter.",
            "OK, so that's easy.",
            "So then we just have to change the value of that hyperparameter.",
            "Here it's on the X axis, is the hyperparameter value on the X is the hyperparameter value.",
            "What we have in Gray here is the corresponding.",
            "Error of interest as measured on the training set.",
            "So the set that was the data set that was used to tune the parameters of the algorithm.",
            "OK, for each value of hypermedia an similarly we have the error on the validation set.",
            "So that's kind of typical.",
            "We see the training on the the error on the training set.",
            "That's actually a monotone no sleep here increasing.",
            "You might have seen it often decreasing.",
            "Here it's increasing its desired and you have the.",
            "Error on the validation set.",
            "Actually the test set was going to follow the similar curve.",
            "Normally there's they play the same role right there, set the validation set and the test set the exactly the same role until you choose the performance on the validation set to choose a hyperparameter.",
            "Then they don't say the same or anymore, but until you've made that choice, they exchangeable.",
            "Save.",
            "Um, so here what you see typically is you see a decreasing up to some point and then increasing, increasing again OK.",
            "So what is the optimal hyperparameter value here?",
            "OK, it's here hyperparameter, which is smallest error on validation set is 5.",
            "OK, if you choose it on the training set you get one.",
            "It's very different.",
            "Can someone guess what kind of algorithm this might be?",
            "K nearest neighbors number that scale in the neighbors.",
            "So you're smoothing your neighborhood here by taking five neighbors instead of 1 neighbor.",
            "And finding the optimal capacity this way."
        ],
        [
            "Optimum might sometimes might often be one.",
            "Also, depending on the training on the side of the training set, right?",
            "So, uh, the question, what if we selected the capacity control hyperparameters that yield the best performance, not on validation set, but on the training set?",
            "What would happen then?",
            "What would we tend to select?",
            "We would select the hyperparameters that choose the highest capacity, because that's what it will.",
            "It will allow the training algorithm to make the smallest errors on the training set by probably lead to what?",
            "Overfitting OK?"
        ],
        [
            "If your model class is big enough, right?",
            "If you're if you're within something that's ready to constrained, maybe not, but OK, let's skip that, that's not."
        ],
        [
            "Yeah, just mentioned that their assemble methods and sample methods combine multiple predictors to good effect.",
            "So you train multiple predictors.",
            "And you can combine them.",
            "So there's bagging, bagging averages many high variance, high variance predictors.",
            "OK, so by averaging many high rents predictor actually decrease the variance.",
            "So one example, one very successful example of this is maybe heard.",
            "This one very successful argument.",
            "That's random decision forests, random decision forest the forest, right?",
            "The average trees, the average deep trees, deep trees are known to be very high variance.",
            "So you average them and they take including or more variance in random decision for us and you get pretty good algorithm.",
            "On the other hand, you can if you if you're not using high variance of high capacity predictors but low capacity classifiers, you can actually get a higher capacity classifier by using boosting by combining them with boosting.",
            "It will learn a weighted combination of them that will lead to higher capacity.",
            "So it's common, for instance, to boost shallow trees which have low capacity or linear classifiers."
        ],
        [
            "Here's just a view of of bagging.",
            "On our regression problem, if you see all the little Gray lines.",
            "These are all the lines that you get if you resample your training set by a bootstrapping, and if you average them altogether while you get that red line that has lower variance.",
            "OK, I'll skip that."
        ],
        [
            "I get to my very too lost light.",
            "Well, let's maybe more to make transitions with what Josh was going to tell you, so I haven't looked into detailed customer, except maybe a linear linear predictors.",
            "Any?",
            "I mean, I haven't even looked at linear classifiers, but linear linear models are an important class of model, but they're pretty limited, right?",
            "So it's just a linear function space is just linear, so there's one way.",
            "One easy way to actually make them richer, and that's to take your input and to map it into some other representation.",
            "So you take your.",
            "X and you map it to X~ and there's basically three ways of doing that.",
            "You can either use an explicit fixed mapping, and that's what essentially what you do and use handcrafted features.",
            "You take your raw input, you take some handcrafted feature extractors OK, which are often nonlinear, and you get some new representation X~ That's a fixed set of handcrafted features that's an explicit mapping.",
            "There's no previous example.",
            "Another way you can use an implicit fixed mapping, and that's essentially what kernel methods do, like kernel, SVM's, or kernel logistic regression.",
            "Take your input, they map it implicitly into some high dimensional feature presentation and do a linear classifier linear model there and then.",
            "Finally, and that's what a lot of this summer school will be focused on.",
            "You can actually learn the parameterized mapping, and that's that's where you get.",
            "The machine learning algo, that tool and representations, and that's where you get the multilayer feedforward networks.",
            "That is, you add layers that actually."
        ],
        [
            "Change their presentation.",
            "So I guess your show will show you something probably similar to this site.",
            "Also that gives us the notion of levels of presentation.",
            "If you have whatever.",
            "Model here you can always try to put further nonlinear function compositions on top of it.",
            "OK, that will produce better representations for it."
        ],
        [
            "So that's it for me."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm glad to be here with you this morning.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do something that some of you might find a little boring.",
                    "label": 0
                },
                {
                    "sent": "If you've already been exposed to that material, that's an introduction to machine learning, so machine learning is quite broad.",
                    "label": 1
                },
                {
                    "sent": "I try to focus on what I feel is really essential.",
                    "label": 0
                },
                {
                    "sent": "The core that kind of binds all of this together, maybe beyond the specific algorithms, so I won't go into much details into specific algorithms.",
                    "label": 0
                },
                {
                    "sent": "But I try to cover more principles.",
                    "label": 0
                },
                {
                    "sent": "Can I do that, then wait to give you a more of an intuitive feeling than an informal, very formal mathematical presentation of those concepts?",
                    "label": 0
                },
                {
                    "sent": "Although there be a couple of formulas but?",
                    "label": 0
                },
                {
                    "sent": "OK, so let me start with the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very, very brief historical perspective on what machine learning is so historically.",
                    "label": 0
                },
                {
                    "sent": "Machine Learning was born from that very ambitious goal of creating an artificial intelligence.",
                    "label": 0
                },
                {
                    "sent": "At that time, that was called research on that was called cybernetics, I think, so some of you might have notice time already.",
                    "label": 0
                },
                {
                    "sent": "Half Cyborg that's happened like 3.",
                    "label": 0
                },
                {
                    "sent": "Few weeks ago, and apparently I still have to do that for another three weeks with a bad experiment on Montreal pavements.",
                    "label": 0
                },
                {
                    "sent": "But that's about it.",
                    "label": 0
                },
                {
                    "sent": "The founding projects that's often thought of being the founder for this field is Frank Rosenblatt, who invented an algorithm with Perceptron in 1957.",
                    "label": 0
                },
                {
                    "sent": "That was the first artificial neuron that learned from examples.",
                    "label": 0
                },
                {
                    "sent": "Cape and historically, there's been two opposed approaches to artificial intelligence.",
                    "label": 0
                },
                {
                    "sent": "One is a neuroscience is inspired.",
                    "label": 0
                },
                {
                    "sent": "And that's what we're talking about here, where we have these neural Nets that learning from examples used for artificial artificial perception and the other is what we call classical symbolic artificial intelligence, which primary focus was on the logical reasoning capabilities.",
                    "label": 0
                },
                {
                    "sent": "So what characterized that is that there was no learning.",
                    "label": 0
                },
                {
                    "sent": "Initially, it was mostly humans coding rules, and it had a poor handling of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So now overtime this kind of got fixed from that field.",
                    "label": 0
                },
                {
                    "sent": "We can trace back the event event of a Bayes net which are able probabilistic forms of reasoning and that got merged into.",
                    "label": 0
                },
                {
                    "sent": "Basically what we now call machine learning.",
                    "label": 0
                },
                {
                    "sent": "But what we can say from today is that learning approve astic models and pricing models, largely one those kind of battle and machine learning has now is not pervasive in all.",
                    "label": 0
                },
                {
                    "sent": "Walks almost walks of artificial intentions, let's say.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The view in the artificial intelligence in the 60s.",
                    "label": 1
                },
                {
                    "sent": "We could summarize it like this.",
                    "label": 0
                },
                {
                    "sent": "There was a computer science where artificial intelligence, which was largely a symbolic and neuroscience.",
                    "label": 0
                },
                {
                    "sent": "Inspired by the brain and at the interface between the two, we had artificial neural networks.",
                    "label": 0
                },
                {
                    "sent": "If I was to draw my current view of the founding disciplines of what is machine learning, you still have these two now in the middle.",
                    "label": 0
                },
                {
                    "sent": "This has a little bit split between artificial neural networks, which are more on the artificial intelligence side and computational neuroscience, which use try to simulate more of the real networks, but.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we found that a lot of what's been discovered had already been formalized in a nice mathematical way in statistics, and there's also been a lot of influence from physics, and most specifically at the interface of statistical physics.",
                    "label": 0
                },
                {
                    "sent": "That's why you sometimes hear about energy functions and.",
                    "label": 0
                },
                {
                    "sent": "Boltzmann machines and these things from statistical physics.",
                    "label": 1
                },
                {
                    "sent": "Related to physics and information theory and optimization control.",
                    "label": 0
                },
                {
                    "sent": "An at the interface of all this you have this field.",
                    "label": 0
                },
                {
                    "sent": "That's machine learning.",
                    "label": 0
                },
                {
                    "sent": "So that's a very exciting field, right?",
                    "label": 0
                },
                {
                    "sent": "It has all these influences.",
                    "label": 0
                },
                {
                    "sent": "I am.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that's my partial view.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to try to answer what is machine learning up from her house or historical perspective.",
                    "label": 0
                },
                {
                    "sent": "Other kind of a little bit of crazy users.",
                    "label": 0
                },
                {
                    "sent": "Professor perspective.",
                    "label": 0
                },
                {
                    "sent": "OK, and here I'll take on their user.",
                    "label": 0
                },
                {
                    "sent": "SPECT Evanthe perspective, and they had hypnotized users perspective so.",
                    "label": 0
                },
                {
                    "sent": "What I say is machine learning is really a scientific field, but really it's witchcraft.",
                    "label": 0
                },
                {
                    "sent": "Believe me, it's witchcraft.",
                    "label": 0
                },
                {
                    "sent": "It researches fundamental principles of what they really do is potions, and it develops magical algorithm.",
                    "label": 0
                },
                {
                    "sent": "Believe me, it spells to invoke capable of leveraging collected data to automatically produce accurate predictive functions applicable to similar data in the future.",
                    "label": 1
                },
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a crazy.",
                    "label": 0
                },
                {
                    "sent": "The user specific perspective you're happy with that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So from that the question given this, what is the key ingredient to machine learning?",
                    "label": 0
                },
                {
                    "sent": "Going to you.",
                    "label": 0
                },
                {
                    "sent": "Oh, I heard it.",
                    "label": 0
                },
                {
                    "sent": "Thank you data.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's what gets into the cauldron.",
                    "label": 0
                },
                {
                    "sent": "The magical cauldron, right?",
                    "label": 0
                },
                {
                    "sent": "And data is collected from nature or from industrial processes.",
                    "label": 1
                },
                {
                    "sent": "It comes in many forms and many formats.",
                    "label": 0
                },
                {
                    "sent": "Sometimes clean, if you're lucky, sometimes structured, unstructured, often messy, messy.",
                    "label": 0
                },
                {
                    "sent": "An in machine learning we like to view that data as a list of examples, and if it's not like that, trust us, we'll turn it into one.",
                    "label": 1
                },
                {
                    "sent": "Ideally, many examples of the same source and preferably with each example, a vector of numbers and again will first turn it into one if it isn't already.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "So here's what we work with.",
                    "label": 1
                },
                {
                    "sent": "We work with the training data, so training data.",
                    "label": 0
                },
                {
                    "sent": "It is made of several things.",
                    "label": 0
                },
                {
                    "sent": "A list of examples here we have N small and the number of examples.",
                    "label": 1
                },
                {
                    "sent": "And each example here.",
                    "label": 0
                },
                {
                    "sent": "That's a case of supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I'll get back to that is a pair of input.",
                    "label": 0
                },
                {
                    "sent": "And target input is what we observe here.",
                    "label": 1
                },
                {
                    "sent": "It's really visual images and target is what we'd like to predict, right?",
                    "label": 0
                },
                {
                    "sent": "Remember, we're building this crystal ball automatically automatically.",
                    "label": 0
                },
                {
                    "sent": "Same for a cat, same force.",
                    "label": 1
                },
                {
                    "sent": "OK, and that's our training data set, OK call IDN.",
                    "label": 0
                },
                {
                    "sent": "Here, because it has an examples.",
                    "label": 0
                },
                {
                    "sent": "So we like we first turn it into a nice datamatrix.",
                    "label": 1
                },
                {
                    "sent": "OK, that's the role of preprocessing or feature extraction, where will represent each row will now be vector, so there's part of that vector that corresponds to the input image.",
                    "label": 0
                },
                {
                    "sent": "An another little part here that corresponds to the target that want to predict.",
                    "label": 0
                },
                {
                    "sent": "Here is a two class problems classification problem, horse versus cat.",
                    "label": 0
                },
                {
                    "sent": "So we have plus one through to note the class of horse and minus one to denote the class of cat.",
                    "label": 0
                },
                {
                    "sent": "OK, so now it's a nice little matrix.",
                    "label": 0
                },
                {
                    "sent": "And the goal.",
                    "label": 1
                },
                {
                    "sent": "The goal of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Is to learn from that, or rather from that, 'cause that's what the computer gets to see.",
                    "label": 0
                },
                {
                    "sent": "Such in such a way that when you get shown a new test point that you've never seen never part of your training set initially.",
                    "label": 0
                },
                {
                    "sent": "You can predict the right class.",
                    "label": 0
                },
                {
                    "sent": "So how is this?",
                    "label": 0
                },
                {
                    "sent": "Use the test time you take that represented as a vector and hopefully you've learned a function here called F. Theta F is for function.",
                    "label": 0
                },
                {
                    "sent": "Theater is usually for its per meters, 'cause it's parameterized function.",
                    "label": 0
                },
                {
                    "sent": "That is going to map an input vector to hear a class at Target, so there's a dimension that important here.",
                    "label": 0
                },
                {
                    "sent": "An is what I call the number examples, and D is the input dimensionality.",
                    "label": 0
                },
                {
                    "sent": "That's the number of dimensions of your input vector.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so these are very important dimension ladies.",
                    "label": 0
                },
                {
                    "sent": "Number of examples.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you have several millions input dimensionality the between 100,000, sometimes more and the target dimensionality, here here for classification classes.",
                    "label": 1
                },
                {
                    "sent": "It's the number of classes that's relevant.",
                    "label": 1
                },
                {
                    "sent": "So data will often be in a matrix and times D + 1 or end times 2 plus.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "OK, now turning data into a nice list of examples, turning messy data into a nice list of examples.",
                    "label": 1
                },
                {
                    "sent": "That's more my practical side slide, so if you're ever in the situation where you have this wonderful Lee Overengineered database that you need to extract some data to learn something on OK, and it looks a little bit like that, so it's kind of a hopeless situation, really, that's the realm of data plumbing.",
                    "label": 0
                },
                {
                    "sent": "So you can have your favorite date of lumbar.",
                    "label": 0
                },
                {
                    "sent": "There's a name for that.",
                    "label": 0
                },
                {
                    "sent": "Is Dayton plumbing OK?",
                    "label": 0
                },
                {
                    "sent": "An turn it into a nice little list of examples like that.",
                    "label": 0
                },
                {
                    "sent": "So if you're ever in a situation to where you have to do that.",
                    "label": 0
                },
                {
                    "sent": "Try to get someone else to do it.",
                    "label": 0
                },
                {
                    "sent": "But if you can't, or if you have to guide that person, there's two questions that you should really be asking yourself, because it's really not trivial to turn this into that.",
                    "label": 0
                },
                {
                    "sent": "OK, the key question that you need to ask yourself is to decide what exactly what an example should be.",
                    "label": 0
                },
                {
                    "sent": "And there's a question regarding the input, and the input is ask yourself what is all the potentially relevant information that you will have at your disposal about the case when you will have to make a prediction about it.",
                    "label": 1
                },
                {
                    "sent": "At Test time for the future cases, now you have a test case.",
                    "label": 0
                },
                {
                    "sent": "What is all the information that's available at that time?",
                    "label": 1
                },
                {
                    "sent": "That should be your inputs over your examples and for target the important question target is what you want to predict is can you get your hands on many such examples that are actually labeled with prediction targets?",
                    "label": 0
                },
                {
                    "sent": "Always the case?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now the second part, so that's how to turn our thing into a list of examples.",
                    "label": 0
                },
                {
                    "sent": "The second part is turning an example into a vector.",
                    "label": 1
                },
                {
                    "sent": "OK, so there we can use what we call raw input.",
                    "label": 0
                },
                {
                    "sent": "Representation is an example of a of a bitmap image of a digit, so it's a list of grayscale values, light or light values between zero and 2:55, so it's naturally a vector.",
                    "label": 0
                },
                {
                    "sent": "It's almost probably already like that.",
                    "label": 0
                },
                {
                    "sent": "Or something close to that when it's stored in the computer.",
                    "label": 0
                },
                {
                    "sent": "Similarly here for sound file sound file, you can extract the magnitude here at every time step.",
                    "label": 0
                },
                {
                    "sent": "Or we can use some preprocessor presentation.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of of a spectrogram of that sound file, so that gives you another vector.",
                    "label": 1
                },
                {
                    "sent": "Here's an example for how we could represent a small sentence the cat jumped.",
                    "label": 1
                },
                {
                    "sent": "In what we call a bag of words or presentation.",
                    "label": 0
                },
                {
                    "sent": "So bag of water presentation is you take that whatever text and make it into a huge vector.",
                    "label": 0
                },
                {
                    "sent": "That's the size of the vocabulary that's filled with zeros but has once at the places where there was such a word in the sentence for the cat.",
                    "label": 0
                },
                {
                    "sent": "Or you can use hand engineered features or handcrafted features, in which case you have some way to extract some features that you think are relevant using some prior knowledge and you build your factor vector as a list of such features.",
                    "label": 0
                },
                {
                    "sent": "OK. Site.",
                    "label": 0
                },
                {
                    "sent": "So far so good.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far so good.",
                    "label": 0
                },
                {
                    "sent": "So here's this view.",
                    "label": 0
                },
                {
                    "sent": "We have this data set.",
                    "label": 0
                },
                {
                    "sent": "That we now have into a nice matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, an examples.",
                    "label": 0
                },
                {
                    "sent": "Each input has D dimension.",
                    "label": 1
                },
                {
                    "sent": "Here in this example we have 5 dimensions, so each X is 5 components.",
                    "label": 0
                },
                {
                    "sent": "Target the label Y.",
                    "label": 0
                },
                {
                    "sent": "Here is one of zero decided to use 1071 plus one.",
                    "label": 0
                },
                {
                    "sent": "So each example each row is a D + 1 dimensional vector here.",
                    "label": 0
                },
                {
                    "sent": "OK, input lost are yet, and that's one way to view your data.",
                    "label": 0
                },
                {
                    "sent": "Another very interesting way is to view it as points in a high dimensional vector space.",
                    "label": 0
                },
                {
                    "sent": "So that's an equivalent view concept of that same data matrix, right?",
                    "label": 0
                },
                {
                    "sent": "So here we've colored the points according to our target.",
                    "label": 0
                },
                {
                    "sent": "Label 01 and each point is in a small D dimensional space, right?",
                    "label": 0
                },
                {
                    "sent": "Each input is a point in a D dimensional vector space.",
                    "label": 1
                },
                {
                    "sent": "So this is your data.",
                    "label": 0
                },
                {
                    "sent": "This is another view of your data.",
                    "label": 0
                },
                {
                    "sent": "Of course we can't display dimension or even imagine them in our head, but conceptually we can think that we're dealing with points in the D dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, we can plot two.",
                    "label": 0
                },
                {
                    "sent": "That's nice two or three or.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's an important view because it gives us a lot of intuition about some algorithms.",
                    "label": 0
                },
                {
                    "sent": "OK, so this geometric view of the data and other problems.",
                    "label": 0
                },
                {
                    "sent": "So here is an example of the nearest neighbor classifier.",
                    "label": 0
                },
                {
                    "sent": "So nearest neighbor classifier is very simple.",
                    "label": 0
                },
                {
                    "sent": "We have our training data set here.",
                    "label": 0
                },
                {
                    "sent": "The blue and red points and our input space.",
                    "label": 0
                },
                {
                    "sent": "And were shown in you test point.",
                    "label": 0
                },
                {
                    "sent": "Remember the goal is to predict for a new test point.",
                    "label": 0
                },
                {
                    "sent": "So what is the nearest neighbor algorithm, very simple.",
                    "label": 0
                },
                {
                    "sent": "Four test point X here in green.",
                    "label": 1
                },
                {
                    "sent": "Find the nearest neighbor of X among their training set.",
                    "label": 1
                },
                {
                    "sent": "That would be that guy according to some distance measure, so that's to be specified.",
                    "label": 0
                },
                {
                    "sent": "And simply predict that X is the same class as its neighbor will predict blue class one of the simplest algorithm based on this geometric view of things.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, machine learning is traditionally viewed as a different set of tasks.",
                    "label": 0
                },
                {
                    "sent": "We have supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And enterprise learning and reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And there's also this thing called semi supervised learning which is kind of a.",
                    "label": 0
                },
                {
                    "sent": "Improvement on supervisor in school.",
                    "label": 0
                },
                {
                    "sent": "It this way predicts the goal is to predict the target Y from input X, so we have an explicit target.",
                    "label": 1
                },
                {
                    "sent": "Was an unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "We have no explicit prediction target.",
                    "label": 0
                },
                {
                    "sent": "That's the key difference.",
                    "label": 0
                },
                {
                    "sent": "And reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "I won't cover it at all here.",
                    "label": 0
                },
                {
                    "sent": "It's a more general framework and more.",
                    "label": 0
                },
                {
                    "sent": "Complicated.",
                    "label": 0
                },
                {
                    "sent": "More sophisticated say.",
                    "label": 0
                },
                {
                    "sent": "That's users that can use all of this to good effect.",
                    "label": 0
                },
                {
                    "sent": "So they the most classical tasks in supervised learning are.",
                    "label": 0
                },
                {
                    "sent": "Called classification and regression classification is when Y the prediction target.",
                    "label": 0
                },
                {
                    "sent": "We want to predict represents a category of class and regression is when Y is a real number.",
                    "label": 1
                },
                {
                    "sent": "So in classification will further distinguish between binary classification where you have to discriminate between two classes, just two classes.",
                    "label": 0
                },
                {
                    "sent": "So often the class labels will be represented either as minus 1 + 1 or 01.",
                    "label": 0
                },
                {
                    "sent": "And the multiclass case in which your class label might be between one and M, 'cause you have M classes or zero and minus one, and we have regression where trica single real number or several real numbers multiple regression.",
                    "label": 0
                },
                {
                    "sent": "And so, in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'd see I'd say the mother of all unsupervised learning problems, or maybe of all machine learning problems will be probably density estimation.",
                    "label": 0
                },
                {
                    "sent": "We want to model the probability distribution X.",
                    "label": 0
                },
                {
                    "sent": "That's very hard, but if we can do that, we can do a lot of other things or discovering underlying structure of data.",
                    "label": 0
                },
                {
                    "sent": "So that will be clustering in groups, dimensionality reduction and an supervisor representation.",
                    "label": 0
                },
                {
                    "sent": "Learning that you'll hear a lot about this summer school.",
                    "label": 0
                },
                {
                    "sent": "So this is it this supervised learning sometimes called predictive modeling.",
                    "label": 0
                },
                {
                    "sent": "An unsupervised learning, descriptive modeling.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now in learning there is typically a two faces.",
                    "label": 0
                },
                {
                    "sent": "There is what we call the training phase, where we learn a predictive function F Theta.",
                    "label": 0
                },
                {
                    "sent": "By optimizing it so that it predicts well on the training set.",
                    "label": 1
                },
                {
                    "sent": "OK. And there is using for prediction where we can use the F data on new test inputs.",
                    "label": 1
                },
                {
                    "sent": "That were not part of the training set.",
                    "label": 1
                },
                {
                    "sent": "So the goal of learning is not to learn perfectly, memorize the training set.",
                    "label": 1
                },
                {
                    "sent": "That's not our goal.",
                    "label": 0
                },
                {
                    "sent": "The goal is what's important is the ability to generalize well on new future cases on you test cases so.",
                    "label": 0
                },
                {
                    "sent": "As I said in the beginning, I suppose that a lot of what I presented so far is pretty basic and get a little more involved in awhile, but it's probably things that you're familiar with already, but it's good to see.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think so.",
                    "label": 0
                },
                {
                    "sent": "Here's our an example of 1D regression.",
                    "label": 1
                },
                {
                    "sent": "Here we have our inputs.",
                    "label": 0
                },
                {
                    "sent": "That's going to be 1D, so just a scalar real value.",
                    "label": 0
                },
                {
                    "sent": "And a target that's another scalar real value, so we're given so the first step is to collect some training data.",
                    "label": 0
                },
                {
                    "sent": "So we find some training data.",
                    "label": 1
                },
                {
                    "sent": "So that's set of data that associate's to a number of input locations.",
                    "label": 0
                },
                {
                    "sent": "Here a target value OK target the label value.",
                    "label": 0
                },
                {
                    "sent": "So that's all training sets here.",
                    "label": 0
                },
                {
                    "sent": "For 1D regression problem.",
                    "label": 0
                },
                {
                    "sent": "Basically it's all these these 12345 examples.",
                    "label": 0
                },
                {
                    "sent": "Of input target pairs.",
                    "label": 0
                },
                {
                    "sent": "2nd, we learn a function that's from this data that's going to predict input to target.",
                    "label": 1
                },
                {
                    "sent": "So the input to target mapping.",
                    "label": 0
                },
                {
                    "sent": "See learn that function.",
                    "label": 0
                },
                {
                    "sent": "And third, once we've learned the function well, usually we can.",
                    "label": 0
                },
                {
                    "sent": "We have called FT to.",
                    "label": 0
                },
                {
                    "sent": "Usually we can get rid of the training points, right?",
                    "label": 0
                },
                {
                    "sent": "Don't need them anymore.",
                    "label": 0
                },
                {
                    "sent": "We have a function, will learn based on them.",
                    "label": 0
                },
                {
                    "sent": "It summarizes all the knowledge, all the relevant nodes, all the nodes that we deemed relevant about it.",
                    "label": 0
                },
                {
                    "sent": "And now we can use it so that the first phase was a training.",
                    "label": 0
                },
                {
                    "sent": "Now we can use it user learn function on new inputs.",
                    "label": 1
                },
                {
                    "sent": "That's going to be our test points are probably our new cases in the in the in the field.",
                    "label": 0
                },
                {
                    "sent": "So these are new points that were not necessary part, probably not part of the training data and that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can now use our function.",
                    "label": 0
                },
                {
                    "sent": "Sorry oh that's going to be long.",
                    "label": 0
                },
                {
                    "sent": "Oh no, it's OK. Hey, you got the idea.",
                    "label": 0
                },
                {
                    "sent": "So here's another way to to picture what's happening.",
                    "label": 0
                },
                {
                    "sent": "How are we going to learn that function?",
                    "label": 0
                },
                {
                    "sent": "We have our training set here.",
                    "label": 0
                },
                {
                    "sent": "And the function we we can represent it as being trained such that when we give it inputs from the training set.",
                    "label": 0
                },
                {
                    "sent": "It's going to map that input to some output F of X and that output.",
                    "label": 0
                },
                {
                    "sent": "Is going to be compared with the target the prediction target in the training set and this comparison is done with something that we call a loss function or or cost.",
                    "label": 0
                },
                {
                    "sent": "That we call that we write L here.",
                    "label": 0
                },
                {
                    "sent": "That's going to measure the discrepancy between our prediction FT devex and the true target that we would like it to predict and learning will adapt.",
                    "label": 0
                },
                {
                    "sent": "Find the function or adapt its parameters in such a way that this is minimized.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now a machine learning algorithm an you can view almost all machine learning algorithms this way.",
                    "label": 0
                },
                {
                    "sent": "They typically composed of the following three elements.",
                    "label": 0
                },
                {
                    "sent": "The choice of a specific function family F. Often it's a parameterized family.",
                    "label": 1
                },
                {
                    "sent": "A way to evaluate the quality of a function small F in that family.",
                    "label": 1
                },
                {
                    "sent": "So that's going typically to use a cost or loss function as just showed, measuring how wrongly F predicts.",
                    "label": 0
                },
                {
                    "sent": "And a way to search for the best function F in that family.",
                    "label": 0
                },
                {
                    "sent": "So that's searching for the best typically optimization problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so optimization of function parameters if we have a permit rest function.",
                    "label": 0
                },
                {
                    "sent": "Oh, sorry so.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I use the keys, it works better.",
                    "label": 0
                },
                {
                    "sent": "OK, so don't want to get get back to.",
                    "label": 0
                },
                {
                    "sent": "I'm going in turn a look at these two aspects about how to evaluate the quality of function and search for the best function, and then I'll spend a serious amount of time on specifying the function family F.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so evaluating the quality and searching for the best function.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we evaluate the predictor function F, the full performance is often evaluated using several different evaluation metrics.",
                    "label": 1
                },
                {
                    "sent": "And that's important among those evaluation metrics, there are the often the true quantities of interest, and these are often ultimately expressed in dollars saved or number of life saved or this kind of quantity of real world interest when using the predictor inside the more complicated system in which it's designed to work.",
                    "label": 0
                },
                {
                    "sent": "So that's what we ideally want to optimize, right?",
                    "label": 0
                },
                {
                    "sent": "Not always easy.",
                    "label": 1
                },
                {
                    "sent": "There's a standard evaluation metrics in this specific field.",
                    "label": 0
                },
                {
                    "sent": "If you're working in.",
                    "label": 0
                },
                {
                    "sent": "Machine translation, for instance, there's a blue scores.",
                    "label": 1
                },
                {
                    "sent": "That may or may not be a super accurate for other purposes, but it's standard in the field, so you have to compare to it.",
                    "label": 0
                },
                {
                    "sent": "There's things like misclassification error rate for a classifier, so that's a much lower thing, but you have other choices like precision, recall, or F score.",
                    "label": 0
                },
                {
                    "sent": "How to evaluate a classifier?",
                    "label": 0
                },
                {
                    "sent": "And then there's this other thing.",
                    "label": 0
                },
                {
                    "sent": "That's the loss that you're actually optimizing.",
                    "label": 1
                },
                {
                    "sent": "OK, by the machine learning algorithm, which may often be very different from all the above.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll give a short example in a minute.",
                    "label": 0
                },
                {
                    "sent": "So here are the standard loss functions for the tasks that I just described.",
                    "label": 0
                },
                {
                    "sent": "Most important task for density estimation F will represent the probability density function or sometimes a probability mass function, and then the loss that will use is negative log likelihood loss.",
                    "label": 1
                },
                {
                    "sent": "So that's simply taking minus the log of F and minimizing that.",
                    "label": 0
                },
                {
                    "sent": "That's one way for regression tasks.",
                    "label": 0
                },
                {
                    "sent": "That's probably the easiest and.",
                    "label": 1
                },
                {
                    "sent": "The simplest, the squared error loss is simply.",
                    "label": 0
                },
                {
                    "sent": "Taking the difference between your prediction target Y you label Y.",
                    "label": 0
                },
                {
                    "sent": "And your whatever your function predicted.",
                    "label": 0
                },
                {
                    "sent": "To the square, to the power of 2.",
                    "label": 0
                },
                {
                    "sent": "So the further your prediction is from Y, the more you pay as a cost.",
                    "label": 1
                },
                {
                    "sent": "And for classification tasks for classification task, remember we map some our input or D dimensional input vector to a class number.",
                    "label": 0
                },
                {
                    "sent": "Well, the misclassification error rate.",
                    "label": 0
                },
                {
                    "sent": "Miss gas station error loss as you pay one.",
                    "label": 0
                },
                {
                    "sent": "If your predictor F of X didn't predict the correct class.",
                    "label": 0
                },
                {
                    "sent": "OK, and you pay 0 otherwise that's easy.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Now here is a leader for classification tasks.",
                    "label": 0
                },
                {
                    "sent": "This simple misclassification error loss OK, well, it's not super usable.",
                    "label": 1
                },
                {
                    "sent": "Actually, it's hard to optimize directly becausw first if you try to find a gradient, you have a gradient of zero everywhere.",
                    "label": 0
                },
                {
                    "sent": "And even if you think of linear classifiers and actually NP hard problem, if it's not linearly separable, so such a simple thing, right?",
                    "label": 0
                },
                {
                    "sent": "OK, can't optimize.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we must use a surrogate loss, so that's not necessarily the real thing that we want, right?",
                    "label": 1
                },
                {
                    "sent": "But it's something that's hopefully correlated to what we want.",
                    "label": 0
                },
                {
                    "sent": "So here's some further distinctions.",
                    "label": 1
                },
                {
                    "sent": "I maybe I won't spend much too much time on that.",
                    "label": 1
                },
                {
                    "sent": "I again there's this binary classifier, multiclass classifier.",
                    "label": 0
                },
                {
                    "sent": "Depending on how you represent your classes.",
                    "label": 0
                },
                {
                    "sent": "And then there's the second probabilistic classifiers on non probabilistic classifiers.",
                    "label": 0
                },
                {
                    "sent": "So for binary classifier that's probabilistic usually.",
                    "label": 0
                },
                {
                    "sent": "I will use a binary cross entropy loss that the formula is given here, and that's nice when it's promising classifier by that what I mean by posting binary classifier is what you actually get as output function is G. Here that gives you the probability for class 0 or class one, probably that your label is 1 given your input and three days the other classes 1 -- G. Right, and then the decision function.",
                    "label": 1
                },
                {
                    "sent": "The F. Actually, if you wanted to compute this misclassification error loss, you can get from G by simply looking is the probability of having class one greater than 0.5, right?",
                    "label": 0
                },
                {
                    "sent": "So that's indicator of G of X greater than 0.5.",
                    "label": 1
                },
                {
                    "sent": "Is that clear question?",
                    "label": 0
                },
                {
                    "sent": "OK, now if we look at binary classifiers not not, not all boundaries affairs are probabilistic.",
                    "label": 0
                },
                {
                    "sent": "One notable example is for instance the support vector machine is not a probabilistic classifier, at least not initially in its initial formulation.",
                    "label": 0
                },
                {
                    "sent": "So here what you get is not a probability.",
                    "label": 0
                },
                {
                    "sent": "G doesn't give you a probability for class one.",
                    "label": 1
                },
                {
                    "sent": "It gives you kind of score a real valued score, right?",
                    "label": 0
                },
                {
                    "sent": "And the score for the other classes.",
                    "label": 0
                },
                {
                    "sent": "The negation of it.",
                    "label": 1
                },
                {
                    "sent": "And what's used in an SVM is the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "So it's a little different here.",
                    "label": 0
                },
                {
                    "sent": "LA rectifier of sorts.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And similarly for multiclass classifiers, well, you have the probabilistic classifier case, and you have the non probability classifier caseware multiclass margin losses generalization of the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "But my point here, they take home messages.",
                    "label": 0
                },
                {
                    "sent": "There's several loss functions that you may want to to monitor.",
                    "label": 0
                },
                {
                    "sent": "OK, and there's some that you're learning algorithm will actually be able to optimize, but they're not necessarily the ones that you're ultimately interested in, maybe not even shortly interested, and the ones that you're really ultimately interested in is probably dollars or number of life saved.",
                    "label": 0
                },
                {
                    "sent": "So the better closer you can optimize, that better.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now I get into.",
                    "label": 0
                },
                {
                    "sent": "Expected risk versus empirical risk.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "From now on, we'll suppose that are examples that we get in our training set are drawn identically and independently, identically distributed from an unknown true distribution P of XY.",
                    "label": 0
                },
                {
                    "sent": "It's unknown as a distribution in nature.",
                    "label": 0
                },
                {
                    "sent": "The distribution of images of cats and of cats and horses, or from some industrial process.",
                    "label": 0
                },
                {
                    "sent": "The distribution of whatever characteristics your customers have OK or whatever web pages they browse are.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea assumption is very strong, it's it's can be.",
                    "label": 0
                },
                {
                    "sent": "It's not necessary, but it makes our exposition simple, simpler here.",
                    "label": 0
                },
                {
                    "sent": "So this is what we call the generalization error.",
                    "label": 0
                },
                {
                    "sent": "Another name for it will be the expected risk, or sometimes just risk.",
                    "label": 0
                },
                {
                    "sent": "And here's a layman's way of understanding it is how poorly we will do our predictor will do on average on the Infinity of future examples from that unknown distribution.",
                    "label": 1
                },
                {
                    "sent": "Remember our goal is generalization, right?",
                    "label": 0
                },
                {
                    "sent": "So as the future of the future example to Infinity of future examples drawn from that same distribution, how poor you are going to do on them, and so that's expressed like this.",
                    "label": 0
                },
                {
                    "sent": "This risk of the predictor function.",
                    "label": 0
                },
                {
                    "sent": "F is going to be the expectation for X / X and Y is drawn from the.",
                    "label": 0
                },
                {
                    "sent": "Probably distribution P off this loss discrepancy between our prediction an the true label little target.",
                    "label": 0
                },
                {
                    "sent": "OK, generalization error.",
                    "label": 0
                },
                {
                    "sent": "Expected risk.",
                    "label": 0
                },
                {
                    "sent": "Now there's another notion that's highly rated as empirical risk.",
                    "label": 1
                },
                {
                    "sent": "Empirical risk will be the average loss on a finite data set.",
                    "label": 1
                },
                {
                    "sent": "So in layman terms at how poorly we're doing on average, so it's not an expectation, is an average that's closely related, but not the same.",
                    "label": 0
                },
                {
                    "sent": "So how poorly we getting doing on average on this finite data set?",
                    "label": 0
                },
                {
                    "sent": "So that's a difference.",
                    "label": 0
                },
                {
                    "sent": "Here we have a finite data set here.",
                    "label": 0
                },
                {
                    "sent": "It's over the distribution or another way to say is overall the infinite number of examples that you get in the future.",
                    "label": 0
                },
                {
                    "sent": "So here is just simply an average over of the loss.",
                    "label": 0
                },
                {
                    "sent": "Over a given data set.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Expect risk and picaresque now.",
                    "label": 0
                },
                {
                    "sent": "There's a big principle as the principle of empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "So the examples here again are supposed to draw from their unknown true distribution.",
                    "label": 1
                },
                {
                    "sent": "And would love to find a predictor that minimizes the generalization error.",
                    "label": 1
                },
                {
                    "sent": "OK, that's our goal is to generalize well, so we'd like to find the function our predictor that minimize the generalization error that minimizes the expected risk.",
                    "label": 1
                },
                {
                    "sent": "But we can't even compute that expected risk.",
                    "label": 1
                },
                {
                    "sent": "Remember, it's an expectation over an unknown distribution, we can't compute it exactly.",
                    "label": 0
                },
                {
                    "sent": "So we can do something that's kind of closer to it.",
                    "label": 0
                },
                {
                    "sent": "It's the use the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "So the principle that we're going to use to actually.",
                    "label": 0
                },
                {
                    "sent": "Learn.",
                    "label": 0
                },
                {
                    "sent": "A function, find the function, find the parameters of a function.",
                    "label": 1
                },
                {
                    "sent": "Is the empirical risk minimization principle.",
                    "label": 0
                },
                {
                    "sent": "And layman terms is final predictor that minimizes the average loss over a specific data set that we're going to call the training except.",
                    "label": 0
                },
                {
                    "sent": "So here's the equation.",
                    "label": 1
                },
                {
                    "sent": "Remember our heart is our empirical risk, so we're going to search for the function small F in our family function.",
                    "label": 0
                },
                {
                    "sent": "That's going to minimize the empirical risk over the training set, so that's a function that was going to make the smallest number of errors in average on the training set.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's what we call F hat of the train.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the optimal function when that's doing the least errors on the training set.",
                    "label": 0
                },
                {
                    "sent": "So that's the training phase in machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, once we have found the predictor by training it, we will also want to evaluate how good it is, right?",
                    "label": 0
                },
                {
                    "sent": "So the problem and how good it is?",
                    "label": 0
                },
                {
                    "sent": "Again, it's a generalization error that we'd like to have, so we can't compute again.",
                    "label": 0
                },
                {
                    "sent": "Their expected risk of a function F, but our empirical risk here of F. / A set D is a good estimate.",
                    "label": 1
                },
                {
                    "sent": "Would say it's an unbiased estimate of our of F of the generalization error of the expected risk.",
                    "label": 0
                },
                {
                    "sent": "Empirical risk over the good estimate of.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Expected risk OK?",
                    "label": 1
                },
                {
                    "sent": "But the conditions for that provided D was not used to find the choose F. In any way OK?",
                    "label": 0
                },
                {
                    "sent": "Otherwise our estimate would be biased, so we can't be using the training set here to estimate how well a function that was trained on the training set does.",
                    "label": 0
                },
                {
                    "sent": "And provide also Dr. Our set that we use here is large enough, otherwise their estimate will be too noisy.",
                    "label": 0
                },
                {
                    "sent": "OK, and on from P obviously.",
                    "label": 0
                },
                {
                    "sent": "So the logical conclusion of that is that we must keep a separate test set that's different from the training set to probably estimate the generalization error of F of the train.",
                    "label": 1
                },
                {
                    "sent": "So here's a very imprecise approximate.",
                    "label": 0
                },
                {
                    "sent": "OK, this can be made more precise.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's it's an unbiased estimate.",
                    "label": 0
                },
                {
                    "sent": "But the risks, the expected risk, so the generalization error.",
                    "label": 0
                },
                {
                    "sent": "Of your function, F hat that you optimize on your training set.",
                    "label": 0
                },
                {
                    "sent": "This expected risk.",
                    "label": 0
                },
                {
                    "sent": "You can get a good unbiased estimate of it by.",
                    "label": 0
                },
                {
                    "sent": "Running it and measuring its empirical risk on the test set, yes.",
                    "label": 0
                },
                {
                    "sent": "OK, it means.",
                    "label": 0
                },
                {
                    "sent": "It means that if you sample.",
                    "label": 0
                },
                {
                    "sent": "Lots of such test sets.",
                    "label": 0
                },
                {
                    "sent": "OK. And there's two ways to see it.",
                    "label": 0
                },
                {
                    "sent": "If you were to use a test set that whose number of examples would go to Infinity that you would have an equal here.",
                    "label": 0
                },
                {
                    "sent": "OK, as in R as a test set grows, you have an equal.",
                    "label": 0
                },
                {
                    "sent": "So as a test set grows, this converges to the.",
                    "label": 0
                },
                {
                    "sent": "So the empirical risk converges to the expected risk.",
                    "label": 0
                },
                {
                    "sent": "Another way to see it is as if you were to repeat that by sampling.",
                    "label": 0
                },
                {
                    "sent": "If you have.",
                    "label": 0
                },
                {
                    "sent": "If you use a finite test, sets A of 10 examples.",
                    "label": 0
                },
                {
                    "sent": "OK, but you repeat that by sampling new test set of 10 example every time and you average that you'll get to the expected risk as well.",
                    "label": 0
                },
                {
                    "sent": "That's what's unbiased here.",
                    "label": 0
                },
                {
                    "sent": "Answer question.",
                    "label": 0
                },
                {
                    "sent": "I can read more.",
                    "label": 0
                },
                {
                    "sent": "More more, more more formal with a limit than everything expression, but OK.",
                    "label": 0
                },
                {
                    "sent": "The practical use here is remember, I'm trying to demystify the mystifier hypnotised users perspective on things and getting you the intuition.",
                    "label": 0
                },
                {
                    "sent": "So the point here is you don't want to use the D train here.",
                    "label": 0
                },
                {
                    "sent": "OK, that's otherwise you won't get a good estimate.",
                    "label": 0
                },
                {
                    "sent": "So this here is really the test phase in machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and then this gives us a simple train test procedure.",
                    "label": 0
                },
                {
                    "sent": "Which provided large enough data, set D drawing from that distribution.",
                    "label": 0
                },
                {
                    "sent": "First you need to make sure that your examples are in a random order.",
                    "label": 1
                },
                {
                    "sent": "Otherwise you're going to have problems.",
                    "label": 0
                },
                {
                    "sent": "That's EII did represent the ID thing and split that data set into a training set part, an test set.",
                    "label": 0
                },
                {
                    "sent": "So these are two subsets of your original data.",
                    "label": 0
                },
                {
                    "sent": "Use a D train to optimize.",
                    "label": 1
                },
                {
                    "sent": "Find the best predictor F = F hat of the train.",
                    "label": 0
                },
                {
                    "sent": "That's a training phase.",
                    "label": 0
                },
                {
                    "sent": "Optimizing to find the function that makes the fewest errors according to our loss on the training set.",
                    "label": 1
                },
                {
                    "sent": "And once you've done that, you found that optimal predictor use the test set to evaluate generalization performance other predictor.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'm coming to what is probably the most important.",
                    "label": 0
                },
                {
                    "sent": "Whatever the most important aspect of that that unites all machine learning.",
                    "label": 0
                },
                {
                    "sent": "Enough of this presentation is how to choose a specific function family F. OK, I remember there was our Third Point.",
                    "label": 1
                },
                {
                    "sent": "We had three things we needed a way to evaluate a predicting predictor function.",
                    "label": 0
                },
                {
                    "sent": "We needed a way to find the best predictor function among a family, but we haven't talked about family yet.",
                    "label": 0
                },
                {
                    "sent": "So how to choose a specific function?",
                    "label": 0
                },
                {
                    "sent": "Family F.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To do that?",
                    "label": 0
                },
                {
                    "sent": "First, let me give you well.",
                    "label": 0
                },
                {
                    "sent": "First, let me ask you a question.",
                    "label": 0
                },
                {
                    "sent": "What is the simple simplest predictor FET events that you can think of?",
                    "label": 0
                },
                {
                    "sent": "OK, I heard it.",
                    "label": 0
                },
                {
                    "sent": "I heard constant, so the simplest is really constant, right constant predictor.",
                    "label": 0
                },
                {
                    "sent": "So I call this family F constant constant predictor is a predictor that always give you the same answer regardless of X.",
                    "label": 0
                },
                {
                    "sent": "It's always going to answer B.",
                    "label": 0
                },
                {
                    "sent": "So it is a permit, parameterized function family.",
                    "label": 0
                },
                {
                    "sent": "It has a parimeter.",
                    "label": 0
                },
                {
                    "sent": "It's what constant should it answer?",
                    "label": 0
                },
                {
                    "sent": "You can learn that parameter right.",
                    "label": 0
                },
                {
                    "sent": "You can learn it.",
                    "label": 0
                },
                {
                    "sent": "So if you have a classification problem, say what should a constant with the optimal concentrate to be?",
                    "label": 0
                },
                {
                    "sent": "What would answer?",
                    "label": 0
                },
                {
                    "sent": "The majority class which class appears more often in your training set, right?",
                    "label": 0
                },
                {
                    "sent": "That's the predictor which is going to make the fewest errors on your training set.",
                    "label": 0
                },
                {
                    "sent": "Among that family.",
                    "label": 0
                },
                {
                    "sent": "So that's a constant predictor.",
                    "label": 0
                },
                {
                    "sent": "If we have a regression problem, what should be the constant predictor?",
                    "label": 0
                },
                {
                    "sent": "The average of the targets of all your training set points.",
                    "label": 0
                },
                {
                    "sent": "Which other targets labels.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have this family.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty small family.",
                    "label": 0
                },
                {
                    "sent": "We have slightly bigger family that I also heard.",
                    "label": 0
                },
                {
                    "sent": "That's called F linear.",
                    "label": 0
                },
                {
                    "sent": "It's actually a slight abuse because I also include the affine in their off and put them together.",
                    "label": 0
                },
                {
                    "sent": "Linear and affine collinear is usually a fine fine predictor.",
                    "label": 0
                },
                {
                    "sent": "So in one dimension there will be a function F. That computer W * X + B so X is a scalar.",
                    "label": 0
                },
                {
                    "sent": "Multiply it by some weight W an add this bias B and the parameters of that function family RWNBW is the same dimension as your input and be a real.",
                    "label": 0
                },
                {
                    "sent": "So indeed I mentioned you have a dot product here.",
                    "label": 0
                },
                {
                    "sent": "OK, so the constant is included in that family, right?",
                    "label": 0
                },
                {
                    "sent": "If you put W 0 you get the constant predictor.",
                    "label": 0
                },
                {
                    "sent": "So slightly bigger than that.",
                    "label": 0
                },
                {
                    "sent": "Logic follows the logic.",
                    "label": 0
                },
                {
                    "sent": "Polynomial thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, so linear is a special case of a polynomial predictor.",
                    "label": 0
                },
                {
                    "sent": "Well, number predictor of degree P. OK, here I write it in one dimension so you still have your BUX.",
                    "label": 0
                },
                {
                    "sent": "Is is just a scalar and you have all these parameters?",
                    "label": 0
                },
                {
                    "sent": "A 18283 that are the coefficients with the exponent of of X?",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "How many scalars do I have in one dimension in the parameters?",
                    "label": 0
                },
                {
                    "sent": "P + 1 OK.",
                    "label": 0
                },
                {
                    "sent": "Here how many I had.",
                    "label": 0
                },
                {
                    "sent": "I had just two, right?",
                    "label": 0
                },
                {
                    "sent": "WNB and the one dimension.",
                    "label": 0
                },
                {
                    "sent": "Now if you move to two dimensions, OK, give me an order of the number of.",
                    "label": 0
                },
                {
                    "sent": "Of four meters.",
                    "label": 0
                },
                {
                    "sent": "I say degree P in D dimension.",
                    "label": 1
                },
                {
                    "sent": "I don't want to precise answer.",
                    "label": 0
                },
                {
                    "sent": "K. PC P2D OKP, 2D.",
                    "label": 0
                },
                {
                    "sent": "So if you're if you're in 100 dimensions, your D OK, and even the polynomial of degree two is grows pretty fast, right?",
                    "label": 0
                },
                {
                    "sent": "Do you want to know what degree three you get lots and lots of parameters?",
                    "label": 0
                },
                {
                    "sent": "Just again, I guess that's one of the problems with problem predictor and we see some consequences of that later.",
                    "label": 0
                },
                {
                    "sent": "Sorry, wrong key.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, yeah just here.",
                    "label": 0
                },
                {
                    "sent": "Just wanted to show the examples so forget the blue curve.",
                    "label": 0
                },
                {
                    "sent": "The blue curve is actually what the examples were kind of drawn from so the examples is X target Y OK. And here's what a linear predictor the function they will learn it can learn.",
                    "label": 0
                },
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "That thing is not working very well.",
                    "label": 0
                },
                {
                    "sent": "Sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "OK, and here here you have a polynomial predictor degree two or three.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this brings me to the key notion of this talk, and that's the notion of the capacity of a learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Please raise your hand who has heard of the notion of capacity.",
                    "label": 0
                },
                {
                    "sent": "I don't see all hands race, so my presentation won't be totally useless OK?",
                    "label": 0
                },
                {
                    "sent": "On so, the notion of capacity.",
                    "label": 0
                },
                {
                    "sent": "Is a following when you choose a specific machine learning algorithm OK?",
                    "label": 0
                },
                {
                    "sent": "That means that you're choosing a specific function family F. Remember that one of the constituents of learning algorithm is a specific function family, family, F. Now we can say that this family, as I showed some example here.",
                    "label": 0
                },
                {
                    "sent": "You can you can picture this informal notion, but you can imagine formally there's a size to that family function as we solve for the constant predicted linear predicted the polynomial predictor.",
                    "label": 0
                },
                {
                    "sent": "So how big that family is, we can also see how rich how flexible, how expressive, how complex it is defines what we informally call the capacity of the machine learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here in our example, the capacity of a polynomial of degree three or predictor would be greater, larger than the capacity of a linear predictor.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what can actually come up with several formal measures of capacity for a function family learning algorithm?",
                    "label": 1
                },
                {
                    "sent": "One such formal measure is for classic classifiers.",
                    "label": 0
                },
                {
                    "sent": "Is the VC dimension vector analysis dimension, but that you can come up with many others.",
                    "label": 1
                },
                {
                    "sent": "So what's important here is to get the notion of the kind of informal notion of capacity.",
                    "label": 0
                },
                {
                    "sent": "So one rule of thumb to estimate the capacity of a function family F or learning algorithm, which is kind of the same, will also call that the model is the number of adaptable para meters, the number of scalars it has insight.",
                    "label": 0
                },
                {
                    "sent": "That's a little bit the exercise that we did.",
                    "label": 0
                },
                {
                    "sent": "You know we're looking at the concept is just one.",
                    "label": 0
                },
                {
                    "sent": "The linear has well in the dimension as D + 1 and the polynomial has depending on the grill promo.",
                    "label": 0
                },
                {
                    "sent": "Many minutes expansion of degree.",
                    "label": 0
                },
                {
                    "sent": "OK, so the number of adaptable para meters, but that's just the rule of thumb of proxy.",
                    "label": 0
                },
                {
                    "sent": "It's kind of very imprecise.",
                    "label": 0
                },
                {
                    "sent": "So how many skill levels are continuing into?",
                    "label": 0
                },
                {
                    "sent": "So there's one notable notable exception to that.",
                    "label": 0
                },
                {
                    "sent": "Rule of thumb.",
                    "label": 0
                },
                {
                    "sent": "Can someone tell me?",
                    "label": 0
                },
                {
                    "sent": "I wouldn't say news name, so in my view nearest neighbors has many, many parameters because it's para meters are the training set.",
                    "label": 0
                },
                {
                    "sent": "That's what it learns that what it needs to make a prediction.",
                    "label": 0
                },
                {
                    "sent": "The whole training set so it has many.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Sorry, So what I'm hinting at here is simply, if you're familiar with neural networks or layered architectures.",
                    "label": 0
                },
                {
                    "sent": "If you compose multiple linear transformations, OK, you get what.",
                    "label": 0
                },
                {
                    "sent": "A linear transformation.",
                    "label": 0
                },
                {
                    "sent": "So if I take 1000 matrices OK, an multiply them together and that's my composition, I have 1000 times more parameter, but is still a linear model.",
                    "label": 0
                },
                {
                    "sent": "OK, so I haven't increased that.",
                    "label": 0
                },
                {
                    "sent": "So just a real thump.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this notion of capacity now introduced some even more fuzzier notion.",
                    "label": 0
                },
                {
                    "sent": "The notion of effective capacity.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's related also to what I call capacity control hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So we have this notion of capacity, that's the size.",
                    "label": 0
                },
                {
                    "sent": "If we want off of our function family F. But the effective capacity of machine learning is the controlled by some couple of other things.",
                    "label": 1
                },
                {
                    "sent": "So there's a choice of machine learning algorithm or.",
                    "label": 0
                },
                {
                    "sent": "That essentially determines the big family Big F. But there's other things.",
                    "label": 0
                },
                {
                    "sent": "There's hyperparameters that further specify F. So a simple example in what we saw is the degree P of a polynomial, right?",
                    "label": 1
                },
                {
                    "sent": "We said we have a family.",
                    "label": 0
                },
                {
                    "sent": "That's the polynomials.",
                    "label": 0
                },
                {
                    "sent": "OK, fine.",
                    "label": 0
                },
                {
                    "sent": "But when you say you haven't specified degree yet, right?",
                    "label": 0
                },
                {
                    "sent": "So you can specify the degree of the polynomial.",
                    "label": 0
                },
                {
                    "sent": "OK, that will further restrict your effective.",
                    "label": 1
                },
                {
                    "sent": "Your function family F. Similarly, if you support vector machines, the choice of the kernel.",
                    "label": 1
                },
                {
                    "sent": "Can further specify the family F or in a neural network that we in a number of layers, the architecture, the number of neurons, etc that will specify.",
                    "label": 0
                },
                {
                    "sent": "The function family.",
                    "label": 0
                },
                {
                    "sent": "So far so good.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "There's other things, there's what we call hyperparameters for regularization schemes, regularization, hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So I give a couple of examples here.",
                    "label": 0
                },
                {
                    "sent": "You can imagine, say, a linear classifier or polynomial crossfire neural network.",
                    "label": 0
                },
                {
                    "sent": "OK, but you can add some constraint on its weights, say the normal weights is no bigger than three.",
                    "label": 0
                },
                {
                    "sent": "OK, well if you do that, you've effectively restricted the set of parameters that you could get.",
                    "label": 1
                },
                {
                    "sent": "The thing can use, so you've restricted your function, your function family.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So often it's not phrased as a constraint on the norm, but rather a penalty on the norm.",
                    "label": 0
                },
                {
                    "sent": "But that's kind of that's equivalent if you're penalizing the norm, it's equivalent to putting a constraint on the norm.",
                    "label": 0
                },
                {
                    "sent": "So that gives you if you take linear regression and you add that kind of constraint or penalty that gives you Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "OK, annual networks.",
                    "label": 0
                },
                {
                    "sent": "That's called weight decay.",
                    "label": 0
                },
                {
                    "sent": "Is everybody heard about weight decay?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                },
                {
                    "sent": "So we're in neural network decade.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So um, sometimes often, well.",
                    "label": 0
                },
                {
                    "sent": "I say in some cases these trade off.",
                    "label": 0
                },
                {
                    "sent": "Well, try to favor some values per meters versus others and this can building tube Asian prior print para meters.",
                    "label": 0
                },
                {
                    "sent": "So that can also be the strength of that Bayesian prior or that vision prior itself can also be seen as a regularization scheme.",
                    "label": 0
                },
                {
                    "sent": "There's other kind of more recent than strange realisation schemes like noise and noise injection.",
                    "label": 0
                },
                {
                    "sent": "You can control the degree of noise where you inject the noise, etc.",
                    "label": 0
                },
                {
                    "sent": "All these things will have an effect.",
                    "label": 0
                },
                {
                    "sent": "Maybe not on the bigger family F. If you view it strictly but effectively on what part of that space of F that you're effectively going to explore.",
                    "label": 0
                },
                {
                    "sent": "OK, and there's a final one here.",
                    "label": 0
                },
                {
                    "sent": "That's early stopping, and that's something that you can use in any iterative search optimization procedure.",
                    "label": 0
                },
                {
                    "sent": "OK, whether it be gradient descent or boosting, you're familiar with boosting, right?",
                    "label": 0
                },
                {
                    "sent": "It's also iterative, right?",
                    "label": 0
                },
                {
                    "sent": "Can also be seen as gradient descent, but.",
                    "label": 0
                },
                {
                    "sent": "Although it's not implemented strictly this way, so when you choose to stop that exploration before convergence, you choose to stop it before convergence.",
                    "label": 0
                },
                {
                    "sent": "This iterative search optimization procedure.",
                    "label": 1
                },
                {
                    "sent": "Well, if you did have a question to stop it before convergence, that means that you that you won't explore as far from the original point as if you had run it to convergence so ineffectively, you're not exploring the whole space F, so that's also an effective restricting the effective capacity of your argument.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "So all regularization schemes are suboptimal.",
                    "label": 0
                },
                {
                    "sent": "If your goal is to find a minimum on your training set.",
                    "label": 0
                },
                {
                    "sent": "But that's not your goal.",
                    "label": 0
                },
                {
                    "sent": "Your goal is to find the best generalization.",
                    "label": 0
                },
                {
                    "sent": "That's why we have we have.",
                    "label": 0
                },
                {
                    "sent": "Capacity control regularization schemes.",
                    "label": 0
                },
                {
                    "sent": "If your goal is to find the minimum error on your training set.",
                    "label": 0
                },
                {
                    "sent": "All these things are going to hurt you.",
                    "label": 0
                },
                {
                    "sent": "All of them.",
                    "label": 0
                },
                {
                    "sent": "You want to be hurt, but I get to that, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I hope.",
                    "label": 0
                },
                {
                    "sent": "I know, I know, it's what do you appetite?",
                    "label": 0
                },
                {
                    "sent": "Yes, sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, that's too, that's my next slide, wonderful.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm making a an important distinction.",
                    "label": 0
                },
                {
                    "sent": "I called hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Some people call them later para meters OK. That's to distinguish them from power meters.",
                    "label": 0
                },
                {
                    "sent": "And per meters are what is being optimized on the training set by your optimizer or your learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so take the example of the polynomial classifier.",
                    "label": 0
                },
                {
                    "sent": "Which isn't there OK?",
                    "label": 0
                },
                {
                    "sent": "OK, it's still take that one.",
                    "label": 0
                },
                {
                    "sent": "OK, the polynomial classifier you're going to learn the coefficient of your polynome, let's call them W and be OK to make it simple.",
                    "label": 0
                },
                {
                    "sent": "I'm going to learn those the algorithm is going to optimize those on the training set.",
                    "label": 0
                },
                {
                    "sent": "OK, it's going to find the values of those parameters such that your model makes the fewest error on the training set, but there's things that you fixed ahead of time before that training, and that will be the degree of your polynomial, so the degree of your polynomial is a parimeter that you fixed ahead of time for your learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "That's called, that's what we call a hyperparameter that's to be distinguished from the parameters of the model that the algorithm is actually going to learn, so these.",
                    "label": 0
                },
                {
                    "sent": "Specify the family.",
                    "label": 0
                },
                {
                    "sent": "A function.",
                    "label": 0
                },
                {
                    "sent": "These allow you specify which function in the family you're going to choose.",
                    "label": 0
                },
                {
                    "sent": "So the distinction is very important.",
                    "label": 0
                },
                {
                    "sent": "They specify the family of the function big F. These are the parameters of the small F that's part of Big F. Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, I automatically automagically.",
                    "label": 0
                },
                {
                    "sent": "Yes automatically sure I will get to that a little bit.",
                    "label": 0
                },
                {
                    "sent": "I'll give the high level principle and then maybe up into some.",
                    "label": 0
                },
                {
                    "sent": "Hopeful future directions.",
                    "label": 0
                },
                {
                    "sent": "OK, so just quickly here, just to give a set of examples.",
                    "label": 0
                },
                {
                    "sent": "Some of you may be familiar with is I picked up a couple of very popular learning algorithms, so for class popular.",
                    "label": 0
                },
                {
                    "sent": "Sorry classifiers more specifically, so there's classic algorithm for classification.",
                    "label": 0
                },
                {
                    "sent": "So one of the simple that's a linear classifiers logistic regression and the other is a linear SVM.",
                    "label": 1
                },
                {
                    "sent": "These are two linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "They don't optimize the same function exactly.",
                    "label": 0
                },
                {
                    "sent": "OK, none of them optimize the misclassification loss.",
                    "label": 0
                },
                {
                    "sent": "Remember why, why?",
                    "label": 0
                },
                {
                    "sent": "Because we can't OK so.",
                    "label": 0
                },
                {
                    "sent": "So they they use a slightly different proxy in both.",
                    "label": 0
                },
                {
                    "sent": "That's a probabilistic classifier.",
                    "label": 0
                },
                {
                    "sent": "That's a non probabilistic classifier.",
                    "label": 1
                },
                {
                    "sent": "And the linear SVM has a hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "If you use the package, you might have encountered AC and annoying, see OK. And it learns WND, similar dogic regression if you add a regularizer, because I want put in power with linear SVM, and you have the strength of this regularizer to set ahead of time before training it with the training set.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if you have kernel support vector machines and you still have that C, But you have other hyperparameters, that's the choice of your kernel and the parameters hidden inside your kernel.",
                    "label": 0
                },
                {
                    "sent": "So for popular kernels like their RBF or Gaussian kernel, it's a Sigma the width of the Gaussians for polynomial kernel, it's the degree of the polynomial.",
                    "label": 0
                },
                {
                    "sent": "And what it learns the parameters.",
                    "label": 1
                },
                {
                    "sent": "So once you've set dizzy ahead of time and you run your SVM, it will learn support vector weights.",
                    "label": 0
                },
                {
                    "sent": "Neural networks you specify layer sizes or stopping criteria.",
                    "label": 0
                },
                {
                    "sent": "Lots of other things.",
                    "label": 0
                },
                {
                    "sent": "So again, these will specify the capacity, the effective capacity, the family function, and how well you're going to explore it are restricted.",
                    "label": 0
                },
                {
                    "sent": "Realistically, you're going to explore it.",
                    "label": 0
                },
                {
                    "sent": "Of your model.",
                    "label": 0
                },
                {
                    "sent": "And what you get after training it on the.",
                    "label": 0
                },
                {
                    "sent": "On the training set is the weight matrix the weights in the weight matrices decision tree.",
                    "label": 1
                },
                {
                    "sent": "The primary capacity control parameter is the depth of the tree.",
                    "label": 0
                },
                {
                    "sent": "An in Kenya's neighbor may see you have two you have.",
                    "label": 0
                },
                {
                    "sent": "KK is essential hyperparameter and the choice of the metrics that you're using and what it learns.",
                    "label": 0
                },
                {
                    "sent": "I answered that question before.",
                    "label": 0
                },
                {
                    "sent": "In my view, it less parameters, it memorizes the training set that sits para meters.",
                    "label": 0
                },
                {
                    "sent": "OK, that's why it's needed.",
                    "label": 0
                },
                {
                    "sent": "That's what it requires to make a decision.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So tuning the capacity, how do you tune that capacity?",
                    "label": 1
                },
                {
                    "sent": "We have this notion of capacity.",
                    "label": 0
                },
                {
                    "sent": "We have ways to control it, and all this lever to control it.",
                    "label": 0
                },
                {
                    "sent": "So how do we?",
                    "label": 0
                },
                {
                    "sent": "Make it best optimal, right?",
                    "label": 0
                },
                {
                    "sent": "So it must be optimally tuned to ensure good generalization, OK?",
                    "label": 1
                },
                {
                    "sent": "And that's done by tuning all these levels.",
                    "label": 0
                },
                {
                    "sent": "So by choosing the right algorithm or.",
                    "label": 1
                },
                {
                    "sent": "One among several an choosing it's right hyperparameters OK, and the goal of that is to avoid underfitting and overfitting.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's an here's again our simple example.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "See, we have these points here, so it's a regression problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have X here, that's our input.",
                    "label": 0
                },
                {
                    "sent": "An why that's what we want the predictor to predict.",
                    "label": 0
                },
                {
                    "sent": "Apples in the training set, that's that are the black dots.",
                    "label": 0
                },
                {
                    "sent": "And basically, those black dots were drawn with a little noise from that blue curve so that blue curve with a little noise is essentially that distribution from well.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's not nature, it's artificial, but that's the unknown distribution, right?",
                    "label": 0
                },
                {
                    "sent": "That's famous, unknown distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, now we know it because it's an artificial experiment, but usually we don't, so it's just to show where they actually come from.",
                    "label": 0
                },
                {
                    "sent": "Now, if we fit a linear predictor to that OK, we're going to fit that little line here, so you can see the error that is making on the training set.",
                    "label": 0
                },
                {
                    "sent": "So what is the error on the training set?",
                    "label": 0
                },
                {
                    "sent": "We have regression problem so squared error.",
                    "label": 0
                },
                {
                    "sent": "So the squared errors with distance.",
                    "label": 0
                },
                {
                    "sent": "Here how far each for each training point we are from the projection on the red.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's this distances squared the sum of all these distances squared, or the average of all this distance squared, right?",
                    "label": 0
                },
                {
                    "sent": "That's our training error.",
                    "label": 0
                },
                {
                    "sent": "That's our actually that's our going to be our empirical.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Risk on the training set and we can get have an idea of the generalization error.",
                    "label": 0
                },
                {
                    "sent": "The generalization error.",
                    "label": 0
                },
                {
                    "sent": "Well, that would be if we drew other points from that same distribution.",
                    "label": 0
                },
                {
                    "sent": "How far the red line with it you see in this here?",
                    "label": 0
                },
                {
                    "sent": "It will be pretty far.",
                    "label": 0
                },
                {
                    "sent": "So here it's a linear classifieds printer locally Facetti, so we have fenneman.",
                    "label": 0
                },
                {
                    "sent": "That's underfitting the function.",
                    "label": 0
                },
                {
                    "sent": "We chose the family found with functional family is not flexible enough to adapt.",
                    "label": 0
                },
                {
                    "sent": "Optimally, to the training data.",
                    "label": 0
                },
                {
                    "sent": "Here's what you'd get if you take a higher degree polynomials that would be like, I know, degree 15.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, take a bit of degree 15.",
                    "label": 0
                },
                {
                    "sent": "What do you see here?",
                    "label": 0
                },
                {
                    "sent": "Well, you see it learn the function in red that passes through all the training points.",
                    "label": 0
                },
                {
                    "sent": "Wonderful, you may think that means that you have.",
                    "label": 0
                },
                {
                    "sent": "Zero training error, right?",
                    "label": 0
                },
                {
                    "sent": "It learned the training set my heart perfectly.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But how is it going to do on new points on new examples?",
                    "label": 0
                },
                {
                    "sent": "OK, if you draw new examples from the from the blue curve, well if you draw some some around here is going to predict something that's pretty far off.",
                    "label": 0
                },
                {
                    "sent": "OK, so here that's the phenomenon of overfitting.",
                    "label": 1
                },
                {
                    "sent": "You've chosen 2 high capacity.",
                    "label": 0
                },
                {
                    "sent": "Your function is too flexible.",
                    "label": 0
                },
                {
                    "sent": "It learns very well on training set, but generalizes very poorly.",
                    "label": 0
                },
                {
                    "sent": "OK. Now what would be the optimal capacity so that would be maybe a polynomial of degree three site?",
                    "label": 0
                },
                {
                    "sent": "23 So that would give you this red curve, so you see, it doesn't do a perfect job at true learning the training set, it's not passing through all the training points.",
                    "label": 0
                },
                {
                    "sent": "The training error is not zero.",
                    "label": 0
                },
                {
                    "sent": "But it will generalize well if you look how far the blue the red curve here is from the blue one.",
                    "label": 0
                },
                {
                    "sent": "OK, there's nowhere.",
                    "label": 0
                },
                {
                    "sent": "It's super far.",
                    "label": 0
                },
                {
                    "sent": "OK, so that would be the optimal capacity.",
                    "label": 0
                },
                {
                    "sent": "Again, here's an example to choose what to choose.",
                    "label": 0
                },
                {
                    "sent": "The degree of a polynomial that's a hyperparameter that controls the family function effectively, what degree polynomial?",
                    "label": 0
                },
                {
                    "sent": "And that controls the capacity.",
                    "label": 0
                },
                {
                    "sent": "OK. And the whole message here is that performance on the training set here.",
                    "label": 0
                },
                {
                    "sent": "OK, so the performance on the on the data that you.",
                    "label": 0
                },
                {
                    "sent": "Learned your chosen your function on.",
                    "label": 1
                },
                {
                    "sent": "Learn this family design is not a good estimate of generalization here.",
                    "label": 0
                },
                {
                    "sent": "It's zero.",
                    "label": 0
                },
                {
                    "sent": "It says, OK, I'm perfect.",
                    "label": 0
                },
                {
                    "sent": "I learned that training set perfectly.",
                    "label": 0
                },
                {
                    "sent": "Very bad realization that's overfitting any questions.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, I'll here to quickly another example in classification.",
                    "label": 0
                },
                {
                    "sent": "Here we have two classes, the red versus the.",
                    "label": 0
                },
                {
                    "sent": "Black, so it's a 2 dimensional input, OK. Luminosity and.",
                    "label": 0
                },
                {
                    "sent": "Anne, what?",
                    "label": 0
                },
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "Is 2D classification with a linear classifier you get there.",
                    "label": 1
                },
                {
                    "sent": "You'll get a hyperplane separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "That's what you get.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe here the capacity is little too low for this problem.",
                    "label": 1
                },
                {
                    "sent": "Relative to the number examples you have an underfitting phenomenon.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the other side, if you take a more flexible predictor, it may learn something like that, say here it predicted perfectly.",
                    "label": 0
                },
                {
                    "sent": "It makes absolutely no error on the training set.",
                    "label": 0
                },
                {
                    "sent": "OK, but maybe it's a little too convoluted for what that really represents, so it's likely to have overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is probably something that would have with optimal capacity.",
                    "label": 0
                },
                {
                    "sent": "We have something that's nice and smooth.",
                    "label": 0
                },
                {
                    "sent": "That's more likely to generalize well.",
                    "label": 0
                },
                {
                    "sent": "Just picture for the same.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so what's happening here?",
                    "label": 0
                },
                {
                    "sent": "I'll use this little decomposition of generalization error as a.",
                    "label": 0
                },
                {
                    "sent": "An explanation of what's going on.",
                    "label": 0
                },
                {
                    "sent": "OK, basically you can think of this green box here as the set of all possible functions in the universe.",
                    "label": 1
                },
                {
                    "sent": "OK among all those best all those powerful possible functions universe.",
                    "label": 0
                },
                {
                    "sent": "Let's say there's one that's the best possible predictor F star.",
                    "label": 0
                },
                {
                    "sent": "OK, it's here.",
                    "label": 0
                },
                {
                    "sent": "Best product.",
                    "label": 0
                },
                {
                    "sent": "Possible predictor in the universe.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Now what we've done is we've that restricted ourselves to a specific function family.",
                    "label": 0
                },
                {
                    "sent": "Big F, yes.",
                    "label": 0
                },
                {
                    "sent": "That's our whole point here.",
                    "label": 0
                },
                {
                    "sent": "OK, so inside that function family big F you have the best possible function for the task that has the lowest generalization error.",
                    "label": 0
                },
                {
                    "sent": "We talk about generalization error here among that family of functions, let's call it F star F to say that it's in the family function.",
                    "label": 0
                },
                {
                    "sent": "And then we have a third guy.",
                    "label": 0
                },
                {
                    "sent": "That third guy is whatever our learning algorithm learned using the training set.",
                    "label": 0
                },
                {
                    "sent": "OK, that's called F hat of the train.",
                    "label": 0
                },
                {
                    "sent": "It was trained on your training set.",
                    "label": 0
                },
                {
                    "sent": "So actually we can decompose the generalization error that this guy is going to make.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "In 2 two important terms, basically at measure how far it is from the best possible function and one is adding how far it is from All Star.",
                    "label": 0
                },
                {
                    "sent": "The functional lumbar algorithm is from the best function in the set in the function family that we chose.",
                    "label": 1
                },
                {
                    "sent": "And the second term is how far that best function in F is far from the best possible function.",
                    "label": 0
                },
                {
                    "sent": "OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "So these two terms have a name, that big name one is called a bias.",
                    "label": 0
                },
                {
                    "sent": "That's not to be confused with the bias that you have in linear models or in neural networks, OK?",
                    "label": 0
                },
                {
                    "sent": "And another term here is variance.",
                    "label": 0
                },
                {
                    "sent": "Is all related to a notion that best approximation, error and variance is an estimation error.",
                    "label": 0
                },
                {
                    "sent": "So can someone tell me what is responsible for the bias?",
                    "label": 0
                },
                {
                    "sent": "Why do we have a bias?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                },
                {
                    "sent": "Basically the bias is due primarily to the fact that we are considering unrestricted family function.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the choice of algorithm.",
                    "label": 0
                },
                {
                    "sent": "That's restricting that, and the choice of hyperparameters OK?",
                    "label": 0
                },
                {
                    "sent": "So that's why we have a bias.",
                    "label": 0
                },
                {
                    "sent": "That's basically how far the best function in that family is.",
                    "label": 0
                },
                {
                    "sent": "You're stuck in that family, right?",
                    "label": 0
                },
                {
                    "sent": "So that's advice.",
                    "label": 0
                },
                {
                    "sent": "How can we reduce the bias?",
                    "label": 0
                },
                {
                    "sent": "Expanding the family.",
                    "label": 0
                },
                {
                    "sent": "Yes, I get back to that.",
                    "label": 0
                },
                {
                    "sent": "So that's what the bias term is do is due to having a restricted family.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "That's the capacity of the family, it's restricted.",
                    "label": 0
                },
                {
                    "sent": "How is the variance term due to?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, so there's different factors, but let's suppose for a minute that we have a wonderful optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "That is really able to optimize that perfectly.",
                    "label": 0
                },
                {
                    "sent": "So given the training set, it's finding the optimal for that training set, it is OK, it's magical, it works.",
                    "label": 0
                },
                {
                    "sent": "No training, no.",
                    "label": 0
                },
                {
                    "sent": "Optimization operators know learning rate to do nothing.",
                    "label": 0
                },
                {
                    "sent": "OK, it works.",
                    "label": 0
                },
                {
                    "sent": "It finds it.",
                    "label": 0
                },
                {
                    "sent": "So then there's still remaining source of variability.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so these two are related.",
                    "label": 0
                },
                {
                    "sent": "Is this the fact that we have a limited finite training set and the variance is really that?",
                    "label": 0
                },
                {
                    "sent": "If you were to sample if you if you were to?",
                    "label": 0
                },
                {
                    "sent": "I mean you got that training set from the true nature distribution, but you could have gotten others right?",
                    "label": 0
                },
                {
                    "sent": "You could have got another training sets actually have a distribution over training sets.",
                    "label": 0
                },
                {
                    "sent": "If you take a training set of size N OK, you have a whole distribution of training sets of size and that you could have gotten.",
                    "label": 0
                },
                {
                    "sent": "And for each of these training set.",
                    "label": 0
                },
                {
                    "sent": "Training set one during set 2 drinks at three, well, you're super optimal for that training set will be at some slightly different place in there, so that's responsible for the variance here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So now we have the bias variance dilemma.",
                    "label": 0
                },
                {
                    "sent": "OK, so as you answered rightly.",
                    "label": 0
                },
                {
                    "sent": "If we want to.",
                    "label": 0
                },
                {
                    "sent": "Reduce the bias.",
                    "label": 0
                },
                {
                    "sent": "We can simply choose a larger family.",
                    "label": 0
                },
                {
                    "sent": "Right then will be closer, so will increase the capacity.",
                    "label": 0
                },
                {
                    "sent": "Choosing richer family will increase the capacity so, but that's what that's going to do is is going to decrease the bias, sure, but it's also going to increase the variance.",
                    "label": 0
                },
                {
                    "sent": "So just to understand, I mean informed way to understand why Richard family means more parameters to tune, but you still have the same number of training examples, OK?",
                    "label": 0
                },
                {
                    "sent": "So your estimate of the parameter is going to be even more noisy.",
                    "label": 0
                },
                {
                    "sent": "You can do the other way round.",
                    "label": 0
                },
                {
                    "sent": "You can reduce the capacity.",
                    "label": 0
                },
                {
                    "sent": "Take a smaller function.",
                    "label": 0
                },
                {
                    "sent": "Family OK, in which case your variance will decrease.",
                    "label": 0
                },
                {
                    "sent": "OK, I mean, if you take the constant predictor.",
                    "label": 0
                },
                {
                    "sent": "The van's gonna be pretty low because it's pretty easy to estimate an average using a couple of points right there.",
                    "label": 0
                },
                {
                    "sent": "It's going to be pretty low, but the bias can be is going to be huge.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "We have to find an optimal compromise between the bias and variance.",
                    "label": 0
                },
                {
                    "sent": "That's called what that's called the bias variance dilemma.",
                    "label": 0
                },
                {
                    "sent": "What I showed and bias variance tradeoff training one for the other.",
                    "label": 0
                },
                {
                    "sent": "There will be some optimal compromise, but that compromise will depend on the number of examples.",
                    "label": 1
                },
                {
                    "sent": "OK, the bigger your number of examples.",
                    "label": 0
                },
                {
                    "sent": "If you don't touch your model family, you don't touch your F. If you have larger, larger training set, well, your variance will automatically decrease.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't have to change the function family.",
                    "label": 0
                },
                {
                    "sent": "The variance will decrease.",
                    "label": 1
                },
                {
                    "sent": "So if you have bigger, if you have more data then you can afford to increase the capacity in order to lower the bias.",
                    "label": 1
                },
                {
                    "sent": "OK, you can use more expressive models with bigger datasets.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And I'll conclude from that side is that the best regularizer is more data.",
                    "label": 0
                },
                {
                    "sent": "Please advise.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you don't have more data unfortunately, so this is my heart, yes?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "That's right, yeah, exactly I. I gave that example.",
                    "label": 0
                },
                {
                    "sent": "Oh, you have very small capacity so that case will be.",
                    "label": 0
                },
                {
                    "sent": "You can view this image here.",
                    "label": 0
                },
                {
                    "sent": "You have a tiny, tiny, tiny little family F. It has just one parameter in it.",
                    "label": 0
                },
                {
                    "sent": "It's very small ish.",
                    "label": 0
                },
                {
                    "sent": "Tiny family F. So that means that the variance is going to be very small, but the bias is really going to be super high unless your true best function is also constant.",
                    "label": 0
                },
                {
                    "sent": "OK, it could be the optimal in some specific cases.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So that's what's happening here.",
                    "label": 0
                },
                {
                    "sent": "Yes, it you're going to have a very small capacity.",
                    "label": 0
                },
                {
                    "sent": "Thus small variance and a high bias.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, it depends on the size of family.",
                    "label": 0
                },
                {
                    "sent": "The capacity is the size of family function.",
                    "label": 0
                },
                {
                    "sent": "Predicting predicting the mean.",
                    "label": 0
                },
                {
                    "sent": "I mean, the mean is just one parameter.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a constant predictor.",
                    "label": 0
                },
                {
                    "sent": "It's very it has very small capacity.",
                    "label": 0
                },
                {
                    "sent": "So very few parameters to adapt.",
                    "label": 0
                },
                {
                    "sent": "That means that it will have very low variance.",
                    "label": 0
                },
                {
                    "sent": "It's very it will be very.",
                    "label": 0
                },
                {
                    "sent": "It won't be very noisy to estimate it given given a training points, but it's going to be very far from OK. Let me show you the example.",
                    "label": 0
                },
                {
                    "sent": "I didn't show it in here, but it could have.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, constant predictor here will be a straight line.",
                    "label": 0
                },
                {
                    "sent": "OK, that's.",
                    "label": 0
                },
                {
                    "sent": "The variance.",
                    "label": 0
                },
                {
                    "sent": "Will be low, but the bias how far it is.",
                    "label": 0
                },
                {
                    "sent": "From how far that family function is from containing the actual true function is very high.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, so model selection.",
                    "label": 0
                },
                {
                    "sent": "This is my how to slide.",
                    "label": 0
                },
                {
                    "sent": "How to do it?",
                    "label": 0
                },
                {
                    "sent": "So remember, initially we split our data set between train and test set.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to split it in three, so that's supposing you have plenty of data, OK.",
                    "label": 0
                },
                {
                    "sent": "If not, this gets more complicated.",
                    "label": 0
                },
                {
                    "sent": "I won't get into it, but suppose it will split it into three trees, three datasets that we call the training set validation set.",
                    "label": 0
                },
                {
                    "sent": "And a test.",
                    "label": 0
                },
                {
                    "sent": "So with better test set and validation entry.",
                    "label": 0
                },
                {
                    "sent": "And then here is a super generic mid algorithm.",
                    "label": 0
                },
                {
                    "sent": "For model selection, so for selecting your family of function, OK, you're F. That includes selecting the actual learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, is it a polynomial?",
                    "label": 0
                },
                {
                    "sent": "Is it linear?",
                    "label": 0
                },
                {
                    "sent": "Is it a?",
                    "label": 0
                },
                {
                    "sent": "Is it a kernel support vector machine?",
                    "label": 0
                },
                {
                    "sent": "Is it whatever your machine learning algorithm and its hyperparameters?",
                    "label": 0
                },
                {
                    "sent": "So instead of hypermedia, quite Lambda.",
                    "label": 0
                },
                {
                    "sent": "So once you can do, that's a principle algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not practical.",
                    "label": 0
                },
                {
                    "sent": "For that you can choose for each considered model, so that includes you put put all in the back OK support vector machine, linear kernel, or kind of kernels, neural networks, decision trees, put them all in the back, all of them OK, they're all in that back.",
                    "label": 0
                },
                {
                    "sent": "You're going to loop over them here in that loop, and then for each of those algorithms you take all their hyper hyper parameters that control capacity.",
                    "label": 0
                },
                {
                    "sent": "You put them all in the bag.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you're going to loop over all possible configurations of values of those parameters.",
                    "label": 0
                },
                {
                    "sent": "That means that for neural networks, you try all layer sizes or whatever for Kernel 4.",
                    "label": 0
                },
                {
                    "sent": "For support vector machines, account support vector machine, say RBF kernels, you try all widths of kernels for nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "You try all K for nearest neighbor etc.",
                    "label": 0
                },
                {
                    "sent": "That's so far all algorithms.",
                    "label": 0
                },
                {
                    "sent": "Then for all consider hyperparameter values.",
                    "label": 0
                },
                {
                    "sent": "And that algorithm?",
                    "label": 0
                },
                {
                    "sent": "That's a huge space.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a problem, but it's a principle.",
                    "label": 0
                },
                {
                    "sent": "So for each such algorithm and specific complication of hyperparameter value, you train that model with that configuration of hyperparameter value on your training set.",
                    "label": 1
                },
                {
                    "sent": "This training procedure I write a Lambda is a learning algorithm specified with a hyper hyper parameters.",
                    "label": 0
                },
                {
                    "sent": "Lambda OK. And you control it on the training set you train it on the training set and what that gives you is the optimal function for that algorithm.",
                    "label": 0
                },
                {
                    "sent": "That parameter under of that function function space.",
                    "label": 0
                },
                {
                    "sent": "On the training set, and that's what I call F hat a Lambda.",
                    "label": 0
                },
                {
                    "sent": "OK, so for each algorithm A are each Lambda you're going to have a different hat alenda different predictor.",
                    "label": 0
                },
                {
                    "sent": "Then you evaluate that predictor on the validation set, don't evaluate it on the training set.",
                    "label": 0
                },
                {
                    "sent": "I mean you can, but don't base your decisions.",
                    "label": 0
                },
                {
                    "sent": "Don't base your model selection on.",
                    "label": 0
                },
                {
                    "sent": "That will get to that in a minute.",
                    "label": 0
                },
                {
                    "sent": "So with your preferred evaluation metric, not that the evaluation here might be different from what the algorithm actually optimized, that's OK. What you want, for example, here is customary to train a, say, a a classifier probabilistic classifier with binary cross entropy back to evaluate it on misclassification error.",
                    "label": 0
                },
                {
                    "sent": "OK, that's common.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we're going to evaluate on the preferred evaluation metric of interest.",
                    "label": 0
                },
                {
                    "sent": "So that you take the risk according to that loss function off your just trained algorithm on the validation set an we call the resulting error EA Lambda.",
                    "label": 0
                },
                {
                    "sent": "So for each algorithm a each hyper printer combination Lambda you have an EA London.",
                    "label": 0
                },
                {
                    "sent": "So among all of them remember you had many areas and many hypermedia commendation you find which one is.",
                    "label": 0
                },
                {
                    "sent": "The best.",
                    "label": 0
                },
                {
                    "sent": "Which one you like the best?",
                    "label": 0
                },
                {
                    "sent": "And we call that algorithm a star.",
                    "label": 0
                },
                {
                    "sent": "Ann hyperedges landstar.",
                    "label": 0
                },
                {
                    "sent": "And then you simply return that the predictor that was obtained with that algorithm.",
                    "label": 0
                },
                {
                    "sent": "That's your model selection you've effectively selected among all the set of models as VM's, whatever all those set of hyperparameters Lander with kernel, etc, you selected the one that performed.",
                    "label": 0
                },
                {
                    "sent": "Best on validation set and you return that optionally what you can do is you can actually take those hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "It doesn't always work, but sometimes helps take those hyperparameters and that algorithm and retrain it on the union of the training set and the validation set.",
                    "label": 0
                },
                {
                    "sent": "That's fair, yes.",
                    "label": 0
                },
                {
                    "sent": "If I recommend it so I just said it doesn't always work, it will work.",
                    "label": 0
                },
                {
                    "sent": "If your optimal algorithm hyperparameters Lander and algorithm scale relatively well when you increase or give relatively well, well being unchanged as you increase your data, your data set.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "We could actually make a test for that bit complicated OK, and so that's the model selection and training.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's a kind of meta algorithm, because you see inside it uses basic learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So that's the higher level.",
                    "label": 0
                },
                {
                    "sent": "Which Toons chooses it algorithm and the hyperparameter values.",
                    "label": 0
                },
                {
                    "sent": "And finally you have this winning algorithm F star here.",
                    "label": 0
                },
                {
                    "sent": "Oh no, the wonder of this thing here.",
                    "label": 0
                },
                {
                    "sent": "This algorithm at last is now going without any hyperparameters to tune.",
                    "label": 0
                },
                {
                    "sent": "Isn't that wonderful, OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's just you you need.",
                    "label": 0
                },
                {
                    "sent": "You need clusters the size of the universe to actually implement it, but.",
                    "label": 0
                },
                {
                    "sent": "And finally, you compute the unbiased estimate authorization performance using our test set.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll test set.",
                    "label": 0
                },
                {
                    "sent": "We still haven't touched it.",
                    "label": 0
                },
                {
                    "sent": "We haven't used it.",
                    "label": 0
                },
                {
                    "sent": "We must never have used it.",
                    "label": 0
                },
                {
                    "sent": "Actually during training or model selection to select, learn or tune anything, you can look at it.",
                    "label": 0
                },
                {
                    "sent": "You're allowed to look at it.",
                    "label": 0
                },
                {
                    "sent": "Yeah wow, some people say you're not allowed to look at it, but everybody looks at it, OK?",
                    "label": 0
                },
                {
                    "sent": "But you're not allowed to select anything based on it tune anything based on it.",
                    "label": 0
                },
                {
                    "sent": "Optimize anything based on anything you see from it.",
                    "label": 0
                },
                {
                    "sent": "OK. You can look at it.",
                    "label": 0
                },
                {
                    "sent": "So yes.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, yeah yeah it it.",
                    "label": 0
                },
                {
                    "sent": "It will appear in time.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So, so that's complicated.",
                    "label": 0
                },
                {
                    "sent": "Usually when you're when you're when you're caught with doing K fold cross validation is because you don't have enough data to begin with, right?",
                    "label": 0
                },
                {
                    "sent": "So it depends, it depends on your.",
                    "label": 0
                },
                {
                    "sent": "How your thing is going to be evaluated if you have a proper evaluation test set that you can touch, that's big enough and reliable enough, then you can use that.",
                    "label": 0
                },
                {
                    "sent": "OK, but often the problem is if you're caught and then you can do.",
                    "label": 0
                },
                {
                    "sent": "You can do careful cross validation to find the best hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Those that on on average well will do best on your moving violation.",
                    "label": 0
                },
                {
                    "sent": "Set OK, but then then use that, use that.",
                    "label": 0
                },
                {
                    "sent": "Retrain on the whole training set.",
                    "label": 0
                },
                {
                    "sent": "Full complete training and validation.",
                    "label": 0
                },
                {
                    "sent": "Set and use your separate test set now.",
                    "label": 0
                },
                {
                    "sent": "If you really have very, very small data, then maybe you need to do something else called double cross validation.",
                    "label": 0
                },
                {
                    "sent": "So you need to do a cross validation.",
                    "label": 0
                },
                {
                    "sent": "So say, for example, we've run out on your test data, OK?",
                    "label": 0
                },
                {
                    "sent": "And and inside you take what's remaining as your training set, and you do another.",
                    "label": 0
                },
                {
                    "sent": "For example, leave one out OK, fold to choose a hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "When you do that, what you will evaluate is not.",
                    "label": 0
                },
                {
                    "sent": "You won't get a single algorithm in hyperparameter out of that.",
                    "label": 0
                },
                {
                    "sent": "You'll get an evaluation actually off on what you're going to evaluate on your test set.",
                    "label": 1
                },
                {
                    "sent": "Is your model selection meter.",
                    "label": 0
                },
                {
                    "sent": "That meet algorithm that has no hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "That's the version of that that's going to be unbiased on your test set.",
                    "label": 0
                },
                {
                    "sent": "It's a little more complicated procedure with a double cross validation.",
                    "label": 0
                },
                {
                    "sent": "Yes, sorry.",
                    "label": 0
                },
                {
                    "sent": "Nope.",
                    "label": 0
                },
                {
                    "sent": "Sorry, did the bigger the better.",
                    "label": 0
                },
                {
                    "sent": "I know it's saying it's customary to take like 110th for training set for a test set.",
                    "label": 0
                },
                {
                    "sent": "110 Fender validation set.",
                    "label": 0
                },
                {
                    "sent": "That's about the same size and training set, but.",
                    "label": 0
                },
                {
                    "sent": "Really, it again, it's a tradeoff, right?",
                    "label": 0
                },
                {
                    "sent": "If you put more examples in your test set.",
                    "label": 0
                },
                {
                    "sent": "Your your evaluation of your generalization error will be less noisy, but you have few examples to train it on, so yeah.",
                    "label": 0
                },
                {
                    "sent": "Size.",
                    "label": 0
                },
                {
                    "sent": "They are OK. That was given the first thing, fixed their other hyperparameters actually is.",
                    "label": 0
                },
                {
                    "sent": "The choice of which albums you're going to consider.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but did that.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't do that.",
                    "label": 0
                },
                {
                    "sent": "Beijing is what they say is you consider all the algorithm within the universe OK, but you put a flyer over them right?",
                    "label": 0
                },
                {
                    "sent": "You put it prior and then you integrate them all.",
                    "label": 0
                },
                {
                    "sent": "Suppose you're able to do that for the answer, but we still have a high performance.",
                    "label": 0
                },
                {
                    "sent": "That's your prior.",
                    "label": 0
                },
                {
                    "sent": "OK, so there there's some priors that are kind of universal.",
                    "label": 0
                },
                {
                    "sent": "Supposedly I don't know.",
                    "label": 0
                },
                {
                    "sent": "Sorry I don't have to advance it.",
                    "label": 0
                },
                {
                    "sent": "Oh, so here's.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There an example of Moe hyperparameter selection procedure so.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm almost up.",
                    "label": 0
                },
                {
                    "sent": "Example hyperparameters, so we're running that algorithm, but for a simple case, we chose just a single algorithm and a single hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's easy.",
                    "label": 0
                },
                {
                    "sent": "So then we just have to change the value of that hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "Here it's on the X axis, is the hyperparameter value on the X is the hyperparameter value.",
                    "label": 0
                },
                {
                    "sent": "What we have in Gray here is the corresponding.",
                    "label": 0
                },
                {
                    "sent": "Error of interest as measured on the training set.",
                    "label": 1
                },
                {
                    "sent": "So the set that was the data set that was used to tune the parameters of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, for each value of hypermedia an similarly we have the error on the validation set.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of typical.",
                    "label": 0
                },
                {
                    "sent": "We see the training on the the error on the training set.",
                    "label": 0
                },
                {
                    "sent": "That's actually a monotone no sleep here increasing.",
                    "label": 0
                },
                {
                    "sent": "You might have seen it often decreasing.",
                    "label": 0
                },
                {
                    "sent": "Here it's increasing its desired and you have the.",
                    "label": 1
                },
                {
                    "sent": "Error on the validation set.",
                    "label": 0
                },
                {
                    "sent": "Actually the test set was going to follow the similar curve.",
                    "label": 0
                },
                {
                    "sent": "Normally there's they play the same role right there, set the validation set and the test set the exactly the same role until you choose the performance on the validation set to choose a hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "Then they don't say the same or anymore, but until you've made that choice, they exchangeable.",
                    "label": 0
                },
                {
                    "sent": "Save.",
                    "label": 0
                },
                {
                    "sent": "Um, so here what you see typically is you see a decreasing up to some point and then increasing, increasing again OK.",
                    "label": 0
                },
                {
                    "sent": "So what is the optimal hyperparameter value here?",
                    "label": 0
                },
                {
                    "sent": "OK, it's here hyperparameter, which is smallest error on validation set is 5.",
                    "label": 1
                },
                {
                    "sent": "OK, if you choose it on the training set you get one.",
                    "label": 0
                },
                {
                    "sent": "It's very different.",
                    "label": 0
                },
                {
                    "sent": "Can someone guess what kind of algorithm this might be?",
                    "label": 0
                },
                {
                    "sent": "K nearest neighbors number that scale in the neighbors.",
                    "label": 0
                },
                {
                    "sent": "So you're smoothing your neighborhood here by taking five neighbors instead of 1 neighbor.",
                    "label": 0
                },
                {
                    "sent": "And finding the optimal capacity this way.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimum might sometimes might often be one.",
                    "label": 0
                },
                {
                    "sent": "Also, depending on the training on the side of the training set, right?",
                    "label": 0
                },
                {
                    "sent": "So, uh, the question, what if we selected the capacity control hyperparameters that yield the best performance, not on validation set, but on the training set?",
                    "label": 1
                },
                {
                    "sent": "What would happen then?",
                    "label": 1
                },
                {
                    "sent": "What would we tend to select?",
                    "label": 0
                },
                {
                    "sent": "We would select the hyperparameters that choose the highest capacity, because that's what it will.",
                    "label": 0
                },
                {
                    "sent": "It will allow the training algorithm to make the smallest errors on the training set by probably lead to what?",
                    "label": 0
                },
                {
                    "sent": "Overfitting OK?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If your model class is big enough, right?",
                    "label": 0
                },
                {
                    "sent": "If you're if you're within something that's ready to constrained, maybe not, but OK, let's skip that, that's not.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, just mentioned that their assemble methods and sample methods combine multiple predictors to good effect.",
                    "label": 1
                },
                {
                    "sent": "So you train multiple predictors.",
                    "label": 0
                },
                {
                    "sent": "And you can combine them.",
                    "label": 0
                },
                {
                    "sent": "So there's bagging, bagging averages many high variance, high variance predictors.",
                    "label": 0
                },
                {
                    "sent": "OK, so by averaging many high rents predictor actually decrease the variance.",
                    "label": 0
                },
                {
                    "sent": "So one example, one very successful example of this is maybe heard.",
                    "label": 0
                },
                {
                    "sent": "This one very successful argument.",
                    "label": 1
                },
                {
                    "sent": "That's random decision forests, random decision forest the forest, right?",
                    "label": 0
                },
                {
                    "sent": "The average trees, the average deep trees, deep trees are known to be very high variance.",
                    "label": 0
                },
                {
                    "sent": "So you average them and they take including or more variance in random decision for us and you get pretty good algorithm.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, you can if you if you're not using high variance of high capacity predictors but low capacity classifiers, you can actually get a higher capacity classifier by using boosting by combining them with boosting.",
                    "label": 0
                },
                {
                    "sent": "It will learn a weighted combination of them that will lead to higher capacity.",
                    "label": 1
                },
                {
                    "sent": "So it's common, for instance, to boost shallow trees which have low capacity or linear classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's just a view of of bagging.",
                    "label": 0
                },
                {
                    "sent": "On our regression problem, if you see all the little Gray lines.",
                    "label": 1
                },
                {
                    "sent": "These are all the lines that you get if you resample your training set by a bootstrapping, and if you average them altogether while you get that red line that has lower variance.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll skip that.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I get to my very too lost light.",
                    "label": 0
                },
                {
                    "sent": "Well, let's maybe more to make transitions with what Josh was going to tell you, so I haven't looked into detailed customer, except maybe a linear linear predictors.",
                    "label": 0
                },
                {
                    "sent": "Any?",
                    "label": 0
                },
                {
                    "sent": "I mean, I haven't even looked at linear classifiers, but linear linear models are an important class of model, but they're pretty limited, right?",
                    "label": 0
                },
                {
                    "sent": "So it's just a linear function space is just linear, so there's one way.",
                    "label": 0
                },
                {
                    "sent": "One easy way to actually make them richer, and that's to take your input and to map it into some other representation.",
                    "label": 0
                },
                {
                    "sent": "So you take your.",
                    "label": 0
                },
                {
                    "sent": "X and you map it to X~ and there's basically three ways of doing that.",
                    "label": 0
                },
                {
                    "sent": "You can either use an explicit fixed mapping, and that's what essentially what you do and use handcrafted features.",
                    "label": 1
                },
                {
                    "sent": "You take your raw input, you take some handcrafted feature extractors OK, which are often nonlinear, and you get some new representation X~ That's a fixed set of handcrafted features that's an explicit mapping.",
                    "label": 0
                },
                {
                    "sent": "There's no previous example.",
                    "label": 0
                },
                {
                    "sent": "Another way you can use an implicit fixed mapping, and that's essentially what kernel methods do, like kernel, SVM's, or kernel logistic regression.",
                    "label": 1
                },
                {
                    "sent": "Take your input, they map it implicitly into some high dimensional feature presentation and do a linear classifier linear model there and then.",
                    "label": 1
                },
                {
                    "sent": "Finally, and that's what a lot of this summer school will be focused on.",
                    "label": 0
                },
                {
                    "sent": "You can actually learn the parameterized mapping, and that's that's where you get.",
                    "label": 0
                },
                {
                    "sent": "The machine learning algo, that tool and representations, and that's where you get the multilayer feedforward networks.",
                    "label": 0
                },
                {
                    "sent": "That is, you add layers that actually.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Change their presentation.",
                    "label": 0
                },
                {
                    "sent": "So I guess your show will show you something probably similar to this site.",
                    "label": 0
                },
                {
                    "sent": "Also that gives us the notion of levels of presentation.",
                    "label": 1
                },
                {
                    "sent": "If you have whatever.",
                    "label": 0
                },
                {
                    "sent": "Model here you can always try to put further nonlinear function compositions on top of it.",
                    "label": 0
                },
                {
                    "sent": "OK, that will produce better representations for it.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's it for me.",
                    "label": 0
                }
            ]
        }
    }
}