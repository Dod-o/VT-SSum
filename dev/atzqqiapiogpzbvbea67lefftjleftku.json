{
    "id": "atzqqiapiogpzbvbea67lefftjleftku",
    "title": "Ambient Sound Provides Supervision for Visual Learning",
    "info": {
        "author": [
            "Andrew Owens, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "Oct. 24, 2016",
        "recorded": "October 2016",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2016_owens_ambient_sound/",
    "segmentation": [
        [
            "I'm going to talk today about our work on training vision systems using sound in place of labeled training data.",
            "In particular, we'd like to take advantage of the situation that happens pretty often.",
            "We both see an object and hear sounds that are closely associated with it.",
            "For example, when we're at the beach, we might see water and sand, and we might also hear the sound of crashing waves.",
            "You might hear the sound of people speaking.",
            "We're on the street.",
            "We hear the sound of cars.",
            "So in all these cases, the audio in these videos is telling us a lot about the semantics of a scene, so would be nice if we could use it as a source of supervision.",
            "In other words, if we could predict the sound and place of object or scene labels to learn visual representations."
        ],
        [
            "And this idea is very closely related to work and unsupervised learning, and in particular recent work and so called self supervised learning.",
            "And these methods generally work by taking an image holding."
        ],
        [
            "Out some part of it and then trying to predict the hellebaut part from the rest of the image and analogously."
        ],
        [
            "We'd like to take a video as input, hold out the audio."
        ],
        [
            "Ann predicted from the image, with the assumption that through through doing this will learn a visual representation through.",
            "To solve this task that's useful for other things and well in some ways this may seem very similar to the image based approach.",
            "There are a number of potential advantages to getting our supervision from audio rather than driving it somehow from the images themselves.",
            "These advantages come from the fact."
        ],
        [
            "Activated audio and visual data are somewhat orthogonal, and in particular you can change the appearance of a scene quite drastically while still preserving the audio.",
            "For example, you can change the lighting."
        ],
        [
            "You can change the."
        ],
        [
            "And of the camera."
        ],
        [
            "And all of these visual transformations won't really significantly change the sound, but the images will change, and so an algorithm that's trying to predict audio from images will have to be in variance to these different transformations, and we can take this idea to more of an extreme."
        ],
        [
            "We can just if we change the composition of the scene, it will make the same sound as long as it has the same sound producing objects.",
            "For example, here's some other video clips that are whose audio is very similar to this beach scene."
        ],
        [
            "In each case, these video clips are very closely clustered in audio space.",
            "They have very similar sounds, but I think we all agree that in image space they're very different.",
            "One image has a picture of a person and another one has a bridge with people on the bridge and so an algorithm that was trained only on image data might not be able to understand what makes these images similar to each other.",
            "On the other hand, an algorithm that's trained to predict audio from images."
        ],
        [
            "I will be forced to explain why all of these videos have the same sounds associated with them.",
            "Why they all have this flowing water sound, so either they'll have to detect water in the images or they'll have to recognize that it's a scene like a bridge or a beach that's likely to contain water."
        ],
        [
            "And so another way to look at this is that there are some structures in the world that produce sound and generate images.",
            "Things like people, flowing water cars, and if we train an algorithm to predict sound, the algorithm will have to detect these structures in order to produce the right sounds.",
            "So how do we actually go about predicting sound from images?"
        ],
        [
            "In some recent work that's addressed this, such as the paper that we published at CPR this year, but the kinds of videos that normally are considered in this other work tend to be very specialized.",
            "For example, they might only consider human speech, or they might consider videos of someone hitting things with a drumstick.",
            "On the other hand, the sounds that we see here in natural videos or more loosely correlated with the actions that we're seeing on the screen."
        ],
        [
            "For example, if we consider this video of this coffee shop again, if you look carefully, you can see that the sounds you're hearing aren't actually.",
            "Totally being generated by the people on the screen.",
            "We hear sounds and some of them might be produced by onscreen objects.",
            "Some of them might be generated by offscreen objects we're hearing is mostly ambient or background."
        ],
        [
            "Sound.",
            "And this has two implications.",
            "First of all, it suggests that we can probably predict the audio from a single frame rather than from a long video without losing too much useful information.",
            "And second, it suggests that since it's so hard to get the timing right, the actions aren't very well indicating what the sound should be at any given time step.",
            "Then we should use an audio representation that captures summary statistics of the audio over longer periods of time."
        ],
        [
            "And we'll do this by using an audio representation known as a sound texture.",
            "It's very similar to the image texture work of Portilla and Simoncelli, and the way this works is we'll start with coclea gram representation of the audio.",
            "And then we'll compute a number of summary statistics from this coclea gram the Coakley gram is just very similar to a spectrogram.",
            "We get it by filtering the audio with the Bank of filters that are two in the different frequencies and taking the envelope of each frequency of each filter response.",
            "And."
        ],
        [
            "The summary statistics are things like the average."
        ],
        [
            "Value of each frequency band and its standard deviation."
        ],
        [
            "Then there are higher order statistics like the response of a filterbank to the Coakley gram."
        ],
        [
            "And we also include.",
            "Correlations between different frequency bands."
        ],
        [
            "So the result is this collection of these different temporally averaged statistics and these are what we're going to try to predict from images."
        ],
        [
            "We're going to do this is using a convolutional neural network.",
            "The convolutional neural net will take a single video frame.",
            "And it will try to predict the sound texture for that video frame.",
            "And for training we used a large data set of unlabeled video we just took about 180,000 videos from the Flickr video data set.",
            "Sample 10 frames for each one and use this train the model.",
            "And then rather than actually."
        ],
        [
            "Predicting the."
        ],
        [
            "Sound texture directly as a regression problem.",
            "We simplified things by converting the problem into a classification problem.",
            "Specifically, we created a vector quantization of the audio features using K means."
        ],
        [
            "Or using a slightly more complicated approach using LSH and then, rather than predicting the audio features we try to predict which close."
        ],
        [
            "After the audio features belong to.",
            "So the end result is a network that's essentially just a vanilla Alex net CNN that picks the sound category that best matches the image.",
            "Now to get a better understanding for what sort of audio features this network is trying to predict, let's take a look at some of the clusters that our model uses."
        ],
        [
            "Here's here's one.",
            "Master this is showing I'm going to show visualization where I play the audio clips.",
            "That are closest to the centroid of this cluster, and I'll also show the video that goes along with it, though I didn't know we didn't specifically use this in the clustering.",
            "So you can hear these audio clips mostly contain the sound of wind.",
            "And even though we didn't actually use vision in the clustering, the images tend to be similar there mostly outdoor scenes there.",
            "Often nearwater were more likely to hear it allowed wind noise, and there are also some clusters, though that contained very different information.",
            "So here's another one of the 30 clusters that we use.",
            "Again, I'll show you the top audio clips.",
            "Who isn't always smiling."
        ],
        [
            "So in contrast to his last cluster, these tend to be quiet indoor scenes, and they often have speech, particularly speech of an infant in them.",
            "Perhaps because it's a very common for parents to record their child in a quiet home.",
            "This is very common kind of video.",
            "So these are the kind of audio features that are models trying to predict.",
            "So now we like to evaluate how much information the content is learning about the world by predicting this audio representation to test this week and just see how well the features that it learns work on object recognition.",
            "So what we're going to do is we're going to represent each image in the Pascal Object classification task using the feature activations from one of the inner layers of the network, specifically the pool five layer of.",
            "Of this, Alex Net Network and then will just train a linear SVM on these features to distinguish between different object categories.",
            "So in particular we're not actually updating the confident we're not fine tuning it or just seeing how well its features work for other tasks."
        ],
        [
            "And we found was that even though this model was trained using a very different technique, the performance is comparable to other previously published unsupervised learning methods.",
            "Though I think in this easy there are some that now perform a bit better than ours.",
            "Anne.",
            "We also compared to a few baselines, we can reduce the information in the audio texture."
        ],
        [
            "So that essentially only contains spectral information which results in a small drop in performance."
        ],
        [
            "And we can also replace the audio, but also just completely remove audio from the system and just use handcrafted visual features based on text ONS to do the cluster assignment.",
            "We find this results in a substantial drop in the performance."
        ],
        [
            "So this is essentially a scene understanding task.",
            "We're trying to predict what sounds are in a scene.",
            "We expect these features also work well on scene recognition tasks.",
            "So to test this we used our feature or features for classification on the sun data set, and we found again that our model performed comparably to other state of the art on supervised learning methods.",
            "So these results suggest to the neck networks capturing some information about objects and scenes would be nice if we could get a better understanding of what the models actually doing, like what aspects of the visual world is it learning about to solve this problem and to address this, we can use some recent visualization work that tries to visualize what the different convolutional units of the network respond to.",
            "This work is found, for example, that if you train the network to recognize hand labeled scenes then it learns units that respond selectively to objects like trees and buildings that make up scenes.",
            "So."
        ],
        [
            "Yes, what sort of units would appear for our problem?",
            "And to test this we followed this other work.",
            "We took the units in the last call."
        ],
        [
            "Solution a layer of the model, and for each unit we found the image IT images in the test said that it responded most strongly to."
        ],
        [
            "And then we masked out the parts of these image that had the strongest response.",
            "For example, this is one of the 256 units in the in the layer and its top responses all corresponds to pictures of faces.",
            "And there are many other units in their work that also respond to faces."
        ],
        [
            "And other units to respond to different other kinds of people.",
            "For example, there is."
        ],
        [
            "There are several.",
            "There are many units that respond to infants faces."
        ],
        [
            "Oops.",
            "There are many units that respond to infants faces and also units respond to crowds.",
            "For example, at a stadium."
        ],
        [
            "They're also units that respond to structures in the world like waterfalls and.",
            "The ocean."
        ],
        [
            "There are units that fire on the Sky."
        ],
        [
            "And also cars that are on a Rd in the paper we more systematically study this using Mechanical Turk workers.",
            "We have mechanical Turk workers evaluate the semantics of the units and we look at the different distribution of objects that the network contains and these results are consistent with the hypothesis.",
            "The network learns units that are selective to objects that are associated with characteristic sounds.",
            "It has more units for things like objects for things like people and waterfalls relatively speaking.",
            "Then a model that's just trained to predict scene categories on places."
        ],
        [
            "Sorry, that's a model that's trained to predict seen labels from human annotations.",
            "So in this work we've seen that ambient sound when you pair it up with video can be a useful signal for learning about objects and scenes, and the information that sound provides is also complementary to what we get from methods to train only on images, and what's most exciting to us about this work is.",
            "Is that the data is very easy to get?",
            "We can just get.",
            "We can just take videos that we already have in these videos, have soundtracks, but they're not currently being used for visual learning."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to talk today about our work on training vision systems using sound in place of labeled training data.",
                    "label": 0
                },
                {
                    "sent": "In particular, we'd like to take advantage of the situation that happens pretty often.",
                    "label": 0
                },
                {
                    "sent": "We both see an object and hear sounds that are closely associated with it.",
                    "label": 0
                },
                {
                    "sent": "For example, when we're at the beach, we might see water and sand, and we might also hear the sound of crashing waves.",
                    "label": 0
                },
                {
                    "sent": "You might hear the sound of people speaking.",
                    "label": 0
                },
                {
                    "sent": "We're on the street.",
                    "label": 0
                },
                {
                    "sent": "We hear the sound of cars.",
                    "label": 0
                },
                {
                    "sent": "So in all these cases, the audio in these videos is telling us a lot about the semantics of a scene, so would be nice if we could use it as a source of supervision.",
                    "label": 0
                },
                {
                    "sent": "In other words, if we could predict the sound and place of object or scene labels to learn visual representations.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this idea is very closely related to work and unsupervised learning, and in particular recent work and so called self supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And these methods generally work by taking an image holding.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out some part of it and then trying to predict the hellebaut part from the rest of the image and analogously.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We'd like to take a video as input, hold out the audio.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ann predicted from the image, with the assumption that through through doing this will learn a visual representation through.",
                    "label": 0
                },
                {
                    "sent": "To solve this task that's useful for other things and well in some ways this may seem very similar to the image based approach.",
                    "label": 0
                },
                {
                    "sent": "There are a number of potential advantages to getting our supervision from audio rather than driving it somehow from the images themselves.",
                    "label": 1
                },
                {
                    "sent": "These advantages come from the fact.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Activated audio and visual data are somewhat orthogonal, and in particular you can change the appearance of a scene quite drastically while still preserving the audio.",
                    "label": 0
                },
                {
                    "sent": "For example, you can change the lighting.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can change the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of the camera.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And all of these visual transformations won't really significantly change the sound, but the images will change, and so an algorithm that's trying to predict audio from images will have to be in variance to these different transformations, and we can take this idea to more of an extreme.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can just if we change the composition of the scene, it will make the same sound as long as it has the same sound producing objects.",
                    "label": 0
                },
                {
                    "sent": "For example, here's some other video clips that are whose audio is very similar to this beach scene.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In each case, these video clips are very closely clustered in audio space.",
                    "label": 1
                },
                {
                    "sent": "They have very similar sounds, but I think we all agree that in image space they're very different.",
                    "label": 0
                },
                {
                    "sent": "One image has a picture of a person and another one has a bridge with people on the bridge and so an algorithm that was trained only on image data might not be able to understand what makes these images similar to each other.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, an algorithm that's trained to predict audio from images.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will be forced to explain why all of these videos have the same sounds associated with them.",
                    "label": 0
                },
                {
                    "sent": "Why they all have this flowing water sound, so either they'll have to detect water in the images or they'll have to recognize that it's a scene like a bridge or a beach that's likely to contain water.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so another way to look at this is that there are some structures in the world that produce sound and generate images.",
                    "label": 0
                },
                {
                    "sent": "Things like people, flowing water cars, and if we train an algorithm to predict sound, the algorithm will have to detect these structures in order to produce the right sounds.",
                    "label": 0
                },
                {
                    "sent": "So how do we actually go about predicting sound from images?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In some recent work that's addressed this, such as the paper that we published at CPR this year, but the kinds of videos that normally are considered in this other work tend to be very specialized.",
                    "label": 0
                },
                {
                    "sent": "For example, they might only consider human speech, or they might consider videos of someone hitting things with a drumstick.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the sounds that we see here in natural videos or more loosely correlated with the actions that we're seeing on the screen.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, if we consider this video of this coffee shop again, if you look carefully, you can see that the sounds you're hearing aren't actually.",
                    "label": 0
                },
                {
                    "sent": "Totally being generated by the people on the screen.",
                    "label": 0
                },
                {
                    "sent": "We hear sounds and some of them might be produced by onscreen objects.",
                    "label": 0
                },
                {
                    "sent": "Some of them might be generated by offscreen objects we're hearing is mostly ambient or background.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sound.",
                    "label": 0
                },
                {
                    "sent": "And this has two implications.",
                    "label": 0
                },
                {
                    "sent": "First of all, it suggests that we can probably predict the audio from a single frame rather than from a long video without losing too much useful information.",
                    "label": 0
                },
                {
                    "sent": "And second, it suggests that since it's so hard to get the timing right, the actions aren't very well indicating what the sound should be at any given time step.",
                    "label": 0
                },
                {
                    "sent": "Then we should use an audio representation that captures summary statistics of the audio over longer periods of time.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we'll do this by using an audio representation known as a sound texture.",
                    "label": 0
                },
                {
                    "sent": "It's very similar to the image texture work of Portilla and Simoncelli, and the way this works is we'll start with coclea gram representation of the audio.",
                    "label": 0
                },
                {
                    "sent": "And then we'll compute a number of summary statistics from this coclea gram the Coakley gram is just very similar to a spectrogram.",
                    "label": 0
                },
                {
                    "sent": "We get it by filtering the audio with the Bank of filters that are two in the different frequencies and taking the envelope of each frequency of each filter response.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The summary statistics are things like the average.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Value of each frequency band and its standard deviation.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then there are higher order statistics like the response of a filterbank to the Coakley gram.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also include.",
                    "label": 0
                },
                {
                    "sent": "Correlations between different frequency bands.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the result is this collection of these different temporally averaged statistics and these are what we're going to try to predict from images.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to do this is using a convolutional neural network.",
                    "label": 0
                },
                {
                    "sent": "The convolutional neural net will take a single video frame.",
                    "label": 0
                },
                {
                    "sent": "And it will try to predict the sound texture for that video frame.",
                    "label": 1
                },
                {
                    "sent": "And for training we used a large data set of unlabeled video we just took about 180,000 videos from the Flickr video data set.",
                    "label": 0
                },
                {
                    "sent": "Sample 10 frames for each one and use this train the model.",
                    "label": 0
                },
                {
                    "sent": "And then rather than actually.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Predicting the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sound texture directly as a regression problem.",
                    "label": 0
                },
                {
                    "sent": "We simplified things by converting the problem into a classification problem.",
                    "label": 0
                },
                {
                    "sent": "Specifically, we created a vector quantization of the audio features using K means.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or using a slightly more complicated approach using LSH and then, rather than predicting the audio features we try to predict which close.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After the audio features belong to.",
                    "label": 0
                },
                {
                    "sent": "So the end result is a network that's essentially just a vanilla Alex net CNN that picks the sound category that best matches the image.",
                    "label": 0
                },
                {
                    "sent": "Now to get a better understanding for what sort of audio features this network is trying to predict, let's take a look at some of the clusters that our model uses.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's here's one.",
                    "label": 0
                },
                {
                    "sent": "Master this is showing I'm going to show visualization where I play the audio clips.",
                    "label": 0
                },
                {
                    "sent": "That are closest to the centroid of this cluster, and I'll also show the video that goes along with it, though I didn't know we didn't specifically use this in the clustering.",
                    "label": 0
                },
                {
                    "sent": "So you can hear these audio clips mostly contain the sound of wind.",
                    "label": 0
                },
                {
                    "sent": "And even though we didn't actually use vision in the clustering, the images tend to be similar there mostly outdoor scenes there.",
                    "label": 0
                },
                {
                    "sent": "Often nearwater were more likely to hear it allowed wind noise, and there are also some clusters, though that contained very different information.",
                    "label": 0
                },
                {
                    "sent": "So here's another one of the 30 clusters that we use.",
                    "label": 0
                },
                {
                    "sent": "Again, I'll show you the top audio clips.",
                    "label": 0
                },
                {
                    "sent": "Who isn't always smiling.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in contrast to his last cluster, these tend to be quiet indoor scenes, and they often have speech, particularly speech of an infant in them.",
                    "label": 0
                },
                {
                    "sent": "Perhaps because it's a very common for parents to record their child in a quiet home.",
                    "label": 0
                },
                {
                    "sent": "This is very common kind of video.",
                    "label": 0
                },
                {
                    "sent": "So these are the kind of audio features that are models trying to predict.",
                    "label": 0
                },
                {
                    "sent": "So now we like to evaluate how much information the content is learning about the world by predicting this audio representation to test this week and just see how well the features that it learns work on object recognition.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to represent each image in the Pascal Object classification task using the feature activations from one of the inner layers of the network, specifically the pool five layer of.",
                    "label": 0
                },
                {
                    "sent": "Of this, Alex Net Network and then will just train a linear SVM on these features to distinguish between different object categories.",
                    "label": 0
                },
                {
                    "sent": "So in particular we're not actually updating the confident we're not fine tuning it or just seeing how well its features work for other tasks.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we found was that even though this model was trained using a very different technique, the performance is comparable to other previously published unsupervised learning methods.",
                    "label": 0
                },
                {
                    "sent": "Though I think in this easy there are some that now perform a bit better than ours.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We also compared to a few baselines, we can reduce the information in the audio texture.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that essentially only contains spectral information which results in a small drop in performance.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can also replace the audio, but also just completely remove audio from the system and just use handcrafted visual features based on text ONS to do the cluster assignment.",
                    "label": 0
                },
                {
                    "sent": "We find this results in a substantial drop in the performance.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is essentially a scene understanding task.",
                    "label": 0
                },
                {
                    "sent": "We're trying to predict what sounds are in a scene.",
                    "label": 0
                },
                {
                    "sent": "We expect these features also work well on scene recognition tasks.",
                    "label": 0
                },
                {
                    "sent": "So to test this we used our feature or features for classification on the sun data set, and we found again that our model performed comparably to other state of the art on supervised learning methods.",
                    "label": 0
                },
                {
                    "sent": "So these results suggest to the neck networks capturing some information about objects and scenes would be nice if we could get a better understanding of what the models actually doing, like what aspects of the visual world is it learning about to solve this problem and to address this, we can use some recent visualization work that tries to visualize what the different convolutional units of the network respond to.",
                    "label": 0
                },
                {
                    "sent": "This work is found, for example, that if you train the network to recognize hand labeled scenes then it learns units that respond selectively to objects like trees and buildings that make up scenes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, what sort of units would appear for our problem?",
                    "label": 0
                },
                {
                    "sent": "And to test this we followed this other work.",
                    "label": 0
                },
                {
                    "sent": "We took the units in the last call.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solution a layer of the model, and for each unit we found the image IT images in the test said that it responded most strongly to.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we masked out the parts of these image that had the strongest response.",
                    "label": 0
                },
                {
                    "sent": "For example, this is one of the 256 units in the in the layer and its top responses all corresponds to pictures of faces.",
                    "label": 1
                },
                {
                    "sent": "And there are many other units in their work that also respond to faces.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And other units to respond to different other kinds of people.",
                    "label": 0
                },
                {
                    "sent": "For example, there is.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are several.",
                    "label": 0
                },
                {
                    "sent": "There are many units that respond to infants faces.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "There are many units that respond to infants faces and also units respond to crowds.",
                    "label": 0
                },
                {
                    "sent": "For example, at a stadium.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They're also units that respond to structures in the world like waterfalls and.",
                    "label": 0
                },
                {
                    "sent": "The ocean.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are units that fire on the Sky.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also cars that are on a Rd in the paper we more systematically study this using Mechanical Turk workers.",
                    "label": 0
                },
                {
                    "sent": "We have mechanical Turk workers evaluate the semantics of the units and we look at the different distribution of objects that the network contains and these results are consistent with the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The network learns units that are selective to objects that are associated with characteristic sounds.",
                    "label": 0
                },
                {
                    "sent": "It has more units for things like objects for things like people and waterfalls relatively speaking.",
                    "label": 0
                },
                {
                    "sent": "Then a model that's just trained to predict scene categories on places.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, that's a model that's trained to predict seen labels from human annotations.",
                    "label": 0
                },
                {
                    "sent": "So in this work we've seen that ambient sound when you pair it up with video can be a useful signal for learning about objects and scenes, and the information that sound provides is also complementary to what we get from methods to train only on images, and what's most exciting to us about this work is.",
                    "label": 0
                },
                {
                    "sent": "Is that the data is very easy to get?",
                    "label": 0
                },
                {
                    "sent": "We can just get.",
                    "label": 0
                },
                {
                    "sent": "We can just take videos that we already have in these videos, have soundtracks, but they're not currently being used for visual learning.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}