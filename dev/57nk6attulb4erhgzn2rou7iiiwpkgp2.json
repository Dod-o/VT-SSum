{
    "id": "57nk6attulb4erhgzn2rou7iiiwpkgp2",
    "title": "Exact Acceleration of Linear Object Detectors",
    "info": {
        "author": [
            "Charles Dubout, IDIAP Research Institute"
        ],
        "chairman": [
            "Antonio Torralba, Center for Future Civic Media, Massachusetts Institute of Technology, MIT",
            "Stefan Carlsson, KTH - Royal Institute of Technology"
        ],
        "published": "Nov. 12, 2012",
        "recorded": "October 2012",
        "category": [
            "Top->Computer Science->Computer Vision->Object Recognition"
        ]
    },
    "url": "http://videolectures.net/eccv2012_dubout_detectors/",
    "segmentation": [
        [
            "So good morning everyone.",
            "Welcome to my presentation of the paper exact acceleration of linear object detectors.",
            "I am Salgia Boo and this is a joint work with my advisor for flooring."
        ],
        [
            "So here's the outline of my talk, and I will first begin with some theory."
        ],
        [
            "Only a little bit.",
            "So the sliding window technique is commonly used to turn a detection problem into a simpler binary classification one."
        ],
        [
            "It consists of applying binary classifier at every image position and scales asking it whether the object of interest is presented at location or not."
        ],
        [
            "It can be interpreted as sweeping the detection window across the whole image at at different sizes.",
            "So here is drawn a simple position grid and some possible examples that one could use in order to train a detector."
        ],
        [
            "Standard detector is the linear SVM combined with like features such as telling tricks, histogram of oriented gradients.",
            "Linear is iiams ever suddenly become hugely popular due to their simplicity and efficiency, allowing large scale training as well as a good performance when combined with discriminative features."
        ],
        [
            "Hog features are organized and agreed as depicted in the left of this slide.",
            "But they can also be seen as organized into planes.",
            "Each plane containing a distinct grid cell feature.",
            "In the case of hug, a distinct edge orientation as depicted on the right.",
            "So detector computes inner products with each plane and send the results together."
        ],
        [
            "Introduced more recently, the mixture of deformable part based models of pheasant rabbit.",
            "All are considered the current state of the art detection method having one all Pascal VLC detection challenges since their introduction in 2007.",
            "They extend the previous model to cope with much more complicated object poses on their formations.",
            "The mixture handles severe changes in appearance, for example, due to the viewpoint.",
            "Why is the models asset deformable?",
            "Because their constituent parts can move relatively to their route.",
            "Here are represented 3.",
            "The other three are just symmetric version of these components of the bicycle mixture used to detect the bicycle in the left images.",
            "On top, as a route filters recognizing the overall shape of the bicycle at the course resolution, and below are the multiple parts filter at the final resolution position that the reference location."
        ],
        [
            "So the strengths of part based models comes at a price that is the number of linear filters."
        ],
        [
            "To give you a better idea of that price, here's a typical example.",
            "On top of the 1080 linear filters coming from the nearest VM's.",
            "Each filter contains 32 feature planes.",
            "On the left, as a pyramid levels containing the hog feature planes extracted from the input image at 50 different scales and of total area roughly equal to the number of pixels of the original image.",
            "Also each containing 32 feature planes."
        ],
        [
            "To compute the detector score at every image location no less than 1.7 millions convolutions between feature planes are required.",
            "Our contributions that they will present now, along with some results, make use of property of the four year transform and several implementation tricks to speed up those convolutions."
        ],
        [
            "This scheme shows the standard convolution process.",
            "So how image is first extracted from the input image before being convolved with the filters feature plane by feature plane before swimming the results together to obtain the final detection score?"
        ],
        [
            "The computational cost is basically proportional to everything, that is, the number of features K of filters, L the size of the hog image MN on the size of the filters PQ."
        ],
        [
            "Well known method to accelerate convolutions is to transform the signals first using a fast Fourier transform and to compute them in the frequency domain.",
            "Once it becomes simple pointwise multiplication's.",
            "So as as before, Hog features are first extracted from the input image, then the full transforms of this hog image under the filters are computed.",
            "Each convolution can now be obtained from a pointwise multiplication in frequency space.",
            "And now it's a competition and cost is no more proportional to the size of a filter speak you.",
            "And is dominated by the cost of the investment firms.",
            "So even though we removed PQ, so the size of the filter because of the additional factor log of MN, the speed gain is small, less than two in our experiments."
        ],
        [
            "So here is a scheme of optimized conversion process.",
            "The only difference from the previous one is that instead of taking the investments from of every feature plane and then summing the results together, we use the linearity property of the transform and do the same before taking the invest.",
            "This reduces the number of investment firms from K to one image filter convolution.",
            "If both scale and L, so the number of features and a filters out of the order of log in or more, the whole cost becoming bigger of KLMN again by a factor of PQ compared to the standard convolution process."
        ],
        [
            "Now let's be more precise and plug in typical numbers and controlled by part based models."
        ],
        [
            "If we do not use the trick with the linearity property of the four year transform, the number of floating point operations compared to the standard convolution process is only reduced by half.",
            "But if we use it, we gain a factor certain.",
            "Now this is a nice in theory, but in practice one is likely to run into two major problems."
        ],
        [
            "This problem is memory usage.",
            "Analysis relied on the fact that the transforms of the filters were already precomputed.",
            "But the pointwise multiplication's requires the sizes of the images and filters transforms to be the same.",
            "So first obvious approach.",
            "Is to store the transforms of the filters, but it at all possible image sizes.",
            "But images come that many different sizes, if only because they are proposed processed at multiple scales and it is unrealistic in terms of memory consumption to store the transform of the filters at all the sizes."
        ],
        [
            "So a second obvious approaches to find all images to the biggest image size.",
            "Then the filters would need to be transformed only at the biggest size and this use as few memory as possible.",
            "Problem is, it is very computationally inefficient because now the images contain mostly padding."
        ],
        [
            "We therefore propose a third approach, getting the best of the first 2 cities.",
            "Efficiency of the first one and memory consumption of the second one.",
            "And we do so by packing images together into Patch works of the size of the biggest image.",
            "The figure is a bit misleading as it used on scaling factor by a factor of two, while in reality a much smaller factor is used.",
            "Also typically a better packing then represented can be obtained."
        ],
        [
            "The second problem is memory bandwidth.",
            "As you may know, more than CPU's have several kind of memories very fast but very small caches and a big but slow main memory."
        ],
        [
            "So another strategy to compute all our times L pointwise multiplication's would look over.",
            "There are Patch works and L filters unload both from the main memory each time he represented in red as a slow main memory accesses."
        ],
        [
            "In green as a beneficial scores compute computed by pointwise multiplying the current patchwork and filter."
        ],
        [
            "We end up reading 2LR Patch works from main memory as a cache is not big enough to hold all of the filters and pet rocks at the same time."
        ],
        [
            "So we use a strategy called blocking to solve this problem."
        ],
        [
            "With the composer transforms into fragments, making sure that fragments are small enough so that they can all fit in the cache."
        ],
        [
            "We can now compute all products involving them, loading them only once from main memory."
        ],
        [
            "So in the end we loaded helpless are Patch works instead of two alarm to compute the same thing."
        ],
        [
            "Finally, here are some results about the average time needed to do the convolutions for one class of the Pascal you see challenge using the publicly available models and implementation distributed by felzenszwalb it all.",
            "We used the fastest convolution implementation as a baseline.",
            "They noted before in this table.",
            "Below are results using our own implementation of the fully convolution using the three previously described tricks and the mean speedup factor is 7.4."
        ],
        [
            "Of course, the method is exact and so obtains the same error rate."
        ],
        [
            "And its numerical accuracy is even a bit better."
        ],
        [
            "So to conclude, the method I presented this computationally very efficient when using multiple features and filters.",
            "Implementation is available online and an open source license, and I invite you to try it out and do not hesitate to contact me if you have any comments or additional questions."
        ],
        [
            "Thank you for your attention.",
            "Maybe you didn't get that soon.",
            "The last slide we were comparing to the."
        ],
        [
            "Diploma cascade approach.",
            "Or the standard approach?",
            "Sorry, I didn't know that reform results there.",
            "The speed it is relative to the DPM cascade approach as cable parts.",
            "So you mean you could get a speedup with cascade, yet I think the speedup is about 10 times with yes, there's a speedup is similar, so the advantage here is that we don't have to tune any cascade.",
            "It's OK exactly.",
            "OK, thank you very much so."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "Welcome to my presentation of the paper exact acceleration of linear object detectors.",
                    "label": 1
                },
                {
                    "sent": "I am Salgia Boo and this is a joint work with my advisor for flooring.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the outline of my talk, and I will first begin with some theory.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only a little bit.",
                    "label": 0
                },
                {
                    "sent": "So the sliding window technique is commonly used to turn a detection problem into a simpler binary classification one.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It consists of applying binary classifier at every image position and scales asking it whether the object of interest is presented at location or not.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It can be interpreted as sweeping the detection window across the whole image at at different sizes.",
                    "label": 0
                },
                {
                    "sent": "So here is drawn a simple position grid and some possible examples that one could use in order to train a detector.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Standard detector is the linear SVM combined with like features such as telling tricks, histogram of oriented gradients.",
                    "label": 0
                },
                {
                    "sent": "Linear is iiams ever suddenly become hugely popular due to their simplicity and efficiency, allowing large scale training as well as a good performance when combined with discriminative features.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hog features are organized and agreed as depicted in the left of this slide.",
                    "label": 0
                },
                {
                    "sent": "But they can also be seen as organized into planes.",
                    "label": 1
                },
                {
                    "sent": "Each plane containing a distinct grid cell feature.",
                    "label": 0
                },
                {
                    "sent": "In the case of hug, a distinct edge orientation as depicted on the right.",
                    "label": 0
                },
                {
                    "sent": "So detector computes inner products with each plane and send the results together.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Introduced more recently, the mixture of deformable part based models of pheasant rabbit.",
                    "label": 0
                },
                {
                    "sent": "All are considered the current state of the art detection method having one all Pascal VLC detection challenges since their introduction in 2007.",
                    "label": 0
                },
                {
                    "sent": "They extend the previous model to cope with much more complicated object poses on their formations.",
                    "label": 0
                },
                {
                    "sent": "The mixture handles severe changes in appearance, for example, due to the viewpoint.",
                    "label": 0
                },
                {
                    "sent": "Why is the models asset deformable?",
                    "label": 0
                },
                {
                    "sent": "Because their constituent parts can move relatively to their route.",
                    "label": 0
                },
                {
                    "sent": "Here are represented 3.",
                    "label": 0
                },
                {
                    "sent": "The other three are just symmetric version of these components of the bicycle mixture used to detect the bicycle in the left images.",
                    "label": 0
                },
                {
                    "sent": "On top, as a route filters recognizing the overall shape of the bicycle at the course resolution, and below are the multiple parts filter at the final resolution position that the reference location.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the strengths of part based models comes at a price that is the number of linear filters.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To give you a better idea of that price, here's a typical example.",
                    "label": 0
                },
                {
                    "sent": "On top of the 1080 linear filters coming from the nearest VM's.",
                    "label": 0
                },
                {
                    "sent": "Each filter contains 32 feature planes.",
                    "label": 0
                },
                {
                    "sent": "On the left, as a pyramid levels containing the hog feature planes extracted from the input image at 50 different scales and of total area roughly equal to the number of pixels of the original image.",
                    "label": 0
                },
                {
                    "sent": "Also each containing 32 feature planes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To compute the detector score at every image location no less than 1.7 millions convolutions between feature planes are required.",
                    "label": 0
                },
                {
                    "sent": "Our contributions that they will present now, along with some results, make use of property of the four year transform and several implementation tricks to speed up those convolutions.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This scheme shows the standard convolution process.",
                    "label": 0
                },
                {
                    "sent": "So how image is first extracted from the input image before being convolved with the filters feature plane by feature plane before swimming the results together to obtain the final detection score?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The computational cost is basically proportional to everything, that is, the number of features K of filters, L the size of the hog image MN on the size of the filters PQ.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well known method to accelerate convolutions is to transform the signals first using a fast Fourier transform and to compute them in the frequency domain.",
                    "label": 0
                },
                {
                    "sent": "Once it becomes simple pointwise multiplication's.",
                    "label": 0
                },
                {
                    "sent": "So as as before, Hog features are first extracted from the input image, then the full transforms of this hog image under the filters are computed.",
                    "label": 0
                },
                {
                    "sent": "Each convolution can now be obtained from a pointwise multiplication in frequency space.",
                    "label": 0
                },
                {
                    "sent": "And now it's a competition and cost is no more proportional to the size of a filter speak you.",
                    "label": 0
                },
                {
                    "sent": "And is dominated by the cost of the investment firms.",
                    "label": 0
                },
                {
                    "sent": "So even though we removed PQ, so the size of the filter because of the additional factor log of MN, the speed gain is small, less than two in our experiments.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a scheme of optimized conversion process.",
                    "label": 0
                },
                {
                    "sent": "The only difference from the previous one is that instead of taking the investments from of every feature plane and then summing the results together, we use the linearity property of the transform and do the same before taking the invest.",
                    "label": 0
                },
                {
                    "sent": "This reduces the number of investment firms from K to one image filter convolution.",
                    "label": 0
                },
                {
                    "sent": "If both scale and L, so the number of features and a filters out of the order of log in or more, the whole cost becoming bigger of KLMN again by a factor of PQ compared to the standard convolution process.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's be more precise and plug in typical numbers and controlled by part based models.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we do not use the trick with the linearity property of the four year transform, the number of floating point operations compared to the standard convolution process is only reduced by half.",
                    "label": 1
                },
                {
                    "sent": "But if we use it, we gain a factor certain.",
                    "label": 0
                },
                {
                    "sent": "Now this is a nice in theory, but in practice one is likely to run into two major problems.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This problem is memory usage.",
                    "label": 0
                },
                {
                    "sent": "Analysis relied on the fact that the transforms of the filters were already precomputed.",
                    "label": 0
                },
                {
                    "sent": "But the pointwise multiplication's requires the sizes of the images and filters transforms to be the same.",
                    "label": 1
                },
                {
                    "sent": "So first obvious approach.",
                    "label": 0
                },
                {
                    "sent": "Is to store the transforms of the filters, but it at all possible image sizes.",
                    "label": 0
                },
                {
                    "sent": "But images come that many different sizes, if only because they are proposed processed at multiple scales and it is unrealistic in terms of memory consumption to store the transform of the filters at all the sizes.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a second obvious approaches to find all images to the biggest image size.",
                    "label": 0
                },
                {
                    "sent": "Then the filters would need to be transformed only at the biggest size and this use as few memory as possible.",
                    "label": 1
                },
                {
                    "sent": "Problem is, it is very computationally inefficient because now the images contain mostly padding.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We therefore propose a third approach, getting the best of the first 2 cities.",
                    "label": 1
                },
                {
                    "sent": "Efficiency of the first one and memory consumption of the second one.",
                    "label": 0
                },
                {
                    "sent": "And we do so by packing images together into Patch works of the size of the biggest image.",
                    "label": 0
                },
                {
                    "sent": "The figure is a bit misleading as it used on scaling factor by a factor of two, while in reality a much smaller factor is used.",
                    "label": 0
                },
                {
                    "sent": "Also typically a better packing then represented can be obtained.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second problem is memory bandwidth.",
                    "label": 0
                },
                {
                    "sent": "As you may know, more than CPU's have several kind of memories very fast but very small caches and a big but slow main memory.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another strategy to compute all our times L pointwise multiplication's would look over.",
                    "label": 0
                },
                {
                    "sent": "There are Patch works and L filters unload both from the main memory each time he represented in red as a slow main memory accesses.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In green as a beneficial scores compute computed by pointwise multiplying the current patchwork and filter.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We end up reading 2LR Patch works from main memory as a cache is not big enough to hold all of the filters and pet rocks at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we use a strategy called blocking to solve this problem.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the composer transforms into fragments, making sure that fragments are small enough so that they can all fit in the cache.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can now compute all products involving them, loading them only once from main memory.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the end we loaded helpless are Patch works instead of two alarm to compute the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, here are some results about the average time needed to do the convolutions for one class of the Pascal you see challenge using the publicly available models and implementation distributed by felzenszwalb it all.",
                    "label": 0
                },
                {
                    "sent": "We used the fastest convolution implementation as a baseline.",
                    "label": 0
                },
                {
                    "sent": "They noted before in this table.",
                    "label": 0
                },
                {
                    "sent": "Below are results using our own implementation of the fully convolution using the three previously described tricks and the mean speedup factor is 7.4.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, the method is exact and so obtains the same error rate.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And its numerical accuracy is even a bit better.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to conclude, the method I presented this computationally very efficient when using multiple features and filters.",
                    "label": 0
                },
                {
                    "sent": "Implementation is available online and an open source license, and I invite you to try it out and do not hesitate to contact me if you have any comments or additional questions.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you for your attention.",
                    "label": 1
                },
                {
                    "sent": "Maybe you didn't get that soon.",
                    "label": 0
                },
                {
                    "sent": "The last slide we were comparing to the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Diploma cascade approach.",
                    "label": 0
                },
                {
                    "sent": "Or the standard approach?",
                    "label": 0
                },
                {
                    "sent": "Sorry, I didn't know that reform results there.",
                    "label": 0
                },
                {
                    "sent": "The speed it is relative to the DPM cascade approach as cable parts.",
                    "label": 0
                },
                {
                    "sent": "So you mean you could get a speedup with cascade, yet I think the speedup is about 10 times with yes, there's a speedup is similar, so the advantage here is that we don't have to tune any cascade.",
                    "label": 0
                },
                {
                    "sent": "It's OK exactly.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much so.",
                    "label": 0
                }
            ]
        }
    }
}