{
    "id": "azvkry672cqokyfjua2xywqd7w4yf6hm",
    "title": "Surrogate Regret Bounds for the Area Under the ROC Curve via Strongly Proper Losses",
    "info": {
        "author": [
            "Shivani Agarwal, Department of Computer Science and Automation (CSA), Indian Institute of Science Bangalore"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_agarwal_bounds/",
    "segmentation": [
        [
            "Good afternoon everyone, so this talk has two titles.",
            "The first one is this which is the official title of the paper and."
        ],
        [
            "The second one is this, which is one of the conclusions of the paper.",
            "You're welcome to choose whichever you like.",
            "Those of you who are familiar with AUC, optimization, and AUC consistency might be a bit surprised by this conclusion.",
            "By the end of the talk, I hope you'll see why this is not really surprising, after all.",
            "So."
        ],
        [
            "The agent of the Auto C curve or the AUC for short, as all of you know, is widely used performance measuring machine learning just to recall quickly in a binary experiment with two types of examples, positive and negative, the auto C curve.",
            "The receiver operating character."
        ],
        [
            "Sticker of a scoring function are real valued function on the instant space.",
            "I just plots the true positive rate versus the false positive rate.",
            "So more specifically, for any given threshold."
        ],
        [
            "Beyond this function, we can compute the true positive rate, the probability of a positive example being scored higher than T. And the false positive rate.",
            "The probability of a negative example being scored higher than T plot.",
            "These are the point and as we vary this."
        ],
        [
            "Threshold these points trace out the RC curve of this function.",
            "The area under this curve is what we are interested in this."
        ],
        [
            "And it turns out that this can be written in this form simply, the probability that a randomly drawn positive example is scored or ranked higher bias then randomly drawn negative example.",
            "And this is also why the AC has been used, especially the performance measure in bipartite ranking problems."
        ],
        [
            "Now the empirical you see which is obtained by computing these true positive and false positive rates on a finite sample of positive and negative examples.",
            "I can be doing this with Coxon Mann Whitney Statistic, which takes us pairwise form, and this led to the use of a number of."
        ],
        [
            "Pairwise risk minimization algorithms for optimizing the AUC.",
            "So, for example, rank SVM which minimizes the pairwise hinge loss rank boost which minimizes the pairwise exponential loss.",
            "Rank net, which essentially minimizes a pairwise logistic loss.",
            "This approach enjoys theoretical support because it turns out that many of these pairwise."
        ],
        [
            "Items are in fact AUC, consistent in that in the limit of infinite training data, these algorithms will converge assuming appropriate function clouds and regularization.",
            "They will converge in the best possible way.",
            "You see, this is true for rank post and for rank net.",
            "It's not actually true for rank SVM, so it turns out that there are some distributions for which rank is theme will not converge.",
            "The best possible you see this was shown recently in a very nice paper biomass only.",
            "Now we could be just done here.",
            "We have algorithms that are widely used in practice and are AUC consistent.",
            "However, these algorithms, experiments, algorithms are typically computationally expensive because they involve a quadratic blowup in the training sample size.",
            "On the other hand, any standard."
        ],
        [
            "Algorithms such as standard logistic regression, edibles and so on have also been observed empirically to show good AUC performance.",
            "This raises the natural question.",
            "Do we really need these pairwise algorithms to optimize the AUC?",
            "So the recent work at ICML 2011 Kottlowski, it'll show that any algorithm minimizing a balanced form of a non pairwise exponential logistic loss is AUC consistent.",
            "This result however, doesn't quite explain the success in terms of AUC performance of these standard algorithms.",
            "So in this paper we show that."
        ],
        [
            "So any algorithm minimizing a standard non pairwise strongly proper loss or strongly proper composite loss will define these later is in fact AUC consistent.",
            "And it turns out that these losses include in particular the exponential logistic and the least squares losses, meaning that the standard algorithms minimizing these losses are also using consistent."
        ],
        [
            "So allow spend some time just going through the formal problem set up and explaining some of these previous results.",
            "Then give a quick overview of proper losses.",
            "Define strongly proper losses.",
            "And then explain how these are used to obtain AUC.",
            "Regret bound inconsistency results.",
            "And feel free to let me know if you have any questions anytime."
        ],
        [
            "So the problem is that this is fairly standard.",
            "There's an instance SpaceX is a probability distribution on binary.",
            "Examples.",
            "In this instance space.",
            "There you see of a scoring function F. As we just saw is just the probability that a randomly drawn positive example from this distribution is ranked higher.",
            "By then it randomly drawn.",
            "Negative example, I should point it for the purposes of this talk, I will sometimes give simplified definitions and statements.",
            "More detailed versions are available in the paper.",
            "The optimally you see for a distribution D is just the highest possible value of the usage you can achieve with any scoring function.",
            "And the NCS regret of scoring function is then just the difference between its AUC and the optimal value.",
            "This is the quantity will be seeking to bound.",
            "In particular, if an algorithm has or loans scoring functions that have banishing AUC regret as the training sample size increases, then the algorithm is AOC consistent.",
            "So we'll be analyzing algorithms that minimize various."
        ],
        [
            "Loss functions useful to review these quickly here as well.",
            "So binary loss acting on some prediction space Y hat.",
            "Just assign some non negative penalty to any label prediction pair.",
            "The editor or L risk of function is just the expected loss under this nozzle, or randomly join example from D. The optimal editor is just the smallest possible editor you can get over all such functions and the Elder eacret of such a function is the difference between its regret and the optimal regret.",
            "So just as quick examples, we."
        ],
        [
            "A01 loss, which acts on binary valued predictions.",
            "Define the 01 error and the 01 regret of a classifier H in this manner."
        ],
        [
            "We have the largest take loss which acts on real valued predictions.",
            "Again, you can define some of the quantities for this.",
            "And somebody we have."
        ],
        [
            "Initial loss, which is also which also acts on three valid predictions.",
            "And again, we can define analogous quantities, the exponential error and the exponential regression this form.",
            "So now let's just first review the reduction of.",
            "Can you see optimization?"
        ],
        [
            "The pairwise binary classification, which as I mentioned has been a dominant algorithmic as well as theoretical approach for this problem.",
            "So starting with the distribution D, let's construct a pairwise distribution D~ on the space of pairwise examples.",
            "Examples consisting of pairs of instances from X.",
            "Do the following.",
            "A sample 2 examples randomly and independently from D if they have the same label.",
            "I reject and repeat.",
            "If they have different labels, then enable this pair of instances with ordered pair of instances positive.",
            "If the first label drawn was plus one second is minus one and negative otherwise.",
            "So this is a distribution detailed are on positive and negative examples containing pairs of instances.",
            "Next, for any scoring function F on the original instance space.",
            "So let's construct a scoring function F death on the GNU space of instance pairs.",
            "That just takes the difference of the scores under this function.",
            "Then what clementsen that all showed is that.",
            "The AUC regret of F under the original distribution D is exactly equal to the pairwise binary regret the pairwise zero regret of the sign of it.",
            "If I remember the 01 loss takes by any valid predictions.",
            "On this pairwise distribution detector, so in particular, if we can make this.",
            "Pairwise 01 regret go to zero, then this pairwise the AUC regret also goes to 0.",
            "Now, this 01 regret of course can be upper bounded in terms of a whole host of."
        ],
        [
            "Classification, calibrated losses and this was a classical result of market and colleagues.",
            "So in particular applying this result we have that for any classification calibrated margin based loss.",
            "There is a function strictly increasing function that vanishes at zero, satisfying this relation, and in particular this means that if we can make this pairwise end regret for such a loss in go to zero, then the AUC regret also goes to 0.",
            "This is exactly what the rank pulls in ranked algorithms effectively do, so both the logistic loss and the exponential loss satisfy these conditions, and these algorithms basically make this pairwise regret go to 0.",
            "The hinge loss used in rank SVM is also a classification calibrated margin based loss also satisfies this inequality, but has also only showed their distributions for which this pairwise hinge regret.",
            "If you learn a scoring function using rank SVM does not converge to zero and therefore the algorithm is not necessarily easy consistent.",
            "OK Hun."
        ],
        [
            "The result of kottlowski at all.",
            "Makes use of balance losses, so for any base loss L the balanced loss basically waits the loss by the inverse probabilities of the true label.",
            "Why so notice in particular that such a balance loss depends upon the distribution D via these probabilities?",
            "What control schedule showed is that for the exponential loss.",
            "The pairwise exponential regret of that F dev function with respect to the pairwise distributor which we saw upper bounds.",
            "The AUC regret of F and the D. Isn't telling Upper bounded by the balanced exponential regret of F under D?",
            "I'm sure some of the result holds for the logistic loss.",
            "And when we combine these with the previous results, then we effective."
        ],
        [
            "To obtain these bounds on the AC regret of the scoring function F&D.",
            "In terms of these balanced exponential and logistic regrets of the same function under the same distribution.",
            "So just to summarize."
        ],
        [
            "We've seen so far.",
            "We can bound the AUC regret of a scoring function F under D. In terms of the balance exponential or the balance logistic regret of the same function F, the same distribution D. Which is nice.",
            "However, there are several aspects of this result which is quite.",
            "We're not quite satisfactory.",
            "So first the analysis here still goes via this pairwise classification analysis.",
            "Second, the analysis actually is specific to these two losses.",
            "The exponential logistic causes it's not clear how you would generalize the analysis to other losses.",
            "3rd The The Balanced Regrets Hide terms dependent upon the distribution and it's also hard to optimize these balance losses in practice because the unknown because the true probabilities are unknown effectively.",
            "And finally, in most importantly for our purposes, this result doesn't quite explain the empirical success in terms of AUC, performance of the standard algorithms we discussed, which minimize standard losses, not the balance versions of them.",
            "So this is where proper losses come in and these will allow us to obtain direct bounds that avoid all of these difficulties."
        ],
        [
            "So proper losses have along history.",
            "I'm sure most of you are aware that they've been of significant interest in recent years due to the import."
        ],
        [
            "Role they play in class probability estimation in order to review proper losses, I just need a little bit more notation.",
            "For any base Losail that accident, prediction space, yhat, we define the conditional L risk, which is a function that takes 2 arguments, Eaton 01 and the value I had.",
            "And returns the expected loss in code when predicting Y hat.",
            "When the true label is one with probability to minus one, probably 1 minus beta.",
            "We also define the conditional Bayes error risk, which is a function of just one argument.",
            "It takes an argument Eater and just returns the lowest possible conditional address you can get for this value beta.",
            "So we could talk about proper losses.",
            "And these are used in class probability estimation.",
            "So these basically operate on a prediction space 01.",
            "We see a binary loss acting on this prediction space is proper.",
            "If for any tail this right value Eater is a minimizer of the conditional risk.",
            "We say it's strictly proper.",
            "If this is a unique minimizer.",
            "Hum.",
            "This is a very nice characterization of strictly proper losses that has various."
        ],
        [
            "Both in the literature, we give an alternative, much simpler and self contained proof in the paper for this result, which basically says that.",
            "A proper loss is strictly proper if and only if its conditional Bayes risk is strictly concave.",
            "It's easy to see that the Bayes Conditional Bayes risk this function is always concave for any loss because it's just the point wise in from of a family of linear functions.",
            "But this is that for a proper loss.",
            "It's strictly concave precisely when the loss is strictly proper.",
            "OK.",
            "So at this point, it's useful to think about just single intuitively about why proper losses might be useful for us.",
            "So what we're interested in doing is basically bounding the AUC.",
            "Regret, which is just the difference of the AUC of a scoring function from the optimal agency.",
            "Now the optimally you see is known to be achieved by the true class probability function.",
            "And so, intuitively, if we can find a class probability estimate that is close to the true clarity function, we might hope that the resulting AUC is also close to the optimal.",
            "This of course is not a full argument that it forms the basic idea behind the approach.",
            "It turns out to formalize this idea, we need a slightly stronger form of these proper losses which."
        ],
        [
            "Still strongly proper losses and.",
            "Very clear in just a few moments why this is a reasonable name for these losses.",
            "So we say a loss acting on 01 is Lambda strongly proper, some positive Lambda.",
            "If for any to the true probability it is not only a unique minimizer of the conditional risk, but.",
            "In addition, for any other Eater hat, the conditional risk is bounded away from the minimum in this manner.",
            "And we can show a similar characterization for strongly proper losses as was there for the case of strictly proper losses under mild regularity condition.",
            "Under this condition, we can show that a proper losses slammed a strongly proper if and only if it's conditional based risk is Lambda strongly concave and This is why we refer to these losses are strongly proper.",
            "I'll just quickly mention, for those who are familiar with proper loss, is another way to view strongly proper losses that the weight function which weights the cost sensitive misclassification losses that make up the proper loss, is lower bounded by a constant.",
            "This provides an alternative perspective.",
            "I'd like to thank anonymous reviewer for pointing out this interpretation of these losses.",
            "Home.",
            "OK, so."
        ],
        [
            "What we've seen so far is."
        ],
        [
            "Basic proper losses that act on the prediction space 01.",
            "It turns out one can compose these using a link function to yield proper composite losses on any prediction space.",
            "I won't go into details over here.",
            "There can be found in the paper or in several other recent papers, for example by Mark Reed and Bob Williamson.",
            "What we have is that for any.",
            "Strongly proper loss.",
            "For our purposes, let's assume this strongly proper composite loss acts on the real valued predictions for any such strongly proper composite loss.",
            "They're using regression of a scoring function can be upper bounded directly in terms of the eldrige rid of this function, and the proof is very direct, very simple.",
            "It follows almost directly from the definition of strongly strong properness.",
            "In particular, it guarantees that there exists class probability estimate who's a little distance from the true probability function is upper bounded in terms of the L regret.",
            "Of course, this class property or the Delta distance also related to the AUC regret of it.",
            "That's also relatively simple to do, I'll just say over here there's a suitable transferability estimate that you go to the paper if you're interested in more details.",
            "The proof is actually very simple, just within half a page.",
            "So what we have is for any strongly proper composite loss, we have a direct result, upper bounding the AUC, regret of a function F on D in terms of the Eldridge of on the same distribution D. There's no need for any pairwise analysis here and there.",
            "No hidden balancing terms.",
            "The dependence of this bound on P is explicit here."
        ],
        [
            "I promised the family of strongly proper composite losses fairly broad in particularly includes the exponential, logistic and squared losses as special cases."
        ],
        [
            "I just mentioned briefly that we can also combine this with the recent result of clemenson robbiano to obtain a tighter regret bound for certain low noise conditions.",
            "So the distribution D satisfies a certain noise condition parameterized by some Alpha where Alpha closer to 1 imposes a stronger lower noise condition.",
            "Then we can obtain a bound.",
            "We can improve the exponent in the bound to be greater than half."
        ],
        [
            "OK, so just to summarize, we've given a quantitative regret bound for the AUC for scoring function if in terms of simple loss space regrets for broad family of strongly proper composite losses.",
            "These include in particular common losses like the exponential squared and logistic losses, which means that the standard algorithms many of these added algorithms are in fact AUC consistent.",
            "This explains some of these previous empirical observations that have been made in several papers about.",
            "Easy performance of these algorithms.",
            "I should mention, of course that the pairwise algorithms are still useful for non bipartite settings.",
            "And even in the bipartite setting, it's not clear how the rates of convergence of the AUC for these different approaches compare on different distributions, but as far as AUC consistency is concerned, these standard algorithms are just as good.",
            "Home."
        ],
        [
            "And finally, I just mentioned that these strongly proper losses that have been defined and characterized here might also be useful in studying other types of problems.",
            "So as one example, we've used these losses in studying consistency of algorithms for classification under class imbalance settings.",
            "Some of this work will be presented at ICM next week.",
            "Thank you, I'll be happy to take questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon everyone, so this talk has two titles.",
                    "label": 0
                },
                {
                    "sent": "The first one is this which is the official title of the paper and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second one is this, which is one of the conclusions of the paper.",
                    "label": 0
                },
                {
                    "sent": "You're welcome to choose whichever you like.",
                    "label": 0
                },
                {
                    "sent": "Those of you who are familiar with AUC, optimization, and AUC consistency might be a bit surprised by this conclusion.",
                    "label": 0
                },
                {
                    "sent": "By the end of the talk, I hope you'll see why this is not really surprising, after all.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The agent of the Auto C curve or the AUC for short, as all of you know, is widely used performance measuring machine learning just to recall quickly in a binary experiment with two types of examples, positive and negative, the auto C curve.",
                    "label": 0
                },
                {
                    "sent": "The receiver operating character.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sticker of a scoring function are real valued function on the instant space.",
                    "label": 1
                },
                {
                    "sent": "I just plots the true positive rate versus the false positive rate.",
                    "label": 0
                },
                {
                    "sent": "So more specifically, for any given threshold.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Beyond this function, we can compute the true positive rate, the probability of a positive example being scored higher than T. And the false positive rate.",
                    "label": 0
                },
                {
                    "sent": "The probability of a negative example being scored higher than T plot.",
                    "label": 0
                },
                {
                    "sent": "These are the point and as we vary this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Threshold these points trace out the RC curve of this function.",
                    "label": 0
                },
                {
                    "sent": "The area under this curve is what we are interested in this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turns out that this can be written in this form simply, the probability that a randomly drawn positive example is scored or ranked higher bias then randomly drawn negative example.",
                    "label": 0
                },
                {
                    "sent": "And this is also why the AC has been used, especially the performance measure in bipartite ranking problems.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the empirical you see which is obtained by computing these true positive and false positive rates on a finite sample of positive and negative examples.",
                    "label": 0
                },
                {
                    "sent": "I can be doing this with Coxon Mann Whitney Statistic, which takes us pairwise form, and this led to the use of a number of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pairwise risk minimization algorithms for optimizing the AUC.",
                    "label": 1
                },
                {
                    "sent": "So, for example, rank SVM which minimizes the pairwise hinge loss rank boost which minimizes the pairwise exponential loss.",
                    "label": 0
                },
                {
                    "sent": "Rank net, which essentially minimizes a pairwise logistic loss.",
                    "label": 0
                },
                {
                    "sent": "This approach enjoys theoretical support because it turns out that many of these pairwise.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Items are in fact AUC, consistent in that in the limit of infinite training data, these algorithms will converge assuming appropriate function clouds and regularization.",
                    "label": 0
                },
                {
                    "sent": "They will converge in the best possible way.",
                    "label": 0
                },
                {
                    "sent": "You see, this is true for rank post and for rank net.",
                    "label": 0
                },
                {
                    "sent": "It's not actually true for rank SVM, so it turns out that there are some distributions for which rank is theme will not converge.",
                    "label": 0
                },
                {
                    "sent": "The best possible you see this was shown recently in a very nice paper biomass only.",
                    "label": 0
                },
                {
                    "sent": "Now we could be just done here.",
                    "label": 0
                },
                {
                    "sent": "We have algorithms that are widely used in practice and are AUC consistent.",
                    "label": 0
                },
                {
                    "sent": "However, these algorithms, experiments, algorithms are typically computationally expensive because they involve a quadratic blowup in the training sample size.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, any standard.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithms such as standard logistic regression, edibles and so on have also been observed empirically to show good AUC performance.",
                    "label": 1
                },
                {
                    "sent": "This raises the natural question.",
                    "label": 0
                },
                {
                    "sent": "Do we really need these pairwise algorithms to optimize the AUC?",
                    "label": 0
                },
                {
                    "sent": "So the recent work at ICML 2011 Kottlowski, it'll show that any algorithm minimizing a balanced form of a non pairwise exponential logistic loss is AUC consistent.",
                    "label": 0
                },
                {
                    "sent": "This result however, doesn't quite explain the success in terms of AUC performance of these standard algorithms.",
                    "label": 0
                },
                {
                    "sent": "So in this paper we show that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So any algorithm minimizing a standard non pairwise strongly proper loss or strongly proper composite loss will define these later is in fact AUC consistent.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that these losses include in particular the exponential logistic and the least squares losses, meaning that the standard algorithms minimizing these losses are also using consistent.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So allow spend some time just going through the formal problem set up and explaining some of these previous results.",
                    "label": 1
                },
                {
                    "sent": "Then give a quick overview of proper losses.",
                    "label": 1
                },
                {
                    "sent": "Define strongly proper losses.",
                    "label": 0
                },
                {
                    "sent": "And then explain how these are used to obtain AUC.",
                    "label": 0
                },
                {
                    "sent": "Regret bound inconsistency results.",
                    "label": 0
                },
                {
                    "sent": "And feel free to let me know if you have any questions anytime.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem is that this is fairly standard.",
                    "label": 0
                },
                {
                    "sent": "There's an instance SpaceX is a probability distribution on binary.",
                    "label": 1
                },
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "In this instance space.",
                    "label": 1
                },
                {
                    "sent": "There you see of a scoring function F. As we just saw is just the probability that a randomly drawn positive example from this distribution is ranked higher.",
                    "label": 0
                },
                {
                    "sent": "By then it randomly drawn.",
                    "label": 0
                },
                {
                    "sent": "Negative example, I should point it for the purposes of this talk, I will sometimes give simplified definitions and statements.",
                    "label": 0
                },
                {
                    "sent": "More detailed versions are available in the paper.",
                    "label": 0
                },
                {
                    "sent": "The optimally you see for a distribution D is just the highest possible value of the usage you can achieve with any scoring function.",
                    "label": 0
                },
                {
                    "sent": "And the NCS regret of scoring function is then just the difference between its AUC and the optimal value.",
                    "label": 0
                },
                {
                    "sent": "This is the quantity will be seeking to bound.",
                    "label": 0
                },
                {
                    "sent": "In particular, if an algorithm has or loans scoring functions that have banishing AUC regret as the training sample size increases, then the algorithm is AOC consistent.",
                    "label": 0
                },
                {
                    "sent": "So we'll be analyzing algorithms that minimize various.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Loss functions useful to review these quickly here as well.",
                    "label": 1
                },
                {
                    "sent": "So binary loss acting on some prediction space Y hat.",
                    "label": 1
                },
                {
                    "sent": "Just assign some non negative penalty to any label prediction pair.",
                    "label": 0
                },
                {
                    "sent": "The editor or L risk of function is just the expected loss under this nozzle, or randomly join example from D. The optimal editor is just the smallest possible editor you can get over all such functions and the Elder eacret of such a function is the difference between its regret and the optimal regret.",
                    "label": 0
                },
                {
                    "sent": "So just as quick examples, we.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A01 loss, which acts on binary valued predictions.",
                    "label": 0
                },
                {
                    "sent": "Define the 01 error and the 01 regret of a classifier H in this manner.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have the largest take loss which acts on real valued predictions.",
                    "label": 0
                },
                {
                    "sent": "Again, you can define some of the quantities for this.",
                    "label": 0
                },
                {
                    "sent": "And somebody we have.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Initial loss, which is also which also acts on three valid predictions.",
                    "label": 0
                },
                {
                    "sent": "And again, we can define analogous quantities, the exponential error and the exponential regression this form.",
                    "label": 1
                },
                {
                    "sent": "So now let's just first review the reduction of.",
                    "label": 0
                },
                {
                    "sent": "Can you see optimization?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The pairwise binary classification, which as I mentioned has been a dominant algorithmic as well as theoretical approach for this problem.",
                    "label": 1
                },
                {
                    "sent": "So starting with the distribution D, let's construct a pairwise distribution D~ on the space of pairwise examples.",
                    "label": 0
                },
                {
                    "sent": "Examples consisting of pairs of instances from X.",
                    "label": 0
                },
                {
                    "sent": "Do the following.",
                    "label": 0
                },
                {
                    "sent": "A sample 2 examples randomly and independently from D if they have the same label.",
                    "label": 0
                },
                {
                    "sent": "I reject and repeat.",
                    "label": 0
                },
                {
                    "sent": "If they have different labels, then enable this pair of instances with ordered pair of instances positive.",
                    "label": 0
                },
                {
                    "sent": "If the first label drawn was plus one second is minus one and negative otherwise.",
                    "label": 0
                },
                {
                    "sent": "So this is a distribution detailed are on positive and negative examples containing pairs of instances.",
                    "label": 1
                },
                {
                    "sent": "Next, for any scoring function F on the original instance space.",
                    "label": 0
                },
                {
                    "sent": "So let's construct a scoring function F death on the GNU space of instance pairs.",
                    "label": 0
                },
                {
                    "sent": "That just takes the difference of the scores under this function.",
                    "label": 0
                },
                {
                    "sent": "Then what clementsen that all showed is that.",
                    "label": 0
                },
                {
                    "sent": "The AUC regret of F under the original distribution D is exactly equal to the pairwise binary regret the pairwise zero regret of the sign of it.",
                    "label": 1
                },
                {
                    "sent": "If I remember the 01 loss takes by any valid predictions.",
                    "label": 0
                },
                {
                    "sent": "On this pairwise distribution detector, so in particular, if we can make this.",
                    "label": 0
                },
                {
                    "sent": "Pairwise 01 regret go to zero, then this pairwise the AUC regret also goes to 0.",
                    "label": 0
                },
                {
                    "sent": "Now, this 01 regret of course can be upper bounded in terms of a whole host of.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classification, calibrated losses and this was a classical result of market and colleagues.",
                    "label": 0
                },
                {
                    "sent": "So in particular applying this result we have that for any classification calibrated margin based loss.",
                    "label": 1
                },
                {
                    "sent": "There is a function strictly increasing function that vanishes at zero, satisfying this relation, and in particular this means that if we can make this pairwise end regret for such a loss in go to zero, then the AUC regret also goes to 0.",
                    "label": 1
                },
                {
                    "sent": "This is exactly what the rank pulls in ranked algorithms effectively do, so both the logistic loss and the exponential loss satisfy these conditions, and these algorithms basically make this pairwise regret go to 0.",
                    "label": 0
                },
                {
                    "sent": "The hinge loss used in rank SVM is also a classification calibrated margin based loss also satisfies this inequality, but has also only showed their distributions for which this pairwise hinge regret.",
                    "label": 0
                },
                {
                    "sent": "If you learn a scoring function using rank SVM does not converge to zero and therefore the algorithm is not necessarily easy consistent.",
                    "label": 0
                },
                {
                    "sent": "OK Hun.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The result of kottlowski at all.",
                    "label": 0
                },
                {
                    "sent": "Makes use of balance losses, so for any base loss L the balanced loss basically waits the loss by the inverse probabilities of the true label.",
                    "label": 0
                },
                {
                    "sent": "Why so notice in particular that such a balance loss depends upon the distribution D via these probabilities?",
                    "label": 0
                },
                {
                    "sent": "What control schedule showed is that for the exponential loss.",
                    "label": 0
                },
                {
                    "sent": "The pairwise exponential regret of that F dev function with respect to the pairwise distributor which we saw upper bounds.",
                    "label": 0
                },
                {
                    "sent": "The AUC regret of F and the D. Isn't telling Upper bounded by the balanced exponential regret of F under D?",
                    "label": 0
                },
                {
                    "sent": "I'm sure some of the result holds for the logistic loss.",
                    "label": 0
                },
                {
                    "sent": "And when we combine these with the previous results, then we effective.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To obtain these bounds on the AC regret of the scoring function F&D.",
                    "label": 0
                },
                {
                    "sent": "In terms of these balanced exponential and logistic regrets of the same function under the same distribution.",
                    "label": 0
                },
                {
                    "sent": "So just to summarize.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've seen so far.",
                    "label": 0
                },
                {
                    "sent": "We can bound the AUC regret of a scoring function F under D. In terms of the balance exponential or the balance logistic regret of the same function F, the same distribution D. Which is nice.",
                    "label": 1
                },
                {
                    "sent": "However, there are several aspects of this result which is quite.",
                    "label": 0
                },
                {
                    "sent": "We're not quite satisfactory.",
                    "label": 1
                },
                {
                    "sent": "So first the analysis here still goes via this pairwise classification analysis.",
                    "label": 0
                },
                {
                    "sent": "Second, the analysis actually is specific to these two losses.",
                    "label": 1
                },
                {
                    "sent": "The exponential logistic causes it's not clear how you would generalize the analysis to other losses.",
                    "label": 0
                },
                {
                    "sent": "3rd The The Balanced Regrets Hide terms dependent upon the distribution and it's also hard to optimize these balance losses in practice because the unknown because the true probabilities are unknown effectively.",
                    "label": 0
                },
                {
                    "sent": "And finally, in most importantly for our purposes, this result doesn't quite explain the empirical success in terms of AUC, performance of the standard algorithms we discussed, which minimize standard losses, not the balance versions of them.",
                    "label": 0
                },
                {
                    "sent": "So this is where proper losses come in and these will allow us to obtain direct bounds that avoid all of these difficulties.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So proper losses have along history.",
                    "label": 0
                },
                {
                    "sent": "I'm sure most of you are aware that they've been of significant interest in recent years due to the import.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Role they play in class probability estimation in order to review proper losses, I just need a little bit more notation.",
                    "label": 0
                },
                {
                    "sent": "For any base Losail that accident, prediction space, yhat, we define the conditional L risk, which is a function that takes 2 arguments, Eaton 01 and the value I had.",
                    "label": 0
                },
                {
                    "sent": "And returns the expected loss in code when predicting Y hat.",
                    "label": 0
                },
                {
                    "sent": "When the true label is one with probability to minus one, probably 1 minus beta.",
                    "label": 0
                },
                {
                    "sent": "We also define the conditional Bayes error risk, which is a function of just one argument.",
                    "label": 0
                },
                {
                    "sent": "It takes an argument Eater and just returns the lowest possible conditional address you can get for this value beta.",
                    "label": 0
                },
                {
                    "sent": "So we could talk about proper losses.",
                    "label": 0
                },
                {
                    "sent": "And these are used in class probability estimation.",
                    "label": 0
                },
                {
                    "sent": "So these basically operate on a prediction space 01.",
                    "label": 0
                },
                {
                    "sent": "We see a binary loss acting on this prediction space is proper.",
                    "label": 1
                },
                {
                    "sent": "If for any tail this right value Eater is a minimizer of the conditional risk.",
                    "label": 0
                },
                {
                    "sent": "We say it's strictly proper.",
                    "label": 0
                },
                {
                    "sent": "If this is a unique minimizer.",
                    "label": 0
                },
                {
                    "sent": "Hum.",
                    "label": 0
                },
                {
                    "sent": "This is a very nice characterization of strictly proper losses that has various.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Both in the literature, we give an alternative, much simpler and self contained proof in the paper for this result, which basically says that.",
                    "label": 0
                },
                {
                    "sent": "A proper loss is strictly proper if and only if its conditional Bayes risk is strictly concave.",
                    "label": 1
                },
                {
                    "sent": "It's easy to see that the Bayes Conditional Bayes risk this function is always concave for any loss because it's just the point wise in from of a family of linear functions.",
                    "label": 0
                },
                {
                    "sent": "But this is that for a proper loss.",
                    "label": 0
                },
                {
                    "sent": "It's strictly concave precisely when the loss is strictly proper.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So at this point, it's useful to think about just single intuitively about why proper losses might be useful for us.",
                    "label": 0
                },
                {
                    "sent": "So what we're interested in doing is basically bounding the AUC.",
                    "label": 0
                },
                {
                    "sent": "Regret, which is just the difference of the AUC of a scoring function from the optimal agency.",
                    "label": 0
                },
                {
                    "sent": "Now the optimally you see is known to be achieved by the true class probability function.",
                    "label": 0
                },
                {
                    "sent": "And so, intuitively, if we can find a class probability estimate that is close to the true clarity function, we might hope that the resulting AUC is also close to the optimal.",
                    "label": 0
                },
                {
                    "sent": "This of course is not a full argument that it forms the basic idea behind the approach.",
                    "label": 0
                },
                {
                    "sent": "It turns out to formalize this idea, we need a slightly stronger form of these proper losses which.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Still strongly proper losses and.",
                    "label": 0
                },
                {
                    "sent": "Very clear in just a few moments why this is a reasonable name for these losses.",
                    "label": 0
                },
                {
                    "sent": "So we say a loss acting on 01 is Lambda strongly proper, some positive Lambda.",
                    "label": 1
                },
                {
                    "sent": "If for any to the true probability it is not only a unique minimizer of the conditional risk, but.",
                    "label": 0
                },
                {
                    "sent": "In addition, for any other Eater hat, the conditional risk is bounded away from the minimum in this manner.",
                    "label": 0
                },
                {
                    "sent": "And we can show a similar characterization for strongly proper losses as was there for the case of strictly proper losses under mild regularity condition.",
                    "label": 0
                },
                {
                    "sent": "Under this condition, we can show that a proper losses slammed a strongly proper if and only if it's conditional based risk is Lambda strongly concave and This is why we refer to these losses are strongly proper.",
                    "label": 1
                },
                {
                    "sent": "I'll just quickly mention, for those who are familiar with proper loss, is another way to view strongly proper losses that the weight function which weights the cost sensitive misclassification losses that make up the proper loss, is lower bounded by a constant.",
                    "label": 0
                },
                {
                    "sent": "This provides an alternative perspective.",
                    "label": 0
                },
                {
                    "sent": "I'd like to thank anonymous reviewer for pointing out this interpretation of these losses.",
                    "label": 0
                },
                {
                    "sent": "Home.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we've seen so far is.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basic proper losses that act on the prediction space 01.",
                    "label": 0
                },
                {
                    "sent": "It turns out one can compose these using a link function to yield proper composite losses on any prediction space.",
                    "label": 0
                },
                {
                    "sent": "I won't go into details over here.",
                    "label": 0
                },
                {
                    "sent": "There can be found in the paper or in several other recent papers, for example by Mark Reed and Bob Williamson.",
                    "label": 0
                },
                {
                    "sent": "What we have is that for any.",
                    "label": 0
                },
                {
                    "sent": "Strongly proper loss.",
                    "label": 0
                },
                {
                    "sent": "For our purposes, let's assume this strongly proper composite loss acts on the real valued predictions for any such strongly proper composite loss.",
                    "label": 1
                },
                {
                    "sent": "They're using regression of a scoring function can be upper bounded directly in terms of the eldrige rid of this function, and the proof is very direct, very simple.",
                    "label": 0
                },
                {
                    "sent": "It follows almost directly from the definition of strongly strong properness.",
                    "label": 0
                },
                {
                    "sent": "In particular, it guarantees that there exists class probability estimate who's a little distance from the true probability function is upper bounded in terms of the L regret.",
                    "label": 0
                },
                {
                    "sent": "Of course, this class property or the Delta distance also related to the AUC regret of it.",
                    "label": 0
                },
                {
                    "sent": "That's also relatively simple to do, I'll just say over here there's a suitable transferability estimate that you go to the paper if you're interested in more details.",
                    "label": 0
                },
                {
                    "sent": "The proof is actually very simple, just within half a page.",
                    "label": 0
                },
                {
                    "sent": "So what we have is for any strongly proper composite loss, we have a direct result, upper bounding the AUC, regret of a function F on D in terms of the Eldridge of on the same distribution D. There's no need for any pairwise analysis here and there.",
                    "label": 0
                },
                {
                    "sent": "No hidden balancing terms.",
                    "label": 0
                },
                {
                    "sent": "The dependence of this bound on P is explicit here.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I promised the family of strongly proper composite losses fairly broad in particularly includes the exponential, logistic and squared losses as special cases.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just mentioned briefly that we can also combine this with the recent result of clemenson robbiano to obtain a tighter regret bound for certain low noise conditions.",
                    "label": 0
                },
                {
                    "sent": "So the distribution D satisfies a certain noise condition parameterized by some Alpha where Alpha closer to 1 imposes a stronger lower noise condition.",
                    "label": 0
                },
                {
                    "sent": "Then we can obtain a bound.",
                    "label": 0
                },
                {
                    "sent": "We can improve the exponent in the bound to be greater than half.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to summarize, we've given a quantitative regret bound for the AUC for scoring function if in terms of simple loss space regrets for broad family of strongly proper composite losses.",
                    "label": 1
                },
                {
                    "sent": "These include in particular common losses like the exponential squared and logistic losses, which means that the standard algorithms many of these added algorithms are in fact AUC consistent.",
                    "label": 0
                },
                {
                    "sent": "This explains some of these previous empirical observations that have been made in several papers about.",
                    "label": 0
                },
                {
                    "sent": "Easy performance of these algorithms.",
                    "label": 0
                },
                {
                    "sent": "I should mention, of course that the pairwise algorithms are still useful for non bipartite settings.",
                    "label": 0
                },
                {
                    "sent": "And even in the bipartite setting, it's not clear how the rates of convergence of the AUC for these different approaches compare on different distributions, but as far as AUC consistency is concerned, these standard algorithms are just as good.",
                    "label": 0
                },
                {
                    "sent": "Home.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, I just mentioned that these strongly proper losses that have been defined and characterized here might also be useful in studying other types of problems.",
                    "label": 1
                },
                {
                    "sent": "So as one example, we've used these losses in studying consistency of algorithms for classification under class imbalance settings.",
                    "label": 1
                },
                {
                    "sent": "Some of this work will be presented at ICM next week.",
                    "label": 0
                },
                {
                    "sent": "Thank you, I'll be happy to take questions.",
                    "label": 0
                }
            ]
        }
    }
}