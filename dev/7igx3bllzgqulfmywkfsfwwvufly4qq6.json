{
    "id": "7igx3bllzgqulfmywkfsfwwvufly4qq6",
    "title": "Deep NLP Recurrent Neural Networks",
    "info": {
        "author": [
            "Richard Socher, Computer Science Department, Stanford University"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_socher_deep_nlp/",
    "segmentation": [
        [
            "So I think we've covered them a little bit.",
            "Then we will on a very high level later on too.",
            "So I thought maybe it's good to really dive into the details of how exactly recurrent neural networks work.",
            "How do we train them?",
            "What are some tips and tricks to actually make them work in practice?",
            "'cause they sort of vanilla techniques will often not work, and then also how they've been extended in the past as well as just last year with better versions of recurrent neural networks, namely gated recurrent units.",
            "An long short term memories.",
            "So I think we've covered that tiny bit with Phil on high level and I want to just dive into details because tomorrow's lecture will be a lot of applications of those techniques as well as taking all these ideas as sort of very basic Lego blocks and then putting them together in a very complex large model called dynamic memory networks.",
            "So let's get."
        ],
        [
            "Then just a quick refresher.",
            "I know we just covered this language.",
            "Models are really just use them as an excuse to introduce recurrent neural networks in their details to you.",
            "We won't go into too many details here.",
            "Basically our language model tries to compute the probability for an entire sequence of words and unlike traditional language models, we won't make sort of incorrect but necessary Markov assumptions where we might say alright, the last the current words probability depends only on.",
            "The window of the previous N words.",
            "Instead we will try to have that window be as large as possible with the recurrent."
        ],
        [
            "Network.",
            "So what's the main idea of a recurrent neural network?",
            "Why aren't they just called neural networks?",
            "The main idea is that we actually tie the weights at each timestep.",
            "So we essentially tried to condition each neural network time step on all the previous words that come in, and this is the most standard way to formulate this where we have a word vector which we've covered already at any given time step T. Usually that will come from look up table or word embedding matrix that we can learn refer to VEC or glove.",
            "Then we will have a hidden state.",
            "Our current memory at each time step T and we will in the case of language models try to output.",
            "The probability of a word YT at each time step.",
            "There are a lot of ways you will see this in the literature and."
        ],
        [
            "I'll try to actually expose you to maybe three or four different formulations to make sure you see whenever you see the main concept, you can kind of look beyond the syntax and see there's just a recurrent neural network inside here, so these are two other ways you will see recurrent neural networks be defined.",
            "So sometimes instead of having."
        ],
        [
            "Sort of output nicely at each time step T and its preview."
        ],
        [
            "And next one we basically just say this as our recurrence.",
            "Or you may see just a single time step without the unfolding to its previous and next one.",
            "The main equation that will usually use.",
            "Is is this one up here where we have our word vectors and then here we have our standard neural network where we have a hidden to hidden matrix that will essentially connect this.",
            "In this case, here or."
        ],
        [
            "In this formulation, this W right here is our hidden to hidden matrix."
        ],
        [
            "And then we'll have the hidden toward vectors matrix, and you'll also often see this parentheses T here.",
            "That says this is not.",
            "This is basically the word vector that we pulled from the current time step T, and then we pulled that from our word look up table.",
            "Alright, so this by now in the summer school hopefully looks very familiar.",
            "So essentially two linear layers put together followed by non linearity in element one.",
            "It could be a sigmoid, but will actually compare and see that other ones or even better.",
            "And then we have our standard softmax layer or softmax classifier on top of that hidden representation.",
            "And that will essentially be the probability for each word is just the J element here of that vector.",
            "Yes.",
            "What do you do with HDR?",
            "That's a good question.",
            "In many cases you can actually initialize it, either random or just zero.",
            "There is more common.",
            "Are there any questions about this 'cause we'll move on from that in a sort of our most basic level piece, yeah?",
            "So WS is just a softmax so."
        ],
        [
            "Maybe this slide here will dive into some more details and will very carefully define here all the terminology.",
            "So we have DH is the dimensionality of our hidden layer, so WH has to be a square matrix.",
            "However WHX from the word vectors to the hidden vectors doesn't have to be squared toward vectors.",
            "Could be very high dimensional.",
            "If you think there is, you know you summarize everything at each time step.",
            "You might have to have a smaller dimensionality if you think you want to accumulate more and more from word vectors, you might want to have a larger dimensionality.",
            "Or your hidden state.",
            "And then WS will be the softmax, weights that in the case for language models, basically have the number of columns equal to the size of the vocabulary.",
            "Any other question?",
            "That's great.",
            "That's a great question.",
            "Yeah, it's not.",
            "It's not a one hot vector that just gives you the index of the word.",
            "It's actually the word vector, like word to vector glove vector, so it's usually 100 to 200 dimensional dense vector representation of each work.",
            "Yeah.",
            "Why are we recomputing it?",
            "It's different at every time step, depending on what your H is at each time step, you'll have a different normalization.",
            "Does that answer your question?",
            "So there will actually be several tricks here, 'cause if you think about this, we have a lot of different words in the English language, so there will be some tricks on how to make that softmax more efficient.",
            "But in general the normalization will be different at each time set.",
            "Yeah.",
            "That's a great question.",
            "So where do we get X from?",
            "We will actually be able to get all these access.",
            "We could actually train them together jointly.",
            "With this model.",
            "We could take derivatives of this."
        ],
        [
            "Objective function here with respect to every single one of these parameters.",
            "It could be including the X, but you can also and this what often is commonly done.",
            "Initialize the word vectors by some pre training method that is unsupervised like word to vector Club.",
            "Yeah.",
            "It usually helps a lot in the convergence and training speed to get to a similar performance without pre trained word vectors it takes takes much longer.",
            "Alright, so."
        ],
        [
            "This is our main definition here.",
            "Again, this is the question here.",
            "For the hidden layer timestep 0.",
            "And I usually define L here as our look up matrix for the word vector.",
            "So we store all the word to VEC or glove vectors in L."
        ],
        [
            "Alright, so Yhat here's our probability distribution over the entire vocabulary at each time step, and so you already looked at the cross entropy error for general classification.",
            "If you just have a standard network, you have a classification problem.",
            "We essentially use that exact same cross entropy loss function, but instead of predicting a specific class, we just predict all the words.",
            "So you some here at each time, step over the entire vocabulary.",
            "Now."
        ],
        [
            "What is often or more commonly used is perplexity, which is just 2 to the power of the negative of the average log probabilities over the entire data set.",
            "So we just add it here.",
            "This is our prediction is the ground truth, so we at each time step T we look at which word actually appeared at that time step we sum over the vocabulary and we sum over all the time steps T that we have in the data set.",
            "Now we have the negative we tried to.",
            "Notice now perplexity is the more common number that you'll see, and it's basically just another.",
            "There's some nice connections there to information theory, but the main point is lower is better.",
            "You want to basically be less perplexed by seeing the next word given your history of previous words.",
            "So this is in many ways just going over some of the details of the previous lecture.",
            "So now why is training these recurrent neural networks heart?",
            "Or do you have any more questions about just a general definition?",
            "So we now have defined exactly where each of these white hats comes from.",
            "We have defined our general objective function or loss function.",
            "Now we would generally just as with backpropagation, take derivatives with respect to all the different parameters that we have in the model.",
            "Yes.",
            "Usually have 102 at most 1000.",
            "Once you have 1000, that means that you're."
        ],
        [
            "WHH will be a million parameters, so that's that's quite a lot of parameters and you would want to have a very large training data set to train a million parameters.",
            "Ideally have at least 1,000,000 words.",
            "So it actually doesn't.",
            "So the interesting thing."
        ],
        [
            "Here is that, in a perfect world this H. Might be able to capture a lot of different previous like the facts and the words that came into all the previous time steps.",
            "That's in theory.",
            "In practice you will actually lose information the further away you go and will actually cover why that is both during training and that will then follow to test time.",
            "So we're not making any strong Markov assumption here.",
            "That would be if we said we delete everything here and at every.",
            "You know after every 3 words we actually set it to zero and then only look at the last three time steps or something like that.",
            "But really this hidden state in theory could capture a lot of the previous history.",
            "Yeah."
        ],
        [
            ".",
            "Thanks.",
            "This one right here.",
            "Absolutely.",
            "I mean, there's no theoretical reason why you couldn't do that.",
            "In most cases, you wouldn't, because the time steps in three are probably even easier and more accurately predictable.",
            "If you looked at next towards that, come just before that.",
            "But there's no theoretical reason why you couldn't predict.",
            "Really, anything on top of this?",
            "So what I'm trying to, what we'll get to in a second, is that really this language modeling formulation where we try to predict the next word is just one of many we can really try to predict the part of speech tags at each timestep, named entity tags.",
            "Really anything we can think of.",
            "Yeah.",
            "Why do we not sum over the vocabulary?",
            "Oh"
        ],
        [
            "Anne."
        ],
        [
            "Ways this is just the definition of the general cross entropy error, but if you think about this, why T is actually the word that really appears, and so this will be a one here and is 0 for everything else.",
            "So it's just a very general definition of this.",
            "You will actually look at only the YT that actually appears, 'cause this will be 0 for all the words that don't appear.",
            "So in some sense you could simplify this formulation here.",
            "Right, great insight."
        ],
        [
            "Alright, so why is training recurrent neural networks hard and even harder than training generally very deep neural networks, you could say?",
            "Oh well, we just have a one layer neural network at each time step.",
            "However, as you unfold this overtime, you actually get an extremely deep neural network, one that has 1000 layers.",
            "If you have 1000 words that you're trying to predict in a row.",
            "And so.",
            "In general, our hope was that the inputs from many time steps ago will actually modify our pretty why.",
            "And one thing that I would encourage all of you to do is actually really go and try to take the derivative of 1 error function at one time step with respect to even just the main hidden to hidden parameters just for two unfolded time steps.",
            "It will be very insightful.",
            "It's relatively simple to go through the matrix algebra to do this, but it's very helpful."
        ],
        [
            "Now the main problem that we have as we try to train this is actually the so called vanishing and exploding gradient problem.",
            "Basically, unlike in standard deep neural networks where we have a different set of weights at every time step.",
            "Here we actually have the exact same matrix at each time step.",
            "And that might result in multiplying by a Jacobian there actually all the derivations are in the appendix of these slides, so we'll put these online.",
            "You can look into the details as you write out all the gradients.",
            "At some point you will get a term.",
            "This Jacobian of.",
            "The hidden to hidden from one time step to the next one and you will see there that if you take the normal that that Norm will be either smaller than one or larger than one, and you multiply that same norm at every given time step on a high level intuition as you take your gradients from one time step.",
            "Here you have your errors that you Delta error messages that you push back through network.",
            "Intuitively what happens at every time step if you have a non linearity that squashes that value to be between.",
            "Minus one and one, and you have that same value that keeps happening over and over.",
            "You multiply your gradients with numbers that are smaller than one each at each time step and basically your gradient will vanish and if you for instance try to predict here a specific word.",
            "That happened, and that word would really only know that it would happen next based on a word that happened 10 timesteps ago, you couldn't train the model.",
            "You couldn't tell this model and train it with this information."
        ],
        [
            "I just said so yeah, so this is basically the problem.",
            "You may at this time step here, modify the weights for XD and for this time step.",
            "But as you go further and further back you will essentially lose that Gray."
        ],
        [
            "And here's a particular example for language modeling where this is a problem.",
            "Basically, we would ideally take into consideration as many words of the past as possible.",
            "So here we have the sentence Jane walked into the room.",
            "John walked into.",
            "It was late in the day.",
            "Jane said hi to now.",
            "In this case, all of you would probably close to probability one, no.",
            "What word comes next here?",
            "But because that word is actually many time steps away, it's very hard for recurrent neural network language model to not just basically put a high probability to many different kinds of names on here.",
            "So is that does the intuition make sense for people the vanishing gradient problem?",
            "Some nodding.",
            "Who here wants to go through all the detailed derivations with the problem that it will eat away at your lunch?",
            "Alright.",
            "Alright, will do it for you."
        ],
        [
            "Are people during lunch alright?",
            "So now I talked about vanishing creating problem.",
            "We multiply numbers that are smaller than one at each time step.",
            "As you send your Delta error messages through the network.",
            "There's also the problem of the exploding gradient problem.",
            "However, that one has a very nice and simple hack to fix it, which is we just clip the gradient.",
            "It sounds pretty hacky, but there's some nice intuition that came from Yoshi's Group, actually.",
            "So if we have if the gradient explode, so we have.",
            "Are Delta error messages flow through the network at each time step they get multiplied by numbers larger than one and they eventually you know."
        ],
        [
            "It might explode and give you not a number.",
            "Now the simplest thing we could do is just looking at the norm of the gradient.",
            "If it's larger than a certain threshold, we basically set it to that threshold and that makes a huge difference and one really great paper from Joshua's group actually gave an intuition and this is something that I think is also really clever inside.",
            "Whenever you're struggling with the neural network and you something isn't working right 'cause you're actually working on some novel research method, I encourage you to try to visualize it.",
            "As much as you can, you can visualize the hidden states as they go overtime.",
            "You can try to visualize your gradients and norms of your gradients overtime.",
            "Or you can even try to create very tiny version of your model where you can visualize pretty much every single wait and hear what Josh's group did.",
            "And Sean, who is the first author of this work, is essentially train a single hidden unit RN and that basically here on the Y.",
            "On this axis we have the value of Y and here we have the value of P, the bias term.",
            "And now we have our error function here and you can see in the solid lines the standard gradient descent trajectory.",
            "So it goes here it goes here and then what they observed is are these really high curvature walls and so the standard gradient would basically get here.",
            "And instead of trying to explore this Valley against jumps against the wall and goes way further back.",
            "Moving away from what clearly would be a better optimum of the error.",
            "And this is essentially one traditions of Y clipping.",
            "The gradient actually helps.",
            "Here you go here and then instead of jumping way far back, you move less far back.",
            "Any questions about that?",
            "Yes.",
            "Exam.",
            "So in general the.",
            "That that gradient that you clip actually applies to all the different parameters of the model so.",
            "I don't see the intuition of how it changes the bias for specific words, imagine they're all word vectors and we take derivatives and it's basically here.",
            "You know the hidden hidden dimensions too, and things like that.",
            "Yes."
        ],
        [
            "So yeah.",
            "That way, if one unit just blows up 'cause that number gets multiplied by value larger than one for many time steps, you could just click that one, but this one doesn't have the nice intuition that this paper had.",
            "Yeah.",
            "Like I said, it's a.",
            "It's a pretty simple hack, but either way.",
            "Yeah.",
            "Yeah, they basically work the same.",
            "I don't think there has been a really proper comparison across a lot of different tasks for the two.",
            "Really.",
            "Like Joshua said, the main thing is just.",
            "Alright, so there's one other way that is actually a very recent trick.",
            "Well semi recent to avoid the vanishing gradient problem and make the exploding gradient actually a little more likely because you have even larger numbers.",
            "Often I think in that case it happens more often than if you had like 10 inch units, for instance, but.",
            "One really neat trick is to use to initialize all your W matrices so the hidden to hidden connections.",
            "Assuming we have the same dimensionality for our hidden dimension as we have for our word vectors, as well as the word vector to hidden dimensions as the identity matrix is very simple idea.",
            "Identity matrix ones on the diagonal and then also use the rectified linear units that already covered.",
            "I'm sure where you basically just have Max Z0 and those two tricks make a huge difference in initial for initialization and then also not having the vanishing gradient problem.",
            "This idea was actually first introduced in a paper of mine 2013 for recursive neural networks, which are just that restructured versions of these change structured models of.",
            "Recurrent neural networks and then there are some new experiments with recurrent neural networks and rectified linear units combined where they show that the test accuracy.",
            "If you have this identity initialized recurrent neural network actually gets way higher than if you had a standard recurrent neural network where you had 10 H units and you didn't initialize with identity matrices.",
            "So very simple idea.",
            "Initialize all your parameters of the recurrent neural network with identity matrices, yeah?",
            "It depends.",
            "So the very originally what we've done in our intuition was as you merge toward vectors, the default.",
            "If you don't know anything about how to merge them, you actually have identity matrices times 1/2.",
            "Which essentially just means you're averaging your word vectors and that is also a reasonable thing to do if you have a recurrent neural network bearing any other knowledge, what you would start out doing is just basically taking sort of a windowed average about over word vectors.",
            "Yeah.",
            "The hidden to WHX.",
            "Yes, yeah you do both.",
            "In this paper.",
            "And they showed that it helps.",
            "Yeah.",
            "So it works even better, so they have, I guess not in this plot, but they tried also have the identity initialized with 10 H and that does not work as well as with rectified linear units.",
            "So yeah, like I said, you have actually the exploding gradient problem.",
            "A lot more common in these kinds of models.",
            "Oh yeah.",
            "Yeah, but you can still remember in fact yeah, they they show on a couple of different.",
            "I mean this is kind of a toy task here, pixel by pixel permitted amnist, but even on other kinds of language tasks they show that this.",
            "This helps a lot and you can indeed memories things."
        ],
        [
            "Alright, so some quick perplexity results just to give you an idea that recurrent neural networks do or help for language modeling.",
            "I don't think we need to go through too many details here, but basically when you combine that, especially with sort of very good memorization of standard language models like Ness and a smooth language models, you really have much lower perplexity and better probabilities for the words that actually appear.",
            "Now."
        ],
        [
            "Just to give you a trick 'cause you say, oh man, I understand him now.",
            "Very simple.",
            "I have all these of my tricks for exploding, creating problems invention, creating problems.",
            "I'm going to implement them and then you want to actually predict reasonably large vocabularies.",
            "You'll run into the problem of having to run a gigantic matrix multiplication of, say your 100 dimensional hidden states times 100,000.",
            "'cause maybe you have 100,000 different words in your vocabulary and you very quickly realize that you won't even be able to run that on your laptop and so.",
            "One really neat trick, also very simple, is essentially have a class based word prediction where instead of directly predicting your WT based on the history or your memory state H, you actually will first predict the class based on the history and then the word based on that class.",
            "And now you can only a train time only look at specific classes at training time and what you observe is the fewer classes you have, the faster you will train.",
            "But it also hurts you a little bit in perplexity, so there's a tradeoff there as you go to higher and higher or more and more classes you essentially.",
            "Have less less error, but it will also again get much lower.",
            "And."
        ],
        [
            "That probably is already clear, but as you go through your network, what you have to do."
        ],
        [
            "As you add your errors.",
            "You really don't just take one error at a time and then back propagate all the way back through time, but what you do is."
        ],
        [
            "You add your errors so as you train, you basically only have to go from the beginning from the end of time all the way to the beginning of time of that current recurrent neural network once and at each time step you add new Delta functions coming from the error that you have at each time step and you basically add your Delta some sort of waterfall where each time step adds its own error.",
            "Yeah.",
            "Oh, it's not close at all.",
            "So as you train it, it basically moves further and further away from that initialization.",
            "Yeah.",
            "It's basically just a really good and there's initialization where you say on average, if I don't know what happens, instead of making a completely random random projection of my word vector into the hidden state, you assume you more more likely average.",
            "So."
        ],
        [
            "Here, just to give you an intuition of this identity.",
            "Basically here you have imagined to identity matrices or just half of this identity matrix times half times this identity matrix so initialized with half's on the diagonal.",
            "What you would do is basically average your current hidden state with the current word vector.",
            "OK.",
            "So that this current state.",
            "Depends on all the previous time states.",
            "No, it depends on all the previous ones.",
            "Oh yes, yeah, the current word vector modifies it.",
            "Well, depending on how you initialize it at the very beginning, right?",
            "You can initialize here just both to halftime.",
            "Set any matrix.",
            "Then it'll depend on half from this and have fun all previous time steps.",
            "That that would be interesting, but you can really play around with that idea and have.",
            "Different initialized different multipliers of your identity matrices in the beginning."
        ],
        [
            "Alright, so now this was for language modeling.",
            "Really language modeling is 1 task, but it's not as interesting to me personally as a lot of other more downstream tasks that real people and companies and.",
            "Sort of organizations really care about, so one of those is named entity recognition.",
            "Basically trying to find locations, names and people names inside larger text.",
            "That way you could try to find all the texts that Barack Obama, for instance, was mentioned or presidents or specific company names and so on.",
            "There's also there's also entity level sentiment in context, so trying to say I thought the iPhone was a good phone when it came out, but then later on I really disliked the screen or something like that.",
            "Now you want to say screen in this context is negative.",
            "So this is essentially entity level sentiment in context, and another fun one that is related to that or opinionated expressions.",
            "And in the next couple of slides I will use the terminology from the paper biozone years are in Clare, Cardi from last year on opinion mining with deep recurrent neural networks, and I'll use another task as an excuse to talk about various extensions now of recurrent neural networks.",
            "Add anymore questions about the basic recurrent one.",
            "Alright, 'cause."
        ],
        [
            "Everything will explode, will have now depths in many directions and so on.",
            "It'll be great alright, so here the next task that will look at is opinion mining.",
            "So it is essentially just another Y hat that we're trying to predict instead of having to predict probability for every single word in vocabulary, we just predict a specific class and the class is that we look at here are over direct subjective expressions and expressive subjective expressions.",
            "The first one is essentially just an explicit.",
            "Mention of private states or speech events that express private States and the second one basically indicate a specific sentiment or emote."
        ],
        [
            "And without explicitly mentioning it, so just to give you an idea here for instance as usual doesn't say very explicitly this is.",
            "A shitty phone or something like that, but it sort of implicitly describes also an emotional state, so the way we want to label.",
            "The words here in context is essentially by giving them.",
            "These kinds of beginning intermediate.",
            "Or non labels.",
            "So if we have for instance EC expression, the first of the EC expression, the first part of that will always start with a B and then the continuation of whatever happened there will have the label I.",
            "This is a very common sort of bio way to label words and sequences."
        ],
        [
            "X is still our word vector and we have Y Now our output labels the BIOS.",
            "And we still use the same softmax.",
            "Now."
        ],
        [
            "Now the reason why they used different letters instead of superscripts for HRH is that will actually use a lot of other superscripts for making this model a lot more complex.",
            "So the first problem is as you try to label a specific word, you would ideally not just use all the context from the left side, but you would also want to use all the context from the right side.",
            "Now this is a more general problem than language modeling, where you don't usually have the future you want to just predict the next word.",
            "In the case for speech, for instance.",
            "Speech recognition, so here we essentially will define a forward recurrent neural network as well as a backwards recurrent neural network.",
            "And they're essentially the same.",
            "It's just one goes from time step.",
            "You know, zero to T and the other one goes from T to 0.",
            "And we can now represent.",
            "The current time step by basically just concatenating the recurrent neural network that went from left to right and the one that went goes from the right to the left.",
            "Are there any questions about the bidirectional recurrent neural net?",
            "Yeah.",
            "To make separate what?",
            "To remain separate or mix them.",
            "I'm not quite sure I understand your question, so why?",
            "Why do you have just additive interactions here?",
            "Is that the question?",
            "OK. Yeah, so there's actually no.",
            "It's not necessary that H here has the same dimensionality as X.",
            "Unless you want to do the identity initialization trick, but even there you could probably add another zero block if you wanted to.",
            "Does that answer your question?",
            "We will chat over lunch.",
            "Alright, so this is our bidirectional recurrent neural network.",
            "I'll describe it like this now.",
            "OK, now we can go even deeper and this is the right summer school to do that where we essentially will say not only do we go from left to right and have have depth in along the time dimension, but will also actually go up at each time step and we'll have recurrent neural Nets that go from left to right and up and down.",
            "So essentially at each time step here, this vector will depend.",
            "Or this one here will depend not just on.",
            "Its own time step and deep neural network at that time step, but also on the time step before.",
            "And that's why you now have back here.",
            "Super Scripts have essentially a very deep neural network at each time step.",
            "So that's a great question.",
            "There probably be different answers from different people in the audience.",
            "Mine is also evolving overtime I think, and the very beginning it will be."
        ],
        [
            "But to implement this kind of model personally from scratch entirely, I would just take a simple, you know Matlab or Python user my matrices.",
            "Here's how I multiply it to gain intuition, but once you've done that once in your life and your fellow like, alright, I get it.",
            "It's really just a bunch of matrix multiplications then you."
        ],
        [
            "I want to start using more and more sophisticated libraries like Theano or Torch Ware."
        ],
        [
            "And you can really very easily define this and then have a loop that just initializes another layer and eventually you can have a hyperparameter just says how many layers do I want, and depending on that hyperparameter, it'll just initialize all these matrices automatically and put them together and very nice Lego blocks, and I think Theano and torture or good alternatives.",
            "Actually, during my PhD, I guess neither of those were that as far along when I start in 2010, especially not for NLP.",
            "So I actually just implemented all of that from scratch.",
            "And with just matrix multiplication, so it's doable to.",
            "But yeah, basically you want to abstract away as much as possible, 'cause eventually you don't want to write down all the derivatives here by hand.",
            "You want to have an automated way of saying here's a block.",
            "That block gets an input vector X and gives back a Delta message from for its errors.",
            "Yeah.",
            "That's right.",
            "No.",
            "So you go basically up at each time step and you predict something and you have connections from the left for all the recurrent neural networks that are from the left to the right, and connections from the right, but only for those that go from the right to the left and then you concatenate.",
            "You concatenate the two states at the very end for the softmax.",
            "It does, yeah.",
            "So.",
            "So you do right so you have here at each time step.",
            "This unit depends on many previous units before and this unit here depends on many sort of succeeding or next time steps that come in the text.",
            "So eventually the combination of these two, the concatenation of these two, keeps the context from the left and the right.",
            "So that is what's implemented.",
            "From top to bottom.",
            "So you want to have this thing modify weights.",
            "To hear but so this thing comes.",
            "It's basically computed based on what's here.",
            "So in general, yes you could add like short circuit connections.",
            "For instance.",
            "Instead of saying this thing will depend only on this.",
            "You could actually now also add a short circuit connection that goes all the way from the word vector, for instance also directly to that wait so there are a lot of other connections that you can add.",
            "So instead of here, WH for the top one you could add plus XD the word vector for instance.",
            "But at some point.",
            "You know the complexity doesn't buy you that much extra.",
            "Yeah.",
            "Well, you could.",
            "I mean combining two classifiers.",
            "If you didn't do that and you just had a distribution that you get at the very end, and then you combine the outputs of those two final classifiers, you could train literally them.",
            "You could train him entirely separately and then just average the weights for your classes from the left and right model.",
            "It's essentially just description, so we would say I personally would say this is 1 model.",
            "That has in the end one classifier with the weights you.",
            "Instead of saying this is 2 classifiers, it's just a model that takes in information from different sources to make a prediction at each time step.",
            "That's how I would describe.",
            "Alright, one more question.",
            "And then we'll move on.",
            "That's a great question, so this is not an LCM yet.",
            "This is just a standard recurrent neural network with standard additive interactions."
        ],
        [
            "I will answer that question actually in the next slide.",
            "So how do we evaluate this?",
            "There's a so called empty corpus basically has only 11,000 sentences, so not that many sentences take that into consideration.",
            "When will in the next slide look over the evaluation and number of layers and we use standard F1 metrics precision recall."
        ],
        [
            "So here here's an intuition.",
            "So we basically here have our F1 metric, and we have the number of layers and we have different models with different sets of different numbers of weights.",
            "So depending on how large your hidden state is.",
            "Yeah, mostly then the size of the hidden state hidden states change.",
            "So here we see how well this changes or how much better it gets with a deeper and deeper architecture.",
            "Essentially the.",
            "Model that has three hidden layers and 200,000 parameters.",
            "Got the best performance here at three layers.",
            "Once you add it even more layers that actually create it.",
            "So just this gives you some intuition.",
            "It's not always that the more layers are better, there's a good chance here.",
            "The more layers you have at some point you overfit even more, but in general it did help to add.",
            "Several of these extra layers here.",
            "Yep.",
            "Overfit, so as you go.",
            "In many cases, the train accuracy was actually close to 100.",
            "What does what mean?",
            "Do we need a special regularizer?",
            "You do in general want to regularize all these models.",
            "I don't know if there's any special regularizer for this specific model.",
            "There wasn't.",
            "There's just L2 regularization all the weights.",
            "But you could try and now you know initialize your different layers with identity matrices.",
            "There are a lot of tricks you can now basically combine all the things we've covered before at different layers, but in general.",
            "This hasn't been done yet.",
            "There's no no good answer to that.",
            "Yeah."
        ],
        [
            "Alright, so we already covered a little bit.",
            "Or will I think Phil later on going to a lot deeper into machine translation, but just to give you the generality, general applicability and also another good excuse to introduce to you the next modifications to recurrent neural networks will talk a little bit about machine translation.",
            "I won't go into all the details for traditional machine translation methods 'cause Phil covers those, but basically just to give you a high level overview, it's been a lot of.",
            "Work in traditional machine translation of course, and most of these models are all of them basically required a lot of human feature engineering and there in the end are very complex systems and to me personally, the worst part is statically is that they often end up having many different and independent machine learning models.",
            "So you train the language model in that separate from training your translation model, which might be separate from training your alignment model and things like that so."
        ],
        [
            "What could we do to use recurrent neural networks to try to do machine translation?",
            "I'll try to walk you sort of through the steps and we initially start with the simplest one is just a standard recurrent neural network, which won't be enough, and then we'll basically extend it and add more and more sort of modeling tricks to try to give you an intuition also of how you could go about improving your own neural network architectures.",
            "Alright, so the simplest model we could have is just we say here I stick a kiss to German.",
            "Word an would be roughly translated as awesome sauce and basically just say we have our standard recurrent neural networks with one W matrix and we just tried to predict here.",
            "Just as with the language modeling the actual words.",
            "Unfortune"
        ],
        [
            "Lee, you know that will require that this single vector here captures the entire T of the phrase, and the longer the sentence, the harder that will be.",
            "OK, assuming that we could do that, what would that model look like?",
            "So basically will define our encoder here as just again we compute at each hidden state at timestep T, and I'll introduce you that five function Phi Ascentia Lee just says for every input here will have a separate matrix.",
            "So whenever I have five off, you know one thing in two things, they're basically just added inside, and each one has its own W matrix of parameters just to simplify notation a little bit.",
            "And we have the same decoder will just be basically taking in the last time step 'cause we don't have a word, so it's just five HT minus one the previous time step, and we'll have a softmax and in a perfect world, if recurrent neural networks would be awesome and simple out of the box, then all we would have to do is basically again minimize the cross entropy error.",
            "And try to predict the translated words here one at a time."
        ],
        [
            "Unfortunately, it's not quite that simple.",
            "Recurrent neural networks are very powerful, but not powerful enough to capture all these complex translation phenomena.",
            "So let's go over several extensions of this model.",
            "The first one is we could actually train a different recurrent neural network for encoding and one for decoding.",
            "So just have different colors, basically have different sets of W, that's fair."
        ],
        [
            "Straight forward.",
            "The second one is that ideally we could actually compute every hidden state in the decoder, not just so the decoder now being defined as this part right here.",
            "Now we can condition that not just on the previous single state, but on multiple other things, such as the last hidden state of the encoder.",
            "So we'll define that here as C Being the last hidden state at capital time T of our encoding.",
            "Just a source language, and now will basically condition the recurrent neural network that outputs the translation on, not just the previous time step, but also that final encoder.",
            "As well as the hidden state and that sort of is also connected to the question that you asked about.",
            "You want to have some connections that go back.",
            "It does make sense to actually do that through the previous timesteps prediction to feed that into the next hidden state.",
            "So in our fine notation here, we essentially just have our last hidden state T -- 1.",
            "The final state from our encoder and the word that we just predicted in the previous time step.",
            "During decoding."
        ],
        [
            "Now, another figure that looks a lot more complex but is essentially has the same semantics to what we just described is right here, yeah.",
            "Yes, that word has to be 1 hot vector 'cause we will eventually want to say this is exactly this translation.",
            "Not here's a word vector, and so there's actually you can think of it in some ways.",
            "That way, right?",
            "You say, here's a word vector.",
            "Now try to find the closest word vectors to this word vector and then make that prediction.",
            "But then you have to already predict the word vector and then also run like an inner product to find the closest one to that.",
            "So in the end what most people do is just have predict the one hot vector.",
            "So now this looks a lot more complex, but it's really the same thing, so that's something I'd like to have you be able to have as a takeaway message to try to sort of look through the different formalisms and different syntax, different visualizations to see something very similar.",
            "So here we have the one hot vectors I guess, described a little more directly you have here your hidden states.",
            "The probabilities that essentially from the word probabilities you have the sample and then that's the sample that you again modify your encoder with and here you condition all the hidden time steps on this C vector.",
            "Alright."
        ],
        [
            "Now here 3 simpler extensions.",
            "The first one is we can train a stacked or deep recurrent neural network with multiple layers just like the one I introduced before.",
            "So instead of having the encoder only have one layer, we can have it go very deep.",
            "We can also potentially train the encoder bidirectionally, so going from left to right and right to left and then condition the decoder based on the last time steps and the first time step so to say.",
            "And if we don't do that, an alternative and one that might actually help even with this, is to train the input sequence in reverse order for to make an easier optimization problem.",
            "So oftentimes we translate something if basically your languages have roughly a similar word order, you might want to say instead of translating ABC to X&Y, you translate CBA Jackson.",
            "Why you're basically just shuffle towards and that way cousin when.",
            "The translation is such that the word order is often is preserved, then a would try to be translated roughly to X and as you back propagate through your very deep network, the error that you get from X here will now be very far away and you are more likely to hit vanishing gradient problems.",
            "Whereas if you flip it.",
            "Then the error from X needs to go right here to A and it's much easier for the model to optimize.",
            "Does that make sense?"
        ],
        [
            "All right now, the main improvement, and that's what we'll cover the last.",
            "Oh boy, we only have 3 more minutes, do we?",
            "Maybe we should end right here and I'll just describe those tomorrow.",
            "So.",
            "Do you have anymore questions?",
            "Oh half hour.",
            "Oh boy, whoops.",
            "I thought it's 1211 OK perfect alright excellent alright.",
            "So I thought lunch was 12 alright sorry cool alright so.",
            "No, no, it's fine.",
            "This is perfect because there are lots of questions, probably around LCMS and so on.",
            "Excellent alright great great alright so more complex.",
            "Unit computations in recurrence so.",
            "The first one actually introduced by somebody in the audience.",
            "I don't see him right now are gated recurrent units.",
            "In some ways, the next couple of slides will be a blast from the past, plus sort of state of the art novel stuff in the sense that Geo user just introduced last year.",
            "But LCMS, which are actually in some ways an extension or more complex version of them were introduced many years ago, so I'll start with them.",
            "I think it's sort of.",
            "Pedagogically, better to start with gated recurrent units, the main idea is that we want to keep around memories to capture long distance dependencies in our current recurrent neural networks that we've covered so far, we really have this matrix multiplication that changes every single unit, every time at every time step, and that's sub optimal if maybe we want to just keep around some information and the result of that will also be that our error messages will flow at different strengths depending.",
            "On the inputs.",
            "So."
        ],
        [
            "Let's define gated recurrent units and I think if you ever wanted to implement a recurrent neural network for real task, it's very likely you you're going to end up using a Gru Ornellas team.",
            "So it's a very important model.",
            "So standard recurrent units had essentially just the simple addition of two matrix multiplications.",
            "And that has the problem that no matter what the xinput is here will make a huge change to H at every given time step, right?",
            "You will always basically sum up these two values.",
            "Now the Gru computes its hidden state in a different, more complex way.",
            "It essentially first computes an update gate which is just another kind of neural network layer with its own weights.",
            "Based on the current input word vector and the hidden state.",
            "So the first thing it will do is essentially just looks very similar to this, but it's not our final state, it's just our state ziti.",
            "Now if the word vector dominates this, see T, for instance.",
            "It might say turn this on or off entirely, right?",
            "So ZTE could now be somewhere between zero and one.",
            "We always use a Sigma unit here.",
            "It's usually not something where we use a rectified linear or 10 H. This is Sigma for reasons that will become apparent in the next slide.",
            "Now we will also compute a reset gate.",
            "It's very similar, very similar to our recurrent neural network.",
            "But again, with a different set of weights.",
            "So now we have here our.",
            "Update Gates Z with superscript Z.",
            "Here for our W&U and we have a reset gates with superscript R for reset and those are both vector or presentations here too.",
            "So before we compute that vector H as right here, we first compute these two intermediate vectors."
        ],
        [
            "And then we will actually compute our memory content.",
            "H tilled T here which is sort of the new memory content.",
            "By having a 10 H layer and in this mu.",
            "A notation here that essentially is an elementwise multiplication.",
            "So this is also just a simple layer like this, but now having this elementwise multiplication here.",
            "So now let's think about what would happen here.",
            "Basically, if the reset gate unit, so one element of the R vector is 0, then this model can entirely ignore all the previous memory and only store the new word information.",
            "So let's say for instance I tried to do sentiment analysis and I said a bunch of content words.",
            "This movie that I saw yesterday with my whole family.",
            "Blah blah blah.",
            "Nothing that matters for the sentiment and in the end I say sucked or was terrible.",
            "Then I want that last word to essentially be able to overrule all the stuff that came before, because it doesn't matter to trying to classify the sentiment that word vector should at that point be much more important than all the previous sort of content.",
            "Words that weren't.",
            "That didn't have much sentiment.",
            "Alright, so that's our new memory content, with an elementwise multiplication here.",
            "Then our final memory, and this is the one that is.",
            "And."
        ],
        [
            "Semantics sort of similar to the final one that we would have computed in a standard RNN."
        ],
        [
            "Directly will be computed with this equation, where we now have our update gates and our update gates have an elementwise multiplication or 1 minus.",
            "That, and we can essentially take only the previous time step.",
            "And we're also able to essentially ignore what happened at this time step.",
            "So maybe we already know.",
            "This movie was bad.",
            "Even though I watched it with my whole family on a weekend last year during Thanksgiving.",
            "So basically none of this stuff matters, and this can essentially also allow you to ignore what's coming in.",
            "So."
        ],
        [
            "So I'm attempting here a clean illustration, though I think in the end the equations are often more useful for the intuition than illustrations.",
            "So just to.",
            "Try at least here we have these straight dotted lines.",
            "Basically, pointing to connections as our gates or multiplicative interactions.",
            "And we compute them from bottom up.",
            "So here at current time Step XT that will get fed into our reset gate, which also gets an input from the previous final memory.",
            "Hours ET.",
            "This computed in a very similar way, but as ET Now influences how we compute this final one, whereas the reset gate influences how we compute our intermediate or reset memory.",
            "Yes.",
            "Something better?",
            "I think that's right, yes.",
            "That's not a question yet.",
            "So the.",
            "So I think I've seen this at XX is just a word embedding.",
            "Oh yeah, HH is our memory that keeps.",
            "Right?",
            "So it keeps track of the entire T of the context before.",
            "So."
        ],
        [
            "I'll actually show you I'll actually show you some embeddings of very deep hidden layers of LCMS for machine translation at the end of this talk, so maybe that will give you some intuition."
        ],
        [
            "So Will will come.",
            "Will talk about this one in the LCM and then you'll see that all the ideas of bidirectionality and depth at each layer they will translate to the fancier versions of how to compute your hidden state as well.",
            "So you can combine all the ideas of previous ones before that's kind of what I'm the one of the main messages I'm trying to give you here is these are all Lego blocks and you can in some ways the most basic new Lego Block is now elementwise multiplication's interactions, and you can now put that one.",
            "Into lots of different other places.",
            "If you have a good reason based on your intuition for them.",
            "They're still there actually, so the interaction is important.",
            "You do really need both of them, and in fact it makes sense to get even more different kinds of gates, which will describe in LCM in a second.",
            "So maybe to give."
        ],
        [
            "Do some more intuition for the Gru.",
            "So just walking through the equations.",
            "In some ways it's intuitive, but it's good to make it explicit.",
            "So if the reset is close to zero, you essentially ignore all your previous hidden states, right?",
            "So your party here at zero you basically H tilt is just defined based on your current word vector.",
            "So that allows the model to drop information that is irrelevant in the future.",
            "Now so you can update your gate Z controls also or sorry the update gate Z controls basically how much of the past states should matter now.",
            "So if UZ Gate is close to one then we can copy information in that unit essentially as is through many time steps.",
            "So if ZTE here is 1 then all the H tilt here is 0 and that's why you have to have the sigmoid 2 so that that actually works out.",
            "And now this would allow you to basically just copy over.",
            "Tific fact so if I asked about John, for instance, and now it says, oh, John, maybe people are important because of word vectors are in that space in this kind of some intuition, it's not.",
            "We don't.",
            "I can't really guarantee you that this is what would happen, but at least that's the intuition of what could happen with these kind of more powerful units.",
            "Let's say John, that the model realizes all the people names are in this part of the vector space.",
            "Now what you essentially have each sigmoid single layer neural network, right?",
            "You have a linear.",
            "Decision boundary saying everything to this side is 1 or below.",
            "Here .5 and now it could say because John was here.",
            "I want to keep around a unit that stays one for a long time and basically doesn't get reset.",
            "Right, and this is what the gates are essentially doing, their gating your memory to allow you to just copy over some facts that seem to be important.",
            "And if you have a lot of them, then some things can be updated and some things can be kept around.",
            "And now if you do the simple math here, if this is just one, then it's also very easy to back propagate through this and not have the vanishing gradient problem.",
            "And that also makes them much easier to train than general recurrent neural networks.",
            "Alright, so yeah, that's another one.",
            "Units with short-term dependencies often have their reset gates be very active.",
            "All."
        ],
        [
            "Some more intuition here.",
            "We have an illustration to be honest.",
            "I again don't think the illustrations are as helpful as just looking at the equations.",
            "But in order for you to understand how to train that really the only new derivative that you have to use for backpropagation is the simple multiplication here, which we should all know and the rest is all the same chain rule that we do for general backpropagation.",
            "Alright, so that was the Gru there?",
            "Yeah, let's answer some more questions 'cause the next one, Alice Sam will be even more complex, yeah?",
            "As always.",
            "For a long paragraph.",
            "Right, so this model doesn't know really where you are in your paragraph of course.",
            "So first and last for general sort of document level sentiment analysis wouldn't be captured per say in this, but it would realize that in the first couple of sentences you have strong sentiment words that can then activate the.",
            "Gates and then those gates will actually stay active for awhile.",
            "They will basically have their Z here.",
            "Be close to 1 so that you can essentially copy over all the content stuff.",
            "That doesn't matter as much for sentiment classification and then the at the end those up those gates will get updated again and may be overruled by the final conclusion of the sentiment or not.",
            "So that would be the intuition here.",
            "Yeah.",
            "There can be.",
            "I don't know what the final implementation was for different people.",
            "It's a pretty simple change.",
            "You just initialize them to 0 again.",
            "Yeah.",
            "I it's kind of hard to hard to say how they both matter for the overall performance of the model, but it's hard to say I don't have an exact number in mind of like this is like 2% accuracy or something like that.",
            "Just one more question.",
            "Sorry, say again.",
            "Sorry if zed is equal to 1, then R doesn't even matter very much.",
            "So yeah, I mean in some ways you could say you could try to predict this before and say if it's like close enough to one you don't even need to compute the other one.",
            "Is that kind of what you're saying?",
            "But you could do that, but in the end it's often easier to make do the same.",
            "Same computation multiple times in a very efficient way.",
            "That is the same every time.",
            "So you can very easily batch it.",
            "You can very easily put things on a GPU and things like that rather than having like a lot of if else statements that will just kind of mess up your flow of your parameters to GPU and things like that.",
            "So it's possible, but it's unlikely to do that well.",
            "I actually implemented this myself a couple of times and you could take out certain things.",
            "And try to then delete those rows in the matrix to only multiply a couple of the rows in the matrix and things like that.",
            "But in the end all that logic were slower and deleting rows out of a matrix were slower than just multiplying the whole thing.",
            "And then potentially ignoring it.",
            "That's one part of the answer.",
            "The other one is it's not that common.",
            "That, or you know, does etzer.",
            "Generally a real number between zero and one, so it's unlikely there, you know.",
            "Like almost entirely one, so you might still want to allow a little bit of the updates.",
            "Yep.",
            "So in general you're right.",
            "These are these models are very hard to paralyse.",
            "Why is this so?",
            "In CNN for instance, all the filters that you compute are essentially independent of one another.",
            "Until you do the next pooling operation across filters.",
            "For instance here.",
            "Unfortunately, you can't really compute any of the current time step parameters until you've computed all the previous time step parameters.",
            "You can create one check entries matrix multiplication and be done with it.",
            "There are some tricks that people have done where they say.",
            "Maybe I will try to put more computation in a deep neural network at each time step that is independent and then have temporal interactions come only at the very last layer.",
            "So intuitively for instance."
        ],
        [
            "And these kinds of models I might get rid of these temporal interactions and these temporal interactions, and I just have a very deep network down here.",
            "And then I can basically paralyzed multiplication of all these units, and then only have this be the slow part, that I can't easily paralyze.",
            "So that is 1 trick that folks at biter used.",
            "I think for speech recognition.",
            "But other than that?",
            "There's no like sort of very simple obvious way to speed them up even more."
        ],
        [
            "Yeah."
        ],
        [
            "If just compute the last layer and then what?",
            "It doesn't.",
            "In the end they in speech recognition still got very good performance out of that system, so they still had state of the art performance on it, so I wouldn't say it has to perform worse than that.",
            "Alright."
        ],
        [
            "So now let's look into LCM's and that's the last big part of this lecture.",
            "So we can essentially make the units to compute the final hidden memory state even more complex, and it will look a little overwhelming, but it's essentially now the Lego pieces that we've covered before, and we just keep stacking them on top of each other and having more and more complex interactions.",
            "So now at each time step, we essentially have an input gate.",
            "That is somewhat similar to the reset gate, but now we have the input gate.",
            "We have a forget gate.",
            "That essentially, if the gate is zero, you would want to forget the past.",
            "Sometimes they forget gates are actually the memorize gates, so depending their different formulations of LCMS two so you have to be careful about how you look into the equations.",
            "And then we also.",
            "And this is a very new one.",
            "Have an output gate.",
            "So how much is the cell that you will actually compute exposed to the classifier?",
            "You may actually now have things where you say this isn't very important, but not for the prediction at this time step, but maybe for a prediction at a later time step.",
            "And then we have the new memory cell.",
            "So here again, so far these are just basically standard recurrent types of units, right?",
            "Very simple standard aren't end type stuff.",
            "But now the main complexity comes in how we actually compute the final memory cell in the final hidden state.",
            "So here the final memory cell is essentially the forget gate times the previous final memory cell, and this is again are elementwise interaction.",
            "Plus our input gate with elementwise interaction of the new memory cell.",
            "Somewhat intuitively, right, we have forgetting things about the past input, things about present, and then the final hidden state will actually be OT the output.",
            "Elementwise Times 10 H of the final memory cell."
        ],
        [
            "Now I think the illustrations are in many ways a little bit overwhelming and don't really.",
            "I think, give you that much intuition, but here are three different ones.",
            "Different attempts at giving you intuitions.",
            "In some ways I like.",
            "I like this one somewhat, but in general I don't.",
            "Yeah, I don't think they give that much intuition.",
            "You basically have here your multiplicative interactions with your input gate.",
            "You have your memory cell input, the forget gate can basically recur on itself, and then you have the output gate and so on.",
            "But basically the main intuition here is that your memory cells can keep their information intact unless the input makes them forget it or overwrite it with new input.",
            "So you can actually forget and or overwrite your input.",
            "And the cell can decide to output this information or just store it for next for the next time steps."
        ],
        [
            "Alright, LCMS are currently very hip.",
            "It's kind of a great blast from the past, or introduced by your computer and one of his students will hide her.",
            "I think.",
            "In the late early 90s.",
            "97 late 90s.",
            "And then yeah, they're basically right now.",
            "In some ways, the default model that many people use for sequence labeling tasks are very, very powerful, especially when you stack them.",
            "And now this is again the idea of taking the Lego pieces that we've had and basically adding making them even deeper, and because of all the weights that you now introduced for even a single time step, you want to have a lot of data to train this.",
            "And where would you have more?"
        ],
        [
            "State out then in machine translation where you have very large parallel corpora.",
            "So here are some numbers were not going to go into too many details here, but essentially the final sort of takeaway messages that they don't quite outperform the monster models with lots of different machine learning subcomponents quite yet, but they're getting very, very close, so here the best WMT result from 14 from last year is basically only half a blue point.",
            "Better.",
            "OK.",
            "Depending on which datasets you look at now, there are some where they outperform them and you sort of, yeah.",
            "Not on all of them, but on several of them.",
            "So this one is actually this one is basically a deep LCM model, so it's a.",
            "It's a very different model to the one that now is outperforming those, so that just the simple model that I just described where you have less teams you have the encoder, LCM decoder, LCM and then you describe it.",
            "So basically similar to the very simple model.",
            "I'll just go back really quick."
        ],
        [
            "This kind of model here.",
            "Where you essentially connect your hidden states to the one, it's not exactly this, but very similar.",
            "That model hasn't yet outperformed it, but then there are other models based on attention and so on that have that you should just mentioned."
        ],
        [
            "Now to give you some intuition, an answer one of your questions once I can about the semantics that pop out of this.",
            "So here you can look at PCA projected version of the vectors from the last time step after your trip train machine translation system and what's cool is it actually captures a lot of interesting semantics here.",
            "So we have Marriott Miis, John or Mary is in love with John, or closer to one another than John at Myers.",
            "Mary and John is in love with Mary, so you basically have here.",
            "Staying active passive variant.",
            "So I was given a card by her in the garden versus in the garden.",
            "She gave me a card so ideally here the OR the cool thing that falls out of this is that active passive translations are actually in a similar part of the vector space.",
            "So now the model can decide do I want to translate this in an active voice or passive voice and it actually captured that in variance in the vector space.",
            "Yep.",
            "It's a little tricky, so there are some some folks, so the identity initialized one actually also use Dennis, but it kind of depends on where you want to go eventually.",
            "If you just want to practice and learn by yourself, then you can create a variety of different sort of synthetic task.",
            "If you want to actually eventually.",
            "Publish something real and have a real model.",
            "See if your model is actually also implemented fast enough to work eventually on anything that's real language modeling, I think is probably the best one you have basically infinite amounts of data.",
            "You just take all of Wikipedia, try to predict the next word.",
            "So you can see how well you scale up overtime.",
            "Perplexity numbers are pretty commonly described, for instance, for the Penn treebank, which is also publicly available data set for just the language modeling.",
            "You know, kind of how good you should be for in terms of your perplexity for language modeling.",
            "So my hunch is probably language modeling isn't easy to use.",
            "Kind of good default task.",
            "It's a little less real in the sense that character level language models haven't gotten like state of the art performance on many things yet, but like yeah.",
            "So yeah, I guess there's a difference between wanting to learn and eventually trying to get famous with that project.",
            "So it will be hard to I. I actually think there's something to be said about words as a unit that makes sense, so that's an interesting discussion, and in some ways you know orthography can be different and people can understand words but not spelled incorrectly.",
            "Right so, especially if you look at Germans, friends with compound nouns, they can get very long if you have to have a specific word vector for each compound noun, of which there are way way more than are often used and you can very creatively put them together in new ways.",
            "It's nicer to actually have a character like character one."
        ],
        [
            "Right basically I described how we can have these Lego pieces and put them together in new ways, and this is a very recent work also.",
            "Where essentially you can combine these ideas of having deep general aren't ends, but you can also assume insight.",
            "Each of these could actually also be in Ellis Tymora, Gru and now you can instead of having them only go up and to the left, you can have them either both go sideways, again, bidirectional, that's sort of an extra Lego piece.",
            "Just flip all your for loops to go backwards, and then combine things.",
            "Or you can add basically gates from higher levels, or basically all the different hidden.",
            "Levels to all other levels and have basically gates say sometimes I want the most abstract or highest layer actually modify the lower layer at the next step so things can get more and more complex, and I think right now we're in this explosion of creativity where people are like, oh, there's a new Lego Block.",
            "Now I'm going to combine it with a lot of other different things because I've played around with all these Lego blocks and I gain intuition about how well they often work for different kinds of problems."
        ],
        [
            "So with that I want to summarize.",
            "Basically, I hope I could show you and give you some more detail.",
            "It's an in depth tricks for recurrent neural networks.",
            "There a very very powerful model for deep learning, I think gated recurrent units or even better than the standard recurrent neural network that just updates everything at every time step.",
            "And LCMS maybe even better, though the jury is still out there.",
            "Some experiments that actually say oh Gee, are using some experiments are better than LCMS.",
            "I think we don't know for sure yet, so if you want to learn, I would encourage you to actually implement both.",
            "It's a lot of ongoing work right now.",
            "I'm pretty sure next year this lecture will have a couple of more sort of state of the art results of these models.",
            "Now tomorrow I will actually put all these things together into what I think is a really cool model that can solve a lot of different tasks in natural language processing anything from logical reasoning to sequence labeling to classification to translation and so on.",
            "So we'll use all the Lego pieces we.",
            "Describe today and put them together in a new way.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think we've covered them a little bit.",
                    "label": 0
                },
                {
                    "sent": "Then we will on a very high level later on too.",
                    "label": 0
                },
                {
                    "sent": "So I thought maybe it's good to really dive into the details of how exactly recurrent neural networks work.",
                    "label": 0
                },
                {
                    "sent": "How do we train them?",
                    "label": 0
                },
                {
                    "sent": "What are some tips and tricks to actually make them work in practice?",
                    "label": 0
                },
                {
                    "sent": "'cause they sort of vanilla techniques will often not work, and then also how they've been extended in the past as well as just last year with better versions of recurrent neural networks, namely gated recurrent units.",
                    "label": 0
                },
                {
                    "sent": "An long short term memories.",
                    "label": 0
                },
                {
                    "sent": "So I think we've covered that tiny bit with Phil on high level and I want to just dive into details because tomorrow's lecture will be a lot of applications of those techniques as well as taking all these ideas as sort of very basic Lego blocks and then putting them together in a very complex large model called dynamic memory networks.",
                    "label": 0
                },
                {
                    "sent": "So let's get.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then just a quick refresher.",
                    "label": 0
                },
                {
                    "sent": "I know we just covered this language.",
                    "label": 0
                },
                {
                    "sent": "Models are really just use them as an excuse to introduce recurrent neural networks in their details to you.",
                    "label": 0
                },
                {
                    "sent": "We won't go into too many details here.",
                    "label": 0
                },
                {
                    "sent": "Basically our language model tries to compute the probability for an entire sequence of words and unlike traditional language models, we won't make sort of incorrect but necessary Markov assumptions where we might say alright, the last the current words probability depends only on.",
                    "label": 1
                },
                {
                    "sent": "The window of the previous N words.",
                    "label": 0
                },
                {
                    "sent": "Instead we will try to have that window be as large as possible with the recurrent.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Network.",
                    "label": 0
                },
                {
                    "sent": "So what's the main idea of a recurrent neural network?",
                    "label": 1
                },
                {
                    "sent": "Why aren't they just called neural networks?",
                    "label": 0
                },
                {
                    "sent": "The main idea is that we actually tie the weights at each timestep.",
                    "label": 1
                },
                {
                    "sent": "So we essentially tried to condition each neural network time step on all the previous words that come in, and this is the most standard way to formulate this where we have a word vector which we've covered already at any given time step T. Usually that will come from look up table or word embedding matrix that we can learn refer to VEC or glove.",
                    "label": 0
                },
                {
                    "sent": "Then we will have a hidden state.",
                    "label": 0
                },
                {
                    "sent": "Our current memory at each time step T and we will in the case of language models try to output.",
                    "label": 0
                },
                {
                    "sent": "The probability of a word YT at each time step.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of ways you will see this in the literature and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll try to actually expose you to maybe three or four different formulations to make sure you see whenever you see the main concept, you can kind of look beyond the syntax and see there's just a recurrent neural network inside here, so these are two other ways you will see recurrent neural networks be defined.",
                    "label": 0
                },
                {
                    "sent": "So sometimes instead of having.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of output nicely at each time step T and its preview.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And next one we basically just say this as our recurrence.",
                    "label": 0
                },
                {
                    "sent": "Or you may see just a single time step without the unfolding to its previous and next one.",
                    "label": 1
                },
                {
                    "sent": "The main equation that will usually use.",
                    "label": 0
                },
                {
                    "sent": "Is is this one up here where we have our word vectors and then here we have our standard neural network where we have a hidden to hidden matrix that will essentially connect this.",
                    "label": 1
                },
                {
                    "sent": "In this case, here or.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this formulation, this W right here is our hidden to hidden matrix.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we'll have the hidden toward vectors matrix, and you'll also often see this parentheses T here.",
                    "label": 0
                },
                {
                    "sent": "That says this is not.",
                    "label": 0
                },
                {
                    "sent": "This is basically the word vector that we pulled from the current time step T, and then we pulled that from our word look up table.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this by now in the summer school hopefully looks very familiar.",
                    "label": 0
                },
                {
                    "sent": "So essentially two linear layers put together followed by non linearity in element one.",
                    "label": 0
                },
                {
                    "sent": "It could be a sigmoid, but will actually compare and see that other ones or even better.",
                    "label": 0
                },
                {
                    "sent": "And then we have our standard softmax layer or softmax classifier on top of that hidden representation.",
                    "label": 0
                },
                {
                    "sent": "And that will essentially be the probability for each word is just the J element here of that vector.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "What do you do with HDR?",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "In many cases you can actually initialize it, either random or just zero.",
                    "label": 0
                },
                {
                    "sent": "There is more common.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about this 'cause we'll move on from that in a sort of our most basic level piece, yeah?",
                    "label": 0
                },
                {
                    "sent": "So WS is just a softmax so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe this slide here will dive into some more details and will very carefully define here all the terminology.",
                    "label": 0
                },
                {
                    "sent": "So we have DH is the dimensionality of our hidden layer, so WH has to be a square matrix.",
                    "label": 1
                },
                {
                    "sent": "However WHX from the word vectors to the hidden vectors doesn't have to be squared toward vectors.",
                    "label": 0
                },
                {
                    "sent": "Could be very high dimensional.",
                    "label": 0
                },
                {
                    "sent": "If you think there is, you know you summarize everything at each time step.",
                    "label": 0
                },
                {
                    "sent": "You might have to have a smaller dimensionality if you think you want to accumulate more and more from word vectors, you might want to have a larger dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Or your hidden state.",
                    "label": 0
                },
                {
                    "sent": "And then WS will be the softmax, weights that in the case for language models, basically have the number of columns equal to the size of the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                },
                {
                    "sent": "That's great.",
                    "label": 0
                },
                {
                    "sent": "That's a great question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not a one hot vector that just gives you the index of the word.",
                    "label": 0
                },
                {
                    "sent": "It's actually the word vector, like word to vector glove vector, so it's usually 100 to 200 dimensional dense vector representation of each work.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Why are we recomputing it?",
                    "label": 0
                },
                {
                    "sent": "It's different at every time step, depending on what your H is at each time step, you'll have a different normalization.",
                    "label": 0
                },
                {
                    "sent": "Does that answer your question?",
                    "label": 0
                },
                {
                    "sent": "So there will actually be several tricks here, 'cause if you think about this, we have a lot of different words in the English language, so there will be some tricks on how to make that softmax more efficient.",
                    "label": 0
                },
                {
                    "sent": "But in general the normalization will be different at each time set.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's a great question.",
                    "label": 0
                },
                {
                    "sent": "So where do we get X from?",
                    "label": 0
                },
                {
                    "sent": "We will actually be able to get all these access.",
                    "label": 0
                },
                {
                    "sent": "We could actually train them together jointly.",
                    "label": 0
                },
                {
                    "sent": "With this model.",
                    "label": 0
                },
                {
                    "sent": "We could take derivatives of this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Objective function here with respect to every single one of these parameters.",
                    "label": 0
                },
                {
                    "sent": "It could be including the X, but you can also and this what often is commonly done.",
                    "label": 0
                },
                {
                    "sent": "Initialize the word vectors by some pre training method that is unsupervised like word to vector Club.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It usually helps a lot in the convergence and training speed to get to a similar performance without pre trained word vectors it takes takes much longer.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is our main definition here.",
                    "label": 0
                },
                {
                    "sent": "Again, this is the question here.",
                    "label": 0
                },
                {
                    "sent": "For the hidden layer timestep 0.",
                    "label": 1
                },
                {
                    "sent": "And I usually define L here as our look up matrix for the word vector.",
                    "label": 0
                },
                {
                    "sent": "So we store all the word to VEC or glove vectors in L.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so Yhat here's our probability distribution over the entire vocabulary at each time step, and so you already looked at the cross entropy error for general classification.",
                    "label": 0
                },
                {
                    "sent": "If you just have a standard network, you have a classification problem.",
                    "label": 0
                },
                {
                    "sent": "We essentially use that exact same cross entropy loss function, but instead of predicting a specific class, we just predict all the words.",
                    "label": 0
                },
                {
                    "sent": "So you some here at each time, step over the entire vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is often or more commonly used is perplexity, which is just 2 to the power of the negative of the average log probabilities over the entire data set.",
                    "label": 0
                },
                {
                    "sent": "So we just add it here.",
                    "label": 0
                },
                {
                    "sent": "This is our prediction is the ground truth, so we at each time step T we look at which word actually appeared at that time step we sum over the vocabulary and we sum over all the time steps T that we have in the data set.",
                    "label": 1
                },
                {
                    "sent": "Now we have the negative we tried to.",
                    "label": 0
                },
                {
                    "sent": "Notice now perplexity is the more common number that you'll see, and it's basically just another.",
                    "label": 0
                },
                {
                    "sent": "There's some nice connections there to information theory, but the main point is lower is better.",
                    "label": 0
                },
                {
                    "sent": "You want to basically be less perplexed by seeing the next word given your history of previous words.",
                    "label": 0
                },
                {
                    "sent": "So this is in many ways just going over some of the details of the previous lecture.",
                    "label": 0
                },
                {
                    "sent": "So now why is training these recurrent neural networks heart?",
                    "label": 0
                },
                {
                    "sent": "Or do you have any more questions about just a general definition?",
                    "label": 0
                },
                {
                    "sent": "So we now have defined exactly where each of these white hats comes from.",
                    "label": 0
                },
                {
                    "sent": "We have defined our general objective function or loss function.",
                    "label": 0
                },
                {
                    "sent": "Now we would generally just as with backpropagation, take derivatives with respect to all the different parameters that we have in the model.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Usually have 102 at most 1000.",
                    "label": 0
                },
                {
                    "sent": "Once you have 1000, that means that you're.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "WHH will be a million parameters, so that's that's quite a lot of parameters and you would want to have a very large training data set to train a million parameters.",
                    "label": 0
                },
                {
                    "sent": "Ideally have at least 1,000,000 words.",
                    "label": 0
                },
                {
                    "sent": "So it actually doesn't.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is that, in a perfect world this H. Might be able to capture a lot of different previous like the facts and the words that came into all the previous time steps.",
                    "label": 0
                },
                {
                    "sent": "That's in theory.",
                    "label": 0
                },
                {
                    "sent": "In practice you will actually lose information the further away you go and will actually cover why that is both during training and that will then follow to test time.",
                    "label": 0
                },
                {
                    "sent": "So we're not making any strong Markov assumption here.",
                    "label": 0
                },
                {
                    "sent": "That would be if we said we delete everything here and at every.",
                    "label": 0
                },
                {
                    "sent": "You know after every 3 words we actually set it to zero and then only look at the last three time steps or something like that.",
                    "label": 0
                },
                {
                    "sent": "But really this hidden state in theory could capture a lot of the previous history.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": ".",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "This one right here.",
                    "label": 0
                },
                {
                    "sent": "Absolutely.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's no theoretical reason why you couldn't do that.",
                    "label": 0
                },
                {
                    "sent": "In most cases, you wouldn't, because the time steps in three are probably even easier and more accurately predictable.",
                    "label": 0
                },
                {
                    "sent": "If you looked at next towards that, come just before that.",
                    "label": 0
                },
                {
                    "sent": "But there's no theoretical reason why you couldn't predict.",
                    "label": 0
                },
                {
                    "sent": "Really, anything on top of this?",
                    "label": 0
                },
                {
                    "sent": "So what I'm trying to, what we'll get to in a second, is that really this language modeling formulation where we try to predict the next word is just one of many we can really try to predict the part of speech tags at each timestep, named entity tags.",
                    "label": 0
                },
                {
                    "sent": "Really anything we can think of.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Why do we not sum over the vocabulary?",
                    "label": 1
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ways this is just the definition of the general cross entropy error, but if you think about this, why T is actually the word that really appears, and so this will be a one here and is 0 for everything else.",
                    "label": 0
                },
                {
                    "sent": "So it's just a very general definition of this.",
                    "label": 0
                },
                {
                    "sent": "You will actually look at only the YT that actually appears, 'cause this will be 0 for all the words that don't appear.",
                    "label": 0
                },
                {
                    "sent": "So in some sense you could simplify this formulation here.",
                    "label": 0
                },
                {
                    "sent": "Right, great insight.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so why is training recurrent neural networks hard and even harder than training generally very deep neural networks, you could say?",
                    "label": 1
                },
                {
                    "sent": "Oh well, we just have a one layer neural network at each time step.",
                    "label": 1
                },
                {
                    "sent": "However, as you unfold this overtime, you actually get an extremely deep neural network, one that has 1000 layers.",
                    "label": 0
                },
                {
                    "sent": "If you have 1000 words that you're trying to predict in a row.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "In general, our hope was that the inputs from many time steps ago will actually modify our pretty why.",
                    "label": 0
                },
                {
                    "sent": "And one thing that I would encourage all of you to do is actually really go and try to take the derivative of 1 error function at one time step with respect to even just the main hidden to hidden parameters just for two unfolded time steps.",
                    "label": 0
                },
                {
                    "sent": "It will be very insightful.",
                    "label": 0
                },
                {
                    "sent": "It's relatively simple to go through the matrix algebra to do this, but it's very helpful.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the main problem that we have as we try to train this is actually the so called vanishing and exploding gradient problem.",
                    "label": 0
                },
                {
                    "sent": "Basically, unlike in standard deep neural networks where we have a different set of weights at every time step.",
                    "label": 1
                },
                {
                    "sent": "Here we actually have the exact same matrix at each time step.",
                    "label": 0
                },
                {
                    "sent": "And that might result in multiplying by a Jacobian there actually all the derivations are in the appendix of these slides, so we'll put these online.",
                    "label": 0
                },
                {
                    "sent": "You can look into the details as you write out all the gradients.",
                    "label": 0
                },
                {
                    "sent": "At some point you will get a term.",
                    "label": 0
                },
                {
                    "sent": "This Jacobian of.",
                    "label": 0
                },
                {
                    "sent": "The hidden to hidden from one time step to the next one and you will see there that if you take the normal that that Norm will be either smaller than one or larger than one, and you multiply that same norm at every given time step on a high level intuition as you take your gradients from one time step.",
                    "label": 0
                },
                {
                    "sent": "Here you have your errors that you Delta error messages that you push back through network.",
                    "label": 0
                },
                {
                    "sent": "Intuitively what happens at every time step if you have a non linearity that squashes that value to be between.",
                    "label": 0
                },
                {
                    "sent": "Minus one and one, and you have that same value that keeps happening over and over.",
                    "label": 0
                },
                {
                    "sent": "You multiply your gradients with numbers that are smaller than one each at each time step and basically your gradient will vanish and if you for instance try to predict here a specific word.",
                    "label": 0
                },
                {
                    "sent": "That happened, and that word would really only know that it would happen next based on a word that happened 10 timesteps ago, you couldn't train the model.",
                    "label": 0
                },
                {
                    "sent": "You couldn't tell this model and train it with this information.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just said so yeah, so this is basically the problem.",
                    "label": 0
                },
                {
                    "sent": "You may at this time step here, modify the weights for XD and for this time step.",
                    "label": 0
                },
                {
                    "sent": "But as you go further and further back you will essentially lose that Gray.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's a particular example for language modeling where this is a problem.",
                    "label": 0
                },
                {
                    "sent": "Basically, we would ideally take into consideration as many words of the past as possible.",
                    "label": 0
                },
                {
                    "sent": "So here we have the sentence Jane walked into the room.",
                    "label": 0
                },
                {
                    "sent": "John walked into.",
                    "label": 0
                },
                {
                    "sent": "It was late in the day.",
                    "label": 0
                },
                {
                    "sent": "Jane said hi to now.",
                    "label": 0
                },
                {
                    "sent": "In this case, all of you would probably close to probability one, no.",
                    "label": 0
                },
                {
                    "sent": "What word comes next here?",
                    "label": 0
                },
                {
                    "sent": "But because that word is actually many time steps away, it's very hard for recurrent neural network language model to not just basically put a high probability to many different kinds of names on here.",
                    "label": 1
                },
                {
                    "sent": "So is that does the intuition make sense for people the vanishing gradient problem?",
                    "label": 0
                },
                {
                    "sent": "Some nodding.",
                    "label": 0
                },
                {
                    "sent": "Who here wants to go through all the detailed derivations with the problem that it will eat away at your lunch?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Alright, will do it for you.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are people during lunch alright?",
                    "label": 0
                },
                {
                    "sent": "So now I talked about vanishing creating problem.",
                    "label": 0
                },
                {
                    "sent": "We multiply numbers that are smaller than one at each time step.",
                    "label": 0
                },
                {
                    "sent": "As you send your Delta error messages through the network.",
                    "label": 0
                },
                {
                    "sent": "There's also the problem of the exploding gradient problem.",
                    "label": 0
                },
                {
                    "sent": "However, that one has a very nice and simple hack to fix it, which is we just clip the gradient.",
                    "label": 0
                },
                {
                    "sent": "It sounds pretty hacky, but there's some nice intuition that came from Yoshi's Group, actually.",
                    "label": 0
                },
                {
                    "sent": "So if we have if the gradient explode, so we have.",
                    "label": 0
                },
                {
                    "sent": "Are Delta error messages flow through the network at each time step they get multiplied by numbers larger than one and they eventually you know.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It might explode and give you not a number.",
                    "label": 0
                },
                {
                    "sent": "Now the simplest thing we could do is just looking at the norm of the gradient.",
                    "label": 0
                },
                {
                    "sent": "If it's larger than a certain threshold, we basically set it to that threshold and that makes a huge difference and one really great paper from Joshua's group actually gave an intuition and this is something that I think is also really clever inside.",
                    "label": 0
                },
                {
                    "sent": "Whenever you're struggling with the neural network and you something isn't working right 'cause you're actually working on some novel research method, I encourage you to try to visualize it.",
                    "label": 0
                },
                {
                    "sent": "As much as you can, you can visualize the hidden states as they go overtime.",
                    "label": 0
                },
                {
                    "sent": "You can try to visualize your gradients and norms of your gradients overtime.",
                    "label": 0
                },
                {
                    "sent": "Or you can even try to create very tiny version of your model where you can visualize pretty much every single wait and hear what Josh's group did.",
                    "label": 0
                },
                {
                    "sent": "And Sean, who is the first author of this work, is essentially train a single hidden unit RN and that basically here on the Y.",
                    "label": 0
                },
                {
                    "sent": "On this axis we have the value of Y and here we have the value of P, the bias term.",
                    "label": 0
                },
                {
                    "sent": "And now we have our error function here and you can see in the solid lines the standard gradient descent trajectory.",
                    "label": 0
                },
                {
                    "sent": "So it goes here it goes here and then what they observed is are these really high curvature walls and so the standard gradient would basically get here.",
                    "label": 0
                },
                {
                    "sent": "And instead of trying to explore this Valley against jumps against the wall and goes way further back.",
                    "label": 0
                },
                {
                    "sent": "Moving away from what clearly would be a better optimum of the error.",
                    "label": 0
                },
                {
                    "sent": "And this is essentially one traditions of Y clipping.",
                    "label": 0
                },
                {
                    "sent": "The gradient actually helps.",
                    "label": 0
                },
                {
                    "sent": "Here you go here and then instead of jumping way far back, you move less far back.",
                    "label": 0
                },
                {
                    "sent": "Any questions about that?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Exam.",
                    "label": 0
                },
                {
                    "sent": "So in general the.",
                    "label": 0
                },
                {
                    "sent": "That that gradient that you clip actually applies to all the different parameters of the model so.",
                    "label": 0
                },
                {
                    "sent": "I don't see the intuition of how it changes the bias for specific words, imagine they're all word vectors and we take derivatives and it's basically here.",
                    "label": 0
                },
                {
                    "sent": "You know the hidden hidden dimensions too, and things like that.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "That way, if one unit just blows up 'cause that number gets multiplied by value larger than one for many time steps, you could just click that one, but this one doesn't have the nice intuition that this paper had.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Like I said, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty simple hack, but either way.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, they basically work the same.",
                    "label": 1
                },
                {
                    "sent": "I don't think there has been a really proper comparison across a lot of different tasks for the two.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "Like Joshua said, the main thing is just.",
                    "label": 0
                },
                {
                    "sent": "Alright, so there's one other way that is actually a very recent trick.",
                    "label": 0
                },
                {
                    "sent": "Well semi recent to avoid the vanishing gradient problem and make the exploding gradient actually a little more likely because you have even larger numbers.",
                    "label": 0
                },
                {
                    "sent": "Often I think in that case it happens more often than if you had like 10 inch units, for instance, but.",
                    "label": 0
                },
                {
                    "sent": "One really neat trick is to use to initialize all your W matrices so the hidden to hidden connections.",
                    "label": 0
                },
                {
                    "sent": "Assuming we have the same dimensionality for our hidden dimension as we have for our word vectors, as well as the word vector to hidden dimensions as the identity matrix is very simple idea.",
                    "label": 0
                },
                {
                    "sent": "Identity matrix ones on the diagonal and then also use the rectified linear units that already covered.",
                    "label": 0
                },
                {
                    "sent": "I'm sure where you basically just have Max Z0 and those two tricks make a huge difference in initial for initialization and then also not having the vanishing gradient problem.",
                    "label": 0
                },
                {
                    "sent": "This idea was actually first introduced in a paper of mine 2013 for recursive neural networks, which are just that restructured versions of these change structured models of.",
                    "label": 0
                },
                {
                    "sent": "Recurrent neural networks and then there are some new experiments with recurrent neural networks and rectified linear units combined where they show that the test accuracy.",
                    "label": 0
                },
                {
                    "sent": "If you have this identity initialized recurrent neural network actually gets way higher than if you had a standard recurrent neural network where you had 10 H units and you didn't initialize with identity matrices.",
                    "label": 0
                },
                {
                    "sent": "So very simple idea.",
                    "label": 0
                },
                {
                    "sent": "Initialize all your parameters of the recurrent neural network with identity matrices, yeah?",
                    "label": 0
                },
                {
                    "sent": "It depends.",
                    "label": 0
                },
                {
                    "sent": "So the very originally what we've done in our intuition was as you merge toward vectors, the default.",
                    "label": 0
                },
                {
                    "sent": "If you don't know anything about how to merge them, you actually have identity matrices times 1/2.",
                    "label": 0
                },
                {
                    "sent": "Which essentially just means you're averaging your word vectors and that is also a reasonable thing to do if you have a recurrent neural network bearing any other knowledge, what you would start out doing is just basically taking sort of a windowed average about over word vectors.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The hidden to WHX.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah you do both.",
                    "label": 0
                },
                {
                    "sent": "In this paper.",
                    "label": 0
                },
                {
                    "sent": "And they showed that it helps.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So it works even better, so they have, I guess not in this plot, but they tried also have the identity initialized with 10 H and that does not work as well as with rectified linear units.",
                    "label": 1
                },
                {
                    "sent": "So yeah, like I said, you have actually the exploding gradient problem.",
                    "label": 0
                },
                {
                    "sent": "A lot more common in these kinds of models.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but you can still remember in fact yeah, they they show on a couple of different.",
                    "label": 0
                },
                {
                    "sent": "I mean this is kind of a toy task here, pixel by pixel permitted amnist, but even on other kinds of language tasks they show that this.",
                    "label": 0
                },
                {
                    "sent": "This helps a lot and you can indeed memories things.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so some quick perplexity results just to give you an idea that recurrent neural networks do or help for language modeling.",
                    "label": 0
                },
                {
                    "sent": "I don't think we need to go through too many details here, but basically when you combine that, especially with sort of very good memorization of standard language models like Ness and a smooth language models, you really have much lower perplexity and better probabilities for the words that actually appear.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to give you a trick 'cause you say, oh man, I understand him now.",
                    "label": 0
                },
                {
                    "sent": "Very simple.",
                    "label": 0
                },
                {
                    "sent": "I have all these of my tricks for exploding, creating problems invention, creating problems.",
                    "label": 0
                },
                {
                    "sent": "I'm going to implement them and then you want to actually predict reasonably large vocabularies.",
                    "label": 0
                },
                {
                    "sent": "You'll run into the problem of having to run a gigantic matrix multiplication of, say your 100 dimensional hidden states times 100,000.",
                    "label": 0
                },
                {
                    "sent": "'cause maybe you have 100,000 different words in your vocabulary and you very quickly realize that you won't even be able to run that on your laptop and so.",
                    "label": 0
                },
                {
                    "sent": "One really neat trick, also very simple, is essentially have a class based word prediction where instead of directly predicting your WT based on the history or your memory state H, you actually will first predict the class based on the history and then the word based on that class.",
                    "label": 0
                },
                {
                    "sent": "And now you can only a train time only look at specific classes at training time and what you observe is the fewer classes you have, the faster you will train.",
                    "label": 0
                },
                {
                    "sent": "But it also hurts you a little bit in perplexity, so there's a tradeoff there as you go to higher and higher or more and more classes you essentially.",
                    "label": 0
                },
                {
                    "sent": "Have less less error, but it will also again get much lower.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That probably is already clear, but as you go through your network, what you have to do.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you add your errors.",
                    "label": 0
                },
                {
                    "sent": "You really don't just take one error at a time and then back propagate all the way back through time, but what you do is.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You add your errors so as you train, you basically only have to go from the beginning from the end of time all the way to the beginning of time of that current recurrent neural network once and at each time step you add new Delta functions coming from the error that you have at each time step and you basically add your Delta some sort of waterfall where each time step adds its own error.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's not close at all.",
                    "label": 0
                },
                {
                    "sent": "So as you train it, it basically moves further and further away from that initialization.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It's basically just a really good and there's initialization where you say on average, if I don't know what happens, instead of making a completely random random projection of my word vector into the hidden state, you assume you more more likely average.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, just to give you an intuition of this identity.",
                    "label": 0
                },
                {
                    "sent": "Basically here you have imagined to identity matrices or just half of this identity matrix times half times this identity matrix so initialized with half's on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "What you would do is basically average your current hidden state with the current word vector.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that this current state.",
                    "label": 0
                },
                {
                    "sent": "Depends on all the previous time states.",
                    "label": 0
                },
                {
                    "sent": "No, it depends on all the previous ones.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, yeah, the current word vector modifies it.",
                    "label": 0
                },
                {
                    "sent": "Well, depending on how you initialize it at the very beginning, right?",
                    "label": 0
                },
                {
                    "sent": "You can initialize here just both to halftime.",
                    "label": 0
                },
                {
                    "sent": "Set any matrix.",
                    "label": 0
                },
                {
                    "sent": "Then it'll depend on half from this and have fun all previous time steps.",
                    "label": 0
                },
                {
                    "sent": "That that would be interesting, but you can really play around with that idea and have.",
                    "label": 0
                },
                {
                    "sent": "Different initialized different multipliers of your identity matrices in the beginning.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now this was for language modeling.",
                    "label": 0
                },
                {
                    "sent": "Really language modeling is 1 task, but it's not as interesting to me personally as a lot of other more downstream tasks that real people and companies and.",
                    "label": 0
                },
                {
                    "sent": "Sort of organizations really care about, so one of those is named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "Basically trying to find locations, names and people names inside larger text.",
                    "label": 0
                },
                {
                    "sent": "That way you could try to find all the texts that Barack Obama, for instance, was mentioned or presidents or specific company names and so on.",
                    "label": 0
                },
                {
                    "sent": "There's also there's also entity level sentiment in context, so trying to say I thought the iPhone was a good phone when it came out, but then later on I really disliked the screen or something like that.",
                    "label": 0
                },
                {
                    "sent": "Now you want to say screen in this context is negative.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially entity level sentiment in context, and another fun one that is related to that or opinionated expressions.",
                    "label": 0
                },
                {
                    "sent": "And in the next couple of slides I will use the terminology from the paper biozone years are in Clare, Cardi from last year on opinion mining with deep recurrent neural networks, and I'll use another task as an excuse to talk about various extensions now of recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "Add anymore questions about the basic recurrent one.",
                    "label": 0
                },
                {
                    "sent": "Alright, 'cause.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everything will explode, will have now depths in many directions and so on.",
                    "label": 0
                },
                {
                    "sent": "It'll be great alright, so here the next task that will look at is opinion mining.",
                    "label": 0
                },
                {
                    "sent": "So it is essentially just another Y hat that we're trying to predict instead of having to predict probability for every single word in vocabulary, we just predict a specific class and the class is that we look at here are over direct subjective expressions and expressive subjective expressions.",
                    "label": 0
                },
                {
                    "sent": "The first one is essentially just an explicit.",
                    "label": 0
                },
                {
                    "sent": "Mention of private states or speech events that express private States and the second one basically indicate a specific sentiment or emote.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And without explicitly mentioning it, so just to give you an idea here for instance as usual doesn't say very explicitly this is.",
                    "label": 0
                },
                {
                    "sent": "A shitty phone or something like that, but it sort of implicitly describes also an emotional state, so the way we want to label.",
                    "label": 0
                },
                {
                    "sent": "The words here in context is essentially by giving them.",
                    "label": 0
                },
                {
                    "sent": "These kinds of beginning intermediate.",
                    "label": 0
                },
                {
                    "sent": "Or non labels.",
                    "label": 0
                },
                {
                    "sent": "So if we have for instance EC expression, the first of the EC expression, the first part of that will always start with a B and then the continuation of whatever happened there will have the label I.",
                    "label": 0
                },
                {
                    "sent": "This is a very common sort of bio way to label words and sequences.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "X is still our word vector and we have Y Now our output labels the BIOS.",
                    "label": 0
                },
                {
                    "sent": "And we still use the same softmax.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the reason why they used different letters instead of superscripts for HRH is that will actually use a lot of other superscripts for making this model a lot more complex.",
                    "label": 0
                },
                {
                    "sent": "So the first problem is as you try to label a specific word, you would ideally not just use all the context from the left side, but you would also want to use all the context from the right side.",
                    "label": 0
                },
                {
                    "sent": "Now this is a more general problem than language modeling, where you don't usually have the future you want to just predict the next word.",
                    "label": 0
                },
                {
                    "sent": "In the case for speech, for instance.",
                    "label": 0
                },
                {
                    "sent": "Speech recognition, so here we essentially will define a forward recurrent neural network as well as a backwards recurrent neural network.",
                    "label": 0
                },
                {
                    "sent": "And they're essentially the same.",
                    "label": 0
                },
                {
                    "sent": "It's just one goes from time step.",
                    "label": 0
                },
                {
                    "sent": "You know, zero to T and the other one goes from T to 0.",
                    "label": 0
                },
                {
                    "sent": "And we can now represent.",
                    "label": 0
                },
                {
                    "sent": "The current time step by basically just concatenating the recurrent neural network that went from left to right and the one that went goes from the right to the left.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about the bidirectional recurrent neural net?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "To make separate what?",
                    "label": 0
                },
                {
                    "sent": "To remain separate or mix them.",
                    "label": 0
                },
                {
                    "sent": "I'm not quite sure I understand your question, so why?",
                    "label": 0
                },
                {
                    "sent": "Why do you have just additive interactions here?",
                    "label": 0
                },
                {
                    "sent": "Is that the question?",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah, so there's actually no.",
                    "label": 0
                },
                {
                    "sent": "It's not necessary that H here has the same dimensionality as X.",
                    "label": 0
                },
                {
                    "sent": "Unless you want to do the identity initialization trick, but even there you could probably add another zero block if you wanted to.",
                    "label": 0
                },
                {
                    "sent": "Does that answer your question?",
                    "label": 0
                },
                {
                    "sent": "We will chat over lunch.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is our bidirectional recurrent neural network.",
                    "label": 1
                },
                {
                    "sent": "I'll describe it like this now.",
                    "label": 0
                },
                {
                    "sent": "OK, now we can go even deeper and this is the right summer school to do that where we essentially will say not only do we go from left to right and have have depth in along the time dimension, but will also actually go up at each time step and we'll have recurrent neural Nets that go from left to right and up and down.",
                    "label": 0
                },
                {
                    "sent": "So essentially at each time step here, this vector will depend.",
                    "label": 0
                },
                {
                    "sent": "Or this one here will depend not just on.",
                    "label": 0
                },
                {
                    "sent": "Its own time step and deep neural network at that time step, but also on the time step before.",
                    "label": 0
                },
                {
                    "sent": "And that's why you now have back here.",
                    "label": 0
                },
                {
                    "sent": "Super Scripts have essentially a very deep neural network at each time step.",
                    "label": 0
                },
                {
                    "sent": "So that's a great question.",
                    "label": 0
                },
                {
                    "sent": "There probably be different answers from different people in the audience.",
                    "label": 0
                },
                {
                    "sent": "Mine is also evolving overtime I think, and the very beginning it will be.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But to implement this kind of model personally from scratch entirely, I would just take a simple, you know Matlab or Python user my matrices.",
                    "label": 0
                },
                {
                    "sent": "Here's how I multiply it to gain intuition, but once you've done that once in your life and your fellow like, alright, I get it.",
                    "label": 0
                },
                {
                    "sent": "It's really just a bunch of matrix multiplications then you.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to start using more and more sophisticated libraries like Theano or Torch Ware.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can really very easily define this and then have a loop that just initializes another layer and eventually you can have a hyperparameter just says how many layers do I want, and depending on that hyperparameter, it'll just initialize all these matrices automatically and put them together and very nice Lego blocks, and I think Theano and torture or good alternatives.",
                    "label": 0
                },
                {
                    "sent": "Actually, during my PhD, I guess neither of those were that as far along when I start in 2010, especially not for NLP.",
                    "label": 0
                },
                {
                    "sent": "So I actually just implemented all of that from scratch.",
                    "label": 0
                },
                {
                    "sent": "And with just matrix multiplication, so it's doable to.",
                    "label": 0
                },
                {
                    "sent": "But yeah, basically you want to abstract away as much as possible, 'cause eventually you don't want to write down all the derivatives here by hand.",
                    "label": 0
                },
                {
                    "sent": "You want to have an automated way of saying here's a block.",
                    "label": 0
                },
                {
                    "sent": "That block gets an input vector X and gives back a Delta message from for its errors.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So you go basically up at each time step and you predict something and you have connections from the left for all the recurrent neural networks that are from the left to the right, and connections from the right, but only for those that go from the right to the left and then you concatenate.",
                    "label": 0
                },
                {
                    "sent": "You concatenate the two states at the very end for the softmax.",
                    "label": 0
                },
                {
                    "sent": "It does, yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So you do right so you have here at each time step.",
                    "label": 0
                },
                {
                    "sent": "This unit depends on many previous units before and this unit here depends on many sort of succeeding or next time steps that come in the text.",
                    "label": 0
                },
                {
                    "sent": "So eventually the combination of these two, the concatenation of these two, keeps the context from the left and the right.",
                    "label": 0
                },
                {
                    "sent": "So that is what's implemented.",
                    "label": 0
                },
                {
                    "sent": "From top to bottom.",
                    "label": 0
                },
                {
                    "sent": "So you want to have this thing modify weights.",
                    "label": 0
                },
                {
                    "sent": "To hear but so this thing comes.",
                    "label": 0
                },
                {
                    "sent": "It's basically computed based on what's here.",
                    "label": 0
                },
                {
                    "sent": "So in general, yes you could add like short circuit connections.",
                    "label": 0
                },
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "Instead of saying this thing will depend only on this.",
                    "label": 0
                },
                {
                    "sent": "You could actually now also add a short circuit connection that goes all the way from the word vector, for instance also directly to that wait so there are a lot of other connections that you can add.",
                    "label": 0
                },
                {
                    "sent": "So instead of here, WH for the top one you could add plus XD the word vector for instance.",
                    "label": 0
                },
                {
                    "sent": "But at some point.",
                    "label": 0
                },
                {
                    "sent": "You know the complexity doesn't buy you that much extra.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, you could.",
                    "label": 0
                },
                {
                    "sent": "I mean combining two classifiers.",
                    "label": 0
                },
                {
                    "sent": "If you didn't do that and you just had a distribution that you get at the very end, and then you combine the outputs of those two final classifiers, you could train literally them.",
                    "label": 0
                },
                {
                    "sent": "You could train him entirely separately and then just average the weights for your classes from the left and right model.",
                    "label": 0
                },
                {
                    "sent": "It's essentially just description, so we would say I personally would say this is 1 model.",
                    "label": 0
                },
                {
                    "sent": "That has in the end one classifier with the weights you.",
                    "label": 0
                },
                {
                    "sent": "Instead of saying this is 2 classifiers, it's just a model that takes in information from different sources to make a prediction at each time step.",
                    "label": 0
                },
                {
                    "sent": "That's how I would describe.",
                    "label": 0
                },
                {
                    "sent": "Alright, one more question.",
                    "label": 0
                },
                {
                    "sent": "And then we'll move on.",
                    "label": 0
                },
                {
                    "sent": "That's a great question, so this is not an LCM yet.",
                    "label": 0
                },
                {
                    "sent": "This is just a standard recurrent neural network with standard additive interactions.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will answer that question actually in the next slide.",
                    "label": 0
                },
                {
                    "sent": "So how do we evaluate this?",
                    "label": 0
                },
                {
                    "sent": "There's a so called empty corpus basically has only 11,000 sentences, so not that many sentences take that into consideration.",
                    "label": 0
                },
                {
                    "sent": "When will in the next slide look over the evaluation and number of layers and we use standard F1 metrics precision recall.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here here's an intuition.",
                    "label": 0
                },
                {
                    "sent": "So we basically here have our F1 metric, and we have the number of layers and we have different models with different sets of different numbers of weights.",
                    "label": 0
                },
                {
                    "sent": "So depending on how large your hidden state is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, mostly then the size of the hidden state hidden states change.",
                    "label": 0
                },
                {
                    "sent": "So here we see how well this changes or how much better it gets with a deeper and deeper architecture.",
                    "label": 0
                },
                {
                    "sent": "Essentially the.",
                    "label": 0
                },
                {
                    "sent": "Model that has three hidden layers and 200,000 parameters.",
                    "label": 0
                },
                {
                    "sent": "Got the best performance here at three layers.",
                    "label": 0
                },
                {
                    "sent": "Once you add it even more layers that actually create it.",
                    "label": 0
                },
                {
                    "sent": "So just this gives you some intuition.",
                    "label": 0
                },
                {
                    "sent": "It's not always that the more layers are better, there's a good chance here.",
                    "label": 0
                },
                {
                    "sent": "The more layers you have at some point you overfit even more, but in general it did help to add.",
                    "label": 0
                },
                {
                    "sent": "Several of these extra layers here.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Overfit, so as you go.",
                    "label": 0
                },
                {
                    "sent": "In many cases, the train accuracy was actually close to 100.",
                    "label": 0
                },
                {
                    "sent": "What does what mean?",
                    "label": 0
                },
                {
                    "sent": "Do we need a special regularizer?",
                    "label": 0
                },
                {
                    "sent": "You do in general want to regularize all these models.",
                    "label": 0
                },
                {
                    "sent": "I don't know if there's any special regularizer for this specific model.",
                    "label": 0
                },
                {
                    "sent": "There wasn't.",
                    "label": 0
                },
                {
                    "sent": "There's just L2 regularization all the weights.",
                    "label": 0
                },
                {
                    "sent": "But you could try and now you know initialize your different layers with identity matrices.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of tricks you can now basically combine all the things we've covered before at different layers, but in general.",
                    "label": 0
                },
                {
                    "sent": "This hasn't been done yet.",
                    "label": 0
                },
                {
                    "sent": "There's no no good answer to that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so we already covered a little bit.",
                    "label": 0
                },
                {
                    "sent": "Or will I think Phil later on going to a lot deeper into machine translation, but just to give you the generality, general applicability and also another good excuse to introduce to you the next modifications to recurrent neural networks will talk a little bit about machine translation.",
                    "label": 0
                },
                {
                    "sent": "I won't go into all the details for traditional machine translation methods 'cause Phil covers those, but basically just to give you a high level overview, it's been a lot of.",
                    "label": 0
                },
                {
                    "sent": "Work in traditional machine translation of course, and most of these models are all of them basically required a lot of human feature engineering and there in the end are very complex systems and to me personally, the worst part is statically is that they often end up having many different and independent machine learning models.",
                    "label": 0
                },
                {
                    "sent": "So you train the language model in that separate from training your translation model, which might be separate from training your alignment model and things like that so.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What could we do to use recurrent neural networks to try to do machine translation?",
                    "label": 0
                },
                {
                    "sent": "I'll try to walk you sort of through the steps and we initially start with the simplest one is just a standard recurrent neural network, which won't be enough, and then we'll basically extend it and add more and more sort of modeling tricks to try to give you an intuition also of how you could go about improving your own neural network architectures.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the simplest model we could have is just we say here I stick a kiss to German.",
                    "label": 0
                },
                {
                    "sent": "Word an would be roughly translated as awesome sauce and basically just say we have our standard recurrent neural networks with one W matrix and we just tried to predict here.",
                    "label": 0
                },
                {
                    "sent": "Just as with the language modeling the actual words.",
                    "label": 0
                },
                {
                    "sent": "Unfortune",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lee, you know that will require that this single vector here captures the entire T of the phrase, and the longer the sentence, the harder that will be.",
                    "label": 0
                },
                {
                    "sent": "OK, assuming that we could do that, what would that model look like?",
                    "label": 0
                },
                {
                    "sent": "So basically will define our encoder here as just again we compute at each hidden state at timestep T, and I'll introduce you that five function Phi Ascentia Lee just says for every input here will have a separate matrix.",
                    "label": 0
                },
                {
                    "sent": "So whenever I have five off, you know one thing in two things, they're basically just added inside, and each one has its own W matrix of parameters just to simplify notation a little bit.",
                    "label": 0
                },
                {
                    "sent": "And we have the same decoder will just be basically taking in the last time step 'cause we don't have a word, so it's just five HT minus one the previous time step, and we'll have a softmax and in a perfect world, if recurrent neural networks would be awesome and simple out of the box, then all we would have to do is basically again minimize the cross entropy error.",
                    "label": 0
                },
                {
                    "sent": "And try to predict the translated words here one at a time.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unfortunately, it's not quite that simple.",
                    "label": 0
                },
                {
                    "sent": "Recurrent neural networks are very powerful, but not powerful enough to capture all these complex translation phenomena.",
                    "label": 0
                },
                {
                    "sent": "So let's go over several extensions of this model.",
                    "label": 0
                },
                {
                    "sent": "The first one is we could actually train a different recurrent neural network for encoding and one for decoding.",
                    "label": 0
                },
                {
                    "sent": "So just have different colors, basically have different sets of W, that's fair.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Straight forward.",
                    "label": 0
                },
                {
                    "sent": "The second one is that ideally we could actually compute every hidden state in the decoder, not just so the decoder now being defined as this part right here.",
                    "label": 0
                },
                {
                    "sent": "Now we can condition that not just on the previous single state, but on multiple other things, such as the last hidden state of the encoder.",
                    "label": 0
                },
                {
                    "sent": "So we'll define that here as C Being the last hidden state at capital time T of our encoding.",
                    "label": 0
                },
                {
                    "sent": "Just a source language, and now will basically condition the recurrent neural network that outputs the translation on, not just the previous time step, but also that final encoder.",
                    "label": 0
                },
                {
                    "sent": "As well as the hidden state and that sort of is also connected to the question that you asked about.",
                    "label": 0
                },
                {
                    "sent": "You want to have some connections that go back.",
                    "label": 0
                },
                {
                    "sent": "It does make sense to actually do that through the previous timesteps prediction to feed that into the next hidden state.",
                    "label": 0
                },
                {
                    "sent": "So in our fine notation here, we essentially just have our last hidden state T -- 1.",
                    "label": 0
                },
                {
                    "sent": "The final state from our encoder and the word that we just predicted in the previous time step.",
                    "label": 0
                },
                {
                    "sent": "During decoding.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, another figure that looks a lot more complex but is essentially has the same semantics to what we just described is right here, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, that word has to be 1 hot vector 'cause we will eventually want to say this is exactly this translation.",
                    "label": 0
                },
                {
                    "sent": "Not here's a word vector, and so there's actually you can think of it in some ways.",
                    "label": 0
                },
                {
                    "sent": "That way, right?",
                    "label": 0
                },
                {
                    "sent": "You say, here's a word vector.",
                    "label": 0
                },
                {
                    "sent": "Now try to find the closest word vectors to this word vector and then make that prediction.",
                    "label": 0
                },
                {
                    "sent": "But then you have to already predict the word vector and then also run like an inner product to find the closest one to that.",
                    "label": 0
                },
                {
                    "sent": "So in the end what most people do is just have predict the one hot vector.",
                    "label": 0
                },
                {
                    "sent": "So now this looks a lot more complex, but it's really the same thing, so that's something I'd like to have you be able to have as a takeaway message to try to sort of look through the different formalisms and different syntax, different visualizations to see something very similar.",
                    "label": 0
                },
                {
                    "sent": "So here we have the one hot vectors I guess, described a little more directly you have here your hidden states.",
                    "label": 0
                },
                {
                    "sent": "The probabilities that essentially from the word probabilities you have the sample and then that's the sample that you again modify your encoder with and here you condition all the hidden time steps on this C vector.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now here 3 simpler extensions.",
                    "label": 0
                },
                {
                    "sent": "The first one is we can train a stacked or deep recurrent neural network with multiple layers just like the one I introduced before.",
                    "label": 0
                },
                {
                    "sent": "So instead of having the encoder only have one layer, we can have it go very deep.",
                    "label": 0
                },
                {
                    "sent": "We can also potentially train the encoder bidirectionally, so going from left to right and right to left and then condition the decoder based on the last time steps and the first time step so to say.",
                    "label": 0
                },
                {
                    "sent": "And if we don't do that, an alternative and one that might actually help even with this, is to train the input sequence in reverse order for to make an easier optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So oftentimes we translate something if basically your languages have roughly a similar word order, you might want to say instead of translating ABC to X&Y, you translate CBA Jackson.",
                    "label": 0
                },
                {
                    "sent": "Why you're basically just shuffle towards and that way cousin when.",
                    "label": 0
                },
                {
                    "sent": "The translation is such that the word order is often is preserved, then a would try to be translated roughly to X and as you back propagate through your very deep network, the error that you get from X here will now be very far away and you are more likely to hit vanishing gradient problems.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you flip it.",
                    "label": 0
                },
                {
                    "sent": "Then the error from X needs to go right here to A and it's much easier for the model to optimize.",
                    "label": 0
                },
                {
                    "sent": "Does that make sense?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All right now, the main improvement, and that's what we'll cover the last.",
                    "label": 0
                },
                {
                    "sent": "Oh boy, we only have 3 more minutes, do we?",
                    "label": 0
                },
                {
                    "sent": "Maybe we should end right here and I'll just describe those tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Do you have anymore questions?",
                    "label": 0
                },
                {
                    "sent": "Oh half hour.",
                    "label": 0
                },
                {
                    "sent": "Oh boy, whoops.",
                    "label": 0
                },
                {
                    "sent": "I thought it's 1211 OK perfect alright excellent alright.",
                    "label": 0
                },
                {
                    "sent": "So I thought lunch was 12 alright sorry cool alright so.",
                    "label": 0
                },
                {
                    "sent": "No, no, it's fine.",
                    "label": 0
                },
                {
                    "sent": "This is perfect because there are lots of questions, probably around LCMS and so on.",
                    "label": 0
                },
                {
                    "sent": "Excellent alright great great alright so more complex.",
                    "label": 0
                },
                {
                    "sent": "Unit computations in recurrence so.",
                    "label": 0
                },
                {
                    "sent": "The first one actually introduced by somebody in the audience.",
                    "label": 0
                },
                {
                    "sent": "I don't see him right now are gated recurrent units.",
                    "label": 0
                },
                {
                    "sent": "In some ways, the next couple of slides will be a blast from the past, plus sort of state of the art novel stuff in the sense that Geo user just introduced last year.",
                    "label": 0
                },
                {
                    "sent": "But LCMS, which are actually in some ways an extension or more complex version of them were introduced many years ago, so I'll start with them.",
                    "label": 0
                },
                {
                    "sent": "I think it's sort of.",
                    "label": 0
                },
                {
                    "sent": "Pedagogically, better to start with gated recurrent units, the main idea is that we want to keep around memories to capture long distance dependencies in our current recurrent neural networks that we've covered so far, we really have this matrix multiplication that changes every single unit, every time at every time step, and that's sub optimal if maybe we want to just keep around some information and the result of that will also be that our error messages will flow at different strengths depending.",
                    "label": 0
                },
                {
                    "sent": "On the inputs.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's define gated recurrent units and I think if you ever wanted to implement a recurrent neural network for real task, it's very likely you you're going to end up using a Gru Ornellas team.",
                    "label": 0
                },
                {
                    "sent": "So it's a very important model.",
                    "label": 0
                },
                {
                    "sent": "So standard recurrent units had essentially just the simple addition of two matrix multiplications.",
                    "label": 0
                },
                {
                    "sent": "And that has the problem that no matter what the xinput is here will make a huge change to H at every given time step, right?",
                    "label": 0
                },
                {
                    "sent": "You will always basically sum up these two values.",
                    "label": 0
                },
                {
                    "sent": "Now the Gru computes its hidden state in a different, more complex way.",
                    "label": 0
                },
                {
                    "sent": "It essentially first computes an update gate which is just another kind of neural network layer with its own weights.",
                    "label": 0
                },
                {
                    "sent": "Based on the current input word vector and the hidden state.",
                    "label": 0
                },
                {
                    "sent": "So the first thing it will do is essentially just looks very similar to this, but it's not our final state, it's just our state ziti.",
                    "label": 0
                },
                {
                    "sent": "Now if the word vector dominates this, see T, for instance.",
                    "label": 0
                },
                {
                    "sent": "It might say turn this on or off entirely, right?",
                    "label": 0
                },
                {
                    "sent": "So ZTE could now be somewhere between zero and one.",
                    "label": 0
                },
                {
                    "sent": "We always use a Sigma unit here.",
                    "label": 0
                },
                {
                    "sent": "It's usually not something where we use a rectified linear or 10 H. This is Sigma for reasons that will become apparent in the next slide.",
                    "label": 0
                },
                {
                    "sent": "Now we will also compute a reset gate.",
                    "label": 0
                },
                {
                    "sent": "It's very similar, very similar to our recurrent neural network.",
                    "label": 0
                },
                {
                    "sent": "But again, with a different set of weights.",
                    "label": 0
                },
                {
                    "sent": "So now we have here our.",
                    "label": 0
                },
                {
                    "sent": "Update Gates Z with superscript Z.",
                    "label": 0
                },
                {
                    "sent": "Here for our W&U and we have a reset gates with superscript R for reset and those are both vector or presentations here too.",
                    "label": 0
                },
                {
                    "sent": "So before we compute that vector H as right here, we first compute these two intermediate vectors.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we will actually compute our memory content.",
                    "label": 0
                },
                {
                    "sent": "H tilled T here which is sort of the new memory content.",
                    "label": 0
                },
                {
                    "sent": "By having a 10 H layer and in this mu.",
                    "label": 0
                },
                {
                    "sent": "A notation here that essentially is an elementwise multiplication.",
                    "label": 0
                },
                {
                    "sent": "So this is also just a simple layer like this, but now having this elementwise multiplication here.",
                    "label": 0
                },
                {
                    "sent": "So now let's think about what would happen here.",
                    "label": 0
                },
                {
                    "sent": "Basically, if the reset gate unit, so one element of the R vector is 0, then this model can entirely ignore all the previous memory and only store the new word information.",
                    "label": 0
                },
                {
                    "sent": "So let's say for instance I tried to do sentiment analysis and I said a bunch of content words.",
                    "label": 0
                },
                {
                    "sent": "This movie that I saw yesterday with my whole family.",
                    "label": 0
                },
                {
                    "sent": "Blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "Nothing that matters for the sentiment and in the end I say sucked or was terrible.",
                    "label": 0
                },
                {
                    "sent": "Then I want that last word to essentially be able to overrule all the stuff that came before, because it doesn't matter to trying to classify the sentiment that word vector should at that point be much more important than all the previous sort of content.",
                    "label": 0
                },
                {
                    "sent": "Words that weren't.",
                    "label": 0
                },
                {
                    "sent": "That didn't have much sentiment.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's our new memory content, with an elementwise multiplication here.",
                    "label": 0
                },
                {
                    "sent": "Then our final memory, and this is the one that is.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Semantics sort of similar to the final one that we would have computed in a standard RNN.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Directly will be computed with this equation, where we now have our update gates and our update gates have an elementwise multiplication or 1 minus.",
                    "label": 0
                },
                {
                    "sent": "That, and we can essentially take only the previous time step.",
                    "label": 0
                },
                {
                    "sent": "And we're also able to essentially ignore what happened at this time step.",
                    "label": 0
                },
                {
                    "sent": "So maybe we already know.",
                    "label": 0
                },
                {
                    "sent": "This movie was bad.",
                    "label": 0
                },
                {
                    "sent": "Even though I watched it with my whole family on a weekend last year during Thanksgiving.",
                    "label": 0
                },
                {
                    "sent": "So basically none of this stuff matters, and this can essentially also allow you to ignore what's coming in.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm attempting here a clean illustration, though I think in the end the equations are often more useful for the intuition than illustrations.",
                    "label": 0
                },
                {
                    "sent": "So just to.",
                    "label": 0
                },
                {
                    "sent": "Try at least here we have these straight dotted lines.",
                    "label": 0
                },
                {
                    "sent": "Basically, pointing to connections as our gates or multiplicative interactions.",
                    "label": 0
                },
                {
                    "sent": "And we compute them from bottom up.",
                    "label": 0
                },
                {
                    "sent": "So here at current time Step XT that will get fed into our reset gate, which also gets an input from the previous final memory.",
                    "label": 0
                },
                {
                    "sent": "Hours ET.",
                    "label": 0
                },
                {
                    "sent": "This computed in a very similar way, but as ET Now influences how we compute this final one, whereas the reset gate influences how we compute our intermediate or reset memory.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Something better?",
                    "label": 0
                },
                {
                    "sent": "I think that's right, yes.",
                    "label": 0
                },
                {
                    "sent": "That's not a question yet.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "So I think I've seen this at XX is just a word embedding.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, HH is our memory that keeps.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So it keeps track of the entire T of the context before.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll actually show you I'll actually show you some embeddings of very deep hidden layers of LCMS for machine translation at the end of this talk, so maybe that will give you some intuition.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Will will come.",
                    "label": 0
                },
                {
                    "sent": "Will talk about this one in the LCM and then you'll see that all the ideas of bidirectionality and depth at each layer they will translate to the fancier versions of how to compute your hidden state as well.",
                    "label": 0
                },
                {
                    "sent": "So you can combine all the ideas of previous ones before that's kind of what I'm the one of the main messages I'm trying to give you here is these are all Lego blocks and you can in some ways the most basic new Lego Block is now elementwise multiplication's interactions, and you can now put that one.",
                    "label": 0
                },
                {
                    "sent": "Into lots of different other places.",
                    "label": 0
                },
                {
                    "sent": "If you have a good reason based on your intuition for them.",
                    "label": 0
                },
                {
                    "sent": "They're still there actually, so the interaction is important.",
                    "label": 0
                },
                {
                    "sent": "You do really need both of them, and in fact it makes sense to get even more different kinds of gates, which will describe in LCM in a second.",
                    "label": 0
                },
                {
                    "sent": "So maybe to give.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do some more intuition for the Gru.",
                    "label": 0
                },
                {
                    "sent": "So just walking through the equations.",
                    "label": 0
                },
                {
                    "sent": "In some ways it's intuitive, but it's good to make it explicit.",
                    "label": 0
                },
                {
                    "sent": "So if the reset is close to zero, you essentially ignore all your previous hidden states, right?",
                    "label": 0
                },
                {
                    "sent": "So your party here at zero you basically H tilt is just defined based on your current word vector.",
                    "label": 1
                },
                {
                    "sent": "So that allows the model to drop information that is irrelevant in the future.",
                    "label": 0
                },
                {
                    "sent": "Now so you can update your gate Z controls also or sorry the update gate Z controls basically how much of the past states should matter now.",
                    "label": 0
                },
                {
                    "sent": "So if UZ Gate is close to one then we can copy information in that unit essentially as is through many time steps.",
                    "label": 0
                },
                {
                    "sent": "So if ZTE here is 1 then all the H tilt here is 0 and that's why you have to have the sigmoid 2 so that that actually works out.",
                    "label": 1
                },
                {
                    "sent": "And now this would allow you to basically just copy over.",
                    "label": 0
                },
                {
                    "sent": "Tific fact so if I asked about John, for instance, and now it says, oh, John, maybe people are important because of word vectors are in that space in this kind of some intuition, it's not.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "I can't really guarantee you that this is what would happen, but at least that's the intuition of what could happen with these kind of more powerful units.",
                    "label": 0
                },
                {
                    "sent": "Let's say John, that the model realizes all the people names are in this part of the vector space.",
                    "label": 0
                },
                {
                    "sent": "Now what you essentially have each sigmoid single layer neural network, right?",
                    "label": 0
                },
                {
                    "sent": "You have a linear.",
                    "label": 0
                },
                {
                    "sent": "Decision boundary saying everything to this side is 1 or below.",
                    "label": 0
                },
                {
                    "sent": "Here .5 and now it could say because John was here.",
                    "label": 0
                },
                {
                    "sent": "I want to keep around a unit that stays one for a long time and basically doesn't get reset.",
                    "label": 0
                },
                {
                    "sent": "Right, and this is what the gates are essentially doing, their gating your memory to allow you to just copy over some facts that seem to be important.",
                    "label": 0
                },
                {
                    "sent": "And if you have a lot of them, then some things can be updated and some things can be kept around.",
                    "label": 0
                },
                {
                    "sent": "And now if you do the simple math here, if this is just one, then it's also very easy to back propagate through this and not have the vanishing gradient problem.",
                    "label": 0
                },
                {
                    "sent": "And that also makes them much easier to train than general recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "Alright, so yeah, that's another one.",
                    "label": 0
                },
                {
                    "sent": "Units with short-term dependencies often have their reset gates be very active.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some more intuition here.",
                    "label": 0
                },
                {
                    "sent": "We have an illustration to be honest.",
                    "label": 0
                },
                {
                    "sent": "I again don't think the illustrations are as helpful as just looking at the equations.",
                    "label": 0
                },
                {
                    "sent": "But in order for you to understand how to train that really the only new derivative that you have to use for backpropagation is the simple multiplication here, which we should all know and the rest is all the same chain rule that we do for general backpropagation.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that was the Gru there?",
                    "label": 0
                },
                {
                    "sent": "Yeah, let's answer some more questions 'cause the next one, Alice Sam will be even more complex, yeah?",
                    "label": 0
                },
                {
                    "sent": "As always.",
                    "label": 0
                },
                {
                    "sent": "For a long paragraph.",
                    "label": 0
                },
                {
                    "sent": "Right, so this model doesn't know really where you are in your paragraph of course.",
                    "label": 0
                },
                {
                    "sent": "So first and last for general sort of document level sentiment analysis wouldn't be captured per say in this, but it would realize that in the first couple of sentences you have strong sentiment words that can then activate the.",
                    "label": 0
                },
                {
                    "sent": "Gates and then those gates will actually stay active for awhile.",
                    "label": 0
                },
                {
                    "sent": "They will basically have their Z here.",
                    "label": 0
                },
                {
                    "sent": "Be close to 1 so that you can essentially copy over all the content stuff.",
                    "label": 0
                },
                {
                    "sent": "That doesn't matter as much for sentiment classification and then the at the end those up those gates will get updated again and may be overruled by the final conclusion of the sentiment or not.",
                    "label": 0
                },
                {
                    "sent": "So that would be the intuition here.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "There can be.",
                    "label": 0
                },
                {
                    "sent": "I don't know what the final implementation was for different people.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty simple change.",
                    "label": 0
                },
                {
                    "sent": "You just initialize them to 0 again.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I it's kind of hard to hard to say how they both matter for the overall performance of the model, but it's hard to say I don't have an exact number in mind of like this is like 2% accuracy or something like that.",
                    "label": 0
                },
                {
                    "sent": "Just one more question.",
                    "label": 0
                },
                {
                    "sent": "Sorry, say again.",
                    "label": 0
                },
                {
                    "sent": "Sorry if zed is equal to 1, then R doesn't even matter very much.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I mean in some ways you could say you could try to predict this before and say if it's like close enough to one you don't even need to compute the other one.",
                    "label": 0
                },
                {
                    "sent": "Is that kind of what you're saying?",
                    "label": 0
                },
                {
                    "sent": "But you could do that, but in the end it's often easier to make do the same.",
                    "label": 0
                },
                {
                    "sent": "Same computation multiple times in a very efficient way.",
                    "label": 0
                },
                {
                    "sent": "That is the same every time.",
                    "label": 0
                },
                {
                    "sent": "So you can very easily batch it.",
                    "label": 0
                },
                {
                    "sent": "You can very easily put things on a GPU and things like that rather than having like a lot of if else statements that will just kind of mess up your flow of your parameters to GPU and things like that.",
                    "label": 0
                },
                {
                    "sent": "So it's possible, but it's unlikely to do that well.",
                    "label": 0
                },
                {
                    "sent": "I actually implemented this myself a couple of times and you could take out certain things.",
                    "label": 0
                },
                {
                    "sent": "And try to then delete those rows in the matrix to only multiply a couple of the rows in the matrix and things like that.",
                    "label": 0
                },
                {
                    "sent": "But in the end all that logic were slower and deleting rows out of a matrix were slower than just multiplying the whole thing.",
                    "label": 0
                },
                {
                    "sent": "And then potentially ignoring it.",
                    "label": 0
                },
                {
                    "sent": "That's one part of the answer.",
                    "label": 0
                },
                {
                    "sent": "The other one is it's not that common.",
                    "label": 0
                },
                {
                    "sent": "That, or you know, does etzer.",
                    "label": 0
                },
                {
                    "sent": "Generally a real number between zero and one, so it's unlikely there, you know.",
                    "label": 0
                },
                {
                    "sent": "Like almost entirely one, so you might still want to allow a little bit of the updates.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So in general you're right.",
                    "label": 0
                },
                {
                    "sent": "These are these models are very hard to paralyse.",
                    "label": 0
                },
                {
                    "sent": "Why is this so?",
                    "label": 0
                },
                {
                    "sent": "In CNN for instance, all the filters that you compute are essentially independent of one another.",
                    "label": 0
                },
                {
                    "sent": "Until you do the next pooling operation across filters.",
                    "label": 0
                },
                {
                    "sent": "For instance here.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, you can't really compute any of the current time step parameters until you've computed all the previous time step parameters.",
                    "label": 0
                },
                {
                    "sent": "You can create one check entries matrix multiplication and be done with it.",
                    "label": 0
                },
                {
                    "sent": "There are some tricks that people have done where they say.",
                    "label": 0
                },
                {
                    "sent": "Maybe I will try to put more computation in a deep neural network at each time step that is independent and then have temporal interactions come only at the very last layer.",
                    "label": 0
                },
                {
                    "sent": "So intuitively for instance.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these kinds of models I might get rid of these temporal interactions and these temporal interactions, and I just have a very deep network down here.",
                    "label": 0
                },
                {
                    "sent": "And then I can basically paralyzed multiplication of all these units, and then only have this be the slow part, that I can't easily paralyze.",
                    "label": 0
                },
                {
                    "sent": "So that is 1 trick that folks at biter used.",
                    "label": 0
                },
                {
                    "sent": "I think for speech recognition.",
                    "label": 0
                },
                {
                    "sent": "But other than that?",
                    "label": 0
                },
                {
                    "sent": "There's no like sort of very simple obvious way to speed them up even more.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If just compute the last layer and then what?",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "In the end they in speech recognition still got very good performance out of that system, so they still had state of the art performance on it, so I wouldn't say it has to perform worse than that.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's look into LCM's and that's the last big part of this lecture.",
                    "label": 0
                },
                {
                    "sent": "So we can essentially make the units to compute the final hidden memory state even more complex, and it will look a little overwhelming, but it's essentially now the Lego pieces that we've covered before, and we just keep stacking them on top of each other and having more and more complex interactions.",
                    "label": 0
                },
                {
                    "sent": "So now at each time step, we essentially have an input gate.",
                    "label": 0
                },
                {
                    "sent": "That is somewhat similar to the reset gate, but now we have the input gate.",
                    "label": 1
                },
                {
                    "sent": "We have a forget gate.",
                    "label": 0
                },
                {
                    "sent": "That essentially, if the gate is zero, you would want to forget the past.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they forget gates are actually the memorize gates, so depending their different formulations of LCMS two so you have to be careful about how you look into the equations.",
                    "label": 0
                },
                {
                    "sent": "And then we also.",
                    "label": 0
                },
                {
                    "sent": "And this is a very new one.",
                    "label": 0
                },
                {
                    "sent": "Have an output gate.",
                    "label": 0
                },
                {
                    "sent": "So how much is the cell that you will actually compute exposed to the classifier?",
                    "label": 0
                },
                {
                    "sent": "You may actually now have things where you say this isn't very important, but not for the prediction at this time step, but maybe for a prediction at a later time step.",
                    "label": 0
                },
                {
                    "sent": "And then we have the new memory cell.",
                    "label": 1
                },
                {
                    "sent": "So here again, so far these are just basically standard recurrent types of units, right?",
                    "label": 0
                },
                {
                    "sent": "Very simple standard aren't end type stuff.",
                    "label": 0
                },
                {
                    "sent": "But now the main complexity comes in how we actually compute the final memory cell in the final hidden state.",
                    "label": 1
                },
                {
                    "sent": "So here the final memory cell is essentially the forget gate times the previous final memory cell, and this is again are elementwise interaction.",
                    "label": 0
                },
                {
                    "sent": "Plus our input gate with elementwise interaction of the new memory cell.",
                    "label": 0
                },
                {
                    "sent": "Somewhat intuitively, right, we have forgetting things about the past input, things about present, and then the final hidden state will actually be OT the output.",
                    "label": 0
                },
                {
                    "sent": "Elementwise Times 10 H of the final memory cell.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I think the illustrations are in many ways a little bit overwhelming and don't really.",
                    "label": 0
                },
                {
                    "sent": "I think, give you that much intuition, but here are three different ones.",
                    "label": 0
                },
                {
                    "sent": "Different attempts at giving you intuitions.",
                    "label": 0
                },
                {
                    "sent": "In some ways I like.",
                    "label": 0
                },
                {
                    "sent": "I like this one somewhat, but in general I don't.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't think they give that much intuition.",
                    "label": 0
                },
                {
                    "sent": "You basically have here your multiplicative interactions with your input gate.",
                    "label": 0
                },
                {
                    "sent": "You have your memory cell input, the forget gate can basically recur on itself, and then you have the output gate and so on.",
                    "label": 0
                },
                {
                    "sent": "But basically the main intuition here is that your memory cells can keep their information intact unless the input makes them forget it or overwrite it with new input.",
                    "label": 0
                },
                {
                    "sent": "So you can actually forget and or overwrite your input.",
                    "label": 0
                },
                {
                    "sent": "And the cell can decide to output this information or just store it for next for the next time steps.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, LCMS are currently very hip.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a great blast from the past, or introduced by your computer and one of his students will hide her.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "In the late early 90s.",
                    "label": 0
                },
                {
                    "sent": "97 late 90s.",
                    "label": 0
                },
                {
                    "sent": "And then yeah, they're basically right now.",
                    "label": 0
                },
                {
                    "sent": "In some ways, the default model that many people use for sequence labeling tasks are very, very powerful, especially when you stack them.",
                    "label": 0
                },
                {
                    "sent": "And now this is again the idea of taking the Lego pieces that we've had and basically adding making them even deeper, and because of all the weights that you now introduced for even a single time step, you want to have a lot of data to train this.",
                    "label": 0
                },
                {
                    "sent": "And where would you have more?",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "State out then in machine translation where you have very large parallel corpora.",
                    "label": 0
                },
                {
                    "sent": "So here are some numbers were not going to go into too many details here, but essentially the final sort of takeaway messages that they don't quite outperform the monster models with lots of different machine learning subcomponents quite yet, but they're getting very, very close, so here the best WMT result from 14 from last year is basically only half a blue point.",
                    "label": 0
                },
                {
                    "sent": "Better.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Depending on which datasets you look at now, there are some where they outperform them and you sort of, yeah.",
                    "label": 0
                },
                {
                    "sent": "Not on all of them, but on several of them.",
                    "label": 0
                },
                {
                    "sent": "So this one is actually this one is basically a deep LCM model, so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a very different model to the one that now is outperforming those, so that just the simple model that I just described where you have less teams you have the encoder, LCM decoder, LCM and then you describe it.",
                    "label": 0
                },
                {
                    "sent": "So basically similar to the very simple model.",
                    "label": 0
                },
                {
                    "sent": "I'll just go back really quick.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of model here.",
                    "label": 0
                },
                {
                    "sent": "Where you essentially connect your hidden states to the one, it's not exactly this, but very similar.",
                    "label": 0
                },
                {
                    "sent": "That model hasn't yet outperformed it, but then there are other models based on attention and so on that have that you should just mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to give you some intuition, an answer one of your questions once I can about the semantics that pop out of this.",
                    "label": 0
                },
                {
                    "sent": "So here you can look at PCA projected version of the vectors from the last time step after your trip train machine translation system and what's cool is it actually captures a lot of interesting semantics here.",
                    "label": 0
                },
                {
                    "sent": "So we have Marriott Miis, John or Mary is in love with John, or closer to one another than John at Myers.",
                    "label": 0
                },
                {
                    "sent": "Mary and John is in love with Mary, so you basically have here.",
                    "label": 0
                },
                {
                    "sent": "Staying active passive variant.",
                    "label": 0
                },
                {
                    "sent": "So I was given a card by her in the garden versus in the garden.",
                    "label": 0
                },
                {
                    "sent": "She gave me a card so ideally here the OR the cool thing that falls out of this is that active passive translations are actually in a similar part of the vector space.",
                    "label": 0
                },
                {
                    "sent": "So now the model can decide do I want to translate this in an active voice or passive voice and it actually captured that in variance in the vector space.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "It's a little tricky, so there are some some folks, so the identity initialized one actually also use Dennis, but it kind of depends on where you want to go eventually.",
                    "label": 0
                },
                {
                    "sent": "If you just want to practice and learn by yourself, then you can create a variety of different sort of synthetic task.",
                    "label": 0
                },
                {
                    "sent": "If you want to actually eventually.",
                    "label": 0
                },
                {
                    "sent": "Publish something real and have a real model.",
                    "label": 0
                },
                {
                    "sent": "See if your model is actually also implemented fast enough to work eventually on anything that's real language modeling, I think is probably the best one you have basically infinite amounts of data.",
                    "label": 0
                },
                {
                    "sent": "You just take all of Wikipedia, try to predict the next word.",
                    "label": 0
                },
                {
                    "sent": "So you can see how well you scale up overtime.",
                    "label": 0
                },
                {
                    "sent": "Perplexity numbers are pretty commonly described, for instance, for the Penn treebank, which is also publicly available data set for just the language modeling.",
                    "label": 0
                },
                {
                    "sent": "You know, kind of how good you should be for in terms of your perplexity for language modeling.",
                    "label": 0
                },
                {
                    "sent": "So my hunch is probably language modeling isn't easy to use.",
                    "label": 0
                },
                {
                    "sent": "Kind of good default task.",
                    "label": 0
                },
                {
                    "sent": "It's a little less real in the sense that character level language models haven't gotten like state of the art performance on many things yet, but like yeah.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I guess there's a difference between wanting to learn and eventually trying to get famous with that project.",
                    "label": 0
                },
                {
                    "sent": "So it will be hard to I. I actually think there's something to be said about words as a unit that makes sense, so that's an interesting discussion, and in some ways you know orthography can be different and people can understand words but not spelled incorrectly.",
                    "label": 0
                },
                {
                    "sent": "Right so, especially if you look at Germans, friends with compound nouns, they can get very long if you have to have a specific word vector for each compound noun, of which there are way way more than are often used and you can very creatively put them together in new ways.",
                    "label": 0
                },
                {
                    "sent": "It's nicer to actually have a character like character one.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right basically I described how we can have these Lego pieces and put them together in new ways, and this is a very recent work also.",
                    "label": 0
                },
                {
                    "sent": "Where essentially you can combine these ideas of having deep general aren't ends, but you can also assume insight.",
                    "label": 0
                },
                {
                    "sent": "Each of these could actually also be in Ellis Tymora, Gru and now you can instead of having them only go up and to the left, you can have them either both go sideways, again, bidirectional, that's sort of an extra Lego piece.",
                    "label": 0
                },
                {
                    "sent": "Just flip all your for loops to go backwards, and then combine things.",
                    "label": 0
                },
                {
                    "sent": "Or you can add basically gates from higher levels, or basically all the different hidden.",
                    "label": 0
                },
                {
                    "sent": "Levels to all other levels and have basically gates say sometimes I want the most abstract or highest layer actually modify the lower layer at the next step so things can get more and more complex, and I think right now we're in this explosion of creativity where people are like, oh, there's a new Lego Block.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to combine it with a lot of other different things because I've played around with all these Lego blocks and I gain intuition about how well they often work for different kinds of problems.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with that I want to summarize.",
                    "label": 0
                },
                {
                    "sent": "Basically, I hope I could show you and give you some more detail.",
                    "label": 0
                },
                {
                    "sent": "It's an in depth tricks for recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "There a very very powerful model for deep learning, I think gated recurrent units or even better than the standard recurrent neural network that just updates everything at every time step.",
                    "label": 1
                },
                {
                    "sent": "And LCMS maybe even better, though the jury is still out there.",
                    "label": 0
                },
                {
                    "sent": "Some experiments that actually say oh Gee, are using some experiments are better than LCMS.",
                    "label": 1
                },
                {
                    "sent": "I think we don't know for sure yet, so if you want to learn, I would encourage you to actually implement both.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of ongoing work right now.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty sure next year this lecture will have a couple of more sort of state of the art results of these models.",
                    "label": 0
                },
                {
                    "sent": "Now tomorrow I will actually put all these things together into what I think is a really cool model that can solve a lot of different tasks in natural language processing anything from logical reasoning to sequence labeling to classification to translation and so on.",
                    "label": 0
                },
                {
                    "sent": "So we'll use all the Lego pieces we.",
                    "label": 0
                },
                {
                    "sent": "Describe today and put them together in a new way.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}