{
    "id": "vehdpqyviq3jdcojlqrakpxtpwa2xpzm",
    "title": "Probabilistic Topic Modeling in Multilingual Settings: A Short Overview of Its Methodology and Applications",
    "info": {
        "author": [
            "Ivan Vuli\u0107, Department of Computer Science, KU Leuven"
        ],
        "published": "Jan. 11, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning->Dirichlet Processes"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_vulic_topic_modeling/",
    "segmentation": [
        [
            "So I'll try to give a really short overview of probabilistic topic modeling modeling with settings, and I'll have exactly 10 minutes or maybe 12 minutes to do that, which is kind of.",
            "I know too little because I could talk about this for hours, but maybe we could take those hours off."
        ],
        [
            "Line.",
            "So first I'm going to start with a really short overview of probabilistic topic modeling in monolingual settings, where actually that's where it all started and it started already like a few years ago, 15 to 10 years ago, and the idea is kind of.",
            "Quite intuitive, so you have those generated probabilistic models of a corpus.",
            "You have a label layer of variables called topics that somehow managed to generate the observed data, or actually text collections.",
            "So the basic idea is that each document in a document collection is represented as a mixture of latent variables or latent topics, and each topic is given in vocabulary words as a probability distribution over these vocabulary words, and two most known models.",
            "Probabilistic models in multiple settings are probabilistic latent semantic analysis and latent Irish let location and they are pretty similar.",
            "Actually, 'cause you can see that LDA is pretty much PSA just with place priors on these distributions, which are distributions of topics over documents and distribution of words over topics."
        ],
        [
            "And.",
            "So basically what do these topics do?",
            "So they are here like place as latent layer of variables and?",
            "You observe arts in documents and.",
            "But you can actually say that.",
            "Instead of just observing that text collection that each topic actually somehow generated a word at each single position in a document and fitting a model is actually trying to find the best set of those latent topics that somehow explain the observed data.",
            "And the problem with another problem.",
            "But PSA and LDA were actually built for monolingual."
        ],
        [
            "Settings and for monolingual text collections, here is one really good example that I stole or maybe borrowed from recent lies paper from communications of ACM that was published here, which is kind of more like popular view on topic models, and you can see that.",
            "Disting this chart bar actually represents the distribution of topics in a document, and then for each single word position in the document, you actually choose appropriate topic, which is like colored circles here and then this.",
            "This topic actually generates words in that position, so you can see here.",
            "This is just a toy data, so you have four topics he ran, for example, the yellow topic concerns genes and DNA and genetic stuff and all that.",
            "Then you have this.",
            "Another topic that concerns like biological terms like life, evolution, Organism and."
        ],
        [
            "Yeah, but what happens when we actually move to multilingual settings and there we have different types of corpora like the most.",
            "The best corpora that you have and the translator is actually built or and or public or private.",
            "For example like Europol, or hands are corpora that were built in countries that are bilingual, or three lingual.",
            "And but the problem is that usually they are just.",
            "Found for really restricted domains like Parliament proceedings and they lack for many language pairs.",
            "So probably you won't have.",
            "I know parallel corpus for Marathi and Hungarian or something like that.",
            "And then you have another type of corpus which is compatible corpus that actually.",
            "Of has different subtypes based on the level of comparability and some for example.",
            "News stories in different languages are discussed.",
            "Similar events or subjects are compareable because they use like vocabulary in different languages that somehow can be connected to the same events or things.",
            "So basically, for example, those news stories or if you take Wikipedia articles as well, discuss similar subjects.",
            "They're kind of team aligned text collections because they focus on similar themes and on the web.",
            "Usually you have such corpora available in abundance.",
            "We can just mind the web, try to find new sites that cover recent events."
        ],
        [
            "Until later.",
            "And early approaches in topic modeling try to somehow reach that multilinguality in different ways and 1st approaches.",
            "Tried to just merging with get concatenate different documents in different languages that discuss similar things and then try to.",
            "Use some logic break model like latent semantic indexing or trying to train those smaller lingual models on such data.",
            "But it didn't work quite well and recently there were several models that were proposed for really like multilingual settings that try to mine a true cross lingual topical space.",
            "And these are some of the models and that actually are just extensions of those monolingual models to multilingual settings.",
            "For example, bilingual LDA that works with two languages and that.",
            "Is also easy to extend for three or four languages, and then it's called pulling.",
            "Will LDA or.",
            "Muto, or what is?",
            "This is an abbreviation of multilingual topic model from Boyd Graber an Bly.",
            "There also is a variant of LDA as it is joint LDA and this is a variant of multilingual PSA model and all these models actually they they vary in how they actually approach the.",
            "The modeling thing, but basically all the."
        ],
        [
            "All have in common that.",
            "They had in final as a final set of distributions output to these two sets of distribution, so they might actually also model something extra so that if you have a more complex model, you could also model like sentiment distribution over copper or learn something extra.",
            "But this is like a minimal thing that all these models could learn, and the first thing is.",
            "Those per document topic distributions which actually say which topic is important and at what degree for each document from the collection.",
            "And another set of distributions are those language specific topic representations or per topic.",
            "For distributions which say which words in which vocabularies are important to describe that language independent concept which is cross lingual topics.",
            "So you might observe those cross lingual topics as some sort of language independent concepts which are not dependent on the language that you have involved training and then each language can actually.",
            "Use its own words from the vocabulary to really describe those topics or those concepts."
        ],
        [
            "So for example, here is an example of.",
            "Few cross lingual topics that we mined from Wikipedia using billing.",
            "Will LDA and you can see like that these desirable properties are quite satisfied so words in the same language that discuss this topic on.",
            "More like motor sport or cars, whatever are quite semantic related to have words like engine recall, car speed, constructor and.",
            "Of in French and the same thing in English, so you can observe both intra semantic coherence, which basically means that words in the same language are semantically related and words across languages.",
            "That's Inter semantic.",
            "Coherence is also satisfied.",
            "Barrels close, semantic related, and third thing that really desirable that similar documents have similar representations meaning by means of those cross lingual topics, both monolingual across languages.",
            "So no matter what language they actually used to describe their content.",
            "They should have similar per documents topic distribution so that that mixture of language independent concepts should be similar in those."
        ],
        [
            "Ocuments and here is 1 example where you is kind of similar to that modeling world example.",
            "So here you have.",
            "Like topics, actually the same language independent concept.",
            "So like Red Topic, Green topic, blue topic yellow topic that are given as probability distributions over words in two different vocabularies.",
            "So this one is Italian and this one is English and then you can actually follow the same paradigm in generating the contents.",
            "So these two documents are kind of similar because they're actually.",
            "Just two Wikipedia articles that discuss the subject of information retrieval and.",
            "Then, based on these per document topic distributions, you can you can take the topic that generated this word at the position one and then you can actually just follow that generating process and generate the entire text in both languages."
        ],
        [
            "And then when you have those two sets of distribution, you can actually do a lot of stuff with that, because they often are really good framework to model and to actually solve certain cross lingual tasks such as crossing news clustering that we did, and document classification or semantic word similarity or information retrieval.",
            "Or if you wanted to try it on your own task, you are free to do that.",
            "Actually the methodology stages."
        ],
        [
            "I'll just like briefly browse through these tags that we already accomplished or tried, so first one is trying to.",
            "Cluster news given in different languages, mind from the web based on the event they discuss and trying to use those per document topic distributions to achieve that.",
            "So for example, if you have words in Dutch, French and English and you try to actually map try to cluster all these documents discussing Obama's visit to.",
            "To the Mexican Bay coast."
        ],
        [
            "So how to do that?",
            "Actually just using those per document topic distributions.",
            "So 2 news stories are considered similar if they.",
            "An arm or most likely discussing the same event if there per document topic distributions are similar.",
            "So and then you can actually apply some sort of clustering algorithm and certain similarity measures to learn the real clusters based on these Word document topic distribution.",
            "So you can see like.",
            "I suggest the example of course is that documents with similar distributions should be grouped together.",
            "Of course, using."
        ],
        [
            "Topic models for this task is just one aspect, because you can also use.",
            "I know named entities shared across across languages in different documents to actually trying to boost your model towards higher scores.",
            "But topics are also useful as we prove in our paper.",
            "Another thing that you could.",
            "Try your cross multilingual topic models on is for example cross single document classification where you in this particular setting you already.",
            "You know for example labels in English which is like well resourced language and you are trying to based on these per document topic distributions in English and per document distributions in for some documents in another language.",
            "Learn correct labels for those documents which you don't know somehow propagative labels."
        ],
        [
            "To another language.",
            "So this is the setting you have labeled documents in the source language being English unlabeled documents in the target language being something else, and you have to classify those unlabeled documents.",
            "And if you just use words as features for classification for work, because you're actually dealing with two different languages.",
            "So the idea is to take advantage of your shared cross lingual topical space.",
            "So first thing is that you can learn or train the multilingual topic model on on the general multilingual corpus, for example.",
            "On large set of Wikipedia articles on a large set of news stories that you mind from the web, then you have to infer that model on both labeled documents and unlabeled documents to learn their per document topic distributions.",
            "And then you use those per document topic distributions as features for your classifier.",
            "You just.",
            "Use the classifier, for example SVM or something else and try to classify those documents together and then based on your classification you can assign those or propagate those labels from source documents to target documents."
        ],
        [
            "So third application is trying to mind semantically similar words across languages and there we use this other set of distributions so we're not operating with per document distributions anymore, but we use those per topic word distributions.",
            "Because your models should ideally group similar words in different languages together to describe the same language independent concepts, and we propose several methods that actually tried to try to mine the knowledge from those per topic word distributions.",
            "For instance, you can actually represent each word in both languages in the shared semantic space that is spanned by those cross lingual topics and then use similarity metric.",
            "Being cool book library.",
            "Jensen, Shannon or Cosine, whatever you want and then make rank lists of semantically similar word candidates.",
            "For example, this is 1 example where you can see for example that in this first column, even if you don't find the correct translation of the word romance in Italian that a lot of these words is actually close to semantic related to the given word.",
            "And here you have correct translation lower in the list.",
            "But all those words are also closely semantic related to the given word.",
            "And here you have the real translation candidate.",
            "Put the first translation, connect in the lists and we actually used these models to mine a small billing lexicon out of compatible data."
        ],
        [
            "And the 4th and the most complex application is cross lingual information retrieval, which deals with retreiving documents from the language at the first from your query language.",
            "And here like you have a target collection and each target document is presented as a mixture over those crossing with topics given.",
            "Again those per document distributions.",
            "And you also have those values that actually are from your per topic word distributions.",
            "And then you can actually model."
        ],
        [
            "The.",
            "Some sort of language modeling framework for cross lingual information retrieval.",
            "So going from documents.",
            "To topics and then those topics actually generate query words and.",
            "In that way you're trying to connect the semantics of the query with the semantics of each document, and you do the ranking as it is done in."
        ],
        [
            "Information retrieval and all that, and here is that procedure.",
            "Like put in formulas.",
            "So first you train your model on some sort of multilingual general corpuses Wikipedia.",
            "Then you learn when you fear it on your target collection to learn those per document topic distributions.",
            "And then you go over over each word in a query, compute those cores.",
            "And then you just do simple.",
            "This is like the core model that uses topic models and 'cause language modeling framework for information retrieval gives you an opportunity to actually build many complex models based on different evidence that you have.",
            "So this is for example.",
            "This is topic representation.",
            "But you can add representation that comes from documents directly and like you can use words that you mind from dictionary as well as additional source information."
        ],
        [
            "And all that is in our papers so.",
            "I think that multilingual probabilistic topic modeling our servers are very interesting concept.",
            "Maybe audience will disagree.",
            "I don't know because it has a really nice theoretical explanation and interpretation as well.",
            "It has and we showed that it has potential in various cross lingual applications.",
            "And a good thing is that those models are actually could be trained and they model uncertainty from non parallel data in a really good way and they provide a way to represent documents in a uniform way using those language independent constants or cross lingual topics.",
            "And it's easy to use the knowledge from those topics in probabilistic frameworks for different single tasks and hopefully more models that catch fine grained, finer grained redundancies from.",
            "Data and more applications will come in the future.",
            "That's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll try to give a really short overview of probabilistic topic modeling modeling with settings, and I'll have exactly 10 minutes or maybe 12 minutes to do that, which is kind of.",
                    "label": 0
                },
                {
                    "sent": "I know too little because I could talk about this for hours, but maybe we could take those hours off.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Line.",
                    "label": 0
                },
                {
                    "sent": "So first I'm going to start with a really short overview of probabilistic topic modeling in monolingual settings, where actually that's where it all started and it started already like a few years ago, 15 to 10 years ago, and the idea is kind of.",
                    "label": 0
                },
                {
                    "sent": "Quite intuitive, so you have those generated probabilistic models of a corpus.",
                    "label": 1
                },
                {
                    "sent": "You have a label layer of variables called topics that somehow managed to generate the observed data, or actually text collections.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that each document in a document collection is represented as a mixture of latent variables or latent topics, and each topic is given in vocabulary words as a probability distribution over these vocabulary words, and two most known models.",
                    "label": 1
                },
                {
                    "sent": "Probabilistic models in multiple settings are probabilistic latent semantic analysis and latent Irish let location and they are pretty similar.",
                    "label": 0
                },
                {
                    "sent": "Actually, 'cause you can see that LDA is pretty much PSA just with place priors on these distributions, which are distributions of topics over documents and distribution of words over topics.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So basically what do these topics do?",
                    "label": 0
                },
                {
                    "sent": "So they are here like place as latent layer of variables and?",
                    "label": 0
                },
                {
                    "sent": "You observe arts in documents and.",
                    "label": 0
                },
                {
                    "sent": "But you can actually say that.",
                    "label": 0
                },
                {
                    "sent": "Instead of just observing that text collection that each topic actually somehow generated a word at each single position in a document and fitting a model is actually trying to find the best set of those latent topics that somehow explain the observed data.",
                    "label": 1
                },
                {
                    "sent": "And the problem with another problem.",
                    "label": 1
                },
                {
                    "sent": "But PSA and LDA were actually built for monolingual.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Settings and for monolingual text collections, here is one really good example that I stole or maybe borrowed from recent lies paper from communications of ACM that was published here, which is kind of more like popular view on topic models, and you can see that.",
                    "label": 0
                },
                {
                    "sent": "Disting this chart bar actually represents the distribution of topics in a document, and then for each single word position in the document, you actually choose appropriate topic, which is like colored circles here and then this.",
                    "label": 0
                },
                {
                    "sent": "This topic actually generates words in that position, so you can see here.",
                    "label": 0
                },
                {
                    "sent": "This is just a toy data, so you have four topics he ran, for example, the yellow topic concerns genes and DNA and genetic stuff and all that.",
                    "label": 0
                },
                {
                    "sent": "Then you have this.",
                    "label": 0
                },
                {
                    "sent": "Another topic that concerns like biological terms like life, evolution, Organism and.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, but what happens when we actually move to multilingual settings and there we have different types of corpora like the most.",
                    "label": 0
                },
                {
                    "sent": "The best corpora that you have and the translator is actually built or and or public or private.",
                    "label": 0
                },
                {
                    "sent": "For example like Europol, or hands are corpora that were built in countries that are bilingual, or three lingual.",
                    "label": 0
                },
                {
                    "sent": "And but the problem is that usually they are just.",
                    "label": 1
                },
                {
                    "sent": "Found for really restricted domains like Parliament proceedings and they lack for many language pairs.",
                    "label": 1
                },
                {
                    "sent": "So probably you won't have.",
                    "label": 0
                },
                {
                    "sent": "I know parallel corpus for Marathi and Hungarian or something like that.",
                    "label": 1
                },
                {
                    "sent": "And then you have another type of corpus which is compatible corpus that actually.",
                    "label": 0
                },
                {
                    "sent": "Of has different subtypes based on the level of comparability and some for example.",
                    "label": 0
                },
                {
                    "sent": "News stories in different languages are discussed.",
                    "label": 0
                },
                {
                    "sent": "Similar events or subjects are compareable because they use like vocabulary in different languages that somehow can be connected to the same events or things.",
                    "label": 1
                },
                {
                    "sent": "So basically, for example, those news stories or if you take Wikipedia articles as well, discuss similar subjects.",
                    "label": 0
                },
                {
                    "sent": "They're kind of team aligned text collections because they focus on similar themes and on the web.",
                    "label": 1
                },
                {
                    "sent": "Usually you have such corpora available in abundance.",
                    "label": 0
                },
                {
                    "sent": "We can just mind the web, try to find new sites that cover recent events.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until later.",
                    "label": 0
                },
                {
                    "sent": "And early approaches in topic modeling try to somehow reach that multilinguality in different ways and 1st approaches.",
                    "label": 0
                },
                {
                    "sent": "Tried to just merging with get concatenate different documents in different languages that discuss similar things and then try to.",
                    "label": 0
                },
                {
                    "sent": "Use some logic break model like latent semantic indexing or trying to train those smaller lingual models on such data.",
                    "label": 0
                },
                {
                    "sent": "But it didn't work quite well and recently there were several models that were proposed for really like multilingual settings that try to mine a true cross lingual topical space.",
                    "label": 0
                },
                {
                    "sent": "And these are some of the models and that actually are just extensions of those monolingual models to multilingual settings.",
                    "label": 0
                },
                {
                    "sent": "For example, bilingual LDA that works with two languages and that.",
                    "label": 0
                },
                {
                    "sent": "Is also easy to extend for three or four languages, and then it's called pulling.",
                    "label": 0
                },
                {
                    "sent": "Will LDA or.",
                    "label": 0
                },
                {
                    "sent": "Muto, or what is?",
                    "label": 0
                },
                {
                    "sent": "This is an abbreviation of multilingual topic model from Boyd Graber an Bly.",
                    "label": 0
                },
                {
                    "sent": "There also is a variant of LDA as it is joint LDA and this is a variant of multilingual PSA model and all these models actually they they vary in how they actually approach the.",
                    "label": 0
                },
                {
                    "sent": "The modeling thing, but basically all the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All have in common that.",
                    "label": 0
                },
                {
                    "sent": "They had in final as a final set of distributions output to these two sets of distribution, so they might actually also model something extra so that if you have a more complex model, you could also model like sentiment distribution over copper or learn something extra.",
                    "label": 0
                },
                {
                    "sent": "But this is like a minimal thing that all these models could learn, and the first thing is.",
                    "label": 1
                },
                {
                    "sent": "Those per document topic distributions which actually say which topic is important and at what degree for each document from the collection.",
                    "label": 1
                },
                {
                    "sent": "And another set of distributions are those language specific topic representations or per topic.",
                    "label": 0
                },
                {
                    "sent": "For distributions which say which words in which vocabularies are important to describe that language independent concept which is cross lingual topics.",
                    "label": 0
                },
                {
                    "sent": "So you might observe those cross lingual topics as some sort of language independent concepts which are not dependent on the language that you have involved training and then each language can actually.",
                    "label": 0
                },
                {
                    "sent": "Use its own words from the vocabulary to really describe those topics or those concepts.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, here is an example of.",
                    "label": 0
                },
                {
                    "sent": "Few cross lingual topics that we mined from Wikipedia using billing.",
                    "label": 0
                },
                {
                    "sent": "Will LDA and you can see like that these desirable properties are quite satisfied so words in the same language that discuss this topic on.",
                    "label": 0
                },
                {
                    "sent": "More like motor sport or cars, whatever are quite semantic related to have words like engine recall, car speed, constructor and.",
                    "label": 0
                },
                {
                    "sent": "Of in French and the same thing in English, so you can observe both intra semantic coherence, which basically means that words in the same language are semantically related and words across languages.",
                    "label": 0
                },
                {
                    "sent": "That's Inter semantic.",
                    "label": 0
                },
                {
                    "sent": "Coherence is also satisfied.",
                    "label": 0
                },
                {
                    "sent": "Barrels close, semantic related, and third thing that really desirable that similar documents have similar representations meaning by means of those cross lingual topics, both monolingual across languages.",
                    "label": 1
                },
                {
                    "sent": "So no matter what language they actually used to describe their content.",
                    "label": 0
                },
                {
                    "sent": "They should have similar per documents topic distribution so that that mixture of language independent concepts should be similar in those.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocuments and here is 1 example where you is kind of similar to that modeling world example.",
                    "label": 0
                },
                {
                    "sent": "So here you have.",
                    "label": 0
                },
                {
                    "sent": "Like topics, actually the same language independent concept.",
                    "label": 0
                },
                {
                    "sent": "So like Red Topic, Green topic, blue topic yellow topic that are given as probability distributions over words in two different vocabularies.",
                    "label": 0
                },
                {
                    "sent": "So this one is Italian and this one is English and then you can actually follow the same paradigm in generating the contents.",
                    "label": 0
                },
                {
                    "sent": "So these two documents are kind of similar because they're actually.",
                    "label": 0
                },
                {
                    "sent": "Just two Wikipedia articles that discuss the subject of information retrieval and.",
                    "label": 0
                },
                {
                    "sent": "Then, based on these per document topic distributions, you can you can take the topic that generated this word at the position one and then you can actually just follow that generating process and generate the entire text in both languages.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then when you have those two sets of distribution, you can actually do a lot of stuff with that, because they often are really good framework to model and to actually solve certain cross lingual tasks such as crossing news clustering that we did, and document classification or semantic word similarity or information retrieval.",
                    "label": 1
                },
                {
                    "sent": "Or if you wanted to try it on your own task, you are free to do that.",
                    "label": 0
                },
                {
                    "sent": "Actually the methodology stages.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll just like briefly browse through these tags that we already accomplished or tried, so first one is trying to.",
                    "label": 0
                },
                {
                    "sent": "Cluster news given in different languages, mind from the web based on the event they discuss and trying to use those per document topic distributions to achieve that.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have words in Dutch, French and English and you try to actually map try to cluster all these documents discussing Obama's visit to.",
                    "label": 0
                },
                {
                    "sent": "To the Mexican Bay coast.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how to do that?",
                    "label": 0
                },
                {
                    "sent": "Actually just using those per document topic distributions.",
                    "label": 1
                },
                {
                    "sent": "So 2 news stories are considered similar if they.",
                    "label": 0
                },
                {
                    "sent": "An arm or most likely discussing the same event if there per document topic distributions are similar.",
                    "label": 0
                },
                {
                    "sent": "So and then you can actually apply some sort of clustering algorithm and certain similarity measures to learn the real clusters based on these Word document topic distribution.",
                    "label": 0
                },
                {
                    "sent": "So you can see like.",
                    "label": 0
                },
                {
                    "sent": "I suggest the example of course is that documents with similar distributions should be grouped together.",
                    "label": 1
                },
                {
                    "sent": "Of course, using.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topic models for this task is just one aspect, because you can also use.",
                    "label": 0
                },
                {
                    "sent": "I know named entities shared across across languages in different documents to actually trying to boost your model towards higher scores.",
                    "label": 0
                },
                {
                    "sent": "But topics are also useful as we prove in our paper.",
                    "label": 0
                },
                {
                    "sent": "Another thing that you could.",
                    "label": 0
                },
                {
                    "sent": "Try your cross multilingual topic models on is for example cross single document classification where you in this particular setting you already.",
                    "label": 1
                },
                {
                    "sent": "You know for example labels in English which is like well resourced language and you are trying to based on these per document topic distributions in English and per document distributions in for some documents in another language.",
                    "label": 0
                },
                {
                    "sent": "Learn correct labels for those documents which you don't know somehow propagative labels.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To another language.",
                    "label": 0
                },
                {
                    "sent": "So this is the setting you have labeled documents in the source language being English unlabeled documents in the target language being something else, and you have to classify those unlabeled documents.",
                    "label": 1
                },
                {
                    "sent": "And if you just use words as features for classification for work, because you're actually dealing with two different languages.",
                    "label": 1
                },
                {
                    "sent": "So the idea is to take advantage of your shared cross lingual topical space.",
                    "label": 1
                },
                {
                    "sent": "So first thing is that you can learn or train the multilingual topic model on on the general multilingual corpus, for example.",
                    "label": 0
                },
                {
                    "sent": "On large set of Wikipedia articles on a large set of news stories that you mind from the web, then you have to infer that model on both labeled documents and unlabeled documents to learn their per document topic distributions.",
                    "label": 0
                },
                {
                    "sent": "And then you use those per document topic distributions as features for your classifier.",
                    "label": 0
                },
                {
                    "sent": "You just.",
                    "label": 0
                },
                {
                    "sent": "Use the classifier, for example SVM or something else and try to classify those documents together and then based on your classification you can assign those or propagate those labels from source documents to target documents.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So third application is trying to mind semantically similar words across languages and there we use this other set of distributions so we're not operating with per document distributions anymore, but we use those per topic word distributions.",
                    "label": 0
                },
                {
                    "sent": "Because your models should ideally group similar words in different languages together to describe the same language independent concepts, and we propose several methods that actually tried to try to mine the knowledge from those per topic word distributions.",
                    "label": 1
                },
                {
                    "sent": "For instance, you can actually represent each word in both languages in the shared semantic space that is spanned by those cross lingual topics and then use similarity metric.",
                    "label": 0
                },
                {
                    "sent": "Being cool book library.",
                    "label": 0
                },
                {
                    "sent": "Jensen, Shannon or Cosine, whatever you want and then make rank lists of semantically similar word candidates.",
                    "label": 0
                },
                {
                    "sent": "For example, this is 1 example where you can see for example that in this first column, even if you don't find the correct translation of the word romance in Italian that a lot of these words is actually close to semantic related to the given word.",
                    "label": 0
                },
                {
                    "sent": "And here you have correct translation lower in the list.",
                    "label": 0
                },
                {
                    "sent": "But all those words are also closely semantic related to the given word.",
                    "label": 0
                },
                {
                    "sent": "And here you have the real translation candidate.",
                    "label": 0
                },
                {
                    "sent": "Put the first translation, connect in the lists and we actually used these models to mine a small billing lexicon out of compatible data.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the 4th and the most complex application is cross lingual information retrieval, which deals with retreiving documents from the language at the first from your query language.",
                    "label": 1
                },
                {
                    "sent": "And here like you have a target collection and each target document is presented as a mixture over those crossing with topics given.",
                    "label": 0
                },
                {
                    "sent": "Again those per document distributions.",
                    "label": 0
                },
                {
                    "sent": "And you also have those values that actually are from your per topic word distributions.",
                    "label": 0
                },
                {
                    "sent": "And then you can actually model.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Some sort of language modeling framework for cross lingual information retrieval.",
                    "label": 0
                },
                {
                    "sent": "So going from documents.",
                    "label": 0
                },
                {
                    "sent": "To topics and then those topics actually generate query words and.",
                    "label": 0
                },
                {
                    "sent": "In that way you're trying to connect the semantics of the query with the semantics of each document, and you do the ranking as it is done in.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Information retrieval and all that, and here is that procedure.",
                    "label": 1
                },
                {
                    "sent": "Like put in formulas.",
                    "label": 0
                },
                {
                    "sent": "So first you train your model on some sort of multilingual general corpuses Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Then you learn when you fear it on your target collection to learn those per document topic distributions.",
                    "label": 0
                },
                {
                    "sent": "And then you go over over each word in a query, compute those cores.",
                    "label": 0
                },
                {
                    "sent": "And then you just do simple.",
                    "label": 0
                },
                {
                    "sent": "This is like the core model that uses topic models and 'cause language modeling framework for information retrieval gives you an opportunity to actually build many complex models based on different evidence that you have.",
                    "label": 0
                },
                {
                    "sent": "So this is for example.",
                    "label": 0
                },
                {
                    "sent": "This is topic representation.",
                    "label": 0
                },
                {
                    "sent": "But you can add representation that comes from documents directly and like you can use words that you mind from dictionary as well as additional source information.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And all that is in our papers so.",
                    "label": 0
                },
                {
                    "sent": "I think that multilingual probabilistic topic modeling our servers are very interesting concept.",
                    "label": 1
                },
                {
                    "sent": "Maybe audience will disagree.",
                    "label": 0
                },
                {
                    "sent": "I don't know because it has a really nice theoretical explanation and interpretation as well.",
                    "label": 1
                },
                {
                    "sent": "It has and we showed that it has potential in various cross lingual applications.",
                    "label": 1
                },
                {
                    "sent": "And a good thing is that those models are actually could be trained and they model uncertainty from non parallel data in a really good way and they provide a way to represent documents in a uniform way using those language independent constants or cross lingual topics.",
                    "label": 0
                },
                {
                    "sent": "And it's easy to use the knowledge from those topics in probabilistic frameworks for different single tasks and hopefully more models that catch fine grained, finer grained redundancies from.",
                    "label": 1
                },
                {
                    "sent": "Data and more applications will come in the future.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                }
            ]
        }
    }
}