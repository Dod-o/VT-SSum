{
    "id": "rbmn5kw7vzbbmrmls67h6g27d6rf77cv",
    "title": "Finding Frequent and Interesting Triples in Text",
    "info": {
        "author": [
            "Janez Brank, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "June 7, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Information Extraction"
        ]
    },
    "url": "http://videolectures.net/akbc2010_brank_ffitt/",
    "segmentation": [
        [
            "Hello everyone, so my name is John is blank and I'll be presenting joint joint work with colleagues from our Department about finding frequent and interesting travels in textual data.",
            "So bit of motivation why we started looking into this problem at all.",
            "The kind of overall idea was that we would have to populate some sort of knowledge base or ontology, for example something like psych which some of the people in our Department are involved with with some sort of common sense facts that could help with some kind of reasoning or querying.",
            "So we will be looking for for triple, so this."
        ],
        [
            "Word concept one concept two in the relation between them and an example of such a potentially interesting triple might be this one person inhabit countries, so this this could be useful in some sort of ontology or some sort of reasoning system, because it tells you, for example, that the country is something that people can inhabitant, and so forth.",
            "So the idea was to kind of try to gather this sort of common sense information and to try to extract them somehow automatically from text and.",
            "So that's kind of the idea.",
            "Now if we look at an actual textual document from the real world, it's not likely to contain this sort of triples explicitly.",
            "This is some sort of abstraction from more concrete statements which we might encounter in texting in a textual document, for example, a piece of news.",
            "They might tell you that a specific person lives in a specific country, but this is a generalization of many such specific triples, so.",
            "One of the main aspects of our problem was to deal with this generalization to be able to identify kind of slightly abstract triples such as this one from the concrete ones which we would encounter in in documents, and we decided to use the Wordnet hypernym relationships to generalize concepts more about that you know few moments."
        ],
        [
            "So he's kind of rough overview of our approach.",
            "So given a corpus of text.",
            "Using partial and some heuristics, we obtain the list of triples which are actually phrases from from sentences.",
            "Now I'm not going to talk about this part of the approach cause it's not really part of our paper.",
            "It was done by some colleagues in our Department and we use their output as an input for our work.",
            "Then the next step was to.",
            "Associate these phrases with specific concepts from word net so that we can use word net from generalization.",
            "From that point onwards, so we get a list of concepts, triples, and then we look for frequent triples using an algorithm which I will present in a few moments to identify triples which exceeds a certain minimum support threshold, and this includes generalized triples which don't appear.",
            "Directly in this input list, but which can be obtained from them with generalization, with word, net, and then the next step would be to process this list further to identify triples which are kind of interesting from the point of view of the problem which I presented on the previous slide, and I'll show some of the heuristics that we used for that as well."
        ],
        [
            "So the first part of our task was to associate this input triples with concepts from word net.",
            "So this is an example of the sort of input triple that we would deal with.",
            "So apparently some some kind of parsing and heuristics processed some sentence in a piece of news or something like that, and identified that this sentence had this subject and this predicate and this object.",
            "And however these things don't appear directly as concepts in Word net.",
            "So we would like to associate each of these three components with the word net concept and we used heuristics via matching the words in the in the in the input with names of word net concepts.",
            "We try to match as long subsequence as possible.",
            "So for example in this case word NET has both of these concepts, minister and Finance Minister and we would take finance minister because it's longer and that usually means it's more specific, which is what we want to identify in this case.",
            "And another heuristic is to break the ties by selecting the right most longest match.",
            "So for example European Union also appears in Word net and it also has two words.",
            "But we would use Finance minister in this case because it appears later.",
            "And that usually means it's the root of the phrase rather than some kind of modifier.",
            "And we also do some normalization of the words with matching.",
            "So yeah, and then finally, of course, when we are when we're dealing with the subjects and objects, we look only for those concepts in Word, NET which are nouns.",
            "And when when they link with the predicate, we only look for those concepts which are verbs, because each concept in Word net is labeled as noun or verb or adjective and so forth.",
            "Um?",
            "OK, so now we have a list of these."
        ],
        [
            "So triple suffered net concepts and now we would like to each of them corresponds roughly to one sentence or one close in the input textual corpus.",
            "And now we would like to identify frequent ones.",
            "So basically this is really straightforward.",
            "So if we are, we are going to define the support of a triple SVO as the number of concepts triples as primary primal prime such that F prime is a hyponym of SV prime is a hyponym of V. And Oprah is a hyponym of oh.",
            "So basically, if we have this triple in the input, it supports not only this triple but also all possible generalizations at all levels of abstraction.",
            "When we move up through the word net hierarchy.",
            "Sorry.",
            "So so one question is if the verb be the relation, V is sort of a is not an upward entailing one, so suppose.",
            "You know, finance ministers.",
            "There's like without fighting or sort of righteousness.",
            "Is this like?",
            "Low interest rates say.",
            "Or it doesn't mean necessarily that finance ministers is like.",
            "I mean it's not done until they did.",
            "It doesn't mean that he dislikes generalizations.",
            "Yeah, OK, I guess that's a good point.",
            "We haven't really kind of considered that aspect of the problem.",
            "I agree that this could be could be a kind of source of noise in the identification of frequent triples.",
            "We kind of rely on the idea that because we have a large amount of data, statistics will kind of help us in the long term, and the really frequent ones will be somehow interesting.",
            "I think the question is more than.",
            "Relations they think.",
            "I mean, it doesn't matter what is this is, are they still going to be done in tail?",
            "However, even say many times, right?",
            "Yes.",
            "But also think it's a question of how to what extent these things are present in the data.",
            "So we we we were dealing with this sort of journalistic articles from writers, and I would also deny that.",
            "Or, you know, there's all kinds of things like that entity.",
            "I agree, I mean, I think this would kind of work more with the sort of more positive statements, so to speak.",
            "Oh also, OK. Also, here's an example of.",
            "So.",
            "Basically we have a.",
            "We have a specific triple like this foreign finance Minister approved plan, but this same trip will also support the slightly more abstract on like this one.",
            "Executive approve idea.",
            "Becausw executive is a hypernym of minister, which is a hypernym of finance.",
            "Minister and Idea is a hypernym of planning word, net.",
            "That's how our definition of support would work, and so now our goal of this step is to identify all all triples.",
            "Regardless of how abstract they are, whose support exceeds a certain."
        ],
        [
            "Yes, hold, so this is a problem that is kind of related to the frequent itemset problems that could be attacked with just five minutes or dear.",
            "OK, so I'm going to skip some of the details.",
            "So basically we use an algorithm inspired by a priority, but we cannot use a priority directly.",
            "We also looked into extensions of a priority for hierarchical data, but the specific aspects of this problem required some further adaptations.",
            "So basically we ordered the search space of triples by depth in Word net.",
            "So for any for any Triple S for any concept in Word net, we can look at its depth.",
            "In the hypernym hierarchy and then for a triple, we can compute the sum of the depth of the three components, so that's the depth of the triple, and we basically make multiple passes through the data, one for each level of depth, and identify the frequent triples adept at that depth, and we use the fact that of course, if you generalize the triple, the support can only grow.",
            "So if a triple is going to be frequent, all of its generalizations also have to be frequent.",
            "So if you have already identified.",
            "All the frequent triples at higher levels of generality, and if one of the generalizations of your support triple does not appear among the frequent ones at the higher level of generality, then you know that your triple won't be frequent either.",
            "So this is a very strong criterion, which enables us to discard a lot of the candidates which would not turn out to be frequent.",
            "That turns out to be crucial, because otherwise the memory requirements of the algorithm would be intractable.",
            "So basic."
        ],
        [
            "That's it, I won't go into too much details.",
            "So the next part of our task was identifying the interesting triple.",
            "So each of these frequent triples are interesting.",
            "'cause if you generalize the triple, it remains frequent.",
            "So when you identify all the frequent triples that includes a lot of very general ones, which don't really tell you anything interesting.",
            "For example, entity is the is the root of the hierarchy of concepts in Word net.",
            "So you can generalize anything into into a triple like this, maybe with another warp.",
            "But still this is a very very abstract triple, which which isn't useful, but it's frequent, so frequency by itself is not really enough and we're looking into heuristics which would try to identify which triples are likely to be interesting.",
            "So here I'll present some ideas along these lines.",
            "So well, so if we consider a concept S, and it's hypernym, a slightly more abstract concept as prime.",
            "So of course every triple that supports S also supports its hypernym.",
            "But the other way around is not necessarily the case, because as prime might have other hyponyms, so we can think of this ratio of their supports as a kind of conditional probability that the triple support South, given that it supports South Prime.",
            "So we might kind of naively expect if we if we take this slightly more abstract triple.",
            "Yes, Prime Video and look at IT support and multiplied by this conditional probability then this will give us some kind of rough idea of the support of this slightly more specific triple SVO, but of course the actual support of SVO can be quite different.",
            "It might be higher, it might be lower, so the idea of our heuristic was if the actual support of SVO is considerably higher than this naive estimate will tell you, then apparently as window R concepts, which kind of fit well together, which tell you something.",
            "That wouldn't follow directly from from this more general concept, so this this ratio between these things the actual support and this kind of naively estimated one might be some kind of measure of interesting Ness of the triple with regards to the subject.",
            "And of course you could do the same things on the other two components as well and."
        ],
        [
            "Turns out that this is.",
            "This measure is a bit too rough if you just list the frequent triples in order of interestingness, a lot of strange outliers are very highly ranked, and so forth.",
            "So we try to go one step further.",
            "If we group triples into neighborhoods where basically all all triples which agree in the first components would form a neighborhood, then it seemed promising to say that the triple is interesting.",
            "If it's the most interesting in its own neighborhood.",
            "Or even in two or three of the neighborhoods to which it belongs.",
            "And it also might be useful to require that the neighborhood has to be large enough in order for us to consider it again to deal with some of the strange parts of Wordnet where some concepts have just once so hyponym and so forth."
        ],
        [
            "So now to present some of our experiments we were dealing with with triples extracted from the Reuters corpus for about 12 million of them we were able to identify concepts from word net, so that's our input and 12 million concept triples.",
            "Here are some of the results of the frequency triple discovery algorithm.",
            "Like I mentioned earlier, we make one pass through the data for each depth, some.",
            "It turns out that there were about 34 passes in this case, and now the blue line shows the number of frequent triples identified at each path and the Gray.",
            "Sorry, the red line shows the number of candidates triples accumulated during that path, so we can see that in no case where there more than about.",
            "60% candidate strippers that did not turn out to be frequent.",
            "So in other words, our algorithm doesn't use up an unacceptably large amount of space for the storage of unpromising candidates, so we were quite happy with this result, although at the same time it does take a long time to deal with data set of this size."
        ],
        [
            "Now for for interesting triples to evaluate our heuristic, we manually evaluate about 1300 triples, which are specializations of this triple.",
            "So we evaluated the owner on a scale of 1 to 5, where we consider four and five as being interesting, and so the way we evaluated this, our heuristics was if we.",
            "If we filter out this set of triples with some of our heuristics, will we get a bigger percentage of interesting triples as measured by our manual evaluation scores?",
            "Among them, so these results show with some of our some of our heuristics, we were able to increase the proportion of interesting cripples in the set, but really, these results are too preliminary for any strong conclusions to be drawn because the amount of data is still too small."
        ],
        [
            "So just in conclusion, server frequent triple algorithm successfully handles large amounts of data, doesn't waste memory unreasonably, so it works nicely.",
            "Our measure for interesting triples.",
            "We think it has some potential, but we haven't yet found the best way to use it, and we're planning to evaluate it on a larger set of triples to get some better feeling for it.",
            "Some ideas for future work are to think of it as a kind of covering problem.",
            "If you.",
            "If you look at the frequent strippers with a specific.",
            "With a specific ask and we and consider the word net tree.",
            "And now these triples cover that the objects of these triples cover some notes of that tree, and you might want to identify those nodes in the tree where the subtree has a lot of covered notes but not too many non covered nodes.",
            "So it's A kind of covering problem where you don't want.",
            "To cover the.",
            "Wrong parts of the tree and we were also thinking of some ideas to generate kind of negative triples randomly and then treat this as a traditional kind of covering problems such as we're used for training Association rules and so forth.",
            "So hopefully we'll explore some of these ideas in future work.",
            "Sorry if I was a bit too long, thank you."
        ],
        [
            "This kind of information.",
            "It's kind of weak compared to.",
            "When you can generally find inside, for example.",
            "Things like.",
            "People sometimes eat food.",
            "Younger and then you know more specific things like you don't learn anything that would let you draw conclusions.",
            "From from a scenario, so do you have ideas about what kinds of applications is quite useful?",
            "Not really to be honest.",
            "I mean the kind of initial idea for the work wasn't really mine, and I was kind of interested primarily in the frequent triple detection part of the work, so I guess the vision was that some kind of weak knowledge is potentially still better than no knowledge at all.",
            "If you are dealing with an area which was previously not covered by your ontology, and it was.",
            "This was kind of an exploration to see.",
            "What sort of things you might be able to get reasonably automatically from, uh, from a large corpus of texts.",
            "But I agree that ultimately it's it's still pretty weak knowledge."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, so my name is John is blank and I'll be presenting joint joint work with colleagues from our Department about finding frequent and interesting travels in textual data.",
                    "label": 1
                },
                {
                    "sent": "So bit of motivation why we started looking into this problem at all.",
                    "label": 0
                },
                {
                    "sent": "The kind of overall idea was that we would have to populate some sort of knowledge base or ontology, for example something like psych which some of the people in our Department are involved with with some sort of common sense facts that could help with some kind of reasoning or querying.",
                    "label": 0
                },
                {
                    "sent": "So we will be looking for for triple, so this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Word concept one concept two in the relation between them and an example of such a potentially interesting triple might be this one person inhabit countries, so this this could be useful in some sort of ontology or some sort of reasoning system, because it tells you, for example, that the country is something that people can inhabitant, and so forth.",
                    "label": 1
                },
                {
                    "sent": "So the idea was to kind of try to gather this sort of common sense information and to try to extract them somehow automatically from text and.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of the idea.",
                    "label": 1
                },
                {
                    "sent": "Now if we look at an actual textual document from the real world, it's not likely to contain this sort of triples explicitly.",
                    "label": 0
                },
                {
                    "sent": "This is some sort of abstraction from more concrete statements which we might encounter in texting in a textual document, for example, a piece of news.",
                    "label": 1
                },
                {
                    "sent": "They might tell you that a specific person lives in a specific country, but this is a generalization of many such specific triples, so.",
                    "label": 0
                },
                {
                    "sent": "One of the main aspects of our problem was to deal with this generalization to be able to identify kind of slightly abstract triples such as this one from the concrete ones which we would encounter in in documents, and we decided to use the Wordnet hypernym relationships to generalize concepts more about that you know few moments.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So he's kind of rough overview of our approach.",
                    "label": 1
                },
                {
                    "sent": "So given a corpus of text.",
                    "label": 1
                },
                {
                    "sent": "Using partial and some heuristics, we obtain the list of triples which are actually phrases from from sentences.",
                    "label": 0
                },
                {
                    "sent": "Now I'm not going to talk about this part of the approach cause it's not really part of our paper.",
                    "label": 1
                },
                {
                    "sent": "It was done by some colleagues in our Department and we use their output as an input for our work.",
                    "label": 0
                },
                {
                    "sent": "Then the next step was to.",
                    "label": 0
                },
                {
                    "sent": "Associate these phrases with specific concepts from word net so that we can use word net from generalization.",
                    "label": 0
                },
                {
                    "sent": "From that point onwards, so we get a list of concepts, triples, and then we look for frequent triples using an algorithm which I will present in a few moments to identify triples which exceeds a certain minimum support threshold, and this includes generalized triples which don't appear.",
                    "label": 0
                },
                {
                    "sent": "Directly in this input list, but which can be obtained from them with generalization, with word, net, and then the next step would be to process this list further to identify triples which are kind of interesting from the point of view of the problem which I presented on the previous slide, and I'll show some of the heuristics that we used for that as well.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first part of our task was to associate this input triples with concepts from word net.",
                    "label": 1
                },
                {
                    "sent": "So this is an example of the sort of input triple that we would deal with.",
                    "label": 0
                },
                {
                    "sent": "So apparently some some kind of parsing and heuristics processed some sentence in a piece of news or something like that, and identified that this sentence had this subject and this predicate and this object.",
                    "label": 0
                },
                {
                    "sent": "And however these things don't appear directly as concepts in Word net.",
                    "label": 0
                },
                {
                    "sent": "So we would like to associate each of these three components with the word net concept and we used heuristics via matching the words in the in the in the input with names of word net concepts.",
                    "label": 0
                },
                {
                    "sent": "We try to match as long subsequence as possible.",
                    "label": 0
                },
                {
                    "sent": "So for example in this case word NET has both of these concepts, minister and Finance Minister and we would take finance minister because it's longer and that usually means it's more specific, which is what we want to identify in this case.",
                    "label": 0
                },
                {
                    "sent": "And another heuristic is to break the ties by selecting the right most longest match.",
                    "label": 1
                },
                {
                    "sent": "So for example European Union also appears in Word net and it also has two words.",
                    "label": 0
                },
                {
                    "sent": "But we would use Finance minister in this case because it appears later.",
                    "label": 0
                },
                {
                    "sent": "And that usually means it's the root of the phrase rather than some kind of modifier.",
                    "label": 0
                },
                {
                    "sent": "And we also do some normalization of the words with matching.",
                    "label": 1
                },
                {
                    "sent": "So yeah, and then finally, of course, when we are when we're dealing with the subjects and objects, we look only for those concepts in Word, NET which are nouns.",
                    "label": 0
                },
                {
                    "sent": "And when when they link with the predicate, we only look for those concepts which are verbs, because each concept in Word net is labeled as noun or verb or adjective and so forth.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "OK, so now we have a list of these.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So triple suffered net concepts and now we would like to each of them corresponds roughly to one sentence or one close in the input textual corpus.",
                    "label": 1
                },
                {
                    "sent": "And now we would like to identify frequent ones.",
                    "label": 0
                },
                {
                    "sent": "So basically this is really straightforward.",
                    "label": 0
                },
                {
                    "sent": "So if we are, we are going to define the support of a triple SVO as the number of concepts triples as primary primal prime such that F prime is a hyponym of SV prime is a hyponym of V. And Oprah is a hyponym of oh.",
                    "label": 0
                },
                {
                    "sent": "So basically, if we have this triple in the input, it supports not only this triple but also all possible generalizations at all levels of abstraction.",
                    "label": 0
                },
                {
                    "sent": "When we move up through the word net hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So so one question is if the verb be the relation, V is sort of a is not an upward entailing one, so suppose.",
                    "label": 0
                },
                {
                    "sent": "You know, finance ministers.",
                    "label": 0
                },
                {
                    "sent": "There's like without fighting or sort of righteousness.",
                    "label": 0
                },
                {
                    "sent": "Is this like?",
                    "label": 0
                },
                {
                    "sent": "Low interest rates say.",
                    "label": 0
                },
                {
                    "sent": "Or it doesn't mean necessarily that finance ministers is like.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not done until they did.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean that he dislikes generalizations.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, I guess that's a good point.",
                    "label": 0
                },
                {
                    "sent": "We haven't really kind of considered that aspect of the problem.",
                    "label": 0
                },
                {
                    "sent": "I agree that this could be could be a kind of source of noise in the identification of frequent triples.",
                    "label": 0
                },
                {
                    "sent": "We kind of rely on the idea that because we have a large amount of data, statistics will kind of help us in the long term, and the really frequent ones will be somehow interesting.",
                    "label": 0
                },
                {
                    "sent": "I think the question is more than.",
                    "label": 0
                },
                {
                    "sent": "Relations they think.",
                    "label": 0
                },
                {
                    "sent": "I mean, it doesn't matter what is this is, are they still going to be done in tail?",
                    "label": 0
                },
                {
                    "sent": "However, even say many times, right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "But also think it's a question of how to what extent these things are present in the data.",
                    "label": 0
                },
                {
                    "sent": "So we we we were dealing with this sort of journalistic articles from writers, and I would also deny that.",
                    "label": 0
                },
                {
                    "sent": "Or, you know, there's all kinds of things like that entity.",
                    "label": 0
                },
                {
                    "sent": "I agree, I mean, I think this would kind of work more with the sort of more positive statements, so to speak.",
                    "label": 0
                },
                {
                    "sent": "Oh also, OK. Also, here's an example of.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically we have a.",
                    "label": 1
                },
                {
                    "sent": "We have a specific triple like this foreign finance Minister approved plan, but this same trip will also support the slightly more abstract on like this one.",
                    "label": 0
                },
                {
                    "sent": "Executive approve idea.",
                    "label": 0
                },
                {
                    "sent": "Becausw executive is a hypernym of minister, which is a hypernym of finance.",
                    "label": 1
                },
                {
                    "sent": "Minister and Idea is a hypernym of planning word, net.",
                    "label": 0
                },
                {
                    "sent": "That's how our definition of support would work, and so now our goal of this step is to identify all all triples.",
                    "label": 1
                },
                {
                    "sent": "Regardless of how abstract they are, whose support exceeds a certain.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, hold, so this is a problem that is kind of related to the frequent itemset problems that could be attacked with just five minutes or dear.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to skip some of the details.",
                    "label": 0
                },
                {
                    "sent": "So basically we use an algorithm inspired by a priority, but we cannot use a priority directly.",
                    "label": 1
                },
                {
                    "sent": "We also looked into extensions of a priority for hierarchical data, but the specific aspects of this problem required some further adaptations.",
                    "label": 1
                },
                {
                    "sent": "So basically we ordered the search space of triples by depth in Word net.",
                    "label": 0
                },
                {
                    "sent": "So for any for any Triple S for any concept in Word net, we can look at its depth.",
                    "label": 0
                },
                {
                    "sent": "In the hypernym hierarchy and then for a triple, we can compute the sum of the depth of the three components, so that's the depth of the triple, and we basically make multiple passes through the data, one for each level of depth, and identify the frequent triples adept at that depth, and we use the fact that of course, if you generalize the triple, the support can only grow.",
                    "label": 1
                },
                {
                    "sent": "So if a triple is going to be frequent, all of its generalizations also have to be frequent.",
                    "label": 0
                },
                {
                    "sent": "So if you have already identified.",
                    "label": 1
                },
                {
                    "sent": "All the frequent triples at higher levels of generality, and if one of the generalizations of your support triple does not appear among the frequent ones at the higher level of generality, then you know that your triple won't be frequent either.",
                    "label": 0
                },
                {
                    "sent": "So this is a very strong criterion, which enables us to discard a lot of the candidates which would not turn out to be frequent.",
                    "label": 0
                },
                {
                    "sent": "That turns out to be crucial, because otherwise the memory requirements of the algorithm would be intractable.",
                    "label": 0
                },
                {
                    "sent": "So basic.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's it, I won't go into too much details.",
                    "label": 0
                },
                {
                    "sent": "So the next part of our task was identifying the interesting triple.",
                    "label": 0
                },
                {
                    "sent": "So each of these frequent triples are interesting.",
                    "label": 1
                },
                {
                    "sent": "'cause if you generalize the triple, it remains frequent.",
                    "label": 0
                },
                {
                    "sent": "So when you identify all the frequent triples that includes a lot of very general ones, which don't really tell you anything interesting.",
                    "label": 0
                },
                {
                    "sent": "For example, entity is the is the root of the hierarchy of concepts in Word net.",
                    "label": 0
                },
                {
                    "sent": "So you can generalize anything into into a triple like this, maybe with another warp.",
                    "label": 0
                },
                {
                    "sent": "But still this is a very very abstract triple, which which isn't useful, but it's frequent, so frequency by itself is not really enough and we're looking into heuristics which would try to identify which triples are likely to be interesting.",
                    "label": 1
                },
                {
                    "sent": "So here I'll present some ideas along these lines.",
                    "label": 1
                },
                {
                    "sent": "So well, so if we consider a concept S, and it's hypernym, a slightly more abstract concept as prime.",
                    "label": 1
                },
                {
                    "sent": "So of course every triple that supports S also supports its hypernym.",
                    "label": 0
                },
                {
                    "sent": "But the other way around is not necessarily the case, because as prime might have other hyponyms, so we can think of this ratio of their supports as a kind of conditional probability that the triple support South, given that it supports South Prime.",
                    "label": 1
                },
                {
                    "sent": "So we might kind of naively expect if we if we take this slightly more abstract triple.",
                    "label": 0
                },
                {
                    "sent": "Yes, Prime Video and look at IT support and multiplied by this conditional probability then this will give us some kind of rough idea of the support of this slightly more specific triple SVO, but of course the actual support of SVO can be quite different.",
                    "label": 0
                },
                {
                    "sent": "It might be higher, it might be lower, so the idea of our heuristic was if the actual support of SVO is considerably higher than this naive estimate will tell you, then apparently as window R concepts, which kind of fit well together, which tell you something.",
                    "label": 0
                },
                {
                    "sent": "That wouldn't follow directly from from this more general concept, so this this ratio between these things the actual support and this kind of naively estimated one might be some kind of measure of interesting Ness of the triple with regards to the subject.",
                    "label": 0
                },
                {
                    "sent": "And of course you could do the same things on the other two components as well and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Turns out that this is.",
                    "label": 0
                },
                {
                    "sent": "This measure is a bit too rough if you just list the frequent triples in order of interestingness, a lot of strange outliers are very highly ranked, and so forth.",
                    "label": 1
                },
                {
                    "sent": "So we try to go one step further.",
                    "label": 0
                },
                {
                    "sent": "If we group triples into neighborhoods where basically all all triples which agree in the first components would form a neighborhood, then it seemed promising to say that the triple is interesting.",
                    "label": 0
                },
                {
                    "sent": "If it's the most interesting in its own neighborhood.",
                    "label": 1
                },
                {
                    "sent": "Or even in two or three of the neighborhoods to which it belongs.",
                    "label": 1
                },
                {
                    "sent": "And it also might be useful to require that the neighborhood has to be large enough in order for us to consider it again to deal with some of the strange parts of Wordnet where some concepts have just once so hyponym and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now to present some of our experiments we were dealing with with triples extracted from the Reuters corpus for about 12 million of them we were able to identify concepts from word net, so that's our input and 12 million concept triples.",
                    "label": 1
                },
                {
                    "sent": "Here are some of the results of the frequency triple discovery algorithm.",
                    "label": 1
                },
                {
                    "sent": "Like I mentioned earlier, we make one pass through the data for each depth, some.",
                    "label": 1
                },
                {
                    "sent": "It turns out that there were about 34 passes in this case, and now the blue line shows the number of frequent triples identified at each path and the Gray.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the red line shows the number of candidates triples accumulated during that path, so we can see that in no case where there more than about.",
                    "label": 0
                },
                {
                    "sent": "60% candidate strippers that did not turn out to be frequent.",
                    "label": 0
                },
                {
                    "sent": "So in other words, our algorithm doesn't use up an unacceptably large amount of space for the storage of unpromising candidates, so we were quite happy with this result, although at the same time it does take a long time to deal with data set of this size.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now for for interesting triples to evaluate our heuristic, we manually evaluate about 1300 triples, which are specializations of this triple.",
                    "label": 1
                },
                {
                    "sent": "So we evaluated the owner on a scale of 1 to 5, where we consider four and five as being interesting, and so the way we evaluated this, our heuristics was if we.",
                    "label": 1
                },
                {
                    "sent": "If we filter out this set of triples with some of our heuristics, will we get a bigger percentage of interesting triples as measured by our manual evaluation scores?",
                    "label": 0
                },
                {
                    "sent": "Among them, so these results show with some of our some of our heuristics, we were able to increase the proportion of interesting cripples in the set, but really, these results are too preliminary for any strong conclusions to be drawn because the amount of data is still too small.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just in conclusion, server frequent triple algorithm successfully handles large amounts of data, doesn't waste memory unreasonably, so it works nicely.",
                    "label": 1
                },
                {
                    "sent": "Our measure for interesting triples.",
                    "label": 1
                },
                {
                    "sent": "We think it has some potential, but we haven't yet found the best way to use it, and we're planning to evaluate it on a larger set of triples to get some better feeling for it.",
                    "label": 1
                },
                {
                    "sent": "Some ideas for future work are to think of it as a kind of covering problem.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "If you look at the frequent strippers with a specific.",
                    "label": 0
                },
                {
                    "sent": "With a specific ask and we and consider the word net tree.",
                    "label": 1
                },
                {
                    "sent": "And now these triples cover that the objects of these triples cover some notes of that tree, and you might want to identify those nodes in the tree where the subtree has a lot of covered notes but not too many non covered nodes.",
                    "label": 0
                },
                {
                    "sent": "So it's A kind of covering problem where you don't want.",
                    "label": 0
                },
                {
                    "sent": "To cover the.",
                    "label": 0
                },
                {
                    "sent": "Wrong parts of the tree and we were also thinking of some ideas to generate kind of negative triples randomly and then treat this as a traditional kind of covering problems such as we're used for training Association rules and so forth.",
                    "label": 0
                },
                {
                    "sent": "So hopefully we'll explore some of these ideas in future work.",
                    "label": 0
                },
                {
                    "sent": "Sorry if I was a bit too long, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of information.",
                    "label": 0
                },
                {
                    "sent": "It's kind of weak compared to.",
                    "label": 0
                },
                {
                    "sent": "When you can generally find inside, for example.",
                    "label": 0
                },
                {
                    "sent": "Things like.",
                    "label": 0
                },
                {
                    "sent": "People sometimes eat food.",
                    "label": 0
                },
                {
                    "sent": "Younger and then you know more specific things like you don't learn anything that would let you draw conclusions.",
                    "label": 0
                },
                {
                    "sent": "From from a scenario, so do you have ideas about what kinds of applications is quite useful?",
                    "label": 0
                },
                {
                    "sent": "Not really to be honest.",
                    "label": 0
                },
                {
                    "sent": "I mean the kind of initial idea for the work wasn't really mine, and I was kind of interested primarily in the frequent triple detection part of the work, so I guess the vision was that some kind of weak knowledge is potentially still better than no knowledge at all.",
                    "label": 0
                },
                {
                    "sent": "If you are dealing with an area which was previously not covered by your ontology, and it was.",
                    "label": 0
                },
                {
                    "sent": "This was kind of an exploration to see.",
                    "label": 0
                },
                {
                    "sent": "What sort of things you might be able to get reasonably automatically from, uh, from a large corpus of texts.",
                    "label": 0
                },
                {
                    "sent": "But I agree that ultimately it's it's still pretty weak knowledge.",
                    "label": 0
                }
            ]
        }
    }
}