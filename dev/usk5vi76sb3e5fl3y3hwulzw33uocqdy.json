{
    "id": "usk5vi76sb3e5fl3y3hwulzw33uocqdy",
    "title": "Visual features: From Fourier to Gabor",
    "info": {
        "author": [
            "Roland Memisevic, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_memisevic_fourier_gabor/",
    "segmentation": [
        [
            "Thanks yeah sure so so that's true.",
            "I'm going to talk about relations and analogies and stuff like that a little bit and how that and how that relates to vision.",
            "In my second talk, but.",
            "To set the stage for that, so to speak, I'm going to talk about something much more basic today, which is basically Fourier transforms on images, and so the reason for that is for one thing, it sets the stage for the relation stuff that I'll talk about later.",
            "The other is that it's kind of underappreciated.",
            "I think in the deep learning community, the fundamental role that I think Fourier transforms play.",
            "In understanding.",
            "Why features look the way they look and why they work and how they are related to invariants and equivariance and all kinds of things?",
            "So."
        ],
        [
            "Leon Bottou and his talk 2 days ago also showed this picture, which is.",
            "The kinds of features that cells in various parts of the brain like to see as found by Hubel and Wiesel in their famous experiment in the 50s or 60s.",
            "Um?",
            "And so it's very common to say, well, these are edge detectors where everyone says edge detectors, which is, I think, due to 1015 year old paper by Terry Sinofsky and Tony Bell, where they call them edge detectors.",
            "But I think it's not quite an accurate characterization becausw.",
            "While this might detect something like an edge, this doesn't really look like an edge detector, and often these these features are not really detecting edges, but they're really WAVY and have many periods and stuff like that.",
            "So the brain finds them through learning maybe or through a biological prewiring or whatever.",
            "I don't know."
        ],
        [
            "Neural Nets also find them and exclusively almost find these right.",
            "So whenever you train a network and it.",
            "Comes up with these kinds of features.",
            "You already know that it kind of works.",
            "You know that the performance is going to be reasonable somewhat, and if not, then you can already be pretty sure that it's not reasonable.",
            "It's not.",
            "It's not going to work right.",
            "So Pascal yesterday showed autoencoder features that are broken, and those that look like this, and these are always the ones that give you reasonable performance.",
            "So this is features from Alex Net.",
            "You've seen this picture 1000 times.",
            "Probably sometimes they can be in color, sometimes in Gray, and depending on how you set up your model, some learn the color some down the great values in.",
            "Whatever."
        ],
        [
            "If you train something completely different like I see a.",
            "You get these kind of features too.",
            "Maybe they look a little bit different, but the general characteristic is the same, of course.",
            "So by the way I'm going to borrow a lot of pictures from this book here, natural image that it sticks by upheaval in Huy and Hoya.",
            "Which is a great book on natural image statistiques.",
            "And a lot of material that I'm going to present this borrowed from that book.",
            "As."
        ],
        [
            "So you can also do other things like sparse coding was a much older.",
            "This is a much older plot from the 90s or so Bruno's house and field, and then you also get these features of constant.",
            "And here you often also see that it's not really an edge detector, it's something different."
        ],
        [
            "You can also do other things, like regularised auto, nicely regularised auto encoders like denoising autoencoders as Pascal mentioned yesterday you get these features of course."
        ],
        [
            "Something kind of interesting that I just wanted to throw in here is if you train in a contractive autoencoder.",
            "Even if you do the wrong thing and trying to contractive autoencoder the way that Pascal defined them, but you just set the contraction penalty to a negative value so you don't actually contract, but you make things worse from the contractive POV.",
            "And you look what the features look like.",
            "They're just couple of features, and in fact they work if you plug them into a pipeline that does recognition, it just works fine.",
            "So even the completely wrong thing.",
            "Works just fine by giving you Gabor features, which is all all you want, presumably.",
            "So this is easy to try to contact autoencoders, set the Lambda 2 -- 1 rather than one for example."
        ],
        [
            "If you do something, lo and behold, K means something really simple and easy and old school, you get a couple of features, so it doesn't really matter what you do with keyboard features sometimes."
        ],
        [
            "Larger, sometimes smaller, and so on.",
            "So because of that, because of the importance of these features and understanding invariants and so on, I'm going to talk about them a little bit today, and so I don't.",
            "I wouldn't claim we have a real understanding of why this couple features always pop up, but we have sort of hints and beginning a better understanding as time goes on, and the fundamental principle behind these Gabor features is actually the Fourier transform that that helps explain what they do, why they come up, and so on.",
            "So this is fully and this is kabua.",
            "As you can see.",
            "OK, so I'm going to start really basic undergrad kind of lecture on for transforms on images, and then I'll move to some slightly more."
        ],
        [
            "Interesting things.",
            "This is all very well known to signal processing people, but it's not very well known to deep learning people and so that's why I'm.",
            "Talking about this here.",
            "So the punch line, the kind of 1 sentence message that explains maybe why you get Gabor features?",
            "Is this on top there so almost all structure and natural images is position invariant and local.",
            "So that means pretty much if I take a picture of you like with my cell phone like this, I could have also taken that picture by just moving slightly to the left or moving slightly to the right or whatever.",
            "All of these are natural images, and in a data set like image net or something which is not nicely normalized and center or something like that, you would get these transformations on mass.",
            "You get tons of slight shifts.",
            "You get this almost the same object, the same feature, the same.",
            "Corner edge object whatever once here.",
            "Once here one slightly tilted and so on.",
            "So the the strong, clearly the strongest kind of structure that you have to find a natural images, is translation invariants.",
            "Um?",
            "And locality, so if you look at this random picture from some street in Montreal structure and the images if you wanted recognition is really concentrated locali right?",
            "So if you want to recognize that this is a car, then you first recognize maybe that this is.",
            "What is this lamp or you call it in English headlights?",
            "Thank you and this piece here, whatever.",
            "So, so all of these things that you see.",
            "Express themselves as a set of pixels that are close nearby, and that's what gives them their meaning, right?",
            "So you never see or hardly ever see something whether the meaning.",
            "But you can deduce is kind of spread from this part of the image to that part of the image, or something like that.",
            "You can build hierarchies and so on later and confidence do that, but fundamentally the structure is local and it's position invariant, so I could have moved to the right and got almost the same picture with almost the same meaning.",
            "So, so this has two consequences.",
            "One is that.",
            "Any vision system that works as we have now, the whole vision community has now found is based on local patches, so the fundamental operation that enter the way the image enters the system is in terms of little small regions that get analyzed and then other stuff happens and the other is that the universal mathematical frameworks for understanding what happens in images, how to understand them?",
            "How about invariances and so on must be the Fourier transform, and that's because the Fourier transform is.",
            "The framework for dealing with translation, encoding translations and encoding, translation invariants, and so on."
        ],
        [
            "So regarding the first point.",
            "Convolution, so I think Homeland is going to introduce convolutional networks in some detail later.",
            "Um?",
            "This is just a one slide reminder of what a convent pretty much does roughly, so you take a little filter.",
            "You put it on top of the image.",
            "Compute the inner product of this value.",
            "This this little filter is this part of the image and then it gives you the output.",
            "This is one plane in the in your feature map.",
            "Then you scan this across the image and do the same thing over and over again, right?",
            "So this is fundamentally local and there's translation invariants somehow built into this.",
            "Or the desire to model something in the translation invariant way.",
            "In practice, you would flip the filter before.",
            "Putting it there and that it has some mathematical reasons, but I'm not going to get into probably at all today.",
            "For a continent it doesn't matter at all, because you're going to learn those features anyway.",
            "So if you flip them or not, that doesn't really make any difference.",
            "Um?",
            "So sometimes people call the operation without the flipping filtering and the one visitor flipping convolution.",
            "But in terms for convolutional networks it doesn't really matter.",
            "And so cons.",
            "Net is the established term, so that's so we can give up on the flipping I guess for now.",
            "Um?"
        ],
        [
            "So that was for 2D in one year.",
            "This is just a picture just for reference from Wikipedia.",
            "So with flipping you would take your filter, flip it around and then scan it across the image and every time you do an inner product and it gives you all the output values."
        ],
        [
            "So there's one signal that's intimately related.",
            "To that operation, and that is known as the phasor signal.",
            "Can you raise your hand if you know phasor signals?",
            "And know what they do.",
            "OK, so let's.",
            "Source 30% of people so.",
            "The phase is defined up here, right?",
            "It's exit off I Omega T for some revalue Omega and AT which is a running variable.",
            "So I'm talking.",
            "I'm going to talk about 1D case for now and later I'm going to switch to 2D.",
            "Which adds a little bit of extra subtlety, and since this is not familiar to many people here, I'm going to just talk about 1D.",
            "For now, just pretend the image was a 1D signal, or just pretend we were talking about speech or so.",
            "OK, so it's XI Omega T is thanks to Euler's formula.",
            "It's the same as cosine, Omega, T, plus items.",
            "Sign Omega T and I is the square root of minus one, so it's a complex.",
            "You need complex arithmetic to talk about these things.",
            "What's important about the phasor?",
            "For one thing is that it is a representation of both the cosine and the cosine and sine signal at the same time back together into one object, which is this exponential here.",
            "Thanks to this relation.",
            "Useful because if you take any two like a sine and cosine.",
            "You can generate anyone in between, right is just a shifted version of the sign.",
            "You can generate any of any phase shifted version of your sine wave just as a linear combination of these two, and so that's another way of saying that.",
            "Sine waves of any given frequency live in a 2 dimensional subspace, so you can just spin around the face and move this thing around and the two basis vectors that you need for that are a cosine into sine.",
            "For example, you could also use something else like a phase shifted cosine and accordingly phase shifted sine.",
            "These two these two things fundamentally represent.",
            "An oscillation, not just one of them.",
            "If you had just one, you would kind of have to enumerate all phases.",
            "If you have these two, you spend the whole substudy subspace of all phases at the same time.",
            "So that's why this is an interesting object.",
            "The reason it's important, and it comes up again and again when you talk about convolution is that phase are the eigenfunctions of convolution.",
            "So.",
            "I'm going to show exactly what that means in a second.",
            "The reason for that is that phase us, first of all, are the eigenfunctions of translation.",
            "So if you take a phasor thanks to being an expert signal, if you take a phasor and shifted to the left or to the right right, you just move it by adding or subtracting something here.",
            "So this is just going to change the T2 T minus set an, since this is an ex you can pull these two apart and you see that you get it to the minus omegas at times each to the Iomega T which is the original phase are.",
            "So shifting the signal left to the left and right is nothing other than multiplying by a number.",
            "And that's another way of saying it's an eigenfunction of translation.",
            "So if you have any questions then interrupt me by all means.",
            "Who here is?"
        ],
        [
            "Familiar with complex arithmetic and fine with it on paper and stuff.",
            "It's something like 40%, so let me just review that real quick 'cause you cannot do any mass without that you cannot do any full year things without that.",
            "So here's the.",
            "Here's a here's order formula again, so it's E to the X real value is the same as cosine plus I times sign a lot of the things that are useful.",
            "Just follow directly from that.",
            "The most important operation when dealing with complex numbers is to flip back and forth between two coordinates.",
            "So a priority complex number is nothing other than a 2 dimensional vector, so one complex number is just think of it as a 2 dimensional vector point in a 2D plane, which has some funny arithmetic associated with it.",
            "That's all.",
            "So in order to to remember that arithmetic and do operations with it, you have to flip back and forth.",
            "As I said, between two representations of that number, the Classic One that everyone is familiar with.",
            "This just in terms of.",
            "Cartesian coordinates just value on the X axis and the value on the Y axis, which are just cosine and sine.",
            "Thanks to that formula, the other is polar coordinates, where you represent that number not using a B but using the an angle with one of the axes and the distance from that from the origin, and so you can go back and forth between these easily using standard formula like this in the second representation, you might think of this value.",
            "The length here as an amplitude as you will see later and the.",
            "The angle here is a phase because that exactly corresponds to the phase and amplitude of a sine wave.",
            "Why is that?",
            "Well, if you spin around.",
            "This here this sucker.",
            "Using the appropriate operation that I mentioned in a second.",
            "Since this whole it's you're just going to trace out the cosine and assign on these two axes axes here.",
            "Um?"
        ],
        [
            "So in order to talk about how to do this spinning, which is essential to define a sine wave and a cosine, let's look at the two fundamental operations.",
            "Addition is just.",
            "Since these are two D vectors, it's just vector addition, and multiplication is much easier to remember if you do that in the polar representation, so.",
            "You have two complex numbers.",
            "Their polar representation is this, and So what you do is just multiply them together just like that in that representation, and that's going to define multiplication of two complex numbers.",
            "Since you're going to multiply these two experts, that just leads to an addition of the angles here in the X, and the lengths are just being multiplied, so that's pretty much all that's necessary to understand for your analysis sufficiently deeply.",
            "Let me just.",
            "OK, let me just refer to one other thing, multiplying a number by a complex number of length one just follows just from those formulas does nothing other than taking that to the vector that complex number and rotating it a little bit or a lot, right?",
            "Let's just because of this.",
            "Way of defining multiplication.",
            "So if the length is 1, it's not going to change the links, so multiplying by length one vector just rotates around.",
            "And so if you start with a complex number and multiply it by E to E to the something again and take it to the item, something again and again it's just going to spin around the circle."
        ],
        [
            "There are a few other properties that I might get back to if necessary.",
            "The most important is conjugation, which is just flipping on the X axis of your 2D space if you want.",
            "Um?",
            "And there's a funny definition of an inner product which turns out to be more convenient than what you're used to.",
            "This looks almost like a vanilla inner product that you that you know, except that one of these two values here is as a conjugate subjected to this flipping operation.",
            "And it turns out the norms become sensible and so on.",
            "So, but this is all technical things that are not so."
        ],
        [
            "Important right now.",
            "So let me get back to this slide so aphasia is an eigenfunction of translation.",
            "So why is the phase i.e to the Iomega T becausw this signal is nothing other than a point that just spins around the origin around the unit circle, right?",
            "Thanks to this arithmetic that I just discussed.",
            "Um?"
        ],
        [
            "So why is that important for convolution?",
            "Because convolution is essentially at its heart, nothing other than a translation embedded in some kind of some.",
            "So here's the formula again.",
            "So you take your signal, you shift it around, and every time you do it in a product.",
            "So the shifting here, if you would subject the convolution with a filter to a phasor, then this shift here is just going to lead to a multiplier as we just saw, right?",
            "So you're just going to multiply the phase up by a constant.",
            "OK. And now you can group things together and you see that evaluating the convolved with H convolved Faiza is the same as evaluating the phasor that has been multiplied by a constant.",
            "Now that constant is a bit more complicated than before because you have this some in there and so on.",
            "But it's still just a constant right so?",
            "Convolving a signal a phasor with some filter is the same as multiplying that phasor with some number.",
            "So recall that multiplying by that number can be a complex number.",
            "Recall that multiplying by complex complex number can do two things.",
            "It can stretch.",
            "The value and it can phase shift it a little bit.",
            "It can take the value, make it longer and it can rotate it around or both.",
            "Right, it's it's in here.",
            "Um?",
            "And what you multiply with to change the length is the the absolute value of that complex number, and the phase is what you use to the for the phase shift.",
            "And so this constant has a name.",
            "It's called amplitude response.",
            "Sorry it's called frequency response, and it's a complex number, so I didn't.",
            "It has an amplitude and phase and those are called amplitude response and phase response, and so they just tell you what happens to the phase of this phase are to the amount of shift that it has as you filter.",
            "And what happens to the amplitude?",
            "Does it get?",
            "Stronger or weaker if you multiply it.",
            "And that's all this constant or depends on Omega.",
            "It depends as you see, it depends on the frequency of the phasor.",
            "So the same filter here that I might apply to some other Phase I might have a different effect on another phase are it might give you a different constant.",
            "So it might shift and enhance this one, and it might do something else to some other one.",
            "OK."
        ],
        [
            "So since phase are so well behaved with respect to convolution and any arbitrary signal is not, because it's much harder to understand what this convolution operation does to an arbitrary signal like an arbitrary image.",
            "A great idea would be to decompose our signal into phase us and then we can read off all the little phases.",
            "What will happen as you convolve this signal with a filter.",
            "So that's one way to arrive at the Fourier transform.",
            "So this is the definition for the so called discrete for your transform where.",
            "Everything is discrete and finite, so your signal is just a vector like it always is for any practical purposes.",
            "You just say your signal is nothing other than a superposition of phases, and I have 2\u03c0 and some normalization in here just to make sure that the phasor goes around once if you.",
            "Go from zero to one and stuff like that.",
            "This is all arbitrary kind of decisions.",
            "One can make changes a little bit also.",
            "It's just due to the fact that 2\u03c0 is the natural unit.",
            "Here, right 2\u03c0 means you go around once.",
            "Unfortunately, due to some historical reasons, maybe one is for us the natural.",
            "Fundamental unit and not 2\u03c0, and so if one was to pie, I wouldn't need this normalization here, but.",
            "OK, so so you have a superposition of phases that defines your signal.",
            "How do you get those coefficients well?",
            "There's some.",
            "Nitty gritty details.",
            "Luckily, these phrases can be shown to form an ortho normal basis, and if you have an orthonormal basis to find the coefficient in which to expand and signal is just the inner product, and so if you apply this little inner product with the conjugation.",
            "That I showed before.",
            "2 the here.",
            "Then you see that you just.",
            "You just have this signal times the conjugate Phase Rs summed up as the way to get your coefficient and so getting your coefficient is called discrete Fourier transform.",
            "Writing your signal in terms of those coefficients by expanding it into those phases called inverse for inverse discrete for your transform.",
            "So for whom is this completely new material?",
            "I guess no one really OK. Um?",
            "Analogously, S is quite well as is called spectrum, and by analogy, as the amplitude of S is called amplitude spectrum and the argument of the phase angle of that complex number is called phase spectrum.",
            "So these are office complex numbers again.",
            "There are many, many different kinds of Fourier transforms.",
            "This is the most.",
            "Common, practical, useful one because it's finite, but they're also theoretical considerations that take into account continuous signals which don't exist really in practice, but on paper, and you can show things with them and so on.",
            "This is the DfT commonly."
        ],
        [
            "And it's usually implemented using FFT, which is a fast way to compute it, that's all.",
            "So that's a 1D fully transform for images you have to do Fourier transforms on.",
            "That would be domain.",
            "Now that's a bit weird 'cause you're going to have to.",
            "You're going to want to have to have a phase A of some sort, and that phase are needs to get a scalar input variable.",
            "Index function is defined on a scalar.",
            "This X function and so how do you then Divas images?",
            "And so while there's a very simple trick, you take your image, you replace every pixel or you assign to every pixel position is scalar in whichever way you want.",
            "And now you just have a bunch of scalar values.",
            "And now you can plug these values into a phasor.",
            "This position value into phasor and voila, you have a Fourier transform.",
            "So one way to transform image position into a scalar is to just multiply it with some vector.",
            "That we also called Omega, which is now called frequency frequency vector.",
            "And so that will take an image position, replace it by a scalar number, and allow us to plug it into a phase A and we have a Fourier transform in 2D.",
            "Of course you could use all kinds of other functions, then projecting onto a frequency vector.",
            "You could take any scalar function and you would define something like a Fourier transform on a 2 dimensional domain.",
            "Cheating in some way, right?",
            "So you take the 2D domain, you replace every point using some loss, of course, because you somehow assign these two D2D grid to 1D.",
            "Sequence and then you plug it into a phasor."
        ],
        [
            "But the most common one is this linear one, and there's some nice properties, and so if the frequency vector points in this direction, for example, then you can plot the value that the phasor gives you.",
            "I don't know if this is the X or the Y axis, the imaginary component, one of them, and you see that it's you're going to get a wave that extends in the direction of that frequency vector.",
            "Yeah.",
            "Midnight Sun equivalent is of course I'm Transformers, yet another.",
            "Transform that I'm not going to talk about at all, and it's yeah, that's.",
            "So I don't know much about this.",
            "I'm actually not a signal processing person at all.",
            "The cosine transform is really popular for compression and stuff like that.",
            "And I'll leave it at that before telling you something that's wrong."
        ],
        [
            "I think eventually once you learn your transforms more and more as time goes on and confidence catch on in more and more areas.",
            "Things like predefining a set of features onto which you project like the cosine transform.",
            "Well, maybe I'm stretching a little bit too far, but might get a little bit less important.",
            "Overtime.",
            "Becausw learning will discover the statistically correct way of projecting your your signal.",
            "OK, so complex valued waves.",
            "Interestingly, are separable.",
            "Yes, another question."
        ],
        [
            "That's right, that's why I said this image here is actually.",
            "And one projection of the actual complex number cannot show you a complex image easily, and so I just show you the one of the two components, the imaginary or real.",
            "I forgot which one I'm showing here, but they are going to differ only in a slight phase shift.",
            "Yeah, it's a good point.",
            "So this is a bit misleading, right?",
            "This is not the phase out.",
            "This is one of the two components of the phasor.",
            "Unfortunately phasers are two dimensional things.",
            "You cannot just show them just like that."
        ],
        [
            "So Interestingly, complex valued waves are separable, which means that you can write this.",
            "This linearly defined wave in terms of as the outer product of two 1 dimensional waves basically and the same is absolutely not true for real valued waves.",
            "So if you."
        ],
        [
            "Wonder?",
            "If there for this real valued cosine wave here if there is an image like an image 1 dimensional image like imagine it to be here and here such that the outer product of these two yields this.",
            "This is clearly not possible.",
            "You won't find these two dimensional things that generate the this image here, but interesting.",
            "Lee for complex valued waves.",
            "Another reason to use complex arithmetic.",
            "This is true, so you can actually decompose this.",
            "Image using a lower dimensional projection using the outer product of two 1 dimensional things."
        ],
        [
            "Could be used.",
            "One could imagine if you would be willing to use.",
            "Complex valued filters in a continent.",
            "You could potentially make things faster that way, for example because you could just convolve in one day this way and then in one do that way.",
            "But that's not going to work if you have real valued filters constant.",
            "The separability doesn't hold.",
            "This is just a."
        ],
        [
            "Teresting side note.",
            "So now that we know what a phase on an image could be defined as, it's easy to define the Fourier transform.",
            "Also by analogy, you just do the exact same thing you plug in your 2D phasor and so the projection onto your frequency vector W. Sorry Omega transpose Y, where Y is image position, it's just the sum of two values.",
            "And again I have to deal with some normalization depending on the number of pixels in each direction, M&N and.",
            "And 2\u03c0 because of the wrong units and so on.",
            "OK, so that's the definition of a Fourier transform on an image.",
            "Superposition of phasor images that look like this?",
            "Using coefficients that you get using just the inner product with that phase or image, that's it.",
            "OK."
        ],
        [
            "Um?",
            "So after having done that, after having defined a free transform, you can look at what the Fourier transform on some image looks like, and you can plot amplitude Spectra on Facebook trying stuff.",
            "Do some interesting analysis that's not relevant for this."
        ],
        [
            "Lecture, there's one interesting thing that people discover in again and again, or discovered again and again, which is that.",
            "The amplitude Spectra of its fall off.",
            "They have always this characteristic fall of.",
            "Behavior so these are cross sections of this app.",
            "Well, you take cross sections of this image and average them, and then you draw the amplitude spectrum.",
            "On average.",
            "It looks like this, but you're always going to see this for every image for every set of images, you're always going to see the amplitudes fall off.",
            "That means low frequency.",
            "Components such as these and these are highly represented in the image and high frequency components such as these are much less represented in a natural image."
        ],
        [
            "Oops, I'm sorry.",
            "This is sometimes called 1 / F Power law behavior, which actually happens in many different domains in many different contexts in science and is a bit of a mystery.",
            "For images it must have something to do with the translation in variance.",
            "The fact that enter probably related fact that structure is localized becausw you for structure to be localized for things not to get transported around a lot, you better use slowly, slowly varying basis functions if you want, and that's what nature decided to choose, and that's why.",
            "We get this kind of behavior, but it's a bit of a mystery.",
            "No one really knows exactly why this power law is always come up, and so on."
        ],
        [
            "So you can do statistics on this.",
            "Here's a plot that shows some amplitude Spectra.",
            "The log domain usually off the contours of those for various kinds of scenes, and so you see that certain up type of objects have different amplitude Spectra than others, and so on, and so you can go very far with this and define feature so they so this is from Antonio Torralba and Oliva.",
            "They went on and to find out something like a just feature which somehow uses these for your transforms to describe local image content and so on, and see if can also be.",
            "Related to that, it's actually almost the same thing.",
            "Well, nowadays no one does this anymore.",
            "'cause the neural net actually discovers.",
            "That's a good thing to do for your transforms."
        ],
        [
            "So.",
            "So what does all of this have to do with learning machine learning and so on?",
            "Here is also very well known fact for signal processing people, but not well known fact for machine learning people.",
            "When you do PCA on natural image patches then you will do essentially nothing other than performing for your transform right?",
            "And that is also just caused by this translation in variance an.",
            "So here's a formal way of showing that.",
            "So first of all, if you take.",
            "Natural image patches and crop some lines from then then.",
            "And then compute the covariance matrix.",
            "If you plot that, it looks like this.",
            "So it has some covariance here from the first pixel has some covariance with respect to all its neighbors, and then the thing just gets shifted right.",
            "The next pixel is going to have a covariance structure, which is just a shifted version of this first one, and that is simply be cause if images are translation invariant.",
            "In other words, I can take them.",
            "At any different position.",
            "The statistics have to be invariant, so if there's some relation it can also be higher order relation between this pixel and that pixel.",
            "Then that same relation must hold between this pixel and pixel.",
            "It can only be a function of the distance, it cannot be a function of the absolute position of the pixel.",
            "Just can be cause.",
            "If I have this image in my database and I have a large enough database, then that image approximately is also going to be there.",
            "List of the database is large enough.",
            "So this is for 1D images and it's easy to understand if you plot the covariance matrix.",
            "For actual images, you vectorize them computer covariance and look at it.",
            "Then it looks like this and it basically says the same thing, but it's harder to interpret.",
            "It says the same thing in 2D, right?",
            "It says that the covariance between any pair of pixels is the same.",
            "Again if you shift it around or not, and that expresses itself as this funny pattern here.",
            "Just because I vectorized the image to make it to the plot.",
            "Yes.",
            "Has has what has.",
            "So and yeah, so the question is why does this show up in human image in actual images, even though humans don't actually take an arbitrary image at an arbitrary angle, but maybe focus on people more or something like that, right?",
            "So in fact there are datasets which are centered like there are many face datasets which are exactly centered on faces, and so that's true.",
            "Then in that case, translation invariance doesn't hold and you wouldn't see this effect.",
            "At all in that case, if for natural images you find this effect and it's extremely strong, which.",
            "Bob probably just suggests that.",
            "The human tendency to focus on certain things more than others is not strong enough to counter that effect.",
            "Yet if you look at the center face data set or something like that, you still see Phooey Gabor like structure in the features that you learn.",
            "It's just that it's concentrated around the.",
            "Person for example.",
            "So common features that you would see in a face databases like something like an I or an eyebrow, which is like this WAVY GABA pentin.",
            "And that's probably because even though the the phases are centered then never perfectly centered and still the main type of variation in that kind of data is small shifts at this particular position then right?",
            "And then again you get keyboards, you get invariants.",
            "But that invariances look highly localized.",
            "Then it's not across the whole image.",
            "It's kind of.",
            "Only in small in certain regions of the image.",
            "Any other questions?",
            "So computing the covariance matrix or natural image is the first step to do PCA.",
            "What would be the next step to do in order to do PCA on your image patches?",
            "After I computed the covariance matrix.",
            "Yes, yes video.",
            "I can decomposition exactly and so."
        ],
        [
            "So it's an interesting question to ask is what are the eigenvectors of the covariance matrix?",
            "And so as I said, this is a really well known fact for for signal processing people, so I'm not telling you anything you hear at all.",
            "They are phase us.",
            "It turns out right, so you have no other choice but to do fully transform when you do PCA.",
            "Essentially, here's one way.",
            "One of two ways.",
            "I'm going to try to show today of showing that.",
            "Multiply the covariance matrix with a phasor.",
            "So that gives you a vector, right?",
            "We're dealing with discrete objects here and then.",
            "What is the value of that vector at the TS dimension?",
            "So to multiply C by P. So you take your matrix times vector the TS value of that is just going to be the TS row of that matrix.",
            "Inner product with this vector, right?",
            "OK.",
            "So we can evaluate what that is.",
            "So you take the tea throw of the covariance matrix times the phasor.",
            "If the covariance matrix has this invariant structure, it's not going to depend on the actual value of T&T prime on the of the IJS index.",
            "That's what TNT prime means here.",
            "But it's only going to depend on the distance of these two, and so I should mention, this is the one the case I can show you the two decades in a second, so it's only depending on the distance, and so I can write it as some function.",
            "That's the function of the distance between the two dimensions.",
            "Um?",
            "And so now I can make a change of variables is one way to show and I just called set T -- T prime so I can just replace.",
            "This buys AT&T Prime as a result of that becomes T minus set.",
            "If you're following an so I get T minus set here, but this is an exponential so I can pull this some apart this difference apart and so I get C * 1 exponential times another exponential right?",
            "And so now I can group the set stuff together and I have the T stuff here.",
            "And if I group those that stuff together, I see this is just going to.",
            "I'm sorry actually maybe I should standing somewhere else.",
            "I just get a number that's going to multiply this phase a value here and it will happen for all T for all values, and so that's just a number, let's call that Lambda and.",
            "So that means doing the product of covariance matrix times phasor is the same as multiplying the phasor by a scalar, which might be a complex number.",
            "Well, it's not actually here, but that's technical detail due to symmetry of that matrix, it doesn't really matter, so it's so the phase.",
            "Because of that is an eigen vector.",
            "And.",
            "And so if you do your PCA, you do like of that matrix, you will get phase us as your result, inevitably almost.",
            "There's one approximation that I did, which is the fact that."
        ],
        [
            "This is not the C function is not perfectly shift invariant, it's only shift invariant up to the edge.",
            "If this was a shift with wrap around so you would get this stuff that moves out here would be coming back in here then it would be perfectly shift invariant, But this is not becausw this large value you don't know.",
            "Don't see it anywhere here 'cause it just falls out.",
            "And so translation invariants holds only up to the edges of edge effects."
        ],
        [
            "And because of that."
        ],
        [
            "You look at the components.",
            "They don't look like perfect for your components, but you see that they are essentially for your components.",
            "But before I show them, I'm just going to extend."
        ],
        [
            "The same to 2D, just really quick.",
            "It's the same calculation.",
            "Let me just.",
            "Refer to it for reference, except that now you have a 2D.",
            "Matrix here and the inner product is funny now because it does you know what I mean is a vectorized.",
            "Way of representing things on images, but again, it's a function of distance, and it has the same effect."
        ],
        [
            "And so then, if you look at the features, of course they always look somehow like this, so they're like fully components essentially.",
            "Messed up slightly due to maybe another effect which is sampling bias, right?",
            "So we're only going to do this on a finite set of image patches that can mess things up a little bit, and then it's not perfectly translation invariant, that also messes it up a little bit.",
            "Why?",
            "Why would that be?",
            "OK.",
            "So that answers the question.",
            "OK, OK, alright, so I don't have to answer the question."
        ],
        [
            "So yet another way of saying this is that since the query."
        ],
        [
            "Matrix has this structure here it's it is a convolution 'cause you were going to convolve with this filter here.",
            "Right, because taking the product of this matrix times vector will take this vector, then this vector then this vector, but these are all shifted versions of 1 vector, so this is actually a convolution that you perform and the eigenvectors are.",
            "Faze us, and so they look like this."
        ],
        [
            "So there are two famous theorems.",
            "The dark the maze, the main kinda.",
            "Lamp posts for signal processing people.",
            "One is one that I think most of you are very familiar with.",
            "Convolution in the time domain is multiplication in the frequency domain.",
            "So instead of convolving you might as well do a Fourier transform to an elementwise product of the Spectra and then do an inverse Fourier transform.",
            "That's going to have the same effect as convolving two signals with one another.",
            "That can be used and it has been used in confidence.",
            "For example, at this recent paper by Jan and some students, but other people have done it and and over the years and tried that and so on.",
            "Sometimes it helps.",
            "In terms of speed, sometimes it doesn't.",
            "It depends on the size of the filter, the size, the number of channels, the size of the image, and all kinds of other things.",
            "So it's one of these constants that Lyon refer to, it's not.",
            "It's not completely obvious if you should do this or shouldn't do this.",
            "The reason it can help is that you can do the.",
            "If you can use the FFT to do your free transform, which is very efficient.",
            "Amazing scientific sorry engineering hack kinda.",
            "Amazing idea to compute this.",
            "Formula here in unlock activation lock and time rather than quadratic, which you would need there.",
            "OK, so that's well known.",
            "I think people are familiar with that."
        ],
        [
            "The converse is true too, though, and I think it's much more important.",
            "Fact that explains much more than the first one, so multiplication in the time domain is convolution in the frequency domain.",
            "So the opposite is true as well.",
            "Take two signals, multiply them elementwise.",
            "That did it.",
            "That's the same as if you had done the Fourier transform and then convolved the Spectra the proof."
        ],
        [
            "Or that is analogous to the proof for the first sort of the opposite of that, and I just leave it there for reference because I don't have enough time, but it's just a little playing around with indexes and so on, and this is not actually a proof.",
            "It's a proven quote, so it's proof sketch that's not very formal, but it's I think it's good enough to believe that this is true.",
            "Or you would believe it anyway, because it's written in textbooks so.",
            "OK, so."
        ],
        [
            "But one of the so there are many effects that are caused by the 2nd or an interplay with the 1st and the 2nd theorem.",
            "Some things that are cost at least to some degree by this went on effects that signal processing folks have to deal with it all the time, like ringing.",
            "When you filter images or aliasing when you do sampling.",
            "So strange effects that you see in images that you wouldn't want there, I wouldn't expect and so on, but there's one effect which is maybe the most important for us right now is called leakage, and I'm just going to try to.",
            "Well, go over this leakage effect real quick and explain why it's important and how an engineer would deal with it and how curiously an Euronet decides to deal with it as well."
        ],
        [
            "So here's here's leakage and."
        ],
        [
            "So here you see a sine wave which perfectly fits into that window, and so here you see the spectrum.",
            "So one thing I didn't mention is that spectral are symmetric.",
            "If the signal is real.",
            "It's also a little calculation, and so the spectrum looks symmetric here.",
            "So just forget about that.",
            "Just imagine I had just shown it from here on or something like that.",
            "So you see, there's one component strongly present that happens to be the 16th index in my vector in my expansion, which is the phasor corresponding to the frequency one, and so that's the signal there.",
            "So it's perfectly fine and I see.",
            "The Free Transform reveals what the signal is.",
            "Um?",
            "If I had a signal that has twice the frequency, I would see this second guy here pop up instead and then three times the frequency I would see this guy pop up and so on and divide.",
            "So superposition of those I would see several of these guys pop up.",
            "But what happens if I just ever so slightly expand the frequency so that I do not have an integer multiple of the window links as my frequency?",
            "So I just make this a little bit longer.",
            "This signal, then since I only have integer multiple frequencies at my disposal?",
            "To model this signal I I wish I would see.",
            "Ever so slightly to the right, a guy pop up and tell me that's the that's the frequency component is.",
            "It's 1.2 for example, but that's not what you see because you don't have the 1.2.",
            "You only have your 32.",
            "In this case, coefficients from that expansion from that formula, and so that's."
        ],
        [
            "But you see, you see some other stuff happening.",
            "Some of these guys go up and.",
            "What else can you do?",
            "You can only use the values that you have and that does not mean that the Fourier transform loses information that it cannot model this.",
            "It just means that in order to model a non integer frequency, it has to make use of integer multiples of frequency.",
            "To model that sequence right?"
        ],
        [
            "If I change it even more."
        ],
        [
            "More than other things happen and goes even worse, so it decides to do matter to to represent this 1.5 frequency by using all these coefficients here right?",
            "So leakage is a problem that you should think of as leaking into.",
            "Imagine you would want to detect a frequency component you would be interested in detecting this frequency over there or some other like this frequency here.",
            "Well, unfortunately if you, say apply a threshold here and say this frequency is present.",
            "Even though it is present in this case because it needs you need it to model this 1.5 frequency, you may not want to detect it.",
            "You may actually be interested in the true 1.5 guy which you cannot see and so.",
            "Frequency components that cannot be modeled leak into the others and mess up analysis that way, right?",
            "So it's hard to distinguish this guy from being a real.",
            "3.",
            "Three times window links.",
            "Feiser from one that's just has been used by the FFT by the DfT to model this weird signal up there.",
            "There's a more."
        ],
        [
            "Nicola Way of describing the same thing.",
            "Which uses so-called sync functions, which are the Fourier transforms of boxes.",
            "So I'm just going to refer to this real quick."
        ],
        [
            "Cause that motivates how to fix that problem.",
            "If you look at this signal over there that's you can always think of it as a periodic signal and you would want the Fourier transform to give you the same result for the periodic extension.",
            "So imagine this wasn't just a window of 1 oscillation here, but it wasn't.",
            "It would be infinitely extended.",
            "Here.",
            "It would be an infinitely long sine wave.",
            "Of course, the Fourier transform of that infinite guy should look just like this because there is nothing other than this frequency sine wave present in your signal.",
            "But that's not what you give the DfT.",
            "What you give the DfT is this infinite sequence that has clearly this spectrum, and you might apply it by a square window if you like, right?",
            "You can think of this as an infinite signal, which you multiply by a window that looks like this.",
            "It just cuts out one piece of it.",
            "Now an interesting fact is that a square window has a so-called sync function.",
            "As the Fourier transform and multiplying elementwise.",
            "Buy a window is the same as.",
            "Convolving the Fourier transform with the full transform of that window, which is a sync function.",
            "And it turns out the sync function is an interesting object.",
            "It has zero crossings at just.",
            "It looks like this it has zero crossings at just the right positions in order not to mess up the Fourier transform for frequencies that are integer multiples of the sequence links.",
            "That's why this works perfectly fine.",
            "But this convolution with the sync function does have the zero crossings, but if you apply to something."
        ],
        [
            "Like this?"
        ],
        [
            "It's going to start shuffling things around in the spectrum.",
            "This convolution operation and so now stuff leaks into other parts of the."
        ],
        [
            "Of the spectrum, so that cannot be avoided, right?",
            "As I said, you're going to have to use what you can do.",
            "The FFT has to use what it can to model the signal, and it can do so losslessly, and so it does what it has to do, but.",
            "It turns out that if you don't use the square window but another differently shaped window.",
            "Before you do your free transform.",
            "Implicitly, you can change the leakage effect so that it's maybe less undesirable for your application at hand, so here's an example of."
        ],
        [
            "This is 2 times or three times or something.",
            "Window length perfect DfT."
        ],
        [
            "This is changing the frequency and things get messed up and so I imagine you would have been interested in distinguishing these two components or these two components, or something.",
            "Your threshold kicks in and you would think this component is present, but it wasn't because that's not what you were interested in.",
            "Well, this was the square window, so why not just use it?"
        ],
        [
            "Different window, such as a Gaussian window for example, in which case.",
            "You still have leakage.",
            "As I said, you cannot avoid it, but the way that stuff leaks around into other bins can be changed anfora Gaussian it's more localized, so you're going to look more into neighbors and less into faraway guys.",
            "And so for example, all this lifted stuff here, which accounts for that signal is just fine, so detection would be perfectly fine here, but the neighbors here are messed up more 'cause it's a Gaussian window, and you're going to have to leave it somewhere that information, right?",
            "But if you're willing to give up on it a little bit of frequency resolution.",
            "You might just want to do this in order not to mess things up far away instead."
        ],
        [
            "Spectrum and then so this is 1."
        ],
        [
            "Window if I use another Gaussian window."
        ],
        [
            "This is with.",
            "With this is 1 this."
        ],
        [
            "There's a smaller one.",
            "Then you get all these different kinds of ways of leaking around.",
            "So you can actually push."
        ],
        [
            "That would be extreme.",
            "Just going to talk about that in as well."
        ],
        [
            "I'm going to just."
        ],
        [
            "Skip ahead."
        ],
        [
            "Second, there is a.",
            "The principle called the uncertainty principle, which is actually related to the physical uncertainty principle in various subtle ways, which states that as you change the size of your window, you're going to trade off accuracy in the time domain for accuracy in the frequency domain, right?",
            "So if you take your whole signal which looks like this, then you get a spectrum that might look like this.",
            "Now you shouldn't do that because of leakage problems and so on.",
            "So you put a window on top.",
            "You know that makes the signal look like this, and so now these guys get wider.",
            "So you lose resolution.",
            "Hear you if you make the window smaller then this gets worse and worse.",
            "And there's actually an easy way to show that it's because the Fourier transform of a Gaussian turns out to be a Gaussian.",
            "You just go one over the variance, and so if smaller Gaussian here has a wider transform here, and so the smaller you make the Gaussian, the wider you spread things around in the spectrum, and so.",
            "One reason this is a principle that's of interest maybe is that you might be interested in doing a Fourier transform locali just at a little piece of an image or a signal, and in that case you need Windows, and so you have this problem.",
            "This is going to show up, and so now you have a choice to make.",
            "Do I want very, very good temporal resolution, in which case I mess up my spectrum.",
            "Or do I want to have?",
            "Good spectrum spectral resolution, in which case I'm going to have to pull over a large region of space or time essentially.",
            "OK, so."
        ],
        [
            "So I just mentioned it is doing that is incredibly common in speech processing, and it turns out in Vision tour it's called a short term short time Fourier transform IFSC DfT.",
            "People do that all the time and continents do that all the time.",
            "Um?",
            "As it turns out, and it's just nothing other than doing a free transform, not on your whole signal, but on a little piece of signal.",
            "Um?",
            "And in order to do that, you could just cut out a little piece.",
            "But of course you wouldn't do that.",
            "You would use a window function, which is better behaved than it, and so one window function to use is a Gaussian, and it's very common, and so one way to call an S TFT.",
            "So doing a Fourier transform here, here, here, here, here, here, here, and every time using a Gaussian window is called the Gabor transform and.",
            "And that's why both features come from.",
            "In one day, the results are usually the amplitude of the result is called spectrogram, right?",
            "Everyone has seen these plots of spectrograms.",
            "Whoops, I don't have it here.",
            "Lost it maybe.",
            "We'll show later, but in 2D you get the same thing."
        ],
        [
            "Um?",
            "Just that the spectrogram is not.",
            "For every position in time, a stack of frequencies, it's rather for every 2D position of image.",
            "It's a stack of frequencies which in turn are two dimensional, so it's a messy object and actually in confidence people call that feature Maps.",
            "Instead.",
            "Any questions?",
            "So how can you get a Gabor feature then to do that in practice?",
            "And literally, people have done this in practice all the time, especially very successfully for face recognition and things like that, you take your complex wave, which now you see it's two.",
            "It's properly plotted as two.",
            "Wait?"
        ],
        [
            "Take a Gaussian window."
        ],
        [
            "And then you multiply your waves by this Gaussian window.",
            "So you have to do that with both and that defines what's called a Gabor feature.",
            "It can have many many parameters becausw you can decide how to choose the Gaussian, make it wide or small, or make it not axis aligned or axis aligned.",
            "Change the covariance and so on.",
            "As other parameters like where to place your Gaussian and.",
            "Which frequencies to choose and which not to choose?",
            "Which frequency frequency vectors?",
            "In other words, which orientation, which frequency and so on, and so there's a whole lot of parameters and but here's one way to define such a function.",
            "So of course messy to play with all of these.",
            "These are hyperparameters essentially in order to model something.",
            "But Luckily people stop doing this, of course, and just train your Nets instead.",
            "But lo and behold, the neural net decides to do nothing other than that in the end."
        ],
        [
            "Of course it chooses the right parameters."
        ],
        [
            "Are you while it's training, this uncertainty principle extends then also to 2D, by the way, so and it's it.",
            "It goes further than than just the position in uncertainty.",
            "For example, here's an example from the from the Hoover in book where you can have a feature like this one which is highly localized along this axis.",
            "Um?",
            "But it would fire on slanted Lions that are anywhere within this region pretty strongly.",
            "You can change that resolution of this land here by making this guy longer maybe, but as a result of that you lose resolution along this axis, or Even so if you have a little bit of feature in the image sitting here or here or here, you're not going to be able to distinguish them.",
            "But again, if you train neural Nets, you don't care because it's going to choose the right features for you."
        ],
        [
            "Through backdrop.",
            "So doing an S TFT.",
            "Means that you take your GABA feature.",
            "And scan it across the image cause you want to do the FFT or whatever everywhere.",
            "And confident does the same thing.",
            "It learns Gabor features.",
            "Which are then scanned across the image to evaluate them everywhere.",
            "Um?",
            "And so that's pretty much, well, that is a convolution.",
            "And convolution is a filtering operation that affects the spectrum of the resulting image, right?",
            "So you're going to in those feature Maps that you get in your convenant.",
            "Each one of them is going to have been a filtered version of the image, which means as an image with whose spectrum you must in some way.",
            "So in what way does that feature then mess with spectrum?",
            "Can someone tell me what is the amplitude response of a GABA feature necessarily by taking everything together that we talked about?",
            "Can you say it again?",
            "Yes, exactly.",
            "So the answer is you multiply.",
            "The spectrum by a localized Gaussian somewhere, and so the reason is you do convolution in the original domain, so that's equivalent to doing multiplication in the answer in the special domains means you do multiplication in the spectral domain.",
            "And what is the spectrum of?",
            "A couple of future.",
            "First of all, it's just a peak 'cause it is, well, it's a slightly widen peak.",
            "Maybe because it has discussion filter on top of it, a Gaussian envelope.",
            "A sine wave is a peak, right?",
            "The spectrum is just a dot.",
            "Multiplied with a Gaussian, it's a smeared out dot.",
            "Now convolving the image with that thing means multiplying with that smeared out dot the spectrum, right?",
            "Is everyone following OK?"
        ],
        [
            "So it's a localized blob in the frequency domain, so the the the the the planes in your feature map that you get from your continent.",
            "I just filtered versions of the original image where you highlight we take the spectrum of the original image and you have a blob that just highlights a little piece of spectrum, little orientation and frequency.",
            "For you.",
            "So because of that, in especially in the neuroscience literature and so on, people called Gabor features, often oriented bandpass filters.",
            "They they zoom in onto a little piece in the in the spectrum and then they go down everything else and it's oriented cause the spectrum in 2D is it to the thing because you have frequency vectors and not just frequencies."
        ],
        [
            "Also, here's the spectrum that I was looking for, so this is the spectrogram in 1D, right?",
            "So this is the amplitude spectrum of a signal and it's just a stack of.",
            "Amplitudes are frequencies.",
            "And so now let time turn into a space, make a 2D.",
            "Then you get these stacked vectors.",
            "Here at every position.",
            "And then that would be called a feature map in a convolutional network.",
            "But it's really to me.",
            "It's really the same thing, right?",
            "It's just if you ended up learning about like features, you will just have done the same thing.",
            "In two D and that's it.",
            "So engineers found that along time ago, right?",
            "So if this is absolutely essential in speech processing, you have to do this first as a preprocessing operation before you continue and people still do this right, you put a neural net on top of this and then do speech recognition or something.",
            "Um?",
            "And it's kind of cute that neural Nets figure out the same thing."
        ],
        [
            "So if you take a continent, and you have Kobol like features on the first layer.",
            "The code that doesn't really work, typically.",
            "Unless you use something like a Relu or sigmoid for tonnage, it kind of maybe works too, but that's for kind of the wrong reasons, because as someone pointed out in the in one of the lectures recently, it's because the network starts to learn.",
            "Something, despite being crippled in some way, right?",
            "Because back prop is so magical that it still finds a way to do something sensible.",
            "But typically it's common to use.",
            "Relevance or signals, or something that goes to zero as you get more and more negative.",
            "It's what you do before the pooling, typically right?",
            "So you expect your features and you have relevant and you pull over them or something.",
            "So that's so.",
            "Imagine you have a future somewhere you are scattered across the image.",
            "You have a set of responses for that feature.",
            "Since you move that thing around you sort of change the phase, at least in the direction of the waves you change the phase a little bit, so you sort of try out that feature at many different phases because the shift of the feature is almost like a phase shift of the of this way of writing, at least locally.",
            "Now you try out this feature at many different phases and then you rectify, which means you only keep positive responses.",
            "Or let's say you do Max pooling.",
            "In fact, then you don't need the rectification.",
            "You say you do Max pooling.",
            "Then you're just going to keep the strongest one.",
            "So that's sort of like you try out a feature at all phases and then you keep the strongest response.",
            "And that's almost like an amplitude spectrum.",
            "You just look at how strongly is this feature represented at this position in the image, regardless of what its face is, right?",
            "It turns out in biology of neuroscience and elsewhere, people came up with other ways of getting amplitudes.",
            "Well, in engineering people just sum the squares they get.",
            "The length of this complex value as another way to get amplitudes, right?",
            "So when a biologist talks about complex cell, then they used typically to means something more like this.",
            "You have a phase shifted a pair like a sine and cosine or something.",
            "And then you sum over the squares of that function and that's this block.",
            "Here is what they would call a complex cell.",
            "I'm not aware of any work that tries to use some mechanism that looks like this in a convent.",
            "Maybe it's not necessary becausw this rectification followed by pooling or Max pooling is going to.",
            "Be sufficiently equal to this such that you will end up getting something like the amplitude spectrum on your bottom in the bottom of your network at least.",
            "But it would be worth trying, probably cause yeah.",
            "Yes, well, it's another way to school this exactly.",
            "It would be L2 pooling or square pooling, but it's not worth.",
            "Typically pulling refers to pulling over spatial positions, and this is pooling across feature Maps, and it's not just pulling across feature Maps, it's very very specific kind of pulling your feature Maps come in pairs here in sine, cosine pairs, and then you pull only over these pairs at a time, right?",
            "So this is a bit more specific than than just what people call Square pulling or anyone pulling, but it is an instance of that, absolutely.",
            "Even though this is a much older thing, right as square pooling came up, sort of, the term came up recently.",
            "So you need this pass that can be sign or cosine, what actually they don't have to be signed in cosine, sine and cosine are going to spend your complex plane like this, but it does.",
            "In practice, you don't care if you spend it like well, like this maybe well for you, it's going to be like like, But anyway, you can also phase shift the whole thing.",
            "It's not going to make any difference you have this 2 dimensional subspace that complex plane.",
            "And if you spend it with using these vectors, these vectors of vectors doesn't matter.",
            "On paper, I mean is right down sine, cosine, you will ended up having used this basis here, but in Euronet can decide to use any other one.",
            "Since this degeneracy holds that you don't have to use sine cosine, people don't call this sine cosine pair, they call it a quadrature pair.",
            "That's another word of.",
            "Characterizing this pair of two features that are waves which are 90 degrees out of phase, which is exactly what's going to help you span the 2D space, which is a complex plane or one frequency phase of 1 particular frequency."
        ],
        [
            "OK, so I think I want to conclude by.",
            "Trying to at least get a little bit in the direction of explaining why.",
            "Why do we get?",
            "Gabor features when we train these networks, even though we only showed that we get fully features when we do PCA.",
            "Um?",
            "So there's no, I don't have a real answer to that.",
            "I if someone knows any references or something where people have been trying to explain that, please let me know.",
            "I think we have some hints, some ideas, even though this is not a complete picture.",
            "So, so here's another way.",
            "First of all.",
            "Of showing that.",
            "Why PCA you?",
            "It's for your components necessarily, and there's a very different way of showing that actually so.",
            "And it talks about invariants so.",
            "Say your data density is governed by some transformations, let's call them T. And it can be a whole bag of transformations.",
            "So in all practical cases, as we've seen, this is going to be the set of shifts.",
            "Local small shifts like shift your image a little bit to the left to the right, up, down, and so on.",
            "So those are the transformations T shifts Interestingly happen to be orthogonal transformations.",
            "They are not talking to orthogonal matrices that that can perform that shift.",
            "Um?",
            "And I'm talking to matrices useful in this derivation, so that's why I just assume this is orthogonal.",
            "But we know it holds for shifts, so we're fine.",
            "And orthogonal matrix is a matrix whose inverse is its transpose.",
            "So to invert it you just transpose it.",
            "And you can think of it intuitively as a matrix that just permutes basically pixels or dimensions.",
            "Just shuffles around ink on your endless digit.",
            "It doesn't introduce new ink or.",
            "Remove ink, just move stuff around.",
            "Kind of basically a permutation.",
            "And the shift is obviously a permutation.",
            "It shifts all the pixels by 1 to the left, or maybe by 1/2.",
            "Then it includes some interpolation, but it's still essentially is something like a permutation, so.",
            "So imagine that your density is invariant to shifts, so then the log probability of some image X.",
            "Has to be the same as the log probability of any shifted version of X.",
            "The probability of this image must be the same as the probability of this.",
            "And Interestingly, you're going to have a whole orbit, right?",
            "So if the probability of this is equal to the probability of this, then it must also be equal to the probability of this and this and this and this and this and this and so on, right?",
            "'cause I can just replace X by TX on the left and then you get T square X on the right so the whole orbit that under the transformation is going to have to be density has to be constant.",
            "Yeah.",
            "Um?",
            "So let's just plug in the Gaussian here for a Gaussian you hav E to the minus some covariance matrix, and so on, and then left and right the normalization constants are going to cancer.",
            "I already took the lock, so you're going to get this stuff that's inside the.",
            "The exponential right is something like X transpose covariance matrix.",
            "Oh sorry, this T shouldn't be there, so you can see that I just wrote this down last night.",
            "And then.",
            "That's the same as X transport.",
            "No plug in TX for X, you get X transpose, transpose covariance, TX.",
            "So does everyone agree that is correct?",
            "Now this has to hold for all X.",
            "This invariants and so.",
            "These matrices have to satisfy that.",
            "To make that whole 4X.",
            "So that means Sigma inverse the inverse covariance matrix has to be the same as T Sigma inverse T. But there's no way other way of saying that Sigma and T have to have to be matrices that commute.",
            "So let's multiply on the left by T. That's going to cancel this away, and we're going to get T Sigma equals Sigma T, and so that means these two matrices commute.",
            "And So what about two matrices that commute?",
            "They have the same eigenvectors.",
            "And so that means the eigenvectors of the covariance matrix have to be the same as all the eigenvectors of all the transformations T that I could choose.",
            "So Luckily, all shifts have also the same eigen basis and it happens to be at the full year basis.",
            "So in other words, all made all permutation matrices T which are going to end up shifting something.",
            "I'm gonna if you do.",
            "I cough that you will see a wave.",
            "And so that means the covariance matrix has to have full components as.",
            "The convectors also.",
            "So I don't have the result for the.",
            "For something more interesting than PCA, but I am going to try to write something down.",
            "Using.",
            "Johnny works, wonder where.",
            "However, you pronounce that.",
            "But this is really speculative and kind of.",
            "In ongoing thought process, I don't think this is necessarily true, or maybe not even interesting.",
            "So let's say you have an auto encoder.",
            "For the sake of the argument, but you can also think of K means cluster oops wow.",
            "How did?",
            "How did I do that?",
            "I don't know.",
            "It's this thing.",
            "So they're going to be points on the manifold.",
            "I think you're sure talked about it quite a bit, and whoops, I think maybe I should use the whiteboard.",
            "So you can have a point X on a manifold and if that point lies on a mode of the density, which we call manifold typically, then the reconstruction of the of X under the autoencoder is going to be X.",
            "So we're not going to move that thing around, right?",
            "So that's yeah.",
            "Because it is a point which the auto encoder doesn't mess with.",
            "It's a point that the auto encoder leaves alone right cause.",
            "Well, I was hoping that you're going to explain that in your lecture becausw autoencoders learn densities.",
            "Learn the data density.",
            "Basically, as you're going to show, presumably next week, and so the derivative of the density is going to be 0 at that mode, and so you're not going to be moving around.",
            "Let me just leave it at that first let me just.",
            "Oh yeah, yeah.",
            "Right?",
            "Oh yeah, OK, so that's actually very interesting point.",
            "So so if you think of the mode as a point, right imagine you have like a mixture of Gaussians, you have a mode which is just a single Gaussian somewhere.",
            "That is not compatible with this view here and is not compatible with reality becausw if your density is translation invariant, then if you have a mode at some position X, there must be a mode nearby at the shifted X, and in fact along the whole orbit of the transformation class.",
            "But there cannot be such a thing as a Gaussian as a blob as a mode, just becausw of translation invariants.",
            "It's a very, very fundamental thing, kind of the mode has to be smeared out.",
            "If your translation invariant, if not, then you're not invariant, but maybe then there's no opportunity to learn anything and you can just go home right?",
            "So if you think there is something you can learn, their invariants says that means modes must be stretched out.",
            "Right Classic is the wrong thing unless that manifold that you get from those transformations is so low dimensional that you can just tile the space densely with little blobs.",
            "Somehow that's right exactly.",
            "So if that's true, then I would claim that also R of T X = T X is true, so these are equivalent.",
            "So if you reconstruct X perfectly, you must reconstruct the translated X perfectly, because we want to get the whole mode.",
            "Otherwise the autoencoder sucks.",
            "And.",
            "So.",
            "Right, so these two are true and now let me mess around a little bit with these symbols, so I'm going to say R of TX, since this is all the same, is the same as T times.",
            "Are of.",
            "X because I can just plug in our of X for X over here right?",
            "X can be replaced by RX right?",
            "So let me just do that mathematically.",
            "Now I'm going to say the reconstruction of the transformed image has to be the same as the transformation of the reconstructed image.",
            "Whenever things get a little bit less messy, Unfortunately, this seems like it has something to do with commuting between the autoencoder transformation and so on, which is nice, but I wasn't able to make this.",
            "Into a.",
            "2nd and first line I just replace X power of X, which I'm allowed to do things to the first line.",
            "Right, I just claimed these two hold and they have two otherwise.",
            "So how much?",
            "OK I have 10 minutes left so that should work so.",
            "So what is the auto encoder?",
            "Does someone remember what another code is?",
            "It's a decoding weight matrix times an activation function of some encoded and let's just tie the weights here.",
            "So sorry, this should be a W. Oh, I wish I had started on the whiteboard.",
            "So.",
            "So this is the definition of Norco.",
            "Does everyone agree it's?",
            "Oh yeah, I'm just talking bout OneNote, layout and chorus?",
            "Yes, yes.",
            "And also not talking about confidence, so I cannot claim anything about confidence or something.",
            "OK, so I think I'm just going to move the screen up and.",
            "And I'm going to cover.",
            "This area was a window.",
            "So so out of X = X and R of T, X = T X.",
            "That's probably true.",
            "Yeah, that's right, that's a good point.",
            "Let me right here.",
            "Our FX is equal to X and that's the same or that has to hold in addition to R of T, X = T X from which I just infer that R of TX.",
            "Equals T * R of X, which is another, which is sort of to say that the auto encoder commutes if you want with the translation process.",
            "But they don't get in each others way in some way.",
            "Also, yes, absolutely yeah, if you move a little bit and then reconstruct is the same as having moved in the reconstruction, absolutely.",
            "So that would be a good auto encoder.",
            "OK possible.",
            "So the auto encoder.",
            "Sorry.",
            "So what is an autoencoder?",
            "So T times?",
            "What do I have to write here?",
            "As I said, it's a decoder matrix times an activation function applied to a linear projection, maybe plus biases or something.",
            "And that's has to be the same as W applied to H. Sorry, W times this reconstruction is going to be the decoder matrix applied to H of the transfer.",
            "Latest image.",
            "Right, so this has to hold.",
            "Now we're screwed because H is a nonlinear function.",
            "Unfortunately cannot do any analysis, and you're kind of lost.",
            "Unfortunately, right so this ends the discussion first of all, but you can come up with the standard trick to linearize around X or something and replace the autoencoder by it's linear approximation, right?",
            "Just let's not use out of X, which is potentially nonlinear mapping, let's use.",
            "The Jacobian of our facts as an approximation for the auto encoder at that position.",
            "Then it's fine.",
            "There's another argument that's easier eventually, but I'm not going to go into that.",
            "If you use reluz, you will see you're only going to use a subset of hidden's, and then you end up.",
            "Actually, it's actually linear everywhere, so you're going to be fine if use red autoencoders.",
            "But if you want to extend this argument to any auto encoder or K means or something, then you're going to have to go through this.",
            "So let's linearize what's the derivative of this that you just go back prop here.",
            "So you're going to do something like W. And I'm going to transpose the whole thing to be able to apply it to the input.",
            "It's going to be a diagonal matrix of the derivatives of this times W transpose times TX and the same analogue expression here, so it's going to be times W. Times the diagonal matrix of the derivative times W transpose X right?",
            "And so let's just call this diagonal matrix D or something WD.",
            "W transpose TX is the same as tee times WDW, transpose X and that again has to hold for all XI claim now, and so if that's true then that has to hold for the matrices themselves.",
            "Again, right?",
            "So we're going to have to see.",
            "WDW transpose T is the same as.",
            "TWDW transpose, where WDW transposes the linearized autoencoder.",
            "It is an autoencoder but it'll take an X and reconstruct it linearly as a matrix, but that matrix is Jacobian of euro encoder so it is actually.",
            "The auto encoder locali.",
            "So that matrix has to commute again with the translation operation here, and so it has no other choice.",
            "This WS here.",
            "Have no other choice but spending the eigenspaces of this T. So for erelu the derivatives are going to be 0 or one, so this D is going to have zeros and ones on it, and so you're going to end up with a subset with a small subset W here and here.",
            "And so W transpose WWW transpose has to be equal to.",
            "Times tears are equal to tee times WW transpose.",
            "In that case, right?",
            "If the derivative is 1 and so.",
            "That just means the features have to commute with the transformation class.",
            "And so if the transformation class is.",
            "Shift.",
            "Well, the features must be fully components then.",
            "But there's one Riddle that's left, of course.",
            "Which is.",
            "You don't get for your components.",
            "You get a couple of features which are full components times a Gaussian yes question.",
            "Yes.",
            "AB is the 2nd.",
            "With this one here.",
            "OK. Possible I don't see that right now, but we can maybe discuss that offline.",
            "Well, it's very possible that there.",
            "Yeah, it's possible.",
            "Yeah yeah.",
            "So this is stuff that I wrote on last night, so this is not well.",
            "So this is like thinking in action, right so?",
            "So if you have to commute.",
            "With.",
            "For this shift feature of this discovery components.",
            "Um?",
            "But we don't let the autoencoder, although came in fact came, is clustering is a special case of this for a single W. Just another day here.",
            "But we don't let it be 4 years becausw we add another constraint, right?",
            "So in all sensible feature learning models in all future learning models that end up giving you sensible gobble like features.",
            "You have very very sparse hidden's like all the way through this very very very rare.",
            "Strange exceptions like this on contractive autoencoder today, short.",
            "So if the autoencoder comes up with three components that cannot be sparse because they are not sparse, they're going to fire all the time.",
            "If I show any image, so it's going to have to back off in some way to satisfy the sparsity requirement, so it cannot give you full pure for your components.",
            "And so the only answer that I can offer is to say well, then it does what engineers do and makes an increasingly small Gaussian window around that wave that we component.",
            "To satisfy the sparsity requirement and still be approximately commutative with the translate transformation class.",
            "Um?"
        ],
        [
            "Right, so this is a little bit of research, research, and action.",
            "Yes, thanks, thank you.",
            "My last slide is just a question.",
            "Or just maybe you thought how?",
            "How the same?",
            "Or if the same idea should apply to higher layers?",
            "And I think it's the same idea should apply to higher or higher layers in the network in exactly the same way, because this T shouldn't be.",
            "Doesn't have to be translation, right?",
            "It can be any kind of transformation, and if it's if the tears translation?",
            "Incidentally, that matrix is called circulant matrix.",
            "That's the matrix that shift things around.",
            "And there's also a so called block circulant matrix which shifts images around vectorized images around.",
            "So the eigenvectors of this are for your components.",
            "That's why everything works out nicely.",
            "And now if you shift a little bit, as I said, you're going to have to cover the whole orbit.",
            "Otherwise you're not going to.",
            "It's not going to.",
            "Bich"
        ],
        [
            "Correct somewhere?"
        ],
        [
            "In higher layers, you're not going to see shifts, right?",
            "The hiddens are sorted in some completely arbitrary way.",
            "You can always replace one by the other.",
            "You're not going to see nice structures after everything gets shifted around.",
            "According to the Invariances variance in the data.",
            "But you're going to see some kind of permutation or approximately permutation matrix on those hidden's that that will give you invariants of the responses of those hidden's.",
            "And there's a whole deep far reaching area called Fourier transforms on groups which tries to tease apart this kind of.",
            "Orbits that you would find in a permutation.",
            "So you could imagine that those permutations have limit cycles like one guy goes to the other and then back and forth, and then one unit travels from here to here and from here to here from here to here and eventually back at the beginning and so on.",
            "All of these if you sort it, your vector in the right order would just look like little few years because they would just be values that travel around.",
            "Let's get shifted around right?",
            "So free free analysis still applies I think, and the same exact same thing.",
            "Can still happen in higher layers of a network, it's just that they don't express themselves as fully ECOMOG aboard components that are up there.",
            "And then much harder to analyze becausw.",
            "It's hard to look at them, right?",
            "It's hard to see what they do, but it's certainly worth thinking about how to further characterize them or to understand what these invariants in higher layers potentially could be OK, I'm just going to stop here, and if you have any other questions, even though I'm running out of time, you may ask now.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks yeah sure so so that's true.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about relations and analogies and stuff like that a little bit and how that and how that relates to vision.",
                    "label": 0
                },
                {
                    "sent": "In my second talk, but.",
                    "label": 0
                },
                {
                    "sent": "To set the stage for that, so to speak, I'm going to talk about something much more basic today, which is basically Fourier transforms on images, and so the reason for that is for one thing, it sets the stage for the relation stuff that I'll talk about later.",
                    "label": 0
                },
                {
                    "sent": "The other is that it's kind of underappreciated.",
                    "label": 0
                },
                {
                    "sent": "I think in the deep learning community, the fundamental role that I think Fourier transforms play.",
                    "label": 1
                },
                {
                    "sent": "In understanding.",
                    "label": 0
                },
                {
                    "sent": "Why features look the way they look and why they work and how they are related to invariants and equivariance and all kinds of things?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Leon Bottou and his talk 2 days ago also showed this picture, which is.",
                    "label": 0
                },
                {
                    "sent": "The kinds of features that cells in various parts of the brain like to see as found by Hubel and Wiesel in their famous experiment in the 50s or 60s.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so it's very common to say, well, these are edge detectors where everyone says edge detectors, which is, I think, due to 1015 year old paper by Terry Sinofsky and Tony Bell, where they call them edge detectors.",
                    "label": 0
                },
                {
                    "sent": "But I think it's not quite an accurate characterization becausw.",
                    "label": 0
                },
                {
                    "sent": "While this might detect something like an edge, this doesn't really look like an edge detector, and often these these features are not really detecting edges, but they're really WAVY and have many periods and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So the brain finds them through learning maybe or through a biological prewiring or whatever.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Neural Nets also find them and exclusively almost find these right.",
                    "label": 0
                },
                {
                    "sent": "So whenever you train a network and it.",
                    "label": 0
                },
                {
                    "sent": "Comes up with these kinds of features.",
                    "label": 0
                },
                {
                    "sent": "You already know that it kind of works.",
                    "label": 0
                },
                {
                    "sent": "You know that the performance is going to be reasonable somewhat, and if not, then you can already be pretty sure that it's not reasonable.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not going to work right.",
                    "label": 0
                },
                {
                    "sent": "So Pascal yesterday showed autoencoder features that are broken, and those that look like this, and these are always the ones that give you reasonable performance.",
                    "label": 0
                },
                {
                    "sent": "So this is features from Alex Net.",
                    "label": 0
                },
                {
                    "sent": "You've seen this picture 1000 times.",
                    "label": 0
                },
                {
                    "sent": "Probably sometimes they can be in color, sometimes in Gray, and depending on how you set up your model, some learn the color some down the great values in.",
                    "label": 0
                },
                {
                    "sent": "Whatever.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you train something completely different like I see a.",
                    "label": 0
                },
                {
                    "sent": "You get these kind of features too.",
                    "label": 0
                },
                {
                    "sent": "Maybe they look a little bit different, but the general characteristic is the same, of course.",
                    "label": 0
                },
                {
                    "sent": "So by the way I'm going to borrow a lot of pictures from this book here, natural image that it sticks by upheaval in Huy and Hoya.",
                    "label": 0
                },
                {
                    "sent": "Which is a great book on natural image statistiques.",
                    "label": 1
                },
                {
                    "sent": "And a lot of material that I'm going to present this borrowed from that book.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can also do other things like sparse coding was a much older.",
                    "label": 1
                },
                {
                    "sent": "This is a much older plot from the 90s or so Bruno's house and field, and then you also get these features of constant.",
                    "label": 0
                },
                {
                    "sent": "And here you often also see that it's not really an edge detector, it's something different.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also do other things, like regularised auto, nicely regularised auto encoders like denoising autoencoders as Pascal mentioned yesterday you get these features of course.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something kind of interesting that I just wanted to throw in here is if you train in a contractive autoencoder.",
                    "label": 1
                },
                {
                    "sent": "Even if you do the wrong thing and trying to contractive autoencoder the way that Pascal defined them, but you just set the contraction penalty to a negative value so you don't actually contract, but you make things worse from the contractive POV.",
                    "label": 0
                },
                {
                    "sent": "And you look what the features look like.",
                    "label": 0
                },
                {
                    "sent": "They're just couple of features, and in fact they work if you plug them into a pipeline that does recognition, it just works fine.",
                    "label": 0
                },
                {
                    "sent": "So even the completely wrong thing.",
                    "label": 0
                },
                {
                    "sent": "Works just fine by giving you Gabor features, which is all all you want, presumably.",
                    "label": 0
                },
                {
                    "sent": "So this is easy to try to contact autoencoders, set the Lambda 2 -- 1 rather than one for example.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you do something, lo and behold, K means something really simple and easy and old school, you get a couple of features, so it doesn't really matter what you do with keyboard features sometimes.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Larger, sometimes smaller, and so on.",
                    "label": 0
                },
                {
                    "sent": "So because of that, because of the importance of these features and understanding invariants and so on, I'm going to talk about them a little bit today, and so I don't.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't claim we have a real understanding of why this couple features always pop up, but we have sort of hints and beginning a better understanding as time goes on, and the fundamental principle behind these Gabor features is actually the Fourier transform that that helps explain what they do, why they come up, and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is fully and this is kabua.",
                    "label": 0
                },
                {
                    "sent": "As you can see.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to start really basic undergrad kind of lecture on for transforms on images, and then I'll move to some slightly more.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interesting things.",
                    "label": 0
                },
                {
                    "sent": "This is all very well known to signal processing people, but it's not very well known to deep learning people and so that's why I'm.",
                    "label": 0
                },
                {
                    "sent": "Talking about this here.",
                    "label": 0
                },
                {
                    "sent": "So the punch line, the kind of 1 sentence message that explains maybe why you get Gabor features?",
                    "label": 0
                },
                {
                    "sent": "Is this on top there so almost all structure and natural images is position invariant and local.",
                    "label": 1
                },
                {
                    "sent": "So that means pretty much if I take a picture of you like with my cell phone like this, I could have also taken that picture by just moving slightly to the left or moving slightly to the right or whatever.",
                    "label": 0
                },
                {
                    "sent": "All of these are natural images, and in a data set like image net or something which is not nicely normalized and center or something like that, you would get these transformations on mass.",
                    "label": 0
                },
                {
                    "sent": "You get tons of slight shifts.",
                    "label": 0
                },
                {
                    "sent": "You get this almost the same object, the same feature, the same.",
                    "label": 0
                },
                {
                    "sent": "Corner edge object whatever once here.",
                    "label": 0
                },
                {
                    "sent": "Once here one slightly tilted and so on.",
                    "label": 0
                },
                {
                    "sent": "So the the strong, clearly the strongest kind of structure that you have to find a natural images, is translation invariants.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And locality, so if you look at this random picture from some street in Montreal structure and the images if you wanted recognition is really concentrated locali right?",
                    "label": 0
                },
                {
                    "sent": "So if you want to recognize that this is a car, then you first recognize maybe that this is.",
                    "label": 0
                },
                {
                    "sent": "What is this lamp or you call it in English headlights?",
                    "label": 0
                },
                {
                    "sent": "Thank you and this piece here, whatever.",
                    "label": 0
                },
                {
                    "sent": "So, so all of these things that you see.",
                    "label": 0
                },
                {
                    "sent": "Express themselves as a set of pixels that are close nearby, and that's what gives them their meaning, right?",
                    "label": 0
                },
                {
                    "sent": "So you never see or hardly ever see something whether the meaning.",
                    "label": 0
                },
                {
                    "sent": "But you can deduce is kind of spread from this part of the image to that part of the image, or something like that.",
                    "label": 0
                },
                {
                    "sent": "You can build hierarchies and so on later and confidence do that, but fundamentally the structure is local and it's position invariant, so I could have moved to the right and got almost the same picture with almost the same meaning.",
                    "label": 0
                },
                {
                    "sent": "So, so this has two consequences.",
                    "label": 0
                },
                {
                    "sent": "One is that.",
                    "label": 0
                },
                {
                    "sent": "Any vision system that works as we have now, the whole vision community has now found is based on local patches, so the fundamental operation that enter the way the image enters the system is in terms of little small regions that get analyzed and then other stuff happens and the other is that the universal mathematical frameworks for understanding what happens in images, how to understand them?",
                    "label": 1
                },
                {
                    "sent": "How about invariances and so on must be the Fourier transform, and that's because the Fourier transform is.",
                    "label": 0
                },
                {
                    "sent": "The framework for dealing with translation, encoding translations and encoding, translation invariants, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So regarding the first point.",
                    "label": 0
                },
                {
                    "sent": "Convolution, so I think Homeland is going to introduce convolutional networks in some detail later.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is just a one slide reminder of what a convent pretty much does roughly, so you take a little filter.",
                    "label": 0
                },
                {
                    "sent": "You put it on top of the image.",
                    "label": 0
                },
                {
                    "sent": "Compute the inner product of this value.",
                    "label": 0
                },
                {
                    "sent": "This this little filter is this part of the image and then it gives you the output.",
                    "label": 0
                },
                {
                    "sent": "This is one plane in the in your feature map.",
                    "label": 0
                },
                {
                    "sent": "Then you scan this across the image and do the same thing over and over again, right?",
                    "label": 0
                },
                {
                    "sent": "So this is fundamentally local and there's translation invariants somehow built into this.",
                    "label": 0
                },
                {
                    "sent": "Or the desire to model something in the translation invariant way.",
                    "label": 0
                },
                {
                    "sent": "In practice, you would flip the filter before.",
                    "label": 0
                },
                {
                    "sent": "Putting it there and that it has some mathematical reasons, but I'm not going to get into probably at all today.",
                    "label": 0
                },
                {
                    "sent": "For a continent it doesn't matter at all, because you're going to learn those features anyway.",
                    "label": 0
                },
                {
                    "sent": "So if you flip them or not, that doesn't really make any difference.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So sometimes people call the operation without the flipping filtering and the one visitor flipping convolution.",
                    "label": 0
                },
                {
                    "sent": "But in terms for convolutional networks it doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "And so cons.",
                    "label": 0
                },
                {
                    "sent": "Net is the established term, so that's so we can give up on the flipping I guess for now.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that was for 2D in one year.",
                    "label": 0
                },
                {
                    "sent": "This is just a picture just for reference from Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So with flipping you would take your filter, flip it around and then scan it across the image and every time you do an inner product and it gives you all the output values.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's one signal that's intimately related.",
                    "label": 0
                },
                {
                    "sent": "To that operation, and that is known as the phasor signal.",
                    "label": 0
                },
                {
                    "sent": "Can you raise your hand if you know phasor signals?",
                    "label": 0
                },
                {
                    "sent": "And know what they do.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's.",
                    "label": 0
                },
                {
                    "sent": "Source 30% of people so.",
                    "label": 0
                },
                {
                    "sent": "The phase is defined up here, right?",
                    "label": 0
                },
                {
                    "sent": "It's exit off I Omega T for some revalue Omega and AT which is a running variable.",
                    "label": 0
                },
                {
                    "sent": "So I'm talking.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about 1D case for now and later I'm going to switch to 2D.",
                    "label": 0
                },
                {
                    "sent": "Which adds a little bit of extra subtlety, and since this is not familiar to many people here, I'm going to just talk about 1D.",
                    "label": 0
                },
                {
                    "sent": "For now, just pretend the image was a 1D signal, or just pretend we were talking about speech or so.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's XI Omega T is thanks to Euler's formula.",
                    "label": 0
                },
                {
                    "sent": "It's the same as cosine, Omega, T, plus items.",
                    "label": 0
                },
                {
                    "sent": "Sign Omega T and I is the square root of minus one, so it's a complex.",
                    "label": 0
                },
                {
                    "sent": "You need complex arithmetic to talk about these things.",
                    "label": 0
                },
                {
                    "sent": "What's important about the phasor?",
                    "label": 0
                },
                {
                    "sent": "For one thing is that it is a representation of both the cosine and the cosine and sine signal at the same time back together into one object, which is this exponential here.",
                    "label": 0
                },
                {
                    "sent": "Thanks to this relation.",
                    "label": 0
                },
                {
                    "sent": "Useful because if you take any two like a sine and cosine.",
                    "label": 1
                },
                {
                    "sent": "You can generate anyone in between, right is just a shifted version of the sign.",
                    "label": 0
                },
                {
                    "sent": "You can generate any of any phase shifted version of your sine wave just as a linear combination of these two, and so that's another way of saying that.",
                    "label": 0
                },
                {
                    "sent": "Sine waves of any given frequency live in a 2 dimensional subspace, so you can just spin around the face and move this thing around and the two basis vectors that you need for that are a cosine into sine.",
                    "label": 1
                },
                {
                    "sent": "For example, you could also use something else like a phase shifted cosine and accordingly phase shifted sine.",
                    "label": 0
                },
                {
                    "sent": "These two these two things fundamentally represent.",
                    "label": 0
                },
                {
                    "sent": "An oscillation, not just one of them.",
                    "label": 0
                },
                {
                    "sent": "If you had just one, you would kind of have to enumerate all phases.",
                    "label": 0
                },
                {
                    "sent": "If you have these two, you spend the whole substudy subspace of all phases at the same time.",
                    "label": 0
                },
                {
                    "sent": "So that's why this is an interesting object.",
                    "label": 0
                },
                {
                    "sent": "The reason it's important, and it comes up again and again when you talk about convolution is that phase are the eigenfunctions of convolution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show exactly what that means in a second.",
                    "label": 0
                },
                {
                    "sent": "The reason for that is that phase us, first of all, are the eigenfunctions of translation.",
                    "label": 0
                },
                {
                    "sent": "So if you take a phasor thanks to being an expert signal, if you take a phasor and shifted to the left or to the right right, you just move it by adding or subtracting something here.",
                    "label": 0
                },
                {
                    "sent": "So this is just going to change the T2 T minus set an, since this is an ex you can pull these two apart and you see that you get it to the minus omegas at times each to the Iomega T which is the original phase are.",
                    "label": 0
                },
                {
                    "sent": "So shifting the signal left to the left and right is nothing other than multiplying by a number.",
                    "label": 0
                },
                {
                    "sent": "And that's another way of saying it's an eigenfunction of translation.",
                    "label": 0
                },
                {
                    "sent": "So if you have any questions then interrupt me by all means.",
                    "label": 0
                },
                {
                    "sent": "Who here is?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Familiar with complex arithmetic and fine with it on paper and stuff.",
                    "label": 0
                },
                {
                    "sent": "It's something like 40%, so let me just review that real quick 'cause you cannot do any mass without that you cannot do any full year things without that.",
                    "label": 0
                },
                {
                    "sent": "So here's the.",
                    "label": 0
                },
                {
                    "sent": "Here's a here's order formula again, so it's E to the X real value is the same as cosine plus I times sign a lot of the things that are useful.",
                    "label": 0
                },
                {
                    "sent": "Just follow directly from that.",
                    "label": 0
                },
                {
                    "sent": "The most important operation when dealing with complex numbers is to flip back and forth between two coordinates.",
                    "label": 0
                },
                {
                    "sent": "So a priority complex number is nothing other than a 2 dimensional vector, so one complex number is just think of it as a 2 dimensional vector point in a 2D plane, which has some funny arithmetic associated with it.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                },
                {
                    "sent": "So in order to to remember that arithmetic and do operations with it, you have to flip back and forth.",
                    "label": 0
                },
                {
                    "sent": "As I said, between two representations of that number, the Classic One that everyone is familiar with.",
                    "label": 0
                },
                {
                    "sent": "This just in terms of.",
                    "label": 0
                },
                {
                    "sent": "Cartesian coordinates just value on the X axis and the value on the Y axis, which are just cosine and sine.",
                    "label": 0
                },
                {
                    "sent": "Thanks to that formula, the other is polar coordinates, where you represent that number not using a B but using the an angle with one of the axes and the distance from that from the origin, and so you can go back and forth between these easily using standard formula like this in the second representation, you might think of this value.",
                    "label": 0
                },
                {
                    "sent": "The length here as an amplitude as you will see later and the.",
                    "label": 0
                },
                {
                    "sent": "The angle here is a phase because that exactly corresponds to the phase and amplitude of a sine wave.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, if you spin around.",
                    "label": 0
                },
                {
                    "sent": "This here this sucker.",
                    "label": 0
                },
                {
                    "sent": "Using the appropriate operation that I mentioned in a second.",
                    "label": 0
                },
                {
                    "sent": "Since this whole it's you're just going to trace out the cosine and assign on these two axes axes here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to talk about how to do this spinning, which is essential to define a sine wave and a cosine, let's look at the two fundamental operations.",
                    "label": 0
                },
                {
                    "sent": "Addition is just.",
                    "label": 0
                },
                {
                    "sent": "Since these are two D vectors, it's just vector addition, and multiplication is much easier to remember if you do that in the polar representation, so.",
                    "label": 1
                },
                {
                    "sent": "You have two complex numbers.",
                    "label": 0
                },
                {
                    "sent": "Their polar representation is this, and So what you do is just multiply them together just like that in that representation, and that's going to define multiplication of two complex numbers.",
                    "label": 0
                },
                {
                    "sent": "Since you're going to multiply these two experts, that just leads to an addition of the angles here in the X, and the lengths are just being multiplied, so that's pretty much all that's necessary to understand for your analysis sufficiently deeply.",
                    "label": 0
                },
                {
                    "sent": "Let me just.",
                    "label": 0
                },
                {
                    "sent": "OK, let me just refer to one other thing, multiplying a number by a complex number of length one just follows just from those formulas does nothing other than taking that to the vector that complex number and rotating it a little bit or a lot, right?",
                    "label": 1
                },
                {
                    "sent": "Let's just because of this.",
                    "label": 0
                },
                {
                    "sent": "Way of defining multiplication.",
                    "label": 0
                },
                {
                    "sent": "So if the length is 1, it's not going to change the links, so multiplying by length one vector just rotates around.",
                    "label": 0
                },
                {
                    "sent": "And so if you start with a complex number and multiply it by E to E to the something again and take it to the item, something again and again it's just going to spin around the circle.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are a few other properties that I might get back to if necessary.",
                    "label": 0
                },
                {
                    "sent": "The most important is conjugation, which is just flipping on the X axis of your 2D space if you want.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And there's a funny definition of an inner product which turns out to be more convenient than what you're used to.",
                    "label": 0
                },
                {
                    "sent": "This looks almost like a vanilla inner product that you that you know, except that one of these two values here is as a conjugate subjected to this flipping operation.",
                    "label": 0
                },
                {
                    "sent": "And it turns out the norms become sensible and so on.",
                    "label": 0
                },
                {
                    "sent": "So, but this is all technical things that are not so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Important right now.",
                    "label": 0
                },
                {
                    "sent": "So let me get back to this slide so aphasia is an eigenfunction of translation.",
                    "label": 0
                },
                {
                    "sent": "So why is the phase i.e to the Iomega T becausw this signal is nothing other than a point that just spins around the origin around the unit circle, right?",
                    "label": 0
                },
                {
                    "sent": "Thanks to this arithmetic that I just discussed.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why is that important for convolution?",
                    "label": 0
                },
                {
                    "sent": "Because convolution is essentially at its heart, nothing other than a translation embedded in some kind of some.",
                    "label": 0
                },
                {
                    "sent": "So here's the formula again.",
                    "label": 0
                },
                {
                    "sent": "So you take your signal, you shift it around, and every time you do it in a product.",
                    "label": 0
                },
                {
                    "sent": "So the shifting here, if you would subject the convolution with a filter to a phasor, then this shift here is just going to lead to a multiplier as we just saw, right?",
                    "label": 0
                },
                {
                    "sent": "So you're just going to multiply the phase up by a constant.",
                    "label": 0
                },
                {
                    "sent": "OK. And now you can group things together and you see that evaluating the convolved with H convolved Faiza is the same as evaluating the phasor that has been multiplied by a constant.",
                    "label": 0
                },
                {
                    "sent": "Now that constant is a bit more complicated than before because you have this some in there and so on.",
                    "label": 0
                },
                {
                    "sent": "But it's still just a constant right so?",
                    "label": 0
                },
                {
                    "sent": "Convolving a signal a phasor with some filter is the same as multiplying that phasor with some number.",
                    "label": 0
                },
                {
                    "sent": "So recall that multiplying by that number can be a complex number.",
                    "label": 0
                },
                {
                    "sent": "Recall that multiplying by complex complex number can do two things.",
                    "label": 0
                },
                {
                    "sent": "It can stretch.",
                    "label": 0
                },
                {
                    "sent": "The value and it can phase shift it a little bit.",
                    "label": 0
                },
                {
                    "sent": "It can take the value, make it longer and it can rotate it around or both.",
                    "label": 0
                },
                {
                    "sent": "Right, it's it's in here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And what you multiply with to change the length is the the absolute value of that complex number, and the phase is what you use to the for the phase shift.",
                    "label": 0
                },
                {
                    "sent": "And so this constant has a name.",
                    "label": 0
                },
                {
                    "sent": "It's called amplitude response.",
                    "label": 0
                },
                {
                    "sent": "Sorry it's called frequency response, and it's a complex number, so I didn't.",
                    "label": 1
                },
                {
                    "sent": "It has an amplitude and phase and those are called amplitude response and phase response, and so they just tell you what happens to the phase of this phase are to the amount of shift that it has as you filter.",
                    "label": 1
                },
                {
                    "sent": "And what happens to the amplitude?",
                    "label": 0
                },
                {
                    "sent": "Does it get?",
                    "label": 0
                },
                {
                    "sent": "Stronger or weaker if you multiply it.",
                    "label": 1
                },
                {
                    "sent": "And that's all this constant or depends on Omega.",
                    "label": 0
                },
                {
                    "sent": "It depends as you see, it depends on the frequency of the phasor.",
                    "label": 0
                },
                {
                    "sent": "So the same filter here that I might apply to some other Phase I might have a different effect on another phase are it might give you a different constant.",
                    "label": 0
                },
                {
                    "sent": "So it might shift and enhance this one, and it might do something else to some other one.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So since phase are so well behaved with respect to convolution and any arbitrary signal is not, because it's much harder to understand what this convolution operation does to an arbitrary signal like an arbitrary image.",
                    "label": 0
                },
                {
                    "sent": "A great idea would be to decompose our signal into phase us and then we can read off all the little phases.",
                    "label": 0
                },
                {
                    "sent": "What will happen as you convolve this signal with a filter.",
                    "label": 0
                },
                {
                    "sent": "So that's one way to arrive at the Fourier transform.",
                    "label": 1
                },
                {
                    "sent": "So this is the definition for the so called discrete for your transform where.",
                    "label": 0
                },
                {
                    "sent": "Everything is discrete and finite, so your signal is just a vector like it always is for any practical purposes.",
                    "label": 0
                },
                {
                    "sent": "You just say your signal is nothing other than a superposition of phases, and I have 2\u03c0 and some normalization in here just to make sure that the phasor goes around once if you.",
                    "label": 0
                },
                {
                    "sent": "Go from zero to one and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "This is all arbitrary kind of decisions.",
                    "label": 0
                },
                {
                    "sent": "One can make changes a little bit also.",
                    "label": 0
                },
                {
                    "sent": "It's just due to the fact that 2\u03c0 is the natural unit.",
                    "label": 0
                },
                {
                    "sent": "Here, right 2\u03c0 means you go around once.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, due to some historical reasons, maybe one is for us the natural.",
                    "label": 0
                },
                {
                    "sent": "Fundamental unit and not 2\u03c0, and so if one was to pie, I wouldn't need this normalization here, but.",
                    "label": 0
                },
                {
                    "sent": "OK, so so you have a superposition of phases that defines your signal.",
                    "label": 0
                },
                {
                    "sent": "How do you get those coefficients well?",
                    "label": 0
                },
                {
                    "sent": "There's some.",
                    "label": 0
                },
                {
                    "sent": "Nitty gritty details.",
                    "label": 0
                },
                {
                    "sent": "Luckily, these phrases can be shown to form an ortho normal basis, and if you have an orthonormal basis to find the coefficient in which to expand and signal is just the inner product, and so if you apply this little inner product with the conjugation.",
                    "label": 0
                },
                {
                    "sent": "That I showed before.",
                    "label": 0
                },
                {
                    "sent": "2 the here.",
                    "label": 0
                },
                {
                    "sent": "Then you see that you just.",
                    "label": 0
                },
                {
                    "sent": "You just have this signal times the conjugate Phase Rs summed up as the way to get your coefficient and so getting your coefficient is called discrete Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "Writing your signal in terms of those coefficients by expanding it into those phases called inverse for inverse discrete for your transform.",
                    "label": 0
                },
                {
                    "sent": "So for whom is this completely new material?",
                    "label": 0
                },
                {
                    "sent": "I guess no one really OK. Um?",
                    "label": 0
                },
                {
                    "sent": "Analogously, S is quite well as is called spectrum, and by analogy, as the amplitude of S is called amplitude spectrum and the argument of the phase angle of that complex number is called phase spectrum.",
                    "label": 1
                },
                {
                    "sent": "So these are office complex numbers again.",
                    "label": 0
                },
                {
                    "sent": "There are many, many different kinds of Fourier transforms.",
                    "label": 0
                },
                {
                    "sent": "This is the most.",
                    "label": 0
                },
                {
                    "sent": "Common, practical, useful one because it's finite, but they're also theoretical considerations that take into account continuous signals which don't exist really in practice, but on paper, and you can show things with them and so on.",
                    "label": 0
                },
                {
                    "sent": "This is the DfT commonly.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's usually implemented using FFT, which is a fast way to compute it, that's all.",
                    "label": 0
                },
                {
                    "sent": "So that's a 1D fully transform for images you have to do Fourier transforms on.",
                    "label": 0
                },
                {
                    "sent": "That would be domain.",
                    "label": 0
                },
                {
                    "sent": "Now that's a bit weird 'cause you're going to have to.",
                    "label": 0
                },
                {
                    "sent": "You're going to want to have to have a phase A of some sort, and that phase are needs to get a scalar input variable.",
                    "label": 0
                },
                {
                    "sent": "Index function is defined on a scalar.",
                    "label": 1
                },
                {
                    "sent": "This X function and so how do you then Divas images?",
                    "label": 0
                },
                {
                    "sent": "And so while there's a very simple trick, you take your image, you replace every pixel or you assign to every pixel position is scalar in whichever way you want.",
                    "label": 0
                },
                {
                    "sent": "And now you just have a bunch of scalar values.",
                    "label": 0
                },
                {
                    "sent": "And now you can plug these values into a phasor.",
                    "label": 0
                },
                {
                    "sent": "This position value into phasor and voila, you have a Fourier transform.",
                    "label": 1
                },
                {
                    "sent": "So one way to transform image position into a scalar is to just multiply it with some vector.",
                    "label": 0
                },
                {
                    "sent": "That we also called Omega, which is now called frequency frequency vector.",
                    "label": 1
                },
                {
                    "sent": "And so that will take an image position, replace it by a scalar number, and allow us to plug it into a phase A and we have a Fourier transform in 2D.",
                    "label": 0
                },
                {
                    "sent": "Of course you could use all kinds of other functions, then projecting onto a frequency vector.",
                    "label": 0
                },
                {
                    "sent": "You could take any scalar function and you would define something like a Fourier transform on a 2 dimensional domain.",
                    "label": 0
                },
                {
                    "sent": "Cheating in some way, right?",
                    "label": 1
                },
                {
                    "sent": "So you take the 2D domain, you replace every point using some loss, of course, because you somehow assign these two D2D grid to 1D.",
                    "label": 0
                },
                {
                    "sent": "Sequence and then you plug it into a phasor.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the most common one is this linear one, and there's some nice properties, and so if the frequency vector points in this direction, for example, then you can plot the value that the phasor gives you.",
                    "label": 0
                },
                {
                    "sent": "I don't know if this is the X or the Y axis, the imaginary component, one of them, and you see that it's you're going to get a wave that extends in the direction of that frequency vector.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Midnight Sun equivalent is of course I'm Transformers, yet another.",
                    "label": 0
                },
                {
                    "sent": "Transform that I'm not going to talk about at all, and it's yeah, that's.",
                    "label": 0
                },
                {
                    "sent": "So I don't know much about this.",
                    "label": 0
                },
                {
                    "sent": "I'm actually not a signal processing person at all.",
                    "label": 0
                },
                {
                    "sent": "The cosine transform is really popular for compression and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "And I'll leave it at that before telling you something that's wrong.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think eventually once you learn your transforms more and more as time goes on and confidence catch on in more and more areas.",
                    "label": 0
                },
                {
                    "sent": "Things like predefining a set of features onto which you project like the cosine transform.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe I'm stretching a little bit too far, but might get a little bit less important.",
                    "label": 0
                },
                {
                    "sent": "Overtime.",
                    "label": 0
                },
                {
                    "sent": "Becausw learning will discover the statistically correct way of projecting your your signal.",
                    "label": 0
                },
                {
                    "sent": "OK, so complex valued waves.",
                    "label": 1
                },
                {
                    "sent": "Interestingly, are separable.",
                    "label": 0
                },
                {
                    "sent": "Yes, another question.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's right, that's why I said this image here is actually.",
                    "label": 0
                },
                {
                    "sent": "And one projection of the actual complex number cannot show you a complex image easily, and so I just show you the one of the two components, the imaginary or real.",
                    "label": 0
                },
                {
                    "sent": "I forgot which one I'm showing here, but they are going to differ only in a slight phase shift.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a good point.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit misleading, right?",
                    "label": 0
                },
                {
                    "sent": "This is not the phase out.",
                    "label": 0
                },
                {
                    "sent": "This is one of the two components of the phasor.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately phasers are two dimensional things.",
                    "label": 0
                },
                {
                    "sent": "You cannot just show them just like that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Interestingly, complex valued waves are separable, which means that you can write this.",
                    "label": 1
                },
                {
                    "sent": "This linearly defined wave in terms of as the outer product of two 1 dimensional waves basically and the same is absolutely not true for real valued waves.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wonder?",
                    "label": 0
                },
                {
                    "sent": "If there for this real valued cosine wave here if there is an image like an image 1 dimensional image like imagine it to be here and here such that the outer product of these two yields this.",
                    "label": 0
                },
                {
                    "sent": "This is clearly not possible.",
                    "label": 0
                },
                {
                    "sent": "You won't find these two dimensional things that generate the this image here, but interesting.",
                    "label": 0
                },
                {
                    "sent": "Lee for complex valued waves.",
                    "label": 0
                },
                {
                    "sent": "Another reason to use complex arithmetic.",
                    "label": 0
                },
                {
                    "sent": "This is true, so you can actually decompose this.",
                    "label": 0
                },
                {
                    "sent": "Image using a lower dimensional projection using the outer product of two 1 dimensional things.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Could be used.",
                    "label": 0
                },
                {
                    "sent": "One could imagine if you would be willing to use.",
                    "label": 0
                },
                {
                    "sent": "Complex valued filters in a continent.",
                    "label": 1
                },
                {
                    "sent": "You could potentially make things faster that way, for example because you could just convolve in one day this way and then in one do that way.",
                    "label": 0
                },
                {
                    "sent": "But that's not going to work if you have real valued filters constant.",
                    "label": 0
                },
                {
                    "sent": "The separability doesn't hold.",
                    "label": 0
                },
                {
                    "sent": "This is just a.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Teresting side note.",
                    "label": 0
                },
                {
                    "sent": "So now that we know what a phase on an image could be defined as, it's easy to define the Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "Also by analogy, you just do the exact same thing you plug in your 2D phasor and so the projection onto your frequency vector W. Sorry Omega transpose Y, where Y is image position, it's just the sum of two values.",
                    "label": 0
                },
                {
                    "sent": "And again I have to deal with some normalization depending on the number of pixels in each direction, M&N and.",
                    "label": 0
                },
                {
                    "sent": "And 2\u03c0 because of the wrong units and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the definition of a Fourier transform on an image.",
                    "label": 0
                },
                {
                    "sent": "Superposition of phasor images that look like this?",
                    "label": 0
                },
                {
                    "sent": "Using coefficients that you get using just the inner product with that phase or image, that's it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So after having done that, after having defined a free transform, you can look at what the Fourier transform on some image looks like, and you can plot amplitude Spectra on Facebook trying stuff.",
                    "label": 0
                },
                {
                    "sent": "Do some interesting analysis that's not relevant for this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lecture, there's one interesting thing that people discover in again and again, or discovered again and again, which is that.",
                    "label": 0
                },
                {
                    "sent": "The amplitude Spectra of its fall off.",
                    "label": 1
                },
                {
                    "sent": "They have always this characteristic fall of.",
                    "label": 0
                },
                {
                    "sent": "Behavior so these are cross sections of this app.",
                    "label": 0
                },
                {
                    "sent": "Well, you take cross sections of this image and average them, and then you draw the amplitude spectrum.",
                    "label": 0
                },
                {
                    "sent": "On average.",
                    "label": 0
                },
                {
                    "sent": "It looks like this, but you're always going to see this for every image for every set of images, you're always going to see the amplitudes fall off.",
                    "label": 0
                },
                {
                    "sent": "That means low frequency.",
                    "label": 0
                },
                {
                    "sent": "Components such as these and these are highly represented in the image and high frequency components such as these are much less represented in a natural image.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oops, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "This is sometimes called 1 / F Power law behavior, which actually happens in many different domains in many different contexts in science and is a bit of a mystery.",
                    "label": 0
                },
                {
                    "sent": "For images it must have something to do with the translation in variance.",
                    "label": 0
                },
                {
                    "sent": "The fact that enter probably related fact that structure is localized becausw you for structure to be localized for things not to get transported around a lot, you better use slowly, slowly varying basis functions if you want, and that's what nature decided to choose, and that's why.",
                    "label": 0
                },
                {
                    "sent": "We get this kind of behavior, but it's a bit of a mystery.",
                    "label": 0
                },
                {
                    "sent": "No one really knows exactly why this power law is always come up, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can do statistics on this.",
                    "label": 0
                },
                {
                    "sent": "Here's a plot that shows some amplitude Spectra.",
                    "label": 0
                },
                {
                    "sent": "The log domain usually off the contours of those for various kinds of scenes, and so you see that certain up type of objects have different amplitude Spectra than others, and so on, and so you can go very far with this and define feature so they so this is from Antonio Torralba and Oliva.",
                    "label": 0
                },
                {
                    "sent": "They went on and to find out something like a just feature which somehow uses these for your transforms to describe local image content and so on, and see if can also be.",
                    "label": 0
                },
                {
                    "sent": "Related to that, it's actually almost the same thing.",
                    "label": 0
                },
                {
                    "sent": "Well, nowadays no one does this anymore.",
                    "label": 0
                },
                {
                    "sent": "'cause the neural net actually discovers.",
                    "label": 0
                },
                {
                    "sent": "That's a good thing to do for your transforms.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what does all of this have to do with learning machine learning and so on?",
                    "label": 0
                },
                {
                    "sent": "Here is also very well known fact for signal processing people, but not well known fact for machine learning people.",
                    "label": 0
                },
                {
                    "sent": "When you do PCA on natural image patches then you will do essentially nothing other than performing for your transform right?",
                    "label": 0
                },
                {
                    "sent": "And that is also just caused by this translation in variance an.",
                    "label": 0
                },
                {
                    "sent": "So here's a formal way of showing that.",
                    "label": 0
                },
                {
                    "sent": "So first of all, if you take.",
                    "label": 0
                },
                {
                    "sent": "Natural image patches and crop some lines from then then.",
                    "label": 0
                },
                {
                    "sent": "And then compute the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "If you plot that, it looks like this.",
                    "label": 0
                },
                {
                    "sent": "So it has some covariance here from the first pixel has some covariance with respect to all its neighbors, and then the thing just gets shifted right.",
                    "label": 0
                },
                {
                    "sent": "The next pixel is going to have a covariance structure, which is just a shifted version of this first one, and that is simply be cause if images are translation invariant.",
                    "label": 0
                },
                {
                    "sent": "In other words, I can take them.",
                    "label": 0
                },
                {
                    "sent": "At any different position.",
                    "label": 0
                },
                {
                    "sent": "The statistics have to be invariant, so if there's some relation it can also be higher order relation between this pixel and that pixel.",
                    "label": 0
                },
                {
                    "sent": "Then that same relation must hold between this pixel and pixel.",
                    "label": 0
                },
                {
                    "sent": "It can only be a function of the distance, it cannot be a function of the absolute position of the pixel.",
                    "label": 0
                },
                {
                    "sent": "Just can be cause.",
                    "label": 0
                },
                {
                    "sent": "If I have this image in my database and I have a large enough database, then that image approximately is also going to be there.",
                    "label": 0
                },
                {
                    "sent": "List of the database is large enough.",
                    "label": 0
                },
                {
                    "sent": "So this is for 1D images and it's easy to understand if you plot the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "For actual images, you vectorize them computer covariance and look at it.",
                    "label": 0
                },
                {
                    "sent": "Then it looks like this and it basically says the same thing, but it's harder to interpret.",
                    "label": 0
                },
                {
                    "sent": "It says the same thing in 2D, right?",
                    "label": 0
                },
                {
                    "sent": "It says that the covariance between any pair of pixels is the same.",
                    "label": 0
                },
                {
                    "sent": "Again if you shift it around or not, and that expresses itself as this funny pattern here.",
                    "label": 0
                },
                {
                    "sent": "Just because I vectorized the image to make it to the plot.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Has has what has.",
                    "label": 0
                },
                {
                    "sent": "So and yeah, so the question is why does this show up in human image in actual images, even though humans don't actually take an arbitrary image at an arbitrary angle, but maybe focus on people more or something like that, right?",
                    "label": 0
                },
                {
                    "sent": "So in fact there are datasets which are centered like there are many face datasets which are exactly centered on faces, and so that's true.",
                    "label": 1
                },
                {
                    "sent": "Then in that case, translation invariance doesn't hold and you wouldn't see this effect.",
                    "label": 0
                },
                {
                    "sent": "At all in that case, if for natural images you find this effect and it's extremely strong, which.",
                    "label": 0
                },
                {
                    "sent": "Bob probably just suggests that.",
                    "label": 0
                },
                {
                    "sent": "The human tendency to focus on certain things more than others is not strong enough to counter that effect.",
                    "label": 0
                },
                {
                    "sent": "Yet if you look at the center face data set or something like that, you still see Phooey Gabor like structure in the features that you learn.",
                    "label": 0
                },
                {
                    "sent": "It's just that it's concentrated around the.",
                    "label": 0
                },
                {
                    "sent": "Person for example.",
                    "label": 0
                },
                {
                    "sent": "So common features that you would see in a face databases like something like an I or an eyebrow, which is like this WAVY GABA pentin.",
                    "label": 0
                },
                {
                    "sent": "And that's probably because even though the the phases are centered then never perfectly centered and still the main type of variation in that kind of data is small shifts at this particular position then right?",
                    "label": 0
                },
                {
                    "sent": "And then again you get keyboards, you get invariants.",
                    "label": 0
                },
                {
                    "sent": "But that invariances look highly localized.",
                    "label": 0
                },
                {
                    "sent": "Then it's not across the whole image.",
                    "label": 0
                },
                {
                    "sent": "It's kind of.",
                    "label": 0
                },
                {
                    "sent": "Only in small in certain regions of the image.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "So computing the covariance matrix or natural image is the first step to do PCA.",
                    "label": 0
                },
                {
                    "sent": "What would be the next step to do in order to do PCA on your image patches?",
                    "label": 0
                },
                {
                    "sent": "After I computed the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes video.",
                    "label": 0
                },
                {
                    "sent": "I can decomposition exactly and so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's an interesting question to ask is what are the eigenvectors of the covariance matrix?",
                    "label": 0
                },
                {
                    "sent": "And so as I said, this is a really well known fact for for signal processing people, so I'm not telling you anything you hear at all.",
                    "label": 0
                },
                {
                    "sent": "They are phase us.",
                    "label": 0
                },
                {
                    "sent": "It turns out right, so you have no other choice but to do fully transform when you do PCA.",
                    "label": 0
                },
                {
                    "sent": "Essentially, here's one way.",
                    "label": 0
                },
                {
                    "sent": "One of two ways.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to show today of showing that.",
                    "label": 0
                },
                {
                    "sent": "Multiply the covariance matrix with a phasor.",
                    "label": 1
                },
                {
                    "sent": "So that gives you a vector, right?",
                    "label": 0
                },
                {
                    "sent": "We're dealing with discrete objects here and then.",
                    "label": 0
                },
                {
                    "sent": "What is the value of that vector at the TS dimension?",
                    "label": 0
                },
                {
                    "sent": "So to multiply C by P. So you take your matrix times vector the TS value of that is just going to be the TS row of that matrix.",
                    "label": 0
                },
                {
                    "sent": "Inner product with this vector, right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we can evaluate what that is.",
                    "label": 1
                },
                {
                    "sent": "So you take the tea throw of the covariance matrix times the phasor.",
                    "label": 0
                },
                {
                    "sent": "If the covariance matrix has this invariant structure, it's not going to depend on the actual value of T&T prime on the of the IJS index.",
                    "label": 0
                },
                {
                    "sent": "That's what TNT prime means here.",
                    "label": 0
                },
                {
                    "sent": "But it's only going to depend on the distance of these two, and so I should mention, this is the one the case I can show you the two decades in a second, so it's only depending on the distance, and so I can write it as some function.",
                    "label": 0
                },
                {
                    "sent": "That's the function of the distance between the two dimensions.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so now I can make a change of variables is one way to show and I just called set T -- T prime so I can just replace.",
                    "label": 0
                },
                {
                    "sent": "This buys AT&T Prime as a result of that becomes T minus set.",
                    "label": 0
                },
                {
                    "sent": "If you're following an so I get T minus set here, but this is an exponential so I can pull this some apart this difference apart and so I get C * 1 exponential times another exponential right?",
                    "label": 0
                },
                {
                    "sent": "And so now I can group the set stuff together and I have the T stuff here.",
                    "label": 0
                },
                {
                    "sent": "And if I group those that stuff together, I see this is just going to.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry actually maybe I should standing somewhere else.",
                    "label": 0
                },
                {
                    "sent": "I just get a number that's going to multiply this phase a value here and it will happen for all T for all values, and so that's just a number, let's call that Lambda and.",
                    "label": 0
                },
                {
                    "sent": "So that means doing the product of covariance matrix times phasor is the same as multiplying the phasor by a scalar, which might be a complex number.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not actually here, but that's technical detail due to symmetry of that matrix, it doesn't really matter, so it's so the phase.",
                    "label": 0
                },
                {
                    "sent": "Because of that is an eigen vector.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And so if you do your PCA, you do like of that matrix, you will get phase us as your result, inevitably almost.",
                    "label": 0
                },
                {
                    "sent": "There's one approximation that I did, which is the fact that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is not the C function is not perfectly shift invariant, it's only shift invariant up to the edge.",
                    "label": 0
                },
                {
                    "sent": "If this was a shift with wrap around so you would get this stuff that moves out here would be coming back in here then it would be perfectly shift invariant, But this is not becausw this large value you don't know.",
                    "label": 0
                },
                {
                    "sent": "Don't see it anywhere here 'cause it just falls out.",
                    "label": 0
                },
                {
                    "sent": "And so translation invariants holds only up to the edges of edge effects.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And because of that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You look at the components.",
                    "label": 0
                },
                {
                    "sent": "They don't look like perfect for your components, but you see that they are essentially for your components.",
                    "label": 0
                },
                {
                    "sent": "But before I show them, I'm just going to extend.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same to 2D, just really quick.",
                    "label": 0
                },
                {
                    "sent": "It's the same calculation.",
                    "label": 0
                },
                {
                    "sent": "Let me just.",
                    "label": 0
                },
                {
                    "sent": "Refer to it for reference, except that now you have a 2D.",
                    "label": 0
                },
                {
                    "sent": "Matrix here and the inner product is funny now because it does you know what I mean is a vectorized.",
                    "label": 0
                },
                {
                    "sent": "Way of representing things on images, but again, it's a function of distance, and it has the same effect.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so then, if you look at the features, of course they always look somehow like this, so they're like fully components essentially.",
                    "label": 0
                },
                {
                    "sent": "Messed up slightly due to maybe another effect which is sampling bias, right?",
                    "label": 0
                },
                {
                    "sent": "So we're only going to do this on a finite set of image patches that can mess things up a little bit, and then it's not perfectly translation invariant, that also messes it up a little bit.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Why would that be?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that answers the question.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, alright, so I don't have to answer the question.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yet another way of saying this is that since the query.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix has this structure here it's it is a convolution 'cause you were going to convolve with this filter here.",
                    "label": 0
                },
                {
                    "sent": "Right, because taking the product of this matrix times vector will take this vector, then this vector then this vector, but these are all shifted versions of 1 vector, so this is actually a convolution that you perform and the eigenvectors are.",
                    "label": 0
                },
                {
                    "sent": "Faze us, and so they look like this.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are two famous theorems.",
                    "label": 0
                },
                {
                    "sent": "The dark the maze, the main kinda.",
                    "label": 0
                },
                {
                    "sent": "Lamp posts for signal processing people.",
                    "label": 0
                },
                {
                    "sent": "One is one that I think most of you are very familiar with.",
                    "label": 0
                },
                {
                    "sent": "Convolution in the time domain is multiplication in the frequency domain.",
                    "label": 1
                },
                {
                    "sent": "So instead of convolving you might as well do a Fourier transform to an elementwise product of the Spectra and then do an inverse Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "That's going to have the same effect as convolving two signals with one another.",
                    "label": 1
                },
                {
                    "sent": "That can be used and it has been used in confidence.",
                    "label": 0
                },
                {
                    "sent": "For example, at this recent paper by Jan and some students, but other people have done it and and over the years and tried that and so on.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it helps.",
                    "label": 0
                },
                {
                    "sent": "In terms of speed, sometimes it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It depends on the size of the filter, the size, the number of channels, the size of the image, and all kinds of other things.",
                    "label": 0
                },
                {
                    "sent": "So it's one of these constants that Lyon refer to, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not completely obvious if you should do this or shouldn't do this.",
                    "label": 0
                },
                {
                    "sent": "The reason it can help is that you can do the.",
                    "label": 0
                },
                {
                    "sent": "If you can use the FFT to do your free transform, which is very efficient.",
                    "label": 0
                },
                {
                    "sent": "Amazing scientific sorry engineering hack kinda.",
                    "label": 0
                },
                {
                    "sent": "Amazing idea to compute this.",
                    "label": 0
                },
                {
                    "sent": "Formula here in unlock activation lock and time rather than quadratic, which you would need there.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's well known.",
                    "label": 0
                },
                {
                    "sent": "I think people are familiar with that.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The converse is true too, though, and I think it's much more important.",
                    "label": 0
                },
                {
                    "sent": "Fact that explains much more than the first one, so multiplication in the time domain is convolution in the frequency domain.",
                    "label": 1
                },
                {
                    "sent": "So the opposite is true as well.",
                    "label": 0
                },
                {
                    "sent": "Take two signals, multiply them elementwise.",
                    "label": 0
                },
                {
                    "sent": "That did it.",
                    "label": 0
                },
                {
                    "sent": "That's the same as if you had done the Fourier transform and then convolved the Spectra the proof.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or that is analogous to the proof for the first sort of the opposite of that, and I just leave it there for reference because I don't have enough time, but it's just a little playing around with indexes and so on, and this is not actually a proof.",
                    "label": 0
                },
                {
                    "sent": "It's a proven quote, so it's proof sketch that's not very formal, but it's I think it's good enough to believe that this is true.",
                    "label": 0
                },
                {
                    "sent": "Or you would believe it anyway, because it's written in textbooks so.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But one of the so there are many effects that are caused by the 2nd or an interplay with the 1st and the 2nd theorem.",
                    "label": 0
                },
                {
                    "sent": "Some things that are cost at least to some degree by this went on effects that signal processing folks have to deal with it all the time, like ringing.",
                    "label": 0
                },
                {
                    "sent": "When you filter images or aliasing when you do sampling.",
                    "label": 0
                },
                {
                    "sent": "So strange effects that you see in images that you wouldn't want there, I wouldn't expect and so on, but there's one effect which is maybe the most important for us right now is called leakage, and I'm just going to try to.",
                    "label": 0
                },
                {
                    "sent": "Well, go over this leakage effect real quick and explain why it's important and how an engineer would deal with it and how curiously an Euronet decides to deal with it as well.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's here's leakage and.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here you see a sine wave which perfectly fits into that window, and so here you see the spectrum.",
                    "label": 0
                },
                {
                    "sent": "So one thing I didn't mention is that spectral are symmetric.",
                    "label": 0
                },
                {
                    "sent": "If the signal is real.",
                    "label": 0
                },
                {
                    "sent": "It's also a little calculation, and so the spectrum looks symmetric here.",
                    "label": 0
                },
                {
                    "sent": "So just forget about that.",
                    "label": 0
                },
                {
                    "sent": "Just imagine I had just shown it from here on or something like that.",
                    "label": 0
                },
                {
                    "sent": "So you see, there's one component strongly present that happens to be the 16th index in my vector in my expansion, which is the phasor corresponding to the frequency one, and so that's the signal there.",
                    "label": 0
                },
                {
                    "sent": "So it's perfectly fine and I see.",
                    "label": 0
                },
                {
                    "sent": "The Free Transform reveals what the signal is.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "If I had a signal that has twice the frequency, I would see this second guy here pop up instead and then three times the frequency I would see this guy pop up and so on and divide.",
                    "label": 0
                },
                {
                    "sent": "So superposition of those I would see several of these guys pop up.",
                    "label": 0
                },
                {
                    "sent": "But what happens if I just ever so slightly expand the frequency so that I do not have an integer multiple of the window links as my frequency?",
                    "label": 0
                },
                {
                    "sent": "So I just make this a little bit longer.",
                    "label": 0
                },
                {
                    "sent": "This signal, then since I only have integer multiple frequencies at my disposal?",
                    "label": 0
                },
                {
                    "sent": "To model this signal I I wish I would see.",
                    "label": 0
                },
                {
                    "sent": "Ever so slightly to the right, a guy pop up and tell me that's the that's the frequency component is.",
                    "label": 0
                },
                {
                    "sent": "It's 1.2 for example, but that's not what you see because you don't have the 1.2.",
                    "label": 0
                },
                {
                    "sent": "You only have your 32.",
                    "label": 0
                },
                {
                    "sent": "In this case, coefficients from that expansion from that formula, and so that's.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you see, you see some other stuff happening.",
                    "label": 0
                },
                {
                    "sent": "Some of these guys go up and.",
                    "label": 0
                },
                {
                    "sent": "What else can you do?",
                    "label": 0
                },
                {
                    "sent": "You can only use the values that you have and that does not mean that the Fourier transform loses information that it cannot model this.",
                    "label": 0
                },
                {
                    "sent": "It just means that in order to model a non integer frequency, it has to make use of integer multiples of frequency.",
                    "label": 0
                },
                {
                    "sent": "To model that sequence right?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I change it even more.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More than other things happen and goes even worse, so it decides to do matter to to represent this 1.5 frequency by using all these coefficients here right?",
                    "label": 0
                },
                {
                    "sent": "So leakage is a problem that you should think of as leaking into.",
                    "label": 0
                },
                {
                    "sent": "Imagine you would want to detect a frequency component you would be interested in detecting this frequency over there or some other like this frequency here.",
                    "label": 0
                },
                {
                    "sent": "Well, unfortunately if you, say apply a threshold here and say this frequency is present.",
                    "label": 0
                },
                {
                    "sent": "Even though it is present in this case because it needs you need it to model this 1.5 frequency, you may not want to detect it.",
                    "label": 0
                },
                {
                    "sent": "You may actually be interested in the true 1.5 guy which you cannot see and so.",
                    "label": 0
                },
                {
                    "sent": "Frequency components that cannot be modeled leak into the others and mess up analysis that way, right?",
                    "label": 0
                },
                {
                    "sent": "So it's hard to distinguish this guy from being a real.",
                    "label": 0
                },
                {
                    "sent": "3.",
                    "label": 0
                },
                {
                    "sent": "Three times window links.",
                    "label": 0
                },
                {
                    "sent": "Feiser from one that's just has been used by the FFT by the DfT to model this weird signal up there.",
                    "label": 0
                },
                {
                    "sent": "There's a more.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nicola Way of describing the same thing.",
                    "label": 0
                },
                {
                    "sent": "Which uses so-called sync functions, which are the Fourier transforms of boxes.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to refer to this real quick.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cause that motivates how to fix that problem.",
                    "label": 0
                },
                {
                    "sent": "If you look at this signal over there that's you can always think of it as a periodic signal and you would want the Fourier transform to give you the same result for the periodic extension.",
                    "label": 0
                },
                {
                    "sent": "So imagine this wasn't just a window of 1 oscillation here, but it wasn't.",
                    "label": 0
                },
                {
                    "sent": "It would be infinitely extended.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "It would be an infinitely long sine wave.",
                    "label": 0
                },
                {
                    "sent": "Of course, the Fourier transform of that infinite guy should look just like this because there is nothing other than this frequency sine wave present in your signal.",
                    "label": 0
                },
                {
                    "sent": "But that's not what you give the DfT.",
                    "label": 0
                },
                {
                    "sent": "What you give the DfT is this infinite sequence that has clearly this spectrum, and you might apply it by a square window if you like, right?",
                    "label": 0
                },
                {
                    "sent": "You can think of this as an infinite signal, which you multiply by a window that looks like this.",
                    "label": 0
                },
                {
                    "sent": "It just cuts out one piece of it.",
                    "label": 0
                },
                {
                    "sent": "Now an interesting fact is that a square window has a so-called sync function.",
                    "label": 0
                },
                {
                    "sent": "As the Fourier transform and multiplying elementwise.",
                    "label": 0
                },
                {
                    "sent": "Buy a window is the same as.",
                    "label": 0
                },
                {
                    "sent": "Convolving the Fourier transform with the full transform of that window, which is a sync function.",
                    "label": 0
                },
                {
                    "sent": "And it turns out the sync function is an interesting object.",
                    "label": 0
                },
                {
                    "sent": "It has zero crossings at just.",
                    "label": 0
                },
                {
                    "sent": "It looks like this it has zero crossings at just the right positions in order not to mess up the Fourier transform for frequencies that are integer multiples of the sequence links.",
                    "label": 0
                },
                {
                    "sent": "That's why this works perfectly fine.",
                    "label": 0
                },
                {
                    "sent": "But this convolution with the sync function does have the zero crossings, but if you apply to something.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's going to start shuffling things around in the spectrum.",
                    "label": 0
                },
                {
                    "sent": "This convolution operation and so now stuff leaks into other parts of the.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the spectrum, so that cannot be avoided, right?",
                    "label": 0
                },
                {
                    "sent": "As I said, you're going to have to use what you can do.",
                    "label": 0
                },
                {
                    "sent": "The FFT has to use what it can to model the signal, and it can do so losslessly, and so it does what it has to do, but.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if you don't use the square window but another differently shaped window.",
                    "label": 0
                },
                {
                    "sent": "Before you do your free transform.",
                    "label": 0
                },
                {
                    "sent": "Implicitly, you can change the leakage effect so that it's maybe less undesirable for your application at hand, so here's an example of.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is 2 times or three times or something.",
                    "label": 0
                },
                {
                    "sent": "Window length perfect DfT.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is changing the frequency and things get messed up and so I imagine you would have been interested in distinguishing these two components or these two components, or something.",
                    "label": 0
                },
                {
                    "sent": "Your threshold kicks in and you would think this component is present, but it wasn't because that's not what you were interested in.",
                    "label": 0
                },
                {
                    "sent": "Well, this was the square window, so why not just use it?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different window, such as a Gaussian window for example, in which case.",
                    "label": 0
                },
                {
                    "sent": "You still have leakage.",
                    "label": 0
                },
                {
                    "sent": "As I said, you cannot avoid it, but the way that stuff leaks around into other bins can be changed anfora Gaussian it's more localized, so you're going to look more into neighbors and less into faraway guys.",
                    "label": 0
                },
                {
                    "sent": "And so for example, all this lifted stuff here, which accounts for that signal is just fine, so detection would be perfectly fine here, but the neighbors here are messed up more 'cause it's a Gaussian window, and you're going to have to leave it somewhere that information, right?",
                    "label": 0
                },
                {
                    "sent": "But if you're willing to give up on it a little bit of frequency resolution.",
                    "label": 0
                },
                {
                    "sent": "You might just want to do this in order not to mess things up far away instead.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spectrum and then so this is 1.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Window if I use another Gaussian window.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is with.",
                    "label": 0
                },
                {
                    "sent": "With this is 1 this.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a smaller one.",
                    "label": 0
                },
                {
                    "sent": "Then you get all these different kinds of ways of leaking around.",
                    "label": 0
                },
                {
                    "sent": "So you can actually push.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That would be extreme.",
                    "label": 0
                },
                {
                    "sent": "Just going to talk about that in as well.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to just.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skip ahead.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second, there is a.",
                    "label": 0
                },
                {
                    "sent": "The principle called the uncertainty principle, which is actually related to the physical uncertainty principle in various subtle ways, which states that as you change the size of your window, you're going to trade off accuracy in the time domain for accuracy in the frequency domain, right?",
                    "label": 0
                },
                {
                    "sent": "So if you take your whole signal which looks like this, then you get a spectrum that might look like this.",
                    "label": 0
                },
                {
                    "sent": "Now you shouldn't do that because of leakage problems and so on.",
                    "label": 0
                },
                {
                    "sent": "So you put a window on top.",
                    "label": 0
                },
                {
                    "sent": "You know that makes the signal look like this, and so now these guys get wider.",
                    "label": 0
                },
                {
                    "sent": "So you lose resolution.",
                    "label": 0
                },
                {
                    "sent": "Hear you if you make the window smaller then this gets worse and worse.",
                    "label": 0
                },
                {
                    "sent": "And there's actually an easy way to show that it's because the Fourier transform of a Gaussian turns out to be a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "You just go one over the variance, and so if smaller Gaussian here has a wider transform here, and so the smaller you make the Gaussian, the wider you spread things around in the spectrum, and so.",
                    "label": 0
                },
                {
                    "sent": "One reason this is a principle that's of interest maybe is that you might be interested in doing a Fourier transform locali just at a little piece of an image or a signal, and in that case you need Windows, and so you have this problem.",
                    "label": 0
                },
                {
                    "sent": "This is going to show up, and so now you have a choice to make.",
                    "label": 0
                },
                {
                    "sent": "Do I want very, very good temporal resolution, in which case I mess up my spectrum.",
                    "label": 0
                },
                {
                    "sent": "Or do I want to have?",
                    "label": 0
                },
                {
                    "sent": "Good spectrum spectral resolution, in which case I'm going to have to pull over a large region of space or time essentially.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I just mentioned it is doing that is incredibly common in speech processing, and it turns out in Vision tour it's called a short term short time Fourier transform IFSC DfT.",
                    "label": 0
                },
                {
                    "sent": "People do that all the time and continents do that all the time.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "As it turns out, and it's just nothing other than doing a free transform, not on your whole signal, but on a little piece of signal.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And in order to do that, you could just cut out a little piece.",
                    "label": 0
                },
                {
                    "sent": "But of course you wouldn't do that.",
                    "label": 0
                },
                {
                    "sent": "You would use a window function, which is better behaved than it, and so one window function to use is a Gaussian, and it's very common, and so one way to call an S TFT.",
                    "label": 0
                },
                {
                    "sent": "So doing a Fourier transform here, here, here, here, here, here, here, and every time using a Gaussian window is called the Gabor transform and.",
                    "label": 1
                },
                {
                    "sent": "And that's why both features come from.",
                    "label": 0
                },
                {
                    "sent": "In one day, the results are usually the amplitude of the result is called spectrogram, right?",
                    "label": 0
                },
                {
                    "sent": "Everyone has seen these plots of spectrograms.",
                    "label": 0
                },
                {
                    "sent": "Whoops, I don't have it here.",
                    "label": 0
                },
                {
                    "sent": "Lost it maybe.",
                    "label": 0
                },
                {
                    "sent": "We'll show later, but in 2D you get the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Just that the spectrogram is not.",
                    "label": 0
                },
                {
                    "sent": "For every position in time, a stack of frequencies, it's rather for every 2D position of image.",
                    "label": 0
                },
                {
                    "sent": "It's a stack of frequencies which in turn are two dimensional, so it's a messy object and actually in confidence people call that feature Maps.",
                    "label": 0
                },
                {
                    "sent": "Instead.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "So how can you get a Gabor feature then to do that in practice?",
                    "label": 0
                },
                {
                    "sent": "And literally, people have done this in practice all the time, especially very successfully for face recognition and things like that, you take your complex wave, which now you see it's two.",
                    "label": 0
                },
                {
                    "sent": "It's properly plotted as two.",
                    "label": 0
                },
                {
                    "sent": "Wait?",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take a Gaussian window.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then you multiply your waves by this Gaussian window.",
                    "label": 0
                },
                {
                    "sent": "So you have to do that with both and that defines what's called a Gabor feature.",
                    "label": 1
                },
                {
                    "sent": "It can have many many parameters becausw you can decide how to choose the Gaussian, make it wide or small, or make it not axis aligned or axis aligned.",
                    "label": 0
                },
                {
                    "sent": "Change the covariance and so on.",
                    "label": 0
                },
                {
                    "sent": "As other parameters like where to place your Gaussian and.",
                    "label": 0
                },
                {
                    "sent": "Which frequencies to choose and which not to choose?",
                    "label": 0
                },
                {
                    "sent": "Which frequency frequency vectors?",
                    "label": 0
                },
                {
                    "sent": "In other words, which orientation, which frequency and so on, and so there's a whole lot of parameters and but here's one way to define such a function.",
                    "label": 0
                },
                {
                    "sent": "So of course messy to play with all of these.",
                    "label": 0
                },
                {
                    "sent": "These are hyperparameters essentially in order to model something.",
                    "label": 0
                },
                {
                    "sent": "But Luckily people stop doing this, of course, and just train your Nets instead.",
                    "label": 0
                },
                {
                    "sent": "But lo and behold, the neural net decides to do nothing other than that in the end.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course it chooses the right parameters.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are you while it's training, this uncertainty principle extends then also to 2D, by the way, so and it's it.",
                    "label": 0
                },
                {
                    "sent": "It goes further than than just the position in uncertainty.",
                    "label": 0
                },
                {
                    "sent": "For example, here's an example from the from the Hoover in book where you can have a feature like this one which is highly localized along this axis.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But it would fire on slanted Lions that are anywhere within this region pretty strongly.",
                    "label": 0
                },
                {
                    "sent": "You can change that resolution of this land here by making this guy longer maybe, but as a result of that you lose resolution along this axis, or Even so if you have a little bit of feature in the image sitting here or here or here, you're not going to be able to distinguish them.",
                    "label": 0
                },
                {
                    "sent": "But again, if you train neural Nets, you don't care because it's going to choose the right features for you.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through backdrop.",
                    "label": 0
                },
                {
                    "sent": "So doing an S TFT.",
                    "label": 0
                },
                {
                    "sent": "Means that you take your GABA feature.",
                    "label": 0
                },
                {
                    "sent": "And scan it across the image cause you want to do the FFT or whatever everywhere.",
                    "label": 0
                },
                {
                    "sent": "And confident does the same thing.",
                    "label": 0
                },
                {
                    "sent": "It learns Gabor features.",
                    "label": 0
                },
                {
                    "sent": "Which are then scanned across the image to evaluate them everywhere.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so that's pretty much, well, that is a convolution.",
                    "label": 0
                },
                {
                    "sent": "And convolution is a filtering operation that affects the spectrum of the resulting image, right?",
                    "label": 0
                },
                {
                    "sent": "So you're going to in those feature Maps that you get in your convenant.",
                    "label": 0
                },
                {
                    "sent": "Each one of them is going to have been a filtered version of the image, which means as an image with whose spectrum you must in some way.",
                    "label": 0
                },
                {
                    "sent": "So in what way does that feature then mess with spectrum?",
                    "label": 0
                },
                {
                    "sent": "Can someone tell me what is the amplitude response of a GABA feature necessarily by taking everything together that we talked about?",
                    "label": 0
                },
                {
                    "sent": "Can you say it again?",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly.",
                    "label": 0
                },
                {
                    "sent": "So the answer is you multiply.",
                    "label": 0
                },
                {
                    "sent": "The spectrum by a localized Gaussian somewhere, and so the reason is you do convolution in the original domain, so that's equivalent to doing multiplication in the answer in the special domains means you do multiplication in the spectral domain.",
                    "label": 0
                },
                {
                    "sent": "And what is the spectrum of?",
                    "label": 0
                },
                {
                    "sent": "A couple of future.",
                    "label": 0
                },
                {
                    "sent": "First of all, it's just a peak 'cause it is, well, it's a slightly widen peak.",
                    "label": 0
                },
                {
                    "sent": "Maybe because it has discussion filter on top of it, a Gaussian envelope.",
                    "label": 0
                },
                {
                    "sent": "A sine wave is a peak, right?",
                    "label": 0
                },
                {
                    "sent": "The spectrum is just a dot.",
                    "label": 0
                },
                {
                    "sent": "Multiplied with a Gaussian, it's a smeared out dot.",
                    "label": 0
                },
                {
                    "sent": "Now convolving the image with that thing means multiplying with that smeared out dot the spectrum, right?",
                    "label": 0
                },
                {
                    "sent": "Is everyone following OK?",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's a localized blob in the frequency domain, so the the the the the planes in your feature map that you get from your continent.",
                    "label": 0
                },
                {
                    "sent": "I just filtered versions of the original image where you highlight we take the spectrum of the original image and you have a blob that just highlights a little piece of spectrum, little orientation and frequency.",
                    "label": 0
                },
                {
                    "sent": "For you.",
                    "label": 0
                },
                {
                    "sent": "So because of that, in especially in the neuroscience literature and so on, people called Gabor features, often oriented bandpass filters.",
                    "label": 0
                },
                {
                    "sent": "They they zoom in onto a little piece in the in the spectrum and then they go down everything else and it's oriented cause the spectrum in 2D is it to the thing because you have frequency vectors and not just frequencies.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, here's the spectrum that I was looking for, so this is the spectrogram in 1D, right?",
                    "label": 1
                },
                {
                    "sent": "So this is the amplitude spectrum of a signal and it's just a stack of.",
                    "label": 0
                },
                {
                    "sent": "Amplitudes are frequencies.",
                    "label": 0
                },
                {
                    "sent": "And so now let time turn into a space, make a 2D.",
                    "label": 0
                },
                {
                    "sent": "Then you get these stacked vectors.",
                    "label": 0
                },
                {
                    "sent": "Here at every position.",
                    "label": 0
                },
                {
                    "sent": "And then that would be called a feature map in a convolutional network.",
                    "label": 0
                },
                {
                    "sent": "But it's really to me.",
                    "label": 0
                },
                {
                    "sent": "It's really the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "It's just if you ended up learning about like features, you will just have done the same thing.",
                    "label": 0
                },
                {
                    "sent": "In two D and that's it.",
                    "label": 0
                },
                {
                    "sent": "So engineers found that along time ago, right?",
                    "label": 0
                },
                {
                    "sent": "So if this is absolutely essential in speech processing, you have to do this first as a preprocessing operation before you continue and people still do this right, you put a neural net on top of this and then do speech recognition or something.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And it's kind of cute that neural Nets figure out the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you take a continent, and you have Kobol like features on the first layer.",
                    "label": 0
                },
                {
                    "sent": "The code that doesn't really work, typically.",
                    "label": 0
                },
                {
                    "sent": "Unless you use something like a Relu or sigmoid for tonnage, it kind of maybe works too, but that's for kind of the wrong reasons, because as someone pointed out in the in one of the lectures recently, it's because the network starts to learn.",
                    "label": 0
                },
                {
                    "sent": "Something, despite being crippled in some way, right?",
                    "label": 0
                },
                {
                    "sent": "Because back prop is so magical that it still finds a way to do something sensible.",
                    "label": 0
                },
                {
                    "sent": "But typically it's common to use.",
                    "label": 0
                },
                {
                    "sent": "Relevance or signals, or something that goes to zero as you get more and more negative.",
                    "label": 0
                },
                {
                    "sent": "It's what you do before the pooling, typically right?",
                    "label": 0
                },
                {
                    "sent": "So you expect your features and you have relevant and you pull over them or something.",
                    "label": 0
                },
                {
                    "sent": "So that's so.",
                    "label": 0
                },
                {
                    "sent": "Imagine you have a future somewhere you are scattered across the image.",
                    "label": 0
                },
                {
                    "sent": "You have a set of responses for that feature.",
                    "label": 0
                },
                {
                    "sent": "Since you move that thing around you sort of change the phase, at least in the direction of the waves you change the phase a little bit, so you sort of try out that feature at many different phases because the shift of the feature is almost like a phase shift of the of this way of writing, at least locally.",
                    "label": 0
                },
                {
                    "sent": "Now you try out this feature at many different phases and then you rectify, which means you only keep positive responses.",
                    "label": 0
                },
                {
                    "sent": "Or let's say you do Max pooling.",
                    "label": 0
                },
                {
                    "sent": "In fact, then you don't need the rectification.",
                    "label": 0
                },
                {
                    "sent": "You say you do Max pooling.",
                    "label": 0
                },
                {
                    "sent": "Then you're just going to keep the strongest one.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of like you try out a feature at all phases and then you keep the strongest response.",
                    "label": 0
                },
                {
                    "sent": "And that's almost like an amplitude spectrum.",
                    "label": 0
                },
                {
                    "sent": "You just look at how strongly is this feature represented at this position in the image, regardless of what its face is, right?",
                    "label": 0
                },
                {
                    "sent": "It turns out in biology of neuroscience and elsewhere, people came up with other ways of getting amplitudes.",
                    "label": 0
                },
                {
                    "sent": "Well, in engineering people just sum the squares they get.",
                    "label": 0
                },
                {
                    "sent": "The length of this complex value as another way to get amplitudes, right?",
                    "label": 0
                },
                {
                    "sent": "So when a biologist talks about complex cell, then they used typically to means something more like this.",
                    "label": 0
                },
                {
                    "sent": "You have a phase shifted a pair like a sine and cosine or something.",
                    "label": 0
                },
                {
                    "sent": "And then you sum over the squares of that function and that's this block.",
                    "label": 0
                },
                {
                    "sent": "Here is what they would call a complex cell.",
                    "label": 0
                },
                {
                    "sent": "I'm not aware of any work that tries to use some mechanism that looks like this in a convent.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's not necessary becausw this rectification followed by pooling or Max pooling is going to.",
                    "label": 0
                },
                {
                    "sent": "Be sufficiently equal to this such that you will end up getting something like the amplitude spectrum on your bottom in the bottom of your network at least.",
                    "label": 0
                },
                {
                    "sent": "But it would be worth trying, probably cause yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, well, it's another way to school this exactly.",
                    "label": 0
                },
                {
                    "sent": "It would be L2 pooling or square pooling, but it's not worth.",
                    "label": 0
                },
                {
                    "sent": "Typically pulling refers to pulling over spatial positions, and this is pooling across feature Maps, and it's not just pulling across feature Maps, it's very very specific kind of pulling your feature Maps come in pairs here in sine, cosine pairs, and then you pull only over these pairs at a time, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a bit more specific than than just what people call Square pulling or anyone pulling, but it is an instance of that, absolutely.",
                    "label": 0
                },
                {
                    "sent": "Even though this is a much older thing, right as square pooling came up, sort of, the term came up recently.",
                    "label": 0
                },
                {
                    "sent": "So you need this pass that can be sign or cosine, what actually they don't have to be signed in cosine, sine and cosine are going to spend your complex plane like this, but it does.",
                    "label": 0
                },
                {
                    "sent": "In practice, you don't care if you spend it like well, like this maybe well for you, it's going to be like like, But anyway, you can also phase shift the whole thing.",
                    "label": 0
                },
                {
                    "sent": "It's not going to make any difference you have this 2 dimensional subspace that complex plane.",
                    "label": 0
                },
                {
                    "sent": "And if you spend it with using these vectors, these vectors of vectors doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "On paper, I mean is right down sine, cosine, you will ended up having used this basis here, but in Euronet can decide to use any other one.",
                    "label": 0
                },
                {
                    "sent": "Since this degeneracy holds that you don't have to use sine cosine, people don't call this sine cosine pair, they call it a quadrature pair.",
                    "label": 0
                },
                {
                    "sent": "That's another word of.",
                    "label": 0
                },
                {
                    "sent": "Characterizing this pair of two features that are waves which are 90 degrees out of phase, which is exactly what's going to help you span the 2D space, which is a complex plane or one frequency phase of 1 particular frequency.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I think I want to conclude by.",
                    "label": 0
                },
                {
                    "sent": "Trying to at least get a little bit in the direction of explaining why.",
                    "label": 0
                },
                {
                    "sent": "Why do we get?",
                    "label": 0
                },
                {
                    "sent": "Gabor features when we train these networks, even though we only showed that we get fully features when we do PCA.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So there's no, I don't have a real answer to that.",
                    "label": 0
                },
                {
                    "sent": "I if someone knows any references or something where people have been trying to explain that, please let me know.",
                    "label": 0
                },
                {
                    "sent": "I think we have some hints, some ideas, even though this is not a complete picture.",
                    "label": 0
                },
                {
                    "sent": "So, so here's another way.",
                    "label": 0
                },
                {
                    "sent": "First of all.",
                    "label": 0
                },
                {
                    "sent": "Of showing that.",
                    "label": 0
                },
                {
                    "sent": "Why PCA you?",
                    "label": 0
                },
                {
                    "sent": "It's for your components necessarily, and there's a very different way of showing that actually so.",
                    "label": 0
                },
                {
                    "sent": "And it talks about invariants so.",
                    "label": 0
                },
                {
                    "sent": "Say your data density is governed by some transformations, let's call them T. And it can be a whole bag of transformations.",
                    "label": 0
                },
                {
                    "sent": "So in all practical cases, as we've seen, this is going to be the set of shifts.",
                    "label": 0
                },
                {
                    "sent": "Local small shifts like shift your image a little bit to the left to the right, up, down, and so on.",
                    "label": 0
                },
                {
                    "sent": "So those are the transformations T shifts Interestingly happen to be orthogonal transformations.",
                    "label": 0
                },
                {
                    "sent": "They are not talking to orthogonal matrices that that can perform that shift.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And I'm talking to matrices useful in this derivation, so that's why I just assume this is orthogonal.",
                    "label": 0
                },
                {
                    "sent": "But we know it holds for shifts, so we're fine.",
                    "label": 0
                },
                {
                    "sent": "And orthogonal matrix is a matrix whose inverse is its transpose.",
                    "label": 0
                },
                {
                    "sent": "So to invert it you just transpose it.",
                    "label": 0
                },
                {
                    "sent": "And you can think of it intuitively as a matrix that just permutes basically pixels or dimensions.",
                    "label": 0
                },
                {
                    "sent": "Just shuffles around ink on your endless digit.",
                    "label": 0
                },
                {
                    "sent": "It doesn't introduce new ink or.",
                    "label": 0
                },
                {
                    "sent": "Remove ink, just move stuff around.",
                    "label": 0
                },
                {
                    "sent": "Kind of basically a permutation.",
                    "label": 0
                },
                {
                    "sent": "And the shift is obviously a permutation.",
                    "label": 0
                },
                {
                    "sent": "It shifts all the pixels by 1 to the left, or maybe by 1/2.",
                    "label": 0
                },
                {
                    "sent": "Then it includes some interpolation, but it's still essentially is something like a permutation, so.",
                    "label": 0
                },
                {
                    "sent": "So imagine that your density is invariant to shifts, so then the log probability of some image X.",
                    "label": 0
                },
                {
                    "sent": "Has to be the same as the log probability of any shifted version of X.",
                    "label": 0
                },
                {
                    "sent": "The probability of this image must be the same as the probability of this.",
                    "label": 0
                },
                {
                    "sent": "And Interestingly, you're going to have a whole orbit, right?",
                    "label": 0
                },
                {
                    "sent": "So if the probability of this is equal to the probability of this, then it must also be equal to the probability of this and this and this and this and this and this and so on, right?",
                    "label": 0
                },
                {
                    "sent": "'cause I can just replace X by TX on the left and then you get T square X on the right so the whole orbit that under the transformation is going to have to be density has to be constant.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So let's just plug in the Gaussian here for a Gaussian you hav E to the minus some covariance matrix, and so on, and then left and right the normalization constants are going to cancer.",
                    "label": 0
                },
                {
                    "sent": "I already took the lock, so you're going to get this stuff that's inside the.",
                    "label": 0
                },
                {
                    "sent": "The exponential right is something like X transpose covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, this T shouldn't be there, so you can see that I just wrote this down last night.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "That's the same as X transport.",
                    "label": 0
                },
                {
                    "sent": "No plug in TX for X, you get X transpose, transpose covariance, TX.",
                    "label": 0
                },
                {
                    "sent": "So does everyone agree that is correct?",
                    "label": 0
                },
                {
                    "sent": "Now this has to hold for all X.",
                    "label": 0
                },
                {
                    "sent": "This invariants and so.",
                    "label": 0
                },
                {
                    "sent": "These matrices have to satisfy that.",
                    "label": 0
                },
                {
                    "sent": "To make that whole 4X.",
                    "label": 0
                },
                {
                    "sent": "So that means Sigma inverse the inverse covariance matrix has to be the same as T Sigma inverse T. But there's no way other way of saying that Sigma and T have to have to be matrices that commute.",
                    "label": 0
                },
                {
                    "sent": "So let's multiply on the left by T. That's going to cancel this away, and we're going to get T Sigma equals Sigma T, and so that means these two matrices commute.",
                    "label": 0
                },
                {
                    "sent": "And So what about two matrices that commute?",
                    "label": 0
                },
                {
                    "sent": "They have the same eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "And so that means the eigenvectors of the covariance matrix have to be the same as all the eigenvectors of all the transformations T that I could choose.",
                    "label": 0
                },
                {
                    "sent": "So Luckily, all shifts have also the same eigen basis and it happens to be at the full year basis.",
                    "label": 0
                },
                {
                    "sent": "So in other words, all made all permutation matrices T which are going to end up shifting something.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna if you do.",
                    "label": 0
                },
                {
                    "sent": "I cough that you will see a wave.",
                    "label": 0
                },
                {
                    "sent": "And so that means the covariance matrix has to have full components as.",
                    "label": 0
                },
                {
                    "sent": "The convectors also.",
                    "label": 0
                },
                {
                    "sent": "So I don't have the result for the.",
                    "label": 0
                },
                {
                    "sent": "For something more interesting than PCA, but I am going to try to write something down.",
                    "label": 0
                },
                {
                    "sent": "Using.",
                    "label": 0
                },
                {
                    "sent": "Johnny works, wonder where.",
                    "label": 0
                },
                {
                    "sent": "However, you pronounce that.",
                    "label": 0
                },
                {
                    "sent": "But this is really speculative and kind of.",
                    "label": 0
                },
                {
                    "sent": "In ongoing thought process, I don't think this is necessarily true, or maybe not even interesting.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have an auto encoder.",
                    "label": 0
                },
                {
                    "sent": "For the sake of the argument, but you can also think of K means cluster oops wow.",
                    "label": 0
                },
                {
                    "sent": "How did?",
                    "label": 0
                },
                {
                    "sent": "How did I do that?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's this thing.",
                    "label": 0
                },
                {
                    "sent": "So they're going to be points on the manifold.",
                    "label": 0
                },
                {
                    "sent": "I think you're sure talked about it quite a bit, and whoops, I think maybe I should use the whiteboard.",
                    "label": 0
                },
                {
                    "sent": "So you can have a point X on a manifold and if that point lies on a mode of the density, which we call manifold typically, then the reconstruction of the of X under the autoencoder is going to be X.",
                    "label": 0
                },
                {
                    "sent": "So we're not going to move that thing around, right?",
                    "label": 0
                },
                {
                    "sent": "So that's yeah.",
                    "label": 0
                },
                {
                    "sent": "Because it is a point which the auto encoder doesn't mess with.",
                    "label": 0
                },
                {
                    "sent": "It's a point that the auto encoder leaves alone right cause.",
                    "label": 0
                },
                {
                    "sent": "Well, I was hoping that you're going to explain that in your lecture becausw autoencoders learn densities.",
                    "label": 0
                },
                {
                    "sent": "Learn the data density.",
                    "label": 0
                },
                {
                    "sent": "Basically, as you're going to show, presumably next week, and so the derivative of the density is going to be 0 at that mode, and so you're not going to be moving around.",
                    "label": 0
                },
                {
                    "sent": "Let me just leave it at that first let me just.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, OK, so that's actually very interesting point.",
                    "label": 0
                },
                {
                    "sent": "So so if you think of the mode as a point, right imagine you have like a mixture of Gaussians, you have a mode which is just a single Gaussian somewhere.",
                    "label": 0
                },
                {
                    "sent": "That is not compatible with this view here and is not compatible with reality becausw if your density is translation invariant, then if you have a mode at some position X, there must be a mode nearby at the shifted X, and in fact along the whole orbit of the transformation class.",
                    "label": 0
                },
                {
                    "sent": "But there cannot be such a thing as a Gaussian as a blob as a mode, just becausw of translation invariants.",
                    "label": 0
                },
                {
                    "sent": "It's a very, very fundamental thing, kind of the mode has to be smeared out.",
                    "label": 0
                },
                {
                    "sent": "If your translation invariant, if not, then you're not invariant, but maybe then there's no opportunity to learn anything and you can just go home right?",
                    "label": 0
                },
                {
                    "sent": "So if you think there is something you can learn, their invariants says that means modes must be stretched out.",
                    "label": 0
                },
                {
                    "sent": "Right Classic is the wrong thing unless that manifold that you get from those transformations is so low dimensional that you can just tile the space densely with little blobs.",
                    "label": 0
                },
                {
                    "sent": "Somehow that's right exactly.",
                    "label": 0
                },
                {
                    "sent": "So if that's true, then I would claim that also R of T X = T X is true, so these are equivalent.",
                    "label": 0
                },
                {
                    "sent": "So if you reconstruct X perfectly, you must reconstruct the translated X perfectly, because we want to get the whole mode.",
                    "label": 0
                },
                {
                    "sent": "Otherwise the autoencoder sucks.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Right, so these two are true and now let me mess around a little bit with these symbols, so I'm going to say R of TX, since this is all the same, is the same as T times.",
                    "label": 0
                },
                {
                    "sent": "Are of.",
                    "label": 0
                },
                {
                    "sent": "X because I can just plug in our of X for X over here right?",
                    "label": 0
                },
                {
                    "sent": "X can be replaced by RX right?",
                    "label": 0
                },
                {
                    "sent": "So let me just do that mathematically.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to say the reconstruction of the transformed image has to be the same as the transformation of the reconstructed image.",
                    "label": 0
                },
                {
                    "sent": "Whenever things get a little bit less messy, Unfortunately, this seems like it has something to do with commuting between the autoencoder transformation and so on, which is nice, but I wasn't able to make this.",
                    "label": 0
                },
                {
                    "sent": "Into a.",
                    "label": 0
                },
                {
                    "sent": "2nd and first line I just replace X power of X, which I'm allowed to do things to the first line.",
                    "label": 0
                },
                {
                    "sent": "Right, I just claimed these two hold and they have two otherwise.",
                    "label": 0
                },
                {
                    "sent": "So how much?",
                    "label": 0
                },
                {
                    "sent": "OK I have 10 minutes left so that should work so.",
                    "label": 0
                },
                {
                    "sent": "So what is the auto encoder?",
                    "label": 0
                },
                {
                    "sent": "Does someone remember what another code is?",
                    "label": 0
                },
                {
                    "sent": "It's a decoding weight matrix times an activation function of some encoded and let's just tie the weights here.",
                    "label": 0
                },
                {
                    "sent": "So sorry, this should be a W. Oh, I wish I had started on the whiteboard.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is the definition of Norco.",
                    "label": 0
                },
                {
                    "sent": "Does everyone agree it's?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, I'm just talking bout OneNote, layout and chorus?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "And also not talking about confidence, so I cannot claim anything about confidence or something.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think I'm just going to move the screen up and.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to cover.",
                    "label": 0
                },
                {
                    "sent": "This area was a window.",
                    "label": 0
                },
                {
                    "sent": "So so out of X = X and R of T, X = T X.",
                    "label": 0
                },
                {
                    "sent": "That's probably true.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right, that's a good point.",
                    "label": 0
                },
                {
                    "sent": "Let me right here.",
                    "label": 0
                },
                {
                    "sent": "Our FX is equal to X and that's the same or that has to hold in addition to R of T, X = T X from which I just infer that R of TX.",
                    "label": 0
                },
                {
                    "sent": "Equals T * R of X, which is another, which is sort of to say that the auto encoder commutes if you want with the translation process.",
                    "label": 0
                },
                {
                    "sent": "But they don't get in each others way in some way.",
                    "label": 0
                },
                {
                    "sent": "Also, yes, absolutely yeah, if you move a little bit and then reconstruct is the same as having moved in the reconstruction, absolutely.",
                    "label": 0
                },
                {
                    "sent": "So that would be a good auto encoder.",
                    "label": 0
                },
                {
                    "sent": "OK possible.",
                    "label": 0
                },
                {
                    "sent": "So the auto encoder.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So what is an autoencoder?",
                    "label": 0
                },
                {
                    "sent": "So T times?",
                    "label": 0
                },
                {
                    "sent": "What do I have to write here?",
                    "label": 0
                },
                {
                    "sent": "As I said, it's a decoder matrix times an activation function applied to a linear projection, maybe plus biases or something.",
                    "label": 0
                },
                {
                    "sent": "And that's has to be the same as W applied to H. Sorry, W times this reconstruction is going to be the decoder matrix applied to H of the transfer.",
                    "label": 0
                },
                {
                    "sent": "Latest image.",
                    "label": 0
                },
                {
                    "sent": "Right, so this has to hold.",
                    "label": 0
                },
                {
                    "sent": "Now we're screwed because H is a nonlinear function.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately cannot do any analysis, and you're kind of lost.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, right so this ends the discussion first of all, but you can come up with the standard trick to linearize around X or something and replace the autoencoder by it's linear approximation, right?",
                    "label": 0
                },
                {
                    "sent": "Just let's not use out of X, which is potentially nonlinear mapping, let's use.",
                    "label": 0
                },
                {
                    "sent": "The Jacobian of our facts as an approximation for the auto encoder at that position.",
                    "label": 0
                },
                {
                    "sent": "Then it's fine.",
                    "label": 0
                },
                {
                    "sent": "There's another argument that's easier eventually, but I'm not going to go into that.",
                    "label": 0
                },
                {
                    "sent": "If you use reluz, you will see you're only going to use a subset of hidden's, and then you end up.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's actually linear everywhere, so you're going to be fine if use red autoencoders.",
                    "label": 0
                },
                {
                    "sent": "But if you want to extend this argument to any auto encoder or K means or something, then you're going to have to go through this.",
                    "label": 0
                },
                {
                    "sent": "So let's linearize what's the derivative of this that you just go back prop here.",
                    "label": 0
                },
                {
                    "sent": "So you're going to do something like W. And I'm going to transpose the whole thing to be able to apply it to the input.",
                    "label": 0
                },
                {
                    "sent": "It's going to be a diagonal matrix of the derivatives of this times W transpose times TX and the same analogue expression here, so it's going to be times W. Times the diagonal matrix of the derivative times W transpose X right?",
                    "label": 0
                },
                {
                    "sent": "And so let's just call this diagonal matrix D or something WD.",
                    "label": 0
                },
                {
                    "sent": "W transpose TX is the same as tee times WDW, transpose X and that again has to hold for all XI claim now, and so if that's true then that has to hold for the matrices themselves.",
                    "label": 0
                },
                {
                    "sent": "Again, right?",
                    "label": 0
                },
                {
                    "sent": "So we're going to have to see.",
                    "label": 0
                },
                {
                    "sent": "WDW transpose T is the same as.",
                    "label": 0
                },
                {
                    "sent": "TWDW transpose, where WDW transposes the linearized autoencoder.",
                    "label": 0
                },
                {
                    "sent": "It is an autoencoder but it'll take an X and reconstruct it linearly as a matrix, but that matrix is Jacobian of euro encoder so it is actually.",
                    "label": 0
                },
                {
                    "sent": "The auto encoder locali.",
                    "label": 0
                },
                {
                    "sent": "So that matrix has to commute again with the translation operation here, and so it has no other choice.",
                    "label": 0
                },
                {
                    "sent": "This WS here.",
                    "label": 0
                },
                {
                    "sent": "Have no other choice but spending the eigenspaces of this T. So for erelu the derivatives are going to be 0 or one, so this D is going to have zeros and ones on it, and so you're going to end up with a subset with a small subset W here and here.",
                    "label": 0
                },
                {
                    "sent": "And so W transpose WWW transpose has to be equal to.",
                    "label": 0
                },
                {
                    "sent": "Times tears are equal to tee times WW transpose.",
                    "label": 0
                },
                {
                    "sent": "In that case, right?",
                    "label": 0
                },
                {
                    "sent": "If the derivative is 1 and so.",
                    "label": 0
                },
                {
                    "sent": "That just means the features have to commute with the transformation class.",
                    "label": 0
                },
                {
                    "sent": "And so if the transformation class is.",
                    "label": 0
                },
                {
                    "sent": "Shift.",
                    "label": 0
                },
                {
                    "sent": "Well, the features must be fully components then.",
                    "label": 0
                },
                {
                    "sent": "But there's one Riddle that's left, of course.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "You don't get for your components.",
                    "label": 0
                },
                {
                    "sent": "You get a couple of features which are full components times a Gaussian yes question.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "AB is the 2nd.",
                    "label": 0
                },
                {
                    "sent": "With this one here.",
                    "label": 0
                },
                {
                    "sent": "OK. Possible I don't see that right now, but we can maybe discuss that offline.",
                    "label": 0
                },
                {
                    "sent": "Well, it's very possible that there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's possible.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is stuff that I wrote on last night, so this is not well.",
                    "label": 0
                },
                {
                    "sent": "So this is like thinking in action, right so?",
                    "label": 0
                },
                {
                    "sent": "So if you have to commute.",
                    "label": 0
                },
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "For this shift feature of this discovery components.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But we don't let the autoencoder, although came in fact came, is clustering is a special case of this for a single W. Just another day here.",
                    "label": 0
                },
                {
                    "sent": "But we don't let it be 4 years becausw we add another constraint, right?",
                    "label": 0
                },
                {
                    "sent": "So in all sensible feature learning models in all future learning models that end up giving you sensible gobble like features.",
                    "label": 0
                },
                {
                    "sent": "You have very very sparse hidden's like all the way through this very very very rare.",
                    "label": 0
                },
                {
                    "sent": "Strange exceptions like this on contractive autoencoder today, short.",
                    "label": 0
                },
                {
                    "sent": "So if the autoencoder comes up with three components that cannot be sparse because they are not sparse, they're going to fire all the time.",
                    "label": 0
                },
                {
                    "sent": "If I show any image, so it's going to have to back off in some way to satisfy the sparsity requirement, so it cannot give you full pure for your components.",
                    "label": 0
                },
                {
                    "sent": "And so the only answer that I can offer is to say well, then it does what engineers do and makes an increasingly small Gaussian window around that wave that we component.",
                    "label": 0
                },
                {
                    "sent": "To satisfy the sparsity requirement and still be approximately commutative with the translate transformation class.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so this is a little bit of research, research, and action.",
                    "label": 0
                },
                {
                    "sent": "Yes, thanks, thank you.",
                    "label": 0
                },
                {
                    "sent": "My last slide is just a question.",
                    "label": 0
                },
                {
                    "sent": "Or just maybe you thought how?",
                    "label": 0
                },
                {
                    "sent": "How the same?",
                    "label": 0
                },
                {
                    "sent": "Or if the same idea should apply to higher layers?",
                    "label": 0
                },
                {
                    "sent": "And I think it's the same idea should apply to higher or higher layers in the network in exactly the same way, because this T shouldn't be.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have to be translation, right?",
                    "label": 0
                },
                {
                    "sent": "It can be any kind of transformation, and if it's if the tears translation?",
                    "label": 0
                },
                {
                    "sent": "Incidentally, that matrix is called circulant matrix.",
                    "label": 0
                },
                {
                    "sent": "That's the matrix that shift things around.",
                    "label": 0
                },
                {
                    "sent": "And there's also a so called block circulant matrix which shifts images around vectorized images around.",
                    "label": 0
                },
                {
                    "sent": "So the eigenvectors of this are for your components.",
                    "label": 0
                },
                {
                    "sent": "That's why everything works out nicely.",
                    "label": 0
                },
                {
                    "sent": "And now if you shift a little bit, as I said, you're going to have to cover the whole orbit.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you're not going to.",
                    "label": 0
                },
                {
                    "sent": "It's not going to.",
                    "label": 0
                },
                {
                    "sent": "Bich",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Correct somewhere?",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In higher layers, you're not going to see shifts, right?",
                    "label": 0
                },
                {
                    "sent": "The hiddens are sorted in some completely arbitrary way.",
                    "label": 0
                },
                {
                    "sent": "You can always replace one by the other.",
                    "label": 0
                },
                {
                    "sent": "You're not going to see nice structures after everything gets shifted around.",
                    "label": 0
                },
                {
                    "sent": "According to the Invariances variance in the data.",
                    "label": 0
                },
                {
                    "sent": "But you're going to see some kind of permutation or approximately permutation matrix on those hidden's that that will give you invariants of the responses of those hidden's.",
                    "label": 0
                },
                {
                    "sent": "And there's a whole deep far reaching area called Fourier transforms on groups which tries to tease apart this kind of.",
                    "label": 0
                },
                {
                    "sent": "Orbits that you would find in a permutation.",
                    "label": 0
                },
                {
                    "sent": "So you could imagine that those permutations have limit cycles like one guy goes to the other and then back and forth, and then one unit travels from here to here and from here to here from here to here and eventually back at the beginning and so on.",
                    "label": 0
                },
                {
                    "sent": "All of these if you sort it, your vector in the right order would just look like little few years because they would just be values that travel around.",
                    "label": 0
                },
                {
                    "sent": "Let's get shifted around right?",
                    "label": 0
                },
                {
                    "sent": "So free free analysis still applies I think, and the same exact same thing.",
                    "label": 0
                },
                {
                    "sent": "Can still happen in higher layers of a network, it's just that they don't express themselves as fully ECOMOG aboard components that are up there.",
                    "label": 0
                },
                {
                    "sent": "And then much harder to analyze becausw.",
                    "label": 0
                },
                {
                    "sent": "It's hard to look at them, right?",
                    "label": 0
                },
                {
                    "sent": "It's hard to see what they do, but it's certainly worth thinking about how to further characterize them or to understand what these invariants in higher layers potentially could be OK, I'm just going to stop here, and if you have any other questions, even though I'm running out of time, you may ask now.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}