{
    "id": "6gqo52ymbf6fd6vgm5tk227jrtrmlkib",
    "title": "Graph Construction and b-Matching for Semi-Supervised Learning",
    "info": {
        "author": [
            "Tony Jebara, Department of Computer Science, Columbia University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_jebara_gcm/",
    "segmentation": [
        [
            "Thanks Kelly and so this is joint work with Jun Wang, an Shih Fu Chang at Columb."
        ],
        [
            "Yeah, and I'll be talking bout semi supervised learning today, but focusing on the graph construction part of the problem.",
            "So there's many ways to construct graphs for semi supervised learning.",
            "The standard ones are neighborhood graphs and kenyeres neighbor graphs.",
            "I'll be talking about how be matching can help, because many of these graphs when they were built from the original data, we don't really have a reliable construction scheme from going from data graphs and then for running the semi supervised learning algorithms.",
            "Then I'll talk about some graph weighting schemes.",
            "People are also exploring.",
            "And then the graph labeling algorithms and so really looking at the cross product of all these three different decisions and seeing how that's going to affect the performance of the semi supervised learning algorithms with experiments at the end.",
            "So this is a little bit of an empirical investigation into the problem."
        ],
        [
            "So what I'll be looking at is semi supervised learning once again where which is most often implemented by building a graph and doing some kind of graph transduction or propagating labels on the graph and so semi supervised learning starts with both labeled data which is typically expensive and scarce and hard to get an lots and lots of unlabeled data which is cheap and abundant.",
            "And the idea is given IID samples from some unknown distribution over this space of Xinput San Y integer labels.",
            "We're going to organize into the labeled set, which is the X&Y data from the labeled part and an unlabeled set.",
            "The X that's unlabeled that doesn't have corresponding wise, and we're going to output missing labels to fill in what's missing over here.",
            "These are the Y hats for the unlabeled part, that set of missing labels, and the hope is these unlabeled predictions will agree with the true missing labels that we never had access to and be close, hopefully in terms of some classification error.",
            "Measurement"
        ],
        [
            "So by far the most popular semi supervised learning techniques are these graph based algorithms that have emerged in the past few years and so there the ideas you do semisupervised learning by first building a graph G from your unlabeled input data and labeled input data alone.",
            "So from excellon ex you you learn a graph G and we typically want this graph to have two properties, we want it to be a sparse graph, so typically people will use something like a nearest neighbor algorithm to build it.",
            "And we also want it sometimes to be a weighted graph, because some edges matter more than others, so those are the two properties will be looking at for building these graphs.",
            "So you first have your input data, you build a graph and then after you have a graph G you use your labels which you populate your graph with the observed labels.",
            "And then you try to predict the unobservables.",
            "The white hats by some kind of labeling algorithm, also called the propagation algorithm.",
            "So there's many such algorithms, including let's say the Laplacian regularization method by Belkin and Yogi.",
            "The previous talks mentioned Gaussian field and harmonic functions.",
            "That's another very good method by Zhu Ghahramani and Lafferty, local and global consistency is another method, Laplacian support vector machines, which are related to the previous Laplacian regularization approach.",
            "And then this other method transduction via alternating minimization, which we've been using lately.",
            "So instead of trying to modify these algorithms are proposing yet another algorithm what this talks about is just let's look at the graph construction step right there instead of focusing on yet another algorithm.",
            "And see how this is actually impacting the performance of Fair amount."
        ],
        [
            "OK, so again, we're given data.",
            "We're going to learn a graph and then typically sparsifying that graph, so we typically have a data set of N samples containing both the labeled an unlabeled data over here in a.",
            "We start by forming a full weighted graph, a complete graph with adjacency matrix A and the way we do that is why typically learning using a kernel function which measures how close pairs the points are, so kernel will output a scalar value, which is typically large when the points are similar close.",
            "So we'll set AIJ to be the kernel between XI and XJ, and so the kernel choice is very open.",
            "It's application dependent.",
            "And typically it's only locally reliable, so nearby points have reliable kernels, but far away points.",
            "The kernels are less reliable and you can also think of the kernel as being equivalent to a distance, and you can imagine instead of a building a distance matrix, which we just defined as the square root of these kernel evaluations.",
            "OK, so the kernel with itself for the pair of points minus two times a kernel between the two points, so this is the fully weighted graph, and then we go out and sparsified that graph by finding some pruning matrix.",
            "P which is zero when you delete the edge in one.",
            "When you when you keep the edge.",
            "So that's the last step of the graph construction.",
            "So here by."
        ],
        [
            "Are the two most popular approaches for building the graph and 1st first sparsifying the graph you set P, which is a binary matrix so that if the distance is less than epsilon, you keep the edge, and otherwise if the distance is larger than epsilon, you delete it.",
            "That's the epsilon neighborhood graph.",
            "The problem with that is that often forms disconnected graphs, and it's because you're using a single epsilon everywhere on your data set.",
            "So one way to make this epsilon adaptive is to use K nearest neighbors, which basically is adapting epsilon based on the number of K, and saying everybody has to have K. Neighbors, so here's the KK nearest neighbors algorithm.",
            "You solve this problem.",
            "By running bikini or stable neighbors algorithm, well, you're basically doing is minimizing the distance is the amount of distance you're using by connecting points with string and subject to the constraint that everybody's got K neighbors.",
            "When I sum this connectivity matrix equals K, you don't connect yourself, so PI0.",
            "And then you symmetrize.",
            "So once you solve for P you take P equals Max of P&P transpose.",
            "So I connected to you and you connected someone else.",
            "Then you'll get both the edges that I I connected with.",
            "And also the ones you selected and so despite its name because of this symmetrization step, K nearest neighbors does not give K nearest neighbors.",
            "OK, you're actually going to get more than K neighbors because of the symmetrization over here.",
            "If instead he said, well, forget about doing the Max, how about them in you can say take the min of both connectivity's.",
            "Then P will sum to less than K not always equal to K. And so here's"
        ],
        [
            "Years in the visualization of the problem, here are these two ring datasets.",
            "There's a small ring in a big ring, and if the small ring is small compared to the big ring, and they run K = 2 nearest neighbors, I get this nice connectivity which I'm happy with.",
            "But if I make the little ring grow more and more, eventually you start getting across the ring connectivity, and if you actually look at some of these points, they have some of them have more than two neighbors.",
            "OK, if the little ring grows.",
            "Similarly, if I start downsampling and I sample fewer points on the same size rings.",
            "At some point the outer points start picking the inner points and then these inner points have many more than K neighbors.",
            "OK, so there's this popularity or crowding effect, and this gets worse.",
            "With high dimensions it grows with this thing called the kissing number, which is 6 in two dimensions, but it's much worse in high dimensions, so you get this over popularity effect.",
            "If everybody in this room picked all their, let's say three nearest neighbors, the people in the middle of the room would end up with many more neighbors that people at the periphery of the room.",
            "So anyone in the middle of the space is going to get a lot of neighbors.",
            "And so there's a way to fix this, and this is what we're proposing.",
            "We"
        ],
        [
            "We're proposing running a B matching algorithm instead, and then you get this type of connectivity.",
            "And here if you examine what happened at the end, you do get B = 2 neighbors everywhere.",
            "OK, so you don't do a greedy algorithm.",
            "You solve this constraint problem so that everybody really has two neighbors while minimizing the total amount of string you're using to connect everybody up.",
            "OK, so it's a less greedy version of Kenyeres neighbors."
        ],
        [
            "The matching is basically going to exploit that explicit symmetry within the optimization problem, so I'm going to minimize the total distance.",
            "With this edge matrix P, subject to the row of P, something to be so everybody has been neighbors, you can't connect yourself and then P is symmetric inside the optimization instead of as a post processing, and so this is also known as Unipart type generalized matching.",
            "And this problem can be solved.",
            "It's been solved since the 60s by the Edmonds Blossom algorithm, and there's a efficient combinatorial solver for solving this.",
            "And it's basically cubic time.",
            "Think of this as a linear program with exponentially many constraints called blossom constraints.",
            "But we can.",
            "We can keep track of those efficiently.",
            "But today, the fastest solvers now actually use Max product belief propagation, which is a technique popularized in machine learning for solving large inference problems.",
            "And it turns out Max product belief propagation solves this problem exactly for bipartite matching.",
            "In BN cubed we showed this in 2007 and then later on salads.",
            "Inchauste said that you can sometimes get these algorithms to run in order N squared under mild assumptions, so message passing or belief propagation solves this big problem.",
            "Very very quickly and some other exactness results on the matching with a scheme where proven later on using a.",
            "Risky integrality and another graph theoretic arguments.",
            "So in other words, this is something we can solve very quickly."
        ],
        [
            "Here's an example of 1 matching and now I'm looking at a bipartite setting.",
            "So turns out this is bipartite one matching.",
            "We're going to match the words here to the advertisers here, and we're given dollar bids, so I get $2.",
            "If I show an ad for Apple when somebody types laptop into my search engine.",
            "So I'm the best dollar solution.",
            "Is this C matrix P matrix over here that should say P which connects laptop to Apple Server to IBM phone to Motorola and that maximizes dollars?",
            "Subject to the constraint that this matrix sums to one row wise and column wise, that's called one matching.",
            "So the matrix sums to one row wise and column wise and I maximize dollars Now this is solvable by the Hungarian algorithm which is also cubic time.",
            "If you implement this as a graphical model, it's very loopy, but it turns out you can get this exactly with Max product in cubic time and you just use C equals negative distances.",
            "To write it on our previous problems where we're trying to minimize distance, here's the version for."
        ],
        [
            "Matching, I just replaced that constraint.",
            "And make you some to be instead of sum to one.",
            "And then I can show 2 ads when someone types laptop into my search engine and I make let's say when laptop is typed I made $2 from Apple and $2 from my VM.",
            "So this generalization is what actually Google uses for the AdWords problem uses an online solver for this, but that's exactly the optimization problem.",
            "The AdWords system uses an.",
            "Again it takes order Bkub for exact map.",
            "If you do this with Max product and we can also apply this.",
            "Bipartite solver to unit partite be matching problems very simply by just running the algorithm."
        ],
        [
            "In the unit partite graph instead of a bipartite graph.",
            "So here is the solver.",
            "It basically sets up this bipartite graph.",
            "Everybody's connected to everybody else.",
            "You have the advertisers as they use and the search words of ease, and then each variable is who do I picked to match up to so XI is who does.",
            "You picked a match to Whyyy is who does VPK to match two and the constraints are enforced in this probability distribution, we exponentiate the dollars that you want to maximize with these five functions.",
            "And then with these side functions you say if I pick you you have to.",
            "Take me back.",
            "You enforce that reciprocity constraint.",
            "So this is just a graphical model.",
            "This is the probability distribution we run Max product on this or the unit partite version and it will give you back the solution."
        ],
        [
            "And so this is basically what's happening.",
            "We're just sending messages on this graph and at the end we settle down where each point basically has two neighbors.",
            "This converges very quickly.",
            "The code is available here, and you can write it in a unit partite setting, as well as a bipartite setting, and then have your be matching.",
            "How am I doing for time?",
            "12 minutes total.",
            "OK so here is."
        ],
        [
            "How much faster this is then the combinatorial solvers it's.",
            "3 plus orders of magnitude faster, and it's because it's exploiting Max product, which is a very fast method.",
            "We've applied this to clustering reply data classification problems, collaborative filtering problems, and the previous talk.",
            "We've applied it to visualization problems and it's extremely fast, and this recent proof shows you that it's actually surprisingly faster than the current combinatorial solvers as well.",
            "When you use Max product.",
            "So I encourage people to download the code here is Sonic."
        ],
        [
            "Apple.",
            "Running this with this is joint work with Blake Shaw for visualization, so if you give me a data set of websites and I want to connect them up, it turns out if I run K nearest neighbors, all these websites seem to connect to these really generic websites in the middle.",
            "These billing websites that have some really generic stuff going on, whereas if I use be matching, I start connecting things up a little more intelligently.",
            "I get MapQuest, Yahoo, Google, eBay over here on the right I get the airlines like Orbitz and American and United Airlines on the left up here.",
            "I get Staples and Best Buy and came out all these store websites over here so things connect up much more reliably in this high dimensional space the way we'd expect them to.",
            "Then if we just run K nearest neighbors and we get this popularity effect, which is really bad in high dimensions.",
            "OK, so once we can."
        ],
        [
            "Acted in sparsified the graph.",
            "There is some weighting schemes.",
            "People exploring these are the standard three weighting schemes.",
            "You can say now that I've learned my sparsification matrix P. My final adjacency matrix W is either just a binary version, so just set W equal to P. That's my adjacency matrix for the graph, or I can use a Gaussian kernel to wait the matrix so it's PIJ time.",
            "Some kernel which prefers points that are closer and they get a little bit higher weight, so it's sparse, but also.",
            "Use an RBF to wait things that are even closer a little higher than things that are not too close and you can use any distance function here.",
            "Another one is the locally linear reconstruction, so this is the railway since all method you set W to reconstruct each point in its neighborhood.",
            "So for each point X, try to reconstruct that point with all the neighbors that PJ lets you use.",
            "And minimize the squared error.",
            "So I'm trying to reconstruct my cornets with the neighbors that P is letting me use, so that gives me wait.",
            "So I make sure those weights sum to one and their positive that's the locally linear reconstruction."
        ],
        [
            "And so now we've obtained the graph, obtained its weights, and we're going to try different labeling algorithms.",
            "So now we have a graph G with adjacency matrix W. We have known labels, YL.",
            "We're going to try to find the unknown labels, why you?",
            "So here are some intermediate matrices we compute from W. One is a degree matrix, which is just the sum of the W's.",
            "One is a Laplacian which is dubbed that degree matrix minus W. There's a normalized Laplacian over here.",
            "And what we're trying to do is use these graph matrices to compute a classification function F which labels the nodes.",
            "And then the final decision is this binary matrix, which basically says is the label for the fifth point equal to J yes or no.",
            "So it's a binary way of labeling, and this is a continuous way.",
            "This is a continuous function of labeling the graph, so we're going around F to get Y, and there's three algorithms, the Gaussian random fields algorithm, local and global consistency and graph transaction by alternating minimization.",
            "So let's try all three algorithms."
        ],
        [
            "For basically labeling this graph, if you give me.",
            "Dark Gray and light Gray.",
            "For two points I should be able to propagate the points and say this is how I want to label the rest of the graph Now, so Gaussian random fields does this very nicely by saying I want my function F to be smoother my Laplacian so minimize the the.",
            "Let's say the non smoothness and enforce these constraints on F and you lock the labels to be equal to the training labels for the F function.",
            "You can solve this very easily with linear algebra and then when you have F you obtain Y by just doing a rounding."
        ],
        [
            "There is a local and global consistency technique which is a softer version.",
            "You penalize the function with the normalized Laplacian instead of the normal of the unnormalized Laplacian and a set of locking F to be equal to the true labels.",
            "You know you pull with least squares with some penalty mu, so it's a software version and then you obtain why from F by rounding and the last method is just."
        ],
        [
            "Is graph transduction by alternating minimization you just treat that same problem we saw before as a bivariate optimization over both Y&F.",
            "So instead of saying fine F and then round it to get Y, you say there's two variables, there's F&Y.",
            "And here's the function looks a lot like the one before you just minimize the disagreement between F&YV is a basically a diagonal matrix that captures the class proportions.",
            "But instead of saying learn F and then use F to predict Y, you're minimizing jointly over F&Y in an iterative scheme.",
            "And so given the current F, you just update this this matrix Y which is binary in a greedy manner, and you're basically solving a generalization of.",
            "A common tutorial problem that's that's hard, but you can.",
            "You can do this greedily very quickly.",
            "OK, so this is now solving over the binary matrix."
        ],
        [
            "As well.",
            "So some experiments.",
            "Here's a synthetic experiment.",
            "This is 2 rings where the inner ring is that you much smaller and not in the middle, and we just sample two rings and just said, let's try the three different connectivity schemes.",
            "So remember there were three connectivity schemes, three weighting schemes, and three algorithms.",
            "We're just going to look at the cross product of all of them.",
            "So here is the epsilon neighborhood connectivity graph.",
            "Here's the key nearest neighbor graph with K = 10.",
            "And here's the be matching graph with B = 10 and what's interesting is even though the number of neighbors is the same, you can see a few a few less edges in the be matching graph across the two rings, which is good.",
            "Here are the results from basically 50 fold.",
            "Labeling with the three algorithms were not showing G Town because it does really well.",
            "Here's the LG C algorithm and its error rate with K nearest neighbors.",
            "As you vary the Gaussian kernel for the weighting versus be matching, which is much lower error.",
            "Similarly, here is the.",
            "The GRF method again Kenny nearest neighbors much worse than be matching here we're using, I think.",
            "Thank you, it's it's cross validated over K&B and we're just sweeping across the different sigmas in the Gaussian kernel.",
            "Similarly, here are 50."
        ],
        [
            "Experiments where for LGCGRF&G time where we try different connectivity scheme.",
            "So on Green we've got Kenyeres neighbors versus be matching and you can see lower error consistently in across all three classifiers.",
            "And also we're trying the different weighting schemes binary Gaussian, an locally linear reconstruction.",
            "So across all the different weighting schemes.",
            "You can see an improvement by doing the matching a set of K nearest neighbors on this synthetic problem for all three algorithms, and we typically see that jitam is doing a little bit better because it's doing this optimization jointly over Y&F.",
            "Queso."
        ],
        [
            "Here are some experiments on real datasets.",
            "We tried all possible combinations of the three algorithms, three weighting schemes, and three sparsification schemes, and we also tried some standard semi supervised learning methods over here.",
            "In the top left and you can see that.",
            "Basically, jitam on these four different datasets, the USPS data set the coil data set, PCI and text data set does better, especially when you use be matching and either Gaussian or locally linear weighting, and sometimes these differences are dramatically better than the canyons neighbor algorithms which few jump up to, let's say K nearest neighbors.",
            "You see a big change.",
            "And a kind of higher error rate."
        ],
        [
            "Anne.",
            "The advantage seems to get less, and if you have more and more labels, so if you give me a lot of labeled data then the bad graphene can use neighbors isn't as disastrous, and so here we see the USPS and text datasets with 10 examples or 100 examples labeled.",
            "And then there's still an advantage for be matching and this this graph transduction alternating minimization algorithm.",
            "You're still doing better, but for example here the K nearest neighbors is very close to.",
            "The be matching and actually performs in this one setting.",
            "When you have enough labels, so having enough labels fixes these problems.",
            "So if you're.",
            "If you're not really doing semi supervised learning, these issues aren't as important.",
            "These graph construction issues."
        ],
        [
            "So just to wrap up the graph construction method actually is a very important part of the semi supervised learning, almost as important as the label propagation algorithms themselves.",
            "We investigated three ways of Sparsifying, three ways of waiting, the graphs and three different algorithms.",
            "In general.",
            "This jitam method has better accuracy than the other algorithms and unreal data.",
            "We've noticed problems with the K nearest neighbors sparsification.",
            "It seems to create very irregular graphs where at the end some noise nodes have.",
            "Really hundreds of neighbors even though you're running K = 10, and so for regularity you should use be matching and ensures you've got this regular graph and a more balanced manifold where things aren't really bunched up and it consistently improves performance over 10 years.",
            "Neighbors.",
            "In these experiments there's fast code that's available now using Max product on the link I showed, and the runtime now is no longer really an issue, so you're not have to solve a giant combinatorial problem.",
            "Running.",
            "Be matching is not the slowest part of this algorithm, and then.",
            "Right now we're also working on theoretical guarantees that guarantee why.",
            "Let's say, the error rate for be matching should be better than the error rate for K nearest neighbors, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks Kelly and so this is joint work with Jun Wang, an Shih Fu Chang at Columb.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, and I'll be talking bout semi supervised learning today, but focusing on the graph construction part of the problem.",
                    "label": 0
                },
                {
                    "sent": "So there's many ways to construct graphs for semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "The standard ones are neighborhood graphs and kenyeres neighbor graphs.",
                    "label": 1
                },
                {
                    "sent": "I'll be talking about how be matching can help, because many of these graphs when they were built from the original data, we don't really have a reliable construction scheme from going from data graphs and then for running the semi supervised learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "Then I'll talk about some graph weighting schemes.",
                    "label": 0
                },
                {
                    "sent": "People are also exploring.",
                    "label": 0
                },
                {
                    "sent": "And then the graph labeling algorithms and so really looking at the cross product of all these three different decisions and seeing how that's going to affect the performance of the semi supervised learning algorithms with experiments at the end.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit of an empirical investigation into the problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'll be looking at is semi supervised learning once again where which is most often implemented by building a graph and doing some kind of graph transduction or propagating labels on the graph and so semi supervised learning starts with both labeled data which is typically expensive and scarce and hard to get an lots and lots of unlabeled data which is cheap and abundant.",
                    "label": 1
                },
                {
                    "sent": "And the idea is given IID samples from some unknown distribution over this space of Xinput San Y integer labels.",
                    "label": 0
                },
                {
                    "sent": "We're going to organize into the labeled set, which is the X&Y data from the labeled part and an unlabeled set.",
                    "label": 0
                },
                {
                    "sent": "The X that's unlabeled that doesn't have corresponding wise, and we're going to output missing labels to fill in what's missing over here.",
                    "label": 0
                },
                {
                    "sent": "These are the Y hats for the unlabeled part, that set of missing labels, and the hope is these unlabeled predictions will agree with the true missing labels that we never had access to and be close, hopefully in terms of some classification error.",
                    "label": 0
                },
                {
                    "sent": "Measurement",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So by far the most popular semi supervised learning techniques are these graph based algorithms that have emerged in the past few years and so there the ideas you do semisupervised learning by first building a graph G from your unlabeled input data and labeled input data alone.",
                    "label": 0
                },
                {
                    "sent": "So from excellon ex you you learn a graph G and we typically want this graph to have two properties, we want it to be a sparse graph, so typically people will use something like a nearest neighbor algorithm to build it.",
                    "label": 1
                },
                {
                    "sent": "And we also want it sometimes to be a weighted graph, because some edges matter more than others, so those are the two properties will be looking at for building these graphs.",
                    "label": 0
                },
                {
                    "sent": "So you first have your input data, you build a graph and then after you have a graph G you use your labels which you populate your graph with the observed labels.",
                    "label": 0
                },
                {
                    "sent": "And then you try to predict the unobservables.",
                    "label": 0
                },
                {
                    "sent": "The white hats by some kind of labeling algorithm, also called the propagation algorithm.",
                    "label": 0
                },
                {
                    "sent": "So there's many such algorithms, including let's say the Laplacian regularization method by Belkin and Yogi.",
                    "label": 1
                },
                {
                    "sent": "The previous talks mentioned Gaussian field and harmonic functions.",
                    "label": 1
                },
                {
                    "sent": "That's another very good method by Zhu Ghahramani and Lafferty, local and global consistency is another method, Laplacian support vector machines, which are related to the previous Laplacian regularization approach.",
                    "label": 1
                },
                {
                    "sent": "And then this other method transduction via alternating minimization, which we've been using lately.",
                    "label": 0
                },
                {
                    "sent": "So instead of trying to modify these algorithms are proposing yet another algorithm what this talks about is just let's look at the graph construction step right there instead of focusing on yet another algorithm.",
                    "label": 0
                },
                {
                    "sent": "And see how this is actually impacting the performance of Fair amount.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so again, we're given data.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn a graph and then typically sparsifying that graph, so we typically have a data set of N samples containing both the labeled an unlabeled data over here in a.",
                    "label": 0
                },
                {
                    "sent": "We start by forming a full weighted graph, a complete graph with adjacency matrix A and the way we do that is why typically learning using a kernel function which measures how close pairs the points are, so kernel will output a scalar value, which is typically large when the points are similar close.",
                    "label": 1
                },
                {
                    "sent": "So we'll set AIJ to be the kernel between XI and XJ, and so the kernel choice is very open.",
                    "label": 0
                },
                {
                    "sent": "It's application dependent.",
                    "label": 1
                },
                {
                    "sent": "And typically it's only locally reliable, so nearby points have reliable kernels, but far away points.",
                    "label": 0
                },
                {
                    "sent": "The kernels are less reliable and you can also think of the kernel as being equivalent to a distance, and you can imagine instead of a building a distance matrix, which we just defined as the square root of these kernel evaluations.",
                    "label": 0
                },
                {
                    "sent": "OK, so the kernel with itself for the pair of points minus two times a kernel between the two points, so this is the fully weighted graph, and then we go out and sparsified that graph by finding some pruning matrix.",
                    "label": 1
                },
                {
                    "sent": "P which is zero when you delete the edge in one.",
                    "label": 0
                },
                {
                    "sent": "When you when you keep the edge.",
                    "label": 0
                },
                {
                    "sent": "So that's the last step of the graph construction.",
                    "label": 0
                },
                {
                    "sent": "So here by.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are the two most popular approaches for building the graph and 1st first sparsifying the graph you set P, which is a binary matrix so that if the distance is less than epsilon, you keep the edge, and otherwise if the distance is larger than epsilon, you delete it.",
                    "label": 0
                },
                {
                    "sent": "That's the epsilon neighborhood graph.",
                    "label": 0
                },
                {
                    "sent": "The problem with that is that often forms disconnected graphs, and it's because you're using a single epsilon everywhere on your data set.",
                    "label": 1
                },
                {
                    "sent": "So one way to make this epsilon adaptive is to use K nearest neighbors, which basically is adapting epsilon based on the number of K, and saying everybody has to have K. Neighbors, so here's the KK nearest neighbors algorithm.",
                    "label": 0
                },
                {
                    "sent": "You solve this problem.",
                    "label": 0
                },
                {
                    "sent": "By running bikini or stable neighbors algorithm, well, you're basically doing is minimizing the distance is the amount of distance you're using by connecting points with string and subject to the constraint that everybody's got K neighbors.",
                    "label": 0
                },
                {
                    "sent": "When I sum this connectivity matrix equals K, you don't connect yourself, so PI0.",
                    "label": 0
                },
                {
                    "sent": "And then you symmetrize.",
                    "label": 0
                },
                {
                    "sent": "So once you solve for P you take P equals Max of P&P transpose.",
                    "label": 0
                },
                {
                    "sent": "So I connected to you and you connected someone else.",
                    "label": 0
                },
                {
                    "sent": "Then you'll get both the edges that I I connected with.",
                    "label": 1
                },
                {
                    "sent": "And also the ones you selected and so despite its name because of this symmetrization step, K nearest neighbors does not give K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "OK, you're actually going to get more than K neighbors because of the symmetrization over here.",
                    "label": 0
                },
                {
                    "sent": "If instead he said, well, forget about doing the Max, how about them in you can say take the min of both connectivity's.",
                    "label": 0
                },
                {
                    "sent": "Then P will sum to less than K not always equal to K. And so here's",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Years in the visualization of the problem, here are these two ring datasets.",
                    "label": 0
                },
                {
                    "sent": "There's a small ring in a big ring, and if the small ring is small compared to the big ring, and they run K = 2 nearest neighbors, I get this nice connectivity which I'm happy with.",
                    "label": 0
                },
                {
                    "sent": "But if I make the little ring grow more and more, eventually you start getting across the ring connectivity, and if you actually look at some of these points, they have some of them have more than two neighbors.",
                    "label": 0
                },
                {
                    "sent": "OK, if the little ring grows.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if I start downsampling and I sample fewer points on the same size rings.",
                    "label": 0
                },
                {
                    "sent": "At some point the outer points start picking the inner points and then these inner points have many more than K neighbors.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's this popularity or crowding effect, and this gets worse.",
                    "label": 0
                },
                {
                    "sent": "With high dimensions it grows with this thing called the kissing number, which is 6 in two dimensions, but it's much worse in high dimensions, so you get this over popularity effect.",
                    "label": 0
                },
                {
                    "sent": "If everybody in this room picked all their, let's say three nearest neighbors, the people in the middle of the room would end up with many more neighbors that people at the periphery of the room.",
                    "label": 0
                },
                {
                    "sent": "So anyone in the middle of the space is going to get a lot of neighbors.",
                    "label": 0
                },
                {
                    "sent": "And so there's a way to fix this, and this is what we're proposing.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're proposing running a B matching algorithm instead, and then you get this type of connectivity.",
                    "label": 0
                },
                {
                    "sent": "And here if you examine what happened at the end, you do get B = 2 neighbors everywhere.",
                    "label": 1
                },
                {
                    "sent": "OK, so you don't do a greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "You solve this constraint problem so that everybody really has two neighbors while minimizing the total amount of string you're using to connect everybody up.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a less greedy version of Kenyeres neighbors.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The matching is basically going to exploit that explicit symmetry within the optimization problem, so I'm going to minimize the total distance.",
                    "label": 0
                },
                {
                    "sent": "With this edge matrix P, subject to the row of P, something to be so everybody has been neighbors, you can't connect yourself and then P is symmetric inside the optimization instead of as a post processing, and so this is also known as Unipart type generalized matching.",
                    "label": 0
                },
                {
                    "sent": "And this problem can be solved.",
                    "label": 0
                },
                {
                    "sent": "It's been solved since the 60s by the Edmonds Blossom algorithm, and there's a efficient combinatorial solver for solving this.",
                    "label": 0
                },
                {
                    "sent": "And it's basically cubic time.",
                    "label": 0
                },
                {
                    "sent": "Think of this as a linear program with exponentially many constraints called blossom constraints.",
                    "label": 0
                },
                {
                    "sent": "But we can.",
                    "label": 0
                },
                {
                    "sent": "We can keep track of those efficiently.",
                    "label": 0
                },
                {
                    "sent": "But today, the fastest solvers now actually use Max product belief propagation, which is a technique popularized in machine learning for solving large inference problems.",
                    "label": 1
                },
                {
                    "sent": "And it turns out Max product belief propagation solves this problem exactly for bipartite matching.",
                    "label": 0
                },
                {
                    "sent": "In BN cubed we showed this in 2007 and then later on salads.",
                    "label": 0
                },
                {
                    "sent": "Inchauste said that you can sometimes get these algorithms to run in order N squared under mild assumptions, so message passing or belief propagation solves this big problem.",
                    "label": 0
                },
                {
                    "sent": "Very very quickly and some other exactness results on the matching with a scheme where proven later on using a.",
                    "label": 0
                },
                {
                    "sent": "Risky integrality and another graph theoretic arguments.",
                    "label": 0
                },
                {
                    "sent": "So in other words, this is something we can solve very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's an example of 1 matching and now I'm looking at a bipartite setting.",
                    "label": 0
                },
                {
                    "sent": "So turns out this is bipartite one matching.",
                    "label": 0
                },
                {
                    "sent": "We're going to match the words here to the advertisers here, and we're given dollar bids, so I get $2.",
                    "label": 0
                },
                {
                    "sent": "If I show an ad for Apple when somebody types laptop into my search engine.",
                    "label": 0
                },
                {
                    "sent": "So I'm the best dollar solution.",
                    "label": 0
                },
                {
                    "sent": "Is this C matrix P matrix over here that should say P which connects laptop to Apple Server to IBM phone to Motorola and that maximizes dollars?",
                    "label": 0
                },
                {
                    "sent": "Subject to the constraint that this matrix sums to one row wise and column wise, that's called one matching.",
                    "label": 0
                },
                {
                    "sent": "So the matrix sums to one row wise and column wise and I maximize dollars Now this is solvable by the Hungarian algorithm which is also cubic time.",
                    "label": 0
                },
                {
                    "sent": "If you implement this as a graphical model, it's very loopy, but it turns out you can get this exactly with Max product in cubic time and you just use C equals negative distances.",
                    "label": 1
                },
                {
                    "sent": "To write it on our previous problems where we're trying to minimize distance, here's the version for.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matching, I just replaced that constraint.",
                    "label": 0
                },
                {
                    "sent": "And make you some to be instead of sum to one.",
                    "label": 0
                },
                {
                    "sent": "And then I can show 2 ads when someone types laptop into my search engine and I make let's say when laptop is typed I made $2 from Apple and $2 from my VM.",
                    "label": 0
                },
                {
                    "sent": "So this generalization is what actually Google uses for the AdWords problem uses an online solver for this, but that's exactly the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "The AdWords system uses an.",
                    "label": 0
                },
                {
                    "sent": "Again it takes order Bkub for exact map.",
                    "label": 1
                },
                {
                    "sent": "If you do this with Max product and we can also apply this.",
                    "label": 0
                },
                {
                    "sent": "Bipartite solver to unit partite be matching problems very simply by just running the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the unit partite graph instead of a bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "So here is the solver.",
                    "label": 0
                },
                {
                    "sent": "It basically sets up this bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "Everybody's connected to everybody else.",
                    "label": 0
                },
                {
                    "sent": "You have the advertisers as they use and the search words of ease, and then each variable is who do I picked to match up to so XI is who does.",
                    "label": 0
                },
                {
                    "sent": "You picked a match to Whyyy is who does VPK to match two and the constraints are enforced in this probability distribution, we exponentiate the dollars that you want to maximize with these five functions.",
                    "label": 0
                },
                {
                    "sent": "And then with these side functions you say if I pick you you have to.",
                    "label": 0
                },
                {
                    "sent": "Take me back.",
                    "label": 0
                },
                {
                    "sent": "You enforce that reciprocity constraint.",
                    "label": 0
                },
                {
                    "sent": "So this is just a graphical model.",
                    "label": 0
                },
                {
                    "sent": "This is the probability distribution we run Max product on this or the unit partite version and it will give you back the solution.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is basically what's happening.",
                    "label": 0
                },
                {
                    "sent": "We're just sending messages on this graph and at the end we settle down where each point basically has two neighbors.",
                    "label": 0
                },
                {
                    "sent": "This converges very quickly.",
                    "label": 0
                },
                {
                    "sent": "The code is available here, and you can write it in a unit partite setting, as well as a bipartite setting, and then have your be matching.",
                    "label": 0
                },
                {
                    "sent": "How am I doing for time?",
                    "label": 0
                },
                {
                    "sent": "12 minutes total.",
                    "label": 0
                },
                {
                    "sent": "OK so here is.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How much faster this is then the combinatorial solvers it's.",
                    "label": 0
                },
                {
                    "sent": "3 plus orders of magnitude faster, and it's because it's exploiting Max product, which is a very fast method.",
                    "label": 0
                },
                {
                    "sent": "We've applied this to clustering reply data classification problems, collaborative filtering problems, and the previous talk.",
                    "label": 0
                },
                {
                    "sent": "We've applied it to visualization problems and it's extremely fast, and this recent proof shows you that it's actually surprisingly faster than the current combinatorial solvers as well.",
                    "label": 0
                },
                {
                    "sent": "When you use Max product.",
                    "label": 0
                },
                {
                    "sent": "So I encourage people to download the code here is Sonic.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apple.",
                    "label": 0
                },
                {
                    "sent": "Running this with this is joint work with Blake Shaw for visualization, so if you give me a data set of websites and I want to connect them up, it turns out if I run K nearest neighbors, all these websites seem to connect to these really generic websites in the middle.",
                    "label": 0
                },
                {
                    "sent": "These billing websites that have some really generic stuff going on, whereas if I use be matching, I start connecting things up a little more intelligently.",
                    "label": 0
                },
                {
                    "sent": "I get MapQuest, Yahoo, Google, eBay over here on the right I get the airlines like Orbitz and American and United Airlines on the left up here.",
                    "label": 0
                },
                {
                    "sent": "I get Staples and Best Buy and came out all these store websites over here so things connect up much more reliably in this high dimensional space the way we'd expect them to.",
                    "label": 0
                },
                {
                    "sent": "Then if we just run K nearest neighbors and we get this popularity effect, which is really bad in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "OK, so once we can.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Acted in sparsified the graph.",
                    "label": 0
                },
                {
                    "sent": "There is some weighting schemes.",
                    "label": 0
                },
                {
                    "sent": "People exploring these are the standard three weighting schemes.",
                    "label": 0
                },
                {
                    "sent": "You can say now that I've learned my sparsification matrix P. My final adjacency matrix W is either just a binary version, so just set W equal to P. That's my adjacency matrix for the graph, or I can use a Gaussian kernel to wait the matrix so it's PIJ time.",
                    "label": 1
                },
                {
                    "sent": "Some kernel which prefers points that are closer and they get a little bit higher weight, so it's sparse, but also.",
                    "label": 0
                },
                {
                    "sent": "Use an RBF to wait things that are even closer a little higher than things that are not too close and you can use any distance function here.",
                    "label": 0
                },
                {
                    "sent": "Another one is the locally linear reconstruction, so this is the railway since all method you set W to reconstruct each point in its neighborhood.",
                    "label": 1
                },
                {
                    "sent": "So for each point X, try to reconstruct that point with all the neighbors that PJ lets you use.",
                    "label": 0
                },
                {
                    "sent": "And minimize the squared error.",
                    "label": 0
                },
                {
                    "sent": "So I'm trying to reconstruct my cornets with the neighbors that P is letting me use, so that gives me wait.",
                    "label": 0
                },
                {
                    "sent": "So I make sure those weights sum to one and their positive that's the locally linear reconstruction.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so now we've obtained the graph, obtained its weights, and we're going to try different labeling algorithms.",
                    "label": 0
                },
                {
                    "sent": "So now we have a graph G with adjacency matrix W. We have known labels, YL.",
                    "label": 1
                },
                {
                    "sent": "We're going to try to find the unknown labels, why you?",
                    "label": 0
                },
                {
                    "sent": "So here are some intermediate matrices we compute from W. One is a degree matrix, which is just the sum of the W's.",
                    "label": 0
                },
                {
                    "sent": "One is a Laplacian which is dubbed that degree matrix minus W. There's a normalized Laplacian over here.",
                    "label": 0
                },
                {
                    "sent": "And what we're trying to do is use these graph matrices to compute a classification function F which labels the nodes.",
                    "label": 0
                },
                {
                    "sent": "And then the final decision is this binary matrix, which basically says is the label for the fifth point equal to J yes or no.",
                    "label": 0
                },
                {
                    "sent": "So it's a binary way of labeling, and this is a continuous way.",
                    "label": 0
                },
                {
                    "sent": "This is a continuous function of labeling the graph, so we're going around F to get Y, and there's three algorithms, the Gaussian random fields algorithm, local and global consistency and graph transaction by alternating minimization.",
                    "label": 1
                },
                {
                    "sent": "So let's try all three algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For basically labeling this graph, if you give me.",
                    "label": 0
                },
                {
                    "sent": "Dark Gray and light Gray.",
                    "label": 0
                },
                {
                    "sent": "For two points I should be able to propagate the points and say this is how I want to label the rest of the graph Now, so Gaussian random fields does this very nicely by saying I want my function F to be smoother my Laplacian so minimize the the.",
                    "label": 0
                },
                {
                    "sent": "Let's say the non smoothness and enforce these constraints on F and you lock the labels to be equal to the training labels for the F function.",
                    "label": 0
                },
                {
                    "sent": "You can solve this very easily with linear algebra and then when you have F you obtain Y by just doing a rounding.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is a local and global consistency technique which is a softer version.",
                    "label": 1
                },
                {
                    "sent": "You penalize the function with the normalized Laplacian instead of the normal of the unnormalized Laplacian and a set of locking F to be equal to the true labels.",
                    "label": 0
                },
                {
                    "sent": "You know you pull with least squares with some penalty mu, so it's a software version and then you obtain why from F by rounding and the last method is just.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is graph transduction by alternating minimization you just treat that same problem we saw before as a bivariate optimization over both Y&F.",
                    "label": 1
                },
                {
                    "sent": "So instead of saying fine F and then round it to get Y, you say there's two variables, there's F&Y.",
                    "label": 1
                },
                {
                    "sent": "And here's the function looks a lot like the one before you just minimize the disagreement between F&YV is a basically a diagonal matrix that captures the class proportions.",
                    "label": 1
                },
                {
                    "sent": "But instead of saying learn F and then use F to predict Y, you're minimizing jointly over F&Y in an iterative scheme.",
                    "label": 0
                },
                {
                    "sent": "And so given the current F, you just update this this matrix Y which is binary in a greedy manner, and you're basically solving a generalization of.",
                    "label": 0
                },
                {
                    "sent": "A common tutorial problem that's that's hard, but you can.",
                    "label": 0
                },
                {
                    "sent": "You can do this greedily very quickly.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is now solving over the binary matrix.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "So some experiments.",
                    "label": 0
                },
                {
                    "sent": "Here's a synthetic experiment.",
                    "label": 0
                },
                {
                    "sent": "This is 2 rings where the inner ring is that you much smaller and not in the middle, and we just sample two rings and just said, let's try the three different connectivity schemes.",
                    "label": 0
                },
                {
                    "sent": "So remember there were three connectivity schemes, three weighting schemes, and three algorithms.",
                    "label": 0
                },
                {
                    "sent": "We're just going to look at the cross product of all of them.",
                    "label": 0
                },
                {
                    "sent": "So here is the epsilon neighborhood connectivity graph.",
                    "label": 0
                },
                {
                    "sent": "Here's the key nearest neighbor graph with K = 10.",
                    "label": 1
                },
                {
                    "sent": "And here's the be matching graph with B = 10 and what's interesting is even though the number of neighbors is the same, you can see a few a few less edges in the be matching graph across the two rings, which is good.",
                    "label": 0
                },
                {
                    "sent": "Here are the results from basically 50 fold.",
                    "label": 0
                },
                {
                    "sent": "Labeling with the three algorithms were not showing G Town because it does really well.",
                    "label": 0
                },
                {
                    "sent": "Here's the LG C algorithm and its error rate with K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "As you vary the Gaussian kernel for the weighting versus be matching, which is much lower error.",
                    "label": 0
                },
                {
                    "sent": "Similarly, here is the.",
                    "label": 0
                },
                {
                    "sent": "The GRF method again Kenny nearest neighbors much worse than be matching here we're using, I think.",
                    "label": 0
                },
                {
                    "sent": "Thank you, it's it's cross validated over K&B and we're just sweeping across the different sigmas in the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "Similarly, here are 50.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experiments where for LGCGRF&G time where we try different connectivity scheme.",
                    "label": 0
                },
                {
                    "sent": "So on Green we've got Kenyeres neighbors versus be matching and you can see lower error consistently in across all three classifiers.",
                    "label": 0
                },
                {
                    "sent": "And also we're trying the different weighting schemes binary Gaussian, an locally linear reconstruction.",
                    "label": 0
                },
                {
                    "sent": "So across all the different weighting schemes.",
                    "label": 0
                },
                {
                    "sent": "You can see an improvement by doing the matching a set of K nearest neighbors on this synthetic problem for all three algorithms, and we typically see that jitam is doing a little bit better because it's doing this optimization jointly over Y&F.",
                    "label": 0
                },
                {
                    "sent": "Queso.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some experiments on real datasets.",
                    "label": 0
                },
                {
                    "sent": "We tried all possible combinations of the three algorithms, three weighting schemes, and three sparsification schemes, and we also tried some standard semi supervised learning methods over here.",
                    "label": 0
                },
                {
                    "sent": "In the top left and you can see that.",
                    "label": 0
                },
                {
                    "sent": "Basically, jitam on these four different datasets, the USPS data set the coil data set, PCI and text data set does better, especially when you use be matching and either Gaussian or locally linear weighting, and sometimes these differences are dramatically better than the canyons neighbor algorithms which few jump up to, let's say K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "You see a big change.",
                    "label": 0
                },
                {
                    "sent": "And a kind of higher error rate.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The advantage seems to get less, and if you have more and more labels, so if you give me a lot of labeled data then the bad graphene can use neighbors isn't as disastrous, and so here we see the USPS and text datasets with 10 examples or 100 examples labeled.",
                    "label": 0
                },
                {
                    "sent": "And then there's still an advantage for be matching and this this graph transduction alternating minimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "You're still doing better, but for example here the K nearest neighbors is very close to.",
                    "label": 0
                },
                {
                    "sent": "The be matching and actually performs in this one setting.",
                    "label": 0
                },
                {
                    "sent": "When you have enough labels, so having enough labels fixes these problems.",
                    "label": 0
                },
                {
                    "sent": "So if you're.",
                    "label": 0
                },
                {
                    "sent": "If you're not really doing semi supervised learning, these issues aren't as important.",
                    "label": 0
                },
                {
                    "sent": "These graph construction issues.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to wrap up the graph construction method actually is a very important part of the semi supervised learning, almost as important as the label propagation algorithms themselves.",
                    "label": 1
                },
                {
                    "sent": "We investigated three ways of Sparsifying, three ways of waiting, the graphs and three different algorithms.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "This jitam method has better accuracy than the other algorithms and unreal data.",
                    "label": 1
                },
                {
                    "sent": "We've noticed problems with the K nearest neighbors sparsification.",
                    "label": 0
                },
                {
                    "sent": "It seems to create very irregular graphs where at the end some noise nodes have.",
                    "label": 0
                },
                {
                    "sent": "Really hundreds of neighbors even though you're running K = 10, and so for regularity you should use be matching and ensures you've got this regular graph and a more balanced manifold where things aren't really bunched up and it consistently improves performance over 10 years.",
                    "label": 0
                },
                {
                    "sent": "Neighbors.",
                    "label": 0
                },
                {
                    "sent": "In these experiments there's fast code that's available now using Max product on the link I showed, and the runtime now is no longer really an issue, so you're not have to solve a giant combinatorial problem.",
                    "label": 0
                },
                {
                    "sent": "Running.",
                    "label": 1
                },
                {
                    "sent": "Be matching is not the slowest part of this algorithm, and then.",
                    "label": 1
                },
                {
                    "sent": "Right now we're also working on theoretical guarantees that guarantee why.",
                    "label": 0
                },
                {
                    "sent": "Let's say, the error rate for be matching should be better than the error rate for K nearest neighbors, thank you.",
                    "label": 0
                }
            ]
        }
    }
}