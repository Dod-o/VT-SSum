{
    "id": "7kzygtwkpms3hpzky53yvn2dvmijuw4l",
    "title": "Sparse Bayesian Nonparametric Regression",
    "info": {
        "author": [
            "Fran\u00e7ois Caron, INRIA Bordeaux - Sud-Ouest"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Regression",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_caron_sbnr/",
    "segmentation": [
        [
            "So I'm from Socal, so I'm from the Department of Computer Science and Statistics at the University of British Columbia, and so this is joint work with Arnold USA."
        ],
        [
            "So I'm interested in the classical linear regression models where Y is equal to X beta plus some Gaussian noise EPS."
        ],
        [
            "Done.",
            "Where Y is an observation of dimension.",
            "LX is some known design matrix of size L * K and beta is an unknown vector of."
        ],
        [
            "Dimension key.",
            "So we are interested in finding sparse estimate of the vector beta, usually in the setting where the dimension of this vector K is much larger than the dimension of the observation.",
            "So K is much."
        ],
        [
            "Other than L. So this can be for the sake of variable selection for the decomposition of a signal over an overcomplete basis and as in."
        ],
        [
            "Compressed sensing.",
            "Ann to address these problems.",
            "So numerous models and algorithm having pro."
        ],
        [
            "'cause in the red?"
        ],
        [
            "Try to like the beige and spike in slab the last."
        ],
        [
            "So.",
            "Or the relevance vector machine, so we will use here Bayesian approach with a maximum of posterior ES."
        ],
        [
            "Made to address this problem.",
            "So we consider general that we define prior distribution over the unknown vector beta.",
            "So where we assume independence over all the feature case.",
            "So P of beta is the product for K = 1, two Capital K of P of beta."
        ],
        [
            "Cake.",
            "An we are interested in finding so local local minimum of the of the objective function.",
            "Well, minus log of P of beta can be interpreted as.",
            "Panelization"
        ],
        [
            "And we focus here on scale mixture of Gaussians.",
            "So where P of beta K can take this?"
        ],
        [
            "This form.",
            "So a lot of people have worked on on this on this kind of model, so if P of beta key is is a LAPLACE prior, then equation 2 reduces to the lasso objective function, which is convex."
        ],
        [
            "Figure it was proposed to use a normal, normal Jeffreys distribution."
        ],
        [
            "Ann Griffin, as discussed, a lot of distribution and especially the normal exponential gamma, which is also a scale mixture of Goshen.",
            "So the models I will I will discuss here are essentially scale."
        ],
        [
            "Mixture of Goshen, as well and so using this type of model you can find local minimum of equation too with the EM algorithm.",
            "So we discuss two distributions, so the normal gamma distribution and the normal inverse caution distribution show their aseptic properties.",
            "When the number of features go to Infinity an show some."
        ],
        [
            "Very cool results.",
            "So first, while this title is very bad, so sparse OK, but one of the model does not lead to strictly sparse."
        ],
        [
            "Estimates basian OK, but we we do not choose a fully Bayesian approach because we're looking for the maximum posteriori."
        ],
        [
            "Via the EM algorithm.",
            "Abbasian nonparametric OK.",
            "But in the experiments the number of parameters is."
        ],
        [
            "Tonight.",
            "Enter at least it is regulated.",
            "So I hope I will show you that this title is is not."
        ],
        [
            "So bad afterwards.",
            "So first so I. I introduced the two model."
        ],
        [
            "Us we use here so first the normal gamma model.",
            "So in that case we define a gamma gamma Pi over the variance parameter Sigma Square.",
            "So see my square distributed from a gamma distribution of parameter Alpha over capital K. And gamma square."
        ],
        [
            "Over 2 and the marginal distribution over a bit.",
            "OK, so after integrating out Sigma Square is P of beta.",
            "Queso is proportional to be tacky power Alpha over K -- 1 / 2 * A modified Bessel function of the second kind.",
            "So this this basal function can be very easily computed with Matlab.",
            "So I represented here.",
            "So the shape of the page of the PDF with respect to the parameter Alpha.",
            "So this PDF has an infinite spike at zero if Alpha over K is less than 0.5 anwen, so the parameter Alpha tunes.",
            "The the shape of the distribution when Alpha tends to zero, the distribution becomes more and more picky around 0.",
            "So this distribution as interesting features when Alpha equal to capital Key capital case.",
            "So we end up with the Laplace distribution and when Alpha goes to zero and gamma goes to 0, then we end up with the normal jeffrees distribu."
        ],
        [
            "OK, so here are some realization from this distribution.",
            "So for different value of the parameter Alpha.",
            "So for the top figure is Sigma squared.",
            "So here I've got 100 features.",
            "And the bottom figure is the parameter of beta.",
            "So when Alpha equal 1, so we only have a few of the features with which have known.",
            "Which weight with which are significantly more than zero and when Alpha increases, we end up with most of the weights which are significantly more than zero.",
            "So the parameter Alpha tunes the sparsity of the distribution."
        ],
        [
            "OK, so some of the fantastic properties of this distribution.",
            "So I said we are interested in the case where K is very large compared to the number of observation L. So when K tends to Infinity, the expectation of the sum of the absolute values of the weights does not depend on.",
            "So is bounded, so is equal to 2A over gamma and the expected value of the sum of the square value is equal to.",
            "Two times Alpha over Gamma Square.",
            "So even when kittens to Infinity the remains."
        ],
        [
            "Bonded.",
            "So another interesting feature to to understand.",
            "What's going on with the distribution so it slows from the gamma construction is still breaking construction."
        ],
        [
            "Gold awaits.",
            "So if we consider the the order statistics of the sequence of the variances, so Sigma bracket one Sigma worked."
        ],
        [
            "Etc.",
            "If we consider.",
            "The normalized version of this order statistic and the normal normalization, then Sigma over line and the normalization are independent an respectively so distributed according to the person directly distribution of Alpha and distributed according to a gamma distribution request under shared distribution basically is the distribution of the ordered weights in a dealership process."
        ],
        [
            "So.",
            "Having made these connections so the distribution of the the order statistic of the sequence can be recovered from this infinite stick breaking construction.",
            "So this is the basic construction for the duration process.",
            "So you sample according to a beta distribution.",
            "And then you multiply so beta minus the product of 1 minus the user user terms.",
            "So the order the order statistics of the sequence of the Pike as a same distribution as.",
            "This.",
            "Sigma Sigma overline."
        ],
        [
            "And so actually the coefficient beta key are when when capital kittens to Infinity there are the weights or the germs of the so called so variance gamma process which is a levy process used Latina in finance which is a Brownian motion evaluated at times given by a by."
        ],
        [
            "Process.",
            "OK, so now the user distribution so normally investigation distribution we now define an inverse.",
            "Prior"
        ],
        [
            "Overusing my square and the marginal PDF.",
            "So is Alpha squareball.",
            "Capital K ^2 + B to square 4 -- 1 / 2 * A basal function.",
            "So again, the shape of the distribution.",
            "Depending on on the value of.",
            "Of Alpha, so for large Alpha the distribution is quite flat.",
            "And when Alpha goes to 0 then the distribution becomes more and more picky around."
        ],
        [
            "0.",
            "So here's some realization.",
            "Depending on the value of the scale parameter Alpha.",
            "So when Alpha is small, so you have a few weights which are significant, while when Alpha increases, most of the weights become significant, so the parameter Alpha tunes the sparsity of the of the distrib."
        ],
        [
            "OK, so we have consider only.",
            "11 regression model.",
            "So with one one vector of dimension L, we can extend this to end vectors or YNN equal 1 to capital N."
        ],
        [
            "Of dimension N. An we want to estimate so vectors beta KN, 4K equal 1 two capital KN equals 12 N. An we want this random variables to be dependent and exchangeable for any give."
        ],
        [
            "Pinky.",
            "So we can.",
            "We can do that by defining.",
            "The following hierarchical models where Sigma Square K is distributed from a gamma and inverse question distribution for each of the feature an for each element beta KN it is distributed from a normal with variance Sigma squared K. So you know some realization from from the model.",
            "So the top figure again is Sigma Square.",
            "So for Alpha equal 1 you have only.",
            "So some of the features which are significant while when Alpha increases most of the feature become become significant."
        ],
        [
            "So how can you interpret that?",
            "So ASCII tends to Infinity.",
            "We can consider this as a prior distribution.",
            "Thank you over infinite matrices with real valued entry."
        ],
        [
            "Anne.",
            "Considering that we can consider this model as complementary complementary to other two by Asian nonparametrics model, which are the Indian buffet process and infinite gamma person process which define prior distribution of over infinite matrices?",
            "In that case with integer values.",
            "Entries"
        ],
        [
            "OK, so I I discuss the.",
            "The models do now we talk about the sparsity properties.",
            "In case of the map list."
        ],
        [
            "Nation.",
            "So as I said, so if we've got capital an object, we want to minimize this objective function.",
            "When?",
            "So where the the penalization is given by this table.",
            "So for the different distribution from the Lasso.",
            "So if we got only one object from for the normal jeffrees normal gamma and normal inverse Goshen."
        ],
        [
            "And so here are the the control of constant value for finalisation of.",
            "Of beta one, personalization of beta two so forth 2 for Capital K called 2 so here is so generalization for the Laplace distribution.",
            "Realization for the normal rephrase your normal gamma normal investigation, so depending on the value of Alpha over K so it's Alpha over K equal 2.",
            "So in the in that case when Alpha over case more strictly more than one it does not lead to sparse solution.",
            "When Alpha over K is equal to 1, we end up with the Laplace distribution and when Alpha work goes to zero in that case.",
            "So we go to sparser and sparser distribution.",
            "So here zero point.",
            "75 and ears 0.01.",
            "So the same for the normal involution distribution.",
            "So here is Alpha work, equal 2A work equals 0.25 an hour for work equals 0.01.",
            "So in the case of the normal easiest Goshen, we don't derivative is continuous in zero and we do not obtain strictly sparse estimate.",
            "But when Alpha becomes very small, so most most of the elements are very very small and close to zero and so we can threshold.",
            "This value.",
            "To obtain a spouse's past."
        ],
        [
            "OK, so your summary.",
            "So the normal gamma prior either thresholding rule for Alpha over capital K is lower or equal to 1.",
            "And it gives sparse estimates, and normal investigation is not strict.",
            "Threshold orbits Canyon Falls almost past estimates."
        ],
        [
            "Can we now show some empirical results?"
        ],
        [
            "With this distribution.",
            "So we simulated 100 datasets with observation of dimension 50, an Sigma equal 1.",
            "So with only a capital N equal 1, so the classical setting the correlation of the design matrix is set to 0.5.",
            "Anne.",
            "So we set a true beta vector, which is these vectors with only three nonzero values and the remaining of the vector is filled with zeros.",
            "So where capital K is equal to 2061 hundred and 200.",
            "And the parameter, so the single parameter of the lasso and the two parameters of the normal gamma normal inverse question distribution are estimated with five fold cross validation."
        ],
        [
            "OK, so here here are the results.",
            "So the box plots of the mean square error with the simulated data.",
            "So we when Capital K is equal to 1, most of the.",
            "So we compare so less so relevant vector machine normal jeffrees normal gamma in normal inverse Gaussian.",
            "So when K is quite small, so most of the.",
            "Most of the.",
            "The methods keep give the summarizers, but when K increases when the vector become sparser and sparser, so these methods so normal, jeffrees normal gamma.",
            "Normally this cushion.",
            "They showed you to give much better results than the other one, so with normally there's questions like will perform normal gamma and outperforms normal normal jeffrees distribution."
        ],
        [
            "OK, so as a conclusion, so why this title is not so bad after all?"
        ],
        [
            "So sparse OK, so we present two classes of models, which lead to sparser estimate done, then lassoo an even if normal investigation is not strictly does not need to strictly sparse estimate we can discard.",
            "Most of the very small elements."
        ],
        [
            "Confidently so beige nonparametric.",
            "So the model is related to a class of.",
            "Based on price."
        ],
        [
            "Trick model when?"
        ],
        [
            "Number of features go to Infinity and for regression, so the extension to probit regression is straightforward."
        ],
        [
            "OK so I'm now working with Kevin Murphy and an extension of this work based on the scale mixture of Goshen for graph learning with groups."
        ],
        [
            "City.",
            "Ongoing work also we.",
            "I give some connection with stick breaking construction.",
            "One of the question is can we make use of these stick breaking construction to design?",
            "So algorithm for full fully Bayesian inference or to design more elaborated model."
        ],
        [
            "Anne, can we give?",
            "A nice definition of the marginal distribution of the.",
            "The parameter of beta K so to obtain a similar.",
            "Similar generative model as with the Indian buffet process or the Chinese."
        ],
        [
            "Restaurant process.",
            "OK, here are some references, thank you.",
            "Could you say?",
            "Excuse me why.",
            "Yeah.",
            "Different models in your experiment.",
            "Model.",
            "OK, so.",
            "So I."
        ],
        [
            "I have to say I don't have a clear intuition why.",
            "Why normal inverse Goshen seems to outperform the normal gamma distribution for the compared to the last two?",
            "There is a clear interpretation becausw normal gamma when Alpha case is smaller than one leads to sparser sparser estimates.",
            "So when the real vector is very sparse, so it makes more sense.",
            "To use normal gamma then then less so.",
            "But I do not have a clear clear understanding why nobody there solution gives better results than the normal gamma.",
            "Low variability.",
            "Distance to Tucson.",
            "Yep.",
            "Yeah, yeah, there are some outliers but.",
            "Specially, yeah so so this method seem to give better results.",
            "Normal Jeffrey grid.",
            "Quite good results in all of the experiments.",
            "And as you don't you do not have to estimate any of the parameters, so there's a good trader.",
            "And also yeah, what I said.",
            "Of course the objective function to optimize is nonconvex for.",
            "For the normal gamma normal interruption in normal compared to lessen.",
            "So how does that affect the computational?",
            "Let's do there.",
            "So, so it's basically so it's a EM algorithm, so it's it's quite fascinating and I used 100 iteration of the VM, so.",
            "Fix for this payment.",
            "So in this experiment I estimate Alpha by cross validation, but I made a lot of.",
            "Experiment and usually what you can do is set Alpha to quite small value and only estimate the user parameter gamma.",
            "Going back to the.",
            "Yeah, sure in the paper.",
            "So I I give the Gibson plagiarism, but when you do Gibbs sampler you do not obtain strictly sparse estimate.",
            "If you don't.",
            "If you do not choose a spike in step prior.",
            "Yeah, but yeah you could.",
            "You could do that.",
            "Yeah yeah, sure sure.",
            "So what's what's the consequences of the problem not being called this meeting that initializes station and different way that you end up in a different local yeah.",
            "Interpret them results knowing that you end up each time a different local minimum.",
            "Yeah, so so here for the.",
            "For the simulation I do not choose a multipolar restart for the EM algorithm, so it's always with the same, with the same starting point obtained by the by the remedy.",
            "An even, even with that, even we.",
            "We are not insured to to find the global maximum.",
            "It gives better result than if you use a convex convex optimization.",
            "Change.",
            "So let's say you average out over the Internet.",
            "So best case would be.",
            "You mean with different starting starting here?",
            "Start.",
            "Yeah, so I so I start with the maximum likelihood estimate here.",
            "Change so yeah it will give for sure different presence.",
            "But"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm from Socal, so I'm from the Department of Computer Science and Statistics at the University of British Columbia, and so this is joint work with Arnold USA.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm interested in the classical linear regression models where Y is equal to X beta plus some Gaussian noise EPS.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Done.",
                    "label": 0
                },
                {
                    "sent": "Where Y is an observation of dimension.",
                    "label": 0
                },
                {
                    "sent": "LX is some known design matrix of size L * K and beta is an unknown vector of.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dimension key.",
                    "label": 0
                },
                {
                    "sent": "So we are interested in finding sparse estimate of the vector beta, usually in the setting where the dimension of this vector K is much larger than the dimension of the observation.",
                    "label": 1
                },
                {
                    "sent": "So K is much.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other than L. So this can be for the sake of variable selection for the decomposition of a signal over an overcomplete basis and as in.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "Ann to address these problems.",
                    "label": 0
                },
                {
                    "sent": "So numerous models and algorithm having pro.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause in the red?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try to like the beige and spike in slab the last.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Or the relevance vector machine, so we will use here Bayesian approach with a maximum of posterior ES.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Made to address this problem.",
                    "label": 0
                },
                {
                    "sent": "So we consider general that we define prior distribution over the unknown vector beta.",
                    "label": 1
                },
                {
                    "sent": "So where we assume independence over all the feature case.",
                    "label": 0
                },
                {
                    "sent": "So P of beta is the product for K = 1, two Capital K of P of beta.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cake.",
                    "label": 0
                },
                {
                    "sent": "An we are interested in finding so local local minimum of the of the objective function.",
                    "label": 1
                },
                {
                    "sent": "Well, minus log of P of beta can be interpreted as.",
                    "label": 0
                },
                {
                    "sent": "Panelization",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we focus here on scale mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So where P of beta K can take this?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This form.",
                    "label": 0
                },
                {
                    "sent": "So a lot of people have worked on on this on this kind of model, so if P of beta key is is a LAPLACE prior, then equation 2 reduces to the lasso objective function, which is convex.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Figure it was proposed to use a normal, normal Jeffreys distribution.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ann Griffin, as discussed, a lot of distribution and especially the normal exponential gamma, which is also a scale mixture of Goshen.",
                    "label": 0
                },
                {
                    "sent": "So the models I will I will discuss here are essentially scale.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mixture of Goshen, as well and so using this type of model you can find local minimum of equation too with the EM algorithm.",
                    "label": 1
                },
                {
                    "sent": "So we discuss two distributions, so the normal gamma distribution and the normal inverse caution distribution show their aseptic properties.",
                    "label": 0
                },
                {
                    "sent": "When the number of features go to Infinity an show some.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very cool results.",
                    "label": 0
                },
                {
                    "sent": "So first, while this title is very bad, so sparse OK, but one of the model does not lead to strictly sparse.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimates basian OK, but we we do not choose a fully Bayesian approach because we're looking for the maximum posteriori.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Via the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "Abbasian nonparametric OK.",
                    "label": 0
                },
                {
                    "sent": "But in the experiments the number of parameters is.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tonight.",
                    "label": 0
                },
                {
                    "sent": "Enter at least it is regulated.",
                    "label": 1
                },
                {
                    "sent": "So I hope I will show you that this title is is not.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So bad afterwards.",
                    "label": 0
                },
                {
                    "sent": "So first so I. I introduced the two model.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Us we use here so first the normal gamma model.",
                    "label": 0
                },
                {
                    "sent": "So in that case we define a gamma gamma Pi over the variance parameter Sigma Square.",
                    "label": 0
                },
                {
                    "sent": "So see my square distributed from a gamma distribution of parameter Alpha over capital K. And gamma square.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over 2 and the marginal distribution over a bit.",
                    "label": 0
                },
                {
                    "sent": "OK, so after integrating out Sigma Square is P of beta.",
                    "label": 0
                },
                {
                    "sent": "Queso is proportional to be tacky power Alpha over K -- 1 / 2 * A modified Bessel function of the second kind.",
                    "label": 1
                },
                {
                    "sent": "So this this basal function can be very easily computed with Matlab.",
                    "label": 0
                },
                {
                    "sent": "So I represented here.",
                    "label": 0
                },
                {
                    "sent": "So the shape of the page of the PDF with respect to the parameter Alpha.",
                    "label": 0
                },
                {
                    "sent": "So this PDF has an infinite spike at zero if Alpha over K is less than 0.5 anwen, so the parameter Alpha tunes.",
                    "label": 0
                },
                {
                    "sent": "The the shape of the distribution when Alpha tends to zero, the distribution becomes more and more picky around 0.",
                    "label": 0
                },
                {
                    "sent": "So this distribution as interesting features when Alpha equal to capital Key capital case.",
                    "label": 0
                },
                {
                    "sent": "So we end up with the Laplace distribution and when Alpha goes to zero and gamma goes to 0, then we end up with the normal jeffrees distribu.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here are some realization from this distribution.",
                    "label": 0
                },
                {
                    "sent": "So for different value of the parameter Alpha.",
                    "label": 0
                },
                {
                    "sent": "So for the top figure is Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "So here I've got 100 features.",
                    "label": 0
                },
                {
                    "sent": "And the bottom figure is the parameter of beta.",
                    "label": 0
                },
                {
                    "sent": "So when Alpha equal 1, so we only have a few of the features with which have known.",
                    "label": 0
                },
                {
                    "sent": "Which weight with which are significantly more than zero and when Alpha increases, we end up with most of the weights which are significantly more than zero.",
                    "label": 0
                },
                {
                    "sent": "So the parameter Alpha tunes the sparsity of the distribution.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so some of the fantastic properties of this distribution.",
                    "label": 0
                },
                {
                    "sent": "So I said we are interested in the case where K is very large compared to the number of observation L. So when K tends to Infinity, the expectation of the sum of the absolute values of the weights does not depend on.",
                    "label": 0
                },
                {
                    "sent": "So is bounded, so is equal to 2A over gamma and the expected value of the sum of the square value is equal to.",
                    "label": 1
                },
                {
                    "sent": "Two times Alpha over Gamma Square.",
                    "label": 0
                },
                {
                    "sent": "So even when kittens to Infinity the remains.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bonded.",
                    "label": 0
                },
                {
                    "sent": "So another interesting feature to to understand.",
                    "label": 0
                },
                {
                    "sent": "What's going on with the distribution so it slows from the gamma construction is still breaking construction.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gold awaits.",
                    "label": 0
                },
                {
                    "sent": "So if we consider the the order statistics of the sequence of the variances, so Sigma bracket one Sigma worked.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Etc.",
                    "label": 0
                },
                {
                    "sent": "If we consider.",
                    "label": 0
                },
                {
                    "sent": "The normalized version of this order statistic and the normal normalization, then Sigma over line and the normalization are independent an respectively so distributed according to the person directly distribution of Alpha and distributed according to a gamma distribution request under shared distribution basically is the distribution of the ordered weights in a dealership process.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Having made these connections so the distribution of the the order statistic of the sequence can be recovered from this infinite stick breaking construction.",
                    "label": 1
                },
                {
                    "sent": "So this is the basic construction for the duration process.",
                    "label": 0
                },
                {
                    "sent": "So you sample according to a beta distribution.",
                    "label": 1
                },
                {
                    "sent": "And then you multiply so beta minus the product of 1 minus the user user terms.",
                    "label": 0
                },
                {
                    "sent": "So the order the order statistics of the sequence of the Pike as a same distribution as.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Sigma Sigma overline.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so actually the coefficient beta key are when when capital kittens to Infinity there are the weights or the germs of the so called so variance gamma process which is a levy process used Latina in finance which is a Brownian motion evaluated at times given by a by.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Process.",
                    "label": 0
                },
                {
                    "sent": "OK, so now the user distribution so normally investigation distribution we now define an inverse.",
                    "label": 0
                },
                {
                    "sent": "Prior",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overusing my square and the marginal PDF.",
                    "label": 0
                },
                {
                    "sent": "So is Alpha squareball.",
                    "label": 0
                },
                {
                    "sent": "Capital K ^2 + B to square 4 -- 1 / 2 * A basal function.",
                    "label": 0
                },
                {
                    "sent": "So again, the shape of the distribution.",
                    "label": 0
                },
                {
                    "sent": "Depending on on the value of.",
                    "label": 0
                },
                {
                    "sent": "Of Alpha, so for large Alpha the distribution is quite flat.",
                    "label": 0
                },
                {
                    "sent": "And when Alpha goes to 0 then the distribution becomes more and more picky around.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "0.",
                    "label": 0
                },
                {
                    "sent": "So here's some realization.",
                    "label": 0
                },
                {
                    "sent": "Depending on the value of the scale parameter Alpha.",
                    "label": 0
                },
                {
                    "sent": "So when Alpha is small, so you have a few weights which are significant, while when Alpha increases, most of the weights become significant, so the parameter Alpha tunes the sparsity of the of the distrib.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we have consider only.",
                    "label": 0
                },
                {
                    "sent": "11 regression model.",
                    "label": 0
                },
                {
                    "sent": "So with one one vector of dimension L, we can extend this to end vectors or YNN equal 1 to capital N.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of dimension N. An we want to estimate so vectors beta KN, 4K equal 1 two capital KN equals 12 N. An we want this random variables to be dependent and exchangeable for any give.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pinky.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "We can do that by defining.",
                    "label": 0
                },
                {
                    "sent": "The following hierarchical models where Sigma Square K is distributed from a gamma and inverse question distribution for each of the feature an for each element beta KN it is distributed from a normal with variance Sigma squared K. So you know some realization from from the model.",
                    "label": 0
                },
                {
                    "sent": "So the top figure again is Sigma Square.",
                    "label": 0
                },
                {
                    "sent": "So for Alpha equal 1 you have only.",
                    "label": 0
                },
                {
                    "sent": "So some of the features which are significant while when Alpha increases most of the feature become become significant.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how can you interpret that?",
                    "label": 0
                },
                {
                    "sent": "So ASCII tends to Infinity.",
                    "label": 0
                },
                {
                    "sent": "We can consider this as a prior distribution.",
                    "label": 0
                },
                {
                    "sent": "Thank you over infinite matrices with real valued entry.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Considering that we can consider this model as complementary complementary to other two by Asian nonparametrics model, which are the Indian buffet process and infinite gamma person process which define prior distribution of over infinite matrices?",
                    "label": 1
                },
                {
                    "sent": "In that case with integer values.",
                    "label": 0
                },
                {
                    "sent": "Entries",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I I discuss the.",
                    "label": 0
                },
                {
                    "sent": "The models do now we talk about the sparsity properties.",
                    "label": 0
                },
                {
                    "sent": "In case of the map list.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "So as I said, so if we've got capital an object, we want to minimize this objective function.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                },
                {
                    "sent": "So where the the penalization is given by this table.",
                    "label": 0
                },
                {
                    "sent": "So for the different distribution from the Lasso.",
                    "label": 0
                },
                {
                    "sent": "So if we got only one object from for the normal jeffrees normal gamma and normal inverse Goshen.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so here are the the control of constant value for finalisation of.",
                    "label": 1
                },
                {
                    "sent": "Of beta one, personalization of beta two so forth 2 for Capital K called 2 so here is so generalization for the Laplace distribution.",
                    "label": 0
                },
                {
                    "sent": "Realization for the normal rephrase your normal gamma normal investigation, so depending on the value of Alpha over K so it's Alpha over K equal 2.",
                    "label": 0
                },
                {
                    "sent": "So in the in that case when Alpha over case more strictly more than one it does not lead to sparse solution.",
                    "label": 0
                },
                {
                    "sent": "When Alpha over K is equal to 1, we end up with the Laplace distribution and when Alpha work goes to zero in that case.",
                    "label": 0
                },
                {
                    "sent": "So we go to sparser and sparser distribution.",
                    "label": 0
                },
                {
                    "sent": "So here zero point.",
                    "label": 0
                },
                {
                    "sent": "75 and ears 0.01.",
                    "label": 0
                },
                {
                    "sent": "So the same for the normal involution distribution.",
                    "label": 0
                },
                {
                    "sent": "So here is Alpha work, equal 2A work equals 0.25 an hour for work equals 0.01.",
                    "label": 0
                },
                {
                    "sent": "So in the case of the normal easiest Goshen, we don't derivative is continuous in zero and we do not obtain strictly sparse estimate.",
                    "label": 0
                },
                {
                    "sent": "But when Alpha becomes very small, so most most of the elements are very very small and close to zero and so we can threshold.",
                    "label": 0
                },
                {
                    "sent": "This value.",
                    "label": 0
                },
                {
                    "sent": "To obtain a spouse's past.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so your summary.",
                    "label": 0
                },
                {
                    "sent": "So the normal gamma prior either thresholding rule for Alpha over capital K is lower or equal to 1.",
                    "label": 1
                },
                {
                    "sent": "And it gives sparse estimates, and normal investigation is not strict.",
                    "label": 1
                },
                {
                    "sent": "Threshold orbits Canyon Falls almost past estimates.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can we now show some empirical results?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this distribution.",
                    "label": 0
                },
                {
                    "sent": "So we simulated 100 datasets with observation of dimension 50, an Sigma equal 1.",
                    "label": 1
                },
                {
                    "sent": "So with only a capital N equal 1, so the classical setting the correlation of the design matrix is set to 0.5.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So we set a true beta vector, which is these vectors with only three nonzero values and the remaining of the vector is filled with zeros.",
                    "label": 0
                },
                {
                    "sent": "So where capital K is equal to 2061 hundred and 200.",
                    "label": 0
                },
                {
                    "sent": "And the parameter, so the single parameter of the lasso and the two parameters of the normal gamma normal inverse question distribution are estimated with five fold cross validation.",
                    "label": 1
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here here are the results.",
                    "label": 0
                },
                {
                    "sent": "So the box plots of the mean square error with the simulated data.",
                    "label": 1
                },
                {
                    "sent": "So we when Capital K is equal to 1, most of the.",
                    "label": 0
                },
                {
                    "sent": "So we compare so less so relevant vector machine normal jeffrees normal gamma in normal inverse Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So when K is quite small, so most of the.",
                    "label": 0
                },
                {
                    "sent": "Most of the.",
                    "label": 0
                },
                {
                    "sent": "The methods keep give the summarizers, but when K increases when the vector become sparser and sparser, so these methods so normal, jeffrees normal gamma.",
                    "label": 0
                },
                {
                    "sent": "Normally this cushion.",
                    "label": 0
                },
                {
                    "sent": "They showed you to give much better results than the other one, so with normally there's questions like will perform normal gamma and outperforms normal normal jeffrees distribution.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so as a conclusion, so why this title is not so bad after all?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So sparse OK, so we present two classes of models, which lead to sparser estimate done, then lassoo an even if normal investigation is not strictly does not need to strictly sparse estimate we can discard.",
                    "label": 0
                },
                {
                    "sent": "Most of the very small elements.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Confidently so beige nonparametric.",
                    "label": 0
                },
                {
                    "sent": "So the model is related to a class of.",
                    "label": 1
                },
                {
                    "sent": "Based on price.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trick model when?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Number of features go to Infinity and for regression, so the extension to probit regression is straightforward.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I'm now working with Kevin Murphy and an extension of this work based on the scale mixture of Goshen for graph learning with groups.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "City.",
                    "label": 0
                },
                {
                    "sent": "Ongoing work also we.",
                    "label": 0
                },
                {
                    "sent": "I give some connection with stick breaking construction.",
                    "label": 0
                },
                {
                    "sent": "One of the question is can we make use of these stick breaking construction to design?",
                    "label": 0
                },
                {
                    "sent": "So algorithm for full fully Bayesian inference or to design more elaborated model.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne, can we give?",
                    "label": 0
                },
                {
                    "sent": "A nice definition of the marginal distribution of the.",
                    "label": 1
                },
                {
                    "sent": "The parameter of beta K so to obtain a similar.",
                    "label": 0
                },
                {
                    "sent": "Similar generative model as with the Indian buffet process or the Chinese.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Restaurant process.",
                    "label": 0
                },
                {
                    "sent": "OK, here are some references, thank you.",
                    "label": 0
                },
                {
                    "sent": "Could you say?",
                    "label": 0
                },
                {
                    "sent": "Excuse me why.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Different models in your experiment.",
                    "label": 0
                },
                {
                    "sent": "Model.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have to say I don't have a clear intuition why.",
                    "label": 0
                },
                {
                    "sent": "Why normal inverse Goshen seems to outperform the normal gamma distribution for the compared to the last two?",
                    "label": 1
                },
                {
                    "sent": "There is a clear interpretation becausw normal gamma when Alpha case is smaller than one leads to sparser sparser estimates.",
                    "label": 0
                },
                {
                    "sent": "So when the real vector is very sparse, so it makes more sense.",
                    "label": 0
                },
                {
                    "sent": "To use normal gamma then then less so.",
                    "label": 0
                },
                {
                    "sent": "But I do not have a clear clear understanding why nobody there solution gives better results than the normal gamma.",
                    "label": 0
                },
                {
                    "sent": "Low variability.",
                    "label": 0
                },
                {
                    "sent": "Distance to Tucson.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, there are some outliers but.",
                    "label": 0
                },
                {
                    "sent": "Specially, yeah so so this method seem to give better results.",
                    "label": 0
                },
                {
                    "sent": "Normal Jeffrey grid.",
                    "label": 0
                },
                {
                    "sent": "Quite good results in all of the experiments.",
                    "label": 1
                },
                {
                    "sent": "And as you don't you do not have to estimate any of the parameters, so there's a good trader.",
                    "label": 0
                },
                {
                    "sent": "And also yeah, what I said.",
                    "label": 0
                },
                {
                    "sent": "Of course the objective function to optimize is nonconvex for.",
                    "label": 0
                },
                {
                    "sent": "For the normal gamma normal interruption in normal compared to lessen.",
                    "label": 0
                },
                {
                    "sent": "So how does that affect the computational?",
                    "label": 0
                },
                {
                    "sent": "Let's do there.",
                    "label": 0
                },
                {
                    "sent": "So, so it's basically so it's a EM algorithm, so it's it's quite fascinating and I used 100 iteration of the VM, so.",
                    "label": 0
                },
                {
                    "sent": "Fix for this payment.",
                    "label": 0
                },
                {
                    "sent": "So in this experiment I estimate Alpha by cross validation, but I made a lot of.",
                    "label": 0
                },
                {
                    "sent": "Experiment and usually what you can do is set Alpha to quite small value and only estimate the user parameter gamma.",
                    "label": 0
                },
                {
                    "sent": "Going back to the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure in the paper.",
                    "label": 0
                },
                {
                    "sent": "So I I give the Gibson plagiarism, but when you do Gibbs sampler you do not obtain strictly sparse estimate.",
                    "label": 0
                },
                {
                    "sent": "If you don't.",
                    "label": 0
                },
                {
                    "sent": "If you do not choose a spike in step prior.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but yeah you could.",
                    "label": 0
                },
                {
                    "sent": "You could do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, sure sure.",
                    "label": 0
                },
                {
                    "sent": "So what's what's the consequences of the problem not being called this meeting that initializes station and different way that you end up in a different local yeah.",
                    "label": 0
                },
                {
                    "sent": "Interpret them results knowing that you end up each time a different local minimum.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so here for the.",
                    "label": 0
                },
                {
                    "sent": "For the simulation I do not choose a multipolar restart for the EM algorithm, so it's always with the same, with the same starting point obtained by the by the remedy.",
                    "label": 0
                },
                {
                    "sent": "An even, even with that, even we.",
                    "label": 0
                },
                {
                    "sent": "We are not insured to to find the global maximum.",
                    "label": 0
                },
                {
                    "sent": "It gives better result than if you use a convex convex optimization.",
                    "label": 0
                },
                {
                    "sent": "Change.",
                    "label": 0
                },
                {
                    "sent": "So let's say you average out over the Internet.",
                    "label": 0
                },
                {
                    "sent": "So best case would be.",
                    "label": 0
                },
                {
                    "sent": "You mean with different starting starting here?",
                    "label": 0
                },
                {
                    "sent": "Start.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I so I start with the maximum likelihood estimate here.",
                    "label": 0
                },
                {
                    "sent": "Change so yeah it will give for sure different presence.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        }
    }
}