{
    "id": "2dwqhhphh6yd4o7gqzrp5xacceiznsjp",
    "title": "Semantic Wide and Deep Learning for Detecting Crisis-Information Categories on Social Media",
    "info": {
        "author": [
            "Gr\u00e9goire Burel, Open University (OU)"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_burel_social_media/",
    "segmentation": [
        [
            "And I'm going to talk about how we can use wider nip learning for detecting crisis information categories on social media.",
            "So this work was done with a sense- Harris Colony and this part of political comrades.",
            "OK, so first let's talk a little about."
        ],
        [
            "Detections, so I've been detection is the task of automatic identifying clues and text that they note specific events type of themes.",
            "So in in the context of crisis, that means that, for instance, it can be used for helping organizations to identify what are key events and also organized relevant informations and decide what to do.",
            "What is important.",
            "If we look at social media, particularly in a Twitters, can see that's a lot of data generated everyday, so around 200 million.",
            "Active users 400 million tweets a day and if we look at previous usage we can see that, for instance, during the 2011 earthquake, 170 seven millions tweets were posted in only one day."
        ],
        [
            "So typically we can divide even detection in crisis context in three different steps.",
            "So first you want do you want to invite if a document is related unrelated to an event, then you want to know what it is about.",
            "So is it about fire?",
            "Is it about an earthquake or anything floodings for instance?",
            "And then finally you want to determine what is actually talked about within those documents.",
            "So you want to know for instance.",
            "If he's about casualties, if it's about building collapsed, it's about people offering help, and that can be used again for organizing response.",
            "So in this."
        ],
        [
            "I'm going just to talk about the last parts.",
            "The reason is from previous work.",
            "We've seen that the first task correctly easy to perform using, for instance, standard classifications methods.",
            "We can get like crazy up more than 80%, and what the last task is much harder.",
            "And we've seen like results up to maybe 60% accuracy."
        ],
        [
            "So here I'm going to use deep learning, so if we consider like traditional approach and deep learning approaches.",
            "So the main difference somehow is that the use of word embeddings compared to bag of words.",
            "Even if you can use one beddings in other situations.",
            "So in bag of words you will get one documents and you will represent it with one vector.",
            "Generally that will represent the size of the vocabulary and you will say this words are there or disorder not there.",
            "And then in embeddings, what you will do, you will represent words as vectors and so documents will not be one vector but a mattress.",
            "And idea of the embeddings is that words that are similarities.",
            "They will tend to be represented with the similar vectors.",
            "So for instance here if you have like Boston in Paris, they will have similar vectors because they share the concept of locations."
        ],
        [
            "So the prime when you deal with tweets are very short documents.",
            "Is that you lack context so they are very short, so you have just very small text to make decision for classification.",
            "So an idea is to, for instance, we can use semantics so we can do until the extraction tools and identify what are within the documents.",
            "For instance here you can do that.",
            "Obama was politicians or at that time and for instance the Boston Marathon is an infant.",
            "If you want to integrate that in in different classifying techniques.",
            "So if you use instructional approaches, you will probably want to extend the vector representations of the document by adding the semantics.",
            "If you are now in a deep learning methods, you can maybe use an beddings for presenting as well the semantics."
        ],
        [
            "So there's different ways to do classifications, and using deep learning's and here we are focusing on one approach which is use of convolutional neural networks.",
            "So this approach was used for sentence classification in percent 14 and the idea is like you get the metrics of embeddings.",
            "That is like the representation of the text and then you run multiple convolutions and then you can do classification.",
            "So if."
        ],
        [
            "If you look at how we can use that so the main issue, as our things like when you just use the text, you don't have the semantics that can actually add extra informations.",
            "So wait to do that that we did in previous work is to for instance use embeddings for representing the content.",
            "The semantic content of the documents.",
            "But the main issue when you do that is that you can start.",
            "You can see that this because of the small text and data that we have.",
            "You get very little semantic vocabulary compared to the word vocabulary, so that means like when you learn the embeddings on the data, you have very little to do with, and it might not be the best to use the embeddings because they might actually be too generalizing on something that is actually not very big already."
        ],
        [
            "So not project exists, not present an interest in 16 is to use a wide and deep learning so wide and deep learning is used.",
            "The idea is like you get 2 representations of the same document, for instance, and you use one is like a more traditional regression approach.",
            "Other side you selected onboarding approach, so you put them together so you have like a very specific models that is kind to be learned with the regression part.",
            "There integration parts which is the white part and you get like a more deep resolution that generalize alot the problem.",
            "Using the deep approach."
        ],
        [
            "So again, if we look at this, we can see that the wide and deep learning could be actually used for problems.",
            "So the idea here is that.",
            "We can use, you can add.",
            "The white part so and to use for the concepts, representations.",
            "And we can use the D pad for the word representation, so it's kind of balancing so you get what representation for deep semantics and you get.",
            "Deep representation for Shadow Wars representations."
        ],
        [
            "So how does that work?",
            "So, so you get the documents you can from the words, you can generate the embeddings, then from the meetings we get a mattress representing the document and then you run the conversation on that side and on the other side you extract concept and entities from the documents and you run it through white requestion.",
            "And model and then you give it together and then you can classify."
        ],
        [
            "So going back toward task.",
            "So in our task, what do we have is?",
            "Is we want to classify documents later tweets to detect crisis information categories so or data set is crisis legs.",
            "So it's 28,000 tweets from Crisis Beach we collected between 2012 months and 15 contains two different crisis types and within those crisis types you have annotations, forces, information, 6 information categories.",
            "So you know for instance categories like affect, individuals, infrastructure and utilities, donation and volunteering.",
            "Question and advice and bathroom supports.",
            "So this this again can be used to help responders to decide what is important information for them.",
            "For instance, they could know where the infected individuals, if they if they have a classifier for helping them, they will know if there is infrastructure collapse and also where people.",
            "If there's people offering help.",
            "So we used X Rays over extraction, so there is many tools that can be used with this one because we provide quite a good coverage.",
            "So so out of those 28,000 tweets, 65% get annotations.",
            "I'm so then there is different ways again that we can integrate the concepts so we do two different approaches.",
            "So the first one is the most straightforward approach is that you gates labels from front X Rays or the transfer disease entities.",
            "Obama.",
            "This is a politician.",
            "We can use those labels like politician and Obama as our presentation in in our vector representation of concepts the other way, which is a bit deeper.",
            "Is to use the abstract.",
            "So for instance, when you get an entity or concept you go to DB pedia.",
            "You get like the abstract, which is kind of the definition.",
            "You can use the first sentence.",
            "So the idea is like in this sentence that is kind of the definitions of that concept of entities.",
            "You get implicit semantics.",
            "So if you look at, for instance Obama definition, you can see that it mentions that is a politician is American.",
            "Is American right?",
            "So if we use the graph, actually we get that from the graph of the semantic graph.",
            "But from the text we cannot get it already.",
            "So it's kind of.",
            "Implicitly expanding the context or presentation of the document.",
            "OK, so."
        ],
        [
            "So if we look again at the data so we can see that the data is quite unbalanced, so you will get maybe more document, more tweets about floating's than than tweets about bombing.",
            "So what we do is we balance the data set.",
            "So we have like representation of documents that are uniformly distributed around this categories and then something else.",
            "So we want to know actually hold the semantics can help.",
            "So what what do we do is that we sampled?",
            "The data is well too.",
            "Just includes documents that have more than two entity or concepts so that reduced a lot of course datasets.",
            "So then we have two baselines, so which seen any previous work that SVM was performing quite well as baseline.",
            "So we use two approaches.",
            "One is with TF IDF to normalize vector representations.",
            "And then we use also what embeddings.",
            "So we've pretty trained embeddings from Google.",
            "So we do traditional activations and report precision recording one.",
            "I."
        ],
        [
            "So if we look at the results, what web services like in general like when we do our approach, we get much better while we get better results than the baseline.",
            "Not much better results than our previous work that was child CNN like around 4% which is still an improvement."
        ],
        [
            "And so again, if we look at the results.",
            "So the semantics in an approach that we presented outperform the baseline, and that's significant.",
            "Also, we can see that the more you semantics, the better the results are.",
            "As you can see, like if you look at the data again.",
            "You can see that if used abstract, you get slightly better results and then if you don't use them.",
            "But of course that's also something to take into account.",
            "Is that the abstract outperform in general, but if we look at the significance not all the time significance particular datasets, so which makes some sense.",
            "But if you look actually at the data that is sample on the semantics, then we get better results.",
            "And it's significant.",
            "So what can we do in future work so there's many things we can do.",
            "Basically, here we use CNN because they are quite practical and quite easy to train.",
            "They don't require too much resources, but we can use more like other models like we can neural networks or attention networks for instance.",
            "So the idea here is like we get a better presentation of the text and dependences between documents.",
            "And then also we can use different embeddings.",
            "As for bootstrapping the model.",
            "So here we use the Google embeddings already trained, but there is also like for instance already embeddings trained on Twitter or data which might be helping.",
            "And finally, we we kind of tested two approaches for interpreting the semantics.",
            "Here the thing we could do is we could extend those integration in different ways, so we could use extended concept graph for instance like dependencies between the entity and concepts on there.",
            "Or we can also like use maybe a graph embeddings as well, which would be an interesting airport.",
            "And finally, also what would be interesting is that this data set is relatively small and also that the documents are relatively small.",
            "So it would be nice to see if this approach actually works better on bigger data and because it has it."
        ],
        [
            "Thank you very much.",
            "You have used Ness.",
            "We haven't a as a classifier.",
            "Affianced entered correctly.",
            "Have you played around with other classifiers?",
            "So we so we found that boosting classifiers works better for this kind of setting.",
            "We increase work.",
            "We did similar tasks and we tested more than just his VM for instance.",
            "So we saw that this game was the best as the baseline in terms of deep learning approach.",
            "As I was saying.",
            "The primary issue is that they were quite a lot of competitions, so CNN is relatively easy to train, so obviously that's the main reason, so we didn't look at other approach which might be better from literature, as we can see that they might perform better.",
            "Yeah, so for the baseline, I agree.",
            "SVM is one of the best performing classifiers, but if you incorporate some different features so it changes a bit if you have this different features, how you can exploit them and then SVM becomes not the best one, and that's the point.",
            "Do you mean like doing like more feature engineering's if you understand, or no no?",
            "It's just so we have you changed the way of space?",
            "So if you go trust with the usual stuff of features with this long vector space, then SVM performs quite well.",
            "But if you incorporate with embeddings the features then it changes and then the question is if other learners before performs better.",
            "And this is what we found out, and that's the question.",
            "If you have played with that.",
            "God embeddings we just done bad things as well in in the SVM classifiers.",
            "So we tried as well to see if they will help traditional classifiers and it didn't really work.",
            "I mean we did a simple which is kind of which has been used in the past, which is actually well approach which is you kind of container down beddings using Princeton average representation with visually simplify lots then beddings.",
            "And obviously as you could see it didn't really help compared to the full deep learning approach.",
            "OK, other questions, interesting talk.",
            "So my question is more related to your future work because you said that in future your you plan to use some extended concept graphs or graphs and knowledge graphs to improve your results.",
            "So for me it looks like intuitively right direction 'cause if in your Barack example Barack Obama example you just used an abstract and of course it makes sense to use existing knowledge graphs to to get the same information structured from but.",
            "I'm just wondering how you're going to integrate existing knowledge graphs in in the in the process of learning and everything.",
            "How would that work?",
            "So for instance, so one ways to you can so you get the entity in concept in the documents and then you can try holding are connected in Knowledge Graph itself.",
            "So then you could extend with all the connections so 2 cannot see how connected they are and how did they are or what are the related connections.",
            "So you just extend the context with by doing that."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'm going to talk about how we can use wider nip learning for detecting crisis information categories on social media.",
                    "label": 1
                },
                {
                    "sent": "So this work was done with a sense- Harris Colony and this part of political comrades.",
                    "label": 0
                },
                {
                    "sent": "OK, so first let's talk a little about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Detections, so I've been detection is the task of automatic identifying clues and text that they note specific events type of themes.",
                    "label": 1
                },
                {
                    "sent": "So in in the context of crisis, that means that, for instance, it can be used for helping organizations to identify what are key events and also organized relevant informations and decide what to do.",
                    "label": 0
                },
                {
                    "sent": "What is important.",
                    "label": 1
                },
                {
                    "sent": "If we look at social media, particularly in a Twitters, can see that's a lot of data generated everyday, so around 200 million.",
                    "label": 0
                },
                {
                    "sent": "Active users 400 million tweets a day and if we look at previous usage we can see that, for instance, during the 2011 earthquake, 170 seven millions tweets were posted in only one day.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So typically we can divide even detection in crisis context in three different steps.",
                    "label": 0
                },
                {
                    "sent": "So first you want do you want to invite if a document is related unrelated to an event, then you want to know what it is about.",
                    "label": 0
                },
                {
                    "sent": "So is it about fire?",
                    "label": 0
                },
                {
                    "sent": "Is it about an earthquake or anything floodings for instance?",
                    "label": 0
                },
                {
                    "sent": "And then finally you want to determine what is actually talked about within those documents.",
                    "label": 0
                },
                {
                    "sent": "So you want to know for instance.",
                    "label": 0
                },
                {
                    "sent": "If he's about casualties, if it's about building collapsed, it's about people offering help, and that can be used again for organizing response.",
                    "label": 0
                },
                {
                    "sent": "So in this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going just to talk about the last parts.",
                    "label": 0
                },
                {
                    "sent": "The reason is from previous work.",
                    "label": 0
                },
                {
                    "sent": "We've seen that the first task correctly easy to perform using, for instance, standard classifications methods.",
                    "label": 0
                },
                {
                    "sent": "We can get like crazy up more than 80%, and what the last task is much harder.",
                    "label": 0
                },
                {
                    "sent": "And we've seen like results up to maybe 60% accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here I'm going to use deep learning, so if we consider like traditional approach and deep learning approaches.",
                    "label": 1
                },
                {
                    "sent": "So the main difference somehow is that the use of word embeddings compared to bag of words.",
                    "label": 1
                },
                {
                    "sent": "Even if you can use one beddings in other situations.",
                    "label": 0
                },
                {
                    "sent": "So in bag of words you will get one documents and you will represent it with one vector.",
                    "label": 0
                },
                {
                    "sent": "Generally that will represent the size of the vocabulary and you will say this words are there or disorder not there.",
                    "label": 0
                },
                {
                    "sent": "And then in embeddings, what you will do, you will represent words as vectors and so documents will not be one vector but a mattress.",
                    "label": 0
                },
                {
                    "sent": "And idea of the embeddings is that words that are similarities.",
                    "label": 0
                },
                {
                    "sent": "They will tend to be represented with the similar vectors.",
                    "label": 0
                },
                {
                    "sent": "So for instance here if you have like Boston in Paris, they will have similar vectors because they share the concept of locations.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the prime when you deal with tweets are very short documents.",
                    "label": 0
                },
                {
                    "sent": "Is that you lack context so they are very short, so you have just very small text to make decision for classification.",
                    "label": 0
                },
                {
                    "sent": "So an idea is to, for instance, we can use semantics so we can do until the extraction tools and identify what are within the documents.",
                    "label": 0
                },
                {
                    "sent": "For instance here you can do that.",
                    "label": 0
                },
                {
                    "sent": "Obama was politicians or at that time and for instance the Boston Marathon is an infant.",
                    "label": 0
                },
                {
                    "sent": "If you want to integrate that in in different classifying techniques.",
                    "label": 0
                },
                {
                    "sent": "So if you use instructional approaches, you will probably want to extend the vector representations of the document by adding the semantics.",
                    "label": 0
                },
                {
                    "sent": "If you are now in a deep learning methods, you can maybe use an beddings for presenting as well the semantics.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's different ways to do classifications, and using deep learning's and here we are focusing on one approach which is use of convolutional neural networks.",
                    "label": 0
                },
                {
                    "sent": "So this approach was used for sentence classification in percent 14 and the idea is like you get the metrics of embeddings.",
                    "label": 1
                },
                {
                    "sent": "That is like the representation of the text and then you run multiple convolutions and then you can do classification.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at how we can use that so the main issue, as our things like when you just use the text, you don't have the semantics that can actually add extra informations.",
                    "label": 0
                },
                {
                    "sent": "So wait to do that that we did in previous work is to for instance use embeddings for representing the content.",
                    "label": 0
                },
                {
                    "sent": "The semantic content of the documents.",
                    "label": 0
                },
                {
                    "sent": "But the main issue when you do that is that you can start.",
                    "label": 0
                },
                {
                    "sent": "You can see that this because of the small text and data that we have.",
                    "label": 0
                },
                {
                    "sent": "You get very little semantic vocabulary compared to the word vocabulary, so that means like when you learn the embeddings on the data, you have very little to do with, and it might not be the best to use the embeddings because they might actually be too generalizing on something that is actually not very big already.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So not project exists, not present an interest in 16 is to use a wide and deep learning so wide and deep learning is used.",
                    "label": 1
                },
                {
                    "sent": "The idea is like you get 2 representations of the same document, for instance, and you use one is like a more traditional regression approach.",
                    "label": 0
                },
                {
                    "sent": "Other side you selected onboarding approach, so you put them together so you have like a very specific models that is kind to be learned with the regression part.",
                    "label": 0
                },
                {
                    "sent": "There integration parts which is the white part and you get like a more deep resolution that generalize alot the problem.",
                    "label": 0
                },
                {
                    "sent": "Using the deep approach.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, if we look at this, we can see that the wide and deep learning could be actually used for problems.",
                    "label": 1
                },
                {
                    "sent": "So the idea here is that.",
                    "label": 0
                },
                {
                    "sent": "We can use, you can add.",
                    "label": 0
                },
                {
                    "sent": "The white part so and to use for the concepts, representations.",
                    "label": 1
                },
                {
                    "sent": "And we can use the D pad for the word representation, so it's kind of balancing so you get what representation for deep semantics and you get.",
                    "label": 0
                },
                {
                    "sent": "Deep representation for Shadow Wars representations.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how does that work?",
                    "label": 0
                },
                {
                    "sent": "So, so you get the documents you can from the words, you can generate the embeddings, then from the meetings we get a mattress representing the document and then you run the conversation on that side and on the other side you extract concept and entities from the documents and you run it through white requestion.",
                    "label": 0
                },
                {
                    "sent": "And model and then you give it together and then you can classify.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So going back toward task.",
                    "label": 0
                },
                {
                    "sent": "So in our task, what do we have is?",
                    "label": 0
                },
                {
                    "sent": "Is we want to classify documents later tweets to detect crisis information categories so or data set is crisis legs.",
                    "label": 0
                },
                {
                    "sent": "So it's 28,000 tweets from Crisis Beach we collected between 2012 months and 15 contains two different crisis types and within those crisis types you have annotations, forces, information, 6 information categories.",
                    "label": 1
                },
                {
                    "sent": "So you know for instance categories like affect, individuals, infrastructure and utilities, donation and volunteering.",
                    "label": 1
                },
                {
                    "sent": "Question and advice and bathroom supports.",
                    "label": 0
                },
                {
                    "sent": "So this this again can be used to help responders to decide what is important information for them.",
                    "label": 0
                },
                {
                    "sent": "For instance, they could know where the infected individuals, if they if they have a classifier for helping them, they will know if there is infrastructure collapse and also where people.",
                    "label": 0
                },
                {
                    "sent": "If there's people offering help.",
                    "label": 0
                },
                {
                    "sent": "So we used X Rays over extraction, so there is many tools that can be used with this one because we provide quite a good coverage.",
                    "label": 0
                },
                {
                    "sent": "So so out of those 28,000 tweets, 65% get annotations.",
                    "label": 0
                },
                {
                    "sent": "I'm so then there is different ways again that we can integrate the concepts so we do two different approaches.",
                    "label": 0
                },
                {
                    "sent": "So the first one is the most straightforward approach is that you gates labels from front X Rays or the transfer disease entities.",
                    "label": 0
                },
                {
                    "sent": "Obama.",
                    "label": 0
                },
                {
                    "sent": "This is a politician.",
                    "label": 0
                },
                {
                    "sent": "We can use those labels like politician and Obama as our presentation in in our vector representation of concepts the other way, which is a bit deeper.",
                    "label": 0
                },
                {
                    "sent": "Is to use the abstract.",
                    "label": 0
                },
                {
                    "sent": "So for instance, when you get an entity or concept you go to DB pedia.",
                    "label": 0
                },
                {
                    "sent": "You get like the abstract, which is kind of the definition.",
                    "label": 0
                },
                {
                    "sent": "You can use the first sentence.",
                    "label": 0
                },
                {
                    "sent": "So the idea is like in this sentence that is kind of the definitions of that concept of entities.",
                    "label": 0
                },
                {
                    "sent": "You get implicit semantics.",
                    "label": 0
                },
                {
                    "sent": "So if you look at, for instance Obama definition, you can see that it mentions that is a politician is American.",
                    "label": 0
                },
                {
                    "sent": "Is American right?",
                    "label": 0
                },
                {
                    "sent": "So if we use the graph, actually we get that from the graph of the semantic graph.",
                    "label": 0
                },
                {
                    "sent": "But from the text we cannot get it already.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of.",
                    "label": 0
                },
                {
                    "sent": "Implicitly expanding the context or presentation of the document.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we look again at the data so we can see that the data is quite unbalanced, so you will get maybe more document, more tweets about floating's than than tweets about bombing.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we balance the data set.",
                    "label": 0
                },
                {
                    "sent": "So we have like representation of documents that are uniformly distributed around this categories and then something else.",
                    "label": 0
                },
                {
                    "sent": "So we want to know actually hold the semantics can help.",
                    "label": 0
                },
                {
                    "sent": "So what what do we do is that we sampled?",
                    "label": 0
                },
                {
                    "sent": "The data is well too.",
                    "label": 0
                },
                {
                    "sent": "Just includes documents that have more than two entity or concepts so that reduced a lot of course datasets.",
                    "label": 0
                },
                {
                    "sent": "So then we have two baselines, so which seen any previous work that SVM was performing quite well as baseline.",
                    "label": 0
                },
                {
                    "sent": "So we use two approaches.",
                    "label": 0
                },
                {
                    "sent": "One is with TF IDF to normalize vector representations.",
                    "label": 0
                },
                {
                    "sent": "And then we use also what embeddings.",
                    "label": 0
                },
                {
                    "sent": "So we've pretty trained embeddings from Google.",
                    "label": 0
                },
                {
                    "sent": "So we do traditional activations and report precision recording one.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we look at the results, what web services like in general like when we do our approach, we get much better while we get better results than the baseline.",
                    "label": 0
                },
                {
                    "sent": "Not much better results than our previous work that was child CNN like around 4% which is still an improvement.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so again, if we look at the results.",
                    "label": 0
                },
                {
                    "sent": "So the semantics in an approach that we presented outperform the baseline, and that's significant.",
                    "label": 1
                },
                {
                    "sent": "Also, we can see that the more you semantics, the better the results are.",
                    "label": 0
                },
                {
                    "sent": "As you can see, like if you look at the data again.",
                    "label": 0
                },
                {
                    "sent": "You can see that if used abstract, you get slightly better results and then if you don't use them.",
                    "label": 1
                },
                {
                    "sent": "But of course that's also something to take into account.",
                    "label": 1
                },
                {
                    "sent": "Is that the abstract outperform in general, but if we look at the significance not all the time significance particular datasets, so which makes some sense.",
                    "label": 0
                },
                {
                    "sent": "But if you look actually at the data that is sample on the semantics, then we get better results.",
                    "label": 1
                },
                {
                    "sent": "And it's significant.",
                    "label": 0
                },
                {
                    "sent": "So what can we do in future work so there's many things we can do.",
                    "label": 0
                },
                {
                    "sent": "Basically, here we use CNN because they are quite practical and quite easy to train.",
                    "label": 1
                },
                {
                    "sent": "They don't require too much resources, but we can use more like other models like we can neural networks or attention networks for instance.",
                    "label": 1
                },
                {
                    "sent": "So the idea here is like we get a better presentation of the text and dependences between documents.",
                    "label": 0
                },
                {
                    "sent": "And then also we can use different embeddings.",
                    "label": 0
                },
                {
                    "sent": "As for bootstrapping the model.",
                    "label": 0
                },
                {
                    "sent": "So here we use the Google embeddings already trained, but there is also like for instance already embeddings trained on Twitter or data which might be helping.",
                    "label": 0
                },
                {
                    "sent": "And finally, we we kind of tested two approaches for interpreting the semantics.",
                    "label": 0
                },
                {
                    "sent": "Here the thing we could do is we could extend those integration in different ways, so we could use extended concept graph for instance like dependencies between the entity and concepts on there.",
                    "label": 0
                },
                {
                    "sent": "Or we can also like use maybe a graph embeddings as well, which would be an interesting airport.",
                    "label": 0
                },
                {
                    "sent": "And finally, also what would be interesting is that this data set is relatively small and also that the documents are relatively small.",
                    "label": 0
                },
                {
                    "sent": "So it would be nice to see if this approach actually works better on bigger data and because it has it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "You have used Ness.",
                    "label": 0
                },
                {
                    "sent": "We haven't a as a classifier.",
                    "label": 0
                },
                {
                    "sent": "Affianced entered correctly.",
                    "label": 0
                },
                {
                    "sent": "Have you played around with other classifiers?",
                    "label": 0
                },
                {
                    "sent": "So we so we found that boosting classifiers works better for this kind of setting.",
                    "label": 0
                },
                {
                    "sent": "We increase work.",
                    "label": 0
                },
                {
                    "sent": "We did similar tasks and we tested more than just his VM for instance.",
                    "label": 0
                },
                {
                    "sent": "So we saw that this game was the best as the baseline in terms of deep learning approach.",
                    "label": 0
                },
                {
                    "sent": "As I was saying.",
                    "label": 0
                },
                {
                    "sent": "The primary issue is that they were quite a lot of competitions, so CNN is relatively easy to train, so obviously that's the main reason, so we didn't look at other approach which might be better from literature, as we can see that they might perform better.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so for the baseline, I agree.",
                    "label": 0
                },
                {
                    "sent": "SVM is one of the best performing classifiers, but if you incorporate some different features so it changes a bit if you have this different features, how you can exploit them and then SVM becomes not the best one, and that's the point.",
                    "label": 0
                },
                {
                    "sent": "Do you mean like doing like more feature engineering's if you understand, or no no?",
                    "label": 0
                },
                {
                    "sent": "It's just so we have you changed the way of space?",
                    "label": 0
                },
                {
                    "sent": "So if you go trust with the usual stuff of features with this long vector space, then SVM performs quite well.",
                    "label": 0
                },
                {
                    "sent": "But if you incorporate with embeddings the features then it changes and then the question is if other learners before performs better.",
                    "label": 0
                },
                {
                    "sent": "And this is what we found out, and that's the question.",
                    "label": 0
                },
                {
                    "sent": "If you have played with that.",
                    "label": 0
                },
                {
                    "sent": "God embeddings we just done bad things as well in in the SVM classifiers.",
                    "label": 0
                },
                {
                    "sent": "So we tried as well to see if they will help traditional classifiers and it didn't really work.",
                    "label": 0
                },
                {
                    "sent": "I mean we did a simple which is kind of which has been used in the past, which is actually well approach which is you kind of container down beddings using Princeton average representation with visually simplify lots then beddings.",
                    "label": 0
                },
                {
                    "sent": "And obviously as you could see it didn't really help compared to the full deep learning approach.",
                    "label": 1
                },
                {
                    "sent": "OK, other questions, interesting talk.",
                    "label": 0
                },
                {
                    "sent": "So my question is more related to your future work because you said that in future your you plan to use some extended concept graphs or graphs and knowledge graphs to improve your results.",
                    "label": 0
                },
                {
                    "sent": "So for me it looks like intuitively right direction 'cause if in your Barack example Barack Obama example you just used an abstract and of course it makes sense to use existing knowledge graphs to to get the same information structured from but.",
                    "label": 0
                },
                {
                    "sent": "I'm just wondering how you're going to integrate existing knowledge graphs in in the in the process of learning and everything.",
                    "label": 0
                },
                {
                    "sent": "How would that work?",
                    "label": 0
                },
                {
                    "sent": "So for instance, so one ways to you can so you get the entity in concept in the documents and then you can try holding are connected in Knowledge Graph itself.",
                    "label": 0
                },
                {
                    "sent": "So then you could extend with all the connections so 2 cannot see how connected they are and how did they are or what are the related connections.",
                    "label": 0
                },
                {
                    "sent": "So you just extend the context with by doing that.",
                    "label": 0
                }
            ]
        }
    }
}