{
    "id": "h3meszwwqpcvdx2p6iaglztl3gcxwf2o",
    "title": "Theory of RL",
    "info": {
        "author": [
            "Csaba Szepesv\u00e1ri, Department of Computing Science, University of Alberta"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_szepesvari_theory_of_rl/",
    "segmentation": [
        [
            "Alright, thanks for the nice introduction actual and let's get started with this.",
            "Talk.",
            "So you see the title.",
            "That's what I'm trying to do, but it's how I feel.",
            "How many of you know what is?",
            "Picture is about this picture was taken recently.",
            "Yeah, it's a right.",
            "Like my generation, the Commodore 64 game.",
            "Possible mission.",
            "It's really epic."
        ],
        [
            "Anyways, so the real title is theory of reinforcement learning and the reason I was saying it's Mission Impossible because in, in or in a half and are right now and or in 20 minutes doing all this I feel that.",
            "I'm trying to achieve something really great."
        ],
        [
            "Let's put it that way.",
            "So at anytime during the talk, if you have questions, please raise your hand and also you have to speak.",
            "Loudly when you're asking question because this chairs are, you know, like musical chairs and I'm part deaf, so take those into account and then you have to shot at me.",
            "OK, I will try my best but so here's the content.",
            "So how many people here in this room have ever, you know, touch theory?",
            "In machine learning, yeah, those are my full logs, so how many people haven't done that?",
            "Come on.",
            "It's fine, you are my people too.",
            "Or after this talk.",
            "So first I'm going to spend quite a bit of time on motivating.",
            "Why we should be talking about theory?",
            "Why we should care about theory and what theory can bring to the table.",
            "And then we jump in the middle.",
            "I'm going to talk about various aspects of theory of reinforcement learning, including batch learning.",
            "Using simulators is planning algorithms, and if you don't have simulators, what can you do and what you should be?",
            "Looking at."
        ],
        [
            "And like I really feel that I need to manage expectations here, there is a lot of stuff that I'm not going to cover.",
            "I'm unable to cover given the limited time that I have, so we will only cover very very simple things.",
            "But don't think that only those exist.",
            "So there there are other things that exist.",
            "So there I'll be around for the rest of the day and tomorrow.",
            "And so if you have any questions.",
            "In connection to this talk or anything as, please come and talk to me.",
            "So I'm going to talk about very simple tasks and I'm going to try to illustrate principles that we use in theory and some some hurdles that you need to overcome.",
            "I want to highlight those hurdles that make a reinforcement learning different from supervised learning.",
            "So that's the goal.",
            "So let's jump into the first part, which is what is this guy talking about and why?"
        ],
        [
            "And you should be something you should not be slinking yet.",
            "OK so well then bye.",
            "So.",
            "First, let's discuss what do we mean by theory.",
            "We're not going to discuss what do we mean by reinforcement learning.",
            "That would be hilarious after full day and a half of 2:30 as about reinforcement learning.",
            "And who needs theory and how does learning theory work?",
            "So in this part in particular, I'm going to brief you a little bit about statistical learning theory, which is the foundations for machine learning.",
            "What people do there, but again, like right now I'm going to be presenting a tiny, tiny, tiny fraction of what, so there, so that's important to keep in mind.",
            "So just for inspiration and aspiration.",
            "I thought that it's good to cover some basics of classical learning theory in supervised learning.",
            "So."
        ],
        [
            "First, what is theory?",
            "So let me ask you this question what is theory?",
            "What do you say?",
            "Through math, math, test theory.",
            "OK, yeah, maybe it's part of it.",
            "Is it necessary to do math to do theory?",
            "Fruit.",
            "Select it's a tool and language.",
            "So what are the important ingredients of theory?",
            "If we're saying that we're building it here, or we have, so we have assumptions and then what else?",
            "We should have some theorems right, but more generally, if you're thinking about Sciences theory, basically what you have is that you have models and predictions that make predictions about how things work.",
            "Right?",
            "So that's theory.",
            "You have models and you have predictions, but at the same time everyone was right.",
            "Who said that we need Matt?",
            "And predictions about what, right?",
            "So here we are not really studying.",
            "Physical process is we're not.",
            "We're not really doing science here.",
            "What we're doing is different.",
            "We're doing mathematics.",
            "We're studying.",
            "You know we think are set up, things that can be computed.",
            "These are conceptual things, right?",
            "These are in mind.",
            "We can instant ate them in a computer in some approximate fashion, but mostly these are just conceptual things.",
            "And not really that much connected to the physical work, but at the end of the day, if you want to build a robot, then of course it's going to be connected to the physical world.",
            "So to make the theory useful, maybe you should have some connection to it, so so that's the definition of theory for me."
        ],
        [
            "And so my next slide is about these two gentlemen.",
            "Does anyone in the audience know who these gentlemen are?",
            "Any guesses?",
            "I've heard Maxwell.",
            "So that's the guy on the left, right, right?",
            "For you, yes.",
            "And the the other guy.",
            "No.",
            "I cheated Marconi.",
            "Yes.",
            "So the gentleman is a big.",
            "This Max fell, the other guy is Marconi.",
            "So what did these guys do?",
            "Electromagnetism is basically the invention of Maxwell.",
            "What is the invention of Marconi?",
            "The radio?",
            "Yep.",
            "So in a way, Maxwell, you know he came way earlier than Marconi.",
            "He invented to solve Maxwell equations that summarize how electromagnetic waves traveled in space time.",
            "And with that he created a foundation with his theory for building useful devices like the radio, right?",
            "So oftentimes, this is what happens that Maxwell was not really interested in building a radio.",
            "He wanted to understand some phenomena.",
            "He wanted to build a theory for the sake of understanding something.",
            "And.",
            "And this is often but theoreticians would do.",
            "They would, you know, go about their little problems.",
            "That may look like a story problems, but along the way.",
            "Who knows.",
            "In this case you had to wait quite a cover of decades before the discovery of the Maxwell equations lead to something ultimately so useful, like the radio.",
            "But it happened."
        ],
        [
            "Right so I.",
            "There are those of us who want to do theory for the sake of you know, seeking knowledge, an ultimate source of knowledge, but a lot of folks who say that I don't really feel that I should be doing theory.",
            "Question is, should you still care?",
            "Of course, if you are coming to this talk, what do you expect from this speaker?",
            "I'm going to say yes.",
            "So why?",
            "Why should you care?",
            "Well, I say that predictions and theory in an idea setting are going to help you to do the following things.",
            "Maybe design algorithms design you, algorithms designed, bagger algorithms that exist today.",
            "So you could use theory for that.",
            "If you have some algorithm, you can ask questions about how does it behave.",
            "When does it function?",
            "When does it work?",
            "And you can try to study this in an experimental setting that you should, and that's all fine, but to ultimately understand what are the boundaries of the scope of an algorithm, well, it's hard to imagine doing that without here.",
            "Or if you want more quantified information about, for example, you have an uncertain situation and you want to quantify uncertainty, then theory can help you to do that.",
            "Lastly, but at least if you want to identify new challenges, sorry.",
            "Final challenges.",
            "Yes, that's awesome theory.",
            "So I'm not sure that icon is everyone that they should care about.",
            "I hope that I did, but if it's not."
        ],
        [
            "Then let's look at this.",
            "Right, so we know that shit happens.",
            "So why is this important?",
            "So when you are trying to tweak your neural network and running those experiments, they're night and it doesn't work.",
            "How do you figure out why it doesn't work?",
            "Well, if you have a model in place that is able to describe when things are expected to work.",
            "Then you can look at deviations from those idealistic situations.",
            "And find out what is causing your actual experiment not to run successfully, and I think that we don't execration we can say that.",
            "Like in this fear the fast these days I think there is a very very tiny little fraction of.",
            "Experiments that lead to published results and there will be a lot of sweat and work that goes into triangles, things and then seeing them fair.",
            "And if you want to like my.",
            "My mission is to try to convince you that if you think a little bit about.",
            "Then maybe you will have some shortcuts there to prevent some failures.",
            "Or to recognize the source of the failures and then prevent future failures.",
            "Alright, but the truth?"
        ],
        [
            "Is that.",
            "Practice are not antagonistic at all there together liking and young and even in the case of Maxwell Maxwell didn't just invent the electromagnetic waves and their equations out of the scenar.",
            "There were lots of experimental evidence that there is something that needs to be explained right and then eventually that beautiful theory lead to future things.",
            "So that's how.",
            "In practice, are working hands in hands.",
            "Alright."
        ],
        [
            "OK, so with that I finish my.",
            "You know Thurmont for.",
            "Praising theory, so let's jump into the middle of things.",
            "I'm going to talk a little bit about statistical learning theory, as I promised before.",
            "So what are the ingredients of this theory so that distributions, samples, learning algorithms, predictors, and loss functions?",
            "So how many people are you are familiar with this notation and terminology.",
            "Let's raise hands.",
            "OK, well that's good.",
            "Nevertheless, I think that it's good to have some recordings of these ideas and and let's do this in the context stuff binary classification, which is the simplest.",
            "Possible.",
            "Supervised learning task.",
            "Alright, so I'm going to instantiate all these things in the context of binary classification.",
            "So the distributions.",
            "What are they?",
            "So there will be some input space.",
            "I usually do note that by Capital X an it could be like Rd.",
            "So D dimensional Euclidean space and there is an output space.",
            "And since we're talking about binary classification, the output space is going to have two elements conveniently denoted by zero and one.",
            "Then we're going to have a class of distributions over.",
            "The cross product.",
            "Of the input space and output space.",
            "So M is my notation for all the distributions over that set, right?",
            "So what's in the argument of the set?",
            "That's where these samples are going to leave, and so if I say P, the subset of order distributions, if I take any element of P, that means that from that distribution I can generate samples.",
            "I can generate samples in an IID fashion so.",
            "Do you guys in the back see my writing or you just OK?",
            "Alright, so we can generate these samples.",
            "So I sample.",
            "Is a list of topless.",
            "XI anvi.",
            "I leaving this SpaceX cross Y.",
            "And in classical learning theory, we assume that.",
            "XI&YI identically distributed according to some distribution in our family of distributions in learning theory.",
            "In one part it's called Agnostic learning theory.",
            "You make no restriction on P. You're going to equate P. This class of distributions.",
            "With the set of all possible distributions.",
            "So there is no further restrictions in other parts of learning theory.",
            "You start to make some restrictions and the results are going to differ because of that.",
            "So anyways, back to here.",
            "So XINVI at random variable that are sampled from this distribution P. And we're going to assume independence between them.",
            "So when you have this situation, then we say that this XY sequence is an IID sequence.",
            "Independent, I think identically distributed sequence of random variables.",
            "So this is how you think about your data.",
            "So you're thinking about that my data SN was some hole.",
            "Born by someone throwing dice.",
            "Repeatedly and times and the dice had so many different ways to so many different size that I, as a result of the dice rules I got these guys and acts.",
            "I described my input and why I described what I want to predict.",
            "Given the input.",
            "So the next ingredient in learning.",
            "Is predictors.",
            "So in this case.",
            "The the predictors are just Maps.",
            "We also call them hypothesis from the input.",
            "To the label space, so to Y.",
            "Why is this?",
            "Why space is just.",
            "Does 801.",
            "So these are functions that map imports to either zero or one.",
            "We are trying to predict the labels.",
            "And for what?",
            "Well, not for the data that is in our data set, but for future data.",
            "OK. How are we going to evaluate?",
            "How good is a predictor?",
            "So if you pick a predictor like H?",
            "We're going to assign a loss to it.",
            "And when we are assigning loss, we have many, many choices.",
            "The simplest possible choice to assign a loss.",
            "Is the following so the loss function is going to map?",
            "Um?",
            "A hypothesis.",
            "And.",
            "An element of the XY cross product space too.",
            "Generally it it Maps to non negative numbers and the way it works is that you plug in the.",
            "The age.",
            "And then you plug in an X&Y into the loss function.",
            "And the simplest possible loss function is called a binary loss.",
            "Is defined as a loss of 1 if H of X is not equal to Y and otherwise it's 0.",
            "Alright.",
            "So we dressed up this obsolete in this abstract terminology.",
            "With this specific case, this is binary classification.",
            "Oh, I didn't say about the learning algorithm.",
            "Was a learning algorithm.",
            "It's not truly an algorithm, so that's a misnomer.",
            "It's a learning strategy or what not is going to map.",
            "A sample too.",
            "A hypothesis.",
            "Right, so it Maps the SpaceX cross Y to the power of N space.",
            "To the space of all possible hypothesis and all the space of all possible hypothesis, is in this case could be all the Maps age that Maps X to Y.",
            "So we don't care about.",
            "How to compute this?",
            "We just say, well, the learning algorithm at the end of the day is just going to deliver a predictor.",
            "If you have a predictor.",
            "And you have this distribution.",
            "Then you can ask.",
            "OK, how good is this predictor?",
            "So that's the risk.",
            "So there is a risk of the predictor.",
            "It's just the expected loss of the predictor and the distribution piece.",
            "So let's denote this by RH&P, which is the probability that age of X.",
            "Is not equal to Y in the case of binary classification where X&Y are jointly distributed according to P. So now a learning algorithm delivers based on the sample, which is random hypothesis.",
            "Hypothesis is going to be random itself.",
            "So if I plug in the output, I guess I better write here if I plug.",
            "In the output of the learning algorithm.",
            "Which is a F SN into the risk function which is going to deliver me around the number that depends on.",
            "The randomness of the sample right and then you can say oh OK, so that's the risk of the hypothesis produced by the algorithm.",
            "In general, you're interested in our Gothams.",
            "That produce hypothesis low risk?",
            "The hypothesis is the lowest risk is called a base hypothesis for whatever reason.",
            "And you want to be close to that.",
            "So this is a random number.",
            "You can just say that, well, it's five.",
            "Well, it depends on what the input is SN is.",
            "It's going to have a distribution, right?",
            "So important thing is that.",
            "This is going to have a distribution, so it's not negative because we said that the loss and all negative.",
            "Is distributed in some way.",
            "So again, characterize it by its mean by its variance, and then when we're talking about oh, how confident are you that the risk of your.",
            "Of the hypothesis produced by our learning algorithm is above.",
            "I don't know below 0.1 we were talking about this protein mass that's there.",
            "Which depends on unknown P which you don't know.",
            "So even the problem of knowing how well you're doing becomes kind of challenging in this framework, so you need to have tools to deal with that.",
            "So this is kind of the setting where we are starting from the learning theory setting."
        ],
        [
            "OK.",
            "So how does learning theory work?",
            "Um?",
            "There are many questions asked in learning theory.",
            "One of the questions is belongs to what I call a priori analysis.",
            "How well a learning algorithm will perform on new data and future data that tries to answer questions about this distribution.",
            "Without seeing P without seeing as it's a no prior prediction, like if you have P coming from this family and you're learning algorithms like that, this is what's going to happen.",
            "It's a priori prediction.",
            "And the poster reran that Easyserve prediction is after seeing the data.",
            "Still not knowing P. What can you conclude about the distribution of the risk?",
            "So there is a subject of a posteriori analysis.",
            "OK, so learning theory studies all these."
        ],
        [
            "Seen a priori analysis you have for example, results for questions like can we compete with the best hypothesis from a given set of hypothesis is.",
            "So.",
            "For example, you could restrict here the set of hypothesis in some Montreal away.",
            "And you say that I just want to compete with the best hyper these over there without putting any restrictions and P that's called agnostic crack framework vapnik's learning theory.",
            "And the problem is, can we match the best possible loss, assuming the data generating distribution belongs to a known family, so that's what mostly people do in parametric and nonparametric statistical analysis.",
            "Other problems include does argutum.",
            "Some algorithm achieves something?"
        ],
        [
            "Alright.",
            "So I'm going to skip over this slide and just skip to two fundamental results of statistical learning theory, which are crucially important to know about, at least know because it's just, you know, foundation for a lot of the thinking that goes into learning theory.",
            "So I'm going to present two designs, the fundamental theorem of statistical learning theory and the result about computation complexity.",
            "So notice that we haven't talked about computation complexity of the algorithm yet, but there is a question question of whether you can achieve a small risk.",
            "In an efficient way, efficiently computable way, it's not enough to say that yes, you can achieve a small risk, because maybe it's not computable."
        ],
        [
            "Right, so the fundamental theorem of statistical learning theory answers the question for problem number one.",
            "And it goes like this.",
            "It's very simple.",
            "If you take any hypothesis class H, which is a subset of this big hypothesis class.",
            "There is a unique number.",
            "It's called the VC dimension of H and I'm not going to define it.",
            "Most likely you have heard about it anyways, that characterizes this class.",
            "And if you want to compete.",
            "With the loss of the best hypothesis up to an accuracy of epsilon, so your access risk shouldn't be more than epsilon of the best possible risk over the hypothesis space age, then you are going to need at least the VC dimension of H divided by epsilon squared samples.",
            "And it's possible to to achieve this risk with this many samples, so this up to log factors.",
            "This is tight.",
            "So this is the inherent difficulty of competing with the best hypothesis in a hypothesis class.",
            "In this scale learning theory setting.",
            "So that's one of the foundational resigns.",
            "It's cooler.",
            "I think it's cool.",
            "It's a pure information theory is that and they are getting that achieves.",
            "This is super simple.",
            "It's called empirical risk memoization.",
            "It's basically a compute the empirical loss and assume that you can minimize it over the hypothesis space of your choice.",
            "And you can prove that this is this.",
            "Is this algorithm algorithm.",
            "It's not, it's like Arcmin algorithm is going to achieve this guarantee."
        ],
        [
            "At the same time, things are not rosy.",
            "Not that rosy altogether.",
            "So the next result concerns the computation complexity of dealing with some very simple hyperdata spaces.",
            "So the linear classification in linear classification, the input space has a vector space.",
            "Structure, it's like Cardi.",
            "Let's say it's already.",
            "And.",
            "And the hypothesis space is, you know, on one side of a linear hyperplane.",
            "You have a plus one the other side you have a zero label, so very very simple hypothesis and you're trying to compete with the best hyperplane.",
            "And this result says that.",
            "Unfortunately, in this framework, when you have no idea about the underlying distribution, no restrictions on the underlying distribution.",
            "This seems to be computationally hard.",
            "The exact result says that unless RNP is equal to RP.",
            "RP is this class, which is maybe a little bit bigger than P. It's randomized time polynomial.",
            "So unless I'm MP equals RP, which most of the people don't believe, it's true, linear classifiers cannot be learned in polynomial time in this framework.",
            "Yes.",
            "It will be harder.",
            "Right, so if linear is this hard, then lonelier could be much harder.",
            "Why would it be easier?",
            "OK, so if it's a subset, right?",
            "I don't know actually people study this questions on a case by case basis.",
            "Unfortunately we don't really have a generic good understanding of the computation complexity of this task.",
            "But I don't think that we should have high hopes about that suddenly because.",
            "You're canceling only their classes.",
            "It's going to be easier, but at the same time there are some interesting phenomenas sometimes.",
            "For example, if you allow yourself a little bit of lag.",
            "Like a bigger class sometimes actually could have.",
            "If you're choosing a bigger class of functions, then there are two things that are happening.",
            "The smallest risk becomes smaller.",
            "So it might be harder to get to there to that point, but at the same time, maybe somehow the search space is changed in some miraculous fashion, but in general we know that.",
            "Like we have harness results for training neural networks.",
            "If you're really interested in that, like I can talk offline about that and also about whether those results are relevant or not, because that's also question right?",
            "So this hardness results always use its worst case thing.",
            "They make up something right?",
            "So I think this is also cool.",
            "Why is it cool?",
            "It's a negative result, shouldn't be said.",
            "Anime, yeah, you can be sad about it, but it's a fact you have to live with it.",
            "It's cool to know a fact right?",
            "And you know that.",
            "Well, if this is how things are here.",
            "But maybe we have to change something if we want to deal with real stuff.",
            "So I think that negative results are absolutely fantastic, and that's like the unique contribution of theory to machine learning, it's.",
            "There is nothing better than that.",
            "It's kind of like you're trying to to know what's possible and what's not possible, and you're trying to push the boundaries from inside and from outside.",
            "Both face.",
            "You want to figure out whether there is the thing that is possible to do, right, yeah.",
            "Or would you mean by therapy?",
            "Learn because earlier you had a simple complexity result, right?",
            "Polynomial time, yes.",
            "Epsilon regret you would need right?",
            "The computation time cannot be polynomial, because then you would be serving some hard,, tutorial problems.",
            "So there is a reduction from a computer problem.",
            "To minimizing risk up to an epsilon accuracy.",
            "If you could do that in time which is polynomial in what in one over epsilon.",
            "The VC dimension of H, so you can't enumerate all the hypothesis and basically this.",
            "Now you can't do it.",
            "OK. John.",
            "The end is not.",
            "One over epsilon to the N is not polynomial.",
            "Yeah, so you can do things like that.",
            "Just takes too much time and is the number of samples.",
            "Yeah.",
            "OK, other questions.",
            "Alright, I'm doing very badly this time."
        ],
        [
            "OK, let's skip.",
            "So batch learning finally getting to reinforcement."
        ],
        [
            "Running so batch learning was mentioned already in Choice tutorial.",
            "It is this problem where someone was collecting data for you.",
            "And you try to learn a policy based on the data.",
            "OK, so I'm going to put this into a framework that is the closest to the supervised learning framework.",
            "So how does it work?",
            "The way it works is that you have this.",
            "XDA TV TRT topple.",
            "So this is estate that is sampled from this distribution U.",
            "This is an action that sampled from some policy of your choice.",
            "It's like a Markovian policy, so it can be stochastic, or it could be deterministic.",
            "So given the state.",
            "Gives you an action, maybe randomly.",
            "And then there is a next state that is sampled from the property kernel that describes the NDP underlying.",
            "So I'm using an MDP framework here.",
            "So why is the next state?",
            "And for simplicity, and to avoid further complications, is the nicest case.",
            "We're going to assume that the data is IID.",
            "OK, yes I'm confused where it comes from source.",
            "You're specifying that there's one fixed policy Pi that generated all of this data, right, right?",
            "Yeah, so I'm just imagining a very simple scenario where someone is running a policy to generate data.",
            "So if you have observational studies, for example, this is kind of what's going on, right?",
            "Like someone is generating the data, they are running their processes.",
            "You may or may not know the data generating policy.",
            "You are recording everything.",
            "Basically you are recording that.",
            "Well there was this state and then there was this action.",
            "There was a next state, by the way.",
            "States are evil, but let's move on.",
            "For simplicity, let's assume that you have access to this.",
            "I'm like it's like an impossible thing like you don't have access to states, but like for simplicity and like you will see where I'm going with this.",
            "So then you will see that it was fine, but I always want to scream when someone states states I do say states often, yes.",
            "Spadard yeah it's not a trace.",
            "We get talk about traces later, but it's like there is a distribution.",
            "It's like the supervised learning setting, right?",
            "Like it's just, but we have this little little transitions.",
            "Right, so someone samples and initial state and then this is 1 transition that is made and you record the transition.",
            "So you learn about the dynamics, but it's little pieces of the dynamics that you can learn about and you learn about the reward as fast so you see the reward and you have a fixed horizon H. OK so I want to simplify things.",
            "No discounting it just fixed horizon.",
            "OK. And you have a class of policies, and since I want to mimic what people do in supervised learning, I just want to find.",
            "And epsilon optimal policy in this policy class pie.",
            "And I'm asking the question OK, what is it?",
            "Give me a sample complexity result for this problem, right?",
            "It's like in supervised learning.",
            "If you have your data generated from some unknown distribution P, the only thing that matters is the VC dimension of the class.",
            "So in this case the class is the policy class we want to compete with the best policy in this class, given the distribution that we don't know this probability kernel and the reward.",
            "And can we do it?",
            "It's a very simple question.",
            "We should start there.",
            "Good.",
            "Right, so let's see how it goes."
        ],
        [
            "Or so.",
            "So recall that the value of a policy is what like.",
            "Basically you have to sum up the rewards and take expectations, right?",
            "Like if you had a policy at all and you go for a H steps.",
            "So in this case you go for H steps and you take the expectation.",
            "So for any state you're going to have some value.",
            "Here I'm going to assume that you are initial state is samples from this distribution.",
            "Mu for simplicity.",
            "And I just want the policy that when the initial state is sampled from, you gives you the maximum reward.",
            "Expected reward after 8 steps.",
            "And I find this an absolute to my policy too.",
            "OK, so one easy corollary.",
            "If you take the horizon to be 0 so there is no trajectories.",
            "This just supervised learning.",
            "Right, you just want to maximize immediate reward.",
            "Right just one step, you don't care about dynamics prediction future what not just immediate reward maximization.",
            "This kind of like one particular case of supervised learning.",
            "It's called cost sensitive classification.",
            "It's like not really the binary classification, but it's pretty close and you can imagine how you can get sample complexity results in this case as well, and there will be very similar to the sample complexity result that we had.",
            "Fine.",
            "So let's move on to something more challenging.",
            "So.",
            "Alright, so OK just spelling out the obvious."
        ],
        [
            "Let's take.",
            "The case and the horizon is 2.",
            "So you basically make 2 steps and I'm going to consider a very simple policy space.",
            "And a very simple example that is going to prove this this 1st result.",
            "So the state space is going to be 01.",
            "And the policies are like this.",
            "You choose a threshold.",
            "An on the right of the threshold you take action one.",
            "An on the left of the threshold you choose action minus.",
            "So you cannot change the threshold.",
            "That's your parameter that theater as a policy class.",
            "And now take the following MVP.",
            "If you start at the state zero, you stay at stay there.",
            "If you start at state one, you stay at state one.",
            "If you start anywhere but 0.50 and one, you transition in a single step to 0.5.",
            "OK, and if you are a 0.5.",
            "If you take plus one, you get to 1.",
            "If you take the action minus one, you get to there just clear very very simple.",
            "So let mu this distribution be.",
            "The uniform distribution of an O one.",
            "And let's imagine that we generate this data.",
            "That we have.",
            "From the uniform distribution.",
            "We collect an samples.",
            "And then we need to learn what to do.",
            "The catch is that there are two MVP's I'm saying there not more than two MVP's in one of the MVP's.",
            "If you get to 0, if you get to stay at zero, you're going to get a + 5 are there.",
            "And the minus five art at one and then the other MVP.",
            "It's the other way round.",
            "The good state is 1 and the best set is 0.",
            "OK. And you're generating and generating and generating the samples.",
            "Well, first of all, you're not going to hit zero or one.",
            "So you will never see whether you're going to get the reward of 01.",
            "And you will also not hit 0.5 ever in the sample as a probability of 0.",
            "So you can generate infinitely many samples.",
            "And you're not going to learn to distinguish these two MVP's, and in one of the MDP's you should be going left in the other.",
            "MVP should be going right.",
            "And the policies have a very simple structure as well.",
            "So there is no counterpart of the fundamental theorem of statistical learning theory in batch reinforcement learning.",
            "Just not possible.",
            "Alright, so in this generality.",
            "Sorry.",
            "You can't say much.",
            "This was just meant as a very simple example illustrating how things very quickly ask late in reinforcement learning.",
            "And again you have a negative result.",
            "What do you do?",
            "Go home.",
            "Close shop 1.",
            "Haha, well we should think about what the problem is.",
            "What is causing this?",
            "How are we going to reconcile this with all the good experiences that we have about training for spent learning?",
            "Well?",
            "First of all, what is the problem?",
            "The problem is that the critical decision is at 0.5.",
            "But in the data 0.5 doesn't appear ever or is there and one doesn't appear ever right.",
            "I.",
            "So.",
            "Can we make it so that like we are sure that all the critical states similar to 0.5 there and one appear in the data without any further?",
            "This I mean like.",
            "It is a very strange assumption to to make that.",
            "Is sure that this is happening.",
            "Nevertheless people try.",
            "So we can.",
            "We can try to make assumptions like we're going to have better sampling distributions.",
            "But at the same time, it's kind of like really hard to push this through.",
            "OK, any questions?",
            "What's the example clear?",
            "MU is the distribution that you're generating the initial states from.",
            "So you choose anything, issue states from the uniform distribution.",
            "And then you follow the transition kernel for one step.",
            "So if you happen to be, you know like you're going to be somewhere here.",
            "So all that you're going to see is that all the transitions go to 0.5.",
            "That's all what you have in the data.",
            "There is nothing else in the data.",
            "So you will never learn about the effect of the actions at 0.5 and you will never learn about the rewards in one.",
            "Right, so that would be a good suggestion.",
            "So why not consider at least 2 steps?",
            "I can.",
            "Tweak things in such a way that that doesn't help.",
            "So you could draw a little tree and then you're embedding the intermediate states in such a way that you're never going to hit those, and those would be key to have.",
            "Yeah, you said that you don't see .5 in the data set right?",
            "Because you are generating the data uniformly at random.",
            "And then you only see it as the next state.",
            "You don't see it as a starting state.",
            "But like if you so the dynamics of the environment you're not following the transitions, you're only following the transitions for one step.",
            "Oh what?",
            "It's because that's how I set it up.",
            "If you're following transitions for multiple steps.",
            "Then the problem is a little bit more dedicated, but it has the same.",
            "Nature.",
            "The problem is that the optimal policy.",
            "Might want to, you know.",
            "Gola certain way.",
            "And all the data that you are seeing from the policy that you were using to generate the data go some other way.",
            "An if you're not seeing this, keyhole states which are really important for you know to how to act.",
            "Then the situation is doomed.",
            "I think this is very important to understand because this is one of the challenges that we face all the time in reinforcement learning.",
            "That we are learning with changing distributions as opposed to.",
            "Supervised learning that the train and the test distributions are mostly the same, right?",
            "So when I said that, let's compute the risk of the hypothesis.",
            "It was this under the same distribution that was used to generate the data.",
            "It was not a different distribution in reinforcement learning.",
            "When you switch to a policy.",
            "Your switch to new distribution.",
            "There's no control over that.",
            "That's much higher share.",
            "It's a harder learning problem because of this inherently harder.",
            "OK, one more question.",
            "And then we need to move on.",
            "No, I mean like you pick a distribution, I pick a problem.",
            "We can play this game right.",
            "And then it won't work like you can always imagine that I can trick you by choosing specific MVP, because we didn't put any restriction on the MVP, so I'm going to like be very tricky with that anyways.",
            "So let's move on.",
            "We can talk offline later on so.",
            "What so is this?",
            "The end of the world?"
        ],
        [
            "Well, no, I mean like you can push it.",
            "I'm going to skip this slide and come back to it later, but we have we do have some general recipes for positive results as fast so people are pushing the algorithms and you can do an inverse energy.",
            "This you look at the algorithm and you say that hey, under what conditions can I prove that these are active does work?",
            "And then you can work with the conditions, and that's helpful for debugging things when things don't work, so it's not going to be a general theory like the fundamental theory of learning theory of learning theory, but you can still analyze specific algorithms, and I will show a result of such analysts later, OK?"
        ],
        [
            "So the next topic I wanted to talk a little bit about is when you have a simulator, because this kind of addresses this this issue that we are discovering that.",
            "Distributions mismatch exists in reinforcement learning.",
            "It's inherent to reinforcement learning.",
            "The training distribution in the bad setting, and the distribution is used by the policies are just different, and that's a problem.",
            "But if you have a simulator.",
            "And it's a whole different new word.",
            "You can simulate trajectories as you will.",
            "You can generate new data.",
            "Under your policy.",
            "So this leads to what we call a planning problem, and so in the planning problem, the way I define it, you mentioned that you have a huge MDP.",
            "It's so big that you can't hope to inherit all the states in it.",
            "In fact, you will have problem naming the States in the MDP, and your goal is to compute.",
            "Still may be a good policy within a restricted policy class or over all an action of the good policy."
        ],
        [
            "So right?",
            "So which one is easier?",
            "So that's a very good question.",
            "Is it easier to compute a good action at a given initial state, or is it easier to come up with a good policy from let's say, same policy class?",
            "Well got fitting should be that it should be easier to get a good action because if you have a good policy then you just take the action that it recommends.",
            "So the second problem is definitely not harder, but it's actually easier.",
            "It's a good question.",
            "There are cases when it's actually easier, so that's one general less.",
            "I'm not going to talk about the details of that, but just keep that in mind that if you don't have to solve some problem like getting a globally optimal policy, that's optimal everywhere.",
            "Because you only care about this state.",
            "Then you can focus your computational resources on that state and do more and be better at that state.",
            "And then when the next state comes along, we do some more computation.",
            "Right, you can spread out the computation in time that way, and it's there is a separation between these two results in the sense that the first problem is strictly hard.",
            "In the second one.",
            "OK, important thing here is that there is no information theoretic ignorance.",
            "It's like we're going to assume that you have access to infinitely many samples of the models, so it's not like an information theory problem, but it's more like a computational problem that we're trying to address here."
        ],
        [
            "So working with large M DPS has its own challenges and there are different access models that people use.",
            "One is to access transition probabilities and rewards for any triple transition.",
            "The other is the simulator model, where you assume that there is a simulator at hand and you can simulate the outcome of taking action at some state, which states.",
            "But if you had a starting state, fine.",
            "And then maybe along the way you were generating other states.",
            "If you think about the other games or what not like, can you just like start a game from any state like even naming the States becomes a little bit difficult.",
            "So maybe you could hack the other games, but many times the simulators are not such that you can just generate a state like that.",
            "Like maybe you can sample states from some initial distribution.",
            "So this is how you can deal with a large MDP where you can't just describe everything that you know about the MDP.",
            "I mean like maybe you have equations, but that's.",
            "Not really the same thing as describing all individual transition probabilities."
        ],
        [
            "OK, so here's a little algorithm just to wet your appetite.",
            "It's called fitted value iteration and.",
            "The version of it, whether it uses actual values, is the basis of the QM like.",
            "So how about trying to understand what this algorithm does?",
            "So how does this work?",
            "So they are going to work as follows.",
            "We all know value iteration, right?",
            "So you just have the bammon operator.",
            "You keep applying the BAM operator and eventually what you get is optimal value function and you agreed it after that with respect to the value function that you got.",
            "So this is what we're trying to mimic, but we only have a simulator, so we're going to simulate the effect of the bammon operator.",
            "So how do we do that?",
            "So the iterations?",
            "Iteration are in this loop where K goes from one to K. So capital K iterations of value iterations are simulated here and the way it goes is that we are going to solve a regression problem.",
            "Where we are setting up some targets.",
            "For the regression problem.",
            "And we're trying to regress using a function space to fit at certain states.",
            "The target device.",
            "What are the target values?",
            "Well, it's the target value is what the bamble operator should give.",
            "If we use the current value function, which is we?",
            "And the update we applied it at State X.",
            "So the bammon operator basically computes expected reward plus expected value of next state.",
            "And if you have a simulator, you can just simulate many transitions and take an average instead of taking expectations.",
            "So that's what this very simple algorithm does OK.",
            "So you set up this integration problem and then there you go.",
            "So picture the way it works is that you have this space F, which is the space of functions that you can represent with your regression architecture.",
            "Maybe it's a neural network and you would apply the bammon operator, but it doesn't quite work, so you need to project it back to the function space and you do that in a noisy fashion, right?",
            "So this is going to be a noisy version and then.",
            "You project back noisy and project back and then you hope that you are making progress.",
            "It's a very simple idea how to use a simulator 2.",
            "To emulate value iteration, does."
        ],
        [
            "At work.",
            "Well, let's start with the bad news.",
            "So if you make up an algorithm like this fitted value creation.",
            "And you are iterating and iterating and iterating.",
            "You hope that is going to do something useful convergent, what not?",
            "Well, bad news is that often it doesn't.",
            "So I won't have time to explain the details of this very simple example, but in this simple example you are sure.",
            "That you could actually infinitely many samples, so the only approximation that's going on is that you are projecting to a function space, and so you have this.",
            "Take a step is abandoned operator apply a projection?",
            "Take a step is about an operator, apply a projection.",
            "So you don't suffer from the noise that's coming from the Monte Carlo simulation, and yet what you're going to find is that this nice idea just completely blows up.",
            "Alright, so very simple setting.",
            "You compute how the iterations are updating some parameter in a linear setting.",
            "Just close up.",
            "So that's one thing.",
            "Watch out for instability in reinforcement learning."
        ],
        [
            "So is it really just a theoretical thing?",
            "Well, since for a long time we know that this is not just a theory casting, so there's plenty of experiment that evidence that shows that this disaster really, really exist in the reinforcement learning from this old paper of Justin Brown and Moore.",
            "This is an example Varian agreed word and you're trying to get to some corner and you use.",
            "Linear basis functions.",
            "It's a quadratic approximation.",
            "It's like 3 parameters or whatnot, and you apply this fitted value iteration and what you see is that instead of the agents not learning to get to the corner it was supposed to get, but it's learning to get to the opposite corner.",
            "How beautiful is that?",
            "So it doesn't blow up, but it's it's doing something very interesting."
        ],
        [
            "OK, so another example of this neural networks from the same paper."
        ],
        [
            "So as a result of this.",
            "You could see a run the 90s and later on sinks like this in the papers.",
            "So Justin Boyan and Andrew more rights in light of these experiments, we conclude that the straightforward combination of VPN function approximation is not robust.",
            "So deep mind should be closed down.",
            "Alright.",
            "And Jeff Gordon samay I mean, it's these are really very some.",
            "It's truly very some like you have very simple examples where things don't work right.",
            "Like maybe we should have better algorithms than this.",
            "OK. You can push a little bit harder.",
            "Oh sorry, what happened to my beautiful equations?",
            "Yeah.",
            "It's super complicated.",
            "Right only pictures, no activations from this point down.",
            "It seems anyways, so this is really surprising.",
            "You know what?",
            "Because it's a picture I copied.",
            "So it's.",
            "Impossible.",
            "But the impossible happens.",
            "Anyways, going back to the talk, it was just.",
            "Move it around.",
            "Yep, rotated.",
            "It's much better, right?"
        ],
        [
            "So if you want to know the details of this, you definitely have to talk to me.",
            "Alright, so the point of this was that OK there is this fitted by iteration algorithm.",
            "It's not that like it never works.",
            "It works sometimes, so why not try to understand under what conditions does it work and derive a bond on how good a policy you can get after K iterations.",
            "And with joint work with me, who knows we did this and then this is not the result of it, but the bone has important terms that explain what is important to make this argument work.",
            "And the few things that are important to make this algorithm work are the following.",
            "So first of all, you have to have a very flexible approximation architecture.",
            "That means that whatever value function the bammon operator is going to throw at you, you should be able to approximate it.",
            "If you're starting from a value function that was in your function space.",
            "Or approximated pretty well.",
            "So one of the terms that's hard to see there, so this is the error and one of the terms that you see that is this.",
            "So that captures this approximation error.",
            "What?",
            "The most is not OK. Alright OK. What can you do?",
            "It's getting worse and worse.",
            "Now hey alright OK so that's one of the terms and the term that you don't see this cripes the affinity of the sampling, distribution and the distributions that the policies in use.",
            "As you're learning, you're implicitly using some policies which are the greedy policies to simplify functions that you came up with.",
            "And those in use some distributions, so the diverges between these two are going to be in the bound.",
            "So if these two things are controlled, basically you are in control.",
            "Right?",
            "So why is this important?"
        ],
        [
            "It's important because you can do very well if you're controlling these things, and that's what the Cuban is doing.",
            "Right?",
            "So what is difference compared to?",
            "What was happening before?"
        ],
        [
            "So what happened is that you was not fixed in the queue and but it was slowly changing Scott experience replay to meet better the distributions and the policies that you come up with.",
            "Right, we have.",
            "This diverges termina bond.",
            "I can predict that if you need to keep that diverges term small if you want to have a stable algorithm, that's going to deliver good performance.",
            "The other thing that's important is that you have a very flexible architecture.",
            "It's a convolutional neural networks with many many euros in it.",
            "And the.",
            "It has the right biases as well.",
            "Meaning that you have to work with this images.",
            "In this setting the other games.",
            "And because of that, you don't need that many examples.",
            "And right so these are.",
            "These are key and I say that theory kind of predict it.",
            "Of course it would be very good to see some data published and the relative importance of this individual tricks.",
            "We haven't seen that yet, but hopefully we'll see some of those, yes.",
            "The DTFF is like the worst case approximation error if you're taking one value function in F and you apply the bammon operator to it and you try to approximate the resulting function with a function from your function space.",
            "It's an app emu error because he choose the apnu norm to measure the errors.",
            "But it's worst case over those.",
            "Anne.",
            "And then many people actually criticize the result that its worst case like it's it demands that.",
            "This verse gives error should be small, but I think it's really important if the worst case error is not small, you are likely to run into trouble and then what we see is that you have this oversized architectures right now, right driving down the worst case approximation error and then plus you're dealing with Discovery ship these two 2 problems and then you can have better performance.",
            "All right so.",
            "If you push harder.",
            "It work."
        ],
        [
            "That's good, so here is my little map of planning methods, so these are the things I'm not going to talk about.",
            "I'm not going to talk about this this forward planning, which is local planning or the hybrids or actor critic policy search Peter was talking about this.",
            "Actually, all these and I will just mostly just talking about this, but there are very many interesting designs, but."
        ],
        [
            "Let me just skip to the next part, so.",
            "Right, so in the last part, I want to talk about the case when you don't have a simulator.",
            "But you have access to the revert.",
            "So no simulator, no pain well.",
            "Things are getting real.",
            "Look at that."
        ],
        [
            "Right, so this is what we call online learning.",
            "Online learning is an abused and over used term in learning theory.",
            "And when I'm talking about online learning, I'm talking about a system that's learning to achieve some goal by interacting with the real system.",
            "And when I'm talking about online learning, I tend to focus on the goal of collecting lots of lots of rewards.",
            "As you're learning.",
            "So during learning you can't afford to lose much reward, you die.",
            "So the performance metric is simply just the total reward collected, or in many cases in the literature you will see this flipped around metric, which is like the access.",
            "Like the missing reward, which is the regret which is like if I used a good policy from the beginning on.",
            "I would have collected this much reward compared to that I've collected.",
            "This match, which is less and so the gap between the tool, is my regret.",
            "So people are kind of like flipping things around for whatever historical reasons.",
            "It's kind of like a normalization thing.",
            "It's like the access risk you're talking about the access risk of hypothesis over the best hypothesis in the class.",
            "Here you're talking about the reward that you have lost.",
            "That's the regret.",
            "So you want the regret to be small, won't regret small and reward maximized.",
            "I'm not going to talk about this framework.",
            "That's called pack MDP."
        ],
        [
            "No time for that, so why should we care about this?",
            "Well before we jump into believing that we should care about this, let's investigate what is the alternative?",
            "One alternative would be just to build a simulator.",
            "So use your samples, build a model.",
            "And you use planning like in previous parts.",
            "The problem is is that sometimes the models are very, very challenging to build.",
            "So right now I'm working on on some applied project where you have to deal with fluid dynamics.",
            "It's nasty.",
            "I.",
            "And if you have some unloaded dynamics, then the unmodeled dynamics might have unpredictable effect on the performance of your policy, so you might find the best policy for your simulator.",
            "But maybe when you deploy it under system is not going to do any good for you.",
            "So that's that's a problem.",
            "And so sometimes this model base that just doesn't work, or it's too complicated.",
            "And then sometimes online learning can actually be done in an effective way you know, like on the Internet you can scale up things.",
            "And that's the opportunity.",
            "Caveat, so these are not antagonistic model based area and online learning.",
            "I mean like an online learning algorithm can't alter the user model based algorithm underneath.",
            "So what is that?"
        ],
        [
            "French here.",
            "So I want mainly focus on why is this problem hard, so recall that what we're trying to do is that trying to explore an unknown environment to collect as much reward as possible.",
            "So consider this simple setting where you are in the River and you can swim up streams to get to the big bounty.",
            "An if you just like Leslie splashing it on then going to reach the humble pie, OK?",
            "So you can model this MDP with so many States and states.",
            "Where you have the rare action, which is like when you're sweating in the water and splashing and trying to make progress.",
            "But mostly it's probably 1/2 you are staying in place and this probably 1/2 you're making progress towards your intended direction.",
            "And if you're not doing anything, then you're just going down streams, so you have two actions.",
            "OK. And so in this case, I told you that this is.",
            "This is where the.",
            "The big bounty is, but maybe there are Forks in the River and what not, and so that is the real exploration aspect to the question of their to goals OK?",
            "And the change is the following.",
            "So, uh.",
            "If you apply an learning algorithm or whatnot.",
            "You find.",
            "You know, like you make it, you make it work to some extent.",
            "Then the learning algorithm very quickly is going to discover the humble pie.",
            "Now it's going to say that it's better than in the voters, so a good policy is to get to the humble pie, right?",
            "And then maybe you know, like following code advice, you're going to add epsilon exploration.",
            "So that means that with certain probability you're going to deviate from your intended action, which is to go towards a humble pie.",
            "So if you do this.",
            "Imagine how long it's going to take for you to discover that there is this big bound waiting for you.",
            "So this is shown here.",
            "Of course, the time required as the number of state grows to reach from this state to this state is exponential.",
            "If you are doing this random exploration, randomly choosing between the two actions.",
            "So pretty big big Epsilon absolutely 0.5 OK. Running epsilon, greedy after we discover that it's good to go to humble pie.",
            "With Zero Point 5 huge exploration, it takes exponential time on the average to get to the big pie, whereas if you have an algorithm which is saying that, well, I haven't seen this state.",
            "I haven't seen it.",
            "Let's go there.",
            "Let's let's see the states that we haven't seen yet, OK?",
            "Then it takes linear time exponential less time to get to the big bonding.",
            "So the important thing is that there is an exponential gap between the behavior of these two things.",
            "Alright.",
            "So this shows, I hope in a convincing fashion.",
            "That very simple exploration schemes like epsilon greedy are.",
            "Not going to make it.",
            "Oftentimes something they could write, like maybe the environment can be nice, and then it really doesn't really matter that much.",
            "How would exploring?",
            "But if you have a more challenging environment Montezuma's Revenge, for example, well known examples from other games.",
            "The difference between a clever explanation and the not so clever expiration could be very, very human truths.",
            "Right, so just to give you a taste of what people do in exploration."
        ],
        [
            "I'll talk very quickly about bandit problems and then how you generalize everything from bandits.",
            "So by the way, you might have seen this.",
            "This is this is my.",
            "My plaque for this website that we wrote together is to latimorebanditaxe.com.",
            "So if you want to learn about bandits and exploration.",
            "That's your go to place.",
            "Right, So what are bandits?",
            "It's our problems without the state or with a single state.",
            "So simple, it's like you take an action.",
            "You got to the next step.",
            "Yeah, it's the same status you have been before.",
            "Simple, so you don't need to worry about the dynamics.",
            "But it still captures some aspect of the exploration problem.",
            "Why?",
            "Because if you take an action, you see the reward of detection, but you don't see the reward of the other actions, so you have to kind of worry about like I need to take actions that I still uncertain about.",
            "How often should we do that?",
            "I what frequency?",
            "So there's the question of exploration versus exploitation.",
            "So some more time energy contact should bandits.",
            "So that's error.",
            "Other problems with the next state is chosen at random independently of the action chosen Leader bandits.",
            "That's like when you assume that you have a contextual bandit and the reward function itself is linear in some features of state action pairs."
        ],
        [
            "So key results and stochastic bandits.",
            "So here's a graph that kind of shows one of the take home messages.",
            "That we learned from the bandit literature.",
            "Simple strategies, even for the simple bandit case like epsilon, greedy boards, Mon grips, explore, then commit.",
            "They fare to adapt to the difficulty of the problem, so you have a range of problems that's on the X axis, and these are different instances of some algorithm which is similar to epsilon greedy with different values of epsilon.",
            "So different amount of exploration.",
            "And as you have more and less water, fewer explanation.",
            "So this shows the expected regret after so many time steps.",
            "You can see that.",
            "Well, sometimes they're doing pretty well.",
            "So small value circuit, but sometimes they're doing pretty horribly and then you can see this algorithm called UCB, which is doing across the board pretty valve compared to all of these other algorithms.",
            "So we say that this UCB is successfully adapts to the instance that it needs to solve.",
            "So if you want an adaptive algorithm, then yes.",
            "The audio is adaptive algorithms that are cleverly designing about uncertainty and one of them is UCB."
        ],
        [
            "Unfortunately I'm kind of running out of time, so I'm going to just keep the slide about UCB.",
            "If you are interested in these algorithms, go to the website or talk to me.",
            "I'm around very happy to talk to you."
        ],
        [
            "About it important things 2 results that say that UCB is essentially optimal.",
            "So there are two types of results, instance dependent result.",
            "Pendant on the distribution that generates the data.",
            "So in this case Delta is going to be a data or distribution dependent constant.",
            "There is a corresponding lower bound that says that this factor cannot be reduced optical instant."
        ],
        [
            "Packers and the result says that in the worst case, since UCB is as good as it gets, new Organism can do significantly better."
        ],
        [
            "Right, so this just summarizes.",
            "So how about MDP?",
            "So what does this mean for MVP S?",
            "You can actually generalize.",
            "These are Gators, 2M DPS and people have done it.",
            "This from the paper of Yash Ordner, an hour if you have a finite state action.",
            "MVP S is number of states, a number of actions rewards are in 01, then there is an algorithm that achieves a regret for all the MVP's.",
            "Of this size, B is circle diameter of the MVP, so the diameter is the maximum of the best travel times between pairs of states.",
            "So for example, for the reverse theme problem, it was the number of states.",
            "And so this is the upper bond B * X Times Square root 80 and the lower bound says that well, as far as the dependence on the time the horizon is concerned, the result is optimal as far as dependence and number of actions concern it's optimized.",
            "Paris dependence on the other quantities concern it's not optimal.",
            "So why is this important?",
            "So this seems like a silly result.",
            "I mean, it's a finite MDP for heavens sake.",
            "Cares about finite MDP's where you have a few number of states, few number of actions.",
            "It kind of shows that important things already, right?",
            "It shows that.",
            "For example, the diameter, the travel times between the states that's going to show up in your bond.",
            "You can expect the problem to be harder if it's more difficult to get around, and more importantly, you can expect from this result you can expect that you're not going to skip very badly with the diameter.",
            "It's not exponential scaling with diameter, it just you know the inner skating, or maybe square root scaling with the diameter.",
            "Number of states shows up.",
            "Ah, that's a limit very soon because we like to entertain the possibility of dealing with MDP's with gazillion number of states, right?",
            "Well, if you don't assume any further structure, the worst case result this lower bound pass you that you're out of luck, right?",
            "So just ask you that hey, that's the boundary like this result is important.",
            "It says that you need to assume that.",
            "OK, OK. You need to you didn't you have to start to make assumptions and then you better be reasonable, right, right?"
        ],
        [
            "OK.",
            "So we have a bunch of other principled ways of exploring, but I'm kind of running out of time and we have algorithms that are building upon these ideas from amongst other from deep mind colleagues, that in practice are building algorithms that make a huge difference compared to doing exploration in a silly fashion."
        ],
        [
            "Alright.",
            "Conclusions.",
            "We started to define theory and my basic tenet is that theory and practice.",
            "Can work very well together.",
            "Any other important take home messages that already is just not supervised learning?",
            "We have all these special deals that we have to worry about.",
            "There is information or distribution mismatch computationally or problems tend to be way more challenging to do with, so that already in supervised learning.",
            "There could be some computational challenges.",
            "We talked about various problem classes, best simulation, an online and we haven't touched a lot of things.",
            "So for example mixing I never talked about that you don't have IID samples like in the real world like you follow a trajectory and there is mixing.",
            "Going on or not and how to deal with that?",
            "Even predicting how well you're doing is going to be like putting an error bar.",
            "This is what I mean like practically.",
            "It's not just a number that you put there.",
            "Remember I drew this distribution at the beginning.",
            "This is random quality.",
            "Like all these things at random, you want to know about the distribution.",
            "You need the error bar.",
            "Who's going to put that error?",
            "Can you just like use the usual error bars?",
            "Well, imagine the mixing process, which is not mixing, which is repeating, repeating, repeating the same thing.",
            "You shouldn't be using the same error bars, right?"
        ],
        [
            "So it's it's very."
        ],
        [
            "Some challenging OK.",
            "So finally I love negative results.",
            "OK. And.",
            "Finally, bashing limit my field so.",
            "There is such a thing as bad theory, like could be things could go bad in two ways.",
            "One is that proofs are wrong.",
            "We should have that.",
            "But the other thing is that we could get the modeling assumptions wrong.",
            "Which means that they don't quite fit reality, and there's always going to be really, really challenging.",
            "That's the most challenging part of working in this field of coming up is the right set of assumptions, right?",
            "Sort of modeling assumptions, but we should be attentive to this.",
            "We should be conscious about this, and we can make it better."
        ],
        [
            "Great, I think that my time is totally up and everyone really wants to get some coffee.",
            "I'm going to hanger on so if you have any questions just anytime.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, thanks for the nice introduction actual and let's get started with this.",
                    "label": 0
                },
                {
                    "sent": "Talk.",
                    "label": 0
                },
                {
                    "sent": "So you see the title.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm trying to do, but it's how I feel.",
                    "label": 0
                },
                {
                    "sent": "How many of you know what is?",
                    "label": 0
                },
                {
                    "sent": "Picture is about this picture was taken recently.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a right.",
                    "label": 0
                },
                {
                    "sent": "Like my generation, the Commodore 64 game.",
                    "label": 0
                },
                {
                    "sent": "Possible mission.",
                    "label": 0
                },
                {
                    "sent": "It's really epic.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyways, so the real title is theory of reinforcement learning and the reason I was saying it's Mission Impossible because in, in or in a half and are right now and or in 20 minutes doing all this I feel that.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to achieve something really great.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's put it that way.",
                    "label": 0
                },
                {
                    "sent": "So at anytime during the talk, if you have questions, please raise your hand and also you have to speak.",
                    "label": 0
                },
                {
                    "sent": "Loudly when you're asking question because this chairs are, you know, like musical chairs and I'm part deaf, so take those into account and then you have to shot at me.",
                    "label": 1
                },
                {
                    "sent": "OK, I will try my best but so here's the content.",
                    "label": 0
                },
                {
                    "sent": "So how many people here in this room have ever, you know, touch theory?",
                    "label": 0
                },
                {
                    "sent": "In machine learning, yeah, those are my full logs, so how many people haven't done that?",
                    "label": 0
                },
                {
                    "sent": "Come on.",
                    "label": 0
                },
                {
                    "sent": "It's fine, you are my people too.",
                    "label": 0
                },
                {
                    "sent": "Or after this talk.",
                    "label": 0
                },
                {
                    "sent": "So first I'm going to spend quite a bit of time on motivating.",
                    "label": 0
                },
                {
                    "sent": "Why we should be talking about theory?",
                    "label": 1
                },
                {
                    "sent": "Why we should care about theory and what theory can bring to the table.",
                    "label": 1
                },
                {
                    "sent": "And then we jump in the middle.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about various aspects of theory of reinforcement learning, including batch learning.",
                    "label": 0
                },
                {
                    "sent": "Using simulators is planning algorithms, and if you don't have simulators, what can you do and what you should be?",
                    "label": 0
                },
                {
                    "sent": "Looking at.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And like I really feel that I need to manage expectations here, there is a lot of stuff that I'm not going to cover.",
                    "label": 0
                },
                {
                    "sent": "I'm unable to cover given the limited time that I have, so we will only cover very very simple things.",
                    "label": 1
                },
                {
                    "sent": "But don't think that only those exist.",
                    "label": 0
                },
                {
                    "sent": "So there there are other things that exist.",
                    "label": 0
                },
                {
                    "sent": "So there I'll be around for the rest of the day and tomorrow.",
                    "label": 0
                },
                {
                    "sent": "And so if you have any questions.",
                    "label": 0
                },
                {
                    "sent": "In connection to this talk or anything as, please come and talk to me.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about very simple tasks and I'm going to try to illustrate principles that we use in theory and some some hurdles that you need to overcome.",
                    "label": 0
                },
                {
                    "sent": "I want to highlight those hurdles that make a reinforcement learning different from supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So that's the goal.",
                    "label": 0
                },
                {
                    "sent": "So let's jump into the first part, which is what is this guy talking about and why?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you should be something you should not be slinking yet.",
                    "label": 0
                },
                {
                    "sent": "OK so well then bye.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "First, let's discuss what do we mean by theory.",
                    "label": 1
                },
                {
                    "sent": "We're not going to discuss what do we mean by reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "That would be hilarious after full day and a half of 2:30 as about reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And who needs theory and how does learning theory work?",
                    "label": 1
                },
                {
                    "sent": "So in this part in particular, I'm going to brief you a little bit about statistical learning theory, which is the foundations for machine learning.",
                    "label": 0
                },
                {
                    "sent": "What people do there, but again, like right now I'm going to be presenting a tiny, tiny, tiny fraction of what, so there, so that's important to keep in mind.",
                    "label": 0
                },
                {
                    "sent": "So just for inspiration and aspiration.",
                    "label": 0
                },
                {
                    "sent": "I thought that it's good to cover some basics of classical learning theory in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, what is theory?",
                    "label": 0
                },
                {
                    "sent": "So let me ask you this question what is theory?",
                    "label": 1
                },
                {
                    "sent": "What do you say?",
                    "label": 0
                },
                {
                    "sent": "Through math, math, test theory.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, maybe it's part of it.",
                    "label": 0
                },
                {
                    "sent": "Is it necessary to do math to do theory?",
                    "label": 0
                },
                {
                    "sent": "Fruit.",
                    "label": 0
                },
                {
                    "sent": "Select it's a tool and language.",
                    "label": 0
                },
                {
                    "sent": "So what are the important ingredients of theory?",
                    "label": 0
                },
                {
                    "sent": "If we're saying that we're building it here, or we have, so we have assumptions and then what else?",
                    "label": 0
                },
                {
                    "sent": "We should have some theorems right, but more generally, if you're thinking about Sciences theory, basically what you have is that you have models and predictions that make predictions about how things work.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So that's theory.",
                    "label": 0
                },
                {
                    "sent": "You have models and you have predictions, but at the same time everyone was right.",
                    "label": 0
                },
                {
                    "sent": "Who said that we need Matt?",
                    "label": 0
                },
                {
                    "sent": "And predictions about what, right?",
                    "label": 0
                },
                {
                    "sent": "So here we are not really studying.",
                    "label": 0
                },
                {
                    "sent": "Physical process is we're not.",
                    "label": 0
                },
                {
                    "sent": "We're not really doing science here.",
                    "label": 0
                },
                {
                    "sent": "What we're doing is different.",
                    "label": 0
                },
                {
                    "sent": "We're doing mathematics.",
                    "label": 0
                },
                {
                    "sent": "We're studying.",
                    "label": 0
                },
                {
                    "sent": "You know we think are set up, things that can be computed.",
                    "label": 0
                },
                {
                    "sent": "These are conceptual things, right?",
                    "label": 0
                },
                {
                    "sent": "These are in mind.",
                    "label": 0
                },
                {
                    "sent": "We can instant ate them in a computer in some approximate fashion, but mostly these are just conceptual things.",
                    "label": 0
                },
                {
                    "sent": "And not really that much connected to the physical work, but at the end of the day, if you want to build a robot, then of course it's going to be connected to the physical world.",
                    "label": 0
                },
                {
                    "sent": "So to make the theory useful, maybe you should have some connection to it, so so that's the definition of theory for me.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so my next slide is about these two gentlemen.",
                    "label": 0
                },
                {
                    "sent": "Does anyone in the audience know who these gentlemen are?",
                    "label": 0
                },
                {
                    "sent": "Any guesses?",
                    "label": 0
                },
                {
                    "sent": "I've heard Maxwell.",
                    "label": 0
                },
                {
                    "sent": "So that's the guy on the left, right, right?",
                    "label": 0
                },
                {
                    "sent": "For you, yes.",
                    "label": 0
                },
                {
                    "sent": "And the the other guy.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "I cheated Marconi.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So the gentleman is a big.",
                    "label": 0
                },
                {
                    "sent": "This Max fell, the other guy is Marconi.",
                    "label": 0
                },
                {
                    "sent": "So what did these guys do?",
                    "label": 0
                },
                {
                    "sent": "Electromagnetism is basically the invention of Maxwell.",
                    "label": 0
                },
                {
                    "sent": "What is the invention of Marconi?",
                    "label": 1
                },
                {
                    "sent": "The radio?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So in a way, Maxwell, you know he came way earlier than Marconi.",
                    "label": 0
                },
                {
                    "sent": "He invented to solve Maxwell equations that summarize how electromagnetic waves traveled in space time.",
                    "label": 0
                },
                {
                    "sent": "And with that he created a foundation with his theory for building useful devices like the radio, right?",
                    "label": 1
                },
                {
                    "sent": "So oftentimes, this is what happens that Maxwell was not really interested in building a radio.",
                    "label": 0
                },
                {
                    "sent": "He wanted to understand some phenomena.",
                    "label": 0
                },
                {
                    "sent": "He wanted to build a theory for the sake of understanding something.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And this is often but theoreticians would do.",
                    "label": 0
                },
                {
                    "sent": "They would, you know, go about their little problems.",
                    "label": 0
                },
                {
                    "sent": "That may look like a story problems, but along the way.",
                    "label": 0
                },
                {
                    "sent": "Who knows.",
                    "label": 0
                },
                {
                    "sent": "In this case you had to wait quite a cover of decades before the discovery of the Maxwell equations lead to something ultimately so useful, like the radio.",
                    "label": 0
                },
                {
                    "sent": "But it happened.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right so I.",
                    "label": 0
                },
                {
                    "sent": "There are those of us who want to do theory for the sake of you know, seeking knowledge, an ultimate source of knowledge, but a lot of folks who say that I don't really feel that I should be doing theory.",
                    "label": 0
                },
                {
                    "sent": "Question is, should you still care?",
                    "label": 1
                },
                {
                    "sent": "Of course, if you are coming to this talk, what do you expect from this speaker?",
                    "label": 0
                },
                {
                    "sent": "I'm going to say yes.",
                    "label": 0
                },
                {
                    "sent": "So why?",
                    "label": 0
                },
                {
                    "sent": "Why should you care?",
                    "label": 0
                },
                {
                    "sent": "Well, I say that predictions and theory in an idea setting are going to help you to do the following things.",
                    "label": 1
                },
                {
                    "sent": "Maybe design algorithms design you, algorithms designed, bagger algorithms that exist today.",
                    "label": 0
                },
                {
                    "sent": "So you could use theory for that.",
                    "label": 0
                },
                {
                    "sent": "If you have some algorithm, you can ask questions about how does it behave.",
                    "label": 0
                },
                {
                    "sent": "When does it function?",
                    "label": 0
                },
                {
                    "sent": "When does it work?",
                    "label": 0
                },
                {
                    "sent": "And you can try to study this in an experimental setting that you should, and that's all fine, but to ultimately understand what are the boundaries of the scope of an algorithm, well, it's hard to imagine doing that without here.",
                    "label": 1
                },
                {
                    "sent": "Or if you want more quantified information about, for example, you have an uncertain situation and you want to quantify uncertainty, then theory can help you to do that.",
                    "label": 1
                },
                {
                    "sent": "Lastly, but at least if you want to identify new challenges, sorry.",
                    "label": 0
                },
                {
                    "sent": "Final challenges.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's awesome theory.",
                    "label": 0
                },
                {
                    "sent": "So I'm not sure that icon is everyone that they should care about.",
                    "label": 0
                },
                {
                    "sent": "I hope that I did, but if it's not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then let's look at this.",
                    "label": 0
                },
                {
                    "sent": "Right, so we know that shit happens.",
                    "label": 0
                },
                {
                    "sent": "So why is this important?",
                    "label": 0
                },
                {
                    "sent": "So when you are trying to tweak your neural network and running those experiments, they're night and it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "How do you figure out why it doesn't work?",
                    "label": 0
                },
                {
                    "sent": "Well, if you have a model in place that is able to describe when things are expected to work.",
                    "label": 0
                },
                {
                    "sent": "Then you can look at deviations from those idealistic situations.",
                    "label": 0
                },
                {
                    "sent": "And find out what is causing your actual experiment not to run successfully, and I think that we don't execration we can say that.",
                    "label": 0
                },
                {
                    "sent": "Like in this fear the fast these days I think there is a very very tiny little fraction of.",
                    "label": 0
                },
                {
                    "sent": "Experiments that lead to published results and there will be a lot of sweat and work that goes into triangles, things and then seeing them fair.",
                    "label": 0
                },
                {
                    "sent": "And if you want to like my.",
                    "label": 0
                },
                {
                    "sent": "My mission is to try to convince you that if you think a little bit about.",
                    "label": 0
                },
                {
                    "sent": "Then maybe you will have some shortcuts there to prevent some failures.",
                    "label": 0
                },
                {
                    "sent": "Or to recognize the source of the failures and then prevent future failures.",
                    "label": 0
                },
                {
                    "sent": "Alright, but the truth?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that.",
                    "label": 0
                },
                {
                    "sent": "Practice are not antagonistic at all there together liking and young and even in the case of Maxwell Maxwell didn't just invent the electromagnetic waves and their equations out of the scenar.",
                    "label": 0
                },
                {
                    "sent": "There were lots of experimental evidence that there is something that needs to be explained right and then eventually that beautiful theory lead to future things.",
                    "label": 0
                },
                {
                    "sent": "So that's how.",
                    "label": 0
                },
                {
                    "sent": "In practice, are working hands in hands.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so with that I finish my.",
                    "label": 0
                },
                {
                    "sent": "You know Thurmont for.",
                    "label": 0
                },
                {
                    "sent": "Praising theory, so let's jump into the middle of things.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk a little bit about statistical learning theory, as I promised before.",
                    "label": 1
                },
                {
                    "sent": "So what are the ingredients of this theory so that distributions, samples, learning algorithms, predictors, and loss functions?",
                    "label": 1
                },
                {
                    "sent": "So how many people are you are familiar with this notation and terminology.",
                    "label": 0
                },
                {
                    "sent": "Let's raise hands.",
                    "label": 0
                },
                {
                    "sent": "OK, well that's good.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, I think that it's good to have some recordings of these ideas and and let's do this in the context stuff binary classification, which is the simplest.",
                    "label": 0
                },
                {
                    "sent": "Possible.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning task.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to instantiate all these things in the context of binary classification.",
                    "label": 0
                },
                {
                    "sent": "So the distributions.",
                    "label": 0
                },
                {
                    "sent": "What are they?",
                    "label": 0
                },
                {
                    "sent": "So there will be some input space.",
                    "label": 0
                },
                {
                    "sent": "I usually do note that by Capital X an it could be like Rd.",
                    "label": 0
                },
                {
                    "sent": "So D dimensional Euclidean space and there is an output space.",
                    "label": 0
                },
                {
                    "sent": "And since we're talking about binary classification, the output space is going to have two elements conveniently denoted by zero and one.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to have a class of distributions over.",
                    "label": 0
                },
                {
                    "sent": "The cross product.",
                    "label": 0
                },
                {
                    "sent": "Of the input space and output space.",
                    "label": 0
                },
                {
                    "sent": "So M is my notation for all the distributions over that set, right?",
                    "label": 0
                },
                {
                    "sent": "So what's in the argument of the set?",
                    "label": 0
                },
                {
                    "sent": "That's where these samples are going to leave, and so if I say P, the subset of order distributions, if I take any element of P, that means that from that distribution I can generate samples.",
                    "label": 0
                },
                {
                    "sent": "I can generate samples in an IID fashion so.",
                    "label": 0
                },
                {
                    "sent": "Do you guys in the back see my writing or you just OK?",
                    "label": 0
                },
                {
                    "sent": "Alright, so we can generate these samples.",
                    "label": 0
                },
                {
                    "sent": "So I sample.",
                    "label": 0
                },
                {
                    "sent": "Is a list of topless.",
                    "label": 0
                },
                {
                    "sent": "XI anvi.",
                    "label": 0
                },
                {
                    "sent": "I leaving this SpaceX cross Y.",
                    "label": 0
                },
                {
                    "sent": "And in classical learning theory, we assume that.",
                    "label": 0
                },
                {
                    "sent": "XI&YI identically distributed according to some distribution in our family of distributions in learning theory.",
                    "label": 0
                },
                {
                    "sent": "In one part it's called Agnostic learning theory.",
                    "label": 0
                },
                {
                    "sent": "You make no restriction on P. You're going to equate P. This class of distributions.",
                    "label": 0
                },
                {
                    "sent": "With the set of all possible distributions.",
                    "label": 0
                },
                {
                    "sent": "So there is no further restrictions in other parts of learning theory.",
                    "label": 0
                },
                {
                    "sent": "You start to make some restrictions and the results are going to differ because of that.",
                    "label": 0
                },
                {
                    "sent": "So anyways, back to here.",
                    "label": 0
                },
                {
                    "sent": "So XINVI at random variable that are sampled from this distribution P. And we're going to assume independence between them.",
                    "label": 0
                },
                {
                    "sent": "So when you have this situation, then we say that this XY sequence is an IID sequence.",
                    "label": 0
                },
                {
                    "sent": "Independent, I think identically distributed sequence of random variables.",
                    "label": 0
                },
                {
                    "sent": "So this is how you think about your data.",
                    "label": 0
                },
                {
                    "sent": "So you're thinking about that my data SN was some hole.",
                    "label": 0
                },
                {
                    "sent": "Born by someone throwing dice.",
                    "label": 0
                },
                {
                    "sent": "Repeatedly and times and the dice had so many different ways to so many different size that I, as a result of the dice rules I got these guys and acts.",
                    "label": 0
                },
                {
                    "sent": "I described my input and why I described what I want to predict.",
                    "label": 0
                },
                {
                    "sent": "Given the input.",
                    "label": 0
                },
                {
                    "sent": "So the next ingredient in learning.",
                    "label": 0
                },
                {
                    "sent": "Is predictors.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "The the predictors are just Maps.",
                    "label": 0
                },
                {
                    "sent": "We also call them hypothesis from the input.",
                    "label": 0
                },
                {
                    "sent": "To the label space, so to Y.",
                    "label": 0
                },
                {
                    "sent": "Why is this?",
                    "label": 0
                },
                {
                    "sent": "Why space is just.",
                    "label": 0
                },
                {
                    "sent": "Does 801.",
                    "label": 0
                },
                {
                    "sent": "So these are functions that map imports to either zero or one.",
                    "label": 0
                },
                {
                    "sent": "We are trying to predict the labels.",
                    "label": 0
                },
                {
                    "sent": "And for what?",
                    "label": 0
                },
                {
                    "sent": "Well, not for the data that is in our data set, but for future data.",
                    "label": 0
                },
                {
                    "sent": "OK. How are we going to evaluate?",
                    "label": 0
                },
                {
                    "sent": "How good is a predictor?",
                    "label": 0
                },
                {
                    "sent": "So if you pick a predictor like H?",
                    "label": 0
                },
                {
                    "sent": "We're going to assign a loss to it.",
                    "label": 0
                },
                {
                    "sent": "And when we are assigning loss, we have many, many choices.",
                    "label": 0
                },
                {
                    "sent": "The simplest possible choice to assign a loss.",
                    "label": 0
                },
                {
                    "sent": "Is the following so the loss function is going to map?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "A hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "An element of the XY cross product space too.",
                    "label": 0
                },
                {
                    "sent": "Generally it it Maps to non negative numbers and the way it works is that you plug in the.",
                    "label": 0
                },
                {
                    "sent": "The age.",
                    "label": 0
                },
                {
                    "sent": "And then you plug in an X&Y into the loss function.",
                    "label": 0
                },
                {
                    "sent": "And the simplest possible loss function is called a binary loss.",
                    "label": 0
                },
                {
                    "sent": "Is defined as a loss of 1 if H of X is not equal to Y and otherwise it's 0.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So we dressed up this obsolete in this abstract terminology.",
                    "label": 0
                },
                {
                    "sent": "With this specific case, this is binary classification.",
                    "label": 0
                },
                {
                    "sent": "Oh, I didn't say about the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Was a learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's not truly an algorithm, so that's a misnomer.",
                    "label": 0
                },
                {
                    "sent": "It's a learning strategy or what not is going to map.",
                    "label": 0
                },
                {
                    "sent": "A sample too.",
                    "label": 0
                },
                {
                    "sent": "A hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Right, so it Maps the SpaceX cross Y to the power of N space.",
                    "label": 0
                },
                {
                    "sent": "To the space of all possible hypothesis and all the space of all possible hypothesis, is in this case could be all the Maps age that Maps X to Y.",
                    "label": 0
                },
                {
                    "sent": "So we don't care about.",
                    "label": 0
                },
                {
                    "sent": "How to compute this?",
                    "label": 0
                },
                {
                    "sent": "We just say, well, the learning algorithm at the end of the day is just going to deliver a predictor.",
                    "label": 0
                },
                {
                    "sent": "If you have a predictor.",
                    "label": 0
                },
                {
                    "sent": "And you have this distribution.",
                    "label": 0
                },
                {
                    "sent": "Then you can ask.",
                    "label": 0
                },
                {
                    "sent": "OK, how good is this predictor?",
                    "label": 0
                },
                {
                    "sent": "So that's the risk.",
                    "label": 0
                },
                {
                    "sent": "So there is a risk of the predictor.",
                    "label": 0
                },
                {
                    "sent": "It's just the expected loss of the predictor and the distribution piece.",
                    "label": 0
                },
                {
                    "sent": "So let's denote this by RH&P, which is the probability that age of X.",
                    "label": 0
                },
                {
                    "sent": "Is not equal to Y in the case of binary classification where X&Y are jointly distributed according to P. So now a learning algorithm delivers based on the sample, which is random hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Hypothesis is going to be random itself.",
                    "label": 0
                },
                {
                    "sent": "So if I plug in the output, I guess I better write here if I plug.",
                    "label": 0
                },
                {
                    "sent": "In the output of the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which is a F SN into the risk function which is going to deliver me around the number that depends on.",
                    "label": 0
                },
                {
                    "sent": "The randomness of the sample right and then you can say oh OK, so that's the risk of the hypothesis produced by the algorithm.",
                    "label": 0
                },
                {
                    "sent": "In general, you're interested in our Gothams.",
                    "label": 0
                },
                {
                    "sent": "That produce hypothesis low risk?",
                    "label": 0
                },
                {
                    "sent": "The hypothesis is the lowest risk is called a base hypothesis for whatever reason.",
                    "label": 0
                },
                {
                    "sent": "And you want to be close to that.",
                    "label": 0
                },
                {
                    "sent": "So this is a random number.",
                    "label": 0
                },
                {
                    "sent": "You can just say that, well, it's five.",
                    "label": 0
                },
                {
                    "sent": "Well, it depends on what the input is SN is.",
                    "label": 0
                },
                {
                    "sent": "It's going to have a distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So important thing is that.",
                    "label": 0
                },
                {
                    "sent": "This is going to have a distribution, so it's not negative because we said that the loss and all negative.",
                    "label": 0
                },
                {
                    "sent": "Is distributed in some way.",
                    "label": 0
                },
                {
                    "sent": "So again, characterize it by its mean by its variance, and then when we're talking about oh, how confident are you that the risk of your.",
                    "label": 0
                },
                {
                    "sent": "Of the hypothesis produced by our learning algorithm is above.",
                    "label": 0
                },
                {
                    "sent": "I don't know below 0.1 we were talking about this protein mass that's there.",
                    "label": 0
                },
                {
                    "sent": "Which depends on unknown P which you don't know.",
                    "label": 0
                },
                {
                    "sent": "So even the problem of knowing how well you're doing becomes kind of challenging in this framework, so you need to have tools to deal with that.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of the setting where we are starting from the learning theory setting.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how does learning theory work?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "There are many questions asked in learning theory.",
                    "label": 0
                },
                {
                    "sent": "One of the questions is belongs to what I call a priori analysis.",
                    "label": 1
                },
                {
                    "sent": "How well a learning algorithm will perform on new data and future data that tries to answer questions about this distribution.",
                    "label": 1
                },
                {
                    "sent": "Without seeing P without seeing as it's a no prior prediction, like if you have P coming from this family and you're learning algorithms like that, this is what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "It's a priori prediction.",
                    "label": 0
                },
                {
                    "sent": "And the poster reran that Easyserve prediction is after seeing the data.",
                    "label": 0
                },
                {
                    "sent": "Still not knowing P. What can you conclude about the distribution of the risk?",
                    "label": 1
                },
                {
                    "sent": "So there is a subject of a posteriori analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, so learning theory studies all these.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seen a priori analysis you have for example, results for questions like can we compete with the best hypothesis from a given set of hypothesis is.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For example, you could restrict here the set of hypothesis in some Montreal away.",
                    "label": 0
                },
                {
                    "sent": "And you say that I just want to compete with the best hyper these over there without putting any restrictions and P that's called agnostic crack framework vapnik's learning theory.",
                    "label": 0
                },
                {
                    "sent": "And the problem is, can we match the best possible loss, assuming the data generating distribution belongs to a known family, so that's what mostly people do in parametric and nonparametric statistical analysis.",
                    "label": 1
                },
                {
                    "sent": "Other problems include does argutum.",
                    "label": 0
                },
                {
                    "sent": "Some algorithm achieves something?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to skip over this slide and just skip to two fundamental results of statistical learning theory, which are crucially important to know about, at least know because it's just, you know, foundation for a lot of the thinking that goes into learning theory.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to present two designs, the fundamental theorem of statistical learning theory and the result about computation complexity.",
                    "label": 1
                },
                {
                    "sent": "So notice that we haven't talked about computation complexity of the algorithm yet, but there is a question question of whether you can achieve a small risk.",
                    "label": 0
                },
                {
                    "sent": "In an efficient way, efficiently computable way, it's not enough to say that yes, you can achieve a small risk, because maybe it's not computable.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so the fundamental theorem of statistical learning theory answers the question for problem number one.",
                    "label": 1
                },
                {
                    "sent": "And it goes like this.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "If you take any hypothesis class H, which is a subset of this big hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "There is a unique number.",
                    "label": 0
                },
                {
                    "sent": "It's called the VC dimension of H and I'm not going to define it.",
                    "label": 0
                },
                {
                    "sent": "Most likely you have heard about it anyways, that characterizes this class.",
                    "label": 0
                },
                {
                    "sent": "And if you want to compete.",
                    "label": 1
                },
                {
                    "sent": "With the loss of the best hypothesis up to an accuracy of epsilon, so your access risk shouldn't be more than epsilon of the best possible risk over the hypothesis space age, then you are going to need at least the VC dimension of H divided by epsilon squared samples.",
                    "label": 0
                },
                {
                    "sent": "And it's possible to to achieve this risk with this many samples, so this up to log factors.",
                    "label": 1
                },
                {
                    "sent": "This is tight.",
                    "label": 0
                },
                {
                    "sent": "So this is the inherent difficulty of competing with the best hypothesis in a hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "In this scale learning theory setting.",
                    "label": 0
                },
                {
                    "sent": "So that's one of the foundational resigns.",
                    "label": 0
                },
                {
                    "sent": "It's cooler.",
                    "label": 1
                },
                {
                    "sent": "I think it's cool.",
                    "label": 0
                },
                {
                    "sent": "It's a pure information theory is that and they are getting that achieves.",
                    "label": 0
                },
                {
                    "sent": "This is super simple.",
                    "label": 0
                },
                {
                    "sent": "It's called empirical risk memoization.",
                    "label": 0
                },
                {
                    "sent": "It's basically a compute the empirical loss and assume that you can minimize it over the hypothesis space of your choice.",
                    "label": 0
                },
                {
                    "sent": "And you can prove that this is this.",
                    "label": 0
                },
                {
                    "sent": "Is this algorithm algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's not, it's like Arcmin algorithm is going to achieve this guarantee.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the same time, things are not rosy.",
                    "label": 0
                },
                {
                    "sent": "Not that rosy altogether.",
                    "label": 0
                },
                {
                    "sent": "So the next result concerns the computation complexity of dealing with some very simple hyperdata spaces.",
                    "label": 0
                },
                {
                    "sent": "So the linear classification in linear classification, the input space has a vector space.",
                    "label": 0
                },
                {
                    "sent": "Structure, it's like Cardi.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's already.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And the hypothesis space is, you know, on one side of a linear hyperplane.",
                    "label": 0
                },
                {
                    "sent": "You have a plus one the other side you have a zero label, so very very simple hypothesis and you're trying to compete with the best hyperplane.",
                    "label": 0
                },
                {
                    "sent": "And this result says that.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, in this framework, when you have no idea about the underlying distribution, no restrictions on the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "This seems to be computationally hard.",
                    "label": 0
                },
                {
                    "sent": "The exact result says that unless RNP is equal to RP.",
                    "label": 0
                },
                {
                    "sent": "RP is this class, which is maybe a little bit bigger than P. It's randomized time polynomial.",
                    "label": 0
                },
                {
                    "sent": "So unless I'm MP equals RP, which most of the people don't believe, it's true, linear classifiers cannot be learned in polynomial time in this framework.",
                    "label": 1
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It will be harder.",
                    "label": 0
                },
                {
                    "sent": "Right, so if linear is this hard, then lonelier could be much harder.",
                    "label": 0
                },
                {
                    "sent": "Why would it be easier?",
                    "label": 0
                },
                {
                    "sent": "OK, so if it's a subset, right?",
                    "label": 0
                },
                {
                    "sent": "I don't know actually people study this questions on a case by case basis.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately we don't really have a generic good understanding of the computation complexity of this task.",
                    "label": 0
                },
                {
                    "sent": "But I don't think that we should have high hopes about that suddenly because.",
                    "label": 0
                },
                {
                    "sent": "You're canceling only their classes.",
                    "label": 0
                },
                {
                    "sent": "It's going to be easier, but at the same time there are some interesting phenomenas sometimes.",
                    "label": 0
                },
                {
                    "sent": "For example, if you allow yourself a little bit of lag.",
                    "label": 0
                },
                {
                    "sent": "Like a bigger class sometimes actually could have.",
                    "label": 0
                },
                {
                    "sent": "If you're choosing a bigger class of functions, then there are two things that are happening.",
                    "label": 0
                },
                {
                    "sent": "The smallest risk becomes smaller.",
                    "label": 0
                },
                {
                    "sent": "So it might be harder to get to there to that point, but at the same time, maybe somehow the search space is changed in some miraculous fashion, but in general we know that.",
                    "label": 0
                },
                {
                    "sent": "Like we have harness results for training neural networks.",
                    "label": 0
                },
                {
                    "sent": "If you're really interested in that, like I can talk offline about that and also about whether those results are relevant or not, because that's also question right?",
                    "label": 0
                },
                {
                    "sent": "So this hardness results always use its worst case thing.",
                    "label": 0
                },
                {
                    "sent": "They make up something right?",
                    "label": 0
                },
                {
                    "sent": "So I think this is also cool.",
                    "label": 0
                },
                {
                    "sent": "Why is it cool?",
                    "label": 0
                },
                {
                    "sent": "It's a negative result, shouldn't be said.",
                    "label": 0
                },
                {
                    "sent": "Anime, yeah, you can be sad about it, but it's a fact you have to live with it.",
                    "label": 0
                },
                {
                    "sent": "It's cool to know a fact right?",
                    "label": 0
                },
                {
                    "sent": "And you know that.",
                    "label": 0
                },
                {
                    "sent": "Well, if this is how things are here.",
                    "label": 0
                },
                {
                    "sent": "But maybe we have to change something if we want to deal with real stuff.",
                    "label": 0
                },
                {
                    "sent": "So I think that negative results are absolutely fantastic, and that's like the unique contribution of theory to machine learning, it's.",
                    "label": 0
                },
                {
                    "sent": "There is nothing better than that.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like you're trying to to know what's possible and what's not possible, and you're trying to push the boundaries from inside and from outside.",
                    "label": 0
                },
                {
                    "sent": "Both face.",
                    "label": 0
                },
                {
                    "sent": "You want to figure out whether there is the thing that is possible to do, right, yeah.",
                    "label": 0
                },
                {
                    "sent": "Or would you mean by therapy?",
                    "label": 0
                },
                {
                    "sent": "Learn because earlier you had a simple complexity result, right?",
                    "label": 0
                },
                {
                    "sent": "Polynomial time, yes.",
                    "label": 0
                },
                {
                    "sent": "Epsilon regret you would need right?",
                    "label": 0
                },
                {
                    "sent": "The computation time cannot be polynomial, because then you would be serving some hard,, tutorial problems.",
                    "label": 0
                },
                {
                    "sent": "So there is a reduction from a computer problem.",
                    "label": 0
                },
                {
                    "sent": "To minimizing risk up to an epsilon accuracy.",
                    "label": 0
                },
                {
                    "sent": "If you could do that in time which is polynomial in what in one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "The VC dimension of H, so you can't enumerate all the hypothesis and basically this.",
                    "label": 0
                },
                {
                    "sent": "Now you can't do it.",
                    "label": 0
                },
                {
                    "sent": "OK. John.",
                    "label": 0
                },
                {
                    "sent": "The end is not.",
                    "label": 0
                },
                {
                    "sent": "One over epsilon to the N is not polynomial.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can do things like that.",
                    "label": 0
                },
                {
                    "sent": "Just takes too much time and is the number of samples.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, other questions.",
                    "label": 0
                },
                {
                    "sent": "Alright, I'm doing very badly this time.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's skip.",
                    "label": 0
                },
                {
                    "sent": "So batch learning finally getting to reinforcement.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Running so batch learning was mentioned already in Choice tutorial.",
                    "label": 0
                },
                {
                    "sent": "It is this problem where someone was collecting data for you.",
                    "label": 0
                },
                {
                    "sent": "And you try to learn a policy based on the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to put this into a framework that is the closest to the supervised learning framework.",
                    "label": 0
                },
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "The way it works is that you have this.",
                    "label": 0
                },
                {
                    "sent": "XDA TV TRT topple.",
                    "label": 0
                },
                {
                    "sent": "So this is estate that is sampled from this distribution U.",
                    "label": 0
                },
                {
                    "sent": "This is an action that sampled from some policy of your choice.",
                    "label": 0
                },
                {
                    "sent": "It's like a Markovian policy, so it can be stochastic, or it could be deterministic.",
                    "label": 0
                },
                {
                    "sent": "So given the state.",
                    "label": 0
                },
                {
                    "sent": "Gives you an action, maybe randomly.",
                    "label": 0
                },
                {
                    "sent": "And then there is a next state that is sampled from the property kernel that describes the NDP underlying.",
                    "label": 0
                },
                {
                    "sent": "So I'm using an MDP framework here.",
                    "label": 0
                },
                {
                    "sent": "So why is the next state?",
                    "label": 0
                },
                {
                    "sent": "And for simplicity, and to avoid further complications, is the nicest case.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that the data is IID.",
                    "label": 1
                },
                {
                    "sent": "OK, yes I'm confused where it comes from source.",
                    "label": 0
                },
                {
                    "sent": "You're specifying that there's one fixed policy Pi that generated all of this data, right, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'm just imagining a very simple scenario where someone is running a policy to generate data.",
                    "label": 0
                },
                {
                    "sent": "So if you have observational studies, for example, this is kind of what's going on, right?",
                    "label": 0
                },
                {
                    "sent": "Like someone is generating the data, they are running their processes.",
                    "label": 0
                },
                {
                    "sent": "You may or may not know the data generating policy.",
                    "label": 0
                },
                {
                    "sent": "You are recording everything.",
                    "label": 0
                },
                {
                    "sent": "Basically you are recording that.",
                    "label": 0
                },
                {
                    "sent": "Well there was this state and then there was this action.",
                    "label": 0
                },
                {
                    "sent": "There was a next state, by the way.",
                    "label": 0
                },
                {
                    "sent": "States are evil, but let's move on.",
                    "label": 0
                },
                {
                    "sent": "For simplicity, let's assume that you have access to this.",
                    "label": 0
                },
                {
                    "sent": "I'm like it's like an impossible thing like you don't have access to states, but like for simplicity and like you will see where I'm going with this.",
                    "label": 0
                },
                {
                    "sent": "So then you will see that it was fine, but I always want to scream when someone states states I do say states often, yes.",
                    "label": 0
                },
                {
                    "sent": "Spadard yeah it's not a trace.",
                    "label": 0
                },
                {
                    "sent": "We get talk about traces later, but it's like there is a distribution.",
                    "label": 0
                },
                {
                    "sent": "It's like the supervised learning setting, right?",
                    "label": 0
                },
                {
                    "sent": "Like it's just, but we have this little little transitions.",
                    "label": 0
                },
                {
                    "sent": "Right, so someone samples and initial state and then this is 1 transition that is made and you record the transition.",
                    "label": 0
                },
                {
                    "sent": "So you learn about the dynamics, but it's little pieces of the dynamics that you can learn about and you learn about the reward as fast so you see the reward and you have a fixed horizon H. OK so I want to simplify things.",
                    "label": 0
                },
                {
                    "sent": "No discounting it just fixed horizon.",
                    "label": 0
                },
                {
                    "sent": "OK. And you have a class of policies, and since I want to mimic what people do in supervised learning, I just want to find.",
                    "label": 1
                },
                {
                    "sent": "And epsilon optimal policy in this policy class pie.",
                    "label": 1
                },
                {
                    "sent": "And I'm asking the question OK, what is it?",
                    "label": 0
                },
                {
                    "sent": "Give me a sample complexity result for this problem, right?",
                    "label": 0
                },
                {
                    "sent": "It's like in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "If you have your data generated from some unknown distribution P, the only thing that matters is the VC dimension of the class.",
                    "label": 0
                },
                {
                    "sent": "So in this case the class is the policy class we want to compete with the best policy in this class, given the distribution that we don't know this probability kernel and the reward.",
                    "label": 0
                },
                {
                    "sent": "And can we do it?",
                    "label": 0
                },
                {
                    "sent": "It's a very simple question.",
                    "label": 0
                },
                {
                    "sent": "We should start there.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Right, so let's see how it goes.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or so.",
                    "label": 0
                },
                {
                    "sent": "So recall that the value of a policy is what like.",
                    "label": 1
                },
                {
                    "sent": "Basically you have to sum up the rewards and take expectations, right?",
                    "label": 0
                },
                {
                    "sent": "Like if you had a policy at all and you go for a H steps.",
                    "label": 0
                },
                {
                    "sent": "So in this case you go for H steps and you take the expectation.",
                    "label": 0
                },
                {
                    "sent": "So for any state you're going to have some value.",
                    "label": 0
                },
                {
                    "sent": "Here I'm going to assume that you are initial state is samples from this distribution.",
                    "label": 0
                },
                {
                    "sent": "Mu for simplicity.",
                    "label": 0
                },
                {
                    "sent": "And I just want the policy that when the initial state is sampled from, you gives you the maximum reward.",
                    "label": 0
                },
                {
                    "sent": "Expected reward after 8 steps.",
                    "label": 0
                },
                {
                    "sent": "And I find this an absolute to my policy too.",
                    "label": 0
                },
                {
                    "sent": "OK, so one easy corollary.",
                    "label": 1
                },
                {
                    "sent": "If you take the horizon to be 0 so there is no trajectories.",
                    "label": 0
                },
                {
                    "sent": "This just supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Right, you just want to maximize immediate reward.",
                    "label": 1
                },
                {
                    "sent": "Right just one step, you don't care about dynamics prediction future what not just immediate reward maximization.",
                    "label": 0
                },
                {
                    "sent": "This kind of like one particular case of supervised learning.",
                    "label": 0
                },
                {
                    "sent": "It's called cost sensitive classification.",
                    "label": 1
                },
                {
                    "sent": "It's like not really the binary classification, but it's pretty close and you can imagine how you can get sample complexity results in this case as well, and there will be very similar to the sample complexity result that we had.",
                    "label": 0
                },
                {
                    "sent": "Fine.",
                    "label": 0
                },
                {
                    "sent": "So let's move on to something more challenging.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Alright, so OK just spelling out the obvious.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's take.",
                    "label": 0
                },
                {
                    "sent": "The case and the horizon is 2.",
                    "label": 0
                },
                {
                    "sent": "So you basically make 2 steps and I'm going to consider a very simple policy space.",
                    "label": 0
                },
                {
                    "sent": "And a very simple example that is going to prove this this 1st result.",
                    "label": 0
                },
                {
                    "sent": "So the state space is going to be 01.",
                    "label": 0
                },
                {
                    "sent": "And the policies are like this.",
                    "label": 0
                },
                {
                    "sent": "You choose a threshold.",
                    "label": 0
                },
                {
                    "sent": "An on the right of the threshold you take action one.",
                    "label": 0
                },
                {
                    "sent": "An on the left of the threshold you choose action minus.",
                    "label": 0
                },
                {
                    "sent": "So you cannot change the threshold.",
                    "label": 0
                },
                {
                    "sent": "That's your parameter that theater as a policy class.",
                    "label": 0
                },
                {
                    "sent": "And now take the following MVP.",
                    "label": 0
                },
                {
                    "sent": "If you start at the state zero, you stay at stay there.",
                    "label": 0
                },
                {
                    "sent": "If you start at state one, you stay at state one.",
                    "label": 0
                },
                {
                    "sent": "If you start anywhere but 0.50 and one, you transition in a single step to 0.5.",
                    "label": 0
                },
                {
                    "sent": "OK, and if you are a 0.5.",
                    "label": 0
                },
                {
                    "sent": "If you take plus one, you get to 1.",
                    "label": 0
                },
                {
                    "sent": "If you take the action minus one, you get to there just clear very very simple.",
                    "label": 0
                },
                {
                    "sent": "So let mu this distribution be.",
                    "label": 0
                },
                {
                    "sent": "The uniform distribution of an O one.",
                    "label": 0
                },
                {
                    "sent": "And let's imagine that we generate this data.",
                    "label": 0
                },
                {
                    "sent": "That we have.",
                    "label": 0
                },
                {
                    "sent": "From the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "We collect an samples.",
                    "label": 0
                },
                {
                    "sent": "And then we need to learn what to do.",
                    "label": 0
                },
                {
                    "sent": "The catch is that there are two MVP's I'm saying there not more than two MVP's in one of the MVP's.",
                    "label": 0
                },
                {
                    "sent": "If you get to 0, if you get to stay at zero, you're going to get a + 5 are there.",
                    "label": 0
                },
                {
                    "sent": "And the minus five art at one and then the other MVP.",
                    "label": 0
                },
                {
                    "sent": "It's the other way round.",
                    "label": 0
                },
                {
                    "sent": "The good state is 1 and the best set is 0.",
                    "label": 0
                },
                {
                    "sent": "OK. And you're generating and generating and generating the samples.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, you're not going to hit zero or one.",
                    "label": 0
                },
                {
                    "sent": "So you will never see whether you're going to get the reward of 01.",
                    "label": 0
                },
                {
                    "sent": "And you will also not hit 0.5 ever in the sample as a probability of 0.",
                    "label": 1
                },
                {
                    "sent": "So you can generate infinitely many samples.",
                    "label": 0
                },
                {
                    "sent": "And you're not going to learn to distinguish these two MVP's, and in one of the MDP's you should be going left in the other.",
                    "label": 0
                },
                {
                    "sent": "MVP should be going right.",
                    "label": 0
                },
                {
                    "sent": "And the policies have a very simple structure as well.",
                    "label": 0
                },
                {
                    "sent": "So there is no counterpart of the fundamental theorem of statistical learning theory in batch reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Just not possible.",
                    "label": 0
                },
                {
                    "sent": "Alright, so in this generality.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "You can't say much.",
                    "label": 0
                },
                {
                    "sent": "This was just meant as a very simple example illustrating how things very quickly ask late in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And again you have a negative result.",
                    "label": 0
                },
                {
                    "sent": "What do you do?",
                    "label": 0
                },
                {
                    "sent": "Go home.",
                    "label": 0
                },
                {
                    "sent": "Close shop 1.",
                    "label": 0
                },
                {
                    "sent": "Haha, well we should think about what the problem is.",
                    "label": 0
                },
                {
                    "sent": "What is causing this?",
                    "label": 0
                },
                {
                    "sent": "How are we going to reconcile this with all the good experiences that we have about training for spent learning?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "First of all, what is the problem?",
                    "label": 0
                },
                {
                    "sent": "The problem is that the critical decision is at 0.5.",
                    "label": 0
                },
                {
                    "sent": "But in the data 0.5 doesn't appear ever or is there and one doesn't appear ever right.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Can we make it so that like we are sure that all the critical states similar to 0.5 there and one appear in the data without any further?",
                    "label": 0
                },
                {
                    "sent": "This I mean like.",
                    "label": 0
                },
                {
                    "sent": "It is a very strange assumption to to make that.",
                    "label": 0
                },
                {
                    "sent": "Is sure that this is happening.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless people try.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "We can try to make assumptions like we're going to have better sampling distributions.",
                    "label": 0
                },
                {
                    "sent": "But at the same time, it's kind of like really hard to push this through.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions?",
                    "label": 0
                },
                {
                    "sent": "What's the example clear?",
                    "label": 0
                },
                {
                    "sent": "MU is the distribution that you're generating the initial states from.",
                    "label": 0
                },
                {
                    "sent": "So you choose anything, issue states from the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "And then you follow the transition kernel for one step.",
                    "label": 0
                },
                {
                    "sent": "So if you happen to be, you know like you're going to be somewhere here.",
                    "label": 0
                },
                {
                    "sent": "So all that you're going to see is that all the transitions go to 0.5.",
                    "label": 0
                },
                {
                    "sent": "That's all what you have in the data.",
                    "label": 0
                },
                {
                    "sent": "There is nothing else in the data.",
                    "label": 0
                },
                {
                    "sent": "So you will never learn about the effect of the actions at 0.5 and you will never learn about the rewards in one.",
                    "label": 0
                },
                {
                    "sent": "Right, so that would be a good suggestion.",
                    "label": 0
                },
                {
                    "sent": "So why not consider at least 2 steps?",
                    "label": 0
                },
                {
                    "sent": "I can.",
                    "label": 0
                },
                {
                    "sent": "Tweak things in such a way that that doesn't help.",
                    "label": 0
                },
                {
                    "sent": "So you could draw a little tree and then you're embedding the intermediate states in such a way that you're never going to hit those, and those would be key to have.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you said that you don't see .5 in the data set right?",
                    "label": 0
                },
                {
                    "sent": "Because you are generating the data uniformly at random.",
                    "label": 0
                },
                {
                    "sent": "And then you only see it as the next state.",
                    "label": 0
                },
                {
                    "sent": "You don't see it as a starting state.",
                    "label": 0
                },
                {
                    "sent": "But like if you so the dynamics of the environment you're not following the transitions, you're only following the transitions for one step.",
                    "label": 0
                },
                {
                    "sent": "Oh what?",
                    "label": 0
                },
                {
                    "sent": "It's because that's how I set it up.",
                    "label": 0
                },
                {
                    "sent": "If you're following transitions for multiple steps.",
                    "label": 0
                },
                {
                    "sent": "Then the problem is a little bit more dedicated, but it has the same.",
                    "label": 0
                },
                {
                    "sent": "Nature.",
                    "label": 0
                },
                {
                    "sent": "The problem is that the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "Might want to, you know.",
                    "label": 0
                },
                {
                    "sent": "Gola certain way.",
                    "label": 0
                },
                {
                    "sent": "And all the data that you are seeing from the policy that you were using to generate the data go some other way.",
                    "label": 0
                },
                {
                    "sent": "An if you're not seeing this, keyhole states which are really important for you know to how to act.",
                    "label": 0
                },
                {
                    "sent": "Then the situation is doomed.",
                    "label": 0
                },
                {
                    "sent": "I think this is very important to understand because this is one of the challenges that we face all the time in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "That we are learning with changing distributions as opposed to.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning that the train and the test distributions are mostly the same, right?",
                    "label": 0
                },
                {
                    "sent": "So when I said that, let's compute the risk of the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "It was this under the same distribution that was used to generate the data.",
                    "label": 0
                },
                {
                    "sent": "It was not a different distribution in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "When you switch to a policy.",
                    "label": 0
                },
                {
                    "sent": "Your switch to new distribution.",
                    "label": 0
                },
                {
                    "sent": "There's no control over that.",
                    "label": 0
                },
                {
                    "sent": "That's much higher share.",
                    "label": 0
                },
                {
                    "sent": "It's a harder learning problem because of this inherently harder.",
                    "label": 0
                },
                {
                    "sent": "OK, one more question.",
                    "label": 0
                },
                {
                    "sent": "And then we need to move on.",
                    "label": 0
                },
                {
                    "sent": "No, I mean like you pick a distribution, I pick a problem.",
                    "label": 0
                },
                {
                    "sent": "We can play this game right.",
                    "label": 0
                },
                {
                    "sent": "And then it won't work like you can always imagine that I can trick you by choosing specific MVP, because we didn't put any restriction on the MVP, so I'm going to like be very tricky with that anyways.",
                    "label": 0
                },
                {
                    "sent": "So let's move on.",
                    "label": 0
                },
                {
                    "sent": "We can talk offline later on so.",
                    "label": 0
                },
                {
                    "sent": "What so is this?",
                    "label": 0
                },
                {
                    "sent": "The end of the world?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, no, I mean like you can push it.",
                    "label": 0
                },
                {
                    "sent": "I'm going to skip this slide and come back to it later, but we have we do have some general recipes for positive results as fast so people are pushing the algorithms and you can do an inverse energy.",
                    "label": 0
                },
                {
                    "sent": "This you look at the algorithm and you say that hey, under what conditions can I prove that these are active does work?",
                    "label": 0
                },
                {
                    "sent": "And then you can work with the conditions, and that's helpful for debugging things when things don't work, so it's not going to be a general theory like the fundamental theory of learning theory of learning theory, but you can still analyze specific algorithms, and I will show a result of such analysts later, OK?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next topic I wanted to talk a little bit about is when you have a simulator, because this kind of addresses this this issue that we are discovering that.",
                    "label": 0
                },
                {
                    "sent": "Distributions mismatch exists in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "It's inherent to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "The training distribution in the bad setting, and the distribution is used by the policies are just different, and that's a problem.",
                    "label": 0
                },
                {
                    "sent": "But if you have a simulator.",
                    "label": 1
                },
                {
                    "sent": "And it's a whole different new word.",
                    "label": 0
                },
                {
                    "sent": "You can simulate trajectories as you will.",
                    "label": 0
                },
                {
                    "sent": "You can generate new data.",
                    "label": 0
                },
                {
                    "sent": "Under your policy.",
                    "label": 0
                },
                {
                    "sent": "So this leads to what we call a planning problem, and so in the planning problem, the way I define it, you mentioned that you have a huge MDP.",
                    "label": 0
                },
                {
                    "sent": "It's so big that you can't hope to inherit all the states in it.",
                    "label": 0
                },
                {
                    "sent": "In fact, you will have problem naming the States in the MDP, and your goal is to compute.",
                    "label": 0
                },
                {
                    "sent": "Still may be a good policy within a restricted policy class or over all an action of the good policy.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So right?",
                    "label": 0
                },
                {
                    "sent": "So which one is easier?",
                    "label": 1
                },
                {
                    "sent": "So that's a very good question.",
                    "label": 0
                },
                {
                    "sent": "Is it easier to compute a good action at a given initial state, or is it easier to come up with a good policy from let's say, same policy class?",
                    "label": 1
                },
                {
                    "sent": "Well got fitting should be that it should be easier to get a good action because if you have a good policy then you just take the action that it recommends.",
                    "label": 0
                },
                {
                    "sent": "So the second problem is definitely not harder, but it's actually easier.",
                    "label": 0
                },
                {
                    "sent": "It's a good question.",
                    "label": 0
                },
                {
                    "sent": "There are cases when it's actually easier, so that's one general less.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about the details of that, but just keep that in mind that if you don't have to solve some problem like getting a globally optimal policy, that's optimal everywhere.",
                    "label": 0
                },
                {
                    "sent": "Because you only care about this state.",
                    "label": 0
                },
                {
                    "sent": "Then you can focus your computational resources on that state and do more and be better at that state.",
                    "label": 0
                },
                {
                    "sent": "And then when the next state comes along, we do some more computation.",
                    "label": 0
                },
                {
                    "sent": "Right, you can spread out the computation in time that way, and it's there is a separation between these two results in the sense that the first problem is strictly hard.",
                    "label": 0
                },
                {
                    "sent": "In the second one.",
                    "label": 0
                },
                {
                    "sent": "OK, important thing here is that there is no information theoretic ignorance.",
                    "label": 0
                },
                {
                    "sent": "It's like we're going to assume that you have access to infinitely many samples of the models, so it's not like an information theory problem, but it's more like a computational problem that we're trying to address here.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So working with large M DPS has its own challenges and there are different access models that people use.",
                    "label": 1
                },
                {
                    "sent": "One is to access transition probabilities and rewards for any triple transition.",
                    "label": 1
                },
                {
                    "sent": "The other is the simulator model, where you assume that there is a simulator at hand and you can simulate the outcome of taking action at some state, which states.",
                    "label": 0
                },
                {
                    "sent": "But if you had a starting state, fine.",
                    "label": 0
                },
                {
                    "sent": "And then maybe along the way you were generating other states.",
                    "label": 0
                },
                {
                    "sent": "If you think about the other games or what not like, can you just like start a game from any state like even naming the States becomes a little bit difficult.",
                    "label": 1
                },
                {
                    "sent": "So maybe you could hack the other games, but many times the simulators are not such that you can just generate a state like that.",
                    "label": 0
                },
                {
                    "sent": "Like maybe you can sample states from some initial distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is how you can deal with a large MDP where you can't just describe everything that you know about the MDP.",
                    "label": 0
                },
                {
                    "sent": "I mean like maybe you have equations, but that's.",
                    "label": 0
                },
                {
                    "sent": "Not really the same thing as describing all individual transition probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's a little algorithm just to wet your appetite.",
                    "label": 0
                },
                {
                    "sent": "It's called fitted value iteration and.",
                    "label": 0
                },
                {
                    "sent": "The version of it, whether it uses actual values, is the basis of the QM like.",
                    "label": 0
                },
                {
                    "sent": "So how about trying to understand what this algorithm does?",
                    "label": 0
                },
                {
                    "sent": "So how does this work?",
                    "label": 0
                },
                {
                    "sent": "So they are going to work as follows.",
                    "label": 0
                },
                {
                    "sent": "We all know value iteration, right?",
                    "label": 0
                },
                {
                    "sent": "So you just have the bammon operator.",
                    "label": 0
                },
                {
                    "sent": "You keep applying the BAM operator and eventually what you get is optimal value function and you agreed it after that with respect to the value function that you got.",
                    "label": 0
                },
                {
                    "sent": "So this is what we're trying to mimic, but we only have a simulator, so we're going to simulate the effect of the bammon operator.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "So the iterations?",
                    "label": 0
                },
                {
                    "sent": "Iteration are in this loop where K goes from one to K. So capital K iterations of value iterations are simulated here and the way it goes is that we are going to solve a regression problem.",
                    "label": 0
                },
                {
                    "sent": "Where we are setting up some targets.",
                    "label": 0
                },
                {
                    "sent": "For the regression problem.",
                    "label": 0
                },
                {
                    "sent": "And we're trying to regress using a function space to fit at certain states.",
                    "label": 0
                },
                {
                    "sent": "The target device.",
                    "label": 0
                },
                {
                    "sent": "What are the target values?",
                    "label": 0
                },
                {
                    "sent": "Well, it's the target value is what the bamble operator should give.",
                    "label": 0
                },
                {
                    "sent": "If we use the current value function, which is we?",
                    "label": 0
                },
                {
                    "sent": "And the update we applied it at State X.",
                    "label": 0
                },
                {
                    "sent": "So the bammon operator basically computes expected reward plus expected value of next state.",
                    "label": 0
                },
                {
                    "sent": "And if you have a simulator, you can just simulate many transitions and take an average instead of taking expectations.",
                    "label": 0
                },
                {
                    "sent": "So that's what this very simple algorithm does OK.",
                    "label": 0
                },
                {
                    "sent": "So you set up this integration problem and then there you go.",
                    "label": 0
                },
                {
                    "sent": "So picture the way it works is that you have this space F, which is the space of functions that you can represent with your regression architecture.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a neural network and you would apply the bammon operator, but it doesn't quite work, so you need to project it back to the function space and you do that in a noisy fashion, right?",
                    "label": 0
                },
                {
                    "sent": "So this is going to be a noisy version and then.",
                    "label": 0
                },
                {
                    "sent": "You project back noisy and project back and then you hope that you are making progress.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple idea how to use a simulator 2.",
                    "label": 0
                },
                {
                    "sent": "To emulate value iteration, does.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At work.",
                    "label": 0
                },
                {
                    "sent": "Well, let's start with the bad news.",
                    "label": 0
                },
                {
                    "sent": "So if you make up an algorithm like this fitted value creation.",
                    "label": 0
                },
                {
                    "sent": "And you are iterating and iterating and iterating.",
                    "label": 0
                },
                {
                    "sent": "You hope that is going to do something useful convergent, what not?",
                    "label": 0
                },
                {
                    "sent": "Well, bad news is that often it doesn't.",
                    "label": 0
                },
                {
                    "sent": "So I won't have time to explain the details of this very simple example, but in this simple example you are sure.",
                    "label": 0
                },
                {
                    "sent": "That you could actually infinitely many samples, so the only approximation that's going on is that you are projecting to a function space, and so you have this.",
                    "label": 0
                },
                {
                    "sent": "Take a step is abandoned operator apply a projection?",
                    "label": 0
                },
                {
                    "sent": "Take a step is about an operator, apply a projection.",
                    "label": 0
                },
                {
                    "sent": "So you don't suffer from the noise that's coming from the Monte Carlo simulation, and yet what you're going to find is that this nice idea just completely blows up.",
                    "label": 0
                },
                {
                    "sent": "Alright, so very simple setting.",
                    "label": 0
                },
                {
                    "sent": "You compute how the iterations are updating some parameter in a linear setting.",
                    "label": 0
                },
                {
                    "sent": "Just close up.",
                    "label": 0
                },
                {
                    "sent": "So that's one thing.",
                    "label": 0
                },
                {
                    "sent": "Watch out for instability in reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So is it really just a theoretical thing?",
                    "label": 0
                },
                {
                    "sent": "Well, since for a long time we know that this is not just a theory casting, so there's plenty of experiment that evidence that shows that this disaster really, really exist in the reinforcement learning from this old paper of Justin Brown and Moore.",
                    "label": 0
                },
                {
                    "sent": "This is an example Varian agreed word and you're trying to get to some corner and you use.",
                    "label": 0
                },
                {
                    "sent": "Linear basis functions.",
                    "label": 0
                },
                {
                    "sent": "It's a quadratic approximation.",
                    "label": 0
                },
                {
                    "sent": "It's like 3 parameters or whatnot, and you apply this fitted value iteration and what you see is that instead of the agents not learning to get to the corner it was supposed to get, but it's learning to get to the opposite corner.",
                    "label": 0
                },
                {
                    "sent": "How beautiful is that?",
                    "label": 0
                },
                {
                    "sent": "So it doesn't blow up, but it's it's doing something very interesting.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so another example of this neural networks from the same paper.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as a result of this.",
                    "label": 0
                },
                {
                    "sent": "You could see a run the 90s and later on sinks like this in the papers.",
                    "label": 0
                },
                {
                    "sent": "So Justin Boyan and Andrew more rights in light of these experiments, we conclude that the straightforward combination of VPN function approximation is not robust.",
                    "label": 0
                },
                {
                    "sent": "So deep mind should be closed down.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "And Jeff Gordon samay I mean, it's these are really very some.",
                    "label": 0
                },
                {
                    "sent": "It's truly very some like you have very simple examples where things don't work right.",
                    "label": 0
                },
                {
                    "sent": "Like maybe we should have better algorithms than this.",
                    "label": 0
                },
                {
                    "sent": "OK. You can push a little bit harder.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, what happened to my beautiful equations?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It's super complicated.",
                    "label": 0
                },
                {
                    "sent": "Right only pictures, no activations from this point down.",
                    "label": 0
                },
                {
                    "sent": "It seems anyways, so this is really surprising.",
                    "label": 0
                },
                {
                    "sent": "You know what?",
                    "label": 0
                },
                {
                    "sent": "Because it's a picture I copied.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "Impossible.",
                    "label": 0
                },
                {
                    "sent": "But the impossible happens.",
                    "label": 0
                },
                {
                    "sent": "Anyways, going back to the talk, it was just.",
                    "label": 0
                },
                {
                    "sent": "Move it around.",
                    "label": 0
                },
                {
                    "sent": "Yep, rotated.",
                    "label": 0
                },
                {
                    "sent": "It's much better, right?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you want to know the details of this, you definitely have to talk to me.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the point of this was that OK there is this fitted by iteration algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's not that like it never works.",
                    "label": 0
                },
                {
                    "sent": "It works sometimes, so why not try to understand under what conditions does it work and derive a bond on how good a policy you can get after K iterations.",
                    "label": 0
                },
                {
                    "sent": "And with joint work with me, who knows we did this and then this is not the result of it, but the bone has important terms that explain what is important to make this argument work.",
                    "label": 0
                },
                {
                    "sent": "And the few things that are important to make this algorithm work are the following.",
                    "label": 0
                },
                {
                    "sent": "So first of all, you have to have a very flexible approximation architecture.",
                    "label": 0
                },
                {
                    "sent": "That means that whatever value function the bammon operator is going to throw at you, you should be able to approximate it.",
                    "label": 0
                },
                {
                    "sent": "If you're starting from a value function that was in your function space.",
                    "label": 0
                },
                {
                    "sent": "Or approximated pretty well.",
                    "label": 0
                },
                {
                    "sent": "So one of the terms that's hard to see there, so this is the error and one of the terms that you see that is this.",
                    "label": 0
                },
                {
                    "sent": "So that captures this approximation error.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "The most is not OK. Alright OK. What can you do?",
                    "label": 0
                },
                {
                    "sent": "It's getting worse and worse.",
                    "label": 0
                },
                {
                    "sent": "Now hey alright OK so that's one of the terms and the term that you don't see this cripes the affinity of the sampling, distribution and the distributions that the policies in use.",
                    "label": 0
                },
                {
                    "sent": "As you're learning, you're implicitly using some policies which are the greedy policies to simplify functions that you came up with.",
                    "label": 0
                },
                {
                    "sent": "And those in use some distributions, so the diverges between these two are going to be in the bound.",
                    "label": 0
                },
                {
                    "sent": "So if these two things are controlled, basically you are in control.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So why is this important?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's important because you can do very well if you're controlling these things, and that's what the Cuban is doing.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So what is difference compared to?",
                    "label": 0
                },
                {
                    "sent": "What was happening before?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what happened is that you was not fixed in the queue and but it was slowly changing Scott experience replay to meet better the distributions and the policies that you come up with.",
                    "label": 0
                },
                {
                    "sent": "Right, we have.",
                    "label": 0
                },
                {
                    "sent": "This diverges termina bond.",
                    "label": 0
                },
                {
                    "sent": "I can predict that if you need to keep that diverges term small if you want to have a stable algorithm, that's going to deliver good performance.",
                    "label": 0
                },
                {
                    "sent": "The other thing that's important is that you have a very flexible architecture.",
                    "label": 0
                },
                {
                    "sent": "It's a convolutional neural networks with many many euros in it.",
                    "label": 1
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "It has the right biases as well.",
                    "label": 0
                },
                {
                    "sent": "Meaning that you have to work with this images.",
                    "label": 0
                },
                {
                    "sent": "In this setting the other games.",
                    "label": 0
                },
                {
                    "sent": "And because of that, you don't need that many examples.",
                    "label": 0
                },
                {
                    "sent": "And right so these are.",
                    "label": 0
                },
                {
                    "sent": "These are key and I say that theory kind of predict it.",
                    "label": 0
                },
                {
                    "sent": "Of course it would be very good to see some data published and the relative importance of this individual tricks.",
                    "label": 1
                },
                {
                    "sent": "We haven't seen that yet, but hopefully we'll see some of those, yes.",
                    "label": 0
                },
                {
                    "sent": "The DTFF is like the worst case approximation error if you're taking one value function in F and you apply the bammon operator to it and you try to approximate the resulting function with a function from your function space.",
                    "label": 0
                },
                {
                    "sent": "It's an app emu error because he choose the apnu norm to measure the errors.",
                    "label": 0
                },
                {
                    "sent": "But it's worst case over those.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And then many people actually criticize the result that its worst case like it's it demands that.",
                    "label": 0
                },
                {
                    "sent": "This verse gives error should be small, but I think it's really important if the worst case error is not small, you are likely to run into trouble and then what we see is that you have this oversized architectures right now, right driving down the worst case approximation error and then plus you're dealing with Discovery ship these two 2 problems and then you can have better performance.",
                    "label": 0
                },
                {
                    "sent": "All right so.",
                    "label": 0
                },
                {
                    "sent": "If you push harder.",
                    "label": 0
                },
                {
                    "sent": "It work.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's good, so here is my little map of planning methods, so these are the things I'm not going to talk about.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to talk about this this forward planning, which is local planning or the hybrids or actor critic policy search Peter was talking about this.",
                    "label": 0
                },
                {
                    "sent": "Actually, all these and I will just mostly just talking about this, but there are very many interesting designs, but.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me just skip to the next part, so.",
                    "label": 0
                },
                {
                    "sent": "Right, so in the last part, I want to talk about the case when you don't have a simulator.",
                    "label": 0
                },
                {
                    "sent": "But you have access to the revert.",
                    "label": 0
                },
                {
                    "sent": "So no simulator, no pain well.",
                    "label": 1
                },
                {
                    "sent": "Things are getting real.",
                    "label": 0
                },
                {
                    "sent": "Look at that.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so this is what we call online learning.",
                    "label": 1
                },
                {
                    "sent": "Online learning is an abused and over used term in learning theory.",
                    "label": 0
                },
                {
                    "sent": "And when I'm talking about online learning, I'm talking about a system that's learning to achieve some goal by interacting with the real system.",
                    "label": 0
                },
                {
                    "sent": "And when I'm talking about online learning, I tend to focus on the goal of collecting lots of lots of rewards.",
                    "label": 0
                },
                {
                    "sent": "As you're learning.",
                    "label": 0
                },
                {
                    "sent": "So during learning you can't afford to lose much reward, you die.",
                    "label": 0
                },
                {
                    "sent": "So the performance metric is simply just the total reward collected, or in many cases in the literature you will see this flipped around metric, which is like the access.",
                    "label": 1
                },
                {
                    "sent": "Like the missing reward, which is the regret which is like if I used a good policy from the beginning on.",
                    "label": 0
                },
                {
                    "sent": "I would have collected this much reward compared to that I've collected.",
                    "label": 0
                },
                {
                    "sent": "This match, which is less and so the gap between the tool, is my regret.",
                    "label": 0
                },
                {
                    "sent": "So people are kind of like flipping things around for whatever historical reasons.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like a normalization thing.",
                    "label": 0
                },
                {
                    "sent": "It's like the access risk you're talking about the access risk of hypothesis over the best hypothesis in the class.",
                    "label": 0
                },
                {
                    "sent": "Here you're talking about the reward that you have lost.",
                    "label": 0
                },
                {
                    "sent": "That's the regret.",
                    "label": 0
                },
                {
                    "sent": "So you want the regret to be small, won't regret small and reward maximized.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about this framework.",
                    "label": 0
                },
                {
                    "sent": "That's called pack MDP.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No time for that, so why should we care about this?",
                    "label": 1
                },
                {
                    "sent": "Well before we jump into believing that we should care about this, let's investigate what is the alternative?",
                    "label": 0
                },
                {
                    "sent": "One alternative would be just to build a simulator.",
                    "label": 1
                },
                {
                    "sent": "So use your samples, build a model.",
                    "label": 1
                },
                {
                    "sent": "And you use planning like in previous parts.",
                    "label": 0
                },
                {
                    "sent": "The problem is is that sometimes the models are very, very challenging to build.",
                    "label": 0
                },
                {
                    "sent": "So right now I'm working on on some applied project where you have to deal with fluid dynamics.",
                    "label": 0
                },
                {
                    "sent": "It's nasty.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "And if you have some unloaded dynamics, then the unmodeled dynamics might have unpredictable effect on the performance of your policy, so you might find the best policy for your simulator.",
                    "label": 0
                },
                {
                    "sent": "But maybe when you deploy it under system is not going to do any good for you.",
                    "label": 1
                },
                {
                    "sent": "So that's that's a problem.",
                    "label": 0
                },
                {
                    "sent": "And so sometimes this model base that just doesn't work, or it's too complicated.",
                    "label": 0
                },
                {
                    "sent": "And then sometimes online learning can actually be done in an effective way you know, like on the Internet you can scale up things.",
                    "label": 1
                },
                {
                    "sent": "And that's the opportunity.",
                    "label": 0
                },
                {
                    "sent": "Caveat, so these are not antagonistic model based area and online learning.",
                    "label": 0
                },
                {
                    "sent": "I mean like an online learning algorithm can't alter the user model based algorithm underneath.",
                    "label": 0
                },
                {
                    "sent": "So what is that?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "French here.",
                    "label": 0
                },
                {
                    "sent": "So I want mainly focus on why is this problem hard, so recall that what we're trying to do is that trying to explore an unknown environment to collect as much reward as possible.",
                    "label": 0
                },
                {
                    "sent": "So consider this simple setting where you are in the River and you can swim up streams to get to the big bounty.",
                    "label": 0
                },
                {
                    "sent": "An if you just like Leslie splashing it on then going to reach the humble pie, OK?",
                    "label": 0
                },
                {
                    "sent": "So you can model this MDP with so many States and states.",
                    "label": 0
                },
                {
                    "sent": "Where you have the rare action, which is like when you're sweating in the water and splashing and trying to make progress.",
                    "label": 0
                },
                {
                    "sent": "But mostly it's probably 1/2 you are staying in place and this probably 1/2 you're making progress towards your intended direction.",
                    "label": 0
                },
                {
                    "sent": "And if you're not doing anything, then you're just going down streams, so you have two actions.",
                    "label": 0
                },
                {
                    "sent": "OK. And so in this case, I told you that this is.",
                    "label": 0
                },
                {
                    "sent": "This is where the.",
                    "label": 0
                },
                {
                    "sent": "The big bounty is, but maybe there are Forks in the River and what not, and so that is the real exploration aspect to the question of their to goals OK?",
                    "label": 0
                },
                {
                    "sent": "And the change is the following.",
                    "label": 0
                },
                {
                    "sent": "So, uh.",
                    "label": 0
                },
                {
                    "sent": "If you apply an learning algorithm or whatnot.",
                    "label": 0
                },
                {
                    "sent": "You find.",
                    "label": 0
                },
                {
                    "sent": "You know, like you make it, you make it work to some extent.",
                    "label": 0
                },
                {
                    "sent": "Then the learning algorithm very quickly is going to discover the humble pie.",
                    "label": 0
                },
                {
                    "sent": "Now it's going to say that it's better than in the voters, so a good policy is to get to the humble pie, right?",
                    "label": 0
                },
                {
                    "sent": "And then maybe you know, like following code advice, you're going to add epsilon exploration.",
                    "label": 0
                },
                {
                    "sent": "So that means that with certain probability you're going to deviate from your intended action, which is to go towards a humble pie.",
                    "label": 0
                },
                {
                    "sent": "So if you do this.",
                    "label": 0
                },
                {
                    "sent": "Imagine how long it's going to take for you to discover that there is this big bound waiting for you.",
                    "label": 0
                },
                {
                    "sent": "So this is shown here.",
                    "label": 0
                },
                {
                    "sent": "Of course, the time required as the number of state grows to reach from this state to this state is exponential.",
                    "label": 0
                },
                {
                    "sent": "If you are doing this random exploration, randomly choosing between the two actions.",
                    "label": 0
                },
                {
                    "sent": "So pretty big big Epsilon absolutely 0.5 OK. Running epsilon, greedy after we discover that it's good to go to humble pie.",
                    "label": 0
                },
                {
                    "sent": "With Zero Point 5 huge exploration, it takes exponential time on the average to get to the big pie, whereas if you have an algorithm which is saying that, well, I haven't seen this state.",
                    "label": 0
                },
                {
                    "sent": "I haven't seen it.",
                    "label": 0
                },
                {
                    "sent": "Let's go there.",
                    "label": 0
                },
                {
                    "sent": "Let's let's see the states that we haven't seen yet, OK?",
                    "label": 0
                },
                {
                    "sent": "Then it takes linear time exponential less time to get to the big bonding.",
                    "label": 0
                },
                {
                    "sent": "So the important thing is that there is an exponential gap between the behavior of these two things.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So this shows, I hope in a convincing fashion.",
                    "label": 0
                },
                {
                    "sent": "That very simple exploration schemes like epsilon greedy are.",
                    "label": 0
                },
                {
                    "sent": "Not going to make it.",
                    "label": 0
                },
                {
                    "sent": "Oftentimes something they could write, like maybe the environment can be nice, and then it really doesn't really matter that much.",
                    "label": 0
                },
                {
                    "sent": "How would exploring?",
                    "label": 0
                },
                {
                    "sent": "But if you have a more challenging environment Montezuma's Revenge, for example, well known examples from other games.",
                    "label": 0
                },
                {
                    "sent": "The difference between a clever explanation and the not so clever expiration could be very, very human truths.",
                    "label": 0
                },
                {
                    "sent": "Right, so just to give you a taste of what people do in exploration.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll talk very quickly about bandit problems and then how you generalize everything from bandits.",
                    "label": 0
                },
                {
                    "sent": "So by the way, you might have seen this.",
                    "label": 0
                },
                {
                    "sent": "This is this is my.",
                    "label": 0
                },
                {
                    "sent": "My plaque for this website that we wrote together is to latimorebanditaxe.com.",
                    "label": 0
                },
                {
                    "sent": "So if you want to learn about bandits and exploration.",
                    "label": 0
                },
                {
                    "sent": "That's your go to place.",
                    "label": 0
                },
                {
                    "sent": "Right, So what are bandits?",
                    "label": 0
                },
                {
                    "sent": "It's our problems without the state or with a single state.",
                    "label": 1
                },
                {
                    "sent": "So simple, it's like you take an action.",
                    "label": 0
                },
                {
                    "sent": "You got to the next step.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's the same status you have been before.",
                    "label": 0
                },
                {
                    "sent": "Simple, so you don't need to worry about the dynamics.",
                    "label": 0
                },
                {
                    "sent": "But it still captures some aspect of the exploration problem.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because if you take an action, you see the reward of detection, but you don't see the reward of the other actions, so you have to kind of worry about like I need to take actions that I still uncertain about.",
                    "label": 0
                },
                {
                    "sent": "How often should we do that?",
                    "label": 0
                },
                {
                    "sent": "I what frequency?",
                    "label": 0
                },
                {
                    "sent": "So there's the question of exploration versus exploitation.",
                    "label": 0
                },
                {
                    "sent": "So some more time energy contact should bandits.",
                    "label": 0
                },
                {
                    "sent": "So that's error.",
                    "label": 0
                },
                {
                    "sent": "Other problems with the next state is chosen at random independently of the action chosen Leader bandits.",
                    "label": 1
                },
                {
                    "sent": "That's like when you assume that you have a contextual bandit and the reward function itself is linear in some features of state action pairs.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So key results and stochastic bandits.",
                    "label": 1
                },
                {
                    "sent": "So here's a graph that kind of shows one of the take home messages.",
                    "label": 0
                },
                {
                    "sent": "That we learned from the bandit literature.",
                    "label": 1
                },
                {
                    "sent": "Simple strategies, even for the simple bandit case like epsilon, greedy boards, Mon grips, explore, then commit.",
                    "label": 0
                },
                {
                    "sent": "They fare to adapt to the difficulty of the problem, so you have a range of problems that's on the X axis, and these are different instances of some algorithm which is similar to epsilon greedy with different values of epsilon.",
                    "label": 0
                },
                {
                    "sent": "So different amount of exploration.",
                    "label": 0
                },
                {
                    "sent": "And as you have more and less water, fewer explanation.",
                    "label": 0
                },
                {
                    "sent": "So this shows the expected regret after so many time steps.",
                    "label": 0
                },
                {
                    "sent": "You can see that.",
                    "label": 0
                },
                {
                    "sent": "Well, sometimes they're doing pretty well.",
                    "label": 0
                },
                {
                    "sent": "So small value circuit, but sometimes they're doing pretty horribly and then you can see this algorithm called UCB, which is doing across the board pretty valve compared to all of these other algorithms.",
                    "label": 1
                },
                {
                    "sent": "So we say that this UCB is successfully adapts to the instance that it needs to solve.",
                    "label": 0
                },
                {
                    "sent": "So if you want an adaptive algorithm, then yes.",
                    "label": 0
                },
                {
                    "sent": "The audio is adaptive algorithms that are cleverly designing about uncertainty and one of them is UCB.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unfortunately I'm kind of running out of time, so I'm going to just keep the slide about UCB.",
                    "label": 0
                },
                {
                    "sent": "If you are interested in these algorithms, go to the website or talk to me.",
                    "label": 0
                },
                {
                    "sent": "I'm around very happy to talk to you.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About it important things 2 results that say that UCB is essentially optimal.",
                    "label": 1
                },
                {
                    "sent": "So there are two types of results, instance dependent result.",
                    "label": 1
                },
                {
                    "sent": "Pendant on the distribution that generates the data.",
                    "label": 0
                },
                {
                    "sent": "So in this case Delta is going to be a data or distribution dependent constant.",
                    "label": 0
                },
                {
                    "sent": "There is a corresponding lower bound that says that this factor cannot be reduced optical instant.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Packers and the result says that in the worst case, since UCB is as good as it gets, new Organism can do significantly better.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so this just summarizes.",
                    "label": 0
                },
                {
                    "sent": "So how about MDP?",
                    "label": 0
                },
                {
                    "sent": "So what does this mean for MVP S?",
                    "label": 0
                },
                {
                    "sent": "You can actually generalize.",
                    "label": 0
                },
                {
                    "sent": "These are Gators, 2M DPS and people have done it.",
                    "label": 0
                },
                {
                    "sent": "This from the paper of Yash Ordner, an hour if you have a finite state action.",
                    "label": 0
                },
                {
                    "sent": "MVP S is number of states, a number of actions rewards are in 01, then there is an algorithm that achieves a regret for all the MVP's.",
                    "label": 0
                },
                {
                    "sent": "Of this size, B is circle diameter of the MVP, so the diameter is the maximum of the best travel times between pairs of states.",
                    "label": 0
                },
                {
                    "sent": "So for example, for the reverse theme problem, it was the number of states.",
                    "label": 0
                },
                {
                    "sent": "And so this is the upper bond B * X Times Square root 80 and the lower bound says that well, as far as the dependence on the time the horizon is concerned, the result is optimal as far as dependence and number of actions concern it's optimized.",
                    "label": 0
                },
                {
                    "sent": "Paris dependence on the other quantities concern it's not optimal.",
                    "label": 0
                },
                {
                    "sent": "So why is this important?",
                    "label": 0
                },
                {
                    "sent": "So this seems like a silly result.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a finite MDP for heavens sake.",
                    "label": 0
                },
                {
                    "sent": "Cares about finite MDP's where you have a few number of states, few number of actions.",
                    "label": 0
                },
                {
                    "sent": "It kind of shows that important things already, right?",
                    "label": 0
                },
                {
                    "sent": "It shows that.",
                    "label": 0
                },
                {
                    "sent": "For example, the diameter, the travel times between the states that's going to show up in your bond.",
                    "label": 0
                },
                {
                    "sent": "You can expect the problem to be harder if it's more difficult to get around, and more importantly, you can expect from this result you can expect that you're not going to skip very badly with the diameter.",
                    "label": 0
                },
                {
                    "sent": "It's not exponential scaling with diameter, it just you know the inner skating, or maybe square root scaling with the diameter.",
                    "label": 0
                },
                {
                    "sent": "Number of states shows up.",
                    "label": 0
                },
                {
                    "sent": "Ah, that's a limit very soon because we like to entertain the possibility of dealing with MDP's with gazillion number of states, right?",
                    "label": 0
                },
                {
                    "sent": "Well, if you don't assume any further structure, the worst case result this lower bound pass you that you're out of luck, right?",
                    "label": 0
                },
                {
                    "sent": "So just ask you that hey, that's the boundary like this result is important.",
                    "label": 0
                },
                {
                    "sent": "It says that you need to assume that.",
                    "label": 0
                },
                {
                    "sent": "OK, OK. You need to you didn't you have to start to make assumptions and then you better be reasonable, right, right?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we have a bunch of other principled ways of exploring, but I'm kind of running out of time and we have algorithms that are building upon these ideas from amongst other from deep mind colleagues, that in practice are building algorithms that make a huge difference compared to doing exploration in a silly fashion.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Conclusions.",
                    "label": 0
                },
                {
                    "sent": "We started to define theory and my basic tenet is that theory and practice.",
                    "label": 0
                },
                {
                    "sent": "Can work very well together.",
                    "label": 0
                },
                {
                    "sent": "Any other important take home messages that already is just not supervised learning?",
                    "label": 0
                },
                {
                    "sent": "We have all these special deals that we have to worry about.",
                    "label": 0
                },
                {
                    "sent": "There is information or distribution mismatch computationally or problems tend to be way more challenging to do with, so that already in supervised learning.",
                    "label": 1
                },
                {
                    "sent": "There could be some computational challenges.",
                    "label": 1
                },
                {
                    "sent": "We talked about various problem classes, best simulation, an online and we haven't touched a lot of things.",
                    "label": 0
                },
                {
                    "sent": "So for example mixing I never talked about that you don't have IID samples like in the real world like you follow a trajectory and there is mixing.",
                    "label": 0
                },
                {
                    "sent": "Going on or not and how to deal with that?",
                    "label": 0
                },
                {
                    "sent": "Even predicting how well you're doing is going to be like putting an error bar.",
                    "label": 0
                },
                {
                    "sent": "This is what I mean like practically.",
                    "label": 0
                },
                {
                    "sent": "It's not just a number that you put there.",
                    "label": 0
                },
                {
                    "sent": "Remember I drew this distribution at the beginning.",
                    "label": 0
                },
                {
                    "sent": "This is random quality.",
                    "label": 0
                },
                {
                    "sent": "Like all these things at random, you want to know about the distribution.",
                    "label": 0
                },
                {
                    "sent": "You need the error bar.",
                    "label": 0
                },
                {
                    "sent": "Who's going to put that error?",
                    "label": 0
                },
                {
                    "sent": "Can you just like use the usual error bars?",
                    "label": 0
                },
                {
                    "sent": "Well, imagine the mixing process, which is not mixing, which is repeating, repeating, repeating the same thing.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't be using the same error bars, right?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's it's very.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some challenging OK.",
                    "label": 0
                },
                {
                    "sent": "So finally I love negative results.",
                    "label": 1
                },
                {
                    "sent": "OK. And.",
                    "label": 0
                },
                {
                    "sent": "Finally, bashing limit my field so.",
                    "label": 1
                },
                {
                    "sent": "There is such a thing as bad theory, like could be things could go bad in two ways.",
                    "label": 1
                },
                {
                    "sent": "One is that proofs are wrong.",
                    "label": 1
                },
                {
                    "sent": "We should have that.",
                    "label": 0
                },
                {
                    "sent": "But the other thing is that we could get the modeling assumptions wrong.",
                    "label": 0
                },
                {
                    "sent": "Which means that they don't quite fit reality, and there's always going to be really, really challenging.",
                    "label": 0
                },
                {
                    "sent": "That's the most challenging part of working in this field of coming up is the right set of assumptions, right?",
                    "label": 0
                },
                {
                    "sent": "Sort of modeling assumptions, but we should be attentive to this.",
                    "label": 0
                },
                {
                    "sent": "We should be conscious about this, and we can make it better.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great, I think that my time is totally up and everyone really wants to get some coffee.",
                    "label": 0
                },
                {
                    "sent": "I'm going to hanger on so if you have any questions just anytime.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}