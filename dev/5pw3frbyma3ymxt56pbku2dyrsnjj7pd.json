{
    "id": "5pw3frbyma3ymxt56pbku2dyrsnjj7pd",
    "title": "Harnessing Diversity in Crowds and Machines for Better NER performance",
    "info": {
        "author": [
            "Oana Inel, Faculty of Sciences, Vrije Universiteit Amsterdam (VU)"
        ],
        "published": "July 10, 2017",
        "recorded": "May 2017",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2017_inel_harnessing_diversity/",
    "segmentation": [
        [
            "Hello everyone, my name is Wayne L and I'm a PhD student in the web and Media Group at the University Amsterdam under the supervision of Laura Royal.",
            "As mentioned today, I'll talk about my work entitled Harnessing Diversity in Crowds and Machines for better Performance.",
            "Well, I will start my presentation by."
        ],
        [
            "Arguing something that you already heard in the previous presentation.",
            "The fact that current ground truth data sets for named entity recognition contains errors and usually they are not as gold or true as people or as they are supposed to be.",
            "And I'm going to give a couple of examples by referring to two ground truth datasets that have been used during the open knowledge extraction challenge at the SWC in 2015 and 2016.",
            "These two datasets focus on four types of entities, person, place, organizations and roles, and if you want to further check these datasets, you can use the links at the bottom of the slack.",
            "So let."
        ],
        [
            "See what's wrong with the ground truth Werner?",
            "Well, first of all, it contains a lot of inconsistencies when annotating entities of type, person, and role.",
            "If we take example Bishop Richard Williamson.",
            "Well, Bishop was annotated as a role while Richard Williamson was annotated as a person and we have a similar behavior.",
            "In the case of Queen Elizabeth, the second Queen is a role and Elizabeth II.",
            "He's a person.",
            "However, the same rules were not used in the case of Bishop Petronius.",
            "This entity was annotated only as a person.",
            "So in this case Bishop was not annotated as a role as in previous examples.",
            "We see this kind of inconsistency is mostly at the level of the same.",
            "Data set but also across datasets."
        ],
        [
            "Now another inconsistency that we saw was.",
            "In cases of annotating places, for example, in the data set from 2015, every time we had constructions such as city and country, for example, Basel, Switzerland, this construction was annotated as a single entity of type place.",
            "However, when you looked at the offsets of the entity that was annotated, you could see that only Basel was marked, so you had the span Basel Switzerland, but the opposites only matched.",
            "Puzzle.",
            "Something that was quite.",
            "Striking was the fact that in 2016 probably the annotation guideline has changed because such constructions were actually annotated as two entities of type place, not only one."
        ],
        [
            "Further, usually ground truth datasets.",
            "They also contain errors.",
            "For example, we have one of them, which is a clear grammatical mistake annotated as an entity of type person, and something that might bring up a lot of controversy is the fact that all the personal pronouns and possessive pronouns were annotated as entities of type person, and this happened actually in more than probably 30%.",
            "Of the total amount of entities of type person annotated by the experts."
        ],
        [
            "So after we saw this examples, we actually have to ask ourselves how reliable are the noeldner?"
        ],
        [
            "Tools trained using this goal standards and probably I shouldn't mention only training, but also testing and evaluating.",
            "Well, we did some experiments and we."
        ],
        [
            "Shows 5 nurdles We chose nerdanel we chose takes razor THT the pedia spotlight and semi tags and we try to see how they perform on this.",
            "These two gold standard datasets.",
            "First, we are going to check some examples and afterwards we are going to look at their overall performance across the datasets.",
            "So the first thing that we observed was the fact that named entity recognition tools they don't recognize personal pronouns or possessive pronouns as named entities.",
            "I'm not talking here about named entities of type person.",
            "They are not recognized and named entities at all.",
            "We only had three cases where a personal pronoun was annotated as a person as an entity actually, and this happened only for the THD tool when the personal pronoun was at the beginning of the sentence and it was written with a capital letter.",
            "Next when it comes to entities of type person, well, negative recognition tools, they tend to extract each span variation.",
            "So if we have the name of a person, they will extract the entire name.",
            "They will extract the first name, the second name, and this becomes even more complicated when the name is long and it also has abbreviations as in the example here."
        ],
        [
            "Now we also checked the entity types of type organization and actually 50% of those.",
            "Are a combination of organization and place, and usually when named entity recognition tools see this.",
            "They also extract separate entities, so they will also extract at the location to place in the named entity and also the organization.",
            "However, this is a bit ambiguous because usually in the ground truth you usually don't see overlapping entities and ground through datasets they don't allow for multiple perspectives, or granularities over the same entity.",
            "So now we looked at a couple of examples."
        ],
        [
            "Now let's see how the new tools performed on the overall.",
            "What we see here is the fact that, well, there is a high disagreement between the nerd tools, but if we look at their performance, the F1 score, well, we see that it's quite similar, even though the number of false positive cases, true positive and false negatives is quite different.",
            "Something that they need to mention here is the way we computed false positive to positive and false negatives.",
            "So the true positive entity is an entity that is in the gold standard and the Nerd tool distracted it.",
            "So attracted the span and the same offset.",
            "A false positive case is an entity that is in the gold standard, but the nerd tool extracted only a partial match, so not the entire match, and the false negative cases.",
            "An entity in the gold standard that has not been extracted by the North Pole."
        ],
        [
            "OK, so if we look further well we see that the recall for all the tools is quite low and this means that many of the entities in the gold standards have been actually missed.",
            "Now, given this, this very brief performance comparison is."
        ],
        [
            "Very difficult for us to understand the reliability of the goal of the newer tools, and Furthermore, it's even more difficult to choose the best one for our needs.",
            "Now given these."
        ],
        [
            "Starvation.",
            "We came up."
        ],
        [
            "With the following hypothesis that aggregating the output of nerd tools by harnessing the Inter tool disagreement performs better than the individual nurse.",
            "So what I'm saying here is that instead of choosing just one or two, we can just aggregate the output of multiple new tools and see whether we get a better outcome or not."
        ],
        [
            "In this table, in the last row.",
            "We see.",
            "The performance of aggregating the output of the previous NER tools.",
            "So just taking the output of all of them.",
            "And the first conclusion that we can draw here is OK, we have.",
            "We don't have a better performance that nerd ML, for example, which performs the best.",
            "But what we see here is the fact that we have a significantly higher number of true positive and significantly lower number of false negative cases, which means that we have a higher coverage over the gold standard.",
            "And of course."
        ],
        [
            "Because now we aggregate the output of multiple new tools.",
            "It means that we don't have to deal only with advantages built.",
            "We also need to deal with all the mistakes that they do, so we also have a significantly higher number of false positive cases.",
            "So usually when people aggregate multiple systems, they believe that the more systems key."
        ],
        [
            "View the same output.",
            "The higher the chances that that is the best answer, but now we have the following question.",
            "Is performance correlated with the number of nerve tools that extracted the given named entity?",
            "So the more nerd tools extracted the same entity, the higher the chance of being a correct named entity.",
            "In order to evaluate this, we computed one score.",
            "The sentence entity score and this is equal to the ratio of nerd tools that extracted that entity and what we want to measure here is the likelihood of an entity to be contained in the ground truth based on how many nerve tools extracted it."
        ],
        [
            "So in this chart, the horizontal lines.",
            "Represent the F1 score of the newer tools.",
            "The five new tools that I mentioned before and with the Green line we plotted the F1 score of the multi near approach of our approach against at different centers.",
            "Entity score thresholds.",
            "So these thresholds are actually used to differentiate between a positive and a negative case.",
            "And what we see here is the fact that our multi near approach has the best performance at the threshold of 0.4, which means that.",
            "We have the best performance when the entities were extracted by two or more tools, which means that there is indeed a lot of disagreement, and if we also compare now with the majority of vote approach, which is an approach that is widely used when doing this kind of aggregation of systems, we see that even we even perform better because in our case the mall."
        ],
        [
            "The majority vote approach is at the threshold of 0.6 and we are way above that and overall the difference is significant.",
            "OK, so we saw that aggregating multiple new tools actually give us a better result, but can we improve this?"
        ],
        [
            "Anyway."
        ],
        [
            "Well, we came up with the second hypothesis that crowdsourced ground truth by harnessing Inter annotator disagreement produces diversity in annotations and thus improve the aggregated output of nurdles."
        ],
        [
            "The following we performed a couple of crowdsourcing experiments and the goal of the crowdsourcing experiments was first to reduce the number of false positive cases and false negative cases, but was also to identify mistakes or ambiguities in the ground truth."
        ],
        [
            "This is a screenshot of the crowd sourcing task, so the crowd was given one sentence in the top with the named entity highlighted.",
            "What we did, we usually used the largest pan, so for each entity we created some clusters and they had in the first step they had to select all valid expressions that refer to the one highlighted.",
            "And the options that were given were actually first the entity in the ground truth and then all the options provided by the nurdles.",
            "So let's say the false positive cases for each entity.",
            "And as a third step, they were asked to select the type of each.",
            "Highlighted expression.",
            "So also for the false positive positive cases."
        ],
        [
            "And we got this result so.",
            "We analyze the data in a similar way, so for for measuring the performance of the multi NER we computed the sentence entity score based on the ratio.",
            "But here we have a crowd sentence entity score which is computed based on the votes got from the crowd and we measure this not with the ratio but with cosine similarities with the cosine similarity measure.",
            "So on the horizontal line in each graph is the multi near approach.",
            "So the approach that we showed first time across the entire data set and with the green line.",
            "This is actually the performance after the entities were improved by the crowd, so after the reduction of the false positive cases and something that is worth mentioning is the fact that the crowd was able to correct the data.",
            "And if we look at each crowd sentence, entity score.",
            "The crowd performs better, so it gives better results.",
            "Now at the beginning of the presentation I talked a lot about mistakes that are that we find in the ground truth.",
            "And well, maybe there are mistakes.",
            "Maybe they are not, but they are certainly ambiguities, and usually systems need to deal with this.",
            "White did it in the following experiment was to.",
            "We looked again through the data and basically when we had issues such as well, for example, Basel, Switzerland.",
            "So we allowed for multiple correct answers.",
            "So we said that OK. Basel Switzerland is a correct named entity of type person, and but we also allowed for Basel and Switzerland.",
            "So we rechecked the ground truth and we corrected the ground truth based on the answers given by the crowd, because usually when the crowd was faced with examples such as Basel, Switzerland, Basel, Switzerland, they marked all as valid expressions.",
            "Because this is probably true.",
            "And now."
        ],
        [
            "We we've been plotting this result after correcting the ground truth.",
            "Based on these examples with the Dark Green line and they see that in this case we have even a better performance of around 9.3 and similar for it's similar.",
            "Yeah, for both datasets for 2015 and 2016.",
            "Which means that maybe the crowd is actually able to identify this kind of inconsistencies in the ground truth, and it's actually consistent when annotating.",
            "Well."
        ],
        [
            "Now, in conclusion, what I presented today is a hybrid multimer, crowd driven approach for improved NER performance.",
            "And we showed that the crowd is able to correct the mistakes of the nerd tools by reducing the number of false positive cases and false negative cases.",
            "And I would also like to mention the fact that the crowd driven ground truth that harness is diversity perspectives an granularities proves to be a more reliable ground truth, because usually it allows to understand and identify mistakes that are usually in ground truth datasets.",
            "As future work we are working on.",
            "Extending the same approach on event extraction and you are using the fact bank data set for this to see whether we can actually replicate and we show that the crowd is also able to correct other types of entities.",
            "And the final goal of this experiment would be to actually train an event classifier with the crowd driven events and see whether we can improve the performance of it.",
            "If you want to check the tool that has been used to measure the performance of the results, or you want to check the data from the crowdsourcing experiments, you can check the links here.",
            "Thank you.",
            "Thanks so enough dog Ann.",
            "Just two questions and it was a bit puzzled when we are printing things.",
            "So first 'cause you anticipate the problem of coreference ING and define that a lot of the nurtured you try out, we're not able to find that, but the first big things you should say is that those nurtures have been trained on different corpora.",
            "An often occurrence is an option that was not activated when you probably try those tools, so it's obviously didn't find it.",
            "But most of those tools can be tuned to find car references and you would get different results.",
            "At the point I wanted to make.",
            "Is that a new comparisons you mentioned near the Mail, which is already a multi near because it aggregates all the offers near tools that you provided.",
            "So at the end when you aggregate another email with the other is your gate.",
            "And I get to variegate so it's it's a bit meta, so that's perhaps why before the ground truth you get slightly lower results.",
            "Yes, true.",
            "I'm sorry, so can you repeat the last question so about the coreference is yes, it is true.",
            "But what I wanted to show is the fact that these are the tools that people usually use in order to extract named entities and the part with the name with pronounced as being named entities.",
            "This is something that it's quite ambiguous.",
            "So from the previous talk we know that each state sets come with his own annotation guidelines.",
            "Those tools have been you take them off the shelves, they have been trained.",
            "When you watch Coronation guideline, so if you would like we need to then for example use them on UK following their annotation guidelines, you should at least we train that on this data set.",
            "This is what I meant.",
            "OK, thank you Sir.",
            "Sorry, I just got through.",
            "Thank you and thank you for the talk.",
            "I have a few quick questions.",
            "Can you elaborate how you've aggregated the results?",
            "In particular how you have aggregated the partial matches with you've unified everything?",
            "Or yeah, it's actually unifying everything, taking the output of all the tools.",
            "OK, so far.",
            "Question is, why did you do that?",
            "Why didn't you just take?",
            "And perhaps the intersection, the one where there was a stronger signal?",
            "This is exactly what I've been showing.",
            "Saying this one so if we are going to take the majority vote approach then we are missing useful information because there are entities that have been extracted only by 1, two or two tools and the less tools you take, the less answers you have.",
            "Alright, so we are just trying to show that allowing this agreement can actually help you to have better results.",
            "Alright, so my second question is did you by any chance look at?",
            "Specific types of entities and how these curves would look like, because I would imagine certain types of event recognitions is more challenging and you would get perhaps a different shape of the curve then say so events different the way we perceive events is different than the way we perceive people for example.",
            "So do you have any any insight into that?",
            "I don't have in this presentation, but yes I've been looking at this.",
            "Um?",
            "I think they have a very good precision in annotating person.",
            "But for role, for example, nothing was annotated as a role, and this is actually why I chose THD, because it usually gives common entities.",
            "And most of the times the roles are not named entities, but are common entities.",
            "So usually you get.",
            "Good spends for the roles, but you never get the role.",
            "As being this usually may, they may even refer as a person most of the time the roles are persons.",
            "OK, we have time for a couple of more questions.",
            "If we have them.",
            "Yep.",
            "Hi, thank you for representation.",
            "I didn't get perhaps your plan how you compute this call.",
            "So what this place this call that you have?",
            "So here, so we used five named entity recognition tools.",
            "So usually if one entity was extracted by all five extractors, you get a score of 1.",
            "If one is 1 entity was extracted by two extractors, then you get a score of 0.4.",
            "So this is the way so then.",
            "Total number for.",
            "The ratio of North tools that extracted that, given and from the total.",
            "OK, do we have any other question?",
            "Question.",
            "Hey, thank you for your story.",
            "I have a question again about you know the different type of entities.",
            "Any man that you notated every thought about.",
            "Exploiting kind of the strength of each.",
            "An ER like this like I suppose, is some.",
            "Some system can better recognize person whereas some other can but recognize events or rolls.",
            "So before the vote checking then that and then assigning weights to each.",
            "And yes, I I tried this and I also looked at the confidence given by the extractors, but usually you don't know how exactly this confidence score was computed.",
            "It usually a black box and it's quite difficult to make assumptions based on this.",
            "And yes, about.",
            "The performance based on entity types.",
            "I looked at this but I couldn't say that there is one tool that performs very well on a given entity.",
            "OK. Before that I have a question.",
            "If you don't mind, I have a pointless question as promised.",
            "I've seen that in your crowdsourcing task task.",
            "You can you go back to the slide of the one the University of Utrecht or whatever the question.",
            "Do you have the none of them option?",
            "No, no, because I wanted to show them only the options that were given by the extractors.",
            "So what it was if it was wrong you couldn't say no.",
            "This is wrong.",
            "So is there an impact in terms of errors for that?",
            "No, because usually actually.",
            "Just because I took all the entities that were overlapping with the ground truth and all the matches, all the partial matches that I was sure that something was related to the ground truth.",
            "And in the cases where no extractor extracted the named entity, so the false negative cases I didn't show only the false negative case, only the entity, but I showed all the possible options.",
            "For example, if it if I had full professor missed by the extractors but they only extracted professor, then I used full professor, professor and food.",
            "So every option is just to make sure that the I don't introduce bias in the task.",
            "Just providing the good option to choose from."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, my name is Wayne L and I'm a PhD student in the web and Media Group at the University Amsterdam under the supervision of Laura Royal.",
                    "label": 0
                },
                {
                    "sent": "As mentioned today, I'll talk about my work entitled Harnessing Diversity in Crowds and Machines for better Performance.",
                    "label": 1
                },
                {
                    "sent": "Well, I will start my presentation by.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arguing something that you already heard in the previous presentation.",
                    "label": 0
                },
                {
                    "sent": "The fact that current ground truth data sets for named entity recognition contains errors and usually they are not as gold or true as people or as they are supposed to be.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to give a couple of examples by referring to two ground truth datasets that have been used during the open knowledge extraction challenge at the SWC in 2015 and 2016.",
                    "label": 1
                },
                {
                    "sent": "These two datasets focus on four types of entities, person, place, organizations and roles, and if you want to further check these datasets, you can use the links at the bottom of the slack.",
                    "label": 0
                },
                {
                    "sent": "So let.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See what's wrong with the ground truth Werner?",
                    "label": 1
                },
                {
                    "sent": "Well, first of all, it contains a lot of inconsistencies when annotating entities of type, person, and role.",
                    "label": 0
                },
                {
                    "sent": "If we take example Bishop Richard Williamson.",
                    "label": 0
                },
                {
                    "sent": "Well, Bishop was annotated as a role while Richard Williamson was annotated as a person and we have a similar behavior.",
                    "label": 1
                },
                {
                    "sent": "In the case of Queen Elizabeth, the second Queen is a role and Elizabeth II.",
                    "label": 0
                },
                {
                    "sent": "He's a person.",
                    "label": 0
                },
                {
                    "sent": "However, the same rules were not used in the case of Bishop Petronius.",
                    "label": 0
                },
                {
                    "sent": "This entity was annotated only as a person.",
                    "label": 0
                },
                {
                    "sent": "So in this case Bishop was not annotated as a role as in previous examples.",
                    "label": 0
                },
                {
                    "sent": "We see this kind of inconsistency is mostly at the level of the same.",
                    "label": 0
                },
                {
                    "sent": "Data set but also across datasets.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now another inconsistency that we saw was.",
                    "label": 0
                },
                {
                    "sent": "In cases of annotating places, for example, in the data set from 2015, every time we had constructions such as city and country, for example, Basel, Switzerland, this construction was annotated as a single entity of type place.",
                    "label": 1
                },
                {
                    "sent": "However, when you looked at the offsets of the entity that was annotated, you could see that only Basel was marked, so you had the span Basel Switzerland, but the opposites only matched.",
                    "label": 0
                },
                {
                    "sent": "Puzzle.",
                    "label": 0
                },
                {
                    "sent": "Something that was quite.",
                    "label": 1
                },
                {
                    "sent": "Striking was the fact that in 2016 probably the annotation guideline has changed because such constructions were actually annotated as two entities of type place, not only one.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Further, usually ground truth datasets.",
                    "label": 1
                },
                {
                    "sent": "They also contain errors.",
                    "label": 1
                },
                {
                    "sent": "For example, we have one of them, which is a clear grammatical mistake annotated as an entity of type person, and something that might bring up a lot of controversy is the fact that all the personal pronouns and possessive pronouns were annotated as entities of type person, and this happened actually in more than probably 30%.",
                    "label": 1
                },
                {
                    "sent": "Of the total amount of entities of type person annotated by the experts.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So after we saw this examples, we actually have to ask ourselves how reliable are the noeldner?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tools trained using this goal standards and probably I shouldn't mention only training, but also testing and evaluating.",
                    "label": 0
                },
                {
                    "sent": "Well, we did some experiments and we.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Shows 5 nurdles We chose nerdanel we chose takes razor THT the pedia spotlight and semi tags and we try to see how they perform on this.",
                    "label": 1
                },
                {
                    "sent": "These two gold standard datasets.",
                    "label": 0
                },
                {
                    "sent": "First, we are going to check some examples and afterwards we are going to look at their overall performance across the datasets.",
                    "label": 0
                },
                {
                    "sent": "So the first thing that we observed was the fact that named entity recognition tools they don't recognize personal pronouns or possessive pronouns as named entities.",
                    "label": 0
                },
                {
                    "sent": "I'm not talking here about named entities of type person.",
                    "label": 0
                },
                {
                    "sent": "They are not recognized and named entities at all.",
                    "label": 0
                },
                {
                    "sent": "We only had three cases where a personal pronoun was annotated as a person as an entity actually, and this happened only for the THD tool when the personal pronoun was at the beginning of the sentence and it was written with a capital letter.",
                    "label": 0
                },
                {
                    "sent": "Next when it comes to entities of type person, well, negative recognition tools, they tend to extract each span variation.",
                    "label": 0
                },
                {
                    "sent": "So if we have the name of a person, they will extract the entire name.",
                    "label": 0
                },
                {
                    "sent": "They will extract the first name, the second name, and this becomes even more complicated when the name is long and it also has abbreviations as in the example here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we also checked the entity types of type organization and actually 50% of those.",
                    "label": 1
                },
                {
                    "sent": "Are a combination of organization and place, and usually when named entity recognition tools see this.",
                    "label": 1
                },
                {
                    "sent": "They also extract separate entities, so they will also extract at the location to place in the named entity and also the organization.",
                    "label": 1
                },
                {
                    "sent": "However, this is a bit ambiguous because usually in the ground truth you usually don't see overlapping entities and ground through datasets they don't allow for multiple perspectives, or granularities over the same entity.",
                    "label": 0
                },
                {
                    "sent": "So now we looked at a couple of examples.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's see how the new tools performed on the overall.",
                    "label": 1
                },
                {
                    "sent": "What we see here is the fact that, well, there is a high disagreement between the nerd tools, but if we look at their performance, the F1 score, well, we see that it's quite similar, even though the number of false positive cases, true positive and false negatives is quite different.",
                    "label": 0
                },
                {
                    "sent": "Something that they need to mention here is the way we computed false positive to positive and false negatives.",
                    "label": 0
                },
                {
                    "sent": "So the true positive entity is an entity that is in the gold standard and the Nerd tool distracted it.",
                    "label": 0
                },
                {
                    "sent": "So attracted the span and the same offset.",
                    "label": 0
                },
                {
                    "sent": "A false positive case is an entity that is in the gold standard, but the nerd tool extracted only a partial match, so not the entire match, and the false negative cases.",
                    "label": 0
                },
                {
                    "sent": "An entity in the gold standard that has not been extracted by the North Pole.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if we look further well we see that the recall for all the tools is quite low and this means that many of the entities in the gold standards have been actually missed.",
                    "label": 0
                },
                {
                    "sent": "Now, given this, this very brief performance comparison is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very difficult for us to understand the reliability of the goal of the newer tools, and Furthermore, it's even more difficult to choose the best one for our needs.",
                    "label": 0
                },
                {
                    "sent": "Now given these.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Starvation.",
                    "label": 0
                },
                {
                    "sent": "We came up.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the following hypothesis that aggregating the output of nerd tools by harnessing the Inter tool disagreement performs better than the individual nurse.",
                    "label": 0
                },
                {
                    "sent": "So what I'm saying here is that instead of choosing just one or two, we can just aggregate the output of multiple new tools and see whether we get a better outcome or not.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this table, in the last row.",
                    "label": 0
                },
                {
                    "sent": "We see.",
                    "label": 0
                },
                {
                    "sent": "The performance of aggregating the output of the previous NER tools.",
                    "label": 0
                },
                {
                    "sent": "So just taking the output of all of them.",
                    "label": 0
                },
                {
                    "sent": "And the first conclusion that we can draw here is OK, we have.",
                    "label": 0
                },
                {
                    "sent": "We don't have a better performance that nerd ML, for example, which performs the best.",
                    "label": 0
                },
                {
                    "sent": "But what we see here is the fact that we have a significantly higher number of true positive and significantly lower number of false negative cases, which means that we have a higher coverage over the gold standard.",
                    "label": 0
                },
                {
                    "sent": "And of course.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because now we aggregate the output of multiple new tools.",
                    "label": 0
                },
                {
                    "sent": "It means that we don't have to deal only with advantages built.",
                    "label": 0
                },
                {
                    "sent": "We also need to deal with all the mistakes that they do, so we also have a significantly higher number of false positive cases.",
                    "label": 0
                },
                {
                    "sent": "So usually when people aggregate multiple systems, they believe that the more systems key.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "View the same output.",
                    "label": 0
                },
                {
                    "sent": "The higher the chances that that is the best answer, but now we have the following question.",
                    "label": 0
                },
                {
                    "sent": "Is performance correlated with the number of nerve tools that extracted the given named entity?",
                    "label": 1
                },
                {
                    "sent": "So the more nerd tools extracted the same entity, the higher the chance of being a correct named entity.",
                    "label": 0
                },
                {
                    "sent": "In order to evaluate this, we computed one score.",
                    "label": 0
                },
                {
                    "sent": "The sentence entity score and this is equal to the ratio of nerd tools that extracted that entity and what we want to measure here is the likelihood of an entity to be contained in the ground truth based on how many nerve tools extracted it.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this chart, the horizontal lines.",
                    "label": 0
                },
                {
                    "sent": "Represent the F1 score of the newer tools.",
                    "label": 0
                },
                {
                    "sent": "The five new tools that I mentioned before and with the Green line we plotted the F1 score of the multi near approach of our approach against at different centers.",
                    "label": 0
                },
                {
                    "sent": "Entity score thresholds.",
                    "label": 0
                },
                {
                    "sent": "So these thresholds are actually used to differentiate between a positive and a negative case.",
                    "label": 0
                },
                {
                    "sent": "And what we see here is the fact that our multi near approach has the best performance at the threshold of 0.4, which means that.",
                    "label": 0
                },
                {
                    "sent": "We have the best performance when the entities were extracted by two or more tools, which means that there is indeed a lot of disagreement, and if we also compare now with the majority of vote approach, which is an approach that is widely used when doing this kind of aggregation of systems, we see that even we even perform better because in our case the mall.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The majority vote approach is at the threshold of 0.6 and we are way above that and overall the difference is significant.",
                    "label": 0
                },
                {
                    "sent": "OK, so we saw that aggregating multiple new tools actually give us a better result, but can we improve this?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, we came up with the second hypothesis that crowdsourced ground truth by harnessing Inter annotator disagreement produces diversity in annotations and thus improve the aggregated output of nurdles.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The following we performed a couple of crowdsourcing experiments and the goal of the crowdsourcing experiments was first to reduce the number of false positive cases and false negative cases, but was also to identify mistakes or ambiguities in the ground truth.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a screenshot of the crowd sourcing task, so the crowd was given one sentence in the top with the named entity highlighted.",
                    "label": 0
                },
                {
                    "sent": "What we did, we usually used the largest pan, so for each entity we created some clusters and they had in the first step they had to select all valid expressions that refer to the one highlighted.",
                    "label": 0
                },
                {
                    "sent": "And the options that were given were actually first the entity in the ground truth and then all the options provided by the nurdles.",
                    "label": 0
                },
                {
                    "sent": "So let's say the false positive cases for each entity.",
                    "label": 0
                },
                {
                    "sent": "And as a third step, they were asked to select the type of each.",
                    "label": 0
                },
                {
                    "sent": "Highlighted expression.",
                    "label": 0
                },
                {
                    "sent": "So also for the false positive positive cases.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we got this result so.",
                    "label": 0
                },
                {
                    "sent": "We analyze the data in a similar way, so for for measuring the performance of the multi NER we computed the sentence entity score based on the ratio.",
                    "label": 0
                },
                {
                    "sent": "But here we have a crowd sentence entity score which is computed based on the votes got from the crowd and we measure this not with the ratio but with cosine similarities with the cosine similarity measure.",
                    "label": 0
                },
                {
                    "sent": "So on the horizontal line in each graph is the multi near approach.",
                    "label": 0
                },
                {
                    "sent": "So the approach that we showed first time across the entire data set and with the green line.",
                    "label": 0
                },
                {
                    "sent": "This is actually the performance after the entities were improved by the crowd, so after the reduction of the false positive cases and something that is worth mentioning is the fact that the crowd was able to correct the data.",
                    "label": 0
                },
                {
                    "sent": "And if we look at each crowd sentence, entity score.",
                    "label": 0
                },
                {
                    "sent": "The crowd performs better, so it gives better results.",
                    "label": 1
                },
                {
                    "sent": "Now at the beginning of the presentation I talked a lot about mistakes that are that we find in the ground truth.",
                    "label": 0
                },
                {
                    "sent": "And well, maybe there are mistakes.",
                    "label": 0
                },
                {
                    "sent": "Maybe they are not, but they are certainly ambiguities, and usually systems need to deal with this.",
                    "label": 0
                },
                {
                    "sent": "White did it in the following experiment was to.",
                    "label": 0
                },
                {
                    "sent": "We looked again through the data and basically when we had issues such as well, for example, Basel, Switzerland.",
                    "label": 0
                },
                {
                    "sent": "So we allowed for multiple correct answers.",
                    "label": 0
                },
                {
                    "sent": "So we said that OK. Basel Switzerland is a correct named entity of type person, and but we also allowed for Basel and Switzerland.",
                    "label": 0
                },
                {
                    "sent": "So we rechecked the ground truth and we corrected the ground truth based on the answers given by the crowd, because usually when the crowd was faced with examples such as Basel, Switzerland, Basel, Switzerland, they marked all as valid expressions.",
                    "label": 0
                },
                {
                    "sent": "Because this is probably true.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We we've been plotting this result after correcting the ground truth.",
                    "label": 1
                },
                {
                    "sent": "Based on these examples with the Dark Green line and they see that in this case we have even a better performance of around 9.3 and similar for it's similar.",
                    "label": 0
                },
                {
                    "sent": "Yeah, for both datasets for 2015 and 2016.",
                    "label": 1
                },
                {
                    "sent": "Which means that maybe the crowd is actually able to identify this kind of inconsistencies in the ground truth, and it's actually consistent when annotating.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, in conclusion, what I presented today is a hybrid multimer, crowd driven approach for improved NER performance.",
                    "label": 1
                },
                {
                    "sent": "And we showed that the crowd is able to correct the mistakes of the nerd tools by reducing the number of false positive cases and false negative cases.",
                    "label": 1
                },
                {
                    "sent": "And I would also like to mention the fact that the crowd driven ground truth that harness is diversity perspectives an granularities proves to be a more reliable ground truth, because usually it allows to understand and identify mistakes that are usually in ground truth datasets.",
                    "label": 0
                },
                {
                    "sent": "As future work we are working on.",
                    "label": 0
                },
                {
                    "sent": "Extending the same approach on event extraction and you are using the fact bank data set for this to see whether we can actually replicate and we show that the crowd is also able to correct other types of entities.",
                    "label": 0
                },
                {
                    "sent": "And the final goal of this experiment would be to actually train an event classifier with the crowd driven events and see whether we can improve the performance of it.",
                    "label": 0
                },
                {
                    "sent": "If you want to check the tool that has been used to measure the performance of the results, or you want to check the data from the crowdsourcing experiments, you can check the links here.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thanks so enough dog Ann.",
                    "label": 0
                },
                {
                    "sent": "Just two questions and it was a bit puzzled when we are printing things.",
                    "label": 0
                },
                {
                    "sent": "So first 'cause you anticipate the problem of coreference ING and define that a lot of the nurtured you try out, we're not able to find that, but the first big things you should say is that those nurtures have been trained on different corpora.",
                    "label": 0
                },
                {
                    "sent": "An often occurrence is an option that was not activated when you probably try those tools, so it's obviously didn't find it.",
                    "label": 0
                },
                {
                    "sent": "But most of those tools can be tuned to find car references and you would get different results.",
                    "label": 0
                },
                {
                    "sent": "At the point I wanted to make.",
                    "label": 0
                },
                {
                    "sent": "Is that a new comparisons you mentioned near the Mail, which is already a multi near because it aggregates all the offers near tools that you provided.",
                    "label": 0
                },
                {
                    "sent": "So at the end when you aggregate another email with the other is your gate.",
                    "label": 0
                },
                {
                    "sent": "And I get to variegate so it's it's a bit meta, so that's perhaps why before the ground truth you get slightly lower results.",
                    "label": 0
                },
                {
                    "sent": "Yes, true.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, so can you repeat the last question so about the coreference is yes, it is true.",
                    "label": 0
                },
                {
                    "sent": "But what I wanted to show is the fact that these are the tools that people usually use in order to extract named entities and the part with the name with pronounced as being named entities.",
                    "label": 0
                },
                {
                    "sent": "This is something that it's quite ambiguous.",
                    "label": 0
                },
                {
                    "sent": "So from the previous talk we know that each state sets come with his own annotation guidelines.",
                    "label": 0
                },
                {
                    "sent": "Those tools have been you take them off the shelves, they have been trained.",
                    "label": 0
                },
                {
                    "sent": "When you watch Coronation guideline, so if you would like we need to then for example use them on UK following their annotation guidelines, you should at least we train that on this data set.",
                    "label": 0
                },
                {
                    "sent": "This is what I meant.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you Sir.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I just got through.",
                    "label": 0
                },
                {
                    "sent": "Thank you and thank you for the talk.",
                    "label": 0
                },
                {
                    "sent": "I have a few quick questions.",
                    "label": 0
                },
                {
                    "sent": "Can you elaborate how you've aggregated the results?",
                    "label": 0
                },
                {
                    "sent": "In particular how you have aggregated the partial matches with you've unified everything?",
                    "label": 0
                },
                {
                    "sent": "Or yeah, it's actually unifying everything, taking the output of all the tools.",
                    "label": 0
                },
                {
                    "sent": "OK, so far.",
                    "label": 0
                },
                {
                    "sent": "Question is, why did you do that?",
                    "label": 0
                },
                {
                    "sent": "Why didn't you just take?",
                    "label": 0
                },
                {
                    "sent": "And perhaps the intersection, the one where there was a stronger signal?",
                    "label": 0
                },
                {
                    "sent": "This is exactly what I've been showing.",
                    "label": 0
                },
                {
                    "sent": "Saying this one so if we are going to take the majority vote approach then we are missing useful information because there are entities that have been extracted only by 1, two or two tools and the less tools you take, the less answers you have.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we are just trying to show that allowing this agreement can actually help you to have better results.",
                    "label": 0
                },
                {
                    "sent": "Alright, so my second question is did you by any chance look at?",
                    "label": 0
                },
                {
                    "sent": "Specific types of entities and how these curves would look like, because I would imagine certain types of event recognitions is more challenging and you would get perhaps a different shape of the curve then say so events different the way we perceive events is different than the way we perceive people for example.",
                    "label": 0
                },
                {
                    "sent": "So do you have any any insight into that?",
                    "label": 0
                },
                {
                    "sent": "I don't have in this presentation, but yes I've been looking at this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I think they have a very good precision in annotating person.",
                    "label": 0
                },
                {
                    "sent": "But for role, for example, nothing was annotated as a role, and this is actually why I chose THD, because it usually gives common entities.",
                    "label": 0
                },
                {
                    "sent": "And most of the times the roles are not named entities, but are common entities.",
                    "label": 0
                },
                {
                    "sent": "So usually you get.",
                    "label": 0
                },
                {
                    "sent": "Good spends for the roles, but you never get the role.",
                    "label": 0
                },
                {
                    "sent": "As being this usually may, they may even refer as a person most of the time the roles are persons.",
                    "label": 0
                },
                {
                    "sent": "OK, we have time for a couple of more questions.",
                    "label": 0
                },
                {
                    "sent": "If we have them.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Hi, thank you for representation.",
                    "label": 0
                },
                {
                    "sent": "I didn't get perhaps your plan how you compute this call.",
                    "label": 0
                },
                {
                    "sent": "So what this place this call that you have?",
                    "label": 0
                },
                {
                    "sent": "So here, so we used five named entity recognition tools.",
                    "label": 0
                },
                {
                    "sent": "So usually if one entity was extracted by all five extractors, you get a score of 1.",
                    "label": 0
                },
                {
                    "sent": "If one is 1 entity was extracted by two extractors, then you get a score of 0.4.",
                    "label": 0
                },
                {
                    "sent": "So this is the way so then.",
                    "label": 0
                },
                {
                    "sent": "Total number for.",
                    "label": 0
                },
                {
                    "sent": "The ratio of North tools that extracted that, given and from the total.",
                    "label": 0
                },
                {
                    "sent": "OK, do we have any other question?",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Hey, thank you for your story.",
                    "label": 0
                },
                {
                    "sent": "I have a question again about you know the different type of entities.",
                    "label": 0
                },
                {
                    "sent": "Any man that you notated every thought about.",
                    "label": 0
                },
                {
                    "sent": "Exploiting kind of the strength of each.",
                    "label": 0
                },
                {
                    "sent": "An ER like this like I suppose, is some.",
                    "label": 0
                },
                {
                    "sent": "Some system can better recognize person whereas some other can but recognize events or rolls.",
                    "label": 0
                },
                {
                    "sent": "So before the vote checking then that and then assigning weights to each.",
                    "label": 0
                },
                {
                    "sent": "And yes, I I tried this and I also looked at the confidence given by the extractors, but usually you don't know how exactly this confidence score was computed.",
                    "label": 0
                },
                {
                    "sent": "It usually a black box and it's quite difficult to make assumptions based on this.",
                    "label": 0
                },
                {
                    "sent": "And yes, about.",
                    "label": 0
                },
                {
                    "sent": "The performance based on entity types.",
                    "label": 0
                },
                {
                    "sent": "I looked at this but I couldn't say that there is one tool that performs very well on a given entity.",
                    "label": 0
                },
                {
                    "sent": "OK. Before that I have a question.",
                    "label": 0
                },
                {
                    "sent": "If you don't mind, I have a pointless question as promised.",
                    "label": 0
                },
                {
                    "sent": "I've seen that in your crowdsourcing task task.",
                    "label": 0
                },
                {
                    "sent": "You can you go back to the slide of the one the University of Utrecht or whatever the question.",
                    "label": 0
                },
                {
                    "sent": "Do you have the none of them option?",
                    "label": 0
                },
                {
                    "sent": "No, no, because I wanted to show them only the options that were given by the extractors.",
                    "label": 0
                },
                {
                    "sent": "So what it was if it was wrong you couldn't say no.",
                    "label": 0
                },
                {
                    "sent": "This is wrong.",
                    "label": 0
                },
                {
                    "sent": "So is there an impact in terms of errors for that?",
                    "label": 0
                },
                {
                    "sent": "No, because usually actually.",
                    "label": 0
                },
                {
                    "sent": "Just because I took all the entities that were overlapping with the ground truth and all the matches, all the partial matches that I was sure that something was related to the ground truth.",
                    "label": 0
                },
                {
                    "sent": "And in the cases where no extractor extracted the named entity, so the false negative cases I didn't show only the false negative case, only the entity, but I showed all the possible options.",
                    "label": 0
                },
                {
                    "sent": "For example, if it if I had full professor missed by the extractors but they only extracted professor, then I used full professor, professor and food.",
                    "label": 0
                },
                {
                    "sent": "So every option is just to make sure that the I don't introduce bias in the task.",
                    "label": 0
                },
                {
                    "sent": "Just providing the good option to choose from.",
                    "label": 0
                }
            ]
        }
    }
}