{
    "id": "2cevuhn3xk5hmo72qntqgxugpgff266s",
    "title": "KADE: Aligning Knowledge Base and Document Embedding Models using Regularized Multi-Task Learning",
    "info": {
        "author": [
            "Matthias Baumgartner, University of Zurich"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_baumgartner_kade_aligning_regularized/",
    "segmentation": [
        [
            "Good afternoon, I'm not this Pam Gardner from the University of Zurich and I'm about to present a method that learns a combination common representation over document and knowledge graph entities such that the two become incomparable."
        ],
        [
            "This is Mary Mary has created a knowledge graph.",
            "But your friends tell her that they would like to have detailed descriptions of her entities."
        ],
        [
            "This is Max.",
            "Max has crawled to the vet and created a large corpus of documents, but his friends tell him that his corpus is unorganized, which makes it hard to find documents that are about related topics."
        ],
        [
            "One evening, Mary and Max sit together and realize that by combining their data sources, they could solve both of their problems.",
            "This means that each document should be associated with the corresponding knowledge graph entity and vice versa."
        ],
        [
            "The first question is to which entity would you associate the document?",
            "We realized that documents and entities or descriptions of real world objects.",
            "So naturally you'd want to associate those descriptions that are about the same object."
        ],
        [
            "Having identified the problem and sketched out, the solution should look like.",
            "They now face the challenge of how do you actually solve this problem?",
            "In this talk, I'm going to present a method that makes use of three properties.",
            "First, there is no natural way to compare entities or to documents.",
            "So we need to have a format in which documents and entities can both be represented.",
            "Of course, such a representation only makes sense if descriptions of the same object are represented in a similar way.",
            "And Additionally we can make use.",
            "Of similarities of descriptions with within one source.",
            "So that we can use the neighborhood in order to find to infer new alignments.",
            "I will now discuss how we make use of these three properties in detail."
        ],
        [
            "As a common representation, we chose vector spaces.",
            "This is particularly common in computer science and is useful in this task because there are various ways to compare vectors to each other, like the Euclidean or cosine distance."
        ],
        [
            "So we need a way to convert our original sources into that vector space.",
            "For that we have embedding methods.",
            "Embedding models learn this vector space or over one source such that relations in the corpus are also reflected in the vector space.",
            "For example, documents that have similar wordings should also have similar vectors.",
            "Technically speaking.",
            "Embedding methods define a loss function that captures the structure of the input dependent on a latent embedding vector.",
            "This loss function defines an optimization problem, and by solving the optimization problem you produce the embedding vectors.",
            "In our case we used Doctor Back for the document side.",
            "Talk to work uses word Co.",
            "Occurrence is dependent on the document.",
            "And transit for knowledge graph.",
            "Transit is based on the idea that a relation in the Knowledge Graph is translation in the embedding space from the head entity to the tail entity embedding.",
            "So this gives us a representation that already reflects similarity of items in your source."
        ],
        [
            "However.",
            "If you if you compute these embedding models independently of each other.",
            "They likely will not group related documents and entities into the same region.",
            "This is becausw.",
            "Entity embedding embedding models learn the structure of the input.",
            "So if you if you systematically omit a relation like correspondences between sources from the input, then this will also not be reflected in your embeddings.",
            "2nd, if you compute if you run an embedding model twice on the same data, you will very likely end up with different embeddings and this is big cause this optimization problem can have multiple solutions and the learning being a stochastic process, you can end up in different solutions.",
            "For these two reasons.",
            "We need to somehow exchange information between the two models and also train them at the same time.",
            "So we propose a model that we called coding.",
            "Which does this via regular."
        ],
        [
            "Azatian our model builds on top of two existing embedding models, one for each domain.",
            "And we further assume a number of entity documents training pairs.",
            "So first we make the current entity embeddings known in the document embedding model.",
            "And with the regularizer, add a penalty to the document embeddings to this proportional to their distance to the corresponding entity embeddings.",
            "So we now train the document embedding for a number of steps, which effectively makes a document embedding similar to related document embeddings, but also similar to their corresponding entity embeddings.",
            "The procedure is repeated in the other way and also here we train the knowledge base embedding for a number of steps.",
            "The procedure goes back and forth until the embeddings have stabilized.",
            "Now this is the third of our properties.",
            "Giving us a representation that also captures similarity across sources."
        ],
        [
            "So Max thinks that's cool.",
            "What can I use this representation for my documents?",
            "A typical task that he might want to achieve this side of document classification.",
            "So we build a number of classifiers.",
            "To take embeddings as input.",
            "And predict whether or not a document belongs to a certain class.",
            "And on the on the Y axis we see the accuracy statistics over all the classifiers.",
            "So we trained the this classifier once using code embeddings and once using the document embeddings from the document from the Doctor Work model.",
            "Our hypothesis is that we can achieve this alignment task without breaking the characteristics of the underlying embedding models, and indeed we see.",
            "That's called a even achieves slightly higher performance than the plain embeddings."
        ],
        [
            "So many things that's cool.",
            "But can I use this representation for my knowledge graph?",
            "A typical task she would want to achieve is that of predicting missing links.",
            "So we tested the link prediction performance again once using called embeddings and once using embeddings trained on the document on the Knowledge Graph, only with the transition model.",
            "The mean rank indicates how many incorrect triples were favored over the true testing triple, lower mean drank is better, and we see that both models achieve about the same performance.",
            "The hit smasher indicate how many ranks were below 10.",
            "So higher hits score is better, and again both models have about the same performance.",
            "They repeated the experiments on two more knowledge graphs.",
            "These graphs are sparser Dan FP15K.",
            "And we observed that while in general the link prediction performance becomes worse if the graphs become sparser, the effect of called A is more pronounced.",
            "So how about?"
        ],
        [
            "Not alignment task.",
            "We tested this by predicting which document corresponds to an entity and vice versa.",
            "If the line if the alignment accuracy is 1, it means that all the predictions were perfect.",
            "On the other hand, if the accuracy or.",
            "The accuracy is 0.5.",
            "This means that we were no better than a random prediction.",
            "We first confirmed the hypothesis that independently computed model models are incomparable.",
            "With it so by learning a document and the Knowledge Graph model separate from each other and then projecting one embedding space onto the other.",
            "And indeed we see that the performance is only marginally higher than the random baseline.",
            "On the other hand, using called embeddings.",
            "We achieve significantly higher alignment accuracy.",
            "Even if you start with a router, low number of training alignments, the performance is already quite relevant, and if you increase the number of training data then of course it gets better.",
            "Also, in this experiment we used the other two knowledge graphs and again see that the density of knowledge graph has an impact on the alignment precision.",
            "Our intuition here is that in sparser graphs, you can make less use of the neighborhood and of potentially regularised neighbors of an entity.",
            "So income."
        ],
        [
            "Solution.",
            "I've shown that we can learn embeddings jointly with the coding method.",
            "And with that achieve a common representation over two sources such that we can compare items within each source, but also items across those sources."
        ],
        [
            "Of course, our work is not complete.",
            "The main limitation of embedding models and therefore also of our approach is that you cannot easily incorporate new embeddings and new or changing data into existing embeddings.",
            "So for example, once you learn the embeddings with the coding method and you change the Knowledge graph, then you have to essentially re learn all of the embeddings.",
            "So work on this would be greatly appreciated.",
            "Alternatively, we could think of embedding different formats.",
            "There has been some work done on image embeddings and it would be interesting to plug these into a Cardiff framework and then for example align documents to images or entities to images.",
            "Similarly, we limited our work to exactly two sources.",
            "But this limitation could maybe be lifted and.",
            "Alignment between 3:00 or even more sources, like for example documents, entities, and images, could be interesting approach."
        ],
        [
            "If you're interested in the full details of the methods, please read and cite our paper.",
            "Thank you for your attention and I can now take your questions.",
            "Thank you my dears.",
            "So to represent both entities and documents you use the.",
            "I guess some kind of normal to norm or whatever.",
            "In the end, it's a scalar right that you used the difference document amending minus entity embedding, right?",
            "Have you tried?",
            "I don't know mean or difference on the embeddings themselves, since the dimension is the same right the embedding dimension.",
            "Or any other operator binary operator that combines both embeddings.",
            "That's the first question, and the second question is.",
            "So you emphasize regularised multitask learning, but?",
            "Regularization, per say, is too.",
            "Restate the impulse problem right when you have too many features and you don't have enough samples, so in this case, what is the role of regularization?",
            "And if you used any other technique, what would have changed?",
            "Thank you.",
            "So to answer the first question.",
            "We focused on on distance measures.",
            "Also, since we have.",
            "Exactly two sources.",
            "That seems to be the most natural.",
            "To your second question, the regularization.",
            "Essentially introduces some penalty on the parameter you use.",
            "And.",
            "We used regularization in that sense that we add an additional constraint to an additional at two in existing embedding model and that constraint depends on.",
            "On the second source.",
            "So thank you very much for the very nice presentation.",
            "Not only the content but also the form was really very nice.",
            "My question would have been but you're already insert insert it degradation between images and document.",
            "This is really a very nice topic.",
            "I was wondering how did you do this for English or do you expect also with different language to have the same level of?",
            "Relation between the entity world and the document world.",
            "So we were working on a exclusively English corpus.",
            "Of course.",
            "It would also be interesting to compare.",
            "Embedding models trains with like 2 document embedding models but trained on different languages.",
            "I think this was already at least to some extent done by just computing models independently and then trying to map those, and I think this works quite well if the languages have similar pattern in.",
            "So for example, Doctor Beck uses Word Co occurrence.",
            "So if the statistics is similar in both languages, then I think.",
            "Easier approaches work quite well.",
            "Yeah I would like to make a quick one at your recent doctor back here, which seems to be the obvious option because in the end you are trying to compare documents with with entities in a in a in a knowledge base and thinking of have you considered using other final clarity types of embeddings like oriented towards or?",
            "So in a way that you could reuse a very large corpora, which is already available out there could give you additional additional performance in this case.",
            "So we have experimented with multiple models on the Knowledge Graph side.",
            "On the document side, we used Doctor Vac since it's an obvious choice and was considered as.",
            "A state of the art approach.",
            "Would be interesting is with.",
            "In the meantime, done some experiments on comparing DOC.",
            "Two VEC to simply averaged word embeddings or some some of word embeddings, and it looks like the Suns or averages also work quite well, so this would be very interesting because it would to some extent solve the problem that you.",
            "If your document corpus changes.",
            "Alright, since the vert distribution of the vocabulary is less likely to change flexibility, yes exactly OK.",
            "Thank you very much.",
            "Materials list.",
            "Thank Matthews again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon, I'm not this Pam Gardner from the University of Zurich and I'm about to present a method that learns a combination common representation over document and knowledge graph entities such that the two become incomparable.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is Mary Mary has created a knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "But your friends tell her that they would like to have detailed descriptions of her entities.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is Max.",
                    "label": 0
                },
                {
                    "sent": "Max has crawled to the vet and created a large corpus of documents, but his friends tell him that his corpus is unorganized, which makes it hard to find documents that are about related topics.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One evening, Mary and Max sit together and realize that by combining their data sources, they could solve both of their problems.",
                    "label": 0
                },
                {
                    "sent": "This means that each document should be associated with the corresponding knowledge graph entity and vice versa.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first question is to which entity would you associate the document?",
                    "label": 0
                },
                {
                    "sent": "We realized that documents and entities or descriptions of real world objects.",
                    "label": 1
                },
                {
                    "sent": "So naturally you'd want to associate those descriptions that are about the same object.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Having identified the problem and sketched out, the solution should look like.",
                    "label": 0
                },
                {
                    "sent": "They now face the challenge of how do you actually solve this problem?",
                    "label": 1
                },
                {
                    "sent": "In this talk, I'm going to present a method that makes use of three properties.",
                    "label": 0
                },
                {
                    "sent": "First, there is no natural way to compare entities or to documents.",
                    "label": 1
                },
                {
                    "sent": "So we need to have a format in which documents and entities can both be represented.",
                    "label": 0
                },
                {
                    "sent": "Of course, such a representation only makes sense if descriptions of the same object are represented in a similar way.",
                    "label": 0
                },
                {
                    "sent": "And Additionally we can make use.",
                    "label": 0
                },
                {
                    "sent": "Of similarities of descriptions with within one source.",
                    "label": 0
                },
                {
                    "sent": "So that we can use the neighborhood in order to find to infer new alignments.",
                    "label": 0
                },
                {
                    "sent": "I will now discuss how we make use of these three properties in detail.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a common representation, we chose vector spaces.",
                    "label": 0
                },
                {
                    "sent": "This is particularly common in computer science and is useful in this task because there are various ways to compare vectors to each other, like the Euclidean or cosine distance.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we need a way to convert our original sources into that vector space.",
                    "label": 0
                },
                {
                    "sent": "For that we have embedding methods.",
                    "label": 0
                },
                {
                    "sent": "Embedding models learn this vector space or over one source such that relations in the corpus are also reflected in the vector space.",
                    "label": 0
                },
                {
                    "sent": "For example, documents that have similar wordings should also have similar vectors.",
                    "label": 1
                },
                {
                    "sent": "Technically speaking.",
                    "label": 0
                },
                {
                    "sent": "Embedding methods define a loss function that captures the structure of the input dependent on a latent embedding vector.",
                    "label": 0
                },
                {
                    "sent": "This loss function defines an optimization problem, and by solving the optimization problem you produce the embedding vectors.",
                    "label": 0
                },
                {
                    "sent": "In our case we used Doctor Back for the document side.",
                    "label": 0
                },
                {
                    "sent": "Talk to work uses word Co.",
                    "label": 0
                },
                {
                    "sent": "Occurrence is dependent on the document.",
                    "label": 0
                },
                {
                    "sent": "And transit for knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "Transit is based on the idea that a relation in the Knowledge Graph is translation in the embedding space from the head entity to the tail entity embedding.",
                    "label": 0
                },
                {
                    "sent": "So this gives us a representation that already reflects similarity of items in your source.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "If you if you compute these embedding models independently of each other.",
                    "label": 0
                },
                {
                    "sent": "They likely will not group related documents and entities into the same region.",
                    "label": 0
                },
                {
                    "sent": "This is becausw.",
                    "label": 0
                },
                {
                    "sent": "Entity embedding embedding models learn the structure of the input.",
                    "label": 0
                },
                {
                    "sent": "So if you if you systematically omit a relation like correspondences between sources from the input, then this will also not be reflected in your embeddings.",
                    "label": 0
                },
                {
                    "sent": "2nd, if you compute if you run an embedding model twice on the same data, you will very likely end up with different embeddings and this is big cause this optimization problem can have multiple solutions and the learning being a stochastic process, you can end up in different solutions.",
                    "label": 0
                },
                {
                    "sent": "For these two reasons.",
                    "label": 0
                },
                {
                    "sent": "We need to somehow exchange information between the two models and also train them at the same time.",
                    "label": 0
                },
                {
                    "sent": "So we propose a model that we called coding.",
                    "label": 0
                },
                {
                    "sent": "Which does this via regular.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Azatian our model builds on top of two existing embedding models, one for each domain.",
                    "label": 1
                },
                {
                    "sent": "And we further assume a number of entity documents training pairs.",
                    "label": 0
                },
                {
                    "sent": "So first we make the current entity embeddings known in the document embedding model.",
                    "label": 1
                },
                {
                    "sent": "And with the regularizer, add a penalty to the document embeddings to this proportional to their distance to the corresponding entity embeddings.",
                    "label": 1
                },
                {
                    "sent": "So we now train the document embedding for a number of steps, which effectively makes a document embedding similar to related document embeddings, but also similar to their corresponding entity embeddings.",
                    "label": 0
                },
                {
                    "sent": "The procedure is repeated in the other way and also here we train the knowledge base embedding for a number of steps.",
                    "label": 0
                },
                {
                    "sent": "The procedure goes back and forth until the embeddings have stabilized.",
                    "label": 0
                },
                {
                    "sent": "Now this is the third of our properties.",
                    "label": 0
                },
                {
                    "sent": "Giving us a representation that also captures similarity across sources.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Max thinks that's cool.",
                    "label": 0
                },
                {
                    "sent": "What can I use this representation for my documents?",
                    "label": 0
                },
                {
                    "sent": "A typical task that he might want to achieve this side of document classification.",
                    "label": 0
                },
                {
                    "sent": "So we build a number of classifiers.",
                    "label": 0
                },
                {
                    "sent": "To take embeddings as input.",
                    "label": 0
                },
                {
                    "sent": "And predict whether or not a document belongs to a certain class.",
                    "label": 0
                },
                {
                    "sent": "And on the on the Y axis we see the accuracy statistics over all the classifiers.",
                    "label": 0
                },
                {
                    "sent": "So we trained the this classifier once using code embeddings and once using the document embeddings from the document from the Doctor Work model.",
                    "label": 1
                },
                {
                    "sent": "Our hypothesis is that we can achieve this alignment task without breaking the characteristics of the underlying embedding models, and indeed we see.",
                    "label": 0
                },
                {
                    "sent": "That's called a even achieves slightly higher performance than the plain embeddings.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So many things that's cool.",
                    "label": 0
                },
                {
                    "sent": "But can I use this representation for my knowledge graph?",
                    "label": 1
                },
                {
                    "sent": "A typical task she would want to achieve is that of predicting missing links.",
                    "label": 0
                },
                {
                    "sent": "So we tested the link prediction performance again once using called embeddings and once using embeddings trained on the document on the Knowledge Graph, only with the transition model.",
                    "label": 1
                },
                {
                    "sent": "The mean rank indicates how many incorrect triples were favored over the true testing triple, lower mean drank is better, and we see that both models achieve about the same performance.",
                    "label": 0
                },
                {
                    "sent": "The hit smasher indicate how many ranks were below 10.",
                    "label": 0
                },
                {
                    "sent": "So higher hits score is better, and again both models have about the same performance.",
                    "label": 0
                },
                {
                    "sent": "They repeated the experiments on two more knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "These graphs are sparser Dan FP15K.",
                    "label": 0
                },
                {
                    "sent": "And we observed that while in general the link prediction performance becomes worse if the graphs become sparser, the effect of called A is more pronounced.",
                    "label": 0
                },
                {
                    "sent": "So how about?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not alignment task.",
                    "label": 0
                },
                {
                    "sent": "We tested this by predicting which document corresponds to an entity and vice versa.",
                    "label": 0
                },
                {
                    "sent": "If the line if the alignment accuracy is 1, it means that all the predictions were perfect.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if the accuracy or.",
                    "label": 0
                },
                {
                    "sent": "The accuracy is 0.5.",
                    "label": 0
                },
                {
                    "sent": "This means that we were no better than a random prediction.",
                    "label": 0
                },
                {
                    "sent": "We first confirmed the hypothesis that independently computed model models are incomparable.",
                    "label": 0
                },
                {
                    "sent": "With it so by learning a document and the Knowledge Graph model separate from each other and then projecting one embedding space onto the other.",
                    "label": 0
                },
                {
                    "sent": "And indeed we see that the performance is only marginally higher than the random baseline.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, using called embeddings.",
                    "label": 0
                },
                {
                    "sent": "We achieve significantly higher alignment accuracy.",
                    "label": 1
                },
                {
                    "sent": "Even if you start with a router, low number of training alignments, the performance is already quite relevant, and if you increase the number of training data then of course it gets better.",
                    "label": 1
                },
                {
                    "sent": "Also, in this experiment we used the other two knowledge graphs and again see that the density of knowledge graph has an impact on the alignment precision.",
                    "label": 0
                },
                {
                    "sent": "Our intuition here is that in sparser graphs, you can make less use of the neighborhood and of potentially regularised neighbors of an entity.",
                    "label": 0
                },
                {
                    "sent": "So income.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solution.",
                    "label": 0
                },
                {
                    "sent": "I've shown that we can learn embeddings jointly with the coding method.",
                    "label": 0
                },
                {
                    "sent": "And with that achieve a common representation over two sources such that we can compare items within each source, but also items across those sources.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, our work is not complete.",
                    "label": 0
                },
                {
                    "sent": "The main limitation of embedding models and therefore also of our approach is that you cannot easily incorporate new embeddings and new or changing data into existing embeddings.",
                    "label": 0
                },
                {
                    "sent": "So for example, once you learn the embeddings with the coding method and you change the Knowledge graph, then you have to essentially re learn all of the embeddings.",
                    "label": 0
                },
                {
                    "sent": "So work on this would be greatly appreciated.",
                    "label": 0
                },
                {
                    "sent": "Alternatively, we could think of embedding different formats.",
                    "label": 0
                },
                {
                    "sent": "There has been some work done on image embeddings and it would be interesting to plug these into a Cardiff framework and then for example align documents to images or entities to images.",
                    "label": 0
                },
                {
                    "sent": "Similarly, we limited our work to exactly two sources.",
                    "label": 0
                },
                {
                    "sent": "But this limitation could maybe be lifted and.",
                    "label": 0
                },
                {
                    "sent": "Alignment between 3:00 or even more sources, like for example documents, entities, and images, could be interesting approach.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you're interested in the full details of the methods, please read and cite our paper.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention and I can now take your questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you my dears.",
                    "label": 0
                },
                {
                    "sent": "So to represent both entities and documents you use the.",
                    "label": 0
                },
                {
                    "sent": "I guess some kind of normal to norm or whatever.",
                    "label": 0
                },
                {
                    "sent": "In the end, it's a scalar right that you used the difference document amending minus entity embedding, right?",
                    "label": 0
                },
                {
                    "sent": "Have you tried?",
                    "label": 0
                },
                {
                    "sent": "I don't know mean or difference on the embeddings themselves, since the dimension is the same right the embedding dimension.",
                    "label": 0
                },
                {
                    "sent": "Or any other operator binary operator that combines both embeddings.",
                    "label": 0
                },
                {
                    "sent": "That's the first question, and the second question is.",
                    "label": 0
                },
                {
                    "sent": "So you emphasize regularised multitask learning, but?",
                    "label": 1
                },
                {
                    "sent": "Regularization, per say, is too.",
                    "label": 0
                },
                {
                    "sent": "Restate the impulse problem right when you have too many features and you don't have enough samples, so in this case, what is the role of regularization?",
                    "label": 0
                },
                {
                    "sent": "And if you used any other technique, what would have changed?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So to answer the first question.",
                    "label": 0
                },
                {
                    "sent": "We focused on on distance measures.",
                    "label": 0
                },
                {
                    "sent": "Also, since we have.",
                    "label": 0
                },
                {
                    "sent": "Exactly two sources.",
                    "label": 0
                },
                {
                    "sent": "That seems to be the most natural.",
                    "label": 0
                },
                {
                    "sent": "To your second question, the regularization.",
                    "label": 0
                },
                {
                    "sent": "Essentially introduces some penalty on the parameter you use.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We used regularization in that sense that we add an additional constraint to an additional at two in existing embedding model and that constraint depends on.",
                    "label": 0
                },
                {
                    "sent": "On the second source.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much for the very nice presentation.",
                    "label": 0
                },
                {
                    "sent": "Not only the content but also the form was really very nice.",
                    "label": 0
                },
                {
                    "sent": "My question would have been but you're already insert insert it degradation between images and document.",
                    "label": 0
                },
                {
                    "sent": "This is really a very nice topic.",
                    "label": 0
                },
                {
                    "sent": "I was wondering how did you do this for English or do you expect also with different language to have the same level of?",
                    "label": 0
                },
                {
                    "sent": "Relation between the entity world and the document world.",
                    "label": 0
                },
                {
                    "sent": "So we were working on a exclusively English corpus.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "It would also be interesting to compare.",
                    "label": 0
                },
                {
                    "sent": "Embedding models trains with like 2 document embedding models but trained on different languages.",
                    "label": 1
                },
                {
                    "sent": "I think this was already at least to some extent done by just computing models independently and then trying to map those, and I think this works quite well if the languages have similar pattern in.",
                    "label": 0
                },
                {
                    "sent": "So for example, Doctor Beck uses Word Co occurrence.",
                    "label": 0
                },
                {
                    "sent": "So if the statistics is similar in both languages, then I think.",
                    "label": 0
                },
                {
                    "sent": "Easier approaches work quite well.",
                    "label": 0
                },
                {
                    "sent": "Yeah I would like to make a quick one at your recent doctor back here, which seems to be the obvious option because in the end you are trying to compare documents with with entities in a in a in a knowledge base and thinking of have you considered using other final clarity types of embeddings like oriented towards or?",
                    "label": 0
                },
                {
                    "sent": "So in a way that you could reuse a very large corpora, which is already available out there could give you additional additional performance in this case.",
                    "label": 0
                },
                {
                    "sent": "So we have experimented with multiple models on the Knowledge Graph side.",
                    "label": 0
                },
                {
                    "sent": "On the document side, we used Doctor Vac since it's an obvious choice and was considered as.",
                    "label": 0
                },
                {
                    "sent": "A state of the art approach.",
                    "label": 0
                },
                {
                    "sent": "Would be interesting is with.",
                    "label": 0
                },
                {
                    "sent": "In the meantime, done some experiments on comparing DOC.",
                    "label": 0
                },
                {
                    "sent": "Two VEC to simply averaged word embeddings or some some of word embeddings, and it looks like the Suns or averages also work quite well, so this would be very interesting because it would to some extent solve the problem that you.",
                    "label": 0
                },
                {
                    "sent": "If your document corpus changes.",
                    "label": 0
                },
                {
                    "sent": "Alright, since the vert distribution of the vocabulary is less likely to change flexibility, yes exactly OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Materials list.",
                    "label": 0
                },
                {
                    "sent": "Thank Matthews again.",
                    "label": 0
                }
            ]
        }
    }
}