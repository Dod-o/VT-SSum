{
    "id": "ad525hg2gvxv2litiqqeeq2lbtaaeouq",
    "title": "Bayesian Quadrature: Model-based Approximate Integration",
    "info": {
        "author": [
            "David Kristjanson Duvenaud, Department of Computer Science, University of Toronto"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science",
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_duvenaud_bayesian_quadrature/",
    "segmentation": [
        [
            "So basically today I'm going to introduce patient quadrature, but also.",
            "Talk about some connections to the frequency slider that let us put bounds on its error.",
            "Show some cool things that you can do with that.",
            "I've never heard anyone mention before.",
            "And also I'm going to be talking a lot about pathologies with using Gaussian processes everywhere and offer a few examples of how we can go beyond that so."
        ],
        [
            "So just briefly, I just have to introduce the problem we're trying to solve, which is that we're going to be integrating some function against some input distribution.",
            "And we're going to assume that this function is expensive enough that it's worthwhile doing a little bit more analysis than sampling in this scenario, but.",
            "We can say that most of the problems we care about machine learning actually correspond to computing integrals at least information.",
            "So just computing expectations, marginal distributions, integrating new spammers and computing normalization constants all can be expressed in this form.",
            "And of course, once we can be normalization constants, we can do model comparison."
        ],
        [
            "Well.",
            "Obviously, the workhorse that everyone uses today for these methods is exampling, where we just.",
            "Take some points according to this input distribution.",
            "Evaluate the function at that location and then compute the empirical."
        ],
        [
            "This might be not the best thing we can do for two reasons."
        ],
        [
            "One is that any particular example from this input distribution is going to over represent some regions of density an under represent some other parts of density just by chance.",
            "So like this is actually the first sample that I took, the first set of samples I took from this question.",
            "You see that actually missing a huge part of the mode."
        ],
        [
            "The other thing is that in general we are actually integrating somewhat smooth functions, which means that nearby function values will be similar.",
            "The great thing about sampling is it actually makes no assumptions about the function really, except that it's a function.",
            "But because we're actually usually integrating through functions, we should be taking advantage of that."
        ],
        [
            "So.",
            "So instead of just randomly bunching up examples model based integration and quasi Monte Carlo methods more generally, they spread out the examples to hopefully achieve faster convergence to the true mean."
        ],
        [
            "Alright.",
            "So now I'm just going to briefly introduce model based integration."
        ],
        [
            "Yes so.",
            "Are we going to do is put a prior decision on Fr function?",
            "So in black, here's the true function.",
            "In green we have our GP.",
            "Prior represents mean and standard deviation and here's our input distribution.",
            "And our posterior over F actually implies a posterior over the integral Z.",
            "Here in the case of adoption process prior over ZI mean over F the posterior over Z is actually."
        ],
        [
            "As well.",
            "And now as we evaluate the function at some different locations, the posterior."
        ],
        [
            "Over F is going to converge and then with it the pasta."
        ],
        [
            "Over Z is also going to converge.",
            "And."
        ],
        [
            "Yeah."
        ],
        [
            "Is."
        ],
        [
            "I don't know.",
            "Really sensible way of doing things on the face of it.",
            "Just for this talk, will call using a GP power information quadrature."
        ],
        [
            "Alright.",
            "So I'm just going to briefly introduce what the actual estimator that, or rather what the posterior distribution over Z is when implied by a Gaussian process prior and F. So just like Monte Carlo methods, the expected expectation of the mean is actually linear in the function evaluations.",
            "With the weights given by this this vector here."
        ],
        [
            "And when we're choosing examples, it's natural to like to minimize uncertainty over Z will be minimizing the variance which has this form."
        ],
        [
            "Here's like the first pathology of GPS, which is that the variance of the integral actually doesn't depend on the function values at all.",
            "And it actually can only shrink when we observe a new value as well."
        ],
        [
            "Anne.",
            "So I just want to set this later.",
            "If we choose samples one at a time to minimize our variance, that's called sequential Bayesian quadrature."
        ],
        [
            "So there's a lot of really cool things you can do.",
            "Once you have a model over the function that you're integrating, so the first one is that you can actually encode some interesting prior knowledge, like for instance, if I knew that my function is symmetric, like for instance.",
            "If I'm doing some mixtures of Gaussians model and I have some invariants to label switching.",
            "Then I'll have a symmetry of this form, and I can actually encode that with really simple trick where I just take some base kernel and sum up the kernel, but swapping all pairs of its inputs actually implies a. Gaussian process posterior.",
            "That actually is always symmetric, so this is like a really cheap way to encode.",
            "Sort of fairly complicated symmetries.",
            "I mean, you can extend this to more than just two variables pretty easily."
        ],
        [
            "We did ingredients which I think has been mentioned.",
            "And we can use the posterior variance as a convergence diagnostic if we believe our model.",
            "So this is an example from an experiment I was doing where I was actually computing the marginal likelihood of my model.",
            "And watching the variance of my estimate of that uncertainty shrink.",
            "And of course, here's another one of the policies of using Gaussian processes everywhere is that if I'm using Bayesian quadrature to compute model evidences.",
            "The Goshen Goshen posterior can't help but put a lot of mass on negative model evidences, which is totally nonsensical."
        ],
        [
            "Alright.",
            "Other things you can do is.",
            "You can actually compute the likelihood of the GP that you're using to model your function, so you can use that to check the assumptions that you've made about the function.",
            "Or you can just use it to learn the kernel parameters, which is what people usually do.",
            "And this is something that I've never heard people talk about.",
            "Is that you can actually compute marginal?"
        ],
        [
            "Pretty easily in closed form.",
            "Just as implied by the Gaussian process posterior.",
            "So here's a simple example where I have some function that evaluated at five points.",
            "And then I just evaluated the marginal distribution integrated against some Gaussian prior and you can see I mean sensibly its variance is low where the examples are inside where there's no samples.",
            "And.",
            "It's kind of funny because even though the valuations are noiseless, the mean of the marginals actually doesn't go through the examples.",
            "You have to think about that for a little to you convince yourself why that's correct."
        ],
        [
            "The other thing we can do is actually to just going to take a little bit of setup.",
            "We can parameterise are likely to function with some nuisance parameter that we don't want to put into the GP.",
            "And now after we chosen some samples, we can reevaluate our likelihood with different values of this parameter so that we have some samples and I would just changing the likelihood function.",
            "Using the Bayesian quadrature weights too.",
            "Recompute are integral and this is kind of nice because.",
            "It gives us a.",
            "Well, for instance, if we use it to estimate marginal likelihood, here's just a toy example where we actually know the true nuisance hyperparameter.",
            "We can say that we have this uncertainty about the true value, but we also have.",
            "This model uncertainty about.",
            "The shape of the marginal likelihood function.",
            "So the idea is that by continuing to example from a function, the error bars on this function constraint to 0, but of course.",
            "They say this function here is going to tell us how much the data narrows down the posterior over the nuisance hyperparameter.",
            "So there's two different types of insert."
        ],
        [
            "T. I mean usually when people plot when they're trying to examine their posteriors here, when they're running, MCMC is some sort of histogram.",
            "And this is really nice, right?",
            "Because there's no getting involved.",
            "It's a smooth histogram and it has error bars that vary depending on the location.",
            "So I mean, maybe we still don't believe our model, but I think that this would be just a huge win over MCMC if we could replace histograms with these nice smooth functions that have."
        ],
        [
            "Bars so.",
            "Alright, so now I'm going to talk about some links that were recently shown to the frequentist literature that actually let us say something about the rates of convergence of Bayesian quadrature when assumptions are true and.",
            "It's bound on its convergence rate.",
            "So.",
            "The first thing is that there's an equivalent actually between the posterior variance of a Gaussian process and the worst case error.",
            "In the same arc HS as defined by your kernel with the unit norm.",
            "I kind of want to gloss over that a little bit.",
            "But basically there's a frequentist method called kernel hurting that attempts to choose points such to minimize this worst case bound while still keeping the even waiting of the Monte Carlo estimate.",
            "It was kind of a big deal that.",
            "Well, they proved to be wrong, but they found that it had they proved that they thought they proved it had 1 / N convergence rate as supposed to the one over square root of convergence rate of Monte Carlo methods.",
            "And proved to be wrong, but empirically we do observe something like 1 / N convergence.",
            "Now we can do things like we can take the function locations chosen by.",
            "By hurting and rewrite them by the Beijing Quadrates and get a faster rate of convergence.",
            "When the."
        ],
        [
            "Assumptions basically that our function is drawn from RKS is true."
        ],
        [
            "And then finally we compare this against the rate of convergence when we're actually doing Bayesian quadrature and using that to choose the weights sequentially as well.",
            "We tried to find out exactly exact closed form of this rate of convergence.",
            "I mean, it looks exponential.",
            "This is a log log plot.",
            "I'm kind of hoping that someone here can point me towards some literature, because I bet this has been proven in the 70s.",
            "But anyway, this is a fantastically fast rate of convergence.",
            "I mean, basically you can't make these slots for very long because eventually they is not very long before they bottom out at new machine precision so.",
            "OK, so hurting is just.",
            "Choosing points sequentially such that will minimize the expected variance, or rather worst case bound is the same thing.",
            "But when we're forced to wait all examples evenly just says in Monte Carlo.",
            "Net."
        ],
        [
            "So the only difference is how we choose the example locations so.",
            "Maybe it's kind of funny.",
            "Thing to measure but.",
            "Comparing hurting Salvation quadrature, there's two different things.",
            "Changing one is that we're waiting the points differently.",
            "Any others that were choosing the locations differently as well.",
            "So this is just trying to disentangle which one of those is helping us, and it turns out both so.",
            "So just as a sanity check, we also looked at the empirical error rates for these different estimators on functions drawn from the RKS defined by our kernel, and reassuringly they matched the same shape as the bounds."
        ],
        [
            "We can also do a sanity check and try to draw functions by some arbitrary method that are not actually exactly in the RHS that we're integrating, and again, reassuringly the rates of convergence sort of have the same order, at least in roughly the same shape.",
            "Although well."
        ],
        [
            "But the coolest result here is that because the posterior variance is equivalent to this worst case bound, we can actually put a well.",
            "It implies that there's a tight closed form bound on the error of vision.",
            "Quadrant estimator.",
            "Thing is that we have to know dark Age that we're in and the norm of function in that whole space, which is something we never know.",
            "But I mean, I think personally that most bounds.",
            "Are only useful for intuition, but I also think it's kind of refreshing that we can put it down on the aerovision estimator so."
        ],
        [
            "Anyway.",
            "So now I'm going to change tracks a bit and talk about Bayesian quadrature for inference, and this is the domain when.",
            "Our function is actually a likelihood function.",
            "And if we compute our expectation of likely function with tradition will be competing in the model evidence and.",
            "Any other marginal or quantity we want is going to integrate against this likelihood function so.",
            "It's probably worth trying to suss out and try to get basic water working in this domain.",
            "The thing is that."
        ],
        [
            "There's actually this really highlights a lot of the pathologies of using Gaussian processes, at least for this function class.",
            "Things like this are always positive.",
            "The other thing about this is that they spend a lot of time being very close to zero and a small amount of time being very far from zero.",
            "And there's just no way to parameterise a GP to capture this sort of heavy tailed behavior.",
            "Here's a simple example that I've actually chosen to be sort of like really kind to GPS, where it's like it's actually a very smooth likelihood function, and let's just say we've evaluated at these three points.",
            "Here's the posterior over of a GP condition on these points, and basically it's saying there's nothing really going on."
        ],
        [
            "Function is fairly flat.",
            "We can imagine now taking the log of this function, but it's just a question PDF, so the log of it is a quadratic.",
            "And now you know these tiny differences near 0 actually amplified, and we have this function that actually I think would probably be pretty well."
        ],
        [
            "My GP it's quadratic, although there is a minor detail that details go down to minus Infinity, but we'll just ignore that for now.",
            "So here's the poster on the log.",
            "That posterior actually."
        ],
        [
            "Plies appear in the regional space, which is unfortunately named Alog posterior.",
            "Unfortunately, because it's actually the expletive GP.",
            "Anyway, this posterior actually has like a lot of really great properties for modeling likely functions.",
            "I mean, first of all you can see that over here the uncertainty in the log is actually really high, but because the range is only like Y to the one to each of the minus one.",
            "The posterior variance is actually very small here, so basically saying I'm uncertain about the log likelihood function, but that doesn't matter because the X with anything small is going to be small.",
            "And then the small like.",
            "Small amount of variance at the peak of this function is actually amplified because you know the difference between each of the four in each of either is actually.",
            "Pretty significant."
        ],
        [
            "Of course, I don't know if you guys have ever tried to plot a likelihood function is actually really bad idea, because usually you just getting exactly 0 and then some spike at like 10 to the 100 and then exactly 0.",
            "So I can actually plot that and still have a central plot, but I'm going to have even more realistic likelihood function which is more Peaky."
        ],
        [
            "And we can see that actually that becomes even more pathological.",
            "I mean, if we're not in this mode, the GPS basically going to say that there's nothing going on anywhere at all."
        ],
        [
            "And it's kind of yeah.",
            "Very little mass, right?",
            "Well, no, this is the likelihood function.",
            "Let's pretend that our priorities actually like are the input decisions like uniform in this space or something like that?",
            "Yeah.",
            "So they all have even less.",
            "This example.",
            "So far we're not actually doing quadrature, which trying to model likelihood functions.",
            "Right?",
            "Anyway.",
            "Right, although there's ample, might start off somewhere here, although it would be looking at the log likelihood, and so it would be able to move pretty quickly to this region, I think.",
            "Well, I mean.",
            "The expectation of the likelihood with respect to the in participation is the model evidence, which is something we might be interested in, I mean.",
            "We can also like propose more interesting questions that have the same form, but this is just the simplest way to introduce it.",
            "Anyway, this small differences are actually, like, you know, the difference is actually getting smaller here, but there also."
        ],
        [
            "Large in log space and."
        ],
        [
            "Now the LGP posterior over here is actually has a huge dynamic range.",
            "And actually, something really funny is going on over here where.",
            "In the log space I plotted the mean plus minus 2 standard deviations, but the marginals of LGP posterior are lognormal and that's a non symmetric distribution.",
            "So I plotted the third 97 quantiles instead.",
            "And here the mean actually escapes the 97 tile and it goes off to like 10 to the 10.",
            "And this is kind of neat, right?",
            "Like to finally have a distribution that has a distinct mean, median and mode, like being a student of Carlos.",
            "I like never see this ever, and this is really amazing to me.",
            "Hung anyway.",
            "I mean basically what I'm saying is that almost certainly the function is very small here, but there's a small chance that it's going to be something that's going to be extremely high, so.",
            "Variance of our model evidence is actually not never going to be small until we check whether there is actually a mode here.",
            "That's basically what our model is saying.",
            "Alright, so I hope I hope I've convinced you that the log GP posterior is our model is a much better model of likely functions than the Jeep."
        ],
        [
            "But I mean.",
            "The thing is, we introduced the problem we're trying to solve in the 1st place, right?",
            "Because there's no closed form for the integral under our spectation under a large posterior."
        ],
        [
            "So on Tuesday me and Mike and.",
            "Roman introduced just like 1/2 to try to address this where we evaluated the.",
            "Mean of the GP on the log at a few extra points.",
            "Took the Expo.",
            "Those plus half their variance.",
            "And then fit a GP to these inducing points.",
            "Plus we also took a linear relation to give it this model and more sensible variance estimate and this."
        ],
        [
            "Cooked up example that actually matches pretty well the posterior.",
            "So the basic idea is that we have the original bad GP model, but it's been given hints."
        ],
        [
            "From the good model right?",
            "And this actually works pretty well.",
            "Not to mention, but in high dimensions there's no way that we can tell the whole space with these inducing points, so it's not clear how to move on from here, but.",
            "We've sort of been the bulletin, said well.",
            "We're not going to use GPS anymore, but this is going to cause us to need an extra round of integration on our surrogate model.",
            "So one thing is that like this circuit model will never be able to capture the behavior on."
        ],
        [
            "This previous slide where the mean goes outside of the variance, right?"
        ],
        [
            "It just has to have one or the other."
        ],
        [
            "So that's not my content for today.",
            "So just in conclusion, model based integration lets us through active learning about the integrals we're trying to compute instead of running Markov chains.",
            "And require a lot fewer samples in MCMC.",
            "And also it lets us check the assumptions that we're making on the.",
            "Unlike."
        ],
        [
            "Functions.",
            "It has really nice convergence properties if its assumptions are correct."
        ],
        [
            "But for inference, those assumptions are not correct at all and.",
            "You know, we could use a better model, but other models intractable.",
            "One thing I should mention is this actually implications for the frequency literature because people use kernel hurting to do inference, and now we can say, well, we can actually kind of show that this RKS assumption is kind of bogus for likelihood functions.",
            "So maybe there could be like log kernel hurting or something."
        ],
        [
            "So I just wanted to raise a few questions.",
            "One is that.",
            "You know, or rather a few points, so there's this, you know, there's been a bunch of work on innovation culture, but still it only really works in like less than 10 dimensions when the function is fairly smooth and when F is expensive enough to make this worthwhile."
        ],
        [
            "And it's not quite clear how to extend to high dimension.",
            "I mean usually in Monte Carlo when we have high dimensional problems with something like Hamiltonian Monte Carlo which exploits the gradient information and this really works really well.",
            "But indeed dimensions just conditioning on a gradient is actually going to correspond.",
            "Adding the extra observations to our GP.",
            "So we could take some subset of those, but it's not really clear how to do that."
        ],
        [
            "And so the other thing I wanted to raise is that it seems unlikely that we're actually going to be able to find another distribution as nice as GPS and.",
            "So maybe we should just bite the bullet and accept that we're going to need a second round of integration under some surrogate model like the GP.",
            "And this is kind of like in Bayesian optimization where there's no closed form for the maximum of GP.",
            "We still use them, but then we still have to do optimization on the circuit function.",
            "And I think that we've kind of been sort of like there's a siren, right?",
            "This closed form integral of the GP that sort of made us just not want to move beyond them, but I think that once we do, we'll have this whole world of different models that we can actually use.",
            "Although they also have to be able to be approximately integrated as well, so."
        ],
        [
            "Anyway, I love this really depends on how expensive F is and the setting of the problem and.",
            "There may be some principled way that we can ask, like how much overhead is worthwhile, right?",
            "Like when it's cheapest to take home examples or when should we?",
            "Have fancy integration and there's been some work on bounded rationality like it's very early days by Stuart Russell that I think in the long run will inform this sort of Beijing numerical analysis."
        ],
        [
            "That's all, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically today I'm going to introduce patient quadrature, but also.",
                    "label": 0
                },
                {
                    "sent": "Talk about some connections to the frequency slider that let us put bounds on its error.",
                    "label": 0
                },
                {
                    "sent": "Show some cool things that you can do with that.",
                    "label": 0
                },
                {
                    "sent": "I've never heard anyone mention before.",
                    "label": 0
                },
                {
                    "sent": "And also I'm going to be talking a lot about pathologies with using Gaussian processes everywhere and offer a few examples of how we can go beyond that so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just briefly, I just have to introduce the problem we're trying to solve, which is that we're going to be integrating some function against some input distribution.",
                    "label": 0
                },
                {
                    "sent": "And we're going to assume that this function is expensive enough that it's worthwhile doing a little bit more analysis than sampling in this scenario, but.",
                    "label": 0
                },
                {
                    "sent": "We can say that most of the problems we care about machine learning actually correspond to computing integrals at least information.",
                    "label": 0
                },
                {
                    "sent": "So just computing expectations, marginal distributions, integrating new spammers and computing normalization constants all can be expressed in this form.",
                    "label": 1
                },
                {
                    "sent": "And of course, once we can be normalization constants, we can do model comparison.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Obviously, the workhorse that everyone uses today for these methods is exampling, where we just.",
                    "label": 0
                },
                {
                    "sent": "Take some points according to this input distribution.",
                    "label": 0
                },
                {
                    "sent": "Evaluate the function at that location and then compute the empirical.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This might be not the best thing we can do for two reasons.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is that any particular example from this input distribution is going to over represent some regions of density an under represent some other parts of density just by chance.",
                    "label": 0
                },
                {
                    "sent": "So like this is actually the first sample that I took, the first set of samples I took from this question.",
                    "label": 0
                },
                {
                    "sent": "You see that actually missing a huge part of the mode.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other thing is that in general we are actually integrating somewhat smooth functions, which means that nearby function values will be similar.",
                    "label": 1
                },
                {
                    "sent": "The great thing about sampling is it actually makes no assumptions about the function really, except that it's a function.",
                    "label": 0
                },
                {
                    "sent": "But because we're actually usually integrating through functions, we should be taking advantage of that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So instead of just randomly bunching up examples model based integration and quasi Monte Carlo methods more generally, they spread out the examples to hopefully achieve faster convergence to the true mean.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So now I'm just going to briefly introduce model based integration.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes so.",
                    "label": 0
                },
                {
                    "sent": "Are we going to do is put a prior decision on Fr function?",
                    "label": 0
                },
                {
                    "sent": "So in black, here's the true function.",
                    "label": 0
                },
                {
                    "sent": "In green we have our GP.",
                    "label": 0
                },
                {
                    "sent": "Prior represents mean and standard deviation and here's our input distribution.",
                    "label": 0
                },
                {
                    "sent": "And our posterior over F actually implies a posterior over the integral Z.",
                    "label": 1
                },
                {
                    "sent": "Here in the case of adoption process prior over ZI mean over F the posterior over Z is actually.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "And now as we evaluate the function at some different locations, the posterior.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over F is going to converge and then with it the pasta.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over Z is also going to converge.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Really sensible way of doing things on the face of it.",
                    "label": 0
                },
                {
                    "sent": "Just for this talk, will call using a GP power information quadrature.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to briefly introduce what the actual estimator that, or rather what the posterior distribution over Z is when implied by a Gaussian process prior and F. So just like Monte Carlo methods, the expected expectation of the mean is actually linear in the function evaluations.",
                    "label": 1
                },
                {
                    "sent": "With the weights given by this this vector here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when we're choosing examples, it's natural to like to minimize uncertainty over Z will be minimizing the variance which has this form.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's like the first pathology of GPS, which is that the variance of the integral actually doesn't depend on the function values at all.",
                    "label": 0
                },
                {
                    "sent": "And it actually can only shrink when we observe a new value as well.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So I just want to set this later.",
                    "label": 0
                },
                {
                    "sent": "If we choose samples one at a time to minimize our variance, that's called sequential Bayesian quadrature.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's a lot of really cool things you can do.",
                    "label": 1
                },
                {
                    "sent": "Once you have a model over the function that you're integrating, so the first one is that you can actually encode some interesting prior knowledge, like for instance, if I knew that my function is symmetric, like for instance.",
                    "label": 0
                },
                {
                    "sent": "If I'm doing some mixtures of Gaussians model and I have some invariants to label switching.",
                    "label": 0
                },
                {
                    "sent": "Then I'll have a symmetry of this form, and I can actually encode that with really simple trick where I just take some base kernel and sum up the kernel, but swapping all pairs of its inputs actually implies a. Gaussian process posterior.",
                    "label": 0
                },
                {
                    "sent": "That actually is always symmetric, so this is like a really cheap way to encode.",
                    "label": 0
                },
                {
                    "sent": "Sort of fairly complicated symmetries.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can extend this to more than just two variables pretty easily.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We did ingredients which I think has been mentioned.",
                    "label": 0
                },
                {
                    "sent": "And we can use the posterior variance as a convergence diagnostic if we believe our model.",
                    "label": 1
                },
                {
                    "sent": "So this is an example from an experiment I was doing where I was actually computing the marginal likelihood of my model.",
                    "label": 0
                },
                {
                    "sent": "And watching the variance of my estimate of that uncertainty shrink.",
                    "label": 1
                },
                {
                    "sent": "And of course, here's another one of the policies of using Gaussian processes everywhere is that if I'm using Bayesian quadrature to compute model evidences.",
                    "label": 0
                },
                {
                    "sent": "The Goshen Goshen posterior can't help but put a lot of mass on negative model evidences, which is totally nonsensical.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Other things you can do is.",
                    "label": 1
                },
                {
                    "sent": "You can actually compute the likelihood of the GP that you're using to model your function, so you can use that to check the assumptions that you've made about the function.",
                    "label": 0
                },
                {
                    "sent": "Or you can just use it to learn the kernel parameters, which is what people usually do.",
                    "label": 0
                },
                {
                    "sent": "And this is something that I've never heard people talk about.",
                    "label": 0
                },
                {
                    "sent": "Is that you can actually compute marginal?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty easily in closed form.",
                    "label": 0
                },
                {
                    "sent": "Just as implied by the Gaussian process posterior.",
                    "label": 0
                },
                {
                    "sent": "So here's a simple example where I have some function that evaluated at five points.",
                    "label": 0
                },
                {
                    "sent": "And then I just evaluated the marginal distribution integrated against some Gaussian prior and you can see I mean sensibly its variance is low where the examples are inside where there's no samples.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's kind of funny because even though the valuations are noiseless, the mean of the marginals actually doesn't go through the examples.",
                    "label": 0
                },
                {
                    "sent": "You have to think about that for a little to you convince yourself why that's correct.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other thing we can do is actually to just going to take a little bit of setup.",
                    "label": 0
                },
                {
                    "sent": "We can parameterise are likely to function with some nuisance parameter that we don't want to put into the GP.",
                    "label": 0
                },
                {
                    "sent": "And now after we chosen some samples, we can reevaluate our likelihood with different values of this parameter so that we have some samples and I would just changing the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Using the Bayesian quadrature weights too.",
                    "label": 0
                },
                {
                    "sent": "Recompute are integral and this is kind of nice because.",
                    "label": 0
                },
                {
                    "sent": "It gives us a.",
                    "label": 0
                },
                {
                    "sent": "Well, for instance, if we use it to estimate marginal likelihood, here's just a toy example where we actually know the true nuisance hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "We can say that we have this uncertainty about the true value, but we also have.",
                    "label": 0
                },
                {
                    "sent": "This model uncertainty about.",
                    "label": 0
                },
                {
                    "sent": "The shape of the marginal likelihood function.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that by continuing to example from a function, the error bars on this function constraint to 0, but of course.",
                    "label": 0
                },
                {
                    "sent": "They say this function here is going to tell us how much the data narrows down the posterior over the nuisance hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "So there's two different types of insert.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "T. I mean usually when people plot when they're trying to examine their posteriors here, when they're running, MCMC is some sort of histogram.",
                    "label": 0
                },
                {
                    "sent": "And this is really nice, right?",
                    "label": 0
                },
                {
                    "sent": "Because there's no getting involved.",
                    "label": 0
                },
                {
                    "sent": "It's a smooth histogram and it has error bars that vary depending on the location.",
                    "label": 0
                },
                {
                    "sent": "So I mean, maybe we still don't believe our model, but I think that this would be just a huge win over MCMC if we could replace histograms with these nice smooth functions that have.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bars so.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now I'm going to talk about some links that were recently shown to the frequentist literature that actually let us say something about the rates of convergence of Bayesian quadrature when assumptions are true and.",
                    "label": 1
                },
                {
                    "sent": "It's bound on its convergence rate.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The first thing is that there's an equivalent actually between the posterior variance of a Gaussian process and the worst case error.",
                    "label": 0
                },
                {
                    "sent": "In the same arc HS as defined by your kernel with the unit norm.",
                    "label": 0
                },
                {
                    "sent": "I kind of want to gloss over that a little bit.",
                    "label": 0
                },
                {
                    "sent": "But basically there's a frequentist method called kernel hurting that attempts to choose points such to minimize this worst case bound while still keeping the even waiting of the Monte Carlo estimate.",
                    "label": 0
                },
                {
                    "sent": "It was kind of a big deal that.",
                    "label": 0
                },
                {
                    "sent": "Well, they proved to be wrong, but they found that it had they proved that they thought they proved it had 1 / N convergence rate as supposed to the one over square root of convergence rate of Monte Carlo methods.",
                    "label": 0
                },
                {
                    "sent": "And proved to be wrong, but empirically we do observe something like 1 / N convergence.",
                    "label": 0
                },
                {
                    "sent": "Now we can do things like we can take the function locations chosen by.",
                    "label": 1
                },
                {
                    "sent": "By hurting and rewrite them by the Beijing Quadrates and get a faster rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "When the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assumptions basically that our function is drawn from RKS is true.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then finally we compare this against the rate of convergence when we're actually doing Bayesian quadrature and using that to choose the weights sequentially as well.",
                    "label": 0
                },
                {
                    "sent": "We tried to find out exactly exact closed form of this rate of convergence.",
                    "label": 1
                },
                {
                    "sent": "I mean, it looks exponential.",
                    "label": 0
                },
                {
                    "sent": "This is a log log plot.",
                    "label": 0
                },
                {
                    "sent": "I'm kind of hoping that someone here can point me towards some literature, because I bet this has been proven in the 70s.",
                    "label": 0
                },
                {
                    "sent": "But anyway, this is a fantastically fast rate of convergence.",
                    "label": 1
                },
                {
                    "sent": "I mean, basically you can't make these slots for very long because eventually they is not very long before they bottom out at new machine precision so.",
                    "label": 1
                },
                {
                    "sent": "OK, so hurting is just.",
                    "label": 0
                },
                {
                    "sent": "Choosing points sequentially such that will minimize the expected variance, or rather worst case bound is the same thing.",
                    "label": 0
                },
                {
                    "sent": "But when we're forced to wait all examples evenly just says in Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Net.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the only difference is how we choose the example locations so.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's kind of funny.",
                    "label": 0
                },
                {
                    "sent": "Thing to measure but.",
                    "label": 0
                },
                {
                    "sent": "Comparing hurting Salvation quadrature, there's two different things.",
                    "label": 0
                },
                {
                    "sent": "Changing one is that we're waiting the points differently.",
                    "label": 0
                },
                {
                    "sent": "Any others that were choosing the locations differently as well.",
                    "label": 0
                },
                {
                    "sent": "So this is just trying to disentangle which one of those is helping us, and it turns out both so.",
                    "label": 0
                },
                {
                    "sent": "So just as a sanity check, we also looked at the empirical error rates for these different estimators on functions drawn from the RKS defined by our kernel, and reassuringly they matched the same shape as the bounds.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also do a sanity check and try to draw functions by some arbitrary method that are not actually exactly in the RHS that we're integrating, and again, reassuringly the rates of convergence sort of have the same order, at least in roughly the same shape.",
                    "label": 0
                },
                {
                    "sent": "Although well.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the coolest result here is that because the posterior variance is equivalent to this worst case bound, we can actually put a well.",
                    "label": 0
                },
                {
                    "sent": "It implies that there's a tight closed form bound on the error of vision.",
                    "label": 0
                },
                {
                    "sent": "Quadrant estimator.",
                    "label": 0
                },
                {
                    "sent": "Thing is that we have to know dark Age that we're in and the norm of function in that whole space, which is something we never know.",
                    "label": 0
                },
                {
                    "sent": "But I mean, I think personally that most bounds.",
                    "label": 0
                },
                {
                    "sent": "Are only useful for intuition, but I also think it's kind of refreshing that we can put it down on the aerovision estimator so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to change tracks a bit and talk about Bayesian quadrature for inference, and this is the domain when.",
                    "label": 1
                },
                {
                    "sent": "Our function is actually a likelihood function.",
                    "label": 0
                },
                {
                    "sent": "And if we compute our expectation of likely function with tradition will be competing in the model evidence and.",
                    "label": 0
                },
                {
                    "sent": "Any other marginal or quantity we want is going to integrate against this likelihood function so.",
                    "label": 0
                },
                {
                    "sent": "It's probably worth trying to suss out and try to get basic water working in this domain.",
                    "label": 0
                },
                {
                    "sent": "The thing is that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's actually this really highlights a lot of the pathologies of using Gaussian processes, at least for this function class.",
                    "label": 0
                },
                {
                    "sent": "Things like this are always positive.",
                    "label": 0
                },
                {
                    "sent": "The other thing about this is that they spend a lot of time being very close to zero and a small amount of time being very far from zero.",
                    "label": 0
                },
                {
                    "sent": "And there's just no way to parameterise a GP to capture this sort of heavy tailed behavior.",
                    "label": 0
                },
                {
                    "sent": "Here's a simple example that I've actually chosen to be sort of like really kind to GPS, where it's like it's actually a very smooth likelihood function, and let's just say we've evaluated at these three points.",
                    "label": 0
                },
                {
                    "sent": "Here's the posterior over of a GP condition on these points, and basically it's saying there's nothing really going on.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function is fairly flat.",
                    "label": 0
                },
                {
                    "sent": "We can imagine now taking the log of this function, but it's just a question PDF, so the log of it is a quadratic.",
                    "label": 0
                },
                {
                    "sent": "And now you know these tiny differences near 0 actually amplified, and we have this function that actually I think would probably be pretty well.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My GP it's quadratic, although there is a minor detail that details go down to minus Infinity, but we'll just ignore that for now.",
                    "label": 0
                },
                {
                    "sent": "So here's the poster on the log.",
                    "label": 0
                },
                {
                    "sent": "That posterior actually.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plies appear in the regional space, which is unfortunately named Alog posterior.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, because it's actually the expletive GP.",
                    "label": 0
                },
                {
                    "sent": "Anyway, this posterior actually has like a lot of really great properties for modeling likely functions.",
                    "label": 0
                },
                {
                    "sent": "I mean, first of all you can see that over here the uncertainty in the log is actually really high, but because the range is only like Y to the one to each of the minus one.",
                    "label": 0
                },
                {
                    "sent": "The posterior variance is actually very small here, so basically saying I'm uncertain about the log likelihood function, but that doesn't matter because the X with anything small is going to be small.",
                    "label": 0
                },
                {
                    "sent": "And then the small like.",
                    "label": 0
                },
                {
                    "sent": "Small amount of variance at the peak of this function is actually amplified because you know the difference between each of the four in each of either is actually.",
                    "label": 0
                },
                {
                    "sent": "Pretty significant.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, I don't know if you guys have ever tried to plot a likelihood function is actually really bad idea, because usually you just getting exactly 0 and then some spike at like 10 to the 100 and then exactly 0.",
                    "label": 0
                },
                {
                    "sent": "So I can actually plot that and still have a central plot, but I'm going to have even more realistic likelihood function which is more Peaky.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can see that actually that becomes even more pathological.",
                    "label": 0
                },
                {
                    "sent": "I mean, if we're not in this mode, the GPS basically going to say that there's nothing going on anywhere at all.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's kind of yeah.",
                    "label": 0
                },
                {
                    "sent": "Very little mass, right?",
                    "label": 0
                },
                {
                    "sent": "Well, no, this is the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Let's pretend that our priorities actually like are the input decisions like uniform in this space or something like that?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So they all have even less.",
                    "label": 0
                },
                {
                    "sent": "This example.",
                    "label": 0
                },
                {
                    "sent": "So far we're not actually doing quadrature, which trying to model likelihood functions.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "Right, although there's ample, might start off somewhere here, although it would be looking at the log likelihood, and so it would be able to move pretty quickly to this region, I think.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean.",
                    "label": 0
                },
                {
                    "sent": "The expectation of the likelihood with respect to the in participation is the model evidence, which is something we might be interested in, I mean.",
                    "label": 0
                },
                {
                    "sent": "We can also like propose more interesting questions that have the same form, but this is just the simplest way to introduce it.",
                    "label": 0
                },
                {
                    "sent": "Anyway, this small differences are actually, like, you know, the difference is actually getting smaller here, but there also.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Large in log space and.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the LGP posterior over here is actually has a huge dynamic range.",
                    "label": 0
                },
                {
                    "sent": "And actually, something really funny is going on over here where.",
                    "label": 0
                },
                {
                    "sent": "In the log space I plotted the mean plus minus 2 standard deviations, but the marginals of LGP posterior are lognormal and that's a non symmetric distribution.",
                    "label": 0
                },
                {
                    "sent": "So I plotted the third 97 quantiles instead.",
                    "label": 0
                },
                {
                    "sent": "And here the mean actually escapes the 97 tile and it goes off to like 10 to the 10.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of neat, right?",
                    "label": 0
                },
                {
                    "sent": "Like to finally have a distribution that has a distinct mean, median and mode, like being a student of Carlos.",
                    "label": 0
                },
                {
                    "sent": "I like never see this ever, and this is really amazing to me.",
                    "label": 0
                },
                {
                    "sent": "Hung anyway.",
                    "label": 0
                },
                {
                    "sent": "I mean basically what I'm saying is that almost certainly the function is very small here, but there's a small chance that it's going to be something that's going to be extremely high, so.",
                    "label": 0
                },
                {
                    "sent": "Variance of our model evidence is actually not never going to be small until we check whether there is actually a mode here.",
                    "label": 0
                },
                {
                    "sent": "That's basically what our model is saying.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I hope I hope I've convinced you that the log GP posterior is our model is a much better model of likely functions than the Jeep.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I mean.",
                    "label": 0
                },
                {
                    "sent": "The thing is, we introduced the problem we're trying to solve in the 1st place, right?",
                    "label": 0
                },
                {
                    "sent": "Because there's no closed form for the integral under our spectation under a large posterior.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So on Tuesday me and Mike and.",
                    "label": 0
                },
                {
                    "sent": "Roman introduced just like 1/2 to try to address this where we evaluated the.",
                    "label": 0
                },
                {
                    "sent": "Mean of the GP on the log at a few extra points.",
                    "label": 1
                },
                {
                    "sent": "Took the Expo.",
                    "label": 0
                },
                {
                    "sent": "Those plus half their variance.",
                    "label": 0
                },
                {
                    "sent": "And then fit a GP to these inducing points.",
                    "label": 1
                },
                {
                    "sent": "Plus we also took a linear relation to give it this model and more sensible variance estimate and this.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cooked up example that actually matches pretty well the posterior.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that we have the original bad GP model, but it's been given hints.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the good model right?",
                    "label": 0
                },
                {
                    "sent": "And this actually works pretty well.",
                    "label": 0
                },
                {
                    "sent": "Not to mention, but in high dimensions there's no way that we can tell the whole space with these inducing points, so it's not clear how to move on from here, but.",
                    "label": 0
                },
                {
                    "sent": "We've sort of been the bulletin, said well.",
                    "label": 0
                },
                {
                    "sent": "We're not going to use GPS anymore, but this is going to cause us to need an extra round of integration on our surrogate model.",
                    "label": 0
                },
                {
                    "sent": "So one thing is that like this circuit model will never be able to capture the behavior on.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This previous slide where the mean goes outside of the variance, right?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It just has to have one or the other.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's not my content for today.",
                    "label": 0
                },
                {
                    "sent": "So just in conclusion, model based integration lets us through active learning about the integrals we're trying to compute instead of running Markov chains.",
                    "label": 1
                },
                {
                    "sent": "And require a lot fewer samples in MCMC.",
                    "label": 0
                },
                {
                    "sent": "And also it lets us check the assumptions that we're making on the.",
                    "label": 0
                },
                {
                    "sent": "Unlike.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions.",
                    "label": 0
                },
                {
                    "sent": "It has really nice convergence properties if its assumptions are correct.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But for inference, those assumptions are not correct at all and.",
                    "label": 1
                },
                {
                    "sent": "You know, we could use a better model, but other models intractable.",
                    "label": 0
                },
                {
                    "sent": "One thing I should mention is this actually implications for the frequency literature because people use kernel hurting to do inference, and now we can say, well, we can actually kind of show that this RKS assumption is kind of bogus for likelihood functions.",
                    "label": 0
                },
                {
                    "sent": "So maybe there could be like log kernel hurting or something.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I just wanted to raise a few questions.",
                    "label": 0
                },
                {
                    "sent": "One is that.",
                    "label": 0
                },
                {
                    "sent": "You know, or rather a few points, so there's this, you know, there's been a bunch of work on innovation culture, but still it only really works in like less than 10 dimensions when the function is fairly smooth and when F is expensive enough to make this worthwhile.",
                    "label": 1
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's not quite clear how to extend to high dimension.",
                    "label": 1
                },
                {
                    "sent": "I mean usually in Monte Carlo when we have high dimensional problems with something like Hamiltonian Monte Carlo which exploits the gradient information and this really works really well.",
                    "label": 1
                },
                {
                    "sent": "But indeed dimensions just conditioning on a gradient is actually going to correspond.",
                    "label": 0
                },
                {
                    "sent": "Adding the extra observations to our GP.",
                    "label": 0
                },
                {
                    "sent": "So we could take some subset of those, but it's not really clear how to do that.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the other thing I wanted to raise is that it seems unlikely that we're actually going to be able to find another distribution as nice as GPS and.",
                    "label": 1
                },
                {
                    "sent": "So maybe we should just bite the bullet and accept that we're going to need a second round of integration under some surrogate model like the GP.",
                    "label": 1
                },
                {
                    "sent": "And this is kind of like in Bayesian optimization where there's no closed form for the maximum of GP.",
                    "label": 0
                },
                {
                    "sent": "We still use them, but then we still have to do optimization on the circuit function.",
                    "label": 0
                },
                {
                    "sent": "And I think that we've kind of been sort of like there's a siren, right?",
                    "label": 0
                },
                {
                    "sent": "This closed form integral of the GP that sort of made us just not want to move beyond them, but I think that once we do, we'll have this whole world of different models that we can actually use.",
                    "label": 0
                },
                {
                    "sent": "Although they also have to be able to be approximately integrated as well, so.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyway, I love this really depends on how expensive F is and the setting of the problem and.",
                    "label": 0
                },
                {
                    "sent": "There may be some principled way that we can ask, like how much overhead is worthwhile, right?",
                    "label": 1
                },
                {
                    "sent": "Like when it's cheapest to take home examples or when should we?",
                    "label": 0
                },
                {
                    "sent": "Have fancy integration and there's been some work on bounded rationality like it's very early days by Stuart Russell that I think in the long run will inform this sort of Beijing numerical analysis.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's all, thanks.",
                    "label": 0
                }
            ]
        }
    }
}