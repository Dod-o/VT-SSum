{
    "id": "tvhninrbpgipckhefnccysm5hsogqf27",
    "title": "Sparse Methods for Machine Learning: Theory and Algorithms",
    "info": {
        "author": [
            "Francis R. Bach, INRIA - SIERRA project-team"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips09_bach_smm/",
    "segmentation": [
        [
            "Thanks for the introduction.",
            "So first before I start I would like to give a very special thanks to the students and postdocs who have helped me preparing those slides, in particular hydrogen atoms, ramirel and Gusinsky.",
            "Also, So what I will try to do today is to give you an overview of the of these hot topic of sparse methods for machine learning and I will try and as much as I can to work to talk about the work of others.",
            "But as young as said, sometimes it's hard to do so and you always try to snip in some of your own work, which I will do and with limited amounts.",
            "I also have been doing a lot of reading in the last two or three weeks, but I may have forgotten someone.",
            "So if you don't see yourself yourself on the slides, don't get mad at me and it does not mean that you don't do good work on sparse methods, so let me."
        ],
        [
            "Start by giving an overview before I go more into the details.",
            "So this is very usual setting in machine learning, at least for supervised learning.",
            "You have input data X belonging to some input space capital X, so I'm not used to manipulating these little hands, so I will try to a point towards the screen with the hand.",
            "So this is a capital X will be any kind of input space, and you want to predict function Y value Y and half of machine learning is considering this.",
            "Empirical risk minimization framework.",
            "When you minimize an objective function which will try to, so we want to optimize with respect to a function from X to Y.",
            "Give me an X.",
            "We want to predict Y Ann, you have a data fitting term error on data points which is the sum of all you data points of the loss between your true value Yi annual prediction effort XI, an regularization term which is usually a norm or squared norm.",
            "There's a lot of issues associated with this framework, and the main two issues are which loss should you choose for which type of Y, and this.",
            "I won't consider this to mention this tutorial will assume that this is solved other if you use least square regression.",
            "If Yi is a real number, or if you use a support vector machine or logistic if you use.",
            "Why I being discreet?",
            "Just keep value the number.",
            "So I will mainly focus on this.",
            "The choice of the function space and the norm.",
            "Now we consider various types of norm and here you have two."
        ],
        [
            "The goal of adding a norm, of course, is to avoid overfitting, and now you have two main lines of works in this setting.",
            "You have to camp the first camp, the one of Acadian norms, or Henderson norms, and this is essentially the camp of kernel methods.",
            "And the good thing is, it allows you to do a non linear predictor."
        ],
        [
            "Well and also the theory at least as far as I'm concerned, is well developed and you have a lot of algorithms which are available for those type of of regularizer.",
            "So second, the camp, the camp of sparsity inducing norms.",
            "So here usually as a strategy, is very strict.",
            "If two linear predictors.",
            "So you want to predict from X as a linear function of X.",
            "And of course the main example is the L1 norm, which is just the sum of the absolute values.",
            "So here's the cool thing is that it does two things at the same time.",
            "It will both avoid overfitting and also create some zeros into your estimator estimator.",
            "W so it does both model selection and regularisation."
        ],
        [
            "And here this is a mental picture.",
            "The tutorial, the theory, the theoretical part and algorithmic work is really in progress and this is the topic of this tutorial.",
            "So the first aspect of recover if you aspect of algorithms.",
            "So which one?",
            "Which one is faster is L2 or L1.",
            "So here this is a picture taken from the Statistics Journal and kindly given to me by Steven.",
            "Can you see the Gaussian her against the Laplacian toys?",
            "So Gauss was in favor of L2.",
            "And Laplace was in favor of L1 and sits at one is not differentiable.",
            "We will see that it might look at.",
            "It is harder to optimize.",
            "So this is the lesson toys, and since L2 usually leads to nice smooth convex optimization problems, where if you're a square loss you get just a linear system is supposed to be easier and this is a Gaussian her.",
            "And of course which.",
            "Which one wins?",
            "This is adult toys OK, and we will see.",
            "So first, it's nice for two things because Laplace is French.",
            "An even better Laplace.",
            "What the Professor article number supplier.",
            "So you see that it's really important to peek at my school and this will be achieved by essentially first of the methods we will we will see at the same complexity for L1L2.",
            "And also I think also overtopping methods which can use the fact that you're going to get this power solution and we will get that into the details we will get into the details of that."
        ],
        [
            "Then we go a bit over the theoretical results associated with with those type of regularizer and that will mainly focus on two types of results.",
            "The 1st result will be on support recovery and innocence.",
            "In this setting.",
            "I think theory was kind of late with respect to applications so people have been using L1 norms for awhile and will always promise that it does two things.",
            "It will prevent overfitting and select good zeros and it's been only recently that people have really analyzed.",
            "How which zeros are we actually getting?",
            "Do we get the correct zeros and what I will show you if that you get necessary?"
        ],
        [
            "Condition for exact support recovery an it involves the covariance matrix of your covariates Q and essentially if you remove these parts, it says that to get the correct support you need Q to have low correlation between variables and we see that it is both a positive result.",
            "It will tell you when you don't get the good support.",
            "So to me a very negative one in a sense that signature data often correlated.",
            "This shows that we never get."
        ],
        [
            "Support then we would get to the second aspect, which is the high dimensional inference where people try to derive results which say that you can still predict well if the number of features P is at most exponential in the number of observations.",
            "So throughout this tutorial P will be the number of features and and will be the number of observations.",
            "Then we go over trying to go beyond the lasso because I personally think that are playing the lesser directly is almost never useful.",
            "You know, saying that you will you only have a few linear problems in machine learning, and you really want to be able to go beyond that, and I will consider first nonlinearities through adding kernels."
        ],
        [
            "Then trying to see if you can actually deal with exponentially many features.",
            "Can you really run an algorithm where log P = N and then I will consider the topic of sparse learning on matrices.",
            "So in terms of features, so theory is promising that we could get good results in terms of predictive performance.",
            "If log P is at most equal to N. But what happens when N = 1000?",
            "Theory says in the sense."
        ],
        [
            "That you could deal with the exponential 1000 features, so it's kind of a lot if you want to run an algorithm with that, and I will see how you can use structure within the features to design algorithms that can be run in polynomial time in N even."
        ],
        [
            "So you are implicitly considering exponentially many features, and this will be evolving.",
            "Norms which are grouped with groups of fibers which may overlap.",
            "Then I would consider source path of matrices and this includes a lot of application of machine learning, multi task, multi class, matrix completion.",
            "Imagine Noising topic models and MF and I will mostly consider two types of positive low rank and what people often called sparse PCA or dictionary learning.",
            "So this will be the outline of the tutorial.",
            "The first part on the on the left so."
        ],
        [
            "With the algorithms and the theoretical results an one parts of steel on vectors where we have to consider a structured sparse methods and finally Xpath methods and matrices.",
            "So before I start to go into the details, I think we've all seen that Slice is going to be pretty pretty quick.",
            "So why at one norm leads to sparse solutions, an one?"
        ],
        [
            "Simple way to do that is to consider the constraint problem.",
            "So first, first throughout this talk I will either penalized by the element nor more constrained by the element norm.",
            "This is equivalent in the sense that for any T where you constrain there's a Lambda for which you can penalize by Lambda times normal W. OK, so I will go back and forth between those two.",
            "And of course the mapping from London T is not depends on data.",
            "OK, that's why I say equivalent between between quotes.",
            "So why does everyone?",
            "Normally sparsity is here.",
            "I've plotted the level sets of the quadratic function.",
            "OK, so good is to minimize.",
            "So if you do unconstrained minimization you will end up at the center of the... and of course will be constrained to the L1 bowl.",
            "And then everyone vote is simply a square like that.",
            "So you see that depending on where the cratic function is located, we get no zeros over there.",
            "You will get to.",
            "The optimum is obtained when the red curve touches the Blue Square.",
            "Here you get no sparsity, but since you have corners in the urban body, will be attracted to the corners.",
            "And this is exactly the point which way equal to 0.",
            "So this is a simple geometric intuition Y L1 norm lead to sparsity.",
            "OK, so did you see the part that we can?"
        ],
        [
            "Weather for the next half hour.",
            "So so here you assume linear linear model where we have XYZ, often referred to as covariates and why I as responses.",
            "So here I will leave script Capital Y and specifying so it can be real numbers or discrete values and so they will consider this.",
            "Regularize problem, we have a sum of an error on data plus regularizer.",
            "So here are we not include a constant term B because it just makes life harder, which is.",
            "This can be easily added an again it can be either using a penalized formulation like that or constraint formulation.",
            "So when you have the square loss, this has been used a lot in mainly two communities.",
            "In signal processing this is called basis pursuit.",
            "Anesthetic 6 in machine learning.",
            "It is called the lasso.",
            "So I will start first by."
        ],
        [
            "Leave a small review of nonsmooth convex analysis and convex optimization innocence.",
            "The album is non differentiable and this creates some technicalities and I will spend 15 minutes trying to deal with those in the simplest possible way, so we go over analysis which will how can we say that we are optimal and how we can optimize later, so there's a nice sequence of books related to those topics from Boyd and two which is a."
        ],
        [
            "Focused a lot on entire points to bonus it'll and also by Ann Lewis, which is a bit more related to analysis, so those are all good books to get more into this topic.",
            "So let's start with a simple things.",
            "First, when you have an L2 organizer, so we have the same the same term error on data.",
            "Plus now the square, the square of the L2 norm.",
            "This is the square of the L2 norm.",
            "So now this is a differentiable problem.",
            "If L is differentiable with respect to the second variable, you can compute the gradients.",
            "So if you simply the gradients for the loss plus the gradient for the regularizer, and of course to be optimal, you just said the gradient equal to 0.",
            "In the case of the square loss.",
            "So we can re parameterized a bit this loss function, where Y is just a vector with all values of.",
            "Why I?",
            "So it's a vector of size N and capital X is a design matrix which is N * P where all of the observations are stacked row by row.",
            "So this loss function is.",
            "Some of the losses of the order point."
        ],
        [
            "Whether the losses the square is simply can be done with this simple formula.",
            "So if you take the relative of that one ingredient, you get this value, and if you invert together usual normal equations.",
            "So this is when life is easy when you have the square loss and the square regularizer.",
            "So if you remember one thing about the tutorial, this is this one.",
            "The L1 norm is none, is not differentiable.",
            "So if you try to complete this gradient is bound to fail at one point.",
            "OK, so the good tools to do that you have two sets of."
        ],
        [
            "Tools with one of them is to consider directional derivatives, which I will looked at in this tutorial.",
            "Or you may do subgradient so you can go back and forth between those two, but it's important to present directional derivatives which I will do in the this line.",
            "So never try to compute the gradient of the element normal.",
            "You can always cook at some stuff when W is far from zero, but zero is kind of a problematic.",
            "So what you need to do is to consider in fact derivatives along all possible directions.",
            "OK, so you look at the direction Delta in RP and you take that up W an go along this direction and see the rate of change along that direction, which is exactly exactly that this thing, and this is exactly what people cause additional drive active in the direction of Delta.",
            "The good thing when J is convex continuous is that it always there always exists an why do we need to consider that essentially, in the nonsmooth optimization, you need to be able to consider all possible directions and not only P different ones, so the gradient is essentially looking at P different directions.",
            "With nonsmooth optimization, you need to look at all possible ones, and 1D it means looking at two OK left and right.",
            "Of course, when J is differentiable, then you have a strong link between directional derivatives and gradients, namely that the directional derivative is a linear function of your direction.",
            "So now how do we decide whether you optimal or not?",
            "This is simple since this is just the rate of change in direction Delta.",
            "If to optimal this rate of change would be going up so.",
            "For unconstrained minimization, your optimal, if only for all directions you go up, meaning that you look really go up, meaning that the directional derivative is positive for every directions 1st if the function is smooth, it does reduce to the zero gradient condition, because having a linear function being positive, let's be 0.",
            "So this is just safety."
        ],
        [
            "Check we get back zero gradients and for constant imitation I won't go over.",
            "It is just for reference.",
            "So now how does it look like for for the element norm so that you see is one of the few slides with the details and proofs.",
            "But I think this one is quite important, so if you get that point you get most of it.",
            "Not totally true, but a bit true.",
            "OK, so this is a loss plus."
        ],
        [
            "In that time, the ugly riser, so I will call L capital FW the loss.",
            "Which is a summer or data points and you need to so this one that will assume differentiable.",
            "OK, so when you need to compute the derivative, the first term is simply the gradients times Delta.",
            "Now for the norm, if you compute the rate of change that change along direction Delta, you immediately immediately see that you have to make a distinction between J index indices for which the value J zero and indices for which it is non 0.",
            "So when it is 0 essentially far away from zero.",
            "Your function is linear.",
            "OK, so it behaves like a linear function."
        ],
        [
            "But at zero when WJ Zero, then if you always get this positive term.",
            "So when you divide by epsilon and take the limit you will get nonlinear term of the directional derivative an which is happening here.",
            "OK, so now if you mix the L1 norm with the differential part you get to some so you get a separable Sone sort of the all possible Jay.",
            "So Jay goes from one 2P.",
            "And you have the differentiable part on the left, for which JWJ is non zero and you have the nondifferential part when double J is zero.",
            "OK, so now how do we get optimality conditions?",
            "W will be optimal if this is positive for all possible Delta.",
            "So since you have since this is separable in the Delta J for all J you have, you need all those terms to be to be positive.",
            "So for this one it means that this is 0 because this is a linear function which is positive only if it is always equal to 0 and you get this one.",
            "This has to be positive and this essentially require that this term is less than Lambda.",
            "In absolute value, why?",
            "Because if it is.",
            "Laser Lambda then this is this is always positive because it is always bigger than minus.",
            "The value is measured value is bigger than minus Lambda.",
            "You get a positive value.",
            "OK, so this is so if you think about it a few a few more seconds you will see that you need the absolute value of the gradient to be less than Lambda, and this is what you get then.",
            "You get at optimal if you're the gradient of the loss plus Lambda times a second WJ 0.",
            "For non 0 WJ and the gradient is smaller than Lambda when.",
            "When WJ zero OK, so this is optimistic conditions and for the square losses those can be specialized.",
            "Becausw gradient can be expressed.",
            "Easily, so throughout this tutorial, X sub J will essentially be the design matrix corresponding to variables which are indexed by J. OK, so X sub.",
            "Jay is just the data, suggests viable, so this is just a gradient of the loss like we saw in the previous slide for the square.",
            "For the square loss.",
            "OK, so it sees those are the automated conditions.",
            "So now this will tell you if your optimal or not, and now you have to see how do we get to those two those conditions, because those cannot be inverted with zero gradient and with the square loss the cool thing is that if you write down the optimality conditions you can invert the system and get a solution.",
            "Here this is not the case and usually iterative algorithms are needed, so I will hear focus first on smooth optimization just to give a review of existing results in that."
        ],
        [
            "Eating so we mainly focus on the gradient descent where you go from WT2WD, press 1 by stress going on the negative following the opposite direction of the gradient with a small parameter Alpha septi.",
            "So here we have a lot of strategies to do.",
            "To choose Alpha sub G and the one which I like.",
            "The other ones is to do a line search to, not an exact line.",
            "Search it because it is kind of useless to spend so many so much time optimizing the wrong function.",
            "But Lens search with the stopping rules like the Army jawan and also you can consider fixed diminishing step size where which we decay in as one of our key OK plus some constant.",
            "So those with those two line searches, yes, a strong body of literature trying to see how fast they do converge.",
            "How far they convert to the global minimum of the function.",
            "So here I assume that F is convex, so if there is no, there are no local minima.",
            "And if you look at the on the nice book by Nesterov, then you can really find convergence rates for all of these methods.",
            "So what the take home message is that.",
            "The nicer you function looks, the faster the convergence rate is for the same method.",
            "OK, so if X is convex and just widgets, so this is just.",
            "The simplest assumption you can make on the on the on the function.",
            "Then you converge to the to the best value at rate one of earth code FT, which is rather slow if now you assume that you are differentiable.",
            "So what people call but smooth with L Lipschitz every Lipschitz gradients.",
            "So for those who are not familiar to the lips gradients, it is as it essentially means that if your function was twice differentiable, it means that the second derivative is.",
            "Uniformly bounded from above by L. So if you assume that your function is differentiable, you know that you go faster an if you assume.",
            "Moreover, that the function is strongly convex.",
            "So essentially it means that if you function was twice differentiable, the secondary waves are bounded from below.",
            "This time then you get an exponential rate of convergence, so it goes.",
            "It goes a lot faster than the inverse of T. It goes as as as.",
            "Exponential function of T. So this number mu over L is exactly what people call the condition number of the optimization problem.",
            "Essentially, if this is small and for the code to zero gravity, Selby converge very slowly because you get back to this Lt World or M of the square root of the world.",
            "But if you knew over L is large then you get exponential convergence.",
            "So this is to meet very important because it will tell you when simple methods are expected to work.",
            "Unknot and this depends on the condition number, and again, if your function is twice differentiable, this is just the ratio of the minimal.",
            "Again, value of the Haitian over the maximal eigenvalue of the Hessian.",
            "Then for cognitive services.",
            "Similar types of property and just to mention that gradient decent is by no means the best possible methods.",
            "Best possible 1st order methods for optimization and you have also in the nice book by Nesterov we have a lot of other schemes which we retain a lot better bounds, often called as a, sometimes Nesterov Nesterov methods which we achieve rates which are quicker.",
            "BECAUSW T -- 2 goes quicker and square root.",
            "Of Of your N is larger than your.",
            "OK.",
            "So just keep in mind that gradient descent is not the end of the story.",
            "For now, for now, this works for smooth optimization.",
            "So now for nonsmooth optimization then you have no gradients, so you have the first simple method is to follow the."
        ],
        [
            "The gradients, so I didn't want to define some gradients, but innocence have to hear.",
            "So subgradient is simply.",
            "Jyoti subgradient, if it is below the if it is locali below the function.",
            "So if the giant drive 80 that's greater than the corresponding linear function.",
            "So in a sense it is a gradient where the function is differentiable and it is something which is tangent which defines a tangent when the function is not differentiable.",
            "So here you have to be very careful if you replace just gradient descent by submitting dissent, you might not converge even if you do the exact line search.",
            "So this is somewhat counter intuitive, and there's the contract sample in the slides.",
            "I won't go, I won't go over, but this is very important.",
            "Simple methods in the context of nonsmooth optimization might not work.",
            "Exactly exactly and search it doesn't work.",
            "Word for diminishing step size.",
            "It does convert to the global minimum, so it's kind of counter intuitive and they have no explanation why, but sometimes it works like this, sometimes it doesn't."
        ],
        [
            "Also coordinate dissent, which is also very simple methods.",
            "Mine is not always convergent to the global optimum.",
            "So let's give for this.",
            "For this simple counterexample.",
            "So it is a function of two variables though, that the level sets value 1234512345, so the function goes up in that direction.",
            "So if you want to do that, if you start from zero, if you look at along the two axes along W one, you go up.",
            "So you're you think you should stop if you look at along W2 you go up so you think we should stop.",
            "So any coordinate descent algorithm would stop at that point, but there's a direction.",
            "Direction of dissent, which is this one.",
            "So this is a very simple example that will tell everything you have to be able to look at all directions.",
            "Since coordinate descent does not.",
            "It might not be convergent to the global minimum."
        ],
        [
            "And also have funny stories with great laundry trying to optimize such a function at 4:00 AM before the deadline, and I can tell it does not converge.",
            "OK and OK so.",
            "And in terms of competence, wait, if you do regular subgradient is the same as gradient descent with no assumptions.",
            "OK, so this.",
            "Keep in mind that call that they sent is not always convergent, but I will say in two slides is that possible so it is convergent.",
            "OK, so it's not always convergent, but it is in some in some settings and for the last three days.",
            "So before I go on, this is a."
        ],
        [
            "I won't give it so last point, I think quite important for sparse methods, is that so true?",
            "The objective function for sparse problem is non differentiable, but it's not any non differentiable function.",
            "So if you assume the loss is differentiable you get an objective function which is of the form of the value which is differentiable plus Lambda times the norm which might not be differentiable.",
            "Or if you use the constraint version you get minimizer.",
            "You want to minimize.",
            "A convex differentiable function with the constraint.",
            "And if you assume that Alice moves an if on top of it, you assume that you know to project on the ball on the board defined by the norm or the board you find the dual norm.",
            "So if you don't know what to do or not is simply the dual norm of the L1 norm, is the L Infinity norm OK, and that we should be enough at the moment.",
            "So if you know how to project on the ball, define methanol and this we can do for the L1 bowl.",
            "This we can do for the error.",
            "City Bowl then you may use similar techniques than smooth optimization, which I won't go into in this tutorial for because I don't have the time to talk only about algorithms and simple ones are projected gradient descent.",
            "And also possible methods which I won't talk a lot.",
            "So just for reference, if you want to look at that and the good thing is that with those types of methods you get the similar convergence rates than for smooth optimization.",
            "Namely it will depend a lot on the condition, number of the loss function.",
            "OK, so if the loss function is well conditioned, it will converge quickly at exponential rates, and if it's not, we convert convert slowly at a rate of 1 / T. OK, so this is very important in terms of optimization.",
            "So now let's look at the lasso and consider."
        ],
        [
            "Simple algorithms for that.",
            "So the first one is causing a dissent.",
            "So I told you I told you that you should not use it, but in fact you can in this setting and the main reason is that the optimality conditions or separable.",
            "So if you go back here."
        ],
        [
            "This setting you look that if you optimize."
        ],
        [
            "Only along one viable.",
            "You want that to be true and you see that the old J, the condition for the variable J is independent from the condition from the other variables.",
            "And this is one of the reasons why coordinate dissent is convergent convergent with the.",
            "With the TL norm.",
            "OK, so we have another simple proof later, but this is so the good thing is that if you optimize with respect to one variable, this is exactly iterative thresholding because you optimize Yep.",
            "You mean linear case squared loss?",
            "In fact, so the if you have, I don't see your question.",
            "Shops OK, but if you're a non linear prediction or you not convex anymore and then I don't exactly know.",
            "If it is not separate, it will not converge.",
            "It might in some, in some settings, and in fact the same stories got it did on some simple you see that assets, but if you take another data set it doesn't converge.",
            "So sometimes it does, sometimes it doesn't and you don't want a method which might not be convergent.",
            "This is very important because it might get stuck and you don't notice it, which is a problem.",
            "So before I got one so good.",
            "Good thing about coordinate descent is that this is very simple to implement because we can optimize with one if you have one viable, you have to optimize the quadratic function of 1 variable plus an absolute value, and this can be done in closed form.",
            "And this is just iterative thresholding and I should example in the slides.",
            "So this is very simple methods.",
            "But here you have to keep in mind that this is a first order method, so if you if your covariance matrix.",
            "Is a as low correlations?",
            "It would be quite fast, but if you had a high correlation between you between your variables, which means that you have low condition number, that means that it might get very slow.",
            "Second type of technique.",
            "So I called at the ETA trick because this is how we thought to it in my group.",
            "And it is simply a method that will do it at iteratively squares.",
            "So if you take the L1 norm like that and you can get, you can see it as a minimum over any of the vector data.",
            "So it is a vector in RP, which is positive.",
            "So here if you optimize with respect to ETA here, the optimal ETA is exactly the square root of WJ.",
            "Know the absolute value of WJ and you get back the absolute value of WJ.",
            "So now if you want to optimize.",
            "With this term, you can iteratively optimize that time instead by updating ETA each iterations.",
            "So essentially you spend one time minimizing the expected ETA, and then you minimize respect to the value with better fix and those two problems are simple at 828 A, it is in close form and will rapidly W. Since we have turned the L1 norm into the L2 norm is supposed to be easier.",
            "But all those do not use a financial sparse and I think this is nice, but do prefer method that use sparsity and we know see how we can we can do that.",
            "So for that I will get 3 to the square loss because it allows it is a lot simpler."
        ],
        [
            "Were to present.",
            "So first something that we something which should never do is to write that as a quadratic program.",
            "OK, so is anyone nor can we return you can you can separate the positive part as a negative part and you get a nice quiet and ask you pee.",
            "OK quieting term with Lena or linear constraint so this is all nice but this if you use any generic toolbox this will be very very slow so never never do that.",
            "But for the square loss you have other stuff that you might use, and the main one, and it is that."
        ],
        [
            "If you know in advance the sign pattern of the solution, so if you know which variable should be positive, which ones should be negative and which one should be 0, then.",
            "That you know the solution in closed form.",
            "Wybie 'cause if you know which one should be 0 non 0 so J will be those ones then you can rewrite the objective.",
            "This is the loss.",
            "We just restrict two variables in J and replace the other one now but just.",
            "Linear function which depends on the on the sign.",
            "OK, if you do the sign you can do that.",
            "And of course this is a quadratic function which you can optimize in close form and you get exactly this simple equation.",
            "So if you give me the sign in advance, assign vector in advance, then you can simply get a solution.",
            "So of course we have a lot of time vectors.",
            "You have therapy difference and vector, so this is not.",
            "You still need to guess in some sense assigned vector and once we have the subject or you need to check whether you optimal or not.",
            "So how do you check whether you optimal or not?"
        ],
        [
            "So this just your candidate vector candidate sign vector, the nonzero pattern, which are sometimes called the support of your solution.",
            "And if you have that side vector, Union advance is going to be of that form.",
            "So now you have to check the two optimality conditions that I've described in an earlier in the tutorial.",
            "The first one at the sign of the Blue Jay is actually actually subject.",
            "OK, this is 1 condition and also that the gradient with respect to the inactive Bibles or lesser number.",
            "So these are two conditions you need.",
            "You need to check and now you have two strategies for that.",
            "Either you construct the said J iteratively, so you start from zero and keep adding viables until you can have that satisfying and the good thing is that it only requires a small number of linear systems, so to get WJ OK you need to invert to solve least least square problem reduced to Cardinal J."
        ],
        [
            "Viable, which is efficient OK?",
            "But the techniques, the technique I really like is a homotopy methods for the square loss.",
            "So this has been relevant in multiple times, and in fact the first one to do it was macovitz in 1956, then Osborne and then the last in 2004.",
            "And the idea is very simple if that is to notice that if is to get first, the goal is some of the goals are somewhat ambitious.",
            "You want to get not one solution for one number, but all solutions for all possible Lambda.",
            "And for that you just notice that if you know the sun vector South, this is a solution of your problem.",
            "It is a final Lambda, great, and it is valid as long as those two conditions are satisfied and the sign condition is also.",
            "Linear constraint the subgradient condition of the directional derivative condition is also something which is linear in Lambda.",
            "So at the end you get an interval of Lambda for each side vector you get an interval of Lambda which may be empty for which the solution is optimal for that for that Lambda.",
            "So at the end it means that you have a set of possible affine function for your problem.",
            "For each intervals of Lambda.",
            "So your solution WJ.",
            "The function of Lambda is piecewise.",
            "A fine, so we are sleeping the drug.",
            "The possibility of having non unique solutions so you can deal with that but I won't.",
            "I won't hear so now what you need is simply simply should be put to be put in quotes fine."
        ],
        [
            "In find break points in that piecewise affine function, and this is exactly what markovits and others have been doing for the last.",
            "53 years.",
            "OK, so how does it?",
            "I won't go into details because it's kind of boring just to show you how it looks like when you when you when you run the algorithm so abscessed you get Lambda OK and you get the weights over here.",
            "So I'm just showing this picture for the yellow.",
            "Can you see it?",
            "Yes.",
            "I'm just doing this for the yellow for the yellow wait.",
            "OK, so when you start with a large Lambda so everything is equal to 0, you select nobody.",
            "You regularize a lot and as soon as we use Lambda you start to add some variables.",
            "OK, so these ones and you see that you might.",
            "Somebody else might disappear from the active set.",
            "OK, so this is.",
            "Somewhat content if you might decide at one point that you should add the yellow viable and then decide that you should remove it.",
            "OK, so it's quite rare that variables come in and out, but it does happen, so This is why there is no currently no complexity bound for this method, because you can design examples for which all variables will come in and out many, many times, but in practice it took me 10 minutes to find such an example, but you think random matrices, so it's not.",
            "It's not so."
        ],
        [
            "So this is now going back to this gas in her lesson toys.",
            "This is with this homotopy method that you really see the difference.",
            "All those first order methods which are direct methods like coordinate descent.",
            "So what if you don't?",
            "They don't do sparsity directly, so you might be create some technicalities, but if you just apply them straight you get the same cost per iteration, which is all of times PRN P. The number of features and or the number of variables.",
            "But if use exact so exact mean solutions which rely on inverting the system.",
            "So which we have a precision of your machine precision usually and you see that there's a strong gain from P square or the L2 norm to KPN for the album."
        ],
        [
            "So this is just one of the reasons why the L1 norm does win.",
            "It uses the fact that if you stop at only KLT viables then you get a faster solution.",
            "Of course, if you need to select all of the variables you get back to P square N and there's no magic because at the end when not, they call 0, you get back the ordinary least square, so it must be peace caravan.",
            "When K = P. But many case since since you hope for you hope for sparsity, so you hope forecast model.",
            "This is so much efficient.",
            "So there are a lot of additional methods which I want to.",
            "I won't go into, namely possible methods, so optimal method in the sense for the Nesterov methods have been applied with this problem, and this makes some advertisement for for software from the group.",
            "So spam software essentially implementing all of these in C++ and MATLAB, and it's rather efficient as you will see in the later part of the tutorial where we are able to use that for image denoising."
        ],
        [
            "So no, I went over the convex optimization algorithms, so I'm already quite late.",
            "OK, so let's go over the theoretical results.",
            "So here you have, we will consider the square loss because life is easier with the square loss.",
            "So the main assumption that we will make it as a data or generated from a certain sparse W. So W bowl and it's supposed to be spa."
        ],
        [
            "We have a lot of zeros and K, but we will denote K the number of non zero values for that for that setting.",
            "So you have three main problems that people have been considering.",
            "First, the problem of regular consistency.",
            "You want W hat so W hat will be estimator obtained from the lasso over quick W had converted the value.",
            "Bold is regular consistency.",
            "You have model selection consistency.",
            "They get the correct zeros when the number of data points increase increases and also something which is more classical machine learning.",
            "You want your estimator to lead.",
            "Good predictions you don't care if you estimate the good zeros.",
            "You don't care if you're close to double what you want is predictions.",
            "We saw which are as good as the ones obtained by the valuable so show 2 main results.",
            "The first one, the first one, is condition."
        ],
        [
            "For model consistency and then the high dimensional inference.",
            "So I will assume that WS Pass an everyday note capital J Bolt J as a set of nonzero, the nonzero pattern.",
            "But people also called the support, and this is very nice support recovery condition for the lasso, which essentially say that the last two brigades, the correct science and hence the correct zeros if and only if you have that condition which is true.",
            "So Q here in the comments metrics JC is Jay.",
            "Compliment is a set of elect."
        ],
        [
            "Of 11 variables, the ones you want to remove.",
            "J other words you want to keep and assign a WJ to sign of the generating loading vector.",
            "WJ so here the take home message, at least for me, is that what's important is that term.",
            "That term is large when you have strong correlations between relevant and irrelevant variables and small otherwise.",
            "So essentially the Lasso finds the good pattern only if you have local relations.",
            "OK, this is, uh, maybe come back to that later.",
            "So small small nodes.",
            "So this has been done by many people.",
            "OK, seriously and then so first notice that the condition depend on WNJ, so this is of no practical value.",
            "OK, if you want to know in advance, but they're going to get a good pattern, you need to know the pattern in advance.",
            "OK, so is rather rather useless, but this being said, you can optimize.",
            "You can take the Max.",
            "The maximum problem over all possible signs of the value of W and you get the condition which only depends on the sparsity pattern.",
            "Still useless because you don't need.",
            "You don't need, you don't need you don't know it in advance, and you can also maximize outlet to the pattern.",
            "The Bolt pattern for the given size, then for all patterns of the given size.",
            "Then you will get the condition, which is a some practical significance.",
            "So again, it's valid in low and high dimensional setting, so one go I won't go too much over that, but.",
            "Look at high dimensions in later slides and also important."
        ],
        [
            "It does require a lower bound on the magnitude of the non 0 WJ.",
            "OK, so to be able to get a good support you need the non zero values to be not too small.",
            "So you need a good good gap between zero and non zero values.",
            "So all these say that essentially the lasso usually does not get the good zeros.",
            "OK, so to me this is more negative results and a positive result.",
            "And it reads it does select more variables and you can see this reference for more details.",
            "So now there have been an industry on fixing that.",
            "Uh, so OK.",
            "So that.",
            "So that's not good zeros then it doesn't.",
            "So how do you make it select the good zeros so you have a lot of work doing that, so I think there is one very nice work which is adaptive lasso which is related to concave."
        ],
        [
            "Panelization, so in the adaptive lasso you replace the L1 norm by a well known weighted by by the inverse of the weights obtained from a previous estimation.",
            "So you first start with maybe two angular eyes regularised estimate or L1 regularizer estimate 'cause that W hat and then do that to wait the element known.",
            "So this will have the effect.",
            "So if W hat is 0.",
            "So you will never allow the bridge being equal to 0 for the next estimation, so if you first estimator zeros, zeros will remain.",
            "And if Wi-Fi is small then you will push your W when you re estimate will put W to be 0.",
            "If your first W hat was close to zero.",
            "OK, so will just help the L1 norm.",
            "To overcome these difficulties.",
            "So this is adaptive lasso and very interesting part is that it can be framed.",
            "In terms of concave panelization.",
            "So here what you do when in concave penalization is to minimize the loss term.",
            "This won't change, and instead of minimizing convex sum of a convex function of the absolute value you consider concave function.",
            "So this has been done by people in signal processing and machine learning which are referred to here.",
            "So essentially you replace the L1 norm which we like this by.",
            "Square root for example here and intuitively, the square root is closer to the edge zero penalty.",
            "So what is the other opener T?",
            "It will be 0 and your zero and one as soon as you get away from zero, so will be something looking like like this.",
            "So the lasso is like that and the square root is like that.",
            "So you can see that it is closer to the zero penalty innocence.",
            "And the good thing is that first you can design it.",
            "Of course James Concave now, so it's not convex anymore, so you cannot get the global the global solution, but you have very simple concave convex procedures or people 'cause that minimization, majorization, or every community as its way of dealing with concave function.",
            "And the ways to bound the concave function by its linear problem.",
            "And the good thing is that if you take the linear upper bound and if you start at W. Had big W. Being only once, we exactly get back the L1 norm.",
            "OK, so essentially if you do this concave convex procedure with the with the square root for example and you start with W being equal to 1, you further so once and then you have to use linear or ban on the square root, which essentially is equivalent to consider Alpha equals 1/2 here.",
            "OK so you can see that the adaptive lasso is simply two stage optimization of concave.",
            "With the Comcast penalty.",
            "I think this is a very nice trick to make more zeros in your problem so you start with the law so you don't get enough zeros.",
            "You just rerun the last once or twice and you will create zeros where the last so was missing them, so I think."
        ],
        [
            "Earlier."
        ],
        [
            "Is.",
            "That's so big, so there's also.",
            "Work better with something by myself with my lack of time I will go goodbye."
        ],
        [
            "So first getting the correct support is nice and good, but did you actually actually care?",
            "OK, so the one big problem with all those methods and other theorems is that they need the nonzero variables to be none too small.",
            "So you need WJ to be bigger than the noise Times Square root of log P over North, so lower bound or the non zero values OK and more importantly it's not 'cause you get the wrong variables that you can't predict very well.",
            "Bad estimation of the support does not imply bad predictions.",
            "So what I will go over right now is to look at results which don't really care whether you get the correct zeros but only care about prediction.",
            "So this will involve involve Oracle inequalities.",
            "So what is?"
        ],
        [
            "Oracle inequality she's away, I understand it is that you assume you know the support in advance.",
            "Let's say you assume that the data was generated from loading vector board W with support birje.",
            "If you know about Jay, you can just do ordinary least square squares restricted to ball J and record that W Oracle.",
            "And you know there's the error made by this Oracle depends on the number of variables.",
            "In a in jail.",
            "So here it should be a bulging.",
            "OK, so the prediction error.",
            "The average prediction error made by the by the Oracle is simply goes as the number of non zero variables times N / N. So this shows that of course for this to work you need this number to this plan to read that to go to zero OK.",
            "So essentially what you try to achieve with this sparsity's path methods is to achieve the rate of convergence of this article.",
            "You cannot beat is hard to beat that one, and you cannot beat it and the goal is to go across to that optimal prediction rate.",
            "So before we go into the thread for the last.",
            "So I think a very important point is that let's look at the situation when you have infinite computational resources.",
            "Let's say you don't care about convexity and you can optimize any possible function.",
            "So essentially why people would do in statistics is consider.",
            "Approach is based on the penetration, like close to be icy or you want to minimize with respect to the to the support sets.",
            "So J is.",
            "Is a subset of 1 one P. This is what is?",
            "This is the best possible error you can make if you only use the variables in J. OK, so you take this is just the ordinary least square when you restrict to variables in Jay.",
            "This is the training error and lackenby ICU.",
            "Consider the training error plus some penetration term that will depend on the number of parameters, which is just capital.",
            "The Cardinal of J plus.",
            "Remove that guy is just that.",
            "I'm sloppy.",
            "So essentially very close to to be icy and the good thing is that setting that you can, if you could optimize with respect to capital J.",
            "So be very clear this requires to look at all possible subsets of variables.",
            "So acquires looking at to the P subsets.",
            "You get this very nice Oracle inequality which say that their order to make at the end is less than the constant times the best possible rate, which is just cover North where case and number of variables which generated your problem times log P of archaon.",
            "If K is more, is it just luck P. So here you see that and even better uses for Gaussian noise.",
            "Make simple but there is no assumptions are guarding correlations you don't need, you don't need.",
            "Your design matrix to be nice.",
            "You don't need your convenience to be a closed entity.",
            "It always always works in the sense this is what we want to achieve.",
            "OK, but we would like to achieve is those those types of results with the convex convex convex problems.",
            "So I start clear whether it is doable or not, but always keep in mind that this is what you want to achieve and we are very far with part methods to achieving it.",
            "OK, as you will see in the next few slides, will always add a condition on correlations will say that.",
            "We need only a small correlation between variables.",
            "OK, please keep in mind that."
        ],
        [
            "If you have time, meaning if you're willing to optimize over a set of size to the P, there is no need for all that and what we have is weaker than those types of results.",
            "At the other side of the spectrum, what's happening when you have orthogonal design?",
            "So let's take the simple as simple as possible setting where your design matrix X add orthogonal columns.",
            "So it means that the covariance matrix is identity and there your problem will take up all because your your cratic function which will define your loss will be a sum of keratic functions on individual variables and now you get exactly soft thresholding, why?",
            "'cause when you optimize quadratic function of the single viable like that which is about you would obtain an.",
            "Then the solution must be of that form.",
            "You take the value T, so if a zero the solution is T OK and if A is not zero then you will just soft threshold your value T. So if T is positive you move a or if she is larger than a you remove a.",
            "If T is smaller than minus a you add A and if T is smaller than a in absolute value.",
            "You put it to 0 so you see what people call service thresholding.",
            "If you're less than Angels, Zero.",
            "If not, you're shrunk towards 0.",
            "So this is essentially how people have been analyzing those methods.",
            "First, I think those methods."
        ],
        [
            "And this is where you can prove easily that the lasso will work.",
            "So let's look very quickly how it works.",
            "And here is just to give you an idea where to an idea of where does this log P come from.",
            "OK, so I don't want to give you a class on the cost of fashion inequality's it's going to be a bit too long, but why does it look come from?",
            "And if you look closely at the proofs and we only focus on that one, essentially the way I understand it may be wrong.",
            "Is that the log P comes from the fact that if you take P independent.",
            "Gaussian variables OK or maybe even not independent the maximum as expectation.",
            "Which role as square root of log P. OK, so this is the main reason why it can be because of the Union bound.",
            "But this will be too long to long to explain.",
            "But this is the log P is not.",
            "It's not magically, it comes from the expectation of the maximum of normally distributed variables.",
            "So if you look at the papers often is hidden, so mines approved.",
            "But this is the main reason why.",
            "OK, so now let's go."
        ],
        [
            "Down to the results with the Lasso before having a pose.",
            "First, the main result is that you only need K log P. You need only.",
            "Canopy can be needs to be smaller than N for to get good good creative performance.",
            "OK so this is the sort of scaling which had promised on the beginning of the tutorial, but for that you need you have two.",
            "You have two constraints are very important.",
            "First W the generating vector should be sufficient.",
            "Sufficiently sparse.",
            "OK should be spawned.",
            "This is already included included in the.",
            "In here, but input variable should also be not two correlated OK and this is very important if variables are correlated, there is no.",
            "There is no way we can prove consistency for the Lost soul.",
            "So now there's a whole industry of sufficient conditions for the lasso, so they start from very simple to very complex and will start for the very simple which just mutual encouragement."
        ],
        [
            "Coherence.",
            "So essentially so the next slide will be somewhat with the theorem, so which which look a bit complicated, but it's not in fact.",
            "So you assume that your data are generated from as a sparse in R model plus some noise where the noises ID normal.",
            "You assume that the covariance matrix, so as you need diagonal and cross terms which are less than 1 / 1 / K * A constant, and then what you could show that if you select that Lambda you get with high probability, because this will be.",
            "This will be small, you get that error error that you make is less time is less than Sigma times the square root of log P. / N OK.",
            "So this so this result can be derived in a few lines, just not.",
            "Easy, but this can be derived.",
            "This reliance and Karen EC does that very well.",
            "So this is sort of research we might get for the lasso if you assume very low correlations, because take care being two OK with skipping two.",
            "This require that the correlation between each of the variable is less than 1 / 28, so 1 / 28 is like like very very small OK.",
            "I know it is but a very small, so you show that you need a very small correlations and of course.",
            "Every time you want to apply those types of methods, if you were to plot the occurrence matrix, do an Instagram, you will be a lot of values close to one.",
            "OK, so by any mean it will be less than 1 / 1."
        ],
        [
            "OK.",
            "So now people have been saying, OK, this is too simple an assumption.",
            "So let's take a better assumption.",
            "And this is what you get.",
            "You get what I call.",
            "It's better and in terms it implies it is strictly better than the previous one.",
            "But it starts to get complicated.",
            "OK, so I won't get into the details of that.",
            "The condition is linked with pathogen values, but just the goal is to give you a flavor of the types of results which are available in this in this field.",
            "Always the same scaling Lambda squared over N times.",
            "Log P. So for those we know the problem.",
            "I have not divided the laws by N OK that's why it is square root of N times log P and not square root of log P / N just to make things clear and what you get is what which I wanted to get to is that if you assume all that then you get the error that you make is less than a constant time times you Oracle governance times log P which was the goal of all that.",
            "So people can achieve this, though the people like Bill Gates back off and Al can achieve Oracle inequalities with the last with a loss of only log P. So of course, here you have a lot of problems.",
            "On top of that, the first one being that the scaling this depends on the K. So the way you're bound depend on K depends on the case.",
            "So much mysterious.",
            "And also of course.",
            "Most most most important is that.",
            "Most."
        ],
        [
            "So these conditions you cannot compute in polynomial time.",
            "OK, So what those results are saying that if this is true, the last would work.",
            "But you can never check that this is true.",
            "OK, so in a sense it is also of no practical value in the sense that you cannot check in advance that your conditions are true.",
            "So if you take for example that one.",
            "This is a I'm not sure it's NP hard, but I'm sure it's close."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks for the introduction.",
                    "label": 0
                },
                {
                    "sent": "So first before I start I would like to give a very special thanks to the students and postdocs who have helped me preparing those slides, in particular hydrogen atoms, ramirel and Gusinsky.",
                    "label": 1
                },
                {
                    "sent": "Also, So what I will try to do today is to give you an overview of the of these hot topic of sparse methods for machine learning and I will try and as much as I can to work to talk about the work of others.",
                    "label": 1
                },
                {
                    "sent": "But as young as said, sometimes it's hard to do so and you always try to snip in some of your own work, which I will do and with limited amounts.",
                    "label": 0
                },
                {
                    "sent": "I also have been doing a lot of reading in the last two or three weeks, but I may have forgotten someone.",
                    "label": 0
                },
                {
                    "sent": "So if you don't see yourself yourself on the slides, don't get mad at me and it does not mean that you don't do good work on sparse methods, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start by giving an overview before I go more into the details.",
                    "label": 0
                },
                {
                    "sent": "So this is very usual setting in machine learning, at least for supervised learning.",
                    "label": 1
                },
                {
                    "sent": "You have input data X belonging to some input space capital X, so I'm not used to manipulating these little hands, so I will try to a point towards the screen with the hand.",
                    "label": 0
                },
                {
                    "sent": "So this is a capital X will be any kind of input space, and you want to predict function Y value Y and half of machine learning is considering this.",
                    "label": 0
                },
                {
                    "sent": "Empirical risk minimization framework.",
                    "label": 0
                },
                {
                    "sent": "When you minimize an objective function which will try to, so we want to optimize with respect to a function from X to Y.",
                    "label": 1
                },
                {
                    "sent": "Give me an X.",
                    "label": 0
                },
                {
                    "sent": "We want to predict Y Ann, you have a data fitting term error on data points which is the sum of all you data points of the loss between your true value Yi annual prediction effort XI, an regularization term which is usually a norm or squared norm.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of issues associated with this framework, and the main two issues are which loss should you choose for which type of Y, and this.",
                    "label": 0
                },
                {
                    "sent": "I won't consider this to mention this tutorial will assume that this is solved other if you use least square regression.",
                    "label": 0
                },
                {
                    "sent": "If Yi is a real number, or if you use a support vector machine or logistic if you use.",
                    "label": 0
                },
                {
                    "sent": "Why I being discreet?",
                    "label": 0
                },
                {
                    "sent": "Just keep value the number.",
                    "label": 0
                },
                {
                    "sent": "So I will mainly focus on this.",
                    "label": 1
                },
                {
                    "sent": "The choice of the function space and the norm.",
                    "label": 0
                },
                {
                    "sent": "Now we consider various types of norm and here you have two.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The goal of adding a norm, of course, is to avoid overfitting, and now you have two main lines of works in this setting.",
                    "label": 1
                },
                {
                    "sent": "You have to camp the first camp, the one of Acadian norms, or Henderson norms, and this is essentially the camp of kernel methods.",
                    "label": 1
                },
                {
                    "sent": "And the good thing is, it allows you to do a non linear predictor.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well and also the theory at least as far as I'm concerned, is well developed and you have a lot of algorithms which are available for those type of of regularizer.",
                    "label": 0
                },
                {
                    "sent": "So second, the camp, the camp of sparsity inducing norms.",
                    "label": 0
                },
                {
                    "sent": "So here usually as a strategy, is very strict.",
                    "label": 0
                },
                {
                    "sent": "If two linear predictors.",
                    "label": 0
                },
                {
                    "sent": "So you want to predict from X as a linear function of X.",
                    "label": 0
                },
                {
                    "sent": "And of course the main example is the L1 norm, which is just the sum of the absolute values.",
                    "label": 1
                },
                {
                    "sent": "So here's the cool thing is that it does two things at the same time.",
                    "label": 0
                },
                {
                    "sent": "It will both avoid overfitting and also create some zeros into your estimator estimator.",
                    "label": 1
                },
                {
                    "sent": "W so it does both model selection and regularisation.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here this is a mental picture.",
                    "label": 0
                },
                {
                    "sent": "The tutorial, the theory, the theoretical part and algorithmic work is really in progress and this is the topic of this tutorial.",
                    "label": 0
                },
                {
                    "sent": "So the first aspect of recover if you aspect of algorithms.",
                    "label": 0
                },
                {
                    "sent": "So which one?",
                    "label": 0
                },
                {
                    "sent": "Which one is faster is L2 or L1.",
                    "label": 0
                },
                {
                    "sent": "So here this is a picture taken from the Statistics Journal and kindly given to me by Steven.",
                    "label": 0
                },
                {
                    "sent": "Can you see the Gaussian her against the Laplacian toys?",
                    "label": 0
                },
                {
                    "sent": "So Gauss was in favor of L2.",
                    "label": 0
                },
                {
                    "sent": "And Laplace was in favor of L1 and sits at one is not differentiable.",
                    "label": 0
                },
                {
                    "sent": "We will see that it might look at.",
                    "label": 0
                },
                {
                    "sent": "It is harder to optimize.",
                    "label": 0
                },
                {
                    "sent": "So this is the lesson toys, and since L2 usually leads to nice smooth convex optimization problems, where if you're a square loss you get just a linear system is supposed to be easier and this is a Gaussian her.",
                    "label": 0
                },
                {
                    "sent": "And of course which.",
                    "label": 0
                },
                {
                    "sent": "Which one wins?",
                    "label": 0
                },
                {
                    "sent": "This is adult toys OK, and we will see.",
                    "label": 0
                },
                {
                    "sent": "So first, it's nice for two things because Laplace is French.",
                    "label": 0
                },
                {
                    "sent": "An even better Laplace.",
                    "label": 0
                },
                {
                    "sent": "What the Professor article number supplier.",
                    "label": 0
                },
                {
                    "sent": "So you see that it's really important to peek at my school and this will be achieved by essentially first of the methods we will we will see at the same complexity for L1L2.",
                    "label": 0
                },
                {
                    "sent": "And also I think also overtopping methods which can use the fact that you're going to get this power solution and we will get that into the details we will get into the details of that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we go a bit over the theoretical results associated with with those type of regularizer and that will mainly focus on two types of results.",
                    "label": 1
                },
                {
                    "sent": "The 1st result will be on support recovery and innocence.",
                    "label": 1
                },
                {
                    "sent": "In this setting.",
                    "label": 0
                },
                {
                    "sent": "I think theory was kind of late with respect to applications so people have been using L1 norms for awhile and will always promise that it does two things.",
                    "label": 0
                },
                {
                    "sent": "It will prevent overfitting and select good zeros and it's been only recently that people have really analyzed.",
                    "label": 0
                },
                {
                    "sent": "How which zeros are we actually getting?",
                    "label": 0
                },
                {
                    "sent": "Do we get the correct zeros and what I will show you if that you get necessary?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Condition for exact support recovery an it involves the covariance matrix of your covariates Q and essentially if you remove these parts, it says that to get the correct support you need Q to have low correlation between variables and we see that it is both a positive result.",
                    "label": 0
                },
                {
                    "sent": "It will tell you when you don't get the good support.",
                    "label": 0
                },
                {
                    "sent": "So to me a very negative one in a sense that signature data often correlated.",
                    "label": 0
                },
                {
                    "sent": "This shows that we never get.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Support then we would get to the second aspect, which is the high dimensional inference where people try to derive results which say that you can still predict well if the number of features P is at most exponential in the number of observations.",
                    "label": 0
                },
                {
                    "sent": "So throughout this tutorial P will be the number of features and and will be the number of observations.",
                    "label": 0
                },
                {
                    "sent": "Then we go over trying to go beyond the lasso because I personally think that are playing the lesser directly is almost never useful.",
                    "label": 1
                },
                {
                    "sent": "You know, saying that you will you only have a few linear problems in machine learning, and you really want to be able to go beyond that, and I will consider first nonlinearities through adding kernels.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then trying to see if you can actually deal with exponentially many features.",
                    "label": 0
                },
                {
                    "sent": "Can you really run an algorithm where log P = N and then I will consider the topic of sparse learning on matrices.",
                    "label": 0
                },
                {
                    "sent": "So in terms of features, so theory is promising that we could get good results in terms of predictive performance.",
                    "label": 0
                },
                {
                    "sent": "If log P is at most equal to N. But what happens when N = 1000?",
                    "label": 0
                },
                {
                    "sent": "Theory says in the sense.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you could deal with the exponential 1000 features, so it's kind of a lot if you want to run an algorithm with that, and I will see how you can use structure within the features to design algorithms that can be run in polynomial time in N even.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you are implicitly considering exponentially many features, and this will be evolving.",
                    "label": 0
                },
                {
                    "sent": "Norms which are grouped with groups of fibers which may overlap.",
                    "label": 0
                },
                {
                    "sent": "Then I would consider source path of matrices and this includes a lot of application of machine learning, multi task, multi class, matrix completion.",
                    "label": 0
                },
                {
                    "sent": "Imagine Noising topic models and MF and I will mostly consider two types of positive low rank and what people often called sparse PCA or dictionary learning.",
                    "label": 1
                },
                {
                    "sent": "So this will be the outline of the tutorial.",
                    "label": 0
                },
                {
                    "sent": "The first part on the on the left so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With the algorithms and the theoretical results an one parts of steel on vectors where we have to consider a structured sparse methods and finally Xpath methods and matrices.",
                    "label": 1
                },
                {
                    "sent": "So before I start to go into the details, I think we've all seen that Slice is going to be pretty pretty quick.",
                    "label": 0
                },
                {
                    "sent": "So why at one norm leads to sparse solutions, an one?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple way to do that is to consider the constraint problem.",
                    "label": 0
                },
                {
                    "sent": "So first, first throughout this talk I will either penalized by the element nor more constrained by the element norm.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent in the sense that for any T where you constrain there's a Lambda for which you can penalize by Lambda times normal W. OK, so I will go back and forth between those two.",
                    "label": 0
                },
                {
                    "sent": "And of course the mapping from London T is not depends on data.",
                    "label": 0
                },
                {
                    "sent": "OK, that's why I say equivalent between between quotes.",
                    "label": 0
                },
                {
                    "sent": "So why does everyone?",
                    "label": 0
                },
                {
                    "sent": "Normally sparsity is here.",
                    "label": 0
                },
                {
                    "sent": "I've plotted the level sets of the quadratic function.",
                    "label": 0
                },
                {
                    "sent": "OK, so good is to minimize.",
                    "label": 0
                },
                {
                    "sent": "So if you do unconstrained minimization you will end up at the center of the... and of course will be constrained to the L1 bowl.",
                    "label": 0
                },
                {
                    "sent": "And then everyone vote is simply a square like that.",
                    "label": 0
                },
                {
                    "sent": "So you see that depending on where the cratic function is located, we get no zeros over there.",
                    "label": 0
                },
                {
                    "sent": "You will get to.",
                    "label": 0
                },
                {
                    "sent": "The optimum is obtained when the red curve touches the Blue Square.",
                    "label": 0
                },
                {
                    "sent": "Here you get no sparsity, but since you have corners in the urban body, will be attracted to the corners.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly the point which way equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple geometric intuition Y L1 norm lead to sparsity.",
                    "label": 0
                },
                {
                    "sent": "OK, so did you see the part that we can?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Weather for the next half hour.",
                    "label": 0
                },
                {
                    "sent": "So so here you assume linear linear model where we have XYZ, often referred to as covariates and why I as responses.",
                    "label": 0
                },
                {
                    "sent": "So here I will leave script Capital Y and specifying so it can be real numbers or discrete values and so they will consider this.",
                    "label": 0
                },
                {
                    "sent": "Regularize problem, we have a sum of an error on data plus regularizer.",
                    "label": 0
                },
                {
                    "sent": "So here are we not include a constant term B because it just makes life harder, which is.",
                    "label": 0
                },
                {
                    "sent": "This can be easily added an again it can be either using a penalized formulation like that or constraint formulation.",
                    "label": 0
                },
                {
                    "sent": "So when you have the square loss, this has been used a lot in mainly two communities.",
                    "label": 0
                },
                {
                    "sent": "In signal processing this is called basis pursuit.",
                    "label": 0
                },
                {
                    "sent": "Anesthetic 6 in machine learning.",
                    "label": 1
                },
                {
                    "sent": "It is called the lasso.",
                    "label": 0
                },
                {
                    "sent": "So I will start first by.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Leave a small review of nonsmooth convex analysis and convex optimization innocence.",
                    "label": 0
                },
                {
                    "sent": "The album is non differentiable and this creates some technicalities and I will spend 15 minutes trying to deal with those in the simplest possible way, so we go over analysis which will how can we say that we are optimal and how we can optimize later, so there's a nice sequence of books related to those topics from Boyd and two which is a.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Focused a lot on entire points to bonus it'll and also by Ann Lewis, which is a bit more related to analysis, so those are all good books to get more into this topic.",
                    "label": 0
                },
                {
                    "sent": "So let's start with a simple things.",
                    "label": 0
                },
                {
                    "sent": "First, when you have an L2 organizer, so we have the same the same term error on data.",
                    "label": 0
                },
                {
                    "sent": "Plus now the square, the square of the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "This is the square of the L2 norm.",
                    "label": 1
                },
                {
                    "sent": "So now this is a differentiable problem.",
                    "label": 0
                },
                {
                    "sent": "If L is differentiable with respect to the second variable, you can compute the gradients.",
                    "label": 1
                },
                {
                    "sent": "So if you simply the gradients for the loss plus the gradient for the regularizer, and of course to be optimal, you just said the gradient equal to 0.",
                    "label": 0
                },
                {
                    "sent": "In the case of the square loss.",
                    "label": 1
                },
                {
                    "sent": "So we can re parameterized a bit this loss function, where Y is just a vector with all values of.",
                    "label": 0
                },
                {
                    "sent": "Why I?",
                    "label": 0
                },
                {
                    "sent": "So it's a vector of size N and capital X is a design matrix which is N * P where all of the observations are stacked row by row.",
                    "label": 0
                },
                {
                    "sent": "So this loss function is.",
                    "label": 0
                },
                {
                    "sent": "Some of the losses of the order point.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whether the losses the square is simply can be done with this simple formula.",
                    "label": 0
                },
                {
                    "sent": "So if you take the relative of that one ingredient, you get this value, and if you invert together usual normal equations.",
                    "label": 0
                },
                {
                    "sent": "So this is when life is easy when you have the square loss and the square regularizer.",
                    "label": 0
                },
                {
                    "sent": "So if you remember one thing about the tutorial, this is this one.",
                    "label": 0
                },
                {
                    "sent": "The L1 norm is none, is not differentiable.",
                    "label": 0
                },
                {
                    "sent": "So if you try to complete this gradient is bound to fail at one point.",
                    "label": 0
                },
                {
                    "sent": "OK, so the good tools to do that you have two sets of.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tools with one of them is to consider directional derivatives, which I will looked at in this tutorial.",
                    "label": 0
                },
                {
                    "sent": "Or you may do subgradient so you can go back and forth between those two, but it's important to present directional derivatives which I will do in the this line.",
                    "label": 0
                },
                {
                    "sent": "So never try to compute the gradient of the element normal.",
                    "label": 0
                },
                {
                    "sent": "You can always cook at some stuff when W is far from zero, but zero is kind of a problematic.",
                    "label": 0
                },
                {
                    "sent": "So what you need to do is to consider in fact derivatives along all possible directions.",
                    "label": 0
                },
                {
                    "sent": "OK, so you look at the direction Delta in RP and you take that up W an go along this direction and see the rate of change along that direction, which is exactly exactly that this thing, and this is exactly what people cause additional drive active in the direction of Delta.",
                    "label": 0
                },
                {
                    "sent": "The good thing when J is convex continuous is that it always there always exists an why do we need to consider that essentially, in the nonsmooth optimization, you need to be able to consider all possible directions and not only P different ones, so the gradient is essentially looking at P different directions.",
                    "label": 1
                },
                {
                    "sent": "With nonsmooth optimization, you need to look at all possible ones, and 1D it means looking at two OK left and right.",
                    "label": 1
                },
                {
                    "sent": "Of course, when J is differentiable, then you have a strong link between directional derivatives and gradients, namely that the directional derivative is a linear function of your direction.",
                    "label": 0
                },
                {
                    "sent": "So now how do we decide whether you optimal or not?",
                    "label": 0
                },
                {
                    "sent": "This is simple since this is just the rate of change in direction Delta.",
                    "label": 0
                },
                {
                    "sent": "If to optimal this rate of change would be going up so.",
                    "label": 0
                },
                {
                    "sent": "For unconstrained minimization, your optimal, if only for all directions you go up, meaning that you look really go up, meaning that the directional derivative is positive for every directions 1st if the function is smooth, it does reduce to the zero gradient condition, because having a linear function being positive, let's be 0.",
                    "label": 0
                },
                {
                    "sent": "So this is just safety.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Check we get back zero gradients and for constant imitation I won't go over.",
                    "label": 0
                },
                {
                    "sent": "It is just for reference.",
                    "label": 0
                },
                {
                    "sent": "So now how does it look like for for the element norm so that you see is one of the few slides with the details and proofs.",
                    "label": 0
                },
                {
                    "sent": "But I think this one is quite important, so if you get that point you get most of it.",
                    "label": 0
                },
                {
                    "sent": "Not totally true, but a bit true.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a loss plus.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In that time, the ugly riser, so I will call L capital FW the loss.",
                    "label": 0
                },
                {
                    "sent": "Which is a summer or data points and you need to so this one that will assume differentiable.",
                    "label": 0
                },
                {
                    "sent": "OK, so when you need to compute the derivative, the first term is simply the gradients times Delta.",
                    "label": 0
                },
                {
                    "sent": "Now for the norm, if you compute the rate of change that change along direction Delta, you immediately immediately see that you have to make a distinction between J index indices for which the value J zero and indices for which it is non 0.",
                    "label": 0
                },
                {
                    "sent": "So when it is 0 essentially far away from zero.",
                    "label": 0
                },
                {
                    "sent": "Your function is linear.",
                    "label": 0
                },
                {
                    "sent": "OK, so it behaves like a linear function.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But at zero when WJ Zero, then if you always get this positive term.",
                    "label": 0
                },
                {
                    "sent": "So when you divide by epsilon and take the limit you will get nonlinear term of the directional derivative an which is happening here.",
                    "label": 0
                },
                {
                    "sent": "OK, so now if you mix the L1 norm with the differential part you get to some so you get a separable Sone sort of the all possible Jay.",
                    "label": 0
                },
                {
                    "sent": "So Jay goes from one 2P.",
                    "label": 0
                },
                {
                    "sent": "And you have the differentiable part on the left, for which JWJ is non zero and you have the nondifferential part when double J is zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so now how do we get optimality conditions?",
                    "label": 0
                },
                {
                    "sent": "W will be optimal if this is positive for all possible Delta.",
                    "label": 0
                },
                {
                    "sent": "So since you have since this is separable in the Delta J for all J you have, you need all those terms to be to be positive.",
                    "label": 0
                },
                {
                    "sent": "So for this one it means that this is 0 because this is a linear function which is positive only if it is always equal to 0 and you get this one.",
                    "label": 0
                },
                {
                    "sent": "This has to be positive and this essentially require that this term is less than Lambda.",
                    "label": 0
                },
                {
                    "sent": "In absolute value, why?",
                    "label": 0
                },
                {
                    "sent": "Because if it is.",
                    "label": 0
                },
                {
                    "sent": "Laser Lambda then this is this is always positive because it is always bigger than minus.",
                    "label": 0
                },
                {
                    "sent": "The value is measured value is bigger than minus Lambda.",
                    "label": 0
                },
                {
                    "sent": "You get a positive value.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is so if you think about it a few a few more seconds you will see that you need the absolute value of the gradient to be less than Lambda, and this is what you get then.",
                    "label": 0
                },
                {
                    "sent": "You get at optimal if you're the gradient of the loss plus Lambda times a second WJ 0.",
                    "label": 0
                },
                {
                    "sent": "For non 0 WJ and the gradient is smaller than Lambda when.",
                    "label": 0
                },
                {
                    "sent": "When WJ zero OK, so this is optimistic conditions and for the square losses those can be specialized.",
                    "label": 0
                },
                {
                    "sent": "Becausw gradient can be expressed.",
                    "label": 0
                },
                {
                    "sent": "Easily, so throughout this tutorial, X sub J will essentially be the design matrix corresponding to variables which are indexed by J. OK, so X sub.",
                    "label": 0
                },
                {
                    "sent": "Jay is just the data, suggests viable, so this is just a gradient of the loss like we saw in the previous slide for the square.",
                    "label": 0
                },
                {
                    "sent": "For the square loss.",
                    "label": 0
                },
                {
                    "sent": "OK, so it sees those are the automated conditions.",
                    "label": 0
                },
                {
                    "sent": "So now this will tell you if your optimal or not, and now you have to see how do we get to those two those conditions, because those cannot be inverted with zero gradient and with the square loss the cool thing is that if you write down the optimality conditions you can invert the system and get a solution.",
                    "label": 0
                },
                {
                    "sent": "Here this is not the case and usually iterative algorithms are needed, so I will hear focus first on smooth optimization just to give a review of existing results in that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Eating so we mainly focus on the gradient descent where you go from WT2WD, press 1 by stress going on the negative following the opposite direction of the gradient with a small parameter Alpha septi.",
                    "label": 0
                },
                {
                    "sent": "So here we have a lot of strategies to do.",
                    "label": 0
                },
                {
                    "sent": "To choose Alpha sub G and the one which I like.",
                    "label": 0
                },
                {
                    "sent": "The other ones is to do a line search to, not an exact line.",
                    "label": 1
                },
                {
                    "sent": "Search it because it is kind of useless to spend so many so much time optimizing the wrong function.",
                    "label": 0
                },
                {
                    "sent": "But Lens search with the stopping rules like the Army jawan and also you can consider fixed diminishing step size where which we decay in as one of our key OK plus some constant.",
                    "label": 1
                },
                {
                    "sent": "So those with those two line searches, yes, a strong body of literature trying to see how fast they do converge.",
                    "label": 0
                },
                {
                    "sent": "How far they convert to the global minimum of the function.",
                    "label": 0
                },
                {
                    "sent": "So here I assume that F is convex, so if there is no, there are no local minima.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the on the nice book by Nesterov, then you can really find convergence rates for all of these methods.",
                    "label": 0
                },
                {
                    "sent": "So what the take home message is that.",
                    "label": 0
                },
                {
                    "sent": "The nicer you function looks, the faster the convergence rate is for the same method.",
                    "label": 1
                },
                {
                    "sent": "OK, so if X is convex and just widgets, so this is just.",
                    "label": 0
                },
                {
                    "sent": "The simplest assumption you can make on the on the on the function.",
                    "label": 0
                },
                {
                    "sent": "Then you converge to the to the best value at rate one of earth code FT, which is rather slow if now you assume that you are differentiable.",
                    "label": 0
                },
                {
                    "sent": "So what people call but smooth with L Lipschitz every Lipschitz gradients.",
                    "label": 0
                },
                {
                    "sent": "So for those who are not familiar to the lips gradients, it is as it essentially means that if your function was twice differentiable, it means that the second derivative is.",
                    "label": 0
                },
                {
                    "sent": "Uniformly bounded from above by L. So if you assume that your function is differentiable, you know that you go faster an if you assume.",
                    "label": 0
                },
                {
                    "sent": "Moreover, that the function is strongly convex.",
                    "label": 0
                },
                {
                    "sent": "So essentially it means that if you function was twice differentiable, the secondary waves are bounded from below.",
                    "label": 0
                },
                {
                    "sent": "This time then you get an exponential rate of convergence, so it goes.",
                    "label": 0
                },
                {
                    "sent": "It goes a lot faster than the inverse of T. It goes as as as.",
                    "label": 0
                },
                {
                    "sent": "Exponential function of T. So this number mu over L is exactly what people call the condition number of the optimization problem.",
                    "label": 1
                },
                {
                    "sent": "Essentially, if this is small and for the code to zero gravity, Selby converge very slowly because you get back to this Lt World or M of the square root of the world.",
                    "label": 0
                },
                {
                    "sent": "But if you knew over L is large then you get exponential convergence.",
                    "label": 0
                },
                {
                    "sent": "So this is to meet very important because it will tell you when simple methods are expected to work.",
                    "label": 0
                },
                {
                    "sent": "Unknot and this depends on the condition number, and again, if your function is twice differentiable, this is just the ratio of the minimal.",
                    "label": 0
                },
                {
                    "sent": "Again, value of the Haitian over the maximal eigenvalue of the Hessian.",
                    "label": 0
                },
                {
                    "sent": "Then for cognitive services.",
                    "label": 0
                },
                {
                    "sent": "Similar types of property and just to mention that gradient decent is by no means the best possible methods.",
                    "label": 0
                },
                {
                    "sent": "Best possible 1st order methods for optimization and you have also in the nice book by Nesterov we have a lot of other schemes which we retain a lot better bounds, often called as a, sometimes Nesterov Nesterov methods which we achieve rates which are quicker.",
                    "label": 1
                },
                {
                    "sent": "BECAUSW T -- 2 goes quicker and square root.",
                    "label": 0
                },
                {
                    "sent": "Of Of your N is larger than your.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "So just keep in mind that gradient descent is not the end of the story.",
                    "label": 0
                },
                {
                    "sent": "For now, for now, this works for smooth optimization.",
                    "label": 0
                },
                {
                    "sent": "So now for nonsmooth optimization then you have no gradients, so you have the first simple method is to follow the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The gradients, so I didn't want to define some gradients, but innocence have to hear.",
                    "label": 0
                },
                {
                    "sent": "So subgradient is simply.",
                    "label": 0
                },
                {
                    "sent": "Jyoti subgradient, if it is below the if it is locali below the function.",
                    "label": 0
                },
                {
                    "sent": "So if the giant drive 80 that's greater than the corresponding linear function.",
                    "label": 0
                },
                {
                    "sent": "So in a sense it is a gradient where the function is differentiable and it is something which is tangent which defines a tangent when the function is not differentiable.",
                    "label": 0
                },
                {
                    "sent": "So here you have to be very careful if you replace just gradient descent by submitting dissent, you might not converge even if you do the exact line search.",
                    "label": 1
                },
                {
                    "sent": "So this is somewhat counter intuitive, and there's the contract sample in the slides.",
                    "label": 0
                },
                {
                    "sent": "I won't go, I won't go over, but this is very important.",
                    "label": 0
                },
                {
                    "sent": "Simple methods in the context of nonsmooth optimization might not work.",
                    "label": 0
                },
                {
                    "sent": "Exactly exactly and search it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Word for diminishing step size.",
                    "label": 1
                },
                {
                    "sent": "It does convert to the global minimum, so it's kind of counter intuitive and they have no explanation why, but sometimes it works like this, sometimes it doesn't.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also coordinate dissent, which is also very simple methods.",
                    "label": 0
                },
                {
                    "sent": "Mine is not always convergent to the global optimum.",
                    "label": 0
                },
                {
                    "sent": "So let's give for this.",
                    "label": 0
                },
                {
                    "sent": "For this simple counterexample.",
                    "label": 0
                },
                {
                    "sent": "So it is a function of two variables though, that the level sets value 1234512345, so the function goes up in that direction.",
                    "label": 0
                },
                {
                    "sent": "So if you want to do that, if you start from zero, if you look at along the two axes along W one, you go up.",
                    "label": 0
                },
                {
                    "sent": "So you're you think you should stop if you look at along W2 you go up so you think we should stop.",
                    "label": 0
                },
                {
                    "sent": "So any coordinate descent algorithm would stop at that point, but there's a direction.",
                    "label": 0
                },
                {
                    "sent": "Direction of dissent, which is this one.",
                    "label": 0
                },
                {
                    "sent": "So this is a very simple example that will tell everything you have to be able to look at all directions.",
                    "label": 0
                },
                {
                    "sent": "Since coordinate descent does not.",
                    "label": 1
                },
                {
                    "sent": "It might not be convergent to the global minimum.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also have funny stories with great laundry trying to optimize such a function at 4:00 AM before the deadline, and I can tell it does not converge.",
                    "label": 0
                },
                {
                    "sent": "OK and OK so.",
                    "label": 0
                },
                {
                    "sent": "And in terms of competence, wait, if you do regular subgradient is the same as gradient descent with no assumptions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                },
                {
                    "sent": "Keep in mind that call that they sent is not always convergent, but I will say in two slides is that possible so it is convergent.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not always convergent, but it is in some in some settings and for the last three days.",
                    "label": 0
                },
                {
                    "sent": "So before I go on, this is a.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I won't give it so last point, I think quite important for sparse methods, is that so true?",
                    "label": 0
                },
                {
                    "sent": "The objective function for sparse problem is non differentiable, but it's not any non differentiable function.",
                    "label": 0
                },
                {
                    "sent": "So if you assume the loss is differentiable you get an objective function which is of the form of the value which is differentiable plus Lambda times the norm which might not be differentiable.",
                    "label": 0
                },
                {
                    "sent": "Or if you use the constraint version you get minimizer.",
                    "label": 0
                },
                {
                    "sent": "You want to minimize.",
                    "label": 0
                },
                {
                    "sent": "A convex differentiable function with the constraint.",
                    "label": 0
                },
                {
                    "sent": "And if you assume that Alice moves an if on top of it, you assume that you know to project on the ball on the board defined by the norm or the board you find the dual norm.",
                    "label": 1
                },
                {
                    "sent": "So if you don't know what to do or not is simply the dual norm of the L1 norm, is the L Infinity norm OK, and that we should be enough at the moment.",
                    "label": 0
                },
                {
                    "sent": "So if you know how to project on the ball, define methanol and this we can do for the L1 bowl.",
                    "label": 0
                },
                {
                    "sent": "This we can do for the error.",
                    "label": 0
                },
                {
                    "sent": "City Bowl then you may use similar techniques than smooth optimization, which I won't go into in this tutorial for because I don't have the time to talk only about algorithms and simple ones are projected gradient descent.",
                    "label": 1
                },
                {
                    "sent": "And also possible methods which I won't talk a lot.",
                    "label": 0
                },
                {
                    "sent": "So just for reference, if you want to look at that and the good thing is that with those types of methods you get the similar convergence rates than for smooth optimization.",
                    "label": 1
                },
                {
                    "sent": "Namely it will depend a lot on the condition, number of the loss function.",
                    "label": 0
                },
                {
                    "sent": "OK, so if the loss function is well conditioned, it will converge quickly at exponential rates, and if it's not, we convert convert slowly at a rate of 1 / T. OK, so this is very important in terms of optimization.",
                    "label": 0
                },
                {
                    "sent": "So now let's look at the lasso and consider.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple algorithms for that.",
                    "label": 0
                },
                {
                    "sent": "So the first one is causing a dissent.",
                    "label": 0
                },
                {
                    "sent": "So I told you I told you that you should not use it, but in fact you can in this setting and the main reason is that the optimality conditions or separable.",
                    "label": 0
                },
                {
                    "sent": "So if you go back here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This setting you look that if you optimize.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only along one viable.",
                    "label": 0
                },
                {
                    "sent": "You want that to be true and you see that the old J, the condition for the variable J is independent from the condition from the other variables.",
                    "label": 0
                },
                {
                    "sent": "And this is one of the reasons why coordinate dissent is convergent convergent with the.",
                    "label": 0
                },
                {
                    "sent": "With the TL norm.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have another simple proof later, but this is so the good thing is that if you optimize with respect to one variable, this is exactly iterative thresholding because you optimize Yep.",
                    "label": 1
                },
                {
                    "sent": "You mean linear case squared loss?",
                    "label": 0
                },
                {
                    "sent": "In fact, so the if you have, I don't see your question.",
                    "label": 0
                },
                {
                    "sent": "Shops OK, but if you're a non linear prediction or you not convex anymore and then I don't exactly know.",
                    "label": 0
                },
                {
                    "sent": "If it is not separate, it will not converge.",
                    "label": 0
                },
                {
                    "sent": "It might in some, in some settings, and in fact the same stories got it did on some simple you see that assets, but if you take another data set it doesn't converge.",
                    "label": 0
                },
                {
                    "sent": "So sometimes it does, sometimes it doesn't and you don't want a method which might not be convergent.",
                    "label": 0
                },
                {
                    "sent": "This is very important because it might get stuck and you don't notice it, which is a problem.",
                    "label": 0
                },
                {
                    "sent": "So before I got one so good.",
                    "label": 0
                },
                {
                    "sent": "Good thing about coordinate descent is that this is very simple to implement because we can optimize with one if you have one viable, you have to optimize the quadratic function of 1 variable plus an absolute value, and this can be done in closed form.",
                    "label": 0
                },
                {
                    "sent": "And this is just iterative thresholding and I should example in the slides.",
                    "label": 0
                },
                {
                    "sent": "So this is very simple methods.",
                    "label": 0
                },
                {
                    "sent": "But here you have to keep in mind that this is a first order method, so if you if your covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Is a as low correlations?",
                    "label": 0
                },
                {
                    "sent": "It would be quite fast, but if you had a high correlation between you between your variables, which means that you have low condition number, that means that it might get very slow.",
                    "label": 0
                },
                {
                    "sent": "Second type of technique.",
                    "label": 0
                },
                {
                    "sent": "So I called at the ETA trick because this is how we thought to it in my group.",
                    "label": 0
                },
                {
                    "sent": "And it is simply a method that will do it at iteratively squares.",
                    "label": 0
                },
                {
                    "sent": "So if you take the L1 norm like that and you can get, you can see it as a minimum over any of the vector data.",
                    "label": 0
                },
                {
                    "sent": "So it is a vector in RP, which is positive.",
                    "label": 0
                },
                {
                    "sent": "So here if you optimize with respect to ETA here, the optimal ETA is exactly the square root of WJ.",
                    "label": 0
                },
                {
                    "sent": "Know the absolute value of WJ and you get back the absolute value of WJ.",
                    "label": 0
                },
                {
                    "sent": "So now if you want to optimize.",
                    "label": 0
                },
                {
                    "sent": "With this term, you can iteratively optimize that time instead by updating ETA each iterations.",
                    "label": 0
                },
                {
                    "sent": "So essentially you spend one time minimizing the expected ETA, and then you minimize respect to the value with better fix and those two problems are simple at 828 A, it is in close form and will rapidly W. Since we have turned the L1 norm into the L2 norm is supposed to be easier.",
                    "label": 1
                },
                {
                    "sent": "But all those do not use a financial sparse and I think this is nice, but do prefer method that use sparsity and we know see how we can we can do that.",
                    "label": 0
                },
                {
                    "sent": "So for that I will get 3 to the square loss because it allows it is a lot simpler.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Were to present.",
                    "label": 0
                },
                {
                    "sent": "So first something that we something which should never do is to write that as a quadratic program.",
                    "label": 0
                },
                {
                    "sent": "OK, so is anyone nor can we return you can you can separate the positive part as a negative part and you get a nice quiet and ask you pee.",
                    "label": 0
                },
                {
                    "sent": "OK quieting term with Lena or linear constraint so this is all nice but this if you use any generic toolbox this will be very very slow so never never do that.",
                    "label": 0
                },
                {
                    "sent": "But for the square loss you have other stuff that you might use, and the main one, and it is that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you know in advance the sign pattern of the solution, so if you know which variable should be positive, which ones should be negative and which one should be 0, then.",
                    "label": 1
                },
                {
                    "sent": "That you know the solution in closed form.",
                    "label": 0
                },
                {
                    "sent": "Wybie 'cause if you know which one should be 0 non 0 so J will be those ones then you can rewrite the objective.",
                    "label": 0
                },
                {
                    "sent": "This is the loss.",
                    "label": 0
                },
                {
                    "sent": "We just restrict two variables in J and replace the other one now but just.",
                    "label": 0
                },
                {
                    "sent": "Linear function which depends on the on the sign.",
                    "label": 0
                },
                {
                    "sent": "OK, if you do the sign you can do that.",
                    "label": 0
                },
                {
                    "sent": "And of course this is a quadratic function which you can optimize in close form and you get exactly this simple equation.",
                    "label": 0
                },
                {
                    "sent": "So if you give me the sign in advance, assign vector in advance, then you can simply get a solution.",
                    "label": 0
                },
                {
                    "sent": "So of course we have a lot of time vectors.",
                    "label": 0
                },
                {
                    "sent": "You have therapy difference and vector, so this is not.",
                    "label": 0
                },
                {
                    "sent": "You still need to guess in some sense assigned vector and once we have the subject or you need to check whether you optimal or not.",
                    "label": 0
                },
                {
                    "sent": "So how do you check whether you optimal or not?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this just your candidate vector candidate sign vector, the nonzero pattern, which are sometimes called the support of your solution.",
                    "label": 1
                },
                {
                    "sent": "And if you have that side vector, Union advance is going to be of that form.",
                    "label": 0
                },
                {
                    "sent": "So now you have to check the two optimality conditions that I've described in an earlier in the tutorial.",
                    "label": 0
                },
                {
                    "sent": "The first one at the sign of the Blue Jay is actually actually subject.",
                    "label": 0
                },
                {
                    "sent": "OK, this is 1 condition and also that the gradient with respect to the inactive Bibles or lesser number.",
                    "label": 0
                },
                {
                    "sent": "So these are two conditions you need.",
                    "label": 0
                },
                {
                    "sent": "You need to check and now you have two strategies for that.",
                    "label": 1
                },
                {
                    "sent": "Either you construct the said J iteratively, so you start from zero and keep adding viables until you can have that satisfying and the good thing is that it only requires a small number of linear systems, so to get WJ OK you need to invert to solve least least square problem reduced to Cardinal J.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Viable, which is efficient OK?",
                    "label": 0
                },
                {
                    "sent": "But the techniques, the technique I really like is a homotopy methods for the square loss.",
                    "label": 1
                },
                {
                    "sent": "So this has been relevant in multiple times, and in fact the first one to do it was macovitz in 1956, then Osborne and then the last in 2004.",
                    "label": 0
                },
                {
                    "sent": "And the idea is very simple if that is to notice that if is to get first, the goal is some of the goals are somewhat ambitious.",
                    "label": 1
                },
                {
                    "sent": "You want to get not one solution for one number, but all solutions for all possible Lambda.",
                    "label": 1
                },
                {
                    "sent": "And for that you just notice that if you know the sun vector South, this is a solution of your problem.",
                    "label": 1
                },
                {
                    "sent": "It is a final Lambda, great, and it is valid as long as those two conditions are satisfied and the sign condition is also.",
                    "label": 0
                },
                {
                    "sent": "Linear constraint the subgradient condition of the directional derivative condition is also something which is linear in Lambda.",
                    "label": 0
                },
                {
                    "sent": "So at the end you get an interval of Lambda for each side vector you get an interval of Lambda which may be empty for which the solution is optimal for that for that Lambda.",
                    "label": 0
                },
                {
                    "sent": "So at the end it means that you have a set of possible affine function for your problem.",
                    "label": 0
                },
                {
                    "sent": "For each intervals of Lambda.",
                    "label": 0
                },
                {
                    "sent": "So your solution WJ.",
                    "label": 0
                },
                {
                    "sent": "The function of Lambda is piecewise.",
                    "label": 0
                },
                {
                    "sent": "A fine, so we are sleeping the drug.",
                    "label": 0
                },
                {
                    "sent": "The possibility of having non unique solutions so you can deal with that but I won't.",
                    "label": 0
                },
                {
                    "sent": "I won't hear so now what you need is simply simply should be put to be put in quotes fine.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In find break points in that piecewise affine function, and this is exactly what markovits and others have been doing for the last.",
                    "label": 0
                },
                {
                    "sent": "53 years.",
                    "label": 0
                },
                {
                    "sent": "OK, so how does it?",
                    "label": 0
                },
                {
                    "sent": "I won't go into details because it's kind of boring just to show you how it looks like when you when you when you run the algorithm so abscessed you get Lambda OK and you get the weights over here.",
                    "label": 0
                },
                {
                    "sent": "So I'm just showing this picture for the yellow.",
                    "label": 0
                },
                {
                    "sent": "Can you see it?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I'm just doing this for the yellow for the yellow wait.",
                    "label": 0
                },
                {
                    "sent": "OK, so when you start with a large Lambda so everything is equal to 0, you select nobody.",
                    "label": 0
                },
                {
                    "sent": "You regularize a lot and as soon as we use Lambda you start to add some variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so these ones and you see that you might.",
                    "label": 0
                },
                {
                    "sent": "Somebody else might disappear from the active set.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "Somewhat content if you might decide at one point that you should add the yellow viable and then decide that you should remove it.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's quite rare that variables come in and out, but it does happen, so This is why there is no currently no complexity bound for this method, because you can design examples for which all variables will come in and out many, many times, but in practice it took me 10 minutes to find such an example, but you think random matrices, so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is now going back to this gas in her lesson toys.",
                    "label": 0
                },
                {
                    "sent": "This is with this homotopy method that you really see the difference.",
                    "label": 0
                },
                {
                    "sent": "All those first order methods which are direct methods like coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "So what if you don't?",
                    "label": 0
                },
                {
                    "sent": "They don't do sparsity directly, so you might be create some technicalities, but if you just apply them straight you get the same cost per iteration, which is all of times PRN P. The number of features and or the number of variables.",
                    "label": 0
                },
                {
                    "sent": "But if use exact so exact mean solutions which rely on inverting the system.",
                    "label": 0
                },
                {
                    "sent": "So which we have a precision of your machine precision usually and you see that there's a strong gain from P square or the L2 norm to KPN for the album.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just one of the reasons why the L1 norm does win.",
                    "label": 0
                },
                {
                    "sent": "It uses the fact that if you stop at only KLT viables then you get a faster solution.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you need to select all of the variables you get back to P square N and there's no magic because at the end when not, they call 0, you get back the ordinary least square, so it must be peace caravan.",
                    "label": 0
                },
                {
                    "sent": "When K = P. But many case since since you hope for you hope for sparsity, so you hope forecast model.",
                    "label": 0
                },
                {
                    "sent": "This is so much efficient.",
                    "label": 0
                },
                {
                    "sent": "So there are a lot of additional methods which I want to.",
                    "label": 0
                },
                {
                    "sent": "I won't go into, namely possible methods, so optimal method in the sense for the Nesterov methods have been applied with this problem, and this makes some advertisement for for software from the group.",
                    "label": 0
                },
                {
                    "sent": "So spam software essentially implementing all of these in C++ and MATLAB, and it's rather efficient as you will see in the later part of the tutorial where we are able to use that for image denoising.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So no, I went over the convex optimization algorithms, so I'm already quite late.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's go over the theoretical results.",
                    "label": 1
                },
                {
                    "sent": "So here you have, we will consider the square loss because life is easier with the square loss.",
                    "label": 0
                },
                {
                    "sent": "So the main assumption that we will make it as a data or generated from a certain sparse W. So W bowl and it's supposed to be spa.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a lot of zeros and K, but we will denote K the number of non zero values for that for that setting.",
                    "label": 0
                },
                {
                    "sent": "So you have three main problems that people have been considering.",
                    "label": 1
                },
                {
                    "sent": "First, the problem of regular consistency.",
                    "label": 0
                },
                {
                    "sent": "You want W hat so W hat will be estimator obtained from the lasso over quick W had converted the value.",
                    "label": 0
                },
                {
                    "sent": "Bold is regular consistency.",
                    "label": 0
                },
                {
                    "sent": "You have model selection consistency.",
                    "label": 1
                },
                {
                    "sent": "They get the correct zeros when the number of data points increase increases and also something which is more classical machine learning.",
                    "label": 0
                },
                {
                    "sent": "You want your estimator to lead.",
                    "label": 1
                },
                {
                    "sent": "Good predictions you don't care if you estimate the good zeros.",
                    "label": 0
                },
                {
                    "sent": "You don't care if you're close to double what you want is predictions.",
                    "label": 0
                },
                {
                    "sent": "We saw which are as good as the ones obtained by the valuable so show 2 main results.",
                    "label": 0
                },
                {
                    "sent": "The first one, the first one, is condition.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For model consistency and then the high dimensional inference.",
                    "label": 0
                },
                {
                    "sent": "So I will assume that WS Pass an everyday note capital J Bolt J as a set of nonzero, the nonzero pattern.",
                    "label": 1
                },
                {
                    "sent": "But people also called the support, and this is very nice support recovery condition for the lasso, which essentially say that the last two brigades, the correct science and hence the correct zeros if and only if you have that condition which is true.",
                    "label": 1
                },
                {
                    "sent": "So Q here in the comments metrics JC is Jay.",
                    "label": 0
                },
                {
                    "sent": "Compliment is a set of elect.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of 11 variables, the ones you want to remove.",
                    "label": 0
                },
                {
                    "sent": "J other words you want to keep and assign a WJ to sign of the generating loading vector.",
                    "label": 0
                },
                {
                    "sent": "WJ so here the take home message, at least for me, is that what's important is that term.",
                    "label": 0
                },
                {
                    "sent": "That term is large when you have strong correlations between relevant and irrelevant variables and small otherwise.",
                    "label": 0
                },
                {
                    "sent": "So essentially the Lasso finds the good pattern only if you have local relations.",
                    "label": 1
                },
                {
                    "sent": "OK, this is, uh, maybe come back to that later.",
                    "label": 0
                },
                {
                    "sent": "So small small nodes.",
                    "label": 0
                },
                {
                    "sent": "So this has been done by many people.",
                    "label": 0
                },
                {
                    "sent": "OK, seriously and then so first notice that the condition depend on WNJ, so this is of no practical value.",
                    "label": 0
                },
                {
                    "sent": "OK, if you want to know in advance, but they're going to get a good pattern, you need to know the pattern in advance.",
                    "label": 0
                },
                {
                    "sent": "OK, so is rather rather useless, but this being said, you can optimize.",
                    "label": 0
                },
                {
                    "sent": "You can take the Max.",
                    "label": 0
                },
                {
                    "sent": "The maximum problem over all possible signs of the value of W and you get the condition which only depends on the sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "Still useless because you don't need.",
                    "label": 0
                },
                {
                    "sent": "You don't need, you don't need you don't know it in advance, and you can also maximize outlet to the pattern.",
                    "label": 0
                },
                {
                    "sent": "The Bolt pattern for the given size, then for all patterns of the given size.",
                    "label": 0
                },
                {
                    "sent": "Then you will get the condition, which is a some practical significance.",
                    "label": 0
                },
                {
                    "sent": "So again, it's valid in low and high dimensional setting, so one go I won't go too much over that, but.",
                    "label": 1
                },
                {
                    "sent": "Look at high dimensions in later slides and also important.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It does require a lower bound on the magnitude of the non 0 WJ.",
                    "label": 0
                },
                {
                    "sent": "OK, so to be able to get a good support you need the non zero values to be not too small.",
                    "label": 0
                },
                {
                    "sent": "So you need a good good gap between zero and non zero values.",
                    "label": 0
                },
                {
                    "sent": "So all these say that essentially the lasso usually does not get the good zeros.",
                    "label": 0
                },
                {
                    "sent": "OK, so to me this is more negative results and a positive result.",
                    "label": 0
                },
                {
                    "sent": "And it reads it does select more variables and you can see this reference for more details.",
                    "label": 0
                },
                {
                    "sent": "So now there have been an industry on fixing that.",
                    "label": 0
                },
                {
                    "sent": "Uh, so OK.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                },
                {
                    "sent": "So that's not good zeros then it doesn't.",
                    "label": 0
                },
                {
                    "sent": "So how do you make it select the good zeros so you have a lot of work doing that, so I think there is one very nice work which is adaptive lasso which is related to concave.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Panelization, so in the adaptive lasso you replace the L1 norm by a well known weighted by by the inverse of the weights obtained from a previous estimation.",
                    "label": 1
                },
                {
                    "sent": "So you first start with maybe two angular eyes regularised estimate or L1 regularizer estimate 'cause that W hat and then do that to wait the element known.",
                    "label": 0
                },
                {
                    "sent": "So this will have the effect.",
                    "label": 0
                },
                {
                    "sent": "So if W hat is 0.",
                    "label": 0
                },
                {
                    "sent": "So you will never allow the bridge being equal to 0 for the next estimation, so if you first estimator zeros, zeros will remain.",
                    "label": 0
                },
                {
                    "sent": "And if Wi-Fi is small then you will push your W when you re estimate will put W to be 0.",
                    "label": 0
                },
                {
                    "sent": "If your first W hat was close to zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so will just help the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "To overcome these difficulties.",
                    "label": 1
                },
                {
                    "sent": "So this is adaptive lasso and very interesting part is that it can be framed.",
                    "label": 1
                },
                {
                    "sent": "In terms of concave panelization.",
                    "label": 1
                },
                {
                    "sent": "So here what you do when in concave penalization is to minimize the loss term.",
                    "label": 0
                },
                {
                    "sent": "This won't change, and instead of minimizing convex sum of a convex function of the absolute value you consider concave function.",
                    "label": 0
                },
                {
                    "sent": "So this has been done by people in signal processing and machine learning which are referred to here.",
                    "label": 0
                },
                {
                    "sent": "So essentially you replace the L1 norm which we like this by.",
                    "label": 0
                },
                {
                    "sent": "Square root for example here and intuitively, the square root is closer to the edge zero penalty.",
                    "label": 0
                },
                {
                    "sent": "So what is the other opener T?",
                    "label": 0
                },
                {
                    "sent": "It will be 0 and your zero and one as soon as you get away from zero, so will be something looking like like this.",
                    "label": 0
                },
                {
                    "sent": "So the lasso is like that and the square root is like that.",
                    "label": 1
                },
                {
                    "sent": "So you can see that it is closer to the zero penalty innocence.",
                    "label": 0
                },
                {
                    "sent": "And the good thing is that first you can design it.",
                    "label": 0
                },
                {
                    "sent": "Of course James Concave now, so it's not convex anymore, so you cannot get the global the global solution, but you have very simple concave convex procedures or people 'cause that minimization, majorization, or every community as its way of dealing with concave function.",
                    "label": 0
                },
                {
                    "sent": "And the ways to bound the concave function by its linear problem.",
                    "label": 0
                },
                {
                    "sent": "And the good thing is that if you take the linear upper bound and if you start at W. Had big W. Being only once, we exactly get back the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially if you do this concave convex procedure with the with the square root for example and you start with W being equal to 1, you further so once and then you have to use linear or ban on the square root, which essentially is equivalent to consider Alpha equals 1/2 here.",
                    "label": 0
                },
                {
                    "sent": "OK so you can see that the adaptive lasso is simply two stage optimization of concave.",
                    "label": 0
                },
                {
                    "sent": "With the Comcast penalty.",
                    "label": 0
                },
                {
                    "sent": "I think this is a very nice trick to make more zeros in your problem so you start with the law so you don't get enough zeros.",
                    "label": 0
                },
                {
                    "sent": "You just rerun the last once or twice and you will create zeros where the last so was missing them, so I think.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Earlier.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "That's so big, so there's also.",
                    "label": 0
                },
                {
                    "sent": "Work better with something by myself with my lack of time I will go goodbye.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first getting the correct support is nice and good, but did you actually actually care?",
                    "label": 0
                },
                {
                    "sent": "OK, so the one big problem with all those methods and other theorems is that they need the nonzero variables to be none too small.",
                    "label": 0
                },
                {
                    "sent": "So you need WJ to be bigger than the noise Times Square root of log P over North, so lower bound or the non zero values OK and more importantly it's not 'cause you get the wrong variables that you can't predict very well.",
                    "label": 0
                },
                {
                    "sent": "Bad estimation of the support does not imply bad predictions.",
                    "label": 0
                },
                {
                    "sent": "So what I will go over right now is to look at results which don't really care whether you get the correct zeros but only care about prediction.",
                    "label": 0
                },
                {
                    "sent": "So this will involve involve Oracle inequalities.",
                    "label": 0
                },
                {
                    "sent": "So what is?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oracle inequality she's away, I understand it is that you assume you know the support in advance.",
                    "label": 0
                },
                {
                    "sent": "Let's say you assume that the data was generated from loading vector board W with support birje.",
                    "label": 0
                },
                {
                    "sent": "If you know about Jay, you can just do ordinary least square squares restricted to ball J and record that W Oracle.",
                    "label": 0
                },
                {
                    "sent": "And you know there's the error made by this Oracle depends on the number of variables.",
                    "label": 0
                },
                {
                    "sent": "In a in jail.",
                    "label": 0
                },
                {
                    "sent": "So here it should be a bulging.",
                    "label": 0
                },
                {
                    "sent": "OK, so the prediction error.",
                    "label": 0
                },
                {
                    "sent": "The average prediction error made by the by the Oracle is simply goes as the number of non zero variables times N / N. So this shows that of course for this to work you need this number to this plan to read that to go to zero OK.",
                    "label": 0
                },
                {
                    "sent": "So essentially what you try to achieve with this sparsity's path methods is to achieve the rate of convergence of this article.",
                    "label": 0
                },
                {
                    "sent": "You cannot beat is hard to beat that one, and you cannot beat it and the goal is to go across to that optimal prediction rate.",
                    "label": 0
                },
                {
                    "sent": "So before we go into the thread for the last.",
                    "label": 0
                },
                {
                    "sent": "So I think a very important point is that let's look at the situation when you have infinite computational resources.",
                    "label": 0
                },
                {
                    "sent": "Let's say you don't care about convexity and you can optimize any possible function.",
                    "label": 0
                },
                {
                    "sent": "So essentially why people would do in statistics is consider.",
                    "label": 0
                },
                {
                    "sent": "Approach is based on the penetration, like close to be icy or you want to minimize with respect to the to the support sets.",
                    "label": 1
                },
                {
                    "sent": "So J is.",
                    "label": 0
                },
                {
                    "sent": "Is a subset of 1 one P. This is what is?",
                    "label": 0
                },
                {
                    "sent": "This is the best possible error you can make if you only use the variables in J. OK, so you take this is just the ordinary least square when you restrict to variables in Jay.",
                    "label": 0
                },
                {
                    "sent": "This is the training error and lackenby ICU.",
                    "label": 0
                },
                {
                    "sent": "Consider the training error plus some penetration term that will depend on the number of parameters, which is just capital.",
                    "label": 0
                },
                {
                    "sent": "The Cardinal of J plus.",
                    "label": 0
                },
                {
                    "sent": "Remove that guy is just that.",
                    "label": 0
                },
                {
                    "sent": "I'm sloppy.",
                    "label": 0
                },
                {
                    "sent": "So essentially very close to to be icy and the good thing is that setting that you can, if you could optimize with respect to capital J.",
                    "label": 0
                },
                {
                    "sent": "So be very clear this requires to look at all possible subsets of variables.",
                    "label": 0
                },
                {
                    "sent": "So acquires looking at to the P subsets.",
                    "label": 1
                },
                {
                    "sent": "You get this very nice Oracle inequality which say that their order to make at the end is less than the constant times the best possible rate, which is just cover North where case and number of variables which generated your problem times log P of archaon.",
                    "label": 0
                },
                {
                    "sent": "If K is more, is it just luck P. So here you see that and even better uses for Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "Make simple but there is no assumptions are guarding correlations you don't need, you don't need.",
                    "label": 0
                },
                {
                    "sent": "Your design matrix to be nice.",
                    "label": 0
                },
                {
                    "sent": "You don't need your convenience to be a closed entity.",
                    "label": 0
                },
                {
                    "sent": "It always always works in the sense this is what we want to achieve.",
                    "label": 0
                },
                {
                    "sent": "OK, but we would like to achieve is those those types of results with the convex convex convex problems.",
                    "label": 0
                },
                {
                    "sent": "So I start clear whether it is doable or not, but always keep in mind that this is what you want to achieve and we are very far with part methods to achieving it.",
                    "label": 0
                },
                {
                    "sent": "OK, as you will see in the next few slides, will always add a condition on correlations will say that.",
                    "label": 0
                },
                {
                    "sent": "We need only a small correlation between variables.",
                    "label": 0
                },
                {
                    "sent": "OK, please keep in mind that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you have time, meaning if you're willing to optimize over a set of size to the P, there is no need for all that and what we have is weaker than those types of results.",
                    "label": 0
                },
                {
                    "sent": "At the other side of the spectrum, what's happening when you have orthogonal design?",
                    "label": 0
                },
                {
                    "sent": "So let's take the simple as simple as possible setting where your design matrix X add orthogonal columns.",
                    "label": 0
                },
                {
                    "sent": "So it means that the covariance matrix is identity and there your problem will take up all because your your cratic function which will define your loss will be a sum of keratic functions on individual variables and now you get exactly soft thresholding, why?",
                    "label": 0
                },
                {
                    "sent": "'cause when you optimize quadratic function of the single viable like that which is about you would obtain an.",
                    "label": 0
                },
                {
                    "sent": "Then the solution must be of that form.",
                    "label": 0
                },
                {
                    "sent": "You take the value T, so if a zero the solution is T OK and if A is not zero then you will just soft threshold your value T. So if T is positive you move a or if she is larger than a you remove a.",
                    "label": 0
                },
                {
                    "sent": "If T is smaller than minus a you add A and if T is smaller than a in absolute value.",
                    "label": 0
                },
                {
                    "sent": "You put it to 0 so you see what people call service thresholding.",
                    "label": 0
                },
                {
                    "sent": "If you're less than Angels, Zero.",
                    "label": 0
                },
                {
                    "sent": "If not, you're shrunk towards 0.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially how people have been analyzing those methods.",
                    "label": 0
                },
                {
                    "sent": "First, I think those methods.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is where you can prove easily that the lasso will work.",
                    "label": 0
                },
                {
                    "sent": "So let's look very quickly how it works.",
                    "label": 0
                },
                {
                    "sent": "And here is just to give you an idea where to an idea of where does this log P come from.",
                    "label": 1
                },
                {
                    "sent": "OK, so I don't want to give you a class on the cost of fashion inequality's it's going to be a bit too long, but why does it look come from?",
                    "label": 0
                },
                {
                    "sent": "And if you look closely at the proofs and we only focus on that one, essentially the way I understand it may be wrong.",
                    "label": 0
                },
                {
                    "sent": "Is that the log P comes from the fact that if you take P independent.",
                    "label": 0
                },
                {
                    "sent": "Gaussian variables OK or maybe even not independent the maximum as expectation.",
                    "label": 0
                },
                {
                    "sent": "Which role as square root of log P. OK, so this is the main reason why it can be because of the Union bound.",
                    "label": 0
                },
                {
                    "sent": "But this will be too long to long to explain.",
                    "label": 1
                },
                {
                    "sent": "But this is the log P is not.",
                    "label": 0
                },
                {
                    "sent": "It's not magically, it comes from the expectation of the maximum of normally distributed variables.",
                    "label": 1
                },
                {
                    "sent": "So if you look at the papers often is hidden, so mines approved.",
                    "label": 0
                },
                {
                    "sent": "But this is the main reason why.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's go.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Down to the results with the Lasso before having a pose.",
                    "label": 0
                },
                {
                    "sent": "First, the main result is that you only need K log P. You need only.",
                    "label": 1
                },
                {
                    "sent": "Canopy can be needs to be smaller than N for to get good good creative performance.",
                    "label": 0
                },
                {
                    "sent": "OK so this is the sort of scaling which had promised on the beginning of the tutorial, but for that you need you have two.",
                    "label": 0
                },
                {
                    "sent": "You have two constraints are very important.",
                    "label": 0
                },
                {
                    "sent": "First W the generating vector should be sufficient.",
                    "label": 0
                },
                {
                    "sent": "Sufficiently sparse.",
                    "label": 0
                },
                {
                    "sent": "OK should be spawned.",
                    "label": 0
                },
                {
                    "sent": "This is already included included in the.",
                    "label": 0
                },
                {
                    "sent": "In here, but input variable should also be not two correlated OK and this is very important if variables are correlated, there is no.",
                    "label": 0
                },
                {
                    "sent": "There is no way we can prove consistency for the Lost soul.",
                    "label": 0
                },
                {
                    "sent": "So now there's a whole industry of sufficient conditions for the lasso, so they start from very simple to very complex and will start for the very simple which just mutual encouragement.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Coherence.",
                    "label": 0
                },
                {
                    "sent": "So essentially so the next slide will be somewhat with the theorem, so which which look a bit complicated, but it's not in fact.",
                    "label": 0
                },
                {
                    "sent": "So you assume that your data are generated from as a sparse in R model plus some noise where the noises ID normal.",
                    "label": 0
                },
                {
                    "sent": "You assume that the covariance matrix, so as you need diagonal and cross terms which are less than 1 / 1 / K * A constant, and then what you could show that if you select that Lambda you get with high probability, because this will be.",
                    "label": 0
                },
                {
                    "sent": "This will be small, you get that error error that you make is less time is less than Sigma times the square root of log P. / N OK.",
                    "label": 1
                },
                {
                    "sent": "So this so this result can be derived in a few lines, just not.",
                    "label": 0
                },
                {
                    "sent": "Easy, but this can be derived.",
                    "label": 0
                },
                {
                    "sent": "This reliance and Karen EC does that very well.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of research we might get for the lasso if you assume very low correlations, because take care being two OK with skipping two.",
                    "label": 0
                },
                {
                    "sent": "This require that the correlation between each of the variable is less than 1 / 28, so 1 / 28 is like like very very small OK.",
                    "label": 0
                },
                {
                    "sent": "I know it is but a very small, so you show that you need a very small correlations and of course.",
                    "label": 0
                },
                {
                    "sent": "Every time you want to apply those types of methods, if you were to plot the occurrence matrix, do an Instagram, you will be a lot of values close to one.",
                    "label": 0
                },
                {
                    "sent": "OK, so by any mean it will be less than 1 / 1.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now people have been saying, OK, this is too simple an assumption.",
                    "label": 0
                },
                {
                    "sent": "So let's take a better assumption.",
                    "label": 0
                },
                {
                    "sent": "And this is what you get.",
                    "label": 0
                },
                {
                    "sent": "You get what I call.",
                    "label": 0
                },
                {
                    "sent": "It's better and in terms it implies it is strictly better than the previous one.",
                    "label": 0
                },
                {
                    "sent": "But it starts to get complicated.",
                    "label": 0
                },
                {
                    "sent": "OK, so I won't get into the details of that.",
                    "label": 0
                },
                {
                    "sent": "The condition is linked with pathogen values, but just the goal is to give you a flavor of the types of results which are available in this in this field.",
                    "label": 0
                },
                {
                    "sent": "Always the same scaling Lambda squared over N times.",
                    "label": 0
                },
                {
                    "sent": "Log P. So for those we know the problem.",
                    "label": 0
                },
                {
                    "sent": "I have not divided the laws by N OK that's why it is square root of N times log P and not square root of log P / N just to make things clear and what you get is what which I wanted to get to is that if you assume all that then you get the error that you make is less than a constant time times you Oracle governance times log P which was the goal of all that.",
                    "label": 1
                },
                {
                    "sent": "So people can achieve this, though the people like Bill Gates back off and Al can achieve Oracle inequalities with the last with a loss of only log P. So of course, here you have a lot of problems.",
                    "label": 0
                },
                {
                    "sent": "On top of that, the first one being that the scaling this depends on the K. So the way you're bound depend on K depends on the case.",
                    "label": 0
                },
                {
                    "sent": "So much mysterious.",
                    "label": 0
                },
                {
                    "sent": "And also of course.",
                    "label": 0
                },
                {
                    "sent": "Most most most important is that.",
                    "label": 0
                },
                {
                    "sent": "Most.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these conditions you cannot compute in polynomial time.",
                    "label": 1
                },
                {
                    "sent": "OK, So what those results are saying that if this is true, the last would work.",
                    "label": 0
                },
                {
                    "sent": "But you can never check that this is true.",
                    "label": 1
                },
                {
                    "sent": "OK, so in a sense it is also of no practical value in the sense that you cannot check in advance that your conditions are true.",
                    "label": 0
                },
                {
                    "sent": "So if you take for example that one.",
                    "label": 0
                },
                {
                    "sent": "This is a I'm not sure it's NP hard, but I'm sure it's close.",
                    "label": 0
                }
            ]
        }
    }
}