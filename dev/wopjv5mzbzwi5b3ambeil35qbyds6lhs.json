{
    "id": "wopjv5mzbzwi5b3ambeil35qbyds6lhs",
    "title": "The stability of a good clustering",
    "info": {
        "author": [
            "Marina Meila, Department of Statistics, University of Washington"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "August 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_meila_sgc/",
    "segmentation": [
        [
            "So before we start really, I just want to take a fall.",
            "I could give a talk that's mostly on K means, or I could give a talk that also has spectral clustering.",
            "How many people want to hear about spectral clustering tool?",
            "Oh OK, alright and minority team, so I'm going to give this talk that only has came in.",
            "Compliment.",
            "How many?",
            "How many people don't want to hear about spectral clustering?",
            "He says I should ask about the compliment.",
            "So everybody wants to hear about who has an opinion.",
            "Alright, then maybe I'll change the talk.",
            "OK. OK, so the title of the talk is the stability of a good clustering."
        ],
        [
            "And.",
            "The theme is this.",
            "It's about on the left.",
            "You have spectral clustering and on the right you have K means that I presented this morning.",
            "Spectral clustering may appear later in the course.",
            "But the first thing I want to say is that this is not about algorithms.",
            "This is a talk about the cost functions of these two algorithms.",
            "So it's a talk about the K means distortion.",
            "Here Z is the former XI and is a talk about the objective of spectral clustering, which is I call it also D, But it's normalized cut if you don't know what spectral clustering does will get to it later in the talk.",
            "So what do you know?",
            "We know that optimizing this criteria is NP hard, which means in the worst case, finding the true optimum, the global optimum is exponential.",
            "But there is a lot of empirical evidence that says that when there is a good clustering in the data, then actually it's not so hard to find it.",
            "When people create a simple test data set and cluster if they do find the true labels.",
            "Or you know clustering that's close to the true one.",
            "So what's happening here?",
            "First of all?",
            "The case when a good clustering exists.",
            "It's not a frequent case.",
            "So this is the worst case.",
            "We don't know how frequent is off, like if most datasets are NP hard to.",
            "To optimize the distortion on but what we know is that if we care about clustering, not just minimizing a function if we don't care about finding clusters, then this is an interesting case.",
            "Whether it happens often or not.",
            "This means whether our datasets have clusters or don't.",
            "This is 1.",
            "This is an interesting case and we want to study it.",
            "Because we are doing clustering.",
            "And so, in this talk I'm going to talk about these two criteria, mainly about K means, and then I'll generalize to spectral clustering.",
            "And what I'm going to show is that if the data contains a good clustering.",
            "Then that is in some sense unique in the sense that we can't change it very much and still have a good clustering where good is measured with respect to the distortion means low distortion.",
            "That is 1 result.",
            "It's theoretical.",
            "The other result that is practical is that if I do find a good clustering and clustering with low distortion then I can sometimes prove that it is close to the optimal clustering.",
            "Which is not true in the worst case, but it is true in the.",
            "Lucky case.",
            "Which something?"
        ],
        [
            "Exist?",
            "So let's summarize the results.",
            "Once again, if I have data.",
            "If I have a distortion that's quadratic like the ones before and X&Y represent clusterings.",
            "Then we have two things.",
            "And all the results says that there is always a lower bound that's non 0.",
            "That's lower than the lowest distortion.",
            "Or then any distortion of any clustering distortion equals cost.",
            "In this talk, yeah, and the new result is that if my clustering is close to the lower bound.",
            "Then but this is within two the two clusterings the optimal clustering, an X is close is small, but this is meant as the two clusterings are very similar.",
            "So if we do clustering, we don't care so much about how much they distort the value of the distortion.",
            "We care much about this distance.",
            "Have we found something that's close to the optimum or or are we far away?",
            "Yes.",
            "So again, I wanted to be clear that I'm talking about.",
            "Excess clusterings with low distortion and not about how I get not about the algorithm.",
            "And I want to prove that this clusterings are similar.",
            "And so the things in blue have to be defined.",
            "What is small and what is small here?",
            "Like what does it mean to do clustering?",
            "To be simple."
        ],
        [
            "But before that, another graphical view of the same results in a picture.",
            "This is the distortion over the space of clustering.",
            "I don't know this function.",
            "The only thing I am somewhere there is this optimum.",
            "The best clustering possible.",
            "This is the lower bound, so the best clustering is not at the lower bound, but it's at the minimum of this function doesn't achieve the lower bound usually.",
            "I find the clustering X and I measure is distortion, so I have X and the distortion.",
            "That's all I have.",
            "And."
        ],
        [
            "Eddie and what I can show is that I can delete all this function and I can only this interval a ball around X.",
            "And I'm going to prove that the optimal clustering is somewhere in this bar."
        ],
        [
            "So there will be a lot of introduction.",
            "First of all, how I represent what is the distance between clusterings?",
            "Then how I represent the clusterings this will matter for the proofs at least, and how we presented distortions.",
            "That's also matters because it turns out that the results hold when the distortion is quadratic, but we don't know if we have similar results for other distortions.",
            "And then I'll give some arguments of why this result is true and show some simple experiments showing that in fact the result sometimes holds."
        ],
        [
            "No, it doesn't hold all this obviously.",
            "So how do we compare to clastics?",
            "These are two hard clusterings of the same data set and KNN prime K primer.",
            "The size is the number of points in each cluster and this MKK prime is the overlap between each cluster in the first clustering and each cluster in the second cluster.",
            "So if I put this MK in a matrix I get a K by K prime matrix.",
            "Which is called the confusion matrix and every clustering comparison criteria starts with this matrix.",
            "This is a whole somehow the sufficient statistics of the two clusterings that matters for the car."
        ],
        [
            "And now I define the following distance, which is called a misclassification error distance.",
            "So if you know what the misclassification error is, then it's easy to imagine what this distance would be.",
            "I just assumed that my cluster labels are classification labels.",
            "And I compute the misclassification error between.",
            "The two plastics.",
            "Which is actually this.",
            "So.",
            "If a point is here in this diagonal block, it means that it's classified with the same label by the two clusterings, yes.",
            "And so the misclassification error is the method of diagonal mass.",
            "Yes, but now clustering is not classification, so the labels don't carry a class information, so I should actually minimize this, take the minimum of this overall possible label assignments, yes.",
            "Yeah, This is why I'm taking that label the minimum over so I'm taking my name, the clusters in the most advantages way so that I maximize what's on the diagonal the match.",
            "And in fact this can be computed.",
            "There are K factorial permutations here, but it can be computed by bipartite matching algorithm that is polynomial.",
            "So it's not a problem computing this distance.",
            "And in fact, it's also a metric.",
            "That's not relevant to this talk, it's just nice."
        ],
        [
            "So now hopefully we know how to measure the similarity in between two clusterings.",
            "Now I'm going to represent the clustering in as matrices.",
            "How, in a very simple way, I just create a matrix with a row for each point and the column for each cluster.",
            "And then I set up.",
            "Entry IK at one if plus point.",
            "I belongs to cluster K, this is redundant in several ways, but it's a matrix representation and we are going to use linear algebra, so having matrices is good.",
            "Then we do a second step.",
            "We are saying.",
            "OK, so if I take the scalar product of two columns, I always get 0.",
            "Because the ones in a column correspond to zeros in the other columns.",
            "So the columns are orthogonal, and so if I normalize them to have length one then I get an orthogonal matrix and the most interesting so another way of looking at an orthogonal matrix is that it's an orthogonal basis for a subspace.",
            "So I'm going to look at every clustering as representing a subspace.",
            "Of Dimension K and later I'm going to see that.",
            "I can throw a one dimension, it's obvious why, because I can reconstruct any of the columns from the others, and I'm going to represent clusterings as K -- 1 dimensional subspaces.",
            "So basis in K -- 1 dimensional subspaces and why these colors don't correspond to here, because I have actually rotated the basis which doesn't change the subspace, so it doesn't change the clustering."
        ],
        [
            "That's another.",
            "We're showing this.",
            "I represent the clustering as a matrix.",
            "I normalize the row I drop last column because I can always reconstruct it from the data and then I do a rotation that's convenient for some that will actually simply.",
            "It's not necessary, but it simplifies the proofs quite a lot, which proved that you won't really see.",
            "And so every clustering is represented by this Y.",
            "So Y means that it's a basis for a K minus dimension K -- 1 dimensional subspace.",
            "And if I say X, then it's a basis for higher dimensional subspace and I use them interchangeably."
        ],
        [
            "OK, now I said that one column is redundant.",
            "And I can remove it, but should I remove it?",
            "Here is the reason why I actually really want to remove that last column.",
            "Here is a clustering with four clusters and if I look at subspaces, one subspace 1K dimensional subspace that stands out is the principle.",
            "The K principle subspace of the data.",
            "Again, we do principle components on this data.",
            "And look at the Kate principle subspace.",
            "I am not showing the Tiger and that's given by the eigenvectors of the covariance matrix.",
            "Or of the gram matrix?",
            "I'm not saying that vectors here, but I'm showing the eigenvalues and what this shows is that the land before is almost zero.",
            "Basically it's in distinguishable from the other eigenvalues, which are fairly small and sort of are assumed to be represent noise.",
            "And we also know from linear algebra that.",
            "If this happens, it means that the 4th principle subspace is very unstable to perturbations.",
            "However, between Lambda, three Lambda four, there is a large gap.",
            "Which again is known in linear algebra to mean that the third principle subspace, which contains only these three eigenvalues, is very stable to perturbations.",
            "So before even knowing how these subspaces relate to clustering.",
            "You may want to actually use this 3 dimensional subspace instead of the four dimensional model because that is stable to perturbations in the data, so it may reflect some high level structure.",
            "Another explanation of this is the following.",
            "If I have 4th cluster centers.",
            "Nothing changes if I shift today, so in fact they don't define.",
            "They only define a 3 dimensional.",
            "Subspace up to a translation.",
            "And if you like if it's easier to think in three dimensions if I have clusters in three dimensions, three clusters in three dimensions, they define a plane.",
            "So the relevant space for this cluster structure in the data, if there is any is K -- 1 dimensional."
        ],
        [
            "And the third foot of the proof or leg of the proof is that the distortion that came in distortion can be written as a quadratic function in this X representation orthogonal basis of K dimensional subspace.",
            "I'm not explaining why.",
            "Basically it's a constant minus this quadratic function, where a is what's called the gram matrix of the data and so is the normalized cut.",
            "Constant with minus the quadratic function of something that looks the same but with a different matrix.",
            "And so if we think of quadratic functions of 1 variable, is that parabolas, so they have a nice deep minimum.",
            "This is the analogy we're going to think of these functions as actually functions of arbitrary orthogonal matrices, not just of matrices that come from clusterings and as functions like this, they have a deep."
        ],
        [
            "Animal and we know what that is.",
            "It's given by the 1st K eigenvalues, so basically the maximum of this function over orthogonal matrices is given by the 1st K eigenvalues and is attained at the principal subspace given by the leading eigenvectors.",
            "And if X was away with K -- 1 dimensions, then this would be K -- 1 here."
        ],
        [
            "So this.",
            "This gives me the the lower bound on any cluster by basically relaxing the problem instead of.",
            "X initially was representing a clustering not every subspace in not every subspace represents a clustering only some.",
            "So if I actually take any matrix here I relax the problem and then I get this minimum.",
            "So this minimum this F of X star is an upper bound for this problem and so if I change the sign here I get this function gives me a lower bound on the distortion or on the normalized cut.",
            "Now why does Eigen get matter again?",
            "I said because of stability.",
            "So this is the this is the function F. For a matrix where diagram gap is large and this is its maximum, and here is the same function F represented represented as the length on this vector.",
            "When Digon Gap is small, so this... long, this ellipse is almost a circle.",
            "What's important to see is that if we take an X and move it away from the optimum here that the function changes very much from the maximum decreases fast, whereas if we change if we perturb X here the function doesn't change it almost at all.",
            "In fact, this has almost the same value.",
            "And the right way to measure this is by taking the change in F from the upper bound.",
            "Divided by digging up, this is the right measurement unit for this.",
            "And so if that gap is large.",
            "Like if this change is small and this icon gap is large, then this quantity is small and in that case we can show that X is close to X tile.",
            "So if you can't have a high essence, have a high value for any Xbox One over here or office over here."
        ],
        [
            "OK. Um?",
            "So.",
            "As far as I got.",
            "I have at least convince you with pictures that if.",
            "We have a low distortion and good clustering.",
            "And the large icon gap then.",
            "X is close to X star.",
            "A subspace is.",
            "So in their subspace representation that have to be somehow close.",
            "I haven't defined how what I need to do now is to.",
            "Show that they are close as clusterings, so there is small error between the two plus tax.",
            "And so I'm going to show this on this one, slide in pictures.",
            "So again, this is a distortion is minimum here maximum there.",
            "This is the ideal.",
            "Principal subspace now I have gone down to one dimension, but things are still the same.",
            "And this is another way that comes from a cluster.",
            "So the first step is if the distortion is close to that lower bound, in the sense that this Delta, which is distortion over.",
            "Distortion gap over Eigen Gap is small then.",
            "What I can say is that the projection of Y onto the.",
            "The part of why that's not in the principle subspace is bounded by that does the projection on the orthogonal complement.",
            "So in this projection is small, it means that why is almost aligned with the White Star, which is a black line.",
            "Now the next step is I take 2 clusterings, the blue one and orange one.",
            "Huaian why prime?",
            "And each of them has a Delta that small.",
            "What I can show next is that if they are small then.",
            "This quantity is large.",
            "This quantity is upper bounded by K and so if these deltas are small then this quantity is upper bounded by.",
            "Is close to K with is within epsilon funky?",
            "Where epsilon has this formula.",
            "Yes, now what is this quantity?",
            "This content is a measure of how aligned at X&X transpose the subspaces, but in another node.",
            "Of scalar product.",
            "So its maximum when X&X prime arrive at the same.",
            "It's Frobenius norm.",
            "So I have gone from Y to X and I have shown that are aligned the subspaces and then here comes the next step.",
            "Which has nothing to do with matrix algebra, which is to show that if this thing is large then the misclassification error is small.",
            "And we have this.",
            "Really simple formula that.",
            "If I have epsilon here, then the distance is epsilon times the largest cluster probability.",
            "So P Max is the.",
            "Probability of the largest cluster, the number of points in the largest cluster divided by N. Yes.",
            "And again, if you are interested in the proof, this is a convexity proof, it doesn't."
        ],
        [
            "Have to do with matrices very much.",
            "And now I can form.",
            "I have shown the profile can give the theorem for any two clusterings.",
            "If I have these two conditions that both deltas are small, this is a very easy to satisfy condition.",
            "In fact I need that has to be much smaller than that, and if epsilon that result from these two deltas is smaller than the smallest cluster.",
            "In probability then?",
            "The distance between X&X prime is clustering, so the misclassification error is bounded by this quantity.",
            "Yes, there should be a bar here, yes, so these are permanent Max.",
            "So this theorem says that I can bound the distance between two clusterings by.",
            "This epsilon, which itself is a function of the clusterings, and it's a function of the data.",
            "Now, if I know the clustering, this is not very interesting because if I know the two clusterings, I can compute the distance."
        ],
        [
            "So what is interesting about this result?",
            "What's interesting is that even if I don't know one of the clusterings, I can still apply the theorem.",
            "Why because I?",
            "And that's not true for any clustering, but it's true for the best clustering WHI because I know that the best clustering has distortion no larger than D of X that I know.",
            "So this means that it's Delta is no larger than this Delta and all I need about.",
            "So it is actually this Delta.",
            "To apply the theorem.",
            "So basically I can apply the theorem knowing only one clustering and I have this corollary which is actually the main result which says that if this epsilon is smaller than the smallest cluster then I can bound the cluster.",
            "The distance between the good clustering that I found and the best I can do by the same bound.",
            "And that's because the right hand side only depends on one clustering and on the data.",
            "Other things that this result doesn't depend on, it doesn't depend on any data distribution, depends only on the actual data.",
            "So it's.",
            "What in supervised learning you call a distribution free result or a model free result?",
            "You don't make any assumptions about the distribution that generates the data.",
            "But it depends on the fact that I have found a good clustering because if X is not good, then this corollary doesn't apply."
        ],
        [
            "And there are some extensions to weighted data points to kernelized data.",
            "And most importantly to the normal scatter graphs.",
            "And I think I still have 5 minutes, yes or slightly more.",
            "OK, so I'll try to do it in 5 minutes."
        ],
        [
            "Ask questions.",
            "So what is the normalized cut in a graph?",
            "Suppose I eliminate these edges, so this is the cut.",
            "Is the weight of the edges that separates 2.",
            "The graph into two clusters.",
            "And good.",
            "Cost function for graph partitioning.",
            "There is no time to explain why is the cut divided by the volumes.",
            "All of being defined here of the two remaining sizes.",
            "So if this is small, it means that I have cut edges of small weight, but was left on the others on each side is also not too small.",
            "Yes, and that's important cause.",
            "If I don't put this factor here, if I minimize just a cut, then I would actually cut off in this graph.",
            "I could have cut up just an outlier or something and so minimizing the cut is easier, but it's not a good clustering cost.",
            "However, this normalize cut is NP hard to optimize.",
            "For any K, even K equal to."
        ],
        [
            "In general, but so is a K means cost function and what I'm going to show now is that from the previous theorem, for K means you can obtain a by transferring the results.",
            "You can obtain a result for the normalized cut, which is another cost function.",
            "So the result is that that essentially.",
            "This is a cost over good clustering.",
            "Here would be the lower bound.",
            "The lower bound is K minus the sum of some eigenvalues.",
            "Eigenvalue solver.",
            "What is called the graph?",
            "The normal is Laplacian of the graph.",
            "A symmetric matrix obtained from the similarities divided by the Eigen gap.",
            "Notice that here I have K&K plus one, not KN K -- 1, so I have jumped in with the dimension back again.",
            "Why is that for a simple reason?",
            "In a. Normalize cuts the first eigenvector is not informative, so the first eigenvector is redundant and therefore since it's the first we have to actually skip and take the next K -- 1.",
            "And so we get the same result except with a different Delta.",
            "Everything else is the same.",
            "We can also.",
            "There was also a previous bound obtained along the same lines by me and my students earlier.",
            "Or we can show that by this method we get actually about that is better.",
            "Essentially strictly better for our corollary."
        ],
        [
            "And.",
            "I saw a few experiments.",
            "These are not expensive experiments that just proof of concept, showing that in fact the bound applies because it could be if you have experience with VC bounds or with bounds in.",
            "In supervised learning.",
            "I don't know if you saw experiment in this course, but very often the bounds are above 1.",
            "Which means that they are not informative.",
            "They can be used to bound there.",
            "They are informative in other ways.",
            "The shape of the bound is important, so here I'm showing that in fact you can use this bound because sometimes in cases that are.",
            "Close to interesting.",
            "Or the simplest interesting case is the bond is actually below 1.",
            "This is not tight, so this is just the 1st result and there is hope to improve it actually.",
            "So here is here.",
            "Gaussian clusters in the clusters are in in high demand in high dimensions like 20 or 30.",
            "I have projected them on the principle subspace.",
            "And look how nice they look there.",
            "Basically, starting to touch and then I have actually.",
            "Shrunk the variance to almost zero and I have actually completed a bound for.",
            "Various levels, various values of Sigma."
        ],
        [
            "And this is what I got so that the data the same except that are shrunk around the center Sigma is almost zero here, and it's fairly large here.",
            "This is the size of the smallest cluster about a third, and this is the bound.",
            "So remember here the class.",
            "I know the best clustering becausw or something that's extremely close with the best clustering because I have generated the data.",
            "So the best the clustering that I have found.",
            "My ex is always.",
            "Almost equal, so there are two optimum is almost here all the time, so the true error is zero and this is the bound.",
            "The bond is not perfect after it gets loser and loser with the increase on the variance and that's somehow expected to be expected.",
            "Another graph that looks almost the same.",
            "This is the same plot but with the.",
            "10 times more points.",
            "What you can see is that the shape the things don't really depend too much on N, and if there is any dependence, the bounding proofs.",
            "That's that's a relates to what I said in the last five minutes of the course that if you have more data, your principal subspaces gives you better information on the structure of the data."
        ],
        [
            "And the.",
            "Here I know the the I perturbed optimal clustering and this is a true distance.",
            "Again, it's mixture of browsers and here is the bound.",
            "And here is the size of the smallest cluster.",
            "So the bound is.",
            "North Side is the worst case bound so.",
            "You should.",
            "It should almost never be tight.",
            "But is actually much smaller than the smallest cluster.",
            "So this means that with this bound we have approved that we are close to the optimum.",
            "Yes, so this says that the clustering error is about 5% or between 5 and 10% depending on the perturbation and the smallest cluster is has size 1/3.",
            "Which means that we have essentially found all the clusters.",
            "We are not confusing.",
            "Two clusters are not breaking when cluster into two sizes or so.",
            "We're not doing any catastrophic error."
        ],
        [
            "OK, and.",
            "I'll put up the slide about the conclusion.",
            "What I want to say is that.",
            "Is the 1st result in what is basically a\nOf research because it gives a bound on the distance between two clusterings, or an algorithm that is maybe it is surely not the best, but with the criterion that is known to be NP hard and under no assumptions that there are previous results giving bounds on the clustering and some of them come from the.",
            "Almost surely correct algorithms.",
            "I talked in the in the last minutes of the lecture, but those make distributional assumptions here the results are weaker, but we work in a worst case situation.",
            "We don't assume anything about the data.",
            "Alright, this is it.",
            "Thank you.",
            "Other questions.",
            "Hey, can you talk more about the relationship between the steering and noise cases?",
            "The data is noisy.",
            "Then how to apply the theory?",
            "I'm not sure I understand the question, so the noise is other than noise than the sort of being Gaussian or being data around the.",
            "Actually, it seems to me that.",
            "Football I think I can get is hard to decide if.",
            "Space in most cases so.",
            "My question actually.",
            "Is the daily use?",
            "Space.",
            "I wonder how about conditionality?",
            "Again, I'm not sure what you mean by noise, because it's just a fixed data set, so I could phrase it as.",
            "If there is no if the.",
            "If the clusters are not separated well, then the principal subspace will not actually reflect the cluster structure.",
            "So and then the theorem doesn't apply.",
            "So the theorem either applies and then we have a good result.",
            "Because epsilon times P Max is always smaller than one by quite a bit, or it doesn't apply at all and it says and you say I don't know.",
            "I have a clustering and it may be good and sometimes it is good, but you can't prove it.",
            "I hope this helps."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before we start really, I just want to take a fall.",
                    "label": 0
                },
                {
                    "sent": "I could give a talk that's mostly on K means, or I could give a talk that also has spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "How many people want to hear about spectral clustering tool?",
                    "label": 0
                },
                {
                    "sent": "Oh OK, alright and minority team, so I'm going to give this talk that only has came in.",
                    "label": 0
                },
                {
                    "sent": "Compliment.",
                    "label": 0
                },
                {
                    "sent": "How many?",
                    "label": 0
                },
                {
                    "sent": "How many people don't want to hear about spectral clustering?",
                    "label": 0
                },
                {
                    "sent": "He says I should ask about the compliment.",
                    "label": 0
                },
                {
                    "sent": "So everybody wants to hear about who has an opinion.",
                    "label": 0
                },
                {
                    "sent": "Alright, then maybe I'll change the talk.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so the title of the talk is the stability of a good clustering.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The theme is this.",
                    "label": 0
                },
                {
                    "sent": "It's about on the left.",
                    "label": 0
                },
                {
                    "sent": "You have spectral clustering and on the right you have K means that I presented this morning.",
                    "label": 0
                },
                {
                    "sent": "Spectral clustering may appear later in the course.",
                    "label": 1
                },
                {
                    "sent": "But the first thing I want to say is that this is not about algorithms.",
                    "label": 0
                },
                {
                    "sent": "This is a talk about the cost functions of these two algorithms.",
                    "label": 0
                },
                {
                    "sent": "So it's a talk about the K means distortion.",
                    "label": 0
                },
                {
                    "sent": "Here Z is the former XI and is a talk about the objective of spectral clustering, which is I call it also D, But it's normalized cut if you don't know what spectral clustering does will get to it later in the talk.",
                    "label": 0
                },
                {
                    "sent": "So what do you know?",
                    "label": 0
                },
                {
                    "sent": "We know that optimizing this criteria is NP hard, which means in the worst case, finding the true optimum, the global optimum is exponential.",
                    "label": 0
                },
                {
                    "sent": "But there is a lot of empirical evidence that says that when there is a good clustering in the data, then actually it's not so hard to find it.",
                    "label": 0
                },
                {
                    "sent": "When people create a simple test data set and cluster if they do find the true labels.",
                    "label": 0
                },
                {
                    "sent": "Or you know clustering that's close to the true one.",
                    "label": 0
                },
                {
                    "sent": "So what's happening here?",
                    "label": 0
                },
                {
                    "sent": "First of all?",
                    "label": 0
                },
                {
                    "sent": "The case when a good clustering exists.",
                    "label": 1
                },
                {
                    "sent": "It's not a frequent case.",
                    "label": 0
                },
                {
                    "sent": "So this is the worst case.",
                    "label": 0
                },
                {
                    "sent": "We don't know how frequent is off, like if most datasets are NP hard to.",
                    "label": 0
                },
                {
                    "sent": "To optimize the distortion on but what we know is that if we care about clustering, not just minimizing a function if we don't care about finding clusters, then this is an interesting case.",
                    "label": 0
                },
                {
                    "sent": "Whether it happens often or not.",
                    "label": 0
                },
                {
                    "sent": "This means whether our datasets have clusters or don't.",
                    "label": 0
                },
                {
                    "sent": "This is 1.",
                    "label": 0
                },
                {
                    "sent": "This is an interesting case and we want to study it.",
                    "label": 0
                },
                {
                    "sent": "Because we are doing clustering.",
                    "label": 0
                },
                {
                    "sent": "And so, in this talk I'm going to talk about these two criteria, mainly about K means, and then I'll generalize to spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "And what I'm going to show is that if the data contains a good clustering.",
                    "label": 0
                },
                {
                    "sent": "Then that is in some sense unique in the sense that we can't change it very much and still have a good clustering where good is measured with respect to the distortion means low distortion.",
                    "label": 0
                },
                {
                    "sent": "That is 1 result.",
                    "label": 0
                },
                {
                    "sent": "It's theoretical.",
                    "label": 0
                },
                {
                    "sent": "The other result that is practical is that if I do find a good clustering and clustering with low distortion then I can sometimes prove that it is close to the optimal clustering.",
                    "label": 1
                },
                {
                    "sent": "Which is not true in the worst case, but it is true in the.",
                    "label": 0
                },
                {
                    "sent": "Lucky case.",
                    "label": 0
                },
                {
                    "sent": "Which something?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exist?",
                    "label": 0
                },
                {
                    "sent": "So let's summarize the results.",
                    "label": 0
                },
                {
                    "sent": "Once again, if I have data.",
                    "label": 0
                },
                {
                    "sent": "If I have a distortion that's quadratic like the ones before and X&Y represent clusterings.",
                    "label": 0
                },
                {
                    "sent": "Then we have two things.",
                    "label": 1
                },
                {
                    "sent": "And all the results says that there is always a lower bound that's non 0.",
                    "label": 0
                },
                {
                    "sent": "That's lower than the lowest distortion.",
                    "label": 0
                },
                {
                    "sent": "Or then any distortion of any clustering distortion equals cost.",
                    "label": 0
                },
                {
                    "sent": "In this talk, yeah, and the new result is that if my clustering is close to the lower bound.",
                    "label": 0
                },
                {
                    "sent": "Then but this is within two the two clusterings the optimal clustering, an X is close is small, but this is meant as the two clusterings are very similar.",
                    "label": 0
                },
                {
                    "sent": "So if we do clustering, we don't care so much about how much they distort the value of the distortion.",
                    "label": 0
                },
                {
                    "sent": "We care much about this distance.",
                    "label": 0
                },
                {
                    "sent": "Have we found something that's close to the optimum or or are we far away?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So again, I wanted to be clear that I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "Excess clusterings with low distortion and not about how I get not about the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And I want to prove that this clusterings are similar.",
                    "label": 0
                },
                {
                    "sent": "And so the things in blue have to be defined.",
                    "label": 0
                },
                {
                    "sent": "What is small and what is small here?",
                    "label": 0
                },
                {
                    "sent": "Like what does it mean to do clustering?",
                    "label": 0
                },
                {
                    "sent": "To be simple.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But before that, another graphical view of the same results in a picture.",
                    "label": 1
                },
                {
                    "sent": "This is the distortion over the space of clustering.",
                    "label": 0
                },
                {
                    "sent": "I don't know this function.",
                    "label": 0
                },
                {
                    "sent": "The only thing I am somewhere there is this optimum.",
                    "label": 0
                },
                {
                    "sent": "The best clustering possible.",
                    "label": 0
                },
                {
                    "sent": "This is the lower bound, so the best clustering is not at the lower bound, but it's at the minimum of this function doesn't achieve the lower bound usually.",
                    "label": 0
                },
                {
                    "sent": "I find the clustering X and I measure is distortion, so I have X and the distortion.",
                    "label": 0
                },
                {
                    "sent": "That's all I have.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eddie and what I can show is that I can delete all this function and I can only this interval a ball around X.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to prove that the optimal clustering is somewhere in this bar.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there will be a lot of introduction.",
                    "label": 0
                },
                {
                    "sent": "First of all, how I represent what is the distance between clusterings?",
                    "label": 1
                },
                {
                    "sent": "Then how I represent the clusterings this will matter for the proofs at least, and how we presented distortions.",
                    "label": 0
                },
                {
                    "sent": "That's also matters because it turns out that the results hold when the distortion is quadratic, but we don't know if we have similar results for other distortions.",
                    "label": 0
                },
                {
                    "sent": "And then I'll give some arguments of why this result is true and show some simple experiments showing that in fact the result sometimes holds.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, it doesn't hold all this obviously.",
                    "label": 0
                },
                {
                    "sent": "So how do we compare to clastics?",
                    "label": 0
                },
                {
                    "sent": "These are two hard clusterings of the same data set and KNN prime K primer.",
                    "label": 0
                },
                {
                    "sent": "The size is the number of points in each cluster and this MKK prime is the overlap between each cluster in the first clustering and each cluster in the second cluster.",
                    "label": 0
                },
                {
                    "sent": "So if I put this MK in a matrix I get a K by K prime matrix.",
                    "label": 0
                },
                {
                    "sent": "Which is called the confusion matrix and every clustering comparison criteria starts with this matrix.",
                    "label": 1
                },
                {
                    "sent": "This is a whole somehow the sufficient statistics of the two clusterings that matters for the car.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now I define the following distance, which is called a misclassification error distance.",
                    "label": 1
                },
                {
                    "sent": "So if you know what the misclassification error is, then it's easy to imagine what this distance would be.",
                    "label": 0
                },
                {
                    "sent": "I just assumed that my cluster labels are classification labels.",
                    "label": 0
                },
                {
                    "sent": "And I compute the misclassification error between.",
                    "label": 1
                },
                {
                    "sent": "The two plastics.",
                    "label": 0
                },
                {
                    "sent": "Which is actually this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If a point is here in this diagonal block, it means that it's classified with the same label by the two clusterings, yes.",
                    "label": 0
                },
                {
                    "sent": "And so the misclassification error is the method of diagonal mass.",
                    "label": 0
                },
                {
                    "sent": "Yes, but now clustering is not classification, so the labels don't carry a class information, so I should actually minimize this, take the minimum of this overall possible label assignments, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, This is why I'm taking that label the minimum over so I'm taking my name, the clusters in the most advantages way so that I maximize what's on the diagonal the match.",
                    "label": 0
                },
                {
                    "sent": "And in fact this can be computed.",
                    "label": 0
                },
                {
                    "sent": "There are K factorial permutations here, but it can be computed by bipartite matching algorithm that is polynomial.",
                    "label": 1
                },
                {
                    "sent": "So it's not a problem computing this distance.",
                    "label": 0
                },
                {
                    "sent": "And in fact, it's also a metric.",
                    "label": 0
                },
                {
                    "sent": "That's not relevant to this talk, it's just nice.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now hopefully we know how to measure the similarity in between two clusterings.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to represent the clustering in as matrices.",
                    "label": 1
                },
                {
                    "sent": "How, in a very simple way, I just create a matrix with a row for each point and the column for each cluster.",
                    "label": 0
                },
                {
                    "sent": "And then I set up.",
                    "label": 0
                },
                {
                    "sent": "Entry IK at one if plus point.",
                    "label": 0
                },
                {
                    "sent": "I belongs to cluster K, this is redundant in several ways, but it's a matrix representation and we are going to use linear algebra, so having matrices is good.",
                    "label": 0
                },
                {
                    "sent": "Then we do a second step.",
                    "label": 0
                },
                {
                    "sent": "We are saying.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I take the scalar product of two columns, I always get 0.",
                    "label": 0
                },
                {
                    "sent": "Because the ones in a column correspond to zeros in the other columns.",
                    "label": 0
                },
                {
                    "sent": "So the columns are orthogonal, and so if I normalize them to have length one then I get an orthogonal matrix and the most interesting so another way of looking at an orthogonal matrix is that it's an orthogonal basis for a subspace.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to look at every clustering as representing a subspace.",
                    "label": 0
                },
                {
                    "sent": "Of Dimension K and later I'm going to see that.",
                    "label": 1
                },
                {
                    "sent": "I can throw a one dimension, it's obvious why, because I can reconstruct any of the columns from the others, and I'm going to represent clusterings as K -- 1 dimensional subspaces.",
                    "label": 0
                },
                {
                    "sent": "So basis in K -- 1 dimensional subspaces and why these colors don't correspond to here, because I have actually rotated the basis which doesn't change the subspace, so it doesn't change the clustering.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's another.",
                    "label": 0
                },
                {
                    "sent": "We're showing this.",
                    "label": 0
                },
                {
                    "sent": "I represent the clustering as a matrix.",
                    "label": 0
                },
                {
                    "sent": "I normalize the row I drop last column because I can always reconstruct it from the data and then I do a rotation that's convenient for some that will actually simply.",
                    "label": 1
                },
                {
                    "sent": "It's not necessary, but it simplifies the proofs quite a lot, which proved that you won't really see.",
                    "label": 0
                },
                {
                    "sent": "And so every clustering is represented by this Y.",
                    "label": 1
                },
                {
                    "sent": "So Y means that it's a basis for a K minus dimension K -- 1 dimensional subspace.",
                    "label": 1
                },
                {
                    "sent": "And if I say X, then it's a basis for higher dimensional subspace and I use them interchangeably.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now I said that one column is redundant.",
                    "label": 0
                },
                {
                    "sent": "And I can remove it, but should I remove it?",
                    "label": 0
                },
                {
                    "sent": "Here is the reason why I actually really want to remove that last column.",
                    "label": 0
                },
                {
                    "sent": "Here is a clustering with four clusters and if I look at subspaces, one subspace 1K dimensional subspace that stands out is the principle.",
                    "label": 0
                },
                {
                    "sent": "The K principle subspace of the data.",
                    "label": 0
                },
                {
                    "sent": "Again, we do principle components on this data.",
                    "label": 0
                },
                {
                    "sent": "And look at the Kate principle subspace.",
                    "label": 0
                },
                {
                    "sent": "I am not showing the Tiger and that's given by the eigenvectors of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Or of the gram matrix?",
                    "label": 0
                },
                {
                    "sent": "I'm not saying that vectors here, but I'm showing the eigenvalues and what this shows is that the land before is almost zero.",
                    "label": 0
                },
                {
                    "sent": "Basically it's in distinguishable from the other eigenvalues, which are fairly small and sort of are assumed to be represent noise.",
                    "label": 0
                },
                {
                    "sent": "And we also know from linear algebra that.",
                    "label": 0
                },
                {
                    "sent": "If this happens, it means that the 4th principle subspace is very unstable to perturbations.",
                    "label": 0
                },
                {
                    "sent": "However, between Lambda, three Lambda four, there is a large gap.",
                    "label": 0
                },
                {
                    "sent": "Which again is known in linear algebra to mean that the third principle subspace, which contains only these three eigenvalues, is very stable to perturbations.",
                    "label": 0
                },
                {
                    "sent": "So before even knowing how these subspaces relate to clustering.",
                    "label": 0
                },
                {
                    "sent": "You may want to actually use this 3 dimensional subspace instead of the four dimensional model because that is stable to perturbations in the data, so it may reflect some high level structure.",
                    "label": 0
                },
                {
                    "sent": "Another explanation of this is the following.",
                    "label": 0
                },
                {
                    "sent": "If I have 4th cluster centers.",
                    "label": 0
                },
                {
                    "sent": "Nothing changes if I shift today, so in fact they don't define.",
                    "label": 0
                },
                {
                    "sent": "They only define a 3 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Subspace up to a translation.",
                    "label": 0
                },
                {
                    "sent": "And if you like if it's easier to think in three dimensions if I have clusters in three dimensions, three clusters in three dimensions, they define a plane.",
                    "label": 0
                },
                {
                    "sent": "So the relevant space for this cluster structure in the data, if there is any is K -- 1 dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the third foot of the proof or leg of the proof is that the distortion that came in distortion can be written as a quadratic function in this X representation orthogonal basis of K dimensional subspace.",
                    "label": 0
                },
                {
                    "sent": "I'm not explaining why.",
                    "label": 0
                },
                {
                    "sent": "Basically it's a constant minus this quadratic function, where a is what's called the gram matrix of the data and so is the normalized cut.",
                    "label": 0
                },
                {
                    "sent": "Constant with minus the quadratic function of something that looks the same but with a different matrix.",
                    "label": 0
                },
                {
                    "sent": "And so if we think of quadratic functions of 1 variable, is that parabolas, so they have a nice deep minimum.",
                    "label": 0
                },
                {
                    "sent": "This is the analogy we're going to think of these functions as actually functions of arbitrary orthogonal matrices, not just of matrices that come from clusterings and as functions like this, they have a deep.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Animal and we know what that is.",
                    "label": 0
                },
                {
                    "sent": "It's given by the 1st K eigenvalues, so basically the maximum of this function over orthogonal matrices is given by the 1st K eigenvalues and is attained at the principal subspace given by the leading eigenvectors.",
                    "label": 1
                },
                {
                    "sent": "And if X was away with K -- 1 dimensions, then this would be K -- 1 here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "This gives me the the lower bound on any cluster by basically relaxing the problem instead of.",
                    "label": 0
                },
                {
                    "sent": "X initially was representing a clustering not every subspace in not every subspace represents a clustering only some.",
                    "label": 0
                },
                {
                    "sent": "So if I actually take any matrix here I relax the problem and then I get this minimum.",
                    "label": 0
                },
                {
                    "sent": "So this minimum this F of X star is an upper bound for this problem and so if I change the sign here I get this function gives me a lower bound on the distortion or on the normalized cut.",
                    "label": 0
                },
                {
                    "sent": "Now why does Eigen get matter again?",
                    "label": 0
                },
                {
                    "sent": "I said because of stability.",
                    "label": 0
                },
                {
                    "sent": "So this is the this is the function F. For a matrix where diagram gap is large and this is its maximum, and here is the same function F represented represented as the length on this vector.",
                    "label": 0
                },
                {
                    "sent": "When Digon Gap is small, so this... long, this ellipse is almost a circle.",
                    "label": 0
                },
                {
                    "sent": "What's important to see is that if we take an X and move it away from the optimum here that the function changes very much from the maximum decreases fast, whereas if we change if we perturb X here the function doesn't change it almost at all.",
                    "label": 0
                },
                {
                    "sent": "In fact, this has almost the same value.",
                    "label": 0
                },
                {
                    "sent": "And the right way to measure this is by taking the change in F from the upper bound.",
                    "label": 0
                },
                {
                    "sent": "Divided by digging up, this is the right measurement unit for this.",
                    "label": 0
                },
                {
                    "sent": "And so if that gap is large.",
                    "label": 0
                },
                {
                    "sent": "Like if this change is small and this icon gap is large, then this quantity is small and in that case we can show that X is close to X tile.",
                    "label": 1
                },
                {
                    "sent": "So if you can't have a high essence, have a high value for any Xbox One over here or office over here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As far as I got.",
                    "label": 0
                },
                {
                    "sent": "I have at least convince you with pictures that if.",
                    "label": 0
                },
                {
                    "sent": "We have a low distortion and good clustering.",
                    "label": 0
                },
                {
                    "sent": "And the large icon gap then.",
                    "label": 0
                },
                {
                    "sent": "X is close to X star.",
                    "label": 1
                },
                {
                    "sent": "A subspace is.",
                    "label": 0
                },
                {
                    "sent": "So in their subspace representation that have to be somehow close.",
                    "label": 0
                },
                {
                    "sent": "I haven't defined how what I need to do now is to.",
                    "label": 0
                },
                {
                    "sent": "Show that they are close as clusterings, so there is small error between the two plus tax.",
                    "label": 0
                },
                {
                    "sent": "And so I'm going to show this on this one, slide in pictures.",
                    "label": 0
                },
                {
                    "sent": "So again, this is a distortion is minimum here maximum there.",
                    "label": 0
                },
                {
                    "sent": "This is the ideal.",
                    "label": 0
                },
                {
                    "sent": "Principal subspace now I have gone down to one dimension, but things are still the same.",
                    "label": 0
                },
                {
                    "sent": "And this is another way that comes from a cluster.",
                    "label": 0
                },
                {
                    "sent": "So the first step is if the distortion is close to that lower bound, in the sense that this Delta, which is distortion over.",
                    "label": 0
                },
                {
                    "sent": "Distortion gap over Eigen Gap is small then.",
                    "label": 0
                },
                {
                    "sent": "What I can say is that the projection of Y onto the.",
                    "label": 0
                },
                {
                    "sent": "The part of why that's not in the principle subspace is bounded by that does the projection on the orthogonal complement.",
                    "label": 0
                },
                {
                    "sent": "So in this projection is small, it means that why is almost aligned with the White Star, which is a black line.",
                    "label": 0
                },
                {
                    "sent": "Now the next step is I take 2 clusterings, the blue one and orange one.",
                    "label": 0
                },
                {
                    "sent": "Huaian why prime?",
                    "label": 0
                },
                {
                    "sent": "And each of them has a Delta that small.",
                    "label": 0
                },
                {
                    "sent": "What I can show next is that if they are small then.",
                    "label": 0
                },
                {
                    "sent": "This quantity is large.",
                    "label": 0
                },
                {
                    "sent": "This quantity is upper bounded by K and so if these deltas are small then this quantity is upper bounded by.",
                    "label": 1
                },
                {
                    "sent": "Is close to K with is within epsilon funky?",
                    "label": 0
                },
                {
                    "sent": "Where epsilon has this formula.",
                    "label": 0
                },
                {
                    "sent": "Yes, now what is this quantity?",
                    "label": 0
                },
                {
                    "sent": "This content is a measure of how aligned at X&X transpose the subspaces, but in another node.",
                    "label": 0
                },
                {
                    "sent": "Of scalar product.",
                    "label": 0
                },
                {
                    "sent": "So its maximum when X&X prime arrive at the same.",
                    "label": 0
                },
                {
                    "sent": "It's Frobenius norm.",
                    "label": 0
                },
                {
                    "sent": "So I have gone from Y to X and I have shown that are aligned the subspaces and then here comes the next step.",
                    "label": 0
                },
                {
                    "sent": "Which has nothing to do with matrix algebra, which is to show that if this thing is large then the misclassification error is small.",
                    "label": 0
                },
                {
                    "sent": "And we have this.",
                    "label": 0
                },
                {
                    "sent": "Really simple formula that.",
                    "label": 0
                },
                {
                    "sent": "If I have epsilon here, then the distance is epsilon times the largest cluster probability.",
                    "label": 0
                },
                {
                    "sent": "So P Max is the.",
                    "label": 0
                },
                {
                    "sent": "Probability of the largest cluster, the number of points in the largest cluster divided by N. Yes.",
                    "label": 0
                },
                {
                    "sent": "And again, if you are interested in the proof, this is a convexity proof, it doesn't.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have to do with matrices very much.",
                    "label": 0
                },
                {
                    "sent": "And now I can form.",
                    "label": 0
                },
                {
                    "sent": "I have shown the profile can give the theorem for any two clusterings.",
                    "label": 1
                },
                {
                    "sent": "If I have these two conditions that both deltas are small, this is a very easy to satisfy condition.",
                    "label": 0
                },
                {
                    "sent": "In fact I need that has to be much smaller than that, and if epsilon that result from these two deltas is smaller than the smallest cluster.",
                    "label": 0
                },
                {
                    "sent": "In probability then?",
                    "label": 0
                },
                {
                    "sent": "The distance between X&X prime is clustering, so the misclassification error is bounded by this quantity.",
                    "label": 0
                },
                {
                    "sent": "Yes, there should be a bar here, yes, so these are permanent Max.",
                    "label": 0
                },
                {
                    "sent": "So this theorem says that I can bound the distance between two clusterings by.",
                    "label": 0
                },
                {
                    "sent": "This epsilon, which itself is a function of the clusterings, and it's a function of the data.",
                    "label": 0
                },
                {
                    "sent": "Now, if I know the clustering, this is not very interesting because if I know the two clusterings, I can compute the distance.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is interesting about this result?",
                    "label": 0
                },
                {
                    "sent": "What's interesting is that even if I don't know one of the clusterings, I can still apply the theorem.",
                    "label": 0
                },
                {
                    "sent": "Why because I?",
                    "label": 0
                },
                {
                    "sent": "And that's not true for any clustering, but it's true for the best clustering WHI because I know that the best clustering has distortion no larger than D of X that I know.",
                    "label": 0
                },
                {
                    "sent": "So this means that it's Delta is no larger than this Delta and all I need about.",
                    "label": 0
                },
                {
                    "sent": "So it is actually this Delta.",
                    "label": 0
                },
                {
                    "sent": "To apply the theorem.",
                    "label": 0
                },
                {
                    "sent": "So basically I can apply the theorem knowing only one clustering and I have this corollary which is actually the main result which says that if this epsilon is smaller than the smallest cluster then I can bound the cluster.",
                    "label": 0
                },
                {
                    "sent": "The distance between the good clustering that I found and the best I can do by the same bound.",
                    "label": 0
                },
                {
                    "sent": "And that's because the right hand side only depends on one clustering and on the data.",
                    "label": 0
                },
                {
                    "sent": "Other things that this result doesn't depend on, it doesn't depend on any data distribution, depends only on the actual data.",
                    "label": 1
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "What in supervised learning you call a distribution free result or a model free result?",
                    "label": 0
                },
                {
                    "sent": "You don't make any assumptions about the distribution that generates the data.",
                    "label": 0
                },
                {
                    "sent": "But it depends on the fact that I have found a good clustering because if X is not good, then this corollary doesn't apply.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are some extensions to weighted data points to kernelized data.",
                    "label": 1
                },
                {
                    "sent": "And most importantly to the normal scatter graphs.",
                    "label": 0
                },
                {
                    "sent": "And I think I still have 5 minutes, yes or slightly more.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll try to do it in 5 minutes.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ask questions.",
                    "label": 0
                },
                {
                    "sent": "So what is the normalized cut in a graph?",
                    "label": 1
                },
                {
                    "sent": "Suppose I eliminate these edges, so this is the cut.",
                    "label": 0
                },
                {
                    "sent": "Is the weight of the edges that separates 2.",
                    "label": 0
                },
                {
                    "sent": "The graph into two clusters.",
                    "label": 0
                },
                {
                    "sent": "And good.",
                    "label": 0
                },
                {
                    "sent": "Cost function for graph partitioning.",
                    "label": 0
                },
                {
                    "sent": "There is no time to explain why is the cut divided by the volumes.",
                    "label": 0
                },
                {
                    "sent": "All of being defined here of the two remaining sizes.",
                    "label": 0
                },
                {
                    "sent": "So if this is small, it means that I have cut edges of small weight, but was left on the others on each side is also not too small.",
                    "label": 0
                },
                {
                    "sent": "Yes, and that's important cause.",
                    "label": 0
                },
                {
                    "sent": "If I don't put this factor here, if I minimize just a cut, then I would actually cut off in this graph.",
                    "label": 0
                },
                {
                    "sent": "I could have cut up just an outlier or something and so minimizing the cut is easier, but it's not a good clustering cost.",
                    "label": 1
                },
                {
                    "sent": "However, this normalize cut is NP hard to optimize.",
                    "label": 0
                },
                {
                    "sent": "For any K, even K equal to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In general, but so is a K means cost function and what I'm going to show now is that from the previous theorem, for K means you can obtain a by transferring the results.",
                    "label": 1
                },
                {
                    "sent": "You can obtain a result for the normalized cut, which is another cost function.",
                    "label": 0
                },
                {
                    "sent": "So the result is that that essentially.",
                    "label": 0
                },
                {
                    "sent": "This is a cost over good clustering.",
                    "label": 0
                },
                {
                    "sent": "Here would be the lower bound.",
                    "label": 0
                },
                {
                    "sent": "The lower bound is K minus the sum of some eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "Eigenvalue solver.",
                    "label": 0
                },
                {
                    "sent": "What is called the graph?",
                    "label": 0
                },
                {
                    "sent": "The normal is Laplacian of the graph.",
                    "label": 0
                },
                {
                    "sent": "A symmetric matrix obtained from the similarities divided by the Eigen gap.",
                    "label": 0
                },
                {
                    "sent": "Notice that here I have K&K plus one, not KN K -- 1, so I have jumped in with the dimension back again.",
                    "label": 0
                },
                {
                    "sent": "Why is that for a simple reason?",
                    "label": 0
                },
                {
                    "sent": "In a. Normalize cuts the first eigenvector is not informative, so the first eigenvector is redundant and therefore since it's the first we have to actually skip and take the next K -- 1.",
                    "label": 0
                },
                {
                    "sent": "And so we get the same result except with a different Delta.",
                    "label": 0
                },
                {
                    "sent": "Everything else is the same.",
                    "label": 0
                },
                {
                    "sent": "We can also.",
                    "label": 0
                },
                {
                    "sent": "There was also a previous bound obtained along the same lines by me and my students earlier.",
                    "label": 0
                },
                {
                    "sent": "Or we can show that by this method we get actually about that is better.",
                    "label": 0
                },
                {
                    "sent": "Essentially strictly better for our corollary.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I saw a few experiments.",
                    "label": 0
                },
                {
                    "sent": "These are not expensive experiments that just proof of concept, showing that in fact the bound applies because it could be if you have experience with VC bounds or with bounds in.",
                    "label": 0
                },
                {
                    "sent": "In supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you saw experiment in this course, but very often the bounds are above 1.",
                    "label": 0
                },
                {
                    "sent": "Which means that they are not informative.",
                    "label": 0
                },
                {
                    "sent": "They can be used to bound there.",
                    "label": 0
                },
                {
                    "sent": "They are informative in other ways.",
                    "label": 0
                },
                {
                    "sent": "The shape of the bound is important, so here I'm showing that in fact you can use this bound because sometimes in cases that are.",
                    "label": 0
                },
                {
                    "sent": "Close to interesting.",
                    "label": 0
                },
                {
                    "sent": "Or the simplest interesting case is the bond is actually below 1.",
                    "label": 0
                },
                {
                    "sent": "This is not tight, so this is just the 1st result and there is hope to improve it actually.",
                    "label": 0
                },
                {
                    "sent": "So here is here.",
                    "label": 0
                },
                {
                    "sent": "Gaussian clusters in the clusters are in in high demand in high dimensions like 20 or 30.",
                    "label": 0
                },
                {
                    "sent": "I have projected them on the principle subspace.",
                    "label": 0
                },
                {
                    "sent": "And look how nice they look there.",
                    "label": 0
                },
                {
                    "sent": "Basically, starting to touch and then I have actually.",
                    "label": 0
                },
                {
                    "sent": "Shrunk the variance to almost zero and I have actually completed a bound for.",
                    "label": 0
                },
                {
                    "sent": "Various levels, various values of Sigma.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what I got so that the data the same except that are shrunk around the center Sigma is almost zero here, and it's fairly large here.",
                    "label": 0
                },
                {
                    "sent": "This is the size of the smallest cluster about a third, and this is the bound.",
                    "label": 0
                },
                {
                    "sent": "So remember here the class.",
                    "label": 0
                },
                {
                    "sent": "I know the best clustering becausw or something that's extremely close with the best clustering because I have generated the data.",
                    "label": 0
                },
                {
                    "sent": "So the best the clustering that I have found.",
                    "label": 0
                },
                {
                    "sent": "My ex is always.",
                    "label": 0
                },
                {
                    "sent": "Almost equal, so there are two optimum is almost here all the time, so the true error is zero and this is the bound.",
                    "label": 0
                },
                {
                    "sent": "The bond is not perfect after it gets loser and loser with the increase on the variance and that's somehow expected to be expected.",
                    "label": 0
                },
                {
                    "sent": "Another graph that looks almost the same.",
                    "label": 0
                },
                {
                    "sent": "This is the same plot but with the.",
                    "label": 0
                },
                {
                    "sent": "10 times more points.",
                    "label": 0
                },
                {
                    "sent": "What you can see is that the shape the things don't really depend too much on N, and if there is any dependence, the bounding proofs.",
                    "label": 0
                },
                {
                    "sent": "That's that's a relates to what I said in the last five minutes of the course that if you have more data, your principal subspaces gives you better information on the structure of the data.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "Here I know the the I perturbed optimal clustering and this is a true distance.",
                    "label": 0
                },
                {
                    "sent": "Again, it's mixture of browsers and here is the bound.",
                    "label": 0
                },
                {
                    "sent": "And here is the size of the smallest cluster.",
                    "label": 0
                },
                {
                    "sent": "So the bound is.",
                    "label": 0
                },
                {
                    "sent": "North Side is the worst case bound so.",
                    "label": 0
                },
                {
                    "sent": "You should.",
                    "label": 0
                },
                {
                    "sent": "It should almost never be tight.",
                    "label": 0
                },
                {
                    "sent": "But is actually much smaller than the smallest cluster.",
                    "label": 0
                },
                {
                    "sent": "So this means that with this bound we have approved that we are close to the optimum.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this says that the clustering error is about 5% or between 5 and 10% depending on the perturbation and the smallest cluster is has size 1/3.",
                    "label": 0
                },
                {
                    "sent": "Which means that we have essentially found all the clusters.",
                    "label": 0
                },
                {
                    "sent": "We are not confusing.",
                    "label": 0
                },
                {
                    "sent": "Two clusters are not breaking when cluster into two sizes or so.",
                    "label": 0
                },
                {
                    "sent": "We're not doing any catastrophic error.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "I'll put up the slide about the conclusion.",
                    "label": 0
                },
                {
                    "sent": "What I want to say is that.",
                    "label": 0
                },
                {
                    "sent": "Is the 1st result in what is basically a\nOf research because it gives a bound on the distance between two clusterings, or an algorithm that is maybe it is surely not the best, but with the criterion that is known to be NP hard and under no assumptions that there are previous results giving bounds on the clustering and some of them come from the.",
                    "label": 1
                },
                {
                    "sent": "Almost surely correct algorithms.",
                    "label": 0
                },
                {
                    "sent": "I talked in the in the last minutes of the lecture, but those make distributional assumptions here the results are weaker, but we work in a worst case situation.",
                    "label": 0
                },
                {
                    "sent": "We don't assume anything about the data.",
                    "label": 0
                },
                {
                    "sent": "Alright, this is it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Hey, can you talk more about the relationship between the steering and noise cases?",
                    "label": 0
                },
                {
                    "sent": "The data is noisy.",
                    "label": 0
                },
                {
                    "sent": "Then how to apply the theory?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I understand the question, so the noise is other than noise than the sort of being Gaussian or being data around the.",
                    "label": 0
                },
                {
                    "sent": "Actually, it seems to me that.",
                    "label": 0
                },
                {
                    "sent": "Football I think I can get is hard to decide if.",
                    "label": 0
                },
                {
                    "sent": "Space in most cases so.",
                    "label": 0
                },
                {
                    "sent": "My question actually.",
                    "label": 0
                },
                {
                    "sent": "Is the daily use?",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "I wonder how about conditionality?",
                    "label": 0
                },
                {
                    "sent": "Again, I'm not sure what you mean by noise, because it's just a fixed data set, so I could phrase it as.",
                    "label": 0
                },
                {
                    "sent": "If there is no if the.",
                    "label": 0
                },
                {
                    "sent": "If the clusters are not separated well, then the principal subspace will not actually reflect the cluster structure.",
                    "label": 0
                },
                {
                    "sent": "So and then the theorem doesn't apply.",
                    "label": 0
                },
                {
                    "sent": "So the theorem either applies and then we have a good result.",
                    "label": 0
                },
                {
                    "sent": "Because epsilon times P Max is always smaller than one by quite a bit, or it doesn't apply at all and it says and you say I don't know.",
                    "label": 0
                },
                {
                    "sent": "I have a clustering and it may be good and sometimes it is good, but you can't prove it.",
                    "label": 0
                },
                {
                    "sent": "I hope this helps.",
                    "label": 0
                }
            ]
        }
    }
}