{
    "id": "zhuvpkmiagp44z2fadfsqosh7omodj2g",
    "title": "Proximal Reinforcement Learning: Learning to Act in Primal Dual Spaces",
    "info": {
        "author": [
            "Sridhar Mahadevan, College of Information and Computer Sciences, University of Massachusetts Amherst"
        ],
        "published": "July 28, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Social Sciences->Psychology",
            "Top->Social Sciences->Economics",
            "Top->Medicine->Neuroscience",
            "Top->Technology->Engineering->Electrical Engineering->Control Engineering"
        ]
    },
    "url": "http://videolectures.net/rldm2015_mahadevan_dual_spaces/",
    "segmentation": [
        [
            "I would like to thank the organizers of RLDM for giving me an opportunity to talk here today.",
            "And what I'd like to do is present abroad framework for solving reinforcement learning problems.",
            "That my students and I have been.",
            "Developing for the last few years and show you some.",
            "Citing recent results that we've gotten.",
            "So at the outset I'd like to."
        ],
        [
            "Thank my collaborators that helped me with this.",
            "This research, most of which would not have happened without further help so.",
            "The folks listed here are basically all from UMass, and in particular I'd like to single out two students.",
            "Beaulieu, whose PhD thesis contains.",
            "You know, some of the results that I'll be talking about, particularly those that connect reinforcement learning with first order stochastic optimization, an Phil Thamus, whose work on safe reinforcement learning.",
            "I'll also be talking about.",
            "In addition, we've been helped a lot by former students.",
            "Mohammad governs our day, who's known Adobe Research Geo Lou, who is an optimization faculty member, Rochester.",
            "He was a student of Stephen Rider Wiscconsin and Mark Patrick, who was a former student of Shlomo Silverstein at UMass, and now is at IBM Research."
        ],
        [
            "So the goal of the framework that we've been developing is quite broad.",
            "It aims to address a number of outstanding problems in reinforcement learning.",
            "And I don't have the time to talk about all of them today, but I'll be focusing on two particular problems, one having to do with reliability.",
            "How can we design reinforcement learning algorithms that are guaranteed to be stable, particularly in training?",
            "Regimes where the behavior that is generating the data.",
            "Maybe different from the behavior that you're learning about the so called off policy regime, which is a pretty characteristic of methods like you learning.",
            "The other issue I'll be talking about is something we heard this morning about safety.",
            "How can we guarantee that a reinforcement learning algorithm will be stable?",
            "The framework actually also addresses the important issue of scalability, and I will briefly talk about how we can.",
            "At how do we address scalability through our framework?",
            "But I won't have time to get into a lot of details and I won't talk about sparsity at all.",
            "So you've heard a lot of different talks in this in this meeting, and I wanted to sort of position myself in terms of where I'm coming from.",
            "I've been working in reinforcement learning for 25 years.",
            "It's hard to believe that it's been that long, but I guess when you're having a good time, you don't notice how time flies.",
            "And my work in reinforcement learning started in early activating 990 when I was at IBM Research and my first job, they put me in a robotics group and I knew nothing of robotics and it was clear nothing in my thesis would help at all.",
            "And I went to ICM on 1989 and I heard Rich sudden talk about reinforcement learning and I got excited and I said, well, this is obviously the paradigm to useful robots, and that started my interest in.",
            "In reinforcement learning so.",
            "Now we can approach the problem with reinforcement learning in many ways and.",
            "What I'm going to talk about today is."
        ],
        [
            "It's helpful to think of sort of Mars 3 level paradigm, which people, especially neuroscience, might might know about David Marr was a researcher in computer vision, but came from neuroscience and articulated this perspective on modeling the brain, where he argued that rather than think of developing a theory at a single level, you really should think about theorizing about the brain at multiple levels and in particular singled out the computational theory level as the one that had been.",
            "The most neglected at that point in time, although that's since changed Mar, was writing this more than 30 years ago, so the computational level really talks about what is the problem that the Organism is solving.",
            "The algorithmic level, of course, is concerned with details of the procedure that needs to be carried out in order to solve the problem identified at the computational theory level.",
            "And finally, the neural implementation talks about how the algorithm might be realized.",
            "Given the constraints, the physical architecture of the brain.",
            "So what I'm going to present is a new theory of reinforcement learning, particularly temporal difference learning, and I'm going to present a new way to think about it, and that is my contribution at the computational theory level.",
            "I'll also be presenting a new algorithm or a new set of algorithms for doing gradient based temporal difference learning, and these algorithms have some characteristics that are, I believe, unique in the sense that for the first time we can give.",
            "Of convergence rate analysis about these algorithms, we can say you know, given N samples exactly how fast they will converge, which was not possible previously.",
            "And we can say how far from optimal these algorithms are, so we can actually characterize what it means for a gradient TD algorithm to be optimal.",
            "As far as the neural implementation goes.",
            "I have really nothing to say and but one of the advantages of coming to a meeting like this is that there are a lot of very smart neuro scientists who stocks have been enormously enjoyable and I hope somebody in that Community would get interested in what I have to say.",
            "So, um."
        ],
        [
            "What is proximal reinforcement learning at a very broad level, it's this that when you're trying to solve a reinforcement learning problem, we normally think of sort of a space being the state.",
            "The space of states, actions, rewards the parameters or coefficients of the representation of the value function, and so on.",
            "And we think of this is just the one one space in proximal reinforcement learning.",
            "We think very deliberately, but two spaces, namely the primal space, where you observe the world, you take actions, you get rewards, and then a dual space.",
            "And this dual spaces constructed very deliberately to make the problem of solving the particular reinforcement learning problem easier.",
            "In particular where I'll show is it helps enormously in the gradient, e.g.",
            "Problem by a process called operators.",
            "Sliding where you know the previous difficulties with sort of terms getting multiplied go away in the dual space we get the ability to very fast gradient updates and also we get this very convenient saddle point formulation that helps a lot in designing algorithms, so we'll take problems that are like this in the sense of given some convex objective function and we can just minimize this objective function by doing gradient descent.",
            "But rather than do it in the primal will convert it to a saddle point problem.",
            "And this will require doing gradient descent on the convex part of the saddle point function.",
            "Which is this and gradient essence on the concave part of the saddle point function and we'll see why it makes sense to transform problem from this form to the other.",
            "Now."
        ],
        [
            "There's a number of publications where different parts of the theory that we've been developing have been published.",
            "In particular, I'll talk about work that's going to appear in the conference on uncertainty in AI that we presented next month and Amsterdam, and I'll also say a little bit about the work that was published in NIPS two years ago on Safe reinforcement learning.",
            "This previous work that I won't be talking about, and that you're welcome to look at if you wanted to get more background."
        ],
        [
            "OK.",
            "So the way I'm going to go by doing this is.",
            "I'm going to sort of give you a very quick overview of the problems we can address.",
            "Then I get to get into the details of the framework, so let's start with this quest to build a gradient TD algorithm.",
            "And let's ask ourselves the question, why do we?",
            "I mean, some of you may be surprised.",
            "You might think that don't we already have a gradient TD algorithm?",
            "So."
        ],
        [
            "If you look at the space of TD methods and this is a figure that's taken from Mainz, very nice PhD dissertation here in Alberta.",
            "Then we see the reinforcement learning problem broken up into the prediction problem.",
            "The control problem and TD methods can be used in a number of different regimes.",
            "There is a so called on policy regime with the behavior that's generating the experience is the same as the behavioral learning from and the off policy regime where the behavior generating the data is different from the behavior we're learning from.",
            "We can also represent the value function exactly using table look up or we can use some sort of approximation.",
            "Now there's good news and bad news.",
            "The good news is that if we stick to a sort of an policy regime and we use no function approximation and T is nice and stable, this is the green.",
            "Are circles or... And in fact, even if we go off policy, but we use no function approximation, Q learning does converge.",
            "The bad news is all of these gold and diamonds where TD is unstable, so this is actually a really serious problem because it's exactly in all of these cases that we want to use TD.",
            "I know we have to reconcile this with the fact that there's been a 20 year history of people trying TD in all of these cases when you shouldn't be using TD because it's not supposed to be stable, and yet they've been having success.",
            "So clearly there's an issue of reconciling the theoretical analysis with the experiments.",
            "But we can we what we want to get is the best of both worlds where we want to have algorithms that are stable theoretically and have good experimental performance.",
            "And that is certainly what you've seen.",
            "The rest of machine learning.",
            "If you look at things like classification, we have algorithms like support vector machines that have very strong theoretical guarantees and very good experimental performance.",
            "So I'm going to focus on one particular case, namely the case of doing linear function approximation in the off policy prediction case and all the analysis that I'm talking about will extend to the other cases.",
            "I'm just going to focus on this."
        ],
        [
            "So if we look at the very simple example of where TD diverges, is the famous Baird counterexample.",
            "So we have a problem here with six States and we have a very simple function approximator with seven parameters, one parameter for every state in one extra parameter that shared by all the states.",
            "And if we imagine and there's no rewards, he ran all transitions that are Mystic except this last one.",
            "And we set up the problem so that the last date here has a very high value initially.",
            "Then every transition from one of these states that this state is going to cause a positive TD error.",
            "So the values of all these states will go up, but every transition from this data itself or to the terminal state will cost the value of this state to go down.",
            "Now if we sample this problem randomly, so we're executing every transition equally often, then there's going to be a lot more increase in these TD values.",
            "Then there will be decreased, and so the TD learning will simply diverge.",
            "So Leemon Baird, who pointed this out, also came up with a solution.",
            "Unfortunately, his solution was not practical in the sense that it required getting multiple samples per transition.",
            "So."
        ],
        [
            "So there's been.",
            "I thought I would add to the sort of several talks had this matrix analogy, and I was thinking we got.",
            "I have to have a matrix analogy in my talk as well.",
            "So here's my attempt at linking this to the Matrix.",
            "So the work on sort of gradient TD has started by starting with some objective function, so I'm going to explain this objective function later, But this is a particular convex objective function that we want to solve and.",
            "And you know these are some of the variants of this objective function that have been studied and you have sort of two choices, right?",
            "You can take the blue pill and this is what people have done in the past.",
            "And this includes the work of Leemon Baird as well as work of rich sudden and his colleagues in 2009.",
            "Unfortunately, if you take the blue pill, it turns out you end up with some difficulties.",
            "We will see later what they are.",
            "But basically you get an expression that is the product of expectations and it's very hard to sample from.",
            "So then you have to do something ad hoc to get around that.",
            "But nobody thought of taking the red pill, which is what I'm going to talk about later.",
            "But if you take the red pill, it turns out that.",
            "You get put in the strange dual space.",
            "It's kind of like the matrix.",
            "If you take the red pill, you're sort of out of, you know, sort of out of the matrix.",
            "So here you're in the strange dual space.",
            "But all your problems go away because.",
            "You know everything, everything, everything decouples nicely and and you don't have the difficulties with with what the difficulties that was the previous way.",
            "So you have your choice.",
            "Either you take the blue pill and stick in the primal space, but deal with all the difficulties with sort of.",
            "Products of expectations?",
            "Or you take the red pill and you go off with the strange new world, but then you don't have any problems, right?",
            "You have choice.",
            "OK, so."
        ],
        [
            "What does that mean?",
            "So what happens is the following.",
            "This objective function that we're trying to minimize is sort of like the composition of two functions F of G of X.",
            "Now, if you take the blue pill, essentially what happens that you get this derived term, which turns out to involve a product of expectations?",
            "And it doesn't sort of work as nicely as you go this way, and miraculously it turns into a sum, and you can actually example easily.",
            "So this is our one of our major contributions to the study of gradient TD methods."
        ],
        [
            "The other contribution is as computer scientists.",
            "One of the things we're taught is.",
            "We have to analyze algorithms in terms of the convergence properties.",
            "How long does it take to run this algorithm analytically?",
            "What kind of an algorithm is it an?",
            "For the longest time, we really haven't had a good understanding Awareness TD fit in this in this kind of analysis, and now I can.",
            "We can show you exactly how it fits and where, where it lies in the space of algorithms."
        ],
        [
            "The other part of what I learned to quickly talk about is.",
            "Are safe reinforcement learning and we heard a lot about this this morning, so I don't have to motivate this for you."
        ],
        [
            "In the particular case study that we looked at was how do we control a robot?",
            "We're not controlling a drone like that ought.",
            "This morning we're controlling this robot with inverted inverted pendulum dynamics, and if you try to do reinforcement learning on this robot without any safety guarantees, the first thing it does is it falls over and it breaks the joint or something.",
            "So we wanted to sort of have a way of controlling this robot.",
            "Having it learn while ensuring that it never goes into any unsafe area and so our proposal for doing this involves the use of this actor, critic system.",
            "In particular, the natural actor critic.",
            "And we connected this algorithm to a very powerful optimization method, so called Mirror descent method, and then we use the safety guarantees for mirror descent to generalize natural actor critic.",
            "So we came up with a new method called projected Natural actor critic, which could be shown to be safe, and in fact we show 2 case studies, one controlling this robot, the other was very similar to the talk we heard this morning also about the control of.",
            "Robotic harms by humans who are handicapped.",
            "There again, you want safety because you don't want the arm to put the human in sort of control regimes that are dangerous, or the human.",
            "And this was the other case study that we had in this paper.",
            "OK, so."
        ],
        [
            "Lil bit of math now.",
            "One of the things that's central to our approach is the notion of conjugate functions, which you may or may not have seen before.",
            "So think of conjugate functions as a sort of transformation of an original function.",
            "So we're giving some function.",
            "In this case A1 dimensional function on X, and we can drive a new function.",
            "A new coordinate system color Lambda.",
            "And what is this new function?",
            "It's the function that, sort of whose value at any any point.",
            "Lambda is the largest distance between the linear function X Lambda and the original function effects.",
            "So in the vector case, of course this would be X transpose alive, and so in this particular case you know here's this function.",
            "Here's Lambda X, so for any X we just look at the at the largest distance between Lambda X and the original function, and that is the value of the conjugate function.",
            "Conjugate function turns out to have deep properties in optimization.",
            "And we're going to use that.",
            "We're going to use that because."
        ],
        [
            "Struck Ting these so called mirror Maps so we started the private space.",
            "We're going to transform this to the dual space and we're going to do this by this genre transform.",
            "So if I take the convex conjugate of a differentiable function that is Alondra transform.",
            "So that gives me a dual space.",
            "I'm going to do the gradient step in the dual space and then I'm going to take the inverse.",
            "Legendre transform to get back to the primal space.",
            "This is a very powerful regime for doing gradient descent, and it's called Mirror descent.",
            "And."
        ],
        [
            "One of the things that we showed in our work is that mirror descent actually generalizes this previous method, called natural gradient, which was used in the natural actor critic, and so this is the way we address the problem of safe reinforcement learning.",
            "And I'll say a little bit more later about how we ensure safety through this.",
            "OK, so we have these two problems we're trying to solve our design.",
            "True gradient temporal difference learning methods how to do, say farlin.",
            "Both were approaching the problem exactly the same way.",
            "We're going to use this technique of mapping to the dual space, and we're going to use the dual space to make our problem similar.",
            "So what is this notion of proximal?",
            "I haven't said anything about that."
        ],
        [
            "So the notion of proxamol is an abstraction of projections.",
            "We're all familiar the concept of projection.",
            "For example, we know that the temporal difference learning algorithm sort of finds a solution of the representation of the value function that satisfies a particular fixed point equation in terms of the projection of the bellman backed up value function.",
            "So proximal mappings generalize.",
            "Projection.",
            "And so given any convex function you the proxamol map of.",
            "Sorry of H, the proximal map of H at some point, X is the vector U that minimizes the value of this convex function and the distance from U2X.",
            "So in other words we find a vector U that is closest to the original value.",
            "We started with original vector X and that simultaneously minimizes the value of the convex function.",
            "So we look at some examples.",
            "So the proxy map of the zero function is just identity, but here's a more interesting one.",
            "The proxy map of the indicator functions of sees a convex set.",
            "The indicator function is 0 inside the convex set, an Infinity outside.",
            "So if we plug in the indicator function over here, this turns out to be nothing but the projection onto a convex set.",
            "So proximal mappings generalize the concept of projection.",
            "But in particular they do something."
        ],
        [
            "Even more interesting that they are very useful in characterizing gradient descent, so let's start with our well known notion of gradient descent, right?",
            "So we're trying to minimize some function F which is differentiable, so we just walk in the direction of the negative gradient.",
            "Now, if I said if this is the answer, what is the question?",
            "So in other words, what is the optimization problem whose solution is gradient descent?",
            "Then it turns out you can write this as a proximal mapping.",
            "So here's approximate mapping problem that says find me a vector U.",
            "That minimizes the sum of these two terms.",
            "The first term is basically a dot product between the gradient of the function at WK, where you start with and you and the second term is again, this distance between you and the position the vector you started with WK.",
            "If you solve this, it turns out you get back exactly gradient descent.",
            "The advantage of looking at gradient descent this way.",
            "Is that it lets you now see that if you want to do gradient descent in different spaces with different geometric properties, you can adjust the second term.",
            "So, for example, imagine the function you're minimizing is on the probability simplex.",
            "Then this term, so that means you and WCM for probability distributions, the vectors we should in order probability distribution then Euclidean distance isn't a very good way of measuring distance between two probability distributions.",
            "We could use something like kale divergent.",
            "So if we rewrite."
        ],
        [
            "This in terms of khalda vergence.",
            "Now I'm giving you the question I'm saying what does the answer?",
            "The answer is a gradient descent method.",
            "That now minimizes a function on the probability simplex, and it's going to look very different from the previous method we wrote.",
            "It's going to be.",
            "This is sometimes called Entropic Mirror descent.",
            "Um, if you're familiar with the literature on multiplicative methods and online learning, it's exactly that.",
            "So what is the wind here?",
            "The wind here is that we can now adapt to the geometry of the space by by changing this distance measure and it turns out that this is how you get scalability, because if you compare it for example, the regret bounds are the convergence guarantees of Entropic mirror descent with regular gradient descent on a function of the probability simplex, you get a huge win.",
            "We're doing in dropping murder sent, so this is how we're going to exploit scalability.",
            "But I don't have time to say more about that here."
        ],
        [
            "OK."
        ],
        [
            "So a little bit more about how we we.",
            "Um?",
            "We solve each of these two problems, so let's start with the gradient problem so.",
            "We know that in the policy evaluation case we're trying to solve the system of linear equations with TD, where we don't know what the reward function is, but we have to sample it.",
            "We don't know the transition probabilities, and we can only experience transitions in the world.",
            "So how do we do it?",
            "We start with some set of value functions that can be represented using a set of features 5, which is, which are the columns of this matrix and Theta is a set of coefficients.",
            "So we start with some guests of the value function.",
            "We take one step.",
            "In the world we sample from this one step, so that gives us a back to value function.",
            "Now because we are in some confined space of features, this one step back to value function may not lie in our space, so we have to project back to the space.",
            "So this is the projection of.",
            "The backed up value function so we know that the TD algorithm.",
            "Tries to find a fixed point such that the projected vector value function equals the original value function you started with.",
            "But we want to ensure that this algorithm converges in the off policy case.",
            "As well, so let's set up following the previous work by Richard and his colleagues.",
            "Let's set up the.",
            "The measure to be the difference between the original value function and the projected backed up value function."
        ],
        [
            "OK.",
            "So if you follow the, if you take the blue pill, you take the primal path.",
            "This is what happens is analysis from the paper.",
            "This is the original metric, the project, the mean square projected Bellman error.",
            "I'll skip this details of derivation, but you end up with this product of terms.",
            "And these terms are basically this is the norm of the expected PD update, and so this is a quadratic form.",
            "That's just measuring the length of the TD update with respect to this matrix.",
            "Measure the covariance the inverse covariance of your feature matrix.",
            "OK.",
            "So obviously this is very awkward, So what they did in the paper was in order to solve this, they broken apart somewhat arbitrarily and they got a different set of algorithms.",
            "So, for example, you can break, you can combine the first 2 terms separately, and then the third term separately, and then you get 2 timescale algorithms.",
            "What we want to do is to somehow avoid this whole process, and we want to do this without having to actually break things apart arbitrarily.",
            "So we're going to take the red pill."
        ],
        [
            "Now before I show you that, let me show you one convenient lemma from Macy's again that we can take this norm of expected TD update, and we can write it as the difference between the right hand side and left hand side of a system of linear equations.",
            "So it's just B -- a Theta.",
            "Where the a matrix is exactly the a matrix you get in the squares TD and the V matrix the be vectors the same thing and see is just the covariance matrix of the features.",
            "The only extra term you might not be familiar with this row, which is just the probability the ratio of the probability of choosing the action with respect to the target policy by the behavior policy so."
        ],
        [
            "You can combine both of these previous objectives for Gradient TD into one by recognizing that in both cases you're measuring the norm of the expected TD update, but the matrix norm varies in one, it's just identity and the other one is the covariance matrix.",
            "So we can abstract the whole thing to say that this is our objective function.",
            "So how do we solve this?"
        ],
        [
            "So we're going to now take this and fund the LaGrange dual, and it's pretty straightforward.",
            "I'll skip the steps, but when you do the LaGrange dual you end up with a saddle point problem.",
            "And then something remarkable happens.",
            "What happens is that the original complication you had, where you had this term, the function of Theta.",
            "Combined with this now splits.",
            "This is now the Legendre transform of the original matrix norm, so this was M. Inverses becomes M. It's very easy to show that why that happens, but this term splits apart from this term.",
            "So now because these two terms have been split, I can just take this as a function of Theta and Y.",
            "It's convex with respect to Theta concave with respect to Y, and I can do a straightforward.",
            "Gradient algorithm for solving saddle point problems.",
            "Something that people in optimization have known for many many years.",
            "So let's do that.",
            "If we do that, what do we end up with?",
            "Will we end up with gradient TD algorithms that Rich Sutton and his colleagues developed OK?"
        ],
        [
            "So.",
            "We can prove that DTD and GT2 are in fact true stochastic gradient algorithms, but not with respect to mean Square projected Bellman error.",
            "But with respect to the saddle point formulation of Mean Square projected Bellman error in the words.",
            "The dual space version of the original objective.",
            "So this is a very straightforward theorem that's in our original paper."
        ],
        [
            "So that's fine, but you might say, well, OK, well, what else do we get with this?",
            "So now we can dig into this vast literature on solving saddlepoint problems and we can characterize.",
            "The convergence rate off Gradient TD meh."
        ],
        [
            "And this is an example of a theorem that in our paper, and it looks pretty complicated.",
            "So let's try to simplify what's going on."
        ],
        [
            "Basically what it's saying is that gradient TD algorithms that have been proposed so far have this convergence behavior.",
            "The converging at the rate 1 / sqrt N. Where N is the number of samples.",
            "This is not surprising because from optimization we know that gradient methods black box methods will in general converge at this rate, but it also tells us all the other terms on top that we should try to see if we can improve, and in fact it turns out that the best we can do for solving a saddle point problem is this.",
            "OK, and there's a big difference between this because the terms that are expensive here is, for example, the norm of the a matrix, which we can try to somehow reduce here because we can try to improve the convergence of this.",
            "And so the second thing we do not paper is now that we know that we're solving a saddle point problem.",
            "We can simply look at the optimization derision and say what is the best way to solve saddle point problems.",
            "And I don't have to tell you I don't have time to go into the details."
        ],
        [
            "But it requires looking at something called a variational inequality, but."
        ],
        [
            "One of the standard ways to solve variational inequalities is the extra gradient method that was out in the 1970s.",
            "And this looks very much like a gradient method, except it does 2 steps.",
            "Imagine you do one step of gradient and then you look ahead and do another step of the gradient and it turns out, for technical reasons this works better than just doing one step.",
            "So we can now."
        ],
        [
            "Combine that we can get the extra gradient TD learning method.",
            "This is what it looks like.",
            "OK, now we can do two things.",
            "We can analyze this theoretically and we can do this and analyze this experimentally as well."
        ],
        [
            "So the theoretical analysis shows that this algorithm lies in between the optimal and GT2.",
            "It's better than GT GT2 because of this term, which is gives us better convergence rate properties than the original algorithm.",
            "But there's still some more, some somewhat of a gap.",
            "So how can we do better so to do better?",
            "We can now sort of take the red pill twice.",
            "OK, which sounds scary.",
            "What does it mean to take the Red Bull twice so?"
        ],
        [
            "So we launch off into our dual space and we do the extra gradient step.",
            "We do this gradient step twice in the dual space and this is the so called mirror Prox algorithm of numerous key from 2005.",
            "So we can now combine this with gradient TD and we get a new class of all."
        ],
        [
            "Rhythms of proximal gradient TD methods, which are again described by U AI paper.",
            "And So what we have now done is given sort of this new way to design gradient TD methods by going into the dual space and exploiting the saddle point formulation.",
            "And so we can handle a very large number of different objective functions.",
            "For example, if you wanted to add sparsification, that's very easy to do.",
            "We can extend this to the nonlinear case.",
            "It's sort of linear function approximation.",
            "If you want to do a nonlinear function approximator, our analysis.",
            "Extends to that, although the results will obviously be a little different."
        ],
        [
            "I just wanted a very quickly go through a couple of results, so this is the Barrett MVP where this is GTD.",
            "Two and this is our method which converges faster with a lower variance.",
            "This is the 50."
        ],
        [
            "The chain where we wanted to show the convergence to the true value function under different learning rates.",
            "So this is a very low learning rate, is a very larger running rate.",
            "And it turns out that our algorithm GMP converges to the true value function under this enormous range of learning rates, whereas GTD.",
            "The original GT algorithm basically does well at the learning rate, is small, but then as the learning rate increases you can see it increasingly diverges from the original solution.",
            "And there are a couple of other examples that I don't have time to get into."
        ],
        [
            "Something very quick on safe reinforcement learning before I fell."
        ],
        [
            "Wish for safe reinforcement learning.",
            "The key algorithm that we developed is an extension of natural actor critic.",
            "We did that by showing that.",
            "Natural gradient descent, which is the basis for natural actor critic, is a special case of mirror descent.",
            "This is the proof from our NIPS 2013 paper.",
            "We identified a specific Legendre transform under which mirror descent produces exactly the same update as a natural actor critic, and once we showed that, then we could just rewrite the natural actor critic algorithm using mirror descent, and we can use the safety guarantees and mirror descent because it's a projected algorithm.",
            "It always remains within the space of feasible solutions.",
            "It was a pretty straightforward thing to do."
        ],
        [
            "And we wanted this is one of the demonstrations we had where we showed that the projected natural actor critic, which is the safe version of the algorithm, converges to a solution which has much much lower variance than the standard natural actor critic.",
            "If you're interested in some of the."
        ],
        [
            "Tell us the very very long paper on archive that gives you all the details of of this this framework."
        ],
        [
            "And let me just finally end by saying that.",
            "We're now working on a new formulation of gradient TD Networks using these revised gradient TD methods, we're trying to do convergence rate analysis and also.",
            "Get some scalable results and trying to integrate all of this in something that's of much interest recently, namely deep learning.",
            "Let me stop there, thank you.",
            "Aaron, if you could come and set up while we take questions.",
            "So I have a question.",
            "So your critique of GTD was based on the fact that it's essentially there's this estimation where you estimate these W vector, which is you have these two separate expectations, and then you have a two time scale algorithm which is separately estimating these two parts and then the overall algorithm combines them together.",
            "That's right, and yet your algorithm you claim is equivalent to GTD, so I guess I'm missing something, which is how the critique doesn't also apply to your algorithm.",
            "If it's equivalent to a two time scale algorithm.",
            "Right, so that's a good point.",
            "So the it's not really a critique, it's as much as much as it's basically the way of deriving GT and GT2 was through appealing through two time scale and that doesn't lend itself to this convergence rate analysis, whereas the way we drive the algorithm is through the saddle point approach, which gives us the true gradient method, which means we can now build at all the convergence rate analysis.",
            "So we get a true gradient method for which the convergence rate analysis apply, and more importantly, it shows us.",
            "How to improve the methods?",
            "Because now we can tap into this vast literature of methods for solving saddle point problems.",
            "That's really the main win I think.",
            "So thanks for the talk.",
            "In the situation where you use the M matrix to be the covariance during the analysis, how do you dealt with the fact that AMC are correlated actually come from the same?",
            "Random variables, so the user split the data set in.",
            "Use the covariance matrix for one of them and.",
            "I don't think I follow the question, so you presented upon seeing that things work nicely and you go at the root of the rate of root N. And so, how do you deal with the fact that see the matrix?",
            "See that you're using for the norm in the Matrix A there built from the same random variables to split the data to have things concentrate nicely?",
            "Or did you some other technique?",
            "Yeah, let's take that offline.",
            "'cause that sounds like something we have to go into.",
            "Details of scripture.",
            "Sorry, maybe a simple question in the learning rates, you had several constants, Tau and Sigma and normal they could you explain what these other constants are?",
            "Yeah, so yeah, I had to rush to that so the the two major constants were Tau and Tau.",
            "Is the maximal singular value of the covariance matrix of your features.",
            "So it's a pretty standard thing that shows up and so on.",
            "The analysis, particularly squares analysis.",
            "Ann and Sigma is a variance term.",
            "Basically, you know when you do the updates, it's a difference between sort of the value of the random variable an.",
            "It's mean the sort of mean squared differences.",
            "Standard variance term.",
            "OK, let's thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I would like to thank the organizers of RLDM for giving me an opportunity to talk here today.",
                    "label": 0
                },
                {
                    "sent": "And what I'd like to do is present abroad framework for solving reinforcement learning problems.",
                    "label": 1
                },
                {
                    "sent": "That my students and I have been.",
                    "label": 0
                },
                {
                    "sent": "Developing for the last few years and show you some.",
                    "label": 0
                },
                {
                    "sent": "Citing recent results that we've gotten.",
                    "label": 0
                },
                {
                    "sent": "So at the outset I'd like to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank my collaborators that helped me with this.",
                    "label": 1
                },
                {
                    "sent": "This research, most of which would not have happened without further help so.",
                    "label": 0
                },
                {
                    "sent": "The folks listed here are basically all from UMass, and in particular I'd like to single out two students.",
                    "label": 0
                },
                {
                    "sent": "Beaulieu, whose PhD thesis contains.",
                    "label": 0
                },
                {
                    "sent": "You know, some of the results that I'll be talking about, particularly those that connect reinforcement learning with first order stochastic optimization, an Phil Thamus, whose work on safe reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "I'll also be talking about.",
                    "label": 0
                },
                {
                    "sent": "In addition, we've been helped a lot by former students.",
                    "label": 0
                },
                {
                    "sent": "Mohammad governs our day, who's known Adobe Research Geo Lou, who is an optimization faculty member, Rochester.",
                    "label": 0
                },
                {
                    "sent": "He was a student of Stephen Rider Wiscconsin and Mark Patrick, who was a former student of Shlomo Silverstein at UMass, and now is at IBM Research.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the goal of the framework that we've been developing is quite broad.",
                    "label": 0
                },
                {
                    "sent": "It aims to address a number of outstanding problems in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And I don't have the time to talk about all of them today, but I'll be focusing on two particular problems, one having to do with reliability.",
                    "label": 0
                },
                {
                    "sent": "How can we design reinforcement learning algorithms that are guaranteed to be stable, particularly in training?",
                    "label": 0
                },
                {
                    "sent": "Regimes where the behavior that is generating the data.",
                    "label": 0
                },
                {
                    "sent": "Maybe different from the behavior that you're learning about the so called off policy regime, which is a pretty characteristic of methods like you learning.",
                    "label": 0
                },
                {
                    "sent": "The other issue I'll be talking about is something we heard this morning about safety.",
                    "label": 0
                },
                {
                    "sent": "How can we guarantee that a reinforcement learning algorithm will be stable?",
                    "label": 0
                },
                {
                    "sent": "The framework actually also addresses the important issue of scalability, and I will briefly talk about how we can.",
                    "label": 0
                },
                {
                    "sent": "At how do we address scalability through our framework?",
                    "label": 0
                },
                {
                    "sent": "But I won't have time to get into a lot of details and I won't talk about sparsity at all.",
                    "label": 0
                },
                {
                    "sent": "So you've heard a lot of different talks in this in this meeting, and I wanted to sort of position myself in terms of where I'm coming from.",
                    "label": 0
                },
                {
                    "sent": "I've been working in reinforcement learning for 25 years.",
                    "label": 0
                },
                {
                    "sent": "It's hard to believe that it's been that long, but I guess when you're having a good time, you don't notice how time flies.",
                    "label": 0
                },
                {
                    "sent": "And my work in reinforcement learning started in early activating 990 when I was at IBM Research and my first job, they put me in a robotics group and I knew nothing of robotics and it was clear nothing in my thesis would help at all.",
                    "label": 0
                },
                {
                    "sent": "And I went to ICM on 1989 and I heard Rich sudden talk about reinforcement learning and I got excited and I said, well, this is obviously the paradigm to useful robots, and that started my interest in.",
                    "label": 0
                },
                {
                    "sent": "In reinforcement learning so.",
                    "label": 0
                },
                {
                    "sent": "Now we can approach the problem with reinforcement learning in many ways and.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to talk about today is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's helpful to think of sort of Mars 3 level paradigm, which people, especially neuroscience, might might know about David Marr was a researcher in computer vision, but came from neuroscience and articulated this perspective on modeling the brain, where he argued that rather than think of developing a theory at a single level, you really should think about theorizing about the brain at multiple levels and in particular singled out the computational theory level as the one that had been.",
                    "label": 0
                },
                {
                    "sent": "The most neglected at that point in time, although that's since changed Mar, was writing this more than 30 years ago, so the computational level really talks about what is the problem that the Organism is solving.",
                    "label": 1
                },
                {
                    "sent": "The algorithmic level, of course, is concerned with details of the procedure that needs to be carried out in order to solve the problem identified at the computational theory level.",
                    "label": 1
                },
                {
                    "sent": "And finally, the neural implementation talks about how the algorithm might be realized.",
                    "label": 0
                },
                {
                    "sent": "Given the constraints, the physical architecture of the brain.",
                    "label": 1
                },
                {
                    "sent": "So what I'm going to present is a new theory of reinforcement learning, particularly temporal difference learning, and I'm going to present a new way to think about it, and that is my contribution at the computational theory level.",
                    "label": 0
                },
                {
                    "sent": "I'll also be presenting a new algorithm or a new set of algorithms for doing gradient based temporal difference learning, and these algorithms have some characteristics that are, I believe, unique in the sense that for the first time we can give.",
                    "label": 1
                },
                {
                    "sent": "Of convergence rate analysis about these algorithms, we can say you know, given N samples exactly how fast they will converge, which was not possible previously.",
                    "label": 0
                },
                {
                    "sent": "And we can say how far from optimal these algorithms are, so we can actually characterize what it means for a gradient TD algorithm to be optimal.",
                    "label": 0
                },
                {
                    "sent": "As far as the neural implementation goes.",
                    "label": 0
                },
                {
                    "sent": "I have really nothing to say and but one of the advantages of coming to a meeting like this is that there are a lot of very smart neuro scientists who stocks have been enormously enjoyable and I hope somebody in that Community would get interested in what I have to say.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is proximal reinforcement learning at a very broad level, it's this that when you're trying to solve a reinforcement learning problem, we normally think of sort of a space being the state.",
                    "label": 0
                },
                {
                    "sent": "The space of states, actions, rewards the parameters or coefficients of the representation of the value function, and so on.",
                    "label": 0
                },
                {
                    "sent": "And we think of this is just the one one space in proximal reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "We think very deliberately, but two spaces, namely the primal space, where you observe the world, you take actions, you get rewards, and then a dual space.",
                    "label": 1
                },
                {
                    "sent": "And this dual spaces constructed very deliberately to make the problem of solving the particular reinforcement learning problem easier.",
                    "label": 0
                },
                {
                    "sent": "In particular where I'll show is it helps enormously in the gradient, e.g.",
                    "label": 0
                },
                {
                    "sent": "Problem by a process called operators.",
                    "label": 0
                },
                {
                    "sent": "Sliding where you know the previous difficulties with sort of terms getting multiplied go away in the dual space we get the ability to very fast gradient updates and also we get this very convenient saddle point formulation that helps a lot in designing algorithms, so we'll take problems that are like this in the sense of given some convex objective function and we can just minimize this objective function by doing gradient descent.",
                    "label": 1
                },
                {
                    "sent": "But rather than do it in the primal will convert it to a saddle point problem.",
                    "label": 0
                },
                {
                    "sent": "And this will require doing gradient descent on the convex part of the saddle point function.",
                    "label": 0
                },
                {
                    "sent": "Which is this and gradient essence on the concave part of the saddle point function and we'll see why it makes sense to transform problem from this form to the other.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a number of publications where different parts of the theory that we've been developing have been published.",
                    "label": 0
                },
                {
                    "sent": "In particular, I'll talk about work that's going to appear in the conference on uncertainty in AI that we presented next month and Amsterdam, and I'll also say a little bit about the work that was published in NIPS two years ago on Safe reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "This previous work that I won't be talking about, and that you're welcome to look at if you wanted to get more background.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the way I'm going to go by doing this is.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sort of give you a very quick overview of the problems we can address.",
                    "label": 0
                },
                {
                    "sent": "Then I get to get into the details of the framework, so let's start with this quest to build a gradient TD algorithm.",
                    "label": 0
                },
                {
                    "sent": "And let's ask ourselves the question, why do we?",
                    "label": 0
                },
                {
                    "sent": "I mean, some of you may be surprised.",
                    "label": 0
                },
                {
                    "sent": "You might think that don't we already have a gradient TD algorithm?",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the space of TD methods and this is a figure that's taken from Mainz, very nice PhD dissertation here in Alberta.",
                    "label": 0
                },
                {
                    "sent": "Then we see the reinforcement learning problem broken up into the prediction problem.",
                    "label": 0
                },
                {
                    "sent": "The control problem and TD methods can be used in a number of different regimes.",
                    "label": 0
                },
                {
                    "sent": "There is a so called on policy regime with the behavior that's generating the experience is the same as the behavioral learning from and the off policy regime where the behavior generating the data is different from the behavior we're learning from.",
                    "label": 0
                },
                {
                    "sent": "We can also represent the value function exactly using table look up or we can use some sort of approximation.",
                    "label": 0
                },
                {
                    "sent": "Now there's good news and bad news.",
                    "label": 0
                },
                {
                    "sent": "The good news is that if we stick to a sort of an policy regime and we use no function approximation and T is nice and stable, this is the green.",
                    "label": 0
                },
                {
                    "sent": "Are circles or... And in fact, even if we go off policy, but we use no function approximation, Q learning does converge.",
                    "label": 0
                },
                {
                    "sent": "The bad news is all of these gold and diamonds where TD is unstable, so this is actually a really serious problem because it's exactly in all of these cases that we want to use TD.",
                    "label": 0
                },
                {
                    "sent": "I know we have to reconcile this with the fact that there's been a 20 year history of people trying TD in all of these cases when you shouldn't be using TD because it's not supposed to be stable, and yet they've been having success.",
                    "label": 0
                },
                {
                    "sent": "So clearly there's an issue of reconciling the theoretical analysis with the experiments.",
                    "label": 0
                },
                {
                    "sent": "But we can we what we want to get is the best of both worlds where we want to have algorithms that are stable theoretically and have good experimental performance.",
                    "label": 0
                },
                {
                    "sent": "And that is certainly what you've seen.",
                    "label": 0
                },
                {
                    "sent": "The rest of machine learning.",
                    "label": 0
                },
                {
                    "sent": "If you look at things like classification, we have algorithms like support vector machines that have very strong theoretical guarantees and very good experimental performance.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to focus on one particular case, namely the case of doing linear function approximation in the off policy prediction case and all the analysis that I'm talking about will extend to the other cases.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to focus on this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we look at the very simple example of where TD diverges, is the famous Baird counterexample.",
                    "label": 1
                },
                {
                    "sent": "So we have a problem here with six States and we have a very simple function approximator with seven parameters, one parameter for every state in one extra parameter that shared by all the states.",
                    "label": 0
                },
                {
                    "sent": "And if we imagine and there's no rewards, he ran all transitions that are Mystic except this last one.",
                    "label": 0
                },
                {
                    "sent": "And we set up the problem so that the last date here has a very high value initially.",
                    "label": 0
                },
                {
                    "sent": "Then every transition from one of these states that this state is going to cause a positive TD error.",
                    "label": 0
                },
                {
                    "sent": "So the values of all these states will go up, but every transition from this data itself or to the terminal state will cost the value of this state to go down.",
                    "label": 0
                },
                {
                    "sent": "Now if we sample this problem randomly, so we're executing every transition equally often, then there's going to be a lot more increase in these TD values.",
                    "label": 0
                },
                {
                    "sent": "Then there will be decreased, and so the TD learning will simply diverge.",
                    "label": 0
                },
                {
                    "sent": "So Leemon Baird, who pointed this out, also came up with a solution.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, his solution was not practical in the sense that it required getting multiple samples per transition.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's been.",
                    "label": 0
                },
                {
                    "sent": "I thought I would add to the sort of several talks had this matrix analogy, and I was thinking we got.",
                    "label": 0
                },
                {
                    "sent": "I have to have a matrix analogy in my talk as well.",
                    "label": 0
                },
                {
                    "sent": "So here's my attempt at linking this to the Matrix.",
                    "label": 0
                },
                {
                    "sent": "So the work on sort of gradient TD has started by starting with some objective function, so I'm going to explain this objective function later, But this is a particular convex objective function that we want to solve and.",
                    "label": 0
                },
                {
                    "sent": "And you know these are some of the variants of this objective function that have been studied and you have sort of two choices, right?",
                    "label": 0
                },
                {
                    "sent": "You can take the blue pill and this is what people have done in the past.",
                    "label": 1
                },
                {
                    "sent": "And this includes the work of Leemon Baird as well as work of rich sudden and his colleagues in 2009.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, if you take the blue pill, it turns out you end up with some difficulties.",
                    "label": 0
                },
                {
                    "sent": "We will see later what they are.",
                    "label": 0
                },
                {
                    "sent": "But basically you get an expression that is the product of expectations and it's very hard to sample from.",
                    "label": 0
                },
                {
                    "sent": "So then you have to do something ad hoc to get around that.",
                    "label": 0
                },
                {
                    "sent": "But nobody thought of taking the red pill, which is what I'm going to talk about later.",
                    "label": 1
                },
                {
                    "sent": "But if you take the red pill, it turns out that.",
                    "label": 0
                },
                {
                    "sent": "You get put in the strange dual space.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like the matrix.",
                    "label": 0
                },
                {
                    "sent": "If you take the red pill, you're sort of out of, you know, sort of out of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So here you're in the strange dual space.",
                    "label": 0
                },
                {
                    "sent": "But all your problems go away because.",
                    "label": 0
                },
                {
                    "sent": "You know everything, everything, everything decouples nicely and and you don't have the difficulties with with what the difficulties that was the previous way.",
                    "label": 0
                },
                {
                    "sent": "So you have your choice.",
                    "label": 0
                },
                {
                    "sent": "Either you take the blue pill and stick in the primal space, but deal with all the difficulties with sort of.",
                    "label": 0
                },
                {
                    "sent": "Products of expectations?",
                    "label": 0
                },
                {
                    "sent": "Or you take the red pill and you go off with the strange new world, but then you don't have any problems, right?",
                    "label": 0
                },
                {
                    "sent": "You have choice.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "So what happens is the following.",
                    "label": 0
                },
                {
                    "sent": "This objective function that we're trying to minimize is sort of like the composition of two functions F of G of X.",
                    "label": 0
                },
                {
                    "sent": "Now, if you take the blue pill, essentially what happens that you get this derived term, which turns out to involve a product of expectations?",
                    "label": 0
                },
                {
                    "sent": "And it doesn't sort of work as nicely as you go this way, and miraculously it turns into a sum, and you can actually example easily.",
                    "label": 0
                },
                {
                    "sent": "So this is our one of our major contributions to the study of gradient TD methods.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other contribution is as computer scientists.",
                    "label": 0
                },
                {
                    "sent": "One of the things we're taught is.",
                    "label": 0
                },
                {
                    "sent": "We have to analyze algorithms in terms of the convergence properties.",
                    "label": 0
                },
                {
                    "sent": "How long does it take to run this algorithm analytically?",
                    "label": 0
                },
                {
                    "sent": "What kind of an algorithm is it an?",
                    "label": 0
                },
                {
                    "sent": "For the longest time, we really haven't had a good understanding Awareness TD fit in this in this kind of analysis, and now I can.",
                    "label": 1
                },
                {
                    "sent": "We can show you exactly how it fits and where, where it lies in the space of algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other part of what I learned to quickly talk about is.",
                    "label": 0
                },
                {
                    "sent": "Are safe reinforcement learning and we heard a lot about this this morning, so I don't have to motivate this for you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the particular case study that we looked at was how do we control a robot?",
                    "label": 0
                },
                {
                    "sent": "We're not controlling a drone like that ought.",
                    "label": 0
                },
                {
                    "sent": "This morning we're controlling this robot with inverted inverted pendulum dynamics, and if you try to do reinforcement learning on this robot without any safety guarantees, the first thing it does is it falls over and it breaks the joint or something.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to sort of have a way of controlling this robot.",
                    "label": 0
                },
                {
                    "sent": "Having it learn while ensuring that it never goes into any unsafe area and so our proposal for doing this involves the use of this actor, critic system.",
                    "label": 0
                },
                {
                    "sent": "In particular, the natural actor critic.",
                    "label": 1
                },
                {
                    "sent": "And we connected this algorithm to a very powerful optimization method, so called Mirror descent method, and then we use the safety guarantees for mirror descent to generalize natural actor critic.",
                    "label": 0
                },
                {
                    "sent": "So we came up with a new method called projected Natural actor critic, which could be shown to be safe, and in fact we show 2 case studies, one controlling this robot, the other was very similar to the talk we heard this morning also about the control of.",
                    "label": 0
                },
                {
                    "sent": "Robotic harms by humans who are handicapped.",
                    "label": 0
                },
                {
                    "sent": "There again, you want safety because you don't want the arm to put the human in sort of control regimes that are dangerous, or the human.",
                    "label": 0
                },
                {
                    "sent": "And this was the other case study that we had in this paper.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lil bit of math now.",
                    "label": 0
                },
                {
                    "sent": "One of the things that's central to our approach is the notion of conjugate functions, which you may or may not have seen before.",
                    "label": 0
                },
                {
                    "sent": "So think of conjugate functions as a sort of transformation of an original function.",
                    "label": 1
                },
                {
                    "sent": "So we're giving some function.",
                    "label": 0
                },
                {
                    "sent": "In this case A1 dimensional function on X, and we can drive a new function.",
                    "label": 0
                },
                {
                    "sent": "A new coordinate system color Lambda.",
                    "label": 0
                },
                {
                    "sent": "And what is this new function?",
                    "label": 0
                },
                {
                    "sent": "It's the function that, sort of whose value at any any point.",
                    "label": 0
                },
                {
                    "sent": "Lambda is the largest distance between the linear function X Lambda and the original function effects.",
                    "label": 0
                },
                {
                    "sent": "So in the vector case, of course this would be X transpose alive, and so in this particular case you know here's this function.",
                    "label": 0
                },
                {
                    "sent": "Here's Lambda X, so for any X we just look at the at the largest distance between Lambda X and the original function, and that is the value of the conjugate function.",
                    "label": 0
                },
                {
                    "sent": "Conjugate function turns out to have deep properties in optimization.",
                    "label": 0
                },
                {
                    "sent": "And we're going to use that.",
                    "label": 0
                },
                {
                    "sent": "We're going to use that because.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Struck Ting these so called mirror Maps so we started the private space.",
                    "label": 0
                },
                {
                    "sent": "We're going to transform this to the dual space and we're going to do this by this genre transform.",
                    "label": 0
                },
                {
                    "sent": "So if I take the convex conjugate of a differentiable function that is Alondra transform.",
                    "label": 0
                },
                {
                    "sent": "So that gives me a dual space.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do the gradient step in the dual space and then I'm going to take the inverse.",
                    "label": 0
                },
                {
                    "sent": "Legendre transform to get back to the primal space.",
                    "label": 1
                },
                {
                    "sent": "This is a very powerful regime for doing gradient descent, and it's called Mirror descent.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the things that we showed in our work is that mirror descent actually generalizes this previous method, called natural gradient, which was used in the natural actor critic, and so this is the way we address the problem of safe reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And I'll say a little bit more later about how we ensure safety through this.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have these two problems we're trying to solve our design.",
                    "label": 0
                },
                {
                    "sent": "True gradient temporal difference learning methods how to do, say farlin.",
                    "label": 0
                },
                {
                    "sent": "Both were approaching the problem exactly the same way.",
                    "label": 0
                },
                {
                    "sent": "We're going to use this technique of mapping to the dual space, and we're going to use the dual space to make our problem similar.",
                    "label": 0
                },
                {
                    "sent": "So what is this notion of proximal?",
                    "label": 0
                },
                {
                    "sent": "I haven't said anything about that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the notion of proxamol is an abstraction of projections.",
                    "label": 0
                },
                {
                    "sent": "We're all familiar the concept of projection.",
                    "label": 0
                },
                {
                    "sent": "For example, we know that the temporal difference learning algorithm sort of finds a solution of the representation of the value function that satisfies a particular fixed point equation in terms of the projection of the bellman backed up value function.",
                    "label": 0
                },
                {
                    "sent": "So proximal mappings generalize.",
                    "label": 0
                },
                {
                    "sent": "Projection.",
                    "label": 0
                },
                {
                    "sent": "And so given any convex function you the proxamol map of.",
                    "label": 0
                },
                {
                    "sent": "Sorry of H, the proximal map of H at some point, X is the vector U that minimizes the value of this convex function and the distance from U2X.",
                    "label": 1
                },
                {
                    "sent": "So in other words we find a vector U that is closest to the original value.",
                    "label": 0
                },
                {
                    "sent": "We started with original vector X and that simultaneously minimizes the value of the convex function.",
                    "label": 0
                },
                {
                    "sent": "So we look at some examples.",
                    "label": 0
                },
                {
                    "sent": "So the proxy map of the zero function is just identity, but here's a more interesting one.",
                    "label": 1
                },
                {
                    "sent": "The proxy map of the indicator functions of sees a convex set.",
                    "label": 0
                },
                {
                    "sent": "The indicator function is 0 inside the convex set, an Infinity outside.",
                    "label": 0
                },
                {
                    "sent": "So if we plug in the indicator function over here, this turns out to be nothing but the projection onto a convex set.",
                    "label": 0
                },
                {
                    "sent": "So proximal mappings generalize the concept of projection.",
                    "label": 0
                },
                {
                    "sent": "But in particular they do something.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Even more interesting that they are very useful in characterizing gradient descent, so let's start with our well known notion of gradient descent, right?",
                    "label": 0
                },
                {
                    "sent": "So we're trying to minimize some function F which is differentiable, so we just walk in the direction of the negative gradient.",
                    "label": 0
                },
                {
                    "sent": "Now, if I said if this is the answer, what is the question?",
                    "label": 0
                },
                {
                    "sent": "So in other words, what is the optimization problem whose solution is gradient descent?",
                    "label": 0
                },
                {
                    "sent": "Then it turns out you can write this as a proximal mapping.",
                    "label": 1
                },
                {
                    "sent": "So here's approximate mapping problem that says find me a vector U.",
                    "label": 0
                },
                {
                    "sent": "That minimizes the sum of these two terms.",
                    "label": 0
                },
                {
                    "sent": "The first term is basically a dot product between the gradient of the function at WK, where you start with and you and the second term is again, this distance between you and the position the vector you started with WK.",
                    "label": 0
                },
                {
                    "sent": "If you solve this, it turns out you get back exactly gradient descent.",
                    "label": 0
                },
                {
                    "sent": "The advantage of looking at gradient descent this way.",
                    "label": 1
                },
                {
                    "sent": "Is that it lets you now see that if you want to do gradient descent in different spaces with different geometric properties, you can adjust the second term.",
                    "label": 0
                },
                {
                    "sent": "So, for example, imagine the function you're minimizing is on the probability simplex.",
                    "label": 0
                },
                {
                    "sent": "Then this term, so that means you and WCM for probability distributions, the vectors we should in order probability distribution then Euclidean distance isn't a very good way of measuring distance between two probability distributions.",
                    "label": 0
                },
                {
                    "sent": "We could use something like kale divergent.",
                    "label": 0
                },
                {
                    "sent": "So if we rewrite.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This in terms of khalda vergence.",
                    "label": 0
                },
                {
                    "sent": "Now I'm giving you the question I'm saying what does the answer?",
                    "label": 0
                },
                {
                    "sent": "The answer is a gradient descent method.",
                    "label": 1
                },
                {
                    "sent": "That now minimizes a function on the probability simplex, and it's going to look very different from the previous method we wrote.",
                    "label": 0
                },
                {
                    "sent": "It's going to be.",
                    "label": 0
                },
                {
                    "sent": "This is sometimes called Entropic Mirror descent.",
                    "label": 0
                },
                {
                    "sent": "Um, if you're familiar with the literature on multiplicative methods and online learning, it's exactly that.",
                    "label": 0
                },
                {
                    "sent": "So what is the wind here?",
                    "label": 0
                },
                {
                    "sent": "The wind here is that we can now adapt to the geometry of the space by by changing this distance measure and it turns out that this is how you get scalability, because if you compare it for example, the regret bounds are the convergence guarantees of Entropic mirror descent with regular gradient descent on a function of the probability simplex, you get a huge win.",
                    "label": 0
                },
                {
                    "sent": "We're doing in dropping murder sent, so this is how we're going to exploit scalability.",
                    "label": 0
                },
                {
                    "sent": "But I don't have time to say more about that here.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a little bit more about how we we.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We solve each of these two problems, so let's start with the gradient problem so.",
                    "label": 0
                },
                {
                    "sent": "We know that in the policy evaluation case we're trying to solve the system of linear equations with TD, where we don't know what the reward function is, but we have to sample it.",
                    "label": 0
                },
                {
                    "sent": "We don't know the transition probabilities, and we can only experience transitions in the world.",
                    "label": 0
                },
                {
                    "sent": "So how do we do it?",
                    "label": 0
                },
                {
                    "sent": "We start with some set of value functions that can be represented using a set of features 5, which is, which are the columns of this matrix and Theta is a set of coefficients.",
                    "label": 0
                },
                {
                    "sent": "So we start with some guests of the value function.",
                    "label": 0
                },
                {
                    "sent": "We take one step.",
                    "label": 0
                },
                {
                    "sent": "In the world we sample from this one step, so that gives us a back to value function.",
                    "label": 0
                },
                {
                    "sent": "Now because we are in some confined space of features, this one step back to value function may not lie in our space, so we have to project back to the space.",
                    "label": 0
                },
                {
                    "sent": "So this is the projection of.",
                    "label": 0
                },
                {
                    "sent": "The backed up value function so we know that the TD algorithm.",
                    "label": 0
                },
                {
                    "sent": "Tries to find a fixed point such that the projected vector value function equals the original value function you started with.",
                    "label": 0
                },
                {
                    "sent": "But we want to ensure that this algorithm converges in the off policy case.",
                    "label": 0
                },
                {
                    "sent": "As well, so let's set up following the previous work by Richard and his colleagues.",
                    "label": 0
                },
                {
                    "sent": "Let's set up the.",
                    "label": 0
                },
                {
                    "sent": "The measure to be the difference between the original value function and the projected backed up value function.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if you follow the, if you take the blue pill, you take the primal path.",
                    "label": 0
                },
                {
                    "sent": "This is what happens is analysis from the paper.",
                    "label": 0
                },
                {
                    "sent": "This is the original metric, the project, the mean square projected Bellman error.",
                    "label": 0
                },
                {
                    "sent": "I'll skip this details of derivation, but you end up with this product of terms.",
                    "label": 0
                },
                {
                    "sent": "And these terms are basically this is the norm of the expected PD update, and so this is a quadratic form.",
                    "label": 0
                },
                {
                    "sent": "That's just measuring the length of the TD update with respect to this matrix.",
                    "label": 0
                },
                {
                    "sent": "Measure the covariance the inverse covariance of your feature matrix.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So obviously this is very awkward, So what they did in the paper was in order to solve this, they broken apart somewhat arbitrarily and they got a different set of algorithms.",
                    "label": 0
                },
                {
                    "sent": "So, for example, you can break, you can combine the first 2 terms separately, and then the third term separately, and then you get 2 timescale algorithms.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is to somehow avoid this whole process, and we want to do this without having to actually break things apart arbitrarily.",
                    "label": 0
                },
                {
                    "sent": "So we're going to take the red pill.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now before I show you that, let me show you one convenient lemma from Macy's again that we can take this norm of expected TD update, and we can write it as the difference between the right hand side and left hand side of a system of linear equations.",
                    "label": 0
                },
                {
                    "sent": "So it's just B -- a Theta.",
                    "label": 0
                },
                {
                    "sent": "Where the a matrix is exactly the a matrix you get in the squares TD and the V matrix the be vectors the same thing and see is just the covariance matrix of the features.",
                    "label": 0
                },
                {
                    "sent": "The only extra term you might not be familiar with this row, which is just the probability the ratio of the probability of choosing the action with respect to the target policy by the behavior policy so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can combine both of these previous objectives for Gradient TD into one by recognizing that in both cases you're measuring the norm of the expected TD update, but the matrix norm varies in one, it's just identity and the other one is the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So we can abstract the whole thing to say that this is our objective function.",
                    "label": 0
                },
                {
                    "sent": "So how do we solve this?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to now take this and fund the LaGrange dual, and it's pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "I'll skip the steps, but when you do the LaGrange dual you end up with a saddle point problem.",
                    "label": 0
                },
                {
                    "sent": "And then something remarkable happens.",
                    "label": 0
                },
                {
                    "sent": "What happens is that the original complication you had, where you had this term, the function of Theta.",
                    "label": 0
                },
                {
                    "sent": "Combined with this now splits.",
                    "label": 0
                },
                {
                    "sent": "This is now the Legendre transform of the original matrix norm, so this was M. Inverses becomes M. It's very easy to show that why that happens, but this term splits apart from this term.",
                    "label": 0
                },
                {
                    "sent": "So now because these two terms have been split, I can just take this as a function of Theta and Y.",
                    "label": 0
                },
                {
                    "sent": "It's convex with respect to Theta concave with respect to Y, and I can do a straightforward.",
                    "label": 0
                },
                {
                    "sent": "Gradient algorithm for solving saddle point problems.",
                    "label": 0
                },
                {
                    "sent": "Something that people in optimization have known for many many years.",
                    "label": 0
                },
                {
                    "sent": "So let's do that.",
                    "label": 0
                },
                {
                    "sent": "If we do that, what do we end up with?",
                    "label": 0
                },
                {
                    "sent": "Will we end up with gradient TD algorithms that Rich Sutton and his colleagues developed OK?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can prove that DTD and GT2 are in fact true stochastic gradient algorithms, but not with respect to mean Square projected Bellman error.",
                    "label": 0
                },
                {
                    "sent": "But with respect to the saddle point formulation of Mean Square projected Bellman error in the words.",
                    "label": 1
                },
                {
                    "sent": "The dual space version of the original objective.",
                    "label": 0
                },
                {
                    "sent": "So this is a very straightforward theorem that's in our original paper.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's fine, but you might say, well, OK, well, what else do we get with this?",
                    "label": 0
                },
                {
                    "sent": "So now we can dig into this vast literature on solving saddlepoint problems and we can characterize.",
                    "label": 0
                },
                {
                    "sent": "The convergence rate off Gradient TD meh.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is an example of a theorem that in our paper, and it looks pretty complicated.",
                    "label": 0
                },
                {
                    "sent": "So let's try to simplify what's going on.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically what it's saying is that gradient TD algorithms that have been proposed so far have this convergence behavior.",
                    "label": 0
                },
                {
                    "sent": "The converging at the rate 1 / sqrt N. Where N is the number of samples.",
                    "label": 0
                },
                {
                    "sent": "This is not surprising because from optimization we know that gradient methods black box methods will in general converge at this rate, but it also tells us all the other terms on top that we should try to see if we can improve, and in fact it turns out that the best we can do for solving a saddle point problem is this.",
                    "label": 0
                },
                {
                    "sent": "OK, and there's a big difference between this because the terms that are expensive here is, for example, the norm of the a matrix, which we can try to somehow reduce here because we can try to improve the convergence of this.",
                    "label": 0
                },
                {
                    "sent": "And so the second thing we do not paper is now that we know that we're solving a saddle point problem.",
                    "label": 0
                },
                {
                    "sent": "We can simply look at the optimization derision and say what is the best way to solve saddle point problems.",
                    "label": 0
                },
                {
                    "sent": "And I don't have to tell you I don't have time to go into the details.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it requires looking at something called a variational inequality, but.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the standard ways to solve variational inequalities is the extra gradient method that was out in the 1970s.",
                    "label": 0
                },
                {
                    "sent": "And this looks very much like a gradient method, except it does 2 steps.",
                    "label": 0
                },
                {
                    "sent": "Imagine you do one step of gradient and then you look ahead and do another step of the gradient and it turns out, for technical reasons this works better than just doing one step.",
                    "label": 0
                },
                {
                    "sent": "So we can now.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Combine that we can get the extra gradient TD learning method.",
                    "label": 0
                },
                {
                    "sent": "This is what it looks like.",
                    "label": 0
                },
                {
                    "sent": "OK, now we can do two things.",
                    "label": 0
                },
                {
                    "sent": "We can analyze this theoretically and we can do this and analyze this experimentally as well.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the theoretical analysis shows that this algorithm lies in between the optimal and GT2.",
                    "label": 0
                },
                {
                    "sent": "It's better than GT GT2 because of this term, which is gives us better convergence rate properties than the original algorithm.",
                    "label": 0
                },
                {
                    "sent": "But there's still some more, some somewhat of a gap.",
                    "label": 0
                },
                {
                    "sent": "So how can we do better so to do better?",
                    "label": 0
                },
                {
                    "sent": "We can now sort of take the red pill twice.",
                    "label": 0
                },
                {
                    "sent": "OK, which sounds scary.",
                    "label": 0
                },
                {
                    "sent": "What does it mean to take the Red Bull twice so?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we launch off into our dual space and we do the extra gradient step.",
                    "label": 0
                },
                {
                    "sent": "We do this gradient step twice in the dual space and this is the so called mirror Prox algorithm of numerous key from 2005.",
                    "label": 0
                },
                {
                    "sent": "So we can now combine this with gradient TD and we get a new class of all.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rhythms of proximal gradient TD methods, which are again described by U AI paper.",
                    "label": 0
                },
                {
                    "sent": "And So what we have now done is given sort of this new way to design gradient TD methods by going into the dual space and exploiting the saddle point formulation.",
                    "label": 0
                },
                {
                    "sent": "And so we can handle a very large number of different objective functions.",
                    "label": 0
                },
                {
                    "sent": "For example, if you wanted to add sparsification, that's very easy to do.",
                    "label": 0
                },
                {
                    "sent": "We can extend this to the nonlinear case.",
                    "label": 0
                },
                {
                    "sent": "It's sort of linear function approximation.",
                    "label": 0
                },
                {
                    "sent": "If you want to do a nonlinear function approximator, our analysis.",
                    "label": 0
                },
                {
                    "sent": "Extends to that, although the results will obviously be a little different.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just wanted a very quickly go through a couple of results, so this is the Barrett MVP where this is GTD.",
                    "label": 0
                },
                {
                    "sent": "Two and this is our method which converges faster with a lower variance.",
                    "label": 0
                },
                {
                    "sent": "This is the 50.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The chain where we wanted to show the convergence to the true value function under different learning rates.",
                    "label": 0
                },
                {
                    "sent": "So this is a very low learning rate, is a very larger running rate.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that our algorithm GMP converges to the true value function under this enormous range of learning rates, whereas GTD.",
                    "label": 0
                },
                {
                    "sent": "The original GT algorithm basically does well at the learning rate, is small, but then as the learning rate increases you can see it increasingly diverges from the original solution.",
                    "label": 0
                },
                {
                    "sent": "And there are a couple of other examples that I don't have time to get into.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something very quick on safe reinforcement learning before I fell.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wish for safe reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "The key algorithm that we developed is an extension of natural actor critic.",
                    "label": 0
                },
                {
                    "sent": "We did that by showing that.",
                    "label": 0
                },
                {
                    "sent": "Natural gradient descent, which is the basis for natural actor critic, is a special case of mirror descent.",
                    "label": 0
                },
                {
                    "sent": "This is the proof from our NIPS 2013 paper.",
                    "label": 0
                },
                {
                    "sent": "We identified a specific Legendre transform under which mirror descent produces exactly the same update as a natural actor critic, and once we showed that, then we could just rewrite the natural actor critic algorithm using mirror descent, and we can use the safety guarantees and mirror descent because it's a projected algorithm.",
                    "label": 0
                },
                {
                    "sent": "It always remains within the space of feasible solutions.",
                    "label": 0
                },
                {
                    "sent": "It was a pretty straightforward thing to do.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we wanted this is one of the demonstrations we had where we showed that the projected natural actor critic, which is the safe version of the algorithm, converges to a solution which has much much lower variance than the standard natural actor critic.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in some of the.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tell us the very very long paper on archive that gives you all the details of of this this framework.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let me just finally end by saying that.",
                    "label": 0
                },
                {
                    "sent": "We're now working on a new formulation of gradient TD Networks using these revised gradient TD methods, we're trying to do convergence rate analysis and also.",
                    "label": 0
                },
                {
                    "sent": "Get some scalable results and trying to integrate all of this in something that's of much interest recently, namely deep learning.",
                    "label": 0
                },
                {
                    "sent": "Let me stop there, thank you.",
                    "label": 0
                },
                {
                    "sent": "Aaron, if you could come and set up while we take questions.",
                    "label": 0
                },
                {
                    "sent": "So I have a question.",
                    "label": 0
                },
                {
                    "sent": "So your critique of GTD was based on the fact that it's essentially there's this estimation where you estimate these W vector, which is you have these two separate expectations, and then you have a two time scale algorithm which is separately estimating these two parts and then the overall algorithm combines them together.",
                    "label": 0
                },
                {
                    "sent": "That's right, and yet your algorithm you claim is equivalent to GTD, so I guess I'm missing something, which is how the critique doesn't also apply to your algorithm.",
                    "label": 0
                },
                {
                    "sent": "If it's equivalent to a two time scale algorithm.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's a good point.",
                    "label": 0
                },
                {
                    "sent": "So the it's not really a critique, it's as much as much as it's basically the way of deriving GT and GT2 was through appealing through two time scale and that doesn't lend itself to this convergence rate analysis, whereas the way we drive the algorithm is through the saddle point approach, which gives us the true gradient method, which means we can now build at all the convergence rate analysis.",
                    "label": 0
                },
                {
                    "sent": "So we get a true gradient method for which the convergence rate analysis apply, and more importantly, it shows us.",
                    "label": 0
                },
                {
                    "sent": "How to improve the methods?",
                    "label": 0
                },
                {
                    "sent": "Because now we can tap into this vast literature of methods for solving saddle point problems.",
                    "label": 0
                },
                {
                    "sent": "That's really the main win I think.",
                    "label": 0
                },
                {
                    "sent": "So thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "In the situation where you use the M matrix to be the covariance during the analysis, how do you dealt with the fact that AMC are correlated actually come from the same?",
                    "label": 0
                },
                {
                    "sent": "Random variables, so the user split the data set in.",
                    "label": 0
                },
                {
                    "sent": "Use the covariance matrix for one of them and.",
                    "label": 0
                },
                {
                    "sent": "I don't think I follow the question, so you presented upon seeing that things work nicely and you go at the root of the rate of root N. And so, how do you deal with the fact that see the matrix?",
                    "label": 0
                },
                {
                    "sent": "See that you're using for the norm in the Matrix A there built from the same random variables to split the data to have things concentrate nicely?",
                    "label": 0
                },
                {
                    "sent": "Or did you some other technique?",
                    "label": 0
                },
                {
                    "sent": "Yeah, let's take that offline.",
                    "label": 0
                },
                {
                    "sent": "'cause that sounds like something we have to go into.",
                    "label": 0
                },
                {
                    "sent": "Details of scripture.",
                    "label": 0
                },
                {
                    "sent": "Sorry, maybe a simple question in the learning rates, you had several constants, Tau and Sigma and normal they could you explain what these other constants are?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yeah, I had to rush to that so the the two major constants were Tau and Tau.",
                    "label": 0
                },
                {
                    "sent": "Is the maximal singular value of the covariance matrix of your features.",
                    "label": 0
                },
                {
                    "sent": "So it's a pretty standard thing that shows up and so on.",
                    "label": 0
                },
                {
                    "sent": "The analysis, particularly squares analysis.",
                    "label": 0
                },
                {
                    "sent": "Ann and Sigma is a variance term.",
                    "label": 0
                },
                {
                    "sent": "Basically, you know when you do the updates, it's a difference between sort of the value of the random variable an.",
                    "label": 0
                },
                {
                    "sent": "It's mean the sort of mean squared differences.",
                    "label": 0
                },
                {
                    "sent": "Standard variance term.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank you again.",
                    "label": 0
                }
            ]
        }
    }
}