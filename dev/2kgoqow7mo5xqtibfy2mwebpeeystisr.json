{
    "id": "2kgoqow7mo5xqtibfy2mwebpeeystisr",
    "title": "Invited Talk: Empirical Risk Minimization with Statistics of Higher Order with Examples from Bipartite Ranking",
    "info": {
        "author": [
            "Nicolas Vayatis, Centre de Math\u00e9matiques et de Leurs Applications, Ecole normale sup\u00e9rieure de Cachan"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_vayatis_erms/",
    "segmentation": [
        [
            "Work with Steven Clemenceau and Gabor Lugosi for some part of the talk, but I was in the last years I was working on the ranking problem, mainly with Steven and I will give you, well, you will have some quick overview of some of the works we done over the years.",
            "OK, so this talk is about learning from IID data.",
            "I said ID.",
            "And I think you guys are crazy to work with non IID data.",
            "I mean as a theorist.",
            "There are already many, many questions.",
            "Many issues to address in the IDE setup so.",
            "It's a kind of difficult step you don't take and move to non IID, so I will deal with IID sequences of data.",
            "But in many situations because of the nature of the problem and the goal we are pursuing.",
            "We come up with some optimization principles which required to deal with some Association of the data which make them look non.",
            "I well known IID OK so this will be the focus of the presentation and I will mainly consider the mathematical side.",
            "Well this will be the.",
            "This is the stream of the presentation.",
            "The topic will be the study, the mathematical study of empirical risk minimization strategies based on.",
            "Some particular statistics I will describe, but there will also be some underlying topic, and I think this is the most interesting part for learning applications which has to do with the criteria we use in learning application, especially in the bipartite ranking problem for which the setup is simple.",
            "I mean, it's just classification data, but the things you want to do are much more complicated than the standard classification problem.",
            "And there are many different performance measures that can be considered and lead to many questions."
        ],
        [
            "OK so first just to give you a general idea of the kind of problems we look at.",
            "So we consider the case where the individual observations are IID so the size they can be pairs XIYI.",
            "If you deal with supervised classification problem.",
            "But I mean, you can also imagine different setups, but the idea is that this is observed."
        ],
        [
            "Things are IID and we study some learning problem and the losses.",
            "Some function L and we want to find a good decision rule denoted by F in a class of candidate decision rules.",
            "And then you can formulate empirical risk minimization strategies as estimators of F of F of a good F as minimizers of.",
            "This type of statistical functional so like the average of some function of the observations of the loss function of the decision rule F OK and this is an average of IID random variables.",
            "And if you want to assess performance of such strategies then you have to deal with this empirical processes here, which are the of this form and they are supremum over the class of functions.",
            "Of these kind of objects?",
            "OK, so you can think many formulations can lead to this general problem and the I mean the absolute tool to deal with these quantities come from the theory of empirical processes, OK?",
            "So this is the what was done over the years in the theory of binary classification in the study of convex risk minimization methods.",
            "Like boosting or SVM an many in many other setup you would end up with this kind of object here.",
            "OK, so now."
        ],
        [
            "But if we still have these IID data, but now the empirical risk takes the form of an average over functions of a pair of random variables from the the ID sequence.",
            "So, like entrust 1 / N choose two the."
        ],
        [
            "Sum over all pairs IJ of some function of ZIJ&F the decision rule.",
            "Or it can also be something like that.",
            "So some function which depends of.",
            "I saw an average of functions of different functions of all the sample.",
            "OK, so here you no longer have some an average of IID.",
            "OK, so you need to adapt the mathematical tools to describe the same type of properties.",
            "OK, so to get consistency results, rates of convergence.",
            "And this type of thing.",
            "OK so.",
            "This will be the.",
            "The main general setup for the talk.",
            "So first I will just recall what was done in the simple K. Well, the simple it's not so simple, but what is now well known about the stuff?"
        ],
        [
            "Other binary classification and then we'll consider various examples for which we need to adapt the existing results.",
            "So there is also a section on algorithms, but I probably won't have time to discuss it, but I want to advertise it, so maybe I will just pass you some some slides very quickly, but you will see some colors and some advertisement for a demo presentation at NIPS.",
            "OK.",
            "Uh."
        ],
        [
            "So let's start with binary classification.",
            "So first notations.",
            "So we have a pair."
        ],
        [
            "XY, where X is a descriptor of the individual and Y is a label.",
            "So here I will deal only with binary labels.",
            "So why is minus one or plus one and the statistical model for such observations is given by a probability by the joint probability distribution P for the pair XY.",
            "An important quantity here is the posterior probability at of X, which is the probability to observe.",
            "A label equal to plus one given the observed given the observation vector X. OK, so the behavior of this function.",
            "Tells us how difficult the learning problem is.",
            "So to get an idea why is."
        ],
        [
            "But so we have to look at the optimal elements for this problem.",
            "So we look for decision rules which take which are functions over the set over the space where X leaves.",
            "And this functions take values plus one or minus one.",
            "An the performance measure or the risk measure for this problem, while the simplest choice is to take the classification errors.",
            "So the probability of making a wrong prediction.",
            "And then if we knew the distribution P, we would know which are the best possible decision rules an what is the minimal possible value of this classification error.",
            "OK, so the optimal rule is probably most of you know that is the function which predicts plus one if the regression function is larger than 1/2 and minus one otherwise.",
            "OK, so then you can write down the optimal value of the error and you can also write down the excess of risk for any.",
            "Arbitrary classifier G with respect to the optimum.",
            "So what is important here it is to know that what we are after here is just one single level set of the posterior probability at of X.",
            "We want to discover to learn this set of axes such that F of X is larger than 1/2.",
            "Of course, we don't know the ETA, so we need to cook up some method to to try to approximate to try to approximate these sets, but.",
            "What is important here is the behavior of eight of X around 1/2, so the steepest axis at 1/2, the easier the classification problem.",
            "OK, if FX is close to 1/2, then it's difficult to decide whether you should take plus one or minus one, so it makes things more difficult and it makes the value as star larger.",
            "If you have whole regions where at of X is close to 1/2."
        ],
        [
            "OK, So what kind of strategy?",
            "So here I just want to give the general idea, so I just focus on the simple strategy based on minimizing the frequency of error over some data sample of IID copies of the pair XY.",
            "So denote by G hat the minimizer of this empirical error.",
            "So we have an empirical empirical risk.",
            "Minimizer and the common analysis relies on concentration inequality's, where you can control the error of this estimator with respect to the minimal achievable error over the set of candidates.",
            "You can control it by the expected value of the uniform deviation of the empirical risk with respect to the true risk, so the true error plus some corrective term here, which is of the other.",
            "One over square root of North and then the game is to try to.",
            "Gives some explicit bound for this quantity, so if you like to work with VC dimension, then actually this is.",
            "The best possible rate you can find if the class of candidates is not too big.",
            "You can control this complexity term by term, which is of the order of one over square root of North.",
            "So you recover the parametric rate of convergence in statistical problems.",
            "So this is the kind of optimum you can expect for the rate of convergence.",
            "But now if you want to deal with.",
            "Let's say practical algorithms and efficient algorithms.",
            "They work with much larger classes than VC classes, so a good description of the complexity of the I mean the sensitive.",
            "Concept of the critical concept of complexities rather this Rademacher averages.",
            "So if you know about this, that's fine, if not, never mind, but this is a Richard."
        ],
        [
            "Scription of complexity, and I mean most state of the art results are based on the control of such quantities.",
            "OK, so, but we expect that for larger classes this could get maybe larger than one over square root of North.",
            "For some classes, like kernel classes, you can still have this.",
            "This of course it depends on the gram matrix, but you can still get this kind of one over square root of.",
            "And rates OK, so this these rates are really important.",
            "I mean, it's not only a matter of you know, Thierry.",
            "I mean it tells you how many observations you need to learn some functions, so it's a very of course.",
            "These rates are far from the reality, but but it gives you some indication and and if you can achieve a faster rate that maybe that's an indication that you have a better method.",
            "OK.",
            "But this this I called this the 1st order analysis."
        ],
        [
            "Can only bring you down to 1 / sqrt N Now for some distributions.",
            "Imagine that around the frontier where at of X is equal to 1/2.",
            "You have some margin and there are not so many observation in this part of the of the domain.",
            "So then you can reach faster rate than one over square root of North.",
            "But to see this you need to apply more advanced results in concentration inequality such that such like the Talagrand concentration inequality which involve an additional term here.",
            "Which depends on the variance of the process, so I'm not giving you the.",
            "The precise value, but it's it's A kind of variance measure here, and if you play with this quantity then you can achieve faster rates.",
            "So the standard assumption.",
            "Well, this is a sufficient condition to guarantee this faster rates is to have a control of the variance by some power of the excess risk, and then if you plug this inequality in this one then you can see that depending on the value of Alpha, you can reach up to.",
            "Rates of convergence of 1 / N IF Alpha is equal to 1 so of course this puts some additional assumption on the distribution.",
            "OK, so.",
            "So what kind of assumption this is?",
            "Well, there are some."
        ],
        [
            "Other statements of the assumptions which lead to this variance control condition, which for which we can play in the inequality's.",
            "But maybe the easiest way to give a grasp on this one is this inequality.",
            "Here that says that there exists some constant B positive that such that for all T positive we have that the probability of at a twice at of X -- 1 being smaller than T is smaller than this quantity here and this describes.",
            "I mean the density of points around the border of between the two populations.",
            "OK, so for Alpha equal to 0, it's like you have no condition, so you recover the one over square root of rates and if Alpha equals to one, it means that you have really a few.",
            "It actually it means that you have a jump at the level of eight out of X equal to 1/2 for at X, so you don't have a jump in the value, so.",
            "This means that you have a clear cut frontier and this makes the classification problem much easier, and this explains the faster rates.",
            "OK, so in now in many app."
        ],
        [
            "Vacations this kind of criterion we invoked here is not very interesting because in many applications you have an A symmetry between the two types of errors, so predicting that someone is healthy while he's sick is much worse than than the opposite, so.",
            "We want to maybe to distinguish the two types of error and maybe also we have different things in mind and not just, you know, discriminating data.",
            "But maybe we want to rank the data.",
            "But still we have only this binary information on the label, so we know.",
            "We know if we if we do a query on a search engine there are relevant pages and irrelevant pages, but still in the end you want to come up with a list.",
            "OK, so how would we deal with this kind of objective?",
            "So first I will.",
            "I will play here with the the statistical model in the beginning for classification I said that we deal with a sequence of IID xiy I.",
            "And now we will consider a slightly different model, which actually includes the previous one, but which makes clear what kind of goal we are after when we deal, for instance, with the bipartite."
        ],
        [
            "Tracking problem.",
            "OK, so this first part is joint work with Stefan and Gabor.",
            "OK, so this is the."
        ],
        [
            "Different statistical model and we can call this preference model.",
            "So now we have a triple of random elements.",
            "So XX prime are of the same nature.",
            "There are a pair of observations similar to the X we had before.",
            "An R is a ranking label or a preference label.",
            "So R tells us which one of X or X prime is the better OK.",
            "So we'll say that our response it if if X is better than X prime.",
            "OK, so now let's try to write down.",
            "I mean when we know the distribution, what would be the optimal elements?",
            "If we deal with this with this model and just think about doing classification, but over this over the product space of the observations.",
            "OK, So what plays the role of Etta here is this pair of function row plus and row minus that tells us when well that gives us the probability of the ranking label being positive or negative.",
            "Given the pair XX prime.",
            "OK, so if we actually observe the individual labels then you can just take the ranking label equal to the difference between the two individual labels.",
            "OK, so this contains the the classification model.",
            "We stated before, but it can be.",
            "Also it can also contain the regression model, for instance.",
            "OK, so let's just consider now the.",
            "The ranking error, which is the error when we."
        ],
        [
            "Wrongly predict.",
            "I mean, we count how many times we wrongly predict the ranking label with some ranking rule here which plays the role of the classifiers in this space and the ranking rules operate over the product space of X by itself and potentially they can take three values minus 101.",
            "So it's not actually a multiclass, it's you can.",
            "I mean, if you take this kind of measure for the error, you can play with this you can take different conventions, but if you play if you use this kind of convention here for the.",
            "Classification error then you are basically back down to the binary classification set up, except that you have to describe precisely the optimal elements in that case.",
            "So here the best possible rule would predict plus one if Pro Plus is larger than than row minus.",
            "OK, so now if we see what this means, if we go back to the model for the individual observations and this individual observations are classified."
        ],
        [
            "Patient data, then the row plus Andro minus you can well the row plus can be exactly written as the product of 8 of X * 1 -- F of X prime.",
            "Andro minus is just 1 -- 8 of X * F of X prime, and the optimal rule written here is exactly this one, so it predicts that X is better than X prime if F of X is larger than F of X prime, which is intuitive.",
            "I mean it's.",
            "It's it's clear.",
            "OK, it's exactly what we expect and you can write down the optimal air for this setup and what I wanted to show you here is what replaces the quantity of X -- 1/2.",
            "In this case, is the variation of Etta between these two points X&X prime?",
            "OK, so the smaller this quantity, the more difficult.",
            "Learning the preference because it means that X is very close together X prime.",
            "And we cannot really say which one is best.",
            "So ideally in this problem we want to recover, not the exactly the level sets of data, but in some kind of.",
            "Global, I mean global comparison of the variation between two points of these functions.",
            "OK, so now maybe I will skip this slide, but just to tell."
        ],
        [
            "So that if we deal with regression data, you can also write down precisely what is the optimal rule and basically says that the same thing, except that the regression function for regression data is slightly different here.",
            "But you have the same type of behavior.",
            "OK, so it looks like for preference learning is exactly the same as classification, and actually algorithms like Rank Boot."
        ],
        [
            "Step more or less what they suggest that you can just do classification, but you use these pairs of observations and you work on the product space OK and so is it the same.",
            "Well, from theoretical point of view, it's not the same, and the reason is that if you write down the empirical counterpart of the criterion of the ranking error, then it takes this form because we have two pairs, XY&X prime, Y prime.",
            "So if you discretize the previous.",
            "Notion of error.",
            "Then you come up with a quantity like this which takes the average of all pairs of IJ which count the inversions in the ranking between two observations.",
            "OK, so if you have to consider all these pairs together and of course you no longer have an average of IID, you have a very well known quantity in statistics which is called EU statistic.",
            "OK, so the general form of you statistic is this one, so you have individual observations which are IID.",
            "But now you deal with the average of a function of pairs.",
            "You can also.",
            "I mean, this is a you statistique of order two, but you can also consider you statistiques of higher order, except that well in this case in the case in the example we come up with this kind of object here.",
            "So can we redo all the theory of empirical risk minimization for?",
            "Minimizing such principles.",
            "And the answer is yes, but it's quite more difficult that in the in the previous set up and also there was something in training in the beginning when we began to think about this problem is that actually there were some work trying to justify, you know, an algorithm like rank boost or or similar methods.",
            "And in most papers people would take the assumptions that OK. Let's take the pairs such that the.",
            "These pairs are IID OK, so of course in that case it's trivial because it's just simple classification.",
            "So we try to look at what this."
        ],
        [
            "Statistique look like I mean, what does the statistical theory tell us about these objects and in.",
            "In the store ikle paper by hurting where there is this famous earnings inequality, there is a section on you statistiques and there are there is a deviation inequality for you statistics an the key idea is to say that we can write this previous average of function of pairs of the of the observations as an average over permutations of IID blocks.",
            "I mean sums of IID blocks.",
            "OK, so if you just play with indices.",
            "You can reorder.",
            "This some in order to have you know blocks of size N / 2, while the integer part of divided by two, wherein this sum you all the pairs, they don't overlap, so you have only different indices, so this is what is inside this.",
            "Some here is really a sum of IID, but you also have to take the average over all permutations of indices.",
            "OK, so just by this trick here of taking permutation of permutation.",
            "Well, you regroup here Z with an index, which is the the permutation of I and Z where the index is the permutation of I plus and divided by two, and then you can guarantee that that the this indices do not overlap inside the sum.",
            "OK, so if you can, if you use this representation of you statistique then it's very easy by I mean it's just a. Markov inequality applied on an exponential scale, but you can show that and you can show it for other functions that the exponential that the expected value of the exponential of the soup of EU statistic is upper bounded by the expected expected value of the exponential of the supremum of this average.",
            "Here, where I mean for any permutation P, so it does not depend on the permutation, so in fact to go from here.",
            "Here we just remove that.",
            "It's very easy to see, it's just Jensen's inequality.",
            "OK so you have this simplification.",
            "And of course if you continue to work to prove your bounds on this quantity here, then you only deal with averages of IID.",
            "Random variables.",
            "OK, so this.",
            "Would give us a rate of 1 / sqrt N but from yes I'll take the question.",
            "But what is intriguing is that for statistician it's quite strange to remove so many observations.",
            "I mean, there should be some gain of keeping all the pairs.",
            "Because this is a natural you know to say OK, keep only independent ones.",
            "So we should see something better by taking all the pairs together.",
            "So that was the main issue we address, so we worked on a second representation, eustatic.",
            "But first I will take this question.",
            "Play if you.",
            "Just make sure that you don't have overlap with you, guaranteed anytime.",
            "Because the original, the original sequence is ID.",
            "So if I take.",
            "Yes, yes exactly yeah yeah.",
            "Yes, I only deal with IID sequences.",
            "Anne.",
            "OK, of course you could refine this, but I mean for you statistic it's the main assumption is this One South?",
            "So there is actually.",
            "This doesn't tell us much on the structure of this kind of object, So what is much more so here is the."
        ],
        [
            "The idea is that you can show consistency.",
            "You can show the 1 / sqrt N result, but you cannot perform the 2nd order analysis and I will explain why becausw that you cannot guarantee this noise condition.",
            "The variance control assumption that allows you to reach the one over North.",
            "OK so.",
            "I will show now what it takes to reach this kind of bound and you will see that things get ugly.",
            "So the."
        ],
        [
            "The second representation of you statistique is what is called hardings decomposition, and the idea is to some kind of orthogonal projection of EU statistic over the space generated by averages of IID random averages as function of 1 random variable and the sequence is IID.",
            "OK.",
            "So the main term here.",
            "So you have a constant term which is just the expected value of the statistics for a fixed.",
            "F and the leading term here is an empirical average of IID, and you have a remainder turn W Nov F, which is what we call a degenerate you statistique and generate you.",
            "Statistique is typically of smaller order than regular U statistic and the assumption is that the expected value the conditional expectation of the function of the pair given one of the of the two elements in the pair is.",
            "Almost surely equal to zero.",
            "OK, so.",
            "This kind of objects are typically smaller order.",
            "OK, so if we use now, this kind of decomposition, the idea is that while the leading term, if you look at now, you change while the F is a decision rule and it's a candidate.",
            "So you want to control the supremum over the class of candidate F. So this behaves as an empirical process, so you can just choose the previous machinery used for classification to deal with this one and the only thing we need to do now is to.",
            "Make sure that this remainder term will not spoil the rate of convergence, which is not clear because we want to look at the results uniformly and we can have some."
        ],
        [
            "Rises so under which conditions we can guarantee that the remainder term is uniformly small over the class of decision rules.",
            "OK, so there are these two questions.",
            "So first make clear what is the function H here because you need to compute to make some computations to find the projection and then write down the results.",
            "With this, just thinking that the remaining term is not is not interfering and then just guarantee that the remainder term.",
            "Is small enough OK?",
            "So I don't want to spend too much time on this boring details, but I want to give you the some ideas.",
            "So the idea is to don't look at these displays, but the idea is just to apply this second representation of you statistics on the excess risk when you deal with the excess risk when you take the difference between the ranking error an well the empirical rule."
        ],
        [
            "King error and the true ranking error.",
            "So you know, sorry when you write down the excess risk, you have a function here which depends on XYX prime Y prime.",
            "So it's just a centering of of the function involved in the empirical error and then you take the empirical counterpart of this excess risk.",
            "So this is the object for which we build the for which we do the projection and we consider the conditional expectation.",
            "Of this quantity.",
            "So now this is the result."
        ],
        [
            "So the result is that you have when you look at the access risk with respect to the best possible error you have this approximation error term which comes for from the fact that you use a restricted family of ranking rules.",
            "So this we don't deal with this term.",
            "But here you have the estimation error.",
            "So up to logarithmic term you can see that we can reach a rate of one over North if Alpha can be as large as one.",
            "And this Alpha is involved here in the variance control condition, which is the same as before, except that here instead of the indicator of an error we have this function H of R which was on the previous display and this is a power function of the excess risk, so it's similar to the standard set up, except that we have to make explicit what it means for the distribution.",
            "Because this is just I mean.",
            "This is just how we use it, but it doesn't.",
            "It doesn't give us any insight on what it means for the distribution.",
            "Maybe it's too restrictive and maybe there are no distribution that satisfy this this condition.",
            "So we have to say something about that.",
            "So what does this condition means for the regression function for F of X?",
            "So we have a."
        ],
        [
            "Sufficient condition here we call the noise assumption.",
            "That is, if the expected value of the conditional expectation of ETA of X -- F of X prime given X prime to the power minus Alpha is smaller than some constant C, then you can guarantee the previous variance control condition and this is easy to see that.",
            "This condition is true for Alpha strictly smaller than one."
        ],
        [
            "If Alpha equals one, we have a problem, but we can reach Alpha as close as possible to one.",
            "And this is guaranteed only if I mean its surface is to take out of X absolutely continuous on 01 with a bounded density.",
            "So let's not the most general, but it's pretty general.",
            "If we had used, if we had considered the setup where we consider blocks of IID pairs, then we wouldn't have this HR function.",
            "And if we had written down the corresponding condition, we would have found that the condition of X is pretty serious, and it would have meant that at of X is a discrete distribution.",
            "OK, so we cover with this.",
            "Approach a much larger range of distributions.",
            "OK, so."
        ],
        [
            "This is about the regression model.",
            "We have similar results and now there."
        ],
        [
            "Remainder term, so this is the most technical part, so I will skip it.",
            "But just to tell you the idea is that to guarantee that the remainder term is small enough."
        ],
        [
            "We have additional complexity measures which come into the picture and we.",
            "I mean it doesn't.",
            "It's not enough to only consider the Rademacher average.",
            "That I mentioned on one of the first slides, so we have additional complexity measures to which have to be bounded in some sense in order to guarantee that the remainder term will not.",
            "Spoil this nice rate of convergence results.",
            "OK, so this."
        ],
        [
            "Lies on some moment inequality and there are many techniques from the theory of you processes.",
            "Many papers from probability theory.",
            "Which gives some results, but we needed to develop a different one in order to fit with this learning application.",
            "And."
        ],
        [
            "So I won't comment on this right now, except if you have questions later on and we also check that in the VC case W~ North so the remainder term is of the order of one over North.",
            "OK, so if we have a rate up to one over North, this doesn't matter.",
            "OK. Now.",
            "I want to make the connection with specific criteria used in bipartite ranking so the previous model I described, this preference model is actually exactly the program for assessing the performance of strategies which maximize the AUC.",
            "This is exactly it."
        ],
        [
            "So the AUC.",
            "So this is the area under the arosi curve so."
        ],
        [
            "I want to recall what RC curves are, but I imagine that most of you know what they are, but the idea is that the AUC can be rewritten as a probability.",
            "Of so this is like a rate of concording pairs.",
            "People in credit risk screening.",
            "They call it like this.",
            "So this is counting how many pairs of observations are well ordered by some scoring rule South.",
            "So scoring rule."
        ],
        [
            "As here is just a real valued decision rule, OK?",
            "So.",
            "So the AUC looks like that.",
            "So this is a probability of having S of X larger than F of X prime, given that the label of X is larger than the label of X prime.",
            "And you can write down the optimal value of the AUC for a given distribution, and you see again just as just like in our analysis of the ranking error that the critical quantity is this deviation of Etta between two points X&X prime.",
            "OK, so how does it relate to the ranking error?",
            "Well, just consider the ranking rule R which says plus one if F of X is larger than F of X prime and minus one otherwise and then you are back to the previous."
        ],
        [
            "It up OK, so the AUC you can just write down, write it down as a function of the ranking errors, so that maximizing the AUC is exactly equivalent to minimizing the ranking error.",
            "OK, so this is this was it?",
            "I mean we did the all the theorie for."
        ],
        [
            "Empirical performance, maximize maximization based on the AUC.",
            "So.",
            "Is it just a matter of.",
            "You know classification with pairs of observations.",
            "Because of course I describe the theoretical aspect and the rates of convergence, but from the viewpoint of algorithms it doesn't change much.",
            "I mean it just says OK take all the pairs and learn with any of your favorite classification method and then you're good.",
            "OK, so is that it is at the end of the story.",
            "So of course this is not the end of the story, because there you see is not a good criterion for ranking.",
            "I mean now."
        ],
        [
            "The bad one.",
            "But in most of the ranking applications you only deal with some part of the observation space, and you especially are interested in the best possible instances.",
            "OK, so you only care usually about the top of the list.",
            "I mean, at least it should have more weight than what happens at the bottom of the list.",
            "OK, so how can you emphasize the errors in this part of the observation space?"
        ],
        [
            "So of course you.",
            "I mean, there are many other criteria for for answering this question, so I will just give you maybe a few and show the theoretical consequences of dealing with this other criteria.",
            "OK, so now I consider another question, not preference learning, but ranking the best possible instances and there is a subproblem which is first finding these best instances.",
            "'cause ranking the best is a double problem.",
            "You first need to find them and also to rank them.",
            "But you cannot really do it.",
            "I mean you have to do it simultaneously.",
            "So this is a JML.",
            "Our paper of 2007 I think written with Steven but there are also some other aspects we developed in more recent papers so I will mention the papers in."
        ],
        [
            "Again.",
            "So here will.",
            "For get a bit about the ID thing, we'll see how the non IID part.",
            "So we will see the how it comes into the picture.",
            "But first I need to explain the criterion for this problem.",
            "So what criterion should we consider if we are looking for the best possible instances?",
            "We still have only a binary label information.",
            "OK, so.",
            "OK, so this is just like looking at the RC curve.",
            "It's only restricting the attention to a part of the RC curve, meaning in the beginning of the RC curve.",
            "OK, so we need to do some cut off of the RC curve and to do this cutoff we fix a constraint.",
            "So the parameter here is, it will be denoted by U and it is a percentage of the best instances we want to on which we want to focus.",
            "So say I want to.",
            "Rank correctly the 5% the best 5% of the observations.",
            "OK, so this will be our parameter here the constraint.",
            "So now I'm still in the spirit of scoring, so I have a scoring ruler because I want to order the objects so I cannot deal anymore with classifiers but.",
            "I mean in this sub problem of finding the best instances we come up with a classifier which is based with this scoring rule.",
            "But the fact that I have this constraint, actually it means that if I take an arbitrary scoring rule, the thresholds depends on this you and if I want the mass of this set here to be equal to UI have to take as a threshold the quantile of the random variable X of X at level 1 -- 2.",
            "OK so the fact that I consider this constraint has an implication on the form of the candidate sets for finding the best.",
            "When I use a family of scoring rule denoted by S. OK so I have this constraint here that the probability that X is in this kind of set for any S is equal to U.",
            "But another interesting aspect of ranking is that.",
            "Remember in classification, I said we only interested in one single level set of the regression function.",
            "Here we want to recover well.",
            "It's not maybe clear in this slide, but actually want to recover many level sets."
        ],
        [
            "Of the regression function and we don't really care about the regression function.",
            "We don't really care about the actual values of the regression function, we only want to recover the order induced by the regression function.",
            "OK, so there is this invariance property with respect to strictly increasing transformation.",
            "I mean, if I take a.",
            "If I take an increasing transformation of South, I won't change the order, so it should be as good as South from the viewpoint of bipartite ranking.",
            "OK, so I also have this property and it's OK with this.",
            "With this family of candidate decision rules.",
            "So basically you just redefine the varosi, but using these U as a parameter an the constraint on U involves some relationship between the true positive rate and the false positive rate.",
            "Here P is the proportion of plus one labeled.",
            "So I just show on the graphical display probably easier to understand.",
            "So for a fixed scoring rule I have an RC curve here and I want to determine.",
            "What is the well actually I'm after?",
            "A criterion for to optimize for finding the best OK.",
            "So maybe an idea is to take to truncate the AUC at some point and say I want to maximize this partial AUC and people in Biostatistics, that's what they do, but here we have this constraint with the rate of good instances of top ranked instances.",
            "This U and this induces a constraint which is represented here.",
            "But by this control."
        ],
        [
            "Online, so the intersection here should tell us where the cut off is.",
            "So where shall we truncate the orosi?",
            "So this is a standard definition, the partial AUC, except that it's not a good one.",
            "Becausw now take our OC curve which is higher in this graph, so it should be better.",
            "The red line should give us a better scoring rule than the green line.",
            "But if you take the truncation, you should look at the intersection.",
            "With this you with this line here, which depends on you and pee.",
            "So for fixed you and P you have the same line but diff."
        ],
        [
            "It cut off and it's not clear that the partial AUC for the red is larger than the green, and actually this is usually well, it can be wrong.",
            "It's in general it's not true that it's larger.",
            "OK, so we need to make a correction of this partial AUC, and the idea is to.",
            "Add this term here, which is the true."
        ],
        [
            "Positive rate times 1 minus the false positive rate.",
            "And it means that you just have to consider the rectangle which is here and so the the right criterion is to take all this area here together with this rectangle and then you have a consistent criterion for learning the best ranking the best.",
            "Instances OK, so this was not very clear in the literature so.",
            "And also it's I mean it makes clear that finding I mean you have a double problem here.",
            "Finding the best and also ranking."
        ],
        [
            "Them correctly on this part of the domain, so you can think about various strategies for that.",
            "So now for the end of the talk I want to give you an idea of.",
            "I mean all the.",
            "The developments we made on you statistiques.",
            "Can we extend it for other?",
            "Other optimization principles, for instance, to learn the set of best objects so.",
            "This is just the same notations as before, so if we focus on the on this sub problem of just finding the set of best instances then you can think of it as a problem as a classification problem.",
            "But where you have an additional constraint which is that the size of all classifiers is equal to you and if you write down you can also focus on the same notion of error the classes."
        ],
        [
            "Fication error and then the the given the constraint.",
            "Then you don't necessarily recover the optimal element for the unconstrained problem.",
            "This depends on EU.",
            "OK, so.",
            "Here the optimal elements are classifiers, where you threshold at the level of the quantile of the random variable X of X of the other 1 -- 2.",
            "OK, so you have a different threshold.",
            "Instead of having 1/2, you have a different threshold for ETA to find the optimal set.",
            "So when you write down the excess.",
            "Of error then you you are interested in the behavior of FX around its quantile of order 1 -- 2.",
            "OK, but the you see immediately what it means from the viewpoint of estimation.",
            "But you have.",
            "I mean, this is much more complicated.",
            "'cause this quantity also depends on the distribution.",
            "We don't have a fixed value here.",
            "OK, so if we look at."
        ],
        [
            "The empirical risk minimization formulation for this problem, and we consider candidates which have this form and they fulfill the two constraints or the mass constraint and the invariants by through increasing transforms.",
            "So this is a natural object to natural object to consider too, as a possible decision rule for this problem and the empirical counterpart of the risk here is just the previous one where we plug.",
            "The empirical quantile instead of the true contact because the content depends on the distribution.",
            "So you, I mean it's the inverse of the cumulative distribution function.",
            "So you just take the generalized inverse of the empirical CDF of South of XI.",
            "OK, so."
        ],
        [
            "Here you see that we don't have an average of IID because the queue had depends on all the sample.",
            "It depends on all the exercise.",
            "OK, so we cannot use the classical tools for that.",
            "And so yeah, so proving things is not so much fun, But the interesting part is that I mean some some.",
            "Parts of it are not so intuitive in that case, and also the statistical results tell us something about the nature of scoring rules we should use for this problem.",
            "And.",
            "The scoring rules we need to consider.",
            "I mean they're not necessarily linear by parts, but they should be neither too flat nor too steep.",
            "OK, so you cannot have constant parts for the scoring rule.",
            "It's really bad for ranking, so at so here it's not exactly ranking, it's just finding the best.",
            "But you need at the level of the quantile to have.",
            "Maybe not to jump, but you have.",
            "You need to have a significant variation of ActiveX Locali.",
            "OK, so you need this stiffness, but at the same time you need to estimate this quantile and to estimate the contact you need to have enough mass so you cannot have it too steep because otherwise you don't have enough mass to estimate the quantile.",
            "So you have this.",
            "Agonistic effects on the estimation, and this indicates that the scoring rule for this problem should look like that OK.",
            "So the objects we deal with are."
        ],
        [
            "What we call signed rank statistics.",
            "So there are more complicated than you statistics, so maybe I will skip that.",
            "So you have some kind of similar decomposition except so."
        ],
        [
            "Just check the even if you don't know what the notations mean, but this is."
        ],
        [
            "Say the empirical object want to minimize.",
            "And this is the expected value.",
            "So it's like a constant.",
            "This is the leading term and this is the remainder term, so the remainder term we can show that it's on the order of one over North, so we're safe.",
            "And now what does this term look like?",
            "I mean, if we want to go to the fast rate thing and.",
            "And it looks like something quite unnatural.",
            "It takes this form and it's not just the empirical version where we plug the true quantile.",
            "There are some corrections which involve the partial derivatives of this K, which is the well.",
            "Actually, this is the true criterion.",
            "OK, so well, I don't go into the details because probably you're tired now and maybe it's not so interesting, but I just want to illustrate the fact that it's not trivial and it's unexpected.",
            "I mean, it's not just you know you need really to go into the details and to see how things work properly.",
            "So now the interesting part, I think from the viewpoint of machine learning and I'm close to the end.",
            "Is that?",
            "I mentioned this signed rank statistics and actually even you statistique is a rank statistic cause the simple you statistic is known as the Mann Whitney statistic."
        ],
        [
            "And a simple operation.",
            "Makes a connection with a statistic based on the ranks of the random variables and this is called the Wilcoxon statistic, so there is a.",
            "Very classical relation between the simple use statistiken Wilcoxon statistic.",
            "I mean you statistic of order 2.",
            "And you can generalize these rank statistics by applying some function here.",
            "So far here is what is called the score generating function, so nothing to do with the scoring rules, it's just a wait waiting function on the ranks.",
            "OK, so you have the you have a decision rule which is a scoring rule, real valued South and you look at the position of the observation number.",
            "I according to this function.",
            "OK, the position among all other S of XJ's, let's say, and you just record the position and so here.",
            "This type of criterion would.",
            "So the top of the list is.",
            "The corresponds to the highest ranks.",
            "And this function should put more emphasis on the large ranks.",
            "OK, so it should be increasing.",
            "And you want to maximize this kind of criterion.",
            "So this is a very general form.",
            "OK, and it's connected to you statistics and actually it covers most of the standards criteria used in machine learning communities, so it covers the AUC.",
            "Just take five X equal to X.",
            "It covers the local AUC.",
            "So the work I mentioned before with finding and ranking the best objects or just take X and truncated.",
            "At the level 1 -- 2.",
            "But it's also corresponds to the P norm push, so there is a paper by Cynthia Rudin, GM LR, 2007.",
            "An even more Interestingly, it also covers the DCG.",
            "OK, just take a C of X.",
            "The discount factor function and truncate keep the K best instances.",
            "OK, so why not considering also smooth versions of this code generating functions and maybe have a general theory for this criteria?",
            "OK, so this is so.",
            "I don't have a general theory from the viewpoint of statistics, but there is a general formulation at least OK and I just illustrated through some examples what we can do with with this type of description Now too.",
            "To finish with the talk, I want to say that.",
            "All these as the title of the slide points out all these criteria.",
            "They are essentially contained in the RC curve, so the RC curve is richer.",
            "Description of performance.",
            "OK, so our philosophy now is.",
            "Not so much to try, you know, to find optimization algorithms for criteria like that and try to optimize them, but rather try to go back to this arosi curve and try to optimize.",
            "Optimize it directly, which is a difficult problem because it's a function like criterion.",
            "OK, so it's not just like you have a scalar value for the criterion you optimize.",
            "It's a function.",
            "OK, so it's much more difficult, but it's doable.",
            "And the algorithms I mentioned in the introduction.",
            "They try to optimize the arosi directly.",
            "OK, so this is a different way to look at things in this bipartite ranking problem.",
            "So there is all this theory about empirical performance maximization, for which I gave you some ideas, but from the algorithmic point of view, I'm not sure that this is the right direction.",
            "OK, so this is mostly the second option, so I think I'm.",
            "Beyond time, so I'll be happy to talk or to point out some references on the second part and will have as I said, a demo presentation for some software we developed for building scoring rules which optimize the arrow seeker.",
            "OK, so let me finish with the conclusion slide."
        ],
        [
            "And with some references.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work with Steven Clemenceau and Gabor Lugosi for some part of the talk, but I was in the last years I was working on the ranking problem, mainly with Steven and I will give you, well, you will have some quick overview of some of the works we done over the years.",
                    "label": 1
                },
                {
                    "sent": "OK, so this talk is about learning from IID data.",
                    "label": 0
                },
                {
                    "sent": "I said ID.",
                    "label": 0
                },
                {
                    "sent": "And I think you guys are crazy to work with non IID data.",
                    "label": 0
                },
                {
                    "sent": "I mean as a theorist.",
                    "label": 0
                },
                {
                    "sent": "There are already many, many questions.",
                    "label": 0
                },
                {
                    "sent": "Many issues to address in the IDE setup so.",
                    "label": 0
                },
                {
                    "sent": "It's a kind of difficult step you don't take and move to non IID, so I will deal with IID sequences of data.",
                    "label": 0
                },
                {
                    "sent": "But in many situations because of the nature of the problem and the goal we are pursuing.",
                    "label": 0
                },
                {
                    "sent": "We come up with some optimization principles which required to deal with some Association of the data which make them look non.",
                    "label": 0
                },
                {
                    "sent": "I well known IID OK so this will be the focus of the presentation and I will mainly consider the mathematical side.",
                    "label": 0
                },
                {
                    "sent": "Well this will be the.",
                    "label": 0
                },
                {
                    "sent": "This is the stream of the presentation.",
                    "label": 0
                },
                {
                    "sent": "The topic will be the study, the mathematical study of empirical risk minimization strategies based on.",
                    "label": 1
                },
                {
                    "sent": "Some particular statistics I will describe, but there will also be some underlying topic, and I think this is the most interesting part for learning applications which has to do with the criteria we use in learning application, especially in the bipartite ranking problem for which the setup is simple.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just classification data, but the things you want to do are much more complicated than the standard classification problem.",
                    "label": 0
                },
                {
                    "sent": "And there are many different performance measures that can be considered and lead to many questions.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so first just to give you a general idea of the kind of problems we look at.",
                    "label": 0
                },
                {
                    "sent": "So we consider the case where the individual observations are IID so the size they can be pairs XIYI.",
                    "label": 1
                },
                {
                    "sent": "If you deal with supervised classification problem.",
                    "label": 0
                },
                {
                    "sent": "But I mean, you can also imagine different setups, but the idea is that this is observed.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things are IID and we study some learning problem and the losses.",
                    "label": 0
                },
                {
                    "sent": "Some function L and we want to find a good decision rule denoted by F in a class of candidate decision rules.",
                    "label": 0
                },
                {
                    "sent": "And then you can formulate empirical risk minimization strategies as estimators of F of F of a good F as minimizers of.",
                    "label": 0
                },
                {
                    "sent": "This type of statistical functional so like the average of some function of the observations of the loss function of the decision rule F OK and this is an average of IID random variables.",
                    "label": 0
                },
                {
                    "sent": "And if you want to assess performance of such strategies then you have to deal with this empirical processes here, which are the of this form and they are supremum over the class of functions.",
                    "label": 0
                },
                {
                    "sent": "Of these kind of objects?",
                    "label": 0
                },
                {
                    "sent": "OK, so you can think many formulations can lead to this general problem and the I mean the absolute tool to deal with these quantities come from the theory of empirical processes, OK?",
                    "label": 0
                },
                {
                    "sent": "So this is the what was done over the years in the theory of binary classification in the study of convex risk minimization methods.",
                    "label": 0
                },
                {
                    "sent": "Like boosting or SVM an many in many other setup you would end up with this kind of object here.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if we still have these IID data, but now the empirical risk takes the form of an average over functions of a pair of random variables from the the ID sequence.",
                    "label": 0
                },
                {
                    "sent": "So, like entrust 1 / N choose two the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sum over all pairs IJ of some function of ZIJ&F the decision rule.",
                    "label": 1
                },
                {
                    "sent": "Or it can also be something like that.",
                    "label": 0
                },
                {
                    "sent": "So some function which depends of.",
                    "label": 0
                },
                {
                    "sent": "I saw an average of functions of different functions of all the sample.",
                    "label": 0
                },
                {
                    "sent": "OK, so here you no longer have some an average of IID.",
                    "label": 1
                },
                {
                    "sent": "OK, so you need to adapt the mathematical tools to describe the same type of properties.",
                    "label": 0
                },
                {
                    "sent": "OK, so to get consistency results, rates of convergence.",
                    "label": 0
                },
                {
                    "sent": "And this type of thing.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "This will be the.",
                    "label": 0
                },
                {
                    "sent": "The main general setup for the talk.",
                    "label": 0
                },
                {
                    "sent": "So first I will just recall what was done in the simple K. Well, the simple it's not so simple, but what is now well known about the stuff?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other binary classification and then we'll consider various examples for which we need to adapt the existing results.",
                    "label": 0
                },
                {
                    "sent": "So there is also a section on algorithms, but I probably won't have time to discuss it, but I want to advertise it, so maybe I will just pass you some some slides very quickly, but you will see some colors and some advertisement for a demo presentation at NIPS.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with binary classification.",
                    "label": 0
                },
                {
                    "sent": "So first notations.",
                    "label": 0
                },
                {
                    "sent": "So we have a pair.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "XY, where X is a descriptor of the individual and Y is a label.",
                    "label": 0
                },
                {
                    "sent": "So here I will deal only with binary labels.",
                    "label": 0
                },
                {
                    "sent": "So why is minus one or plus one and the statistical model for such observations is given by a probability by the joint probability distribution P for the pair XY.",
                    "label": 1
                },
                {
                    "sent": "An important quantity here is the posterior probability at of X, which is the probability to observe.",
                    "label": 0
                },
                {
                    "sent": "A label equal to plus one given the observed given the observation vector X. OK, so the behavior of this function.",
                    "label": 0
                },
                {
                    "sent": "Tells us how difficult the learning problem is.",
                    "label": 0
                },
                {
                    "sent": "So to get an idea why is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But so we have to look at the optimal elements for this problem.",
                    "label": 1
                },
                {
                    "sent": "So we look for decision rules which take which are functions over the set over the space where X leaves.",
                    "label": 1
                },
                {
                    "sent": "And this functions take values plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "An the performance measure or the risk measure for this problem, while the simplest choice is to take the classification errors.",
                    "label": 0
                },
                {
                    "sent": "So the probability of making a wrong prediction.",
                    "label": 0
                },
                {
                    "sent": "And then if we knew the distribution P, we would know which are the best possible decision rules an what is the minimal possible value of this classification error.",
                    "label": 0
                },
                {
                    "sent": "OK, so the optimal rule is probably most of you know that is the function which predicts plus one if the regression function is larger than 1/2 and minus one otherwise.",
                    "label": 0
                },
                {
                    "sent": "OK, so then you can write down the optimal value of the error and you can also write down the excess of risk for any.",
                    "label": 0
                },
                {
                    "sent": "Arbitrary classifier G with respect to the optimum.",
                    "label": 0
                },
                {
                    "sent": "So what is important here it is to know that what we are after here is just one single level set of the posterior probability at of X.",
                    "label": 0
                },
                {
                    "sent": "We want to discover to learn this set of axes such that F of X is larger than 1/2.",
                    "label": 0
                },
                {
                    "sent": "Of course, we don't know the ETA, so we need to cook up some method to to try to approximate to try to approximate these sets, but.",
                    "label": 0
                },
                {
                    "sent": "What is important here is the behavior of eight of X around 1/2, so the steepest axis at 1/2, the easier the classification problem.",
                    "label": 0
                },
                {
                    "sent": "OK, if FX is close to 1/2, then it's difficult to decide whether you should take plus one or minus one, so it makes things more difficult and it makes the value as star larger.",
                    "label": 0
                },
                {
                    "sent": "If you have whole regions where at of X is close to 1/2.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what kind of strategy?",
                    "label": 0
                },
                {
                    "sent": "So here I just want to give the general idea, so I just focus on the simple strategy based on minimizing the frequency of error over some data sample of IID copies of the pair XY.",
                    "label": 0
                },
                {
                    "sent": "So denote by G hat the minimizer of this empirical error.",
                    "label": 0
                },
                {
                    "sent": "So we have an empirical empirical risk.",
                    "label": 1
                },
                {
                    "sent": "Minimizer and the common analysis relies on concentration inequality's, where you can control the error of this estimator with respect to the minimal achievable error over the set of candidates.",
                    "label": 0
                },
                {
                    "sent": "You can control it by the expected value of the uniform deviation of the empirical risk with respect to the true risk, so the true error plus some corrective term here, which is of the other.",
                    "label": 1
                },
                {
                    "sent": "One over square root of North and then the game is to try to.",
                    "label": 1
                },
                {
                    "sent": "Gives some explicit bound for this quantity, so if you like to work with VC dimension, then actually this is.",
                    "label": 0
                },
                {
                    "sent": "The best possible rate you can find if the class of candidates is not too big.",
                    "label": 0
                },
                {
                    "sent": "You can control this complexity term by term, which is of the order of one over square root of North.",
                    "label": 0
                },
                {
                    "sent": "So you recover the parametric rate of convergence in statistical problems.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of optimum you can expect for the rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "But now if you want to deal with.",
                    "label": 0
                },
                {
                    "sent": "Let's say practical algorithms and efficient algorithms.",
                    "label": 0
                },
                {
                    "sent": "They work with much larger classes than VC classes, so a good description of the complexity of the I mean the sensitive.",
                    "label": 0
                },
                {
                    "sent": "Concept of the critical concept of complexities rather this Rademacher averages.",
                    "label": 0
                },
                {
                    "sent": "So if you know about this, that's fine, if not, never mind, but this is a Richard.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scription of complexity, and I mean most state of the art results are based on the control of such quantities.",
                    "label": 0
                },
                {
                    "sent": "OK, so, but we expect that for larger classes this could get maybe larger than one over square root of North.",
                    "label": 0
                },
                {
                    "sent": "For some classes, like kernel classes, you can still have this.",
                    "label": 0
                },
                {
                    "sent": "This of course it depends on the gram matrix, but you can still get this kind of one over square root of.",
                    "label": 0
                },
                {
                    "sent": "And rates OK, so this these rates are really important.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not only a matter of you know, Thierry.",
                    "label": 0
                },
                {
                    "sent": "I mean it tells you how many observations you need to learn some functions, so it's a very of course.",
                    "label": 0
                },
                {
                    "sent": "These rates are far from the reality, but but it gives you some indication and and if you can achieve a faster rate that maybe that's an indication that you have a better method.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But this this I called this the 1st order analysis.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can only bring you down to 1 / sqrt N Now for some distributions.",
                    "label": 0
                },
                {
                    "sent": "Imagine that around the frontier where at of X is equal to 1/2.",
                    "label": 0
                },
                {
                    "sent": "You have some margin and there are not so many observation in this part of the of the domain.",
                    "label": 0
                },
                {
                    "sent": "So then you can reach faster rate than one over square root of North.",
                    "label": 0
                },
                {
                    "sent": "But to see this you need to apply more advanced results in concentration inequality such that such like the Talagrand concentration inequality which involve an additional term here.",
                    "label": 0
                },
                {
                    "sent": "Which depends on the variance of the process, so I'm not giving you the.",
                    "label": 0
                },
                {
                    "sent": "The precise value, but it's it's A kind of variance measure here, and if you play with this quantity then you can achieve faster rates.",
                    "label": 0
                },
                {
                    "sent": "So the standard assumption.",
                    "label": 0
                },
                {
                    "sent": "Well, this is a sufficient condition to guarantee this faster rates is to have a control of the variance by some power of the excess risk, and then if you plug this inequality in this one then you can see that depending on the value of Alpha, you can reach up to.",
                    "label": 0
                },
                {
                    "sent": "Rates of convergence of 1 / N IF Alpha is equal to 1 so of course this puts some additional assumption on the distribution.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So what kind of assumption this is?",
                    "label": 0
                },
                {
                    "sent": "Well, there are some.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other statements of the assumptions which lead to this variance control condition, which for which we can play in the inequality's.",
                    "label": 0
                },
                {
                    "sent": "But maybe the easiest way to give a grasp on this one is this inequality.",
                    "label": 0
                },
                {
                    "sent": "Here that says that there exists some constant B positive that such that for all T positive we have that the probability of at a twice at of X -- 1 being smaller than T is smaller than this quantity here and this describes.",
                    "label": 0
                },
                {
                    "sent": "I mean the density of points around the border of between the two populations.",
                    "label": 0
                },
                {
                    "sent": "OK, so for Alpha equal to 0, it's like you have no condition, so you recover the one over square root of rates and if Alpha equals to one, it means that you have really a few.",
                    "label": 0
                },
                {
                    "sent": "It actually it means that you have a jump at the level of eight out of X equal to 1/2 for at X, so you don't have a jump in the value, so.",
                    "label": 0
                },
                {
                    "sent": "This means that you have a clear cut frontier and this makes the classification problem much easier, and this explains the faster rates.",
                    "label": 0
                },
                {
                    "sent": "OK, so in now in many app.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vacations this kind of criterion we invoked here is not very interesting because in many applications you have an A symmetry between the two types of errors, so predicting that someone is healthy while he's sick is much worse than than the opposite, so.",
                    "label": 0
                },
                {
                    "sent": "We want to maybe to distinguish the two types of error and maybe also we have different things in mind and not just, you know, discriminating data.",
                    "label": 0
                },
                {
                    "sent": "But maybe we want to rank the data.",
                    "label": 0
                },
                {
                    "sent": "But still we have only this binary information on the label, so we know.",
                    "label": 1
                },
                {
                    "sent": "We know if we if we do a query on a search engine there are relevant pages and irrelevant pages, but still in the end you want to come up with a list.",
                    "label": 0
                },
                {
                    "sent": "OK, so how would we deal with this kind of objective?",
                    "label": 0
                },
                {
                    "sent": "So first I will.",
                    "label": 0
                },
                {
                    "sent": "I will play here with the the statistical model in the beginning for classification I said that we deal with a sequence of IID xiy I.",
                    "label": 0
                },
                {
                    "sent": "And now we will consider a slightly different model, which actually includes the previous one, but which makes clear what kind of goal we are after when we deal, for instance, with the bipartite.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tracking problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so this first part is joint work with Stefan and Gabor.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different statistical model and we can call this preference model.",
                    "label": 0
                },
                {
                    "sent": "So now we have a triple of random elements.",
                    "label": 0
                },
                {
                    "sent": "So XX prime are of the same nature.",
                    "label": 0
                },
                {
                    "sent": "There are a pair of observations similar to the X we had before.",
                    "label": 0
                },
                {
                    "sent": "An R is a ranking label or a preference label.",
                    "label": 0
                },
                {
                    "sent": "So R tells us which one of X or X prime is the better OK.",
                    "label": 0
                },
                {
                    "sent": "So we'll say that our response it if if X is better than X prime.",
                    "label": 1
                },
                {
                    "sent": "OK, so now let's try to write down.",
                    "label": 0
                },
                {
                    "sent": "I mean when we know the distribution, what would be the optimal elements?",
                    "label": 0
                },
                {
                    "sent": "If we deal with this with this model and just think about doing classification, but over this over the product space of the observations.",
                    "label": 0
                },
                {
                    "sent": "OK, So what plays the role of Etta here is this pair of function row plus and row minus that tells us when well that gives us the probability of the ranking label being positive or negative.",
                    "label": 0
                },
                {
                    "sent": "Given the pair XX prime.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we actually observe the individual labels then you can just take the ranking label equal to the difference between the two individual labels.",
                    "label": 0
                },
                {
                    "sent": "OK, so this contains the the classification model.",
                    "label": 0
                },
                {
                    "sent": "We stated before, but it can be.",
                    "label": 0
                },
                {
                    "sent": "Also it can also contain the regression model, for instance.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just consider now the.",
                    "label": 0
                },
                {
                    "sent": "The ranking error, which is the error when we.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wrongly predict.",
                    "label": 0
                },
                {
                    "sent": "I mean, we count how many times we wrongly predict the ranking label with some ranking rule here which plays the role of the classifiers in this space and the ranking rules operate over the product space of X by itself and potentially they can take three values minus 101.",
                    "label": 0
                },
                {
                    "sent": "So it's not actually a multiclass, it's you can.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you take this kind of measure for the error, you can play with this you can take different conventions, but if you play if you use this kind of convention here for the.",
                    "label": 0
                },
                {
                    "sent": "Classification error then you are basically back down to the binary classification set up, except that you have to describe precisely the optimal elements in that case.",
                    "label": 0
                },
                {
                    "sent": "So here the best possible rule would predict plus one if Pro Plus is larger than than row minus.",
                    "label": 0
                },
                {
                    "sent": "OK, so now if we see what this means, if we go back to the model for the individual observations and this individual observations are classified.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient data, then the row plus Andro minus you can well the row plus can be exactly written as the product of 8 of X * 1 -- F of X prime.",
                    "label": 0
                },
                {
                    "sent": "Andro minus is just 1 -- 8 of X * F of X prime, and the optimal rule written here is exactly this one, so it predicts that X is better than X prime if F of X is larger than F of X prime, which is intuitive.",
                    "label": 0
                },
                {
                    "sent": "I mean it's.",
                    "label": 0
                },
                {
                    "sent": "It's it's clear.",
                    "label": 0
                },
                {
                    "sent": "OK, it's exactly what we expect and you can write down the optimal air for this setup and what I wanted to show you here is what replaces the quantity of X -- 1/2.",
                    "label": 0
                },
                {
                    "sent": "In this case, is the variation of Etta between these two points X&X prime?",
                    "label": 0
                },
                {
                    "sent": "OK, so the smaller this quantity, the more difficult.",
                    "label": 0
                },
                {
                    "sent": "Learning the preference because it means that X is very close together X prime.",
                    "label": 0
                },
                {
                    "sent": "And we cannot really say which one is best.",
                    "label": 0
                },
                {
                    "sent": "So ideally in this problem we want to recover, not the exactly the level sets of data, but in some kind of.",
                    "label": 0
                },
                {
                    "sent": "Global, I mean global comparison of the variation between two points of these functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so now maybe I will skip this slide, but just to tell.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that if we deal with regression data, you can also write down precisely what is the optimal rule and basically says that the same thing, except that the regression function for regression data is slightly different here.",
                    "label": 0
                },
                {
                    "sent": "But you have the same type of behavior.",
                    "label": 0
                },
                {
                    "sent": "OK, so it looks like for preference learning is exactly the same as classification, and actually algorithms like Rank Boot.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Step more or less what they suggest that you can just do classification, but you use these pairs of observations and you work on the product space OK and so is it the same.",
                    "label": 0
                },
                {
                    "sent": "Well, from theoretical point of view, it's not the same, and the reason is that if you write down the empirical counterpart of the criterion of the ranking error, then it takes this form because we have two pairs, XY&X prime, Y prime.",
                    "label": 0
                },
                {
                    "sent": "So if you discretize the previous.",
                    "label": 0
                },
                {
                    "sent": "Notion of error.",
                    "label": 0
                },
                {
                    "sent": "Then you come up with a quantity like this which takes the average of all pairs of IJ which count the inversions in the ranking between two observations.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have to consider all these pairs together and of course you no longer have an average of IID, you have a very well known quantity in statistics which is called EU statistic.",
                    "label": 0
                },
                {
                    "sent": "OK, so the general form of you statistic is this one, so you have individual observations which are IID.",
                    "label": 0
                },
                {
                    "sent": "But now you deal with the average of a function of pairs.",
                    "label": 1
                },
                {
                    "sent": "You can also.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is a you statistique of order two, but you can also consider you statistiques of higher order, except that well in this case in the case in the example we come up with this kind of object here.",
                    "label": 1
                },
                {
                    "sent": "So can we redo all the theory of empirical risk minimization for?",
                    "label": 0
                },
                {
                    "sent": "Minimizing such principles.",
                    "label": 0
                },
                {
                    "sent": "And the answer is yes, but it's quite more difficult that in the in the previous set up and also there was something in training in the beginning when we began to think about this problem is that actually there were some work trying to justify, you know, an algorithm like rank boost or or similar methods.",
                    "label": 0
                },
                {
                    "sent": "And in most papers people would take the assumptions that OK. Let's take the pairs such that the.",
                    "label": 0
                },
                {
                    "sent": "These pairs are IID OK, so of course in that case it's trivial because it's just simple classification.",
                    "label": 0
                },
                {
                    "sent": "So we try to look at what this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Statistique look like I mean, what does the statistical theory tell us about these objects and in.",
                    "label": 0
                },
                {
                    "sent": "In the store ikle paper by hurting where there is this famous earnings inequality, there is a section on you statistiques and there are there is a deviation inequality for you statistics an the key idea is to say that we can write this previous average of function of pairs of the of the observations as an average over permutations of IID blocks.",
                    "label": 0
                },
                {
                    "sent": "I mean sums of IID blocks.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you just play with indices.",
                    "label": 0
                },
                {
                    "sent": "You can reorder.",
                    "label": 0
                },
                {
                    "sent": "This some in order to have you know blocks of size N / 2, while the integer part of divided by two, wherein this sum you all the pairs, they don't overlap, so you have only different indices, so this is what is inside this.",
                    "label": 0
                },
                {
                    "sent": "Some here is really a sum of IID, but you also have to take the average over all permutations of indices.",
                    "label": 0
                },
                {
                    "sent": "OK, so just by this trick here of taking permutation of permutation.",
                    "label": 1
                },
                {
                    "sent": "Well, you regroup here Z with an index, which is the the permutation of I and Z where the index is the permutation of I plus and divided by two, and then you can guarantee that that the this indices do not overlap inside the sum.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you can, if you use this representation of you statistique then it's very easy by I mean it's just a. Markov inequality applied on an exponential scale, but you can show that and you can show it for other functions that the exponential that the expected value of the exponential of the soup of EU statistic is upper bounded by the expected expected value of the exponential of the supremum of this average.",
                    "label": 0
                },
                {
                    "sent": "Here, where I mean for any permutation P, so it does not depend on the permutation, so in fact to go from here.",
                    "label": 0
                },
                {
                    "sent": "Here we just remove that.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to see, it's just Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "OK so you have this simplification.",
                    "label": 0
                },
                {
                    "sent": "And of course if you continue to work to prove your bounds on this quantity here, then you only deal with averages of IID.",
                    "label": 0
                },
                {
                    "sent": "Random variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 1
                },
                {
                    "sent": "Would give us a rate of 1 / sqrt N but from yes I'll take the question.",
                    "label": 0
                },
                {
                    "sent": "But what is intriguing is that for statistician it's quite strange to remove so many observations.",
                    "label": 0
                },
                {
                    "sent": "I mean, there should be some gain of keeping all the pairs.",
                    "label": 0
                },
                {
                    "sent": "Because this is a natural you know to say OK, keep only independent ones.",
                    "label": 0
                },
                {
                    "sent": "So we should see something better by taking all the pairs together.",
                    "label": 0
                },
                {
                    "sent": "So that was the main issue we address, so we worked on a second representation, eustatic.",
                    "label": 0
                },
                {
                    "sent": "But first I will take this question.",
                    "label": 0
                },
                {
                    "sent": "Play if you.",
                    "label": 0
                },
                {
                    "sent": "Just make sure that you don't have overlap with you, guaranteed anytime.",
                    "label": 0
                },
                {
                    "sent": "Because the original, the original sequence is ID.",
                    "label": 0
                },
                {
                    "sent": "So if I take.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes exactly yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, I only deal with IID sequences.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK, of course you could refine this, but I mean for you statistic it's the main assumption is this One South?",
                    "label": 1
                },
                {
                    "sent": "So there is actually.",
                    "label": 0
                },
                {
                    "sent": "This doesn't tell us much on the structure of this kind of object, So what is much more so here is the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea is that you can show consistency.",
                    "label": 0
                },
                {
                    "sent": "You can show the 1 / sqrt N result, but you cannot perform the 2nd order analysis and I will explain why becausw that you cannot guarantee this noise condition.",
                    "label": 0
                },
                {
                    "sent": "The variance control assumption that allows you to reach the one over North.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "I will show now what it takes to reach this kind of bound and you will see that things get ugly.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second representation of you statistique is what is called hardings decomposition, and the idea is to some kind of orthogonal projection of EU statistic over the space generated by averages of IID random averages as function of 1 random variable and the sequence is IID.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the main term here.",
                    "label": 0
                },
                {
                    "sent": "So you have a constant term which is just the expected value of the statistics for a fixed.",
                    "label": 0
                },
                {
                    "sent": "F and the leading term here is an empirical average of IID, and you have a remainder turn W Nov F, which is what we call a degenerate you statistique and generate you.",
                    "label": 1
                },
                {
                    "sent": "Statistique is typically of smaller order than regular U statistic and the assumption is that the expected value the conditional expectation of the function of the pair given one of the of the two elements in the pair is.",
                    "label": 0
                },
                {
                    "sent": "Almost surely equal to zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This kind of objects are typically smaller order.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we use now, this kind of decomposition, the idea is that while the leading term, if you look at now, you change while the F is a decision rule and it's a candidate.",
                    "label": 0
                },
                {
                    "sent": "So you want to control the supremum over the class of candidate F. So this behaves as an empirical process, so you can just choose the previous machinery used for classification to deal with this one and the only thing we need to do now is to.",
                    "label": 0
                },
                {
                    "sent": "Make sure that this remainder term will not spoil the rate of convergence, which is not clear because we want to look at the results uniformly and we can have some.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rises so under which conditions we can guarantee that the remainder term is uniformly small over the class of decision rules.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are these two questions.",
                    "label": 0
                },
                {
                    "sent": "So first make clear what is the function H here because you need to compute to make some computations to find the projection and then write down the results.",
                    "label": 1
                },
                {
                    "sent": "With this, just thinking that the remaining term is not is not interfering and then just guarantee that the remainder term.",
                    "label": 0
                },
                {
                    "sent": "Is small enough OK?",
                    "label": 0
                },
                {
                    "sent": "So I don't want to spend too much time on this boring details, but I want to give you the some ideas.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to don't look at these displays, but the idea is just to apply this second representation of you statistics on the excess risk when you deal with the excess risk when you take the difference between the ranking error an well the empirical rule.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "King error and the true ranking error.",
                    "label": 0
                },
                {
                    "sent": "So you know, sorry when you write down the excess risk, you have a function here which depends on XYX prime Y prime.",
                    "label": 0
                },
                {
                    "sent": "So it's just a centering of of the function involved in the empirical error and then you take the empirical counterpart of this excess risk.",
                    "label": 0
                },
                {
                    "sent": "So this is the object for which we build the for which we do the projection and we consider the conditional expectation.",
                    "label": 0
                },
                {
                    "sent": "Of this quantity.",
                    "label": 0
                },
                {
                    "sent": "So now this is the result.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the result is that you have when you look at the access risk with respect to the best possible error you have this approximation error term which comes for from the fact that you use a restricted family of ranking rules.",
                    "label": 1
                },
                {
                    "sent": "So this we don't deal with this term.",
                    "label": 1
                },
                {
                    "sent": "But here you have the estimation error.",
                    "label": 0
                },
                {
                    "sent": "So up to logarithmic term you can see that we can reach a rate of one over North if Alpha can be as large as one.",
                    "label": 0
                },
                {
                    "sent": "And this Alpha is involved here in the variance control condition, which is the same as before, except that here instead of the indicator of an error we have this function H of R which was on the previous display and this is a power function of the excess risk, so it's similar to the standard set up, except that we have to make explicit what it means for the distribution.",
                    "label": 0
                },
                {
                    "sent": "Because this is just I mean.",
                    "label": 0
                },
                {
                    "sent": "This is just how we use it, but it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It doesn't give us any insight on what it means for the distribution.",
                    "label": 1
                },
                {
                    "sent": "Maybe it's too restrictive and maybe there are no distribution that satisfy this this condition.",
                    "label": 0
                },
                {
                    "sent": "So we have to say something about that.",
                    "label": 0
                },
                {
                    "sent": "So what does this condition means for the regression function for F of X?",
                    "label": 0
                },
                {
                    "sent": "So we have a.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sufficient condition here we call the noise assumption.",
                    "label": 1
                },
                {
                    "sent": "That is, if the expected value of the conditional expectation of ETA of X -- F of X prime given X prime to the power minus Alpha is smaller than some constant C, then you can guarantee the previous variance control condition and this is easy to see that.",
                    "label": 0
                },
                {
                    "sent": "This condition is true for Alpha strictly smaller than one.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If Alpha equals one, we have a problem, but we can reach Alpha as close as possible to one.",
                    "label": 0
                },
                {
                    "sent": "And this is guaranteed only if I mean its surface is to take out of X absolutely continuous on 01 with a bounded density.",
                    "label": 1
                },
                {
                    "sent": "So let's not the most general, but it's pretty general.",
                    "label": 0
                },
                {
                    "sent": "If we had used, if we had considered the setup where we consider blocks of IID pairs, then we wouldn't have this HR function.",
                    "label": 0
                },
                {
                    "sent": "And if we had written down the corresponding condition, we would have found that the condition of X is pretty serious, and it would have meant that at of X is a discrete distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so we cover with this.",
                    "label": 0
                },
                {
                    "sent": "Approach a much larger range of distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is about the regression model.",
                    "label": 0
                },
                {
                    "sent": "We have similar results and now there.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remainder term, so this is the most technical part, so I will skip it.",
                    "label": 0
                },
                {
                    "sent": "But just to tell you the idea is that to guarantee that the remainder term is small enough.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have additional complexity measures which come into the picture and we.",
                    "label": 1
                },
                {
                    "sent": "I mean it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It's not enough to only consider the Rademacher average.",
                    "label": 0
                },
                {
                    "sent": "That I mentioned on one of the first slides, so we have additional complexity measures to which have to be bounded in some sense in order to guarantee that the remainder term will not.",
                    "label": 0
                },
                {
                    "sent": "Spoil this nice rate of convergence results.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lies on some moment inequality and there are many techniques from the theory of you processes.",
                    "label": 0
                },
                {
                    "sent": "Many papers from probability theory.",
                    "label": 0
                },
                {
                    "sent": "Which gives some results, but we needed to develop a different one in order to fit with this learning application.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I won't comment on this right now, except if you have questions later on and we also check that in the VC case W~ North so the remainder term is of the order of one over North.",
                    "label": 1
                },
                {
                    "sent": "OK, so if we have a rate up to one over North, this doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "OK. Now.",
                    "label": 0
                },
                {
                    "sent": "I want to make the connection with specific criteria used in bipartite ranking so the previous model I described, this preference model is actually exactly the program for assessing the performance of strategies which maximize the AUC.",
                    "label": 0
                },
                {
                    "sent": "This is exactly it.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the AUC.",
                    "label": 0
                },
                {
                    "sent": "So this is the area under the arosi curve so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to recall what RC curves are, but I imagine that most of you know what they are, but the idea is that the AUC can be rewritten as a probability.",
                    "label": 0
                },
                {
                    "sent": "Of so this is like a rate of concording pairs.",
                    "label": 0
                },
                {
                    "sent": "People in credit risk screening.",
                    "label": 0
                },
                {
                    "sent": "They call it like this.",
                    "label": 0
                },
                {
                    "sent": "So this is counting how many pairs of observations are well ordered by some scoring rule South.",
                    "label": 0
                },
                {
                    "sent": "So scoring rule.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As here is just a real valued decision rule, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the AUC looks like that.",
                    "label": 0
                },
                {
                    "sent": "So this is a probability of having S of X larger than F of X prime, given that the label of X is larger than the label of X prime.",
                    "label": 0
                },
                {
                    "sent": "And you can write down the optimal value of the AUC for a given distribution, and you see again just as just like in our analysis of the ranking error that the critical quantity is this deviation of Etta between two points X&X prime.",
                    "label": 1
                },
                {
                    "sent": "OK, so how does it relate to the ranking error?",
                    "label": 1
                },
                {
                    "sent": "Well, just consider the ranking rule R which says plus one if F of X is larger than F of X prime and minus one otherwise and then you are back to the previous.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It up OK, so the AUC you can just write down, write it down as a function of the ranking errors, so that maximizing the AUC is exactly equivalent to minimizing the ranking error.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this was it?",
                    "label": 0
                },
                {
                    "sent": "I mean we did the all the theorie for.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Empirical performance, maximize maximization based on the AUC.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Is it just a matter of.",
                    "label": 1
                },
                {
                    "sent": "You know classification with pairs of observations.",
                    "label": 1
                },
                {
                    "sent": "Because of course I describe the theoretical aspect and the rates of convergence, but from the viewpoint of algorithms it doesn't change much.",
                    "label": 0
                },
                {
                    "sent": "I mean it just says OK take all the pairs and learn with any of your favorite classification method and then you're good.",
                    "label": 0
                },
                {
                    "sent": "OK, so is that it is at the end of the story.",
                    "label": 0
                },
                {
                    "sent": "So of course this is not the end of the story, because there you see is not a good criterion for ranking.",
                    "label": 0
                },
                {
                    "sent": "I mean now.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bad one.",
                    "label": 0
                },
                {
                    "sent": "But in most of the ranking applications you only deal with some part of the observation space, and you especially are interested in the best possible instances.",
                    "label": 0
                },
                {
                    "sent": "OK, so you only care usually about the top of the list.",
                    "label": 0
                },
                {
                    "sent": "I mean, at least it should have more weight than what happens at the bottom of the list.",
                    "label": 0
                },
                {
                    "sent": "OK, so how can you emphasize the errors in this part of the observation space?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So of course you.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are many other criteria for for answering this question, so I will just give you maybe a few and show the theoretical consequences of dealing with this other criteria.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I consider another question, not preference learning, but ranking the best possible instances and there is a subproblem which is first finding these best instances.",
                    "label": 0
                },
                {
                    "sent": "'cause ranking the best is a double problem.",
                    "label": 1
                },
                {
                    "sent": "You first need to find them and also to rank them.",
                    "label": 0
                },
                {
                    "sent": "But you cannot really do it.",
                    "label": 0
                },
                {
                    "sent": "I mean you have to do it simultaneously.",
                    "label": 0
                },
                {
                    "sent": "So this is a JML.",
                    "label": 0
                },
                {
                    "sent": "Our paper of 2007 I think written with Steven but there are also some other aspects we developed in more recent papers so I will mention the papers in.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "So here will.",
                    "label": 0
                },
                {
                    "sent": "For get a bit about the ID thing, we'll see how the non IID part.",
                    "label": 0
                },
                {
                    "sent": "So we will see the how it comes into the picture.",
                    "label": 0
                },
                {
                    "sent": "But first I need to explain the criterion for this problem.",
                    "label": 0
                },
                {
                    "sent": "So what criterion should we consider if we are looking for the best possible instances?",
                    "label": 0
                },
                {
                    "sent": "We still have only a binary label information.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just like looking at the RC curve.",
                    "label": 0
                },
                {
                    "sent": "It's only restricting the attention to a part of the RC curve, meaning in the beginning of the RC curve.",
                    "label": 0
                },
                {
                    "sent": "OK, so we need to do some cut off of the RC curve and to do this cutoff we fix a constraint.",
                    "label": 0
                },
                {
                    "sent": "So the parameter here is, it will be denoted by U and it is a percentage of the best instances we want to on which we want to focus.",
                    "label": 1
                },
                {
                    "sent": "So say I want to.",
                    "label": 0
                },
                {
                    "sent": "Rank correctly the 5% the best 5% of the observations.",
                    "label": 0
                },
                {
                    "sent": "OK, so this will be our parameter here the constraint.",
                    "label": 0
                },
                {
                    "sent": "So now I'm still in the spirit of scoring, so I have a scoring ruler because I want to order the objects so I cannot deal anymore with classifiers but.",
                    "label": 0
                },
                {
                    "sent": "I mean in this sub problem of finding the best instances we come up with a classifier which is based with this scoring rule.",
                    "label": 1
                },
                {
                    "sent": "But the fact that I have this constraint, actually it means that if I take an arbitrary scoring rule, the thresholds depends on this you and if I want the mass of this set here to be equal to UI have to take as a threshold the quantile of the random variable X of X at level 1 -- 2.",
                    "label": 0
                },
                {
                    "sent": "OK so the fact that I consider this constraint has an implication on the form of the candidate sets for finding the best.",
                    "label": 1
                },
                {
                    "sent": "When I use a family of scoring rule denoted by S. OK so I have this constraint here that the probability that X is in this kind of set for any S is equal to U.",
                    "label": 0
                },
                {
                    "sent": "But another interesting aspect of ranking is that.",
                    "label": 0
                },
                {
                    "sent": "Remember in classification, I said we only interested in one single level set of the regression function.",
                    "label": 0
                },
                {
                    "sent": "Here we want to recover well.",
                    "label": 0
                },
                {
                    "sent": "It's not maybe clear in this slide, but actually want to recover many level sets.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the regression function and we don't really care about the regression function.",
                    "label": 0
                },
                {
                    "sent": "We don't really care about the actual values of the regression function, we only want to recover the order induced by the regression function.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is this invariance property with respect to strictly increasing transformation.",
                    "label": 0
                },
                {
                    "sent": "I mean, if I take a.",
                    "label": 0
                },
                {
                    "sent": "If I take an increasing transformation of South, I won't change the order, so it should be as good as South from the viewpoint of bipartite ranking.",
                    "label": 0
                },
                {
                    "sent": "OK, so I also have this property and it's OK with this.",
                    "label": 0
                },
                {
                    "sent": "With this family of candidate decision rules.",
                    "label": 0
                },
                {
                    "sent": "So basically you just redefine the varosi, but using these U as a parameter an the constraint on U involves some relationship between the true positive rate and the false positive rate.",
                    "label": 0
                },
                {
                    "sent": "Here P is the proportion of plus one labeled.",
                    "label": 0
                },
                {
                    "sent": "So I just show on the graphical display probably easier to understand.",
                    "label": 0
                },
                {
                    "sent": "So for a fixed scoring rule I have an RC curve here and I want to determine.",
                    "label": 0
                },
                {
                    "sent": "What is the well actually I'm after?",
                    "label": 0
                },
                {
                    "sent": "A criterion for to optimize for finding the best OK.",
                    "label": 0
                },
                {
                    "sent": "So maybe an idea is to take to truncate the AUC at some point and say I want to maximize this partial AUC and people in Biostatistics, that's what they do, but here we have this constraint with the rate of good instances of top ranked instances.",
                    "label": 0
                },
                {
                    "sent": "This U and this induces a constraint which is represented here.",
                    "label": 0
                },
                {
                    "sent": "But by this control.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Online, so the intersection here should tell us where the cut off is.",
                    "label": 0
                },
                {
                    "sent": "So where shall we truncate the orosi?",
                    "label": 0
                },
                {
                    "sent": "So this is a standard definition, the partial AUC, except that it's not a good one.",
                    "label": 0
                },
                {
                    "sent": "Becausw now take our OC curve which is higher in this graph, so it should be better.",
                    "label": 0
                },
                {
                    "sent": "The red line should give us a better scoring rule than the green line.",
                    "label": 0
                },
                {
                    "sent": "But if you take the truncation, you should look at the intersection.",
                    "label": 0
                },
                {
                    "sent": "With this you with this line here, which depends on you and pee.",
                    "label": 0
                },
                {
                    "sent": "So for fixed you and P you have the same line but diff.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It cut off and it's not clear that the partial AUC for the red is larger than the green, and actually this is usually well, it can be wrong.",
                    "label": 0
                },
                {
                    "sent": "It's in general it's not true that it's larger.",
                    "label": 0
                },
                {
                    "sent": "OK, so we need to make a correction of this partial AUC, and the idea is to.",
                    "label": 0
                },
                {
                    "sent": "Add this term here, which is the true.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Positive rate times 1 minus the false positive rate.",
                    "label": 0
                },
                {
                    "sent": "And it means that you just have to consider the rectangle which is here and so the the right criterion is to take all this area here together with this rectangle and then you have a consistent criterion for learning the best ranking the best.",
                    "label": 0
                },
                {
                    "sent": "Instances OK, so this was not very clear in the literature so.",
                    "label": 0
                },
                {
                    "sent": "And also it's I mean it makes clear that finding I mean you have a double problem here.",
                    "label": 0
                },
                {
                    "sent": "Finding the best and also ranking.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Them correctly on this part of the domain, so you can think about various strategies for that.",
                    "label": 0
                },
                {
                    "sent": "So now for the end of the talk I want to give you an idea of.",
                    "label": 0
                },
                {
                    "sent": "I mean all the.",
                    "label": 0
                },
                {
                    "sent": "The developments we made on you statistiques.",
                    "label": 0
                },
                {
                    "sent": "Can we extend it for other?",
                    "label": 0
                },
                {
                    "sent": "Other optimization principles, for instance, to learn the set of best objects so.",
                    "label": 1
                },
                {
                    "sent": "This is just the same notations as before, so if we focus on the on this sub problem of just finding the set of best instances then you can think of it as a problem as a classification problem.",
                    "label": 1
                },
                {
                    "sent": "But where you have an additional constraint which is that the size of all classifiers is equal to you and if you write down you can also focus on the same notion of error the classes.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fication error and then the the given the constraint.",
                    "label": 0
                },
                {
                    "sent": "Then you don't necessarily recover the optimal element for the unconstrained problem.",
                    "label": 0
                },
                {
                    "sent": "This depends on EU.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here the optimal elements are classifiers, where you threshold at the level of the quantile of the random variable X of X of the other 1 -- 2.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have a different threshold.",
                    "label": 0
                },
                {
                    "sent": "Instead of having 1/2, you have a different threshold for ETA to find the optimal set.",
                    "label": 0
                },
                {
                    "sent": "So when you write down the excess.",
                    "label": 0
                },
                {
                    "sent": "Of error then you you are interested in the behavior of FX around its quantile of order 1 -- 2.",
                    "label": 0
                },
                {
                    "sent": "OK, but the you see immediately what it means from the viewpoint of estimation.",
                    "label": 0
                },
                {
                    "sent": "But you have.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is much more complicated.",
                    "label": 0
                },
                {
                    "sent": "'cause this quantity also depends on the distribution.",
                    "label": 0
                },
                {
                    "sent": "We don't have a fixed value here.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we look at.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The empirical risk minimization formulation for this problem, and we consider candidates which have this form and they fulfill the two constraints or the mass constraint and the invariants by through increasing transforms.",
                    "label": 1
                },
                {
                    "sent": "So this is a natural object to natural object to consider too, as a possible decision rule for this problem and the empirical counterpart of the risk here is just the previous one where we plug.",
                    "label": 0
                },
                {
                    "sent": "The empirical quantile instead of the true contact because the content depends on the distribution.",
                    "label": 1
                },
                {
                    "sent": "So you, I mean it's the inverse of the cumulative distribution function.",
                    "label": 0
                },
                {
                    "sent": "So you just take the generalized inverse of the empirical CDF of South of XI.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here you see that we don't have an average of IID because the queue had depends on all the sample.",
                    "label": 0
                },
                {
                    "sent": "It depends on all the exercise.",
                    "label": 0
                },
                {
                    "sent": "OK, so we cannot use the classical tools for that.",
                    "label": 0
                },
                {
                    "sent": "And so yeah, so proving things is not so much fun, But the interesting part is that I mean some some.",
                    "label": 0
                },
                {
                    "sent": "Parts of it are not so intuitive in that case, and also the statistical results tell us something about the nature of scoring rules we should use for this problem.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The scoring rules we need to consider.",
                    "label": 0
                },
                {
                    "sent": "I mean they're not necessarily linear by parts, but they should be neither too flat nor too steep.",
                    "label": 0
                },
                {
                    "sent": "OK, so you cannot have constant parts for the scoring rule.",
                    "label": 0
                },
                {
                    "sent": "It's really bad for ranking, so at so here it's not exactly ranking, it's just finding the best.",
                    "label": 0
                },
                {
                    "sent": "But you need at the level of the quantile to have.",
                    "label": 0
                },
                {
                    "sent": "Maybe not to jump, but you have.",
                    "label": 0
                },
                {
                    "sent": "You need to have a significant variation of ActiveX Locali.",
                    "label": 0
                },
                {
                    "sent": "OK, so you need this stiffness, but at the same time you need to estimate this quantile and to estimate the contact you need to have enough mass so you cannot have it too steep because otherwise you don't have enough mass to estimate the quantile.",
                    "label": 0
                },
                {
                    "sent": "So you have this.",
                    "label": 0
                },
                {
                    "sent": "Agonistic effects on the estimation, and this indicates that the scoring rule for this problem should look like that OK.",
                    "label": 0
                },
                {
                    "sent": "So the objects we deal with are.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we call signed rank statistics.",
                    "label": 1
                },
                {
                    "sent": "So there are more complicated than you statistics, so maybe I will skip that.",
                    "label": 0
                },
                {
                    "sent": "So you have some kind of similar decomposition except so.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just check the even if you don't know what the notations mean, but this is.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say the empirical object want to minimize.",
                    "label": 0
                },
                {
                    "sent": "And this is the expected value.",
                    "label": 0
                },
                {
                    "sent": "So it's like a constant.",
                    "label": 0
                },
                {
                    "sent": "This is the leading term and this is the remainder term, so the remainder term we can show that it's on the order of one over North, so we're safe.",
                    "label": 0
                },
                {
                    "sent": "And now what does this term look like?",
                    "label": 0
                },
                {
                    "sent": "I mean, if we want to go to the fast rate thing and.",
                    "label": 0
                },
                {
                    "sent": "And it looks like something quite unnatural.",
                    "label": 0
                },
                {
                    "sent": "It takes this form and it's not just the empirical version where we plug the true quantile.",
                    "label": 0
                },
                {
                    "sent": "There are some corrections which involve the partial derivatives of this K, which is the well.",
                    "label": 0
                },
                {
                    "sent": "Actually, this is the true criterion.",
                    "label": 0
                },
                {
                    "sent": "OK, so well, I don't go into the details because probably you're tired now and maybe it's not so interesting, but I just want to illustrate the fact that it's not trivial and it's unexpected.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not just you know you need really to go into the details and to see how things work properly.",
                    "label": 0
                },
                {
                    "sent": "So now the interesting part, I think from the viewpoint of machine learning and I'm close to the end.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "I mentioned this signed rank statistics and actually even you statistique is a rank statistic cause the simple you statistic is known as the Mann Whitney statistic.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a simple operation.",
                    "label": 0
                },
                {
                    "sent": "Makes a connection with a statistic based on the ranks of the random variables and this is called the Wilcoxon statistic, so there is a.",
                    "label": 0
                },
                {
                    "sent": "Very classical relation between the simple use statistiken Wilcoxon statistic.",
                    "label": 0
                },
                {
                    "sent": "I mean you statistic of order 2.",
                    "label": 0
                },
                {
                    "sent": "And you can generalize these rank statistics by applying some function here.",
                    "label": 0
                },
                {
                    "sent": "So far here is what is called the score generating function, so nothing to do with the scoring rules, it's just a wait waiting function on the ranks.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have the you have a decision rule which is a scoring rule, real valued South and you look at the position of the observation number.",
                    "label": 0
                },
                {
                    "sent": "I according to this function.",
                    "label": 0
                },
                {
                    "sent": "OK, the position among all other S of XJ's, let's say, and you just record the position and so here.",
                    "label": 0
                },
                {
                    "sent": "This type of criterion would.",
                    "label": 0
                },
                {
                    "sent": "So the top of the list is.",
                    "label": 0
                },
                {
                    "sent": "The corresponds to the highest ranks.",
                    "label": 0
                },
                {
                    "sent": "And this function should put more emphasis on the large ranks.",
                    "label": 0
                },
                {
                    "sent": "OK, so it should be increasing.",
                    "label": 0
                },
                {
                    "sent": "And you want to maximize this kind of criterion.",
                    "label": 0
                },
                {
                    "sent": "So this is a very general form.",
                    "label": 0
                },
                {
                    "sent": "OK, and it's connected to you statistics and actually it covers most of the standards criteria used in machine learning communities, so it covers the AUC.",
                    "label": 0
                },
                {
                    "sent": "Just take five X equal to X.",
                    "label": 0
                },
                {
                    "sent": "It covers the local AUC.",
                    "label": 0
                },
                {
                    "sent": "So the work I mentioned before with finding and ranking the best objects or just take X and truncated.",
                    "label": 0
                },
                {
                    "sent": "At the level 1 -- 2.",
                    "label": 0
                },
                {
                    "sent": "But it's also corresponds to the P norm push, so there is a paper by Cynthia Rudin, GM LR, 2007.",
                    "label": 0
                },
                {
                    "sent": "An even more Interestingly, it also covers the DCG.",
                    "label": 0
                },
                {
                    "sent": "OK, just take a C of X.",
                    "label": 0
                },
                {
                    "sent": "The discount factor function and truncate keep the K best instances.",
                    "label": 0
                },
                {
                    "sent": "OK, so why not considering also smooth versions of this code generating functions and maybe have a general theory for this criteria?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is so.",
                    "label": 0
                },
                {
                    "sent": "I don't have a general theory from the viewpoint of statistics, but there is a general formulation at least OK and I just illustrated through some examples what we can do with with this type of description Now too.",
                    "label": 0
                },
                {
                    "sent": "To finish with the talk, I want to say that.",
                    "label": 0
                },
                {
                    "sent": "All these as the title of the slide points out all these criteria.",
                    "label": 0
                },
                {
                    "sent": "They are essentially contained in the RC curve, so the RC curve is richer.",
                    "label": 0
                },
                {
                    "sent": "Description of performance.",
                    "label": 0
                },
                {
                    "sent": "OK, so our philosophy now is.",
                    "label": 0
                },
                {
                    "sent": "Not so much to try, you know, to find optimization algorithms for criteria like that and try to optimize them, but rather try to go back to this arosi curve and try to optimize.",
                    "label": 0
                },
                {
                    "sent": "Optimize it directly, which is a difficult problem because it's a function like criterion.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not just like you have a scalar value for the criterion you optimize.",
                    "label": 0
                },
                {
                    "sent": "It's a function.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's much more difficult, but it's doable.",
                    "label": 0
                },
                {
                    "sent": "And the algorithms I mentioned in the introduction.",
                    "label": 0
                },
                {
                    "sent": "They try to optimize the arosi directly.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a different way to look at things in this bipartite ranking problem.",
                    "label": 0
                },
                {
                    "sent": "So there is all this theory about empirical performance maximization, for which I gave you some ideas, but from the algorithmic point of view, I'm not sure that this is the right direction.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is mostly the second option, so I think I'm.",
                    "label": 0
                },
                {
                    "sent": "Beyond time, so I'll be happy to talk or to point out some references on the second part and will have as I said, a demo presentation for some software we developed for building scoring rules which optimize the arrow seeker.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me finish with the conclusion slide.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with some references.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}