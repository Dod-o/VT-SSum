{
    "id": "ecscskidtmqqorr22dak4pv63ridfhoa",
    "title": "Learning Diverse Rankings with Multi-Armed Bandits",
    "info": {
        "author": [
            "Robert Kleinberg, Department of Computer Science, Cornell University"
        ],
        "published": "Aug. 6, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_kleinberg_ldr/",
    "segmentation": [
        [
            "Thanks very much.",
            "Anne.",
            "This talk is joint work with Philip Radlinski and Torsten Walkins, both from Cornell University and the motivation for this work is."
        ],
        [
            "Similar for the motivation of the paper that you song just presented, so I'm going to try to breeze through it a little bit rapidly.",
            "For those who were here earlier.",
            "Typically, in learning to rank problems, the method that's applied is too.",
            "Learn some parameterized ranking function of a document and query a scoring function which is then applied to produce a ranking by simply extracting the K documents with the highest scores and the theoretical justification for this technique is sometimes called the probabilistic ranking principle.",
            "It's the principle that documents presented in response to a query should be ranked in decreasing order of their probability of relevance to that.",
            "Query as judged by a learns ranking function.",
            "And as was pointed out in the preceding talk, this principle breaks down when you consider the diversity of users information needs and to avoid repeating the example of Jaguar and also because I'm giving a talk that has the word bandits in the title of it.",
            "I'd like to point out a simple example of this.",
            "So if a user is querying for bandits, we can't be sure if they are interested in information about type of online learning problem.",
            "Featured for example in this in this talk.",
            "Alternatively, they might be asking about the professional indoor lacrosse team from Buffalo, NY.",
            "Or a film made by Barry Levinson in 2001, starring Bruce Willis, Billy Bob Thornton, and Cate Blanchett.",
            "When I was preparing this talk, actually I struggled to remember if I even knew that there was a film starring those three actors, and.",
            "If so, why had I never heard of it before?",
            "Then I realized was made by Barry Levy."
        ],
        [
            "Listen.",
            "But let's say, so that's motivation.",
            "Number one for our talk motivation.",
            "#2.",
            "Is this notion that to the extent possible, we should design algorithms for learning to rank that can be trained using implicit feedback from users.",
            "In other words, trains using logs of actual usage data rather than manually labeled relevance judgments collected from experts.",
            "There are many reasons for this usage.",
            "Data is much more plentiful and can be obtained at nearly 0 cost.",
            "As the set of documents available changes overtime or as users, information needs change overtime.",
            "Techniques that rely on manually labeled relevance judgments constantly need retraining, whereas techniques based on usage data can adapt automatically.",
            "Also, if you think about certain information retrieval applications such as.",
            "Designing the search engine to run on you know the homepage of the Cornell University computer Science Department.",
            "For example, Cornell is not going to employ an experts just to label training data for the ranking algorithm applied by the search algorithm on their home site.",
            "So for all these reasons, is desirable.",
            "To develop a ranking algorithms that can capitalize usage data from raw users.",
            "Just based on their clicking behavior alone.",
            "And insight that was part of the impetus for this paper.",
            "Is that the problem of learning to rank from live usage data?",
            "It can naturally be cast as an online learning problem.",
            "In particular, it has that most familiar of aspects in online learning.",
            "The tradeoff between exploration and exploitation, that is to say.",
            "When deciding which documents to present to the current user in response to his or her query.",
            "We have to consider two factors.",
            "The past performance of the documents that we are considering presenting.",
            "Which will determine their likely performance as relevant responses for this current user.",
            "But also the utility of those documents in providing training data for future decisions.",
            "These two goals naturally correspond to exploitation and exploration, and this is an insight which has guided some of the previous work in this area and also certainly guided our work.",
            "So I've identified now two crucial aspects of the work in this paper.",
            "One is the diversity of documents in a list of K return in response to a query.",
            "And the other is using usage data rather than manually labeled examples.",
            "And in surveying the.",
            "If my."
        ],
        [
            "Pewter ever allows me to advance the slide.",
            "Yeah, in surveying the prior work.",
            "I want to point out that if you drop either one of these features or both, there have been many papers in the past on any of those subsets of these features.",
            "So certainly this large box.",
            "Learning to rank using manually labeled training data and applying probabilistic ranking principle to rank by relevance.",
            "There's so many papers that I'm not going to go into that literature here.",
            "We've just seen a talk by you song.",
            "That gave one algorithm that accounts for diversity within the framework of learning to rank, and it did it by training structural SVM's to maximize topic coverage.",
            "There have been many other papers as well.",
            "Maybe one of the most well known techniques is called maximum marginal relevance.",
            "And it requires two inputs.",
            "One is a learned ranking function that predicts the relevance of a document to a query and the other is a similarity or dissimilarity function that, given two documents, outputs the predicted amount of topic dissimilarity between them.",
            "A list is then prepared by.",
            "Greedily choosing each documents to maximize a weighted sum of these two objectives, relevance and dissimilarity from the preceding ones on the list.",
            "And then finally, there's one quite relevant paper by Chen and Karger from 2006, which, however, differs from our model.",
            "And assuming that the algorithm is given a model for estimating the probability of relevance of 1 document to a query given both the query and a list of other non relevant documents, so it assumes quite rich amount of prior knowledge on the part."
        ],
        [
            "The algorithm.",
            "Suppose that we keep the other constraints in our problem and drop the first One South.",
            "Think about algorithms that apply the probabilistic ranking principle, but try to do it in the framework of learning from actual usage data rather than manually labeled examples.",
            "We then come to a bunch of papers that I've listed here.",
            "Most of which rely on passive methods, that is they.",
            "Retroactively look at click through logs to generate feature features, which are then used to train a learning to rank algorithm.",
            "One.",
            "Quite recent paper by Radlinski and you ACAMS is closer in spirit to our work.",
            "It uses active methods that use learning theoretic ideas to choose.",
            "The two documents to present 1st and 2nd in the current ranking.",
            "Uh, based on the type of exploration, exploitation tradeoff that I was referring to two slides ago.",
            "But again, since the goal of that work is really to eventually produce a list of the top K most relevant documents.",
            "Rather than a list that jointly covers the greatest number of users information needs.",
            "The objective of the work is quite different from ours.",
            "OK, so accounting for both of these factors in the same paper, as far as I know this work that I'm presenting is the first to do it and I think I guess the final thing that I want to say before moving on to the technical part of the talk is that combining these two factors is not just a trivial matter of re combining ideas from the existing set of papers.",
            "Probably the best way to see that is that, as I explained two slides ago, what we really have here is an online learning problem.",
            "And if you look at the set of strategies available to the decision maker in this learning problem.",
            "If you're producing a ranking of length K from a set of end documents, then you have on the order of end of the case strategies and for realistic values of NK this number is just astronomically huge and so existing algorithms for that online learning problem would not scale adequately and we had to actually exploit the problem structure in a nontrivial way to design new online learning algorithms for this domain."
        ],
        [
            "So now let me give you a very brief synopsis of our results at this point in the talk.",
            "I haven't really defined our model, so I can't give you rigorous statements about theoretical results, but I'm going to present to you two algorithms for this problem, each of which achieves in the worst case at least 1 -- 1 / E times the optimum number of clicks.",
            "Minus a little low of T term that can be thought of as the regret of the algorithm, although unlike the conventional meaning of regret, I'm comparing against 63% of the optimum instead of the optimum.",
            "This comparison is justified by complexity theoretic lower bounds that says that no algorithm that uses polynomial computation time can do better than 63% of the optimum.",
            "And then in some similar in some simulation results that I'll present at the end of the talk will see first of all that this number 63% is probably much more pessimistic than necessary in real data.",
            "We'll also see confirmation that intentionally learning a diverse ranking outperforms simply ranking by popularity.",
            "And finally, we'll see that third algorithm that uses the same ideas but is not.",
            "You know, constrained by the need to satisfy a provable theorem about his performance can sort of achieve best possible performance over a large range of can."
        ],
        [
            "Times.",
            "So in the rest of the talk I'm going to review very briefly multi arm bandit algorithms because our problem can be thought of as a generalization thereof.",
            "I'm then going to talk about that generalization, which we call ranked bandit algorithms present.",
            "Our simulation results, and finally I'll talk about some."
        ],
        [
            "Extensions and conclusions.",
            "OK, so very briefly.",
            "I use the term multi arm bandit problem to refer to any problem that has the following setup.",
            "It is a problem in which a decision maker has a set of end strategies denoted by S. And an adversary chooses a sequence of payoff functions mapping S to a bounded interval, which I will always normalize to be the interval 01.",
            "Actually, in our examples, S will just be the two elements at 01.",
            "There are different kinds of adversaries.",
            "One might consider an actually in this talk to different notions of adversary or important, and IID adversary is one that generates these payoff functions by sampling from a fixed distribution that does not vary overtime but is unknown to the algorithm designer.",
            "Whereas an adaptive adversary is one that can choose absolutely any payoff function at time T based on the past history of interaction between the adversary and the algorithm, but not knowing the algorithms decision at time T. The way these two parties, algorithm and adversary interact is that in each time step, the algorithm picks one element of strategy set.",
            "The adversary picks a payoff function.",
            "That function is evaluated on the chosen strategy and the payoff of the strategy the algorithm picked is revealed.",
            "And then the parameter by which we evaluate these algorithms is the difference.",
            "An expected payoff between running the algorithm or using the best strategy.",
            "In hindsight that's expressed in this equation that I've written across the bottom of the slide before I move on, I want to point out that in the ranking the document ranking examples that motivate this talk, the strategy sets is the set of all ordered K tuples of distinct documents.",
            "These are all the rankings that you could display in response to users.",
            "Mary and then the payoff function that we will be considering is simply one.",
            "If user clicks on a document indicating that they find the relevant document in the List 0.",
            "If the user abandons the list of results without clicking, indicating that no relevant document is found.",
            "To illustrate these days."
        ],
        [
            "Missions before I move on.",
            "Here's a toy example in which we have a set of three strategies that are represented by slot machines, which also incidentally justifies the name multi armed bandit problems.",
            "Since a slot machine is sometimes called A1 armed bandit.",
            "So in this toy example, gambler is trying to decide which of these three slot machines to play every minute.",
            "And it chose the leftmost one in.",
            "One.",
            "The middle in period two, and so on, receiving a total payoff that amounted to 2.2.",
            "These numbers in white represent pay offs that the gambler would have received had it chosen an alternative slot machine.",
            "From one it shows.",
            "So the data which is revealed to the Gambler or the algorithm consists only of the orange numbers and not the white.",
            "And regret is computed by comparing the algorithm's total payoff with the maximum column sum in this matrix, which happens to be 2.6.",
            "So in this example, regret would be 0.4."
        ],
        [
            "OK, I'm now going to review for you some well known multi armed bandit algorithms.",
            "I hesitate to say that the first one is well known.",
            "I might perhaps describe it as universally known in the sense that if I asked all of you to close your eyes and not look at my slides and just think by pure reasoning, what's a good algorithm for the gambler to use in this situation?",
            "This algorithm exploring commit is one that I predicts a majority of people in the room.",
            "Would come up with just by pure thought.",
            "The idea is simply this.",
            "We have some preconfigured integer parameter S. The number of samples.",
            "And the algorithm begins by sampling every strategy S times.",
            "It computes a sample average for each of them.",
            "It picks out the one with the highest average payoff and forever after that it plays that strategy.",
            "The extremely simple algorithm and against IID adversary this algorithm is justified by the fact that the sample average is computed at the beginning will be representative of average pay offs in the continuation of the game.",
            "In fact, it satisfies regret bound that I've written there and whose salient feature is that if we optimize over the value of S to minimize the bound on the right hand side, we get something which is roughly.",
            "Enter the 1/3 * 2 to the 2/3 so it's, uh.",
            "In particular, the regret is governed by a power of T strictly less than one, so your average time converges.",
            "We passed to 0 using this algorithm.",
            "If you want even faster convergence of regret, you can play this UCB one algorithm.",
            "It's based on computing for every strategy a confidence bound centered on the mean payoff musaab T of X.",
            "And with with WT of X that's defined in such a way that the probability that the true mean payoff of a strategy ever violates its confidence interval.",
            "Is extremely small, about 1 /, T ^2.",
            "I.",
            "Having to find confidence intervals in that way, the algorithm always just picks the strategy whose upper confidence bound is as large as possible and this is a beautiful way of trading off exploration versus exploitation, because in general the strategies with large mean pay offs will have large upper confidence bounds and so exploitation is naturally built into the algorithm.",
            "However, if there's something that you have not played sufficiently many times to be very sure of your estimate, then it's upper confidence bound will be inflated, so exploration is also built-in because we're adding in this WT term.",
            "And OK, so the regret of this algorithm is bounded by about square root of NT.",
            "That's an improvement on the previous one.",
            "For large values of T. Moreover, for every instance of this problem, in other words, for every distribution defining an IID adversary, the regret actually scales logarithmically in T for some in."
        ],
        [
            "Independent constancy.",
            "I finally there's this more complicated algorithm.",
            "It's quite well known Exp 3.",
            "Which is given a parameter gamma.",
            "And which samples from a mixture of two distributions with probability 1 minus gamma at samples from an exponential family which assigns probability.",
            "Ended this exponential in an unbiased estimate of its average payoff up to that time.",
            "And otherwise samples from the uniform distribution.",
            "It then updates its estimates of average pay offs in a way that preserves the invariant that these estimates are unbiased estimators of the true total payoff.",
            "So in summary, we have three algorithms with three different desirable features.",
            "The exploring commit algorithm is desirable in its simplicity.",
            "UCB one is desirable in its fast convergence and Exp.",
            "Three is desirable in that it's robust against an adaptive adversary.",
            "A stronger performance guarantee than either of the other algorithms I've presented."
        ],
        [
            "Let me now talk about applying these algorithms to abandon minimization in learning to rank.",
            "So our problem is going to be that of learning a diverse ranking for a single query.",
            "This restriction to a single query is an important limitation of our work and subject of future ongoing work.",
            "So our standing assumption is going to be that there's a set of documents and a set of users.",
            "User I is for at this point in the talk, completely characterized by a set of documents, a subie that it finds relevant.",
            "And we assume that the user behavior when presented with a list of K documents is to click on the 1st relevant document observed in that list, starting from number one and continuing sequentially down to number K. Later on, we'll generalize this user behavior model to allow probabilistic clicking, but for now a user is just a set of documents and deterministic behavior that clicks on the 1st relevant one.",
            "As I said, payoff is the number of clicks and so maximizing payoff is equivalent to minimizing abandonment.",
            "Minimizing the number of users who van."
        ],
        [
            "From the site without clicking any results.",
            "And if you think about the offline problem of abandonment minimization.",
            "It's actually equivalent to the set coverage problem that you song talked about in his talk immediately preceding mine, so.",
            "Supposing at times Zero that we knew every user's information need exactly.",
            "We could compute for every document the set of users.",
            "That that document satisfies.",
            "And then the problem of learning an optimal list of K documents is simply learning K sets whose union has the maximum cardinality.",
            "This is the NP hard maximum coverage problem, and as you song told us, the greedy algorithm, which always which starts from no sets at all and always picks one to maximize the additional number of users covered.",
            "Is a 1 -- 1 / E approximation for this problem, and that's by the way the best approximation ratio achievable without violating complexity?"
        ],
        [
            "Erratic assumptions."
        ],
        [
            "OK, so.",
            "The online learning version of this problem then is 1, which is made difficult by the exponential size of the strategy set.",
            "There is recent work on designing bandit algorithms for settings where we can only do approximate rather than exact optimization.",
            "That work is inapplicable here because it assumes linear payoff functions and we are dealing with a submodular payoff.",
            "However, there is a simple idea that can explain.",
            "How we design algorithms for this setting?",
            "And it is to model the operation of the online learning algorithm on it's offline counterpart, the greedy algorithm.",
            "What that means is that we instantiate one learning algorithm, one multi armed bandit algorithm for every position in the ranking, and we trained that multi armed bandit algorithm in a way such that its objective is to maximize the marginal benefit that it provides."
        ],
        [
            "Let me show you 2 examples of that in action, and then I'll come to some conclusions.",
            "So we remember the exploring commit algorithm in the ranked exploring commit algorithm.",
            "We go sequentially through every position of the ranking from one to K. Running a sequence of a fixed number of experiments to estimate the marginal benefit of presenting a document at that position in the ranking given committed.",
            "Documents that we've already committed to at higher positions.",
            "So in this way we emulate the offline greedy algorithm and achieve.",
            "I don't even know if you can see this from your seats, but you know 1 -- 1 three minus epsilon times the optimum minus a sublinear regret term.",
            "So arbitrarily close to this one 1 -- 1 over rebound.",
            "Assuming independent idente."
        ],
        [
            "Distributed users.",
            "We can get rid of that assumption by using a more sophisticated algorithm.",
            "It again uses this idea of a sequence of bandit algorithms.",
            "One position, one for each position in the ranking.",
            "And it's not really too important.",
            "Which multi armed bandit algorithm you use there Exp 3 for example is fine.",
            "The feedback that we give to each of these multi unbanded algorithms.",
            "Is simply the marginal contribution that it makes to the combined payoff of the ranking algorithm.",
            "In other words, we give it a feedback of 1 if the user clicked the document that was chosen by the algorithm at position I in the ranking and that document was not displayed at any higher position."
        ],
        [
            "This one also satisfies a guarantee which is 1 -- 1 / 3 times the optimum minus a sublinear regret term."
        ],
        [
            "I.",
            "There's an extension too."
        ],
        [
            "Two probabilistic users that I won't talk about.",
            "There's some simulation results that I already previewed for you earlier in the talk, so.",
            "The ranked exploring commit algorithm actually comes quite close to achieving the best possible performance, even though the theorem only guarantees 63% of the best possible.",
            "It also substantially improves on just ranking by probability of relevance, according to the probabilistic ranking."
        ],
        [
            "Ansible.",
            "Moreover, if you modify the rank bandits algorithm by using UC1 instead of Exp 3 because of its more rapid convergence properties, you actually lose our theoretical guarantees, but you get an algorithm which performs near the best of all algorithms we tested over an entire range of number of trials from about 10 ^3 up to 10 to the 6th, so bye.",
            "Taking the way you might by taking the spirit of these theoretical algorithms without following the letter of the law in the theorems, you can actually get an algorithm which.",
            "Has extremely fast convergence in our simulations and performs best of all the ones that we."
        ],
        [
            "Tried.",
            "So in conclusion, what this paper is shown is that the problem of abandonment is a shanan.",
            "Learning diverse rankings can be formulated in the framework of online learning that we can exploit known online learning algorithms in this ranked fashion to generate algorithms whose worst case bounds are the best possible in polynomial time.",
            "And Moreover, that these algorithms with theoretical guarantees can be tweaked to perform very well in our simulations, and as I pre visaged earlier, the main open problem suggested by this work is how to learn across multiple queries and not just for a single query using some of the same ideas.",
            "Thank you very much.",
            "So can you comment on the what would happen if the clicks are unreliable and particular document may be relevant, but the user sees everything they need from the snippet for example.",
            "So they don't get everything they need from the snippet, and they don't click on it, right?",
            "OK. Well, that example is particularly troubling.",
            "One.",
            "OK, here is exactly the level of generalization that we can deal with.",
            "We can deal with users who have noisy clicking behavior in the sense that for every document they have some probability of clicking it, which is related to but not equal to its relevance.",
            "And they scan down the list, clicking documents according to those probabilities.",
            "Now if a document has a very informative snippet, then it's quite reasonable to model it as a document with low probability of being clicked.",
            "Given it appears on the list and the trouble of course, is that our evaluation measure is the number of.",
            "So the objective that the algorithm is trying to optimize is the number of clicks.",
            "So that's an objective that's antithetical to presenting relevant documents.",
            "In the case that you described, where there's a document whose relevance actually reduces the number of clicks.",
            "You talk about this is inevitable to have this 30% loss due to the requirement of polynomial time algorithm.",
            "But there are actually two independent questions here.",
            "The information theoretic question and complexity theoretic question.",
            "If all I care about is from the information theoretic point of view.",
            "Can you get better approximations if you ignore the polynomial time restriction?",
            "OK, so.",
            "Information theoretically.",
            "Certainly, if you're allowed to run for an exponential number of trials, then you can simply.",
            "Yeah, I know.",
            "OK so in with exponential computation time in polynomial convergence time.",
            "I believe the answer is no, it's impossible to.",
            "Improve on this 1 -- 1 over the regret, but that answer has a bit of guesswork in it and so it's really something we should discuss offline and maybe I'll be able to give you a rigorous argument.",
            "I missed the where you specified which problem you were talking about.",
            "The main open problem.",
            "Yes, generalizing across multiple queries.",
            "OK, I OK.",
            "So I think the difficulty is this.",
            "Um?",
            "Good in the course of answering these queries.",
            "You want to generate training data, which is as likely as possible to be applicable to improving rankings for a broad broad distribution, of course so.",
            "If you have the active learning problem where I could actually choose a query and a list of documents myself presented to a user and ask them what they think.",
            "That would be significantly easier than this problem, in which a sequence of queries is generated outside of the algorithms control, and it has to then decide how best to generate lists of responses to rapidly converge to ranking function, which is as good as possible.",
            "Yeah, we can take it offline.",
            "OK, I think we are.",
            "So you know there is a breakdown.",
            "Young."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks very much.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This talk is joint work with Philip Radlinski and Torsten Walkins, both from Cornell University and the motivation for this work is.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Similar for the motivation of the paper that you song just presented, so I'm going to try to breeze through it a little bit rapidly.",
                    "label": 0
                },
                {
                    "sent": "For those who were here earlier.",
                    "label": 0
                },
                {
                    "sent": "Typically, in learning to rank problems, the method that's applied is too.",
                    "label": 1
                },
                {
                    "sent": "Learn some parameterized ranking function of a document and query a scoring function which is then applied to produce a ranking by simply extracting the K documents with the highest scores and the theoretical justification for this technique is sometimes called the probabilistic ranking principle.",
                    "label": 1
                },
                {
                    "sent": "It's the principle that documents presented in response to a query should be ranked in decreasing order of their probability of relevance to that.",
                    "label": 0
                },
                {
                    "sent": "Query as judged by a learns ranking function.",
                    "label": 0
                },
                {
                    "sent": "And as was pointed out in the preceding talk, this principle breaks down when you consider the diversity of users information needs and to avoid repeating the example of Jaguar and also because I'm giving a talk that has the word bandits in the title of it.",
                    "label": 0
                },
                {
                    "sent": "I'd like to point out a simple example of this.",
                    "label": 0
                },
                {
                    "sent": "So if a user is querying for bandits, we can't be sure if they are interested in information about type of online learning problem.",
                    "label": 0
                },
                {
                    "sent": "Featured for example in this in this talk.",
                    "label": 1
                },
                {
                    "sent": "Alternatively, they might be asking about the professional indoor lacrosse team from Buffalo, NY.",
                    "label": 1
                },
                {
                    "sent": "Or a film made by Barry Levinson in 2001, starring Bruce Willis, Billy Bob Thornton, and Cate Blanchett.",
                    "label": 0
                },
                {
                    "sent": "When I was preparing this talk, actually I struggled to remember if I even knew that there was a film starring those three actors, and.",
                    "label": 0
                },
                {
                    "sent": "If so, why had I never heard of it before?",
                    "label": 0
                },
                {
                    "sent": "Then I realized was made by Barry Levy.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Listen.",
                    "label": 0
                },
                {
                    "sent": "But let's say, so that's motivation.",
                    "label": 0
                },
                {
                    "sent": "Number one for our talk motivation.",
                    "label": 0
                },
                {
                    "sent": "#2.",
                    "label": 0
                },
                {
                    "sent": "Is this notion that to the extent possible, we should design algorithms for learning to rank that can be trained using implicit feedback from users.",
                    "label": 0
                },
                {
                    "sent": "In other words, trains using logs of actual usage data rather than manually labeled relevance judgments collected from experts.",
                    "label": 1
                },
                {
                    "sent": "There are many reasons for this usage.",
                    "label": 1
                },
                {
                    "sent": "Data is much more plentiful and can be obtained at nearly 0 cost.",
                    "label": 0
                },
                {
                    "sent": "As the set of documents available changes overtime or as users, information needs change overtime.",
                    "label": 0
                },
                {
                    "sent": "Techniques that rely on manually labeled relevance judgments constantly need retraining, whereas techniques based on usage data can adapt automatically.",
                    "label": 0
                },
                {
                    "sent": "Also, if you think about certain information retrieval applications such as.",
                    "label": 0
                },
                {
                    "sent": "Designing the search engine to run on you know the homepage of the Cornell University computer Science Department.",
                    "label": 0
                },
                {
                    "sent": "For example, Cornell is not going to employ an experts just to label training data for the ranking algorithm applied by the search algorithm on their home site.",
                    "label": 0
                },
                {
                    "sent": "So for all these reasons, is desirable.",
                    "label": 1
                },
                {
                    "sent": "To develop a ranking algorithms that can capitalize usage data from raw users.",
                    "label": 0
                },
                {
                    "sent": "Just based on their clicking behavior alone.",
                    "label": 0
                },
                {
                    "sent": "And insight that was part of the impetus for this paper.",
                    "label": 1
                },
                {
                    "sent": "Is that the problem of learning to rank from live usage data?",
                    "label": 0
                },
                {
                    "sent": "It can naturally be cast as an online learning problem.",
                    "label": 0
                },
                {
                    "sent": "In particular, it has that most familiar of aspects in online learning.",
                    "label": 1
                },
                {
                    "sent": "The tradeoff between exploration and exploitation, that is to say.",
                    "label": 0
                },
                {
                    "sent": "When deciding which documents to present to the current user in response to his or her query.",
                    "label": 1
                },
                {
                    "sent": "We have to consider two factors.",
                    "label": 0
                },
                {
                    "sent": "The past performance of the documents that we are considering presenting.",
                    "label": 0
                },
                {
                    "sent": "Which will determine their likely performance as relevant responses for this current user.",
                    "label": 0
                },
                {
                    "sent": "But also the utility of those documents in providing training data for future decisions.",
                    "label": 0
                },
                {
                    "sent": "These two goals naturally correspond to exploitation and exploration, and this is an insight which has guided some of the previous work in this area and also certainly guided our work.",
                    "label": 0
                },
                {
                    "sent": "So I've identified now two crucial aspects of the work in this paper.",
                    "label": 0
                },
                {
                    "sent": "One is the diversity of documents in a list of K return in response to a query.",
                    "label": 0
                },
                {
                    "sent": "And the other is using usage data rather than manually labeled examples.",
                    "label": 0
                },
                {
                    "sent": "And in surveying the.",
                    "label": 0
                },
                {
                    "sent": "If my.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pewter ever allows me to advance the slide.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in surveying the prior work.",
                    "label": 1
                },
                {
                    "sent": "I want to point out that if you drop either one of these features or both, there have been many papers in the past on any of those subsets of these features.",
                    "label": 0
                },
                {
                    "sent": "So certainly this large box.",
                    "label": 0
                },
                {
                    "sent": "Learning to rank using manually labeled training data and applying probabilistic ranking principle to rank by relevance.",
                    "label": 1
                },
                {
                    "sent": "There's so many papers that I'm not going to go into that literature here.",
                    "label": 0
                },
                {
                    "sent": "We've just seen a talk by you song.",
                    "label": 0
                },
                {
                    "sent": "That gave one algorithm that accounts for diversity within the framework of learning to rank, and it did it by training structural SVM's to maximize topic coverage.",
                    "label": 1
                },
                {
                    "sent": "There have been many other papers as well.",
                    "label": 0
                },
                {
                    "sent": "Maybe one of the most well known techniques is called maximum marginal relevance.",
                    "label": 0
                },
                {
                    "sent": "And it requires two inputs.",
                    "label": 0
                },
                {
                    "sent": "One is a learned ranking function that predicts the relevance of a document to a query and the other is a similarity or dissimilarity function that, given two documents, outputs the predicted amount of topic dissimilarity between them.",
                    "label": 0
                },
                {
                    "sent": "A list is then prepared by.",
                    "label": 0
                },
                {
                    "sent": "Greedily choosing each documents to maximize a weighted sum of these two objectives, relevance and dissimilarity from the preceding ones on the list.",
                    "label": 0
                },
                {
                    "sent": "And then finally, there's one quite relevant paper by Chen and Karger from 2006, which, however, differs from our model.",
                    "label": 1
                },
                {
                    "sent": "And assuming that the algorithm is given a model for estimating the probability of relevance of 1 document to a query given both the query and a list of other non relevant documents, so it assumes quite rich amount of prior knowledge on the part.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The algorithm.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we keep the other constraints in our problem and drop the first One South.",
                    "label": 0
                },
                {
                    "sent": "Think about algorithms that apply the probabilistic ranking principle, but try to do it in the framework of learning from actual usage data rather than manually labeled examples.",
                    "label": 0
                },
                {
                    "sent": "We then come to a bunch of papers that I've listed here.",
                    "label": 0
                },
                {
                    "sent": "Most of which rely on passive methods, that is they.",
                    "label": 1
                },
                {
                    "sent": "Retroactively look at click through logs to generate feature features, which are then used to train a learning to rank algorithm.",
                    "label": 1
                },
                {
                    "sent": "One.",
                    "label": 1
                },
                {
                    "sent": "Quite recent paper by Radlinski and you ACAMS is closer in spirit to our work.",
                    "label": 0
                },
                {
                    "sent": "It uses active methods that use learning theoretic ideas to choose.",
                    "label": 0
                },
                {
                    "sent": "The two documents to present 1st and 2nd in the current ranking.",
                    "label": 1
                },
                {
                    "sent": "Uh, based on the type of exploration, exploitation tradeoff that I was referring to two slides ago.",
                    "label": 0
                },
                {
                    "sent": "But again, since the goal of that work is really to eventually produce a list of the top K most relevant documents.",
                    "label": 0
                },
                {
                    "sent": "Rather than a list that jointly covers the greatest number of users information needs.",
                    "label": 0
                },
                {
                    "sent": "The objective of the work is quite different from ours.",
                    "label": 0
                },
                {
                    "sent": "OK, so accounting for both of these factors in the same paper, as far as I know this work that I'm presenting is the first to do it and I think I guess the final thing that I want to say before moving on to the technical part of the talk is that combining these two factors is not just a trivial matter of re combining ideas from the existing set of papers.",
                    "label": 1
                },
                {
                    "sent": "Probably the best way to see that is that, as I explained two slides ago, what we really have here is an online learning problem.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the set of strategies available to the decision maker in this learning problem.",
                    "label": 0
                },
                {
                    "sent": "If you're producing a ranking of length K from a set of end documents, then you have on the order of end of the case strategies and for realistic values of NK this number is just astronomically huge and so existing algorithms for that online learning problem would not scale adequately and we had to actually exploit the problem structure in a nontrivial way to design new online learning algorithms for this domain.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let me give you a very brief synopsis of our results at this point in the talk.",
                    "label": 0
                },
                {
                    "sent": "I haven't really defined our model, so I can't give you rigorous statements about theoretical results, but I'm going to present to you two algorithms for this problem, each of which achieves in the worst case at least 1 -- 1 / E times the optimum number of clicks.",
                    "label": 1
                },
                {
                    "sent": "Minus a little low of T term that can be thought of as the regret of the algorithm, although unlike the conventional meaning of regret, I'm comparing against 63% of the optimum instead of the optimum.",
                    "label": 0
                },
                {
                    "sent": "This comparison is justified by complexity theoretic lower bounds that says that no algorithm that uses polynomial computation time can do better than 63% of the optimum.",
                    "label": 0
                },
                {
                    "sent": "And then in some similar in some simulation results that I'll present at the end of the talk will see first of all that this number 63% is probably much more pessimistic than necessary in real data.",
                    "label": 0
                },
                {
                    "sent": "We'll also see confirmation that intentionally learning a diverse ranking outperforms simply ranking by popularity.",
                    "label": 0
                },
                {
                    "sent": "And finally, we'll see that third algorithm that uses the same ideas but is not.",
                    "label": 1
                },
                {
                    "sent": "You know, constrained by the need to satisfy a provable theorem about his performance can sort of achieve best possible performance over a large range of can.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Times.",
                    "label": 0
                },
                {
                    "sent": "So in the rest of the talk I'm going to review very briefly multi arm bandit algorithms because our problem can be thought of as a generalization thereof.",
                    "label": 0
                },
                {
                    "sent": "I'm then going to talk about that generalization, which we call ranked bandit algorithms present.",
                    "label": 1
                },
                {
                    "sent": "Our simulation results, and finally I'll talk about some.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Extensions and conclusions.",
                    "label": 0
                },
                {
                    "sent": "OK, so very briefly.",
                    "label": 0
                },
                {
                    "sent": "I use the term multi arm bandit problem to refer to any problem that has the following setup.",
                    "label": 0
                },
                {
                    "sent": "It is a problem in which a decision maker has a set of end strategies denoted by S. And an adversary chooses a sequence of payoff functions mapping S to a bounded interval, which I will always normalize to be the interval 01.",
                    "label": 0
                },
                {
                    "sent": "Actually, in our examples, S will just be the two elements at 01.",
                    "label": 0
                },
                {
                    "sent": "There are different kinds of adversaries.",
                    "label": 0
                },
                {
                    "sent": "One might consider an actually in this talk to different notions of adversary or important, and IID adversary is one that generates these payoff functions by sampling from a fixed distribution that does not vary overtime but is unknown to the algorithm designer.",
                    "label": 0
                },
                {
                    "sent": "Whereas an adaptive adversary is one that can choose absolutely any payoff function at time T based on the past history of interaction between the adversary and the algorithm, but not knowing the algorithms decision at time T. The way these two parties, algorithm and adversary interact is that in each time step, the algorithm picks one element of strategy set.",
                    "label": 0
                },
                {
                    "sent": "The adversary picks a payoff function.",
                    "label": 0
                },
                {
                    "sent": "That function is evaluated on the chosen strategy and the payoff of the strategy the algorithm picked is revealed.",
                    "label": 0
                },
                {
                    "sent": "And then the parameter by which we evaluate these algorithms is the difference.",
                    "label": 0
                },
                {
                    "sent": "An expected payoff between running the algorithm or using the best strategy.",
                    "label": 0
                },
                {
                    "sent": "In hindsight that's expressed in this equation that I've written across the bottom of the slide before I move on, I want to point out that in the ranking the document ranking examples that motivate this talk, the strategy sets is the set of all ordered K tuples of distinct documents.",
                    "label": 0
                },
                {
                    "sent": "These are all the rankings that you could display in response to users.",
                    "label": 0
                },
                {
                    "sent": "Mary and then the payoff function that we will be considering is simply one.",
                    "label": 0
                },
                {
                    "sent": "If user clicks on a document indicating that they find the relevant document in the List 0.",
                    "label": 0
                },
                {
                    "sent": "If the user abandons the list of results without clicking, indicating that no relevant document is found.",
                    "label": 0
                },
                {
                    "sent": "To illustrate these days.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Missions before I move on.",
                    "label": 0
                },
                {
                    "sent": "Here's a toy example in which we have a set of three strategies that are represented by slot machines, which also incidentally justifies the name multi armed bandit problems.",
                    "label": 0
                },
                {
                    "sent": "Since a slot machine is sometimes called A1 armed bandit.",
                    "label": 0
                },
                {
                    "sent": "So in this toy example, gambler is trying to decide which of these three slot machines to play every minute.",
                    "label": 0
                },
                {
                    "sent": "And it chose the leftmost one in.",
                    "label": 0
                },
                {
                    "sent": "One.",
                    "label": 0
                },
                {
                    "sent": "The middle in period two, and so on, receiving a total payoff that amounted to 2.2.",
                    "label": 0
                },
                {
                    "sent": "These numbers in white represent pay offs that the gambler would have received had it chosen an alternative slot machine.",
                    "label": 0
                },
                {
                    "sent": "From one it shows.",
                    "label": 0
                },
                {
                    "sent": "So the data which is revealed to the Gambler or the algorithm consists only of the orange numbers and not the white.",
                    "label": 0
                },
                {
                    "sent": "And regret is computed by comparing the algorithm's total payoff with the maximum column sum in this matrix, which happens to be 2.6.",
                    "label": 0
                },
                {
                    "sent": "So in this example, regret would be 0.4.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm now going to review for you some well known multi armed bandit algorithms.",
                    "label": 0
                },
                {
                    "sent": "I hesitate to say that the first one is well known.",
                    "label": 0
                },
                {
                    "sent": "I might perhaps describe it as universally known in the sense that if I asked all of you to close your eyes and not look at my slides and just think by pure reasoning, what's a good algorithm for the gambler to use in this situation?",
                    "label": 0
                },
                {
                    "sent": "This algorithm exploring commit is one that I predicts a majority of people in the room.",
                    "label": 0
                },
                {
                    "sent": "Would come up with just by pure thought.",
                    "label": 0
                },
                {
                    "sent": "The idea is simply this.",
                    "label": 0
                },
                {
                    "sent": "We have some preconfigured integer parameter S. The number of samples.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm begins by sampling every strategy S times.",
                    "label": 0
                },
                {
                    "sent": "It computes a sample average for each of them.",
                    "label": 0
                },
                {
                    "sent": "It picks out the one with the highest average payoff and forever after that it plays that strategy.",
                    "label": 0
                },
                {
                    "sent": "The extremely simple algorithm and against IID adversary this algorithm is justified by the fact that the sample average is computed at the beginning will be representative of average pay offs in the continuation of the game.",
                    "label": 0
                },
                {
                    "sent": "In fact, it satisfies regret bound that I've written there and whose salient feature is that if we optimize over the value of S to minimize the bound on the right hand side, we get something which is roughly.",
                    "label": 0
                },
                {
                    "sent": "Enter the 1/3 * 2 to the 2/3 so it's, uh.",
                    "label": 0
                },
                {
                    "sent": "In particular, the regret is governed by a power of T strictly less than one, so your average time converges.",
                    "label": 0
                },
                {
                    "sent": "We passed to 0 using this algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you want even faster convergence of regret, you can play this UCB one algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's based on computing for every strategy a confidence bound centered on the mean payoff musaab T of X.",
                    "label": 0
                },
                {
                    "sent": "And with with WT of X that's defined in such a way that the probability that the true mean payoff of a strategy ever violates its confidence interval.",
                    "label": 0
                },
                {
                    "sent": "Is extremely small, about 1 /, T ^2.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Having to find confidence intervals in that way, the algorithm always just picks the strategy whose upper confidence bound is as large as possible and this is a beautiful way of trading off exploration versus exploitation, because in general the strategies with large mean pay offs will have large upper confidence bounds and so exploitation is naturally built into the algorithm.",
                    "label": 0
                },
                {
                    "sent": "However, if there's something that you have not played sufficiently many times to be very sure of your estimate, then it's upper confidence bound will be inflated, so exploration is also built-in because we're adding in this WT term.",
                    "label": 0
                },
                {
                    "sent": "And OK, so the regret of this algorithm is bounded by about square root of NT.",
                    "label": 0
                },
                {
                    "sent": "That's an improvement on the previous one.",
                    "label": 0
                },
                {
                    "sent": "For large values of T. Moreover, for every instance of this problem, in other words, for every distribution defining an IID adversary, the regret actually scales logarithmically in T for some in.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Independent constancy.",
                    "label": 0
                },
                {
                    "sent": "I finally there's this more complicated algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's quite well known Exp 3.",
                    "label": 0
                },
                {
                    "sent": "Which is given a parameter gamma.",
                    "label": 0
                },
                {
                    "sent": "And which samples from a mixture of two distributions with probability 1 minus gamma at samples from an exponential family which assigns probability.",
                    "label": 0
                },
                {
                    "sent": "Ended this exponential in an unbiased estimate of its average payoff up to that time.",
                    "label": 1
                },
                {
                    "sent": "And otherwise samples from the uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "It then updates its estimates of average pay offs in a way that preserves the invariant that these estimates are unbiased estimators of the true total payoff.",
                    "label": 0
                },
                {
                    "sent": "So in summary, we have three algorithms with three different desirable features.",
                    "label": 1
                },
                {
                    "sent": "The exploring commit algorithm is desirable in its simplicity.",
                    "label": 0
                },
                {
                    "sent": "UCB one is desirable in its fast convergence and Exp.",
                    "label": 1
                },
                {
                    "sent": "Three is desirable in that it's robust against an adaptive adversary.",
                    "label": 0
                },
                {
                    "sent": "A stronger performance guarantee than either of the other algorithms I've presented.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me now talk about applying these algorithms to abandon minimization in learning to rank.",
                    "label": 0
                },
                {
                    "sent": "So our problem is going to be that of learning a diverse ranking for a single query.",
                    "label": 1
                },
                {
                    "sent": "This restriction to a single query is an important limitation of our work and subject of future ongoing work.",
                    "label": 0
                },
                {
                    "sent": "So our standing assumption is going to be that there's a set of documents and a set of users.",
                    "label": 1
                },
                {
                    "sent": "User I is for at this point in the talk, completely characterized by a set of documents, a subie that it finds relevant.",
                    "label": 0
                },
                {
                    "sent": "And we assume that the user behavior when presented with a list of K documents is to click on the 1st relevant document observed in that list, starting from number one and continuing sequentially down to number K. Later on, we'll generalize this user behavior model to allow probabilistic clicking, but for now a user is just a set of documents and deterministic behavior that clicks on the 1st relevant one.",
                    "label": 1
                },
                {
                    "sent": "As I said, payoff is the number of clicks and so maximizing payoff is equivalent to minimizing abandonment.",
                    "label": 0
                },
                {
                    "sent": "Minimizing the number of users who van.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From the site without clicking any results.",
                    "label": 0
                },
                {
                    "sent": "And if you think about the offline problem of abandonment minimization.",
                    "label": 1
                },
                {
                    "sent": "It's actually equivalent to the set coverage problem that you song talked about in his talk immediately preceding mine, so.",
                    "label": 1
                },
                {
                    "sent": "Supposing at times Zero that we knew every user's information need exactly.",
                    "label": 1
                },
                {
                    "sent": "We could compute for every document the set of users.",
                    "label": 1
                },
                {
                    "sent": "That that document satisfies.",
                    "label": 0
                },
                {
                    "sent": "And then the problem of learning an optimal list of K documents is simply learning K sets whose union has the maximum cardinality.",
                    "label": 1
                },
                {
                    "sent": "This is the NP hard maximum coverage problem, and as you song told us, the greedy algorithm, which always which starts from no sets at all and always picks one to maximize the additional number of users covered.",
                    "label": 1
                },
                {
                    "sent": "Is a 1 -- 1 / E approximation for this problem, and that's by the way the best approximation ratio achievable without violating complexity?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Erratic assumptions.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The online learning version of this problem then is 1, which is made difficult by the exponential size of the strategy set.",
                    "label": 0
                },
                {
                    "sent": "There is recent work on designing bandit algorithms for settings where we can only do approximate rather than exact optimization.",
                    "label": 0
                },
                {
                    "sent": "That work is inapplicable here because it assumes linear payoff functions and we are dealing with a submodular payoff.",
                    "label": 1
                },
                {
                    "sent": "However, there is a simple idea that can explain.",
                    "label": 1
                },
                {
                    "sent": "How we design algorithms for this setting?",
                    "label": 1
                },
                {
                    "sent": "And it is to model the operation of the online learning algorithm on it's offline counterpart, the greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "What that means is that we instantiate one learning algorithm, one multi armed bandit algorithm for every position in the ranking, and we trained that multi armed bandit algorithm in a way such that its objective is to maximize the marginal benefit that it provides.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me show you 2 examples of that in action, and then I'll come to some conclusions.",
                    "label": 0
                },
                {
                    "sent": "So we remember the exploring commit algorithm in the ranked exploring commit algorithm.",
                    "label": 0
                },
                {
                    "sent": "We go sequentially through every position of the ranking from one to K. Running a sequence of a fixed number of experiments to estimate the marginal benefit of presenting a document at that position in the ranking given committed.",
                    "label": 1
                },
                {
                    "sent": "Documents that we've already committed to at higher positions.",
                    "label": 0
                },
                {
                    "sent": "So in this way we emulate the offline greedy algorithm and achieve.",
                    "label": 0
                },
                {
                    "sent": "I don't even know if you can see this from your seats, but you know 1 -- 1 three minus epsilon times the optimum minus a sublinear regret term.",
                    "label": 1
                },
                {
                    "sent": "So arbitrarily close to this one 1 -- 1 over rebound.",
                    "label": 0
                },
                {
                    "sent": "Assuming independent idente.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distributed users.",
                    "label": 0
                },
                {
                    "sent": "We can get rid of that assumption by using a more sophisticated algorithm.",
                    "label": 0
                },
                {
                    "sent": "It again uses this idea of a sequence of bandit algorithms.",
                    "label": 0
                },
                {
                    "sent": "One position, one for each position in the ranking.",
                    "label": 0
                },
                {
                    "sent": "And it's not really too important.",
                    "label": 0
                },
                {
                    "sent": "Which multi armed bandit algorithm you use there Exp 3 for example is fine.",
                    "label": 1
                },
                {
                    "sent": "The feedback that we give to each of these multi unbanded algorithms.",
                    "label": 0
                },
                {
                    "sent": "Is simply the marginal contribution that it makes to the combined payoff of the ranking algorithm.",
                    "label": 1
                },
                {
                    "sent": "In other words, we give it a feedback of 1 if the user clicked the document that was chosen by the algorithm at position I in the ranking and that document was not displayed at any higher position.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one also satisfies a guarantee which is 1 -- 1 / 3 times the optimum minus a sublinear regret term.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "There's an extension too.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two probabilistic users that I won't talk about.",
                    "label": 0
                },
                {
                    "sent": "There's some simulation results that I already previewed for you earlier in the talk, so.",
                    "label": 0
                },
                {
                    "sent": "The ranked exploring commit algorithm actually comes quite close to achieving the best possible performance, even though the theorem only guarantees 63% of the best possible.",
                    "label": 0
                },
                {
                    "sent": "It also substantially improves on just ranking by probability of relevance, according to the probabilistic ranking.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ansible.",
                    "label": 0
                },
                {
                    "sent": "Moreover, if you modify the rank bandits algorithm by using UC1 instead of Exp 3 because of its more rapid convergence properties, you actually lose our theoretical guarantees, but you get an algorithm which performs near the best of all algorithms we tested over an entire range of number of trials from about 10 ^3 up to 10 to the 6th, so bye.",
                    "label": 0
                },
                {
                    "sent": "Taking the way you might by taking the spirit of these theoretical algorithms without following the letter of the law in the theorems, you can actually get an algorithm which.",
                    "label": 0
                },
                {
                    "sent": "Has extremely fast convergence in our simulations and performs best of all the ones that we.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tried.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion, what this paper is shown is that the problem of abandonment is a shanan.",
                    "label": 1
                },
                {
                    "sent": "Learning diverse rankings can be formulated in the framework of online learning that we can exploit known online learning algorithms in this ranked fashion to generate algorithms whose worst case bounds are the best possible in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, that these algorithms with theoretical guarantees can be tweaked to perform very well in our simulations, and as I pre visaged earlier, the main open problem suggested by this work is how to learn across multiple queries and not just for a single query using some of the same ideas.",
                    "label": 1
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So can you comment on the what would happen if the clicks are unreliable and particular document may be relevant, but the user sees everything they need from the snippet for example.",
                    "label": 0
                },
                {
                    "sent": "So they don't get everything they need from the snippet, and they don't click on it, right?",
                    "label": 0
                },
                {
                    "sent": "OK. Well, that example is particularly troubling.",
                    "label": 0
                },
                {
                    "sent": "One.",
                    "label": 0
                },
                {
                    "sent": "OK, here is exactly the level of generalization that we can deal with.",
                    "label": 0
                },
                {
                    "sent": "We can deal with users who have noisy clicking behavior in the sense that for every document they have some probability of clicking it, which is related to but not equal to its relevance.",
                    "label": 0
                },
                {
                    "sent": "And they scan down the list, clicking documents according to those probabilities.",
                    "label": 0
                },
                {
                    "sent": "Now if a document has a very informative snippet, then it's quite reasonable to model it as a document with low probability of being clicked.",
                    "label": 0
                },
                {
                    "sent": "Given it appears on the list and the trouble of course, is that our evaluation measure is the number of.",
                    "label": 0
                },
                {
                    "sent": "So the objective that the algorithm is trying to optimize is the number of clicks.",
                    "label": 0
                },
                {
                    "sent": "So that's an objective that's antithetical to presenting relevant documents.",
                    "label": 0
                },
                {
                    "sent": "In the case that you described, where there's a document whose relevance actually reduces the number of clicks.",
                    "label": 0
                },
                {
                    "sent": "You talk about this is inevitable to have this 30% loss due to the requirement of polynomial time algorithm.",
                    "label": 0
                },
                {
                    "sent": "But there are actually two independent questions here.",
                    "label": 0
                },
                {
                    "sent": "The information theoretic question and complexity theoretic question.",
                    "label": 0
                },
                {
                    "sent": "If all I care about is from the information theoretic point of view.",
                    "label": 0
                },
                {
                    "sent": "Can you get better approximations if you ignore the polynomial time restriction?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Information theoretically.",
                    "label": 0
                },
                {
                    "sent": "Certainly, if you're allowed to run for an exponential number of trials, then you can simply.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I know.",
                    "label": 0
                },
                {
                    "sent": "OK so in with exponential computation time in polynomial convergence time.",
                    "label": 0
                },
                {
                    "sent": "I believe the answer is no, it's impossible to.",
                    "label": 0
                },
                {
                    "sent": "Improve on this 1 -- 1 over the regret, but that answer has a bit of guesswork in it and so it's really something we should discuss offline and maybe I'll be able to give you a rigorous argument.",
                    "label": 0
                },
                {
                    "sent": "I missed the where you specified which problem you were talking about.",
                    "label": 0
                },
                {
                    "sent": "The main open problem.",
                    "label": 0
                },
                {
                    "sent": "Yes, generalizing across multiple queries.",
                    "label": 0
                },
                {
                    "sent": "OK, I OK.",
                    "label": 0
                },
                {
                    "sent": "So I think the difficulty is this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Good in the course of answering these queries.",
                    "label": 0
                },
                {
                    "sent": "You want to generate training data, which is as likely as possible to be applicable to improving rankings for a broad broad distribution, of course so.",
                    "label": 0
                },
                {
                    "sent": "If you have the active learning problem where I could actually choose a query and a list of documents myself presented to a user and ask them what they think.",
                    "label": 0
                },
                {
                    "sent": "That would be significantly easier than this problem, in which a sequence of queries is generated outside of the algorithms control, and it has to then decide how best to generate lists of responses to rapidly converge to ranking function, which is as good as possible.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we can take it offline.",
                    "label": 0
                },
                {
                    "sent": "OK, I think we are.",
                    "label": 0
                },
                {
                    "sent": "So you know there is a breakdown.",
                    "label": 0
                },
                {
                    "sent": "Young.",
                    "label": 0
                }
            ]
        }
    }
}