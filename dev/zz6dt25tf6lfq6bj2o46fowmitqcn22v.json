{
    "id": "zz6dt25tf6lfq6bj2o46fowmitqcn22v",
    "title": "Online Variational Inference for the Hierarchical Dirichlet Process",
    "info": {
        "author": [
            "Chong Wang, Computer Science Department, Princeton University"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/aistats2011_wang_online/",
    "segmentation": [
        [
            "So thanks John and Frank for giving a very good interest."
        ],
        [
            "HTP and mixed membership models.",
            "In general.",
            "That actually makes my life much easier, so HTP it can be used like mixed membership model for like data like documents and images.",
            "Documents can have like multiple topics, and images could have like multiple objects.",
            "So one nice property of HTTPS like you can infer the number of components from the data using posterior inference, like you don't need specified, specify them in advance or the bad news.",
            "Here is like the poster inference for the HP in general.",
            "Is intractable, so there were like 2 popular approximation methods.",
            "One, it's keep sampling and the other like it's very slow inference.",
            "This actually batch algorithms, which means they require multiple passes through the data, and this is actually not very good for like a massive scale applications or like streaming data where you can't go back."
        ],
        [
            "So in this paper we are present on GNU Online.",
            "Various inference algorithm actually, which actually lets us to infer the posterior of HTP for like large scale application and streaming data.",
            "So here's the outline.",
            "So first I will describe a new batch variation inference, HP with simple and clothes from updates.",
            "And given this I will describe the on line various inference algorithm.",
            "Finally, I will talk about the."
        ],
        [
            "Garments.",
            "So OK, so why do we need a new batch of various on inference algorithm?",
            "So you know when you do online inference you need another like in general need a batch algorithms start with.",
            "However the existing various inference algorithm for HTP.",
            "Either they require complicated approximations or like numerical optimizations.",
            "And this is not very good for like large scale applications where sub quite important.",
            "So then we sort of need uh-huh need lichen you variously inference algorithm that actually allow.",
            "Important quotes from updates."
        ],
        [
            "And then, given this, we can use forecast natural gradient and then this is similar to the online very similar DNA that was by Hoffman and Blood.",
            "And next I will review the HTP for like a document modeling, so we call like HTP topping model and then describe the new algorithm.",
            "So this is the HP talk model.",
            "So here we focus on A2 level HTP so you know you have a corpus and you have a first.",
            "I would like Corpse IDP for everything and then given that for each document that we have a document specific DP, which DJ here is drawn from Fanon and Jean on you see that gene on is shared across in there across all documents.",
            "And for each word in that specific document, you first draw a topic Theta and then show the word X.",
            "However, discrimination or is quite simple, but it's implicit, so it's really hard to work with for like to develop like various various inference algorithm, so we usually."
        ],
        [
            "Use like breaking construction instead.",
            "So here's the ceramic stick breaking for the DP.",
            "This is an explicit representation for the course level DP gene on where we draw from a DP with parameter gamma and based distribution H. This is like this.",
            "If you stick breaking construction, you have a countable infinite number of items, fique items or topics part of the same.",
            "Here in this talk.",
            "This if you want featured 3 and then go on an then each item has to wait.",
            "Wait, I can't wait here a bit.",
            "One bit about 3:00 this sum to one and beta is drawn from a GM distribution which is like this.",
            "You know GM here and it stands for like 3 names but I guess none of them are here.",
            "So thank you that makes my life complicated.",
            "But I think I should say easy, OK?",
            "So then you can do none.",
            "Actually can be seen as a collection of items with different weights on each.",
            "Each of the item items.",
            "Then we can, you know, draw a sample from gene as well.",
            "So like say you draw aside from Gene on this cycle is pretty simple.",
            "It's first draw.",
            "Any indicator variable C from multinomial distribution with parameter beta.",
            "And then just simply let this biscus.",
            "This slide side to be the specific item that chosen by C."
        ],
        [
            "OK, then we can further generalize this Roman stick breaking for the HTP.",
            "Now, because you know, for document level DJ, it's also a deep drawn from.",
            "AP difference, like you know, the base distribution is different, so here this is the one that's just to show show you like the corpus.",
            "I will DP top level DP with items V12 feet Infinity and beta is the weight.",
            "And for document level DP, here is just one document where the item is drawn from gene and you see the difference like you know you could get duplicate items because say this one.",
            "I artificially put like the one here and this could be 3 and it could be fee one.",
            "The good thing here is like the sorry.",
            "They stick breaking weights, Pi.",
            "Actually it's independent of data.",
            "This is different from the original HTP construction where beta is depending on beta.",
            "Of course they do.",
            "Pais depending on data.",
            "Which actually that is that was."
        ],
        [
            "A code for developing like a good like simple variously inference algorithm and hear the Arctic upload.",
            "And it's actually very simple for us to develop various mediums algorithm.",
            "Now we're ready to develop a GNU Bash version for algorithm here.",
            "So like this, the hidden variables like they have different colors.",
            "For Gene and the corpse with DP, you know it's there's only one of them.",
            "An this fee and beta.",
            "You don't even use.",
            "Remember this this like these notations, but you know this idea with Gina is the course level DP.",
            "It's out there and then we have a.",
            "We use Payton and fee to represent that and this is the course level there is no like hidden sorry the hidden variables and for each document we have also have a GJ the document with DP.",
            "And they are represented by this Pi and see and they are the corresponding document level, like hidden variables for each word have a corresponding.",
            "You know topic indicators Z.",
            "These are the whatever."
        ],
        [
            "Invariables then we use our very simple, you know, fully factorized variational distribution here to approximate the full posterior and the three blocks correspond to different like level of, like a very small distributions I like is written, But yeah, so here within we just minimize the KL divergent of the Q&A P where P is the true posterior.",
            "It's out there.",
            "But you can see that.",
            "So this actually ends up with accordance.",
            "Rhythm with where you know which optimize the lower bound of the marginal likely observed data.",
            "And because remember that we have like infinite number of items in HDP but life is sore so we sort of use like truncations here.",
            "We apply both truncation to the corpse that will.",
            "Like a DP and document with DP, I got little bit why we use different like truncations later.",
            "Accident this new version for inference algorithm actually gives us like a hour updates.",
            "I encourage form and this is a very important to developing the online algorithm later on.",
            "OK, so this is like the batch of."
        ],
        [
            "The inference algorithm.",
            "This is not real algorithm.",
            "You know you can't implement based on this.",
            "So say you have like a stopping criterion and then you re set out a sufficient statistic.",
            "Then for each document you update the document level and each word level parameters.",
            "Any update sufficient statistic when you go throughout documents in the corpus, then you sort of like an update.",
            "The corporate level variant parameters using the sufficient statistic from our documents you have the past.",
            "Clearly this, like you know Batch algorithm which require you know you need in market path through data and this is like you know, speaking asset like it's not very limited for larger applications or streaming data.",
            "So I will throw something here is like using the stochastic natural gradient.",
            "And this new batch inputs algorithm actually."
        ],
        [
            "Give us a very simple online updates as well.",
            "So this is the general online various inference algorithm.",
            "What we do here is like you know, we also have some stopping criterion.",
            "Like say you run you want to run for like 10 days, 20 hours and what we do here we can fetch a random document.",
            "And then update.",
            "It's like a document level and it's worth level bearing the parameters.",
            "And given this like a single document, we also updated a couple problems level parameters.",
            "Using this only.",
            "The only from this document.",
            "So I have two points here.",
            "So first like updating the document level and the word level parameters stay the same as before and we chose, we choose to update the quotes that will parameters using Stochastic natural region.",
            "So here is like a very, very brief instruction to stochastic gradient."
        ],
        [
            "So sarcastic gradient is 1 type of stochastic optimization method, especially efficient for logical applications.",
            "You know stochastic here we can do it like we iteratively take a random subset of data to update the model parameters which back to this, only this through this subset.",
            "Then the Greenway using Raphling is a noisy estimate based on only this subscriber data.",
            "Then we need appropriate learning like decay, decaying learning rate to guarantee the convergence.",
            "Finally, we apply this to category idea to the various not objectively HTP."
        ],
        [
            "So here a little math you know, 80 being positive definite matrix where you can think of like identity matrix like.",
            "Roti as a learning rate thing about like 1 / T. It's, you know like that.",
            "In a batch of variance inference, if we optimize the lower bound using gradient descent, we are so like ending up like this.",
            "So laminates the parameter of the model.",
            "Then we take the gradient of that like this.",
            "Then we updated the parameters using like written assent.",
            "This very very general like equation for online inference.",
            "Various inference we first notice that we can write the lower bound L. As the contribution from our documents like like that like this DLJ is kind of like you treat each document as the entire corpus and then every every over our documents.",
            "This forecast gradient we sort of like using a noisy estimate only from this single document.",
            "And even replace this one as this.",
            "Pretty similar thing, but they're different.",
            "You end up with the stochastic gradient.",
            "Finally, let's make it a little bit complicated.",
            "If you use 80.",
            "As the inverse of remoni metric, then we obtain the stochastic natural gradient in June."
        ],
        [
            "So this natural region is very nasty, but here it's reasonable.",
            "I still that later prison, so it would say we take the topic distribution parameter Lambda for an example.",
            "So let's SJB the statistic from document.",
            "Then you can think about like a exciting word counts.",
            "If I do like a batch inference through coding the scent.",
            "Like the new batch inference algorithm we just developed, then we can sell this gradient zero and this guy was the update.",
            "Like this.",
            "You know, either it's like some sort like a prior parameter to smooth things and then you see that like we need sort like a need out of statistic from out of the disk from our documents with some up an updated this like a topic distribution parameter, Lambda.",
            "In online inference using natural gradient which is enabled by this knew back inverse algorithm has a similar form is like that I should say like in general natural is really complicated.",
            "You see that like this, the learning rate, and actually this if you look at this toolbox here, they almost very similar, like the only difference is like you know this only.",
            "Depending on like.",
            "On one document, but you treated at the entire corpus the.",
            "But here you sort like a summer altogether.",
            "In general we usually don't use like this.",
            "One document is kind of like unstable, so we used like multiple samples like each time."
        ],
        [
            "Then here, OK here is the final online variational inference algorithm that we have in the paper.",
            "So what we do here is like we have some data and have some learning rate, roti and then we have like a mini batch size S and we do.",
            "Here is like we have some stopping criterion out there and then we fetch like S document maybe like say 200 and then we update the various parameters for every document in that specific set and we update next we update the corpus level parameters you gave using this location.",
            "Natural gradient it says show you.",
            "But this time for like using S document, this really is similar.",
            "I start, it's like you know.",
            "So like a reweighting this like like that.",
            "Like this you have as like I stuck him in here but you sort collect the information from only this as documents.",
            "OK, so here's the one thing I should say.",
            "Like if you set roti to be one and as to be D, which means like you, you use constant learning rate as one an.",
            "But each time you sample our documents this type is the best inference algorithm which is no.",
            "Boy"
        ],
        [
            "So this I connected connection of that.",
            "So here comes the experiment.",
            "So we have two corpora.",
            "One is nature with 300 and five 350,000 thousand documents, and the range from like their range from like 1860, nine 2008 and the other data set PNS is little smaller.",
            "It has like 80,000 documents.",
            "So we use a pretty similar predictive likelihood measure, which is similar to also similar jumps, but he exponentiated I didn't.",
            "OK, so we split the test data into two half and.",
            "We predict second half, you know, using using the first half and the training, the training that data we compare with the on line version LDA and the batch variance.",
            "That's very very sleepy for running for six hours."
        ],
        [
            "So here's 1 results on nature.",
            "So we set the crops that will transition K to be 150 and the document level technician to be 5015.",
            "Sorry.",
            "We we we we we did because we did this because we believe that you know Document label transition usually is much smaller than the Corporal punishment because you know a single document usually does not have that many topics in it.",
            "In ink in general.",
            "And in this batch, HDP is only run on like a subset of data because it's really slow and we I think I remember it's like 20 K for this batch HDP.",
            "So the X axis the time this takes hours and this is the predicted log likelihood.",
            "The higher the better.",
            "You know this red curve is online HTP.",
            "It's sort of like a does much better than all the other like methods here.",
            "And it was nice thing I notice like you know it's online version.",
            "HTTP use actually use less topic like fewer topics then then transition level.",
            "So here we we saw like a mini member.",
            "We have a two parameter to set like learning rate and the medium size is.",
            "We sort of very very them and see how they perform compared with our online RDA.",
            "We sort of have this similar thing like a train across different settings.",
            "In most cases the energy that's better than online RDA."
        ],
        [
            "So here is not a result on PNS like this is like a auto setting is almost in almost the same as the nature experiment.",
            "The only exception is like the.",
            "The Bachelors degree is trained on the whole data set.",
            "It's really smaller because.",
            "So here we.",
            "The X axis.",
            "Let's take the time and the access to the predict log likelihood.",
            "So we see that like this online, HDP is little worse than the batch HDP, but it's running much faster than that.",
            "So here's a stimulus at a similar set of results.",
            "Like we change different like learning rate and mini batch sizes, and we sort of find similar things.",
            "So finally that we have here is like we simulate the streaming data on the net."
        ],
        [
            "Corpus imagine like you know nature was like born like a you know around like 1869 and you know.",
            "So we have a HTTP model like in 1869.",
            "Some articles were published in Nature, then we just fed also those article to the HD model and then later on on and then we can run this edgy in order for 200 years."
        ],
        [
            "But in order to avoid doing doing only this thing for my life, I run it for a couple hours.",
            "So sorry.",
            "OK, so here are two topics that I have here.",
            "This topic is about like if you can read this.",
            "I'm happy if you count like can tell you the first topics like astronomy, astronomy research on stars.",
            "They are like they were happening pretty early in Natural History so could see a big jump here.",
            "And this topic was actually making sense.",
            "You can see them.",
            "So this topic is about the neuroscience research on rats.",
            "So apparently this topic.",
            "Didn't make sense until like around somewhere here and you can see.",
            "Also see that from these words.",
            "Yeah, I think that's.",
            "End with."
        ],
        [
            "Meant.",
            "In summary, we have developed a new batch of various inference algorithm for HTP and also given this we present and you online inference algorithm for HP as well.",
            "Empirical demonstrate.",
            "This performance on logical applications.",
            "So for the future work we want to generalize this idea to other Benson nonparametric parametric models and also in order to be a true Blue Basin parametric, we want to.",
            "We don't want to like set functions.",
            "Instead we can try to grow the translations on the fly."
        ],
        [
            "That's it, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks John and Frank for giving a very good interest.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "HTP and mixed membership models.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "That actually makes my life much easier, so HTP it can be used like mixed membership model for like data like documents and images.",
                    "label": 1
                },
                {
                    "sent": "Documents can have like multiple topics, and images could have like multiple objects.",
                    "label": 1
                },
                {
                    "sent": "So one nice property of HTTPS like you can infer the number of components from the data using posterior inference, like you don't need specified, specify them in advance or the bad news.",
                    "label": 1
                },
                {
                    "sent": "Here is like the poster inference for the HP in general.",
                    "label": 0
                },
                {
                    "sent": "Is intractable, so there were like 2 popular approximation methods.",
                    "label": 1
                },
                {
                    "sent": "One, it's keep sampling and the other like it's very slow inference.",
                    "label": 0
                },
                {
                    "sent": "This actually batch algorithms, which means they require multiple passes through the data, and this is actually not very good for like a massive scale applications or like streaming data where you can't go back.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this paper we are present on GNU Online.",
                    "label": 0
                },
                {
                    "sent": "Various inference algorithm actually, which actually lets us to infer the posterior of HTP for like large scale application and streaming data.",
                    "label": 0
                },
                {
                    "sent": "So here's the outline.",
                    "label": 0
                },
                {
                    "sent": "So first I will describe a new batch variation inference, HP with simple and clothes from updates.",
                    "label": 1
                },
                {
                    "sent": "And given this I will describe the on line various inference algorithm.",
                    "label": 0
                },
                {
                    "sent": "Finally, I will talk about the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Garments.",
                    "label": 0
                },
                {
                    "sent": "So OK, so why do we need a new batch of various on inference algorithm?",
                    "label": 1
                },
                {
                    "sent": "So you know when you do online inference you need another like in general need a batch algorithms start with.",
                    "label": 0
                },
                {
                    "sent": "However the existing various inference algorithm for HTP.",
                    "label": 1
                },
                {
                    "sent": "Either they require complicated approximations or like numerical optimizations.",
                    "label": 1
                },
                {
                    "sent": "And this is not very good for like large scale applications where sub quite important.",
                    "label": 0
                },
                {
                    "sent": "So then we sort of need uh-huh need lichen you variously inference algorithm that actually allow.",
                    "label": 0
                },
                {
                    "sent": "Important quotes from updates.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then, given this, we can use forecast natural gradient and then this is similar to the online very similar DNA that was by Hoffman and Blood.",
                    "label": 0
                },
                {
                    "sent": "And next I will review the HTP for like a document modeling, so we call like HTP topping model and then describe the new algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is the HP talk model.",
                    "label": 0
                },
                {
                    "sent": "So here we focus on A2 level HTP so you know you have a corpus and you have a first.",
                    "label": 1
                },
                {
                    "sent": "I would like Corpse IDP for everything and then given that for each document that we have a document specific DP, which DJ here is drawn from Fanon and Jean on you see that gene on is shared across in there across all documents.",
                    "label": 0
                },
                {
                    "sent": "And for each word in that specific document, you first draw a topic Theta and then show the word X.",
                    "label": 0
                },
                {
                    "sent": "However, discrimination or is quite simple, but it's implicit, so it's really hard to work with for like to develop like various various inference algorithm, so we usually.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use like breaking construction instead.",
                    "label": 0
                },
                {
                    "sent": "So here's the ceramic stick breaking for the DP.",
                    "label": 1
                },
                {
                    "sent": "This is an explicit representation for the course level DP gene on where we draw from a DP with parameter gamma and based distribution H. This is like this.",
                    "label": 1
                },
                {
                    "sent": "If you stick breaking construction, you have a countable infinite number of items, fique items or topics part of the same.",
                    "label": 0
                },
                {
                    "sent": "Here in this talk.",
                    "label": 0
                },
                {
                    "sent": "This if you want featured 3 and then go on an then each item has to wait.",
                    "label": 0
                },
                {
                    "sent": "Wait, I can't wait here a bit.",
                    "label": 0
                },
                {
                    "sent": "One bit about 3:00 this sum to one and beta is drawn from a GM distribution which is like this.",
                    "label": 0
                },
                {
                    "sent": "You know GM here and it stands for like 3 names but I guess none of them are here.",
                    "label": 0
                },
                {
                    "sent": "So thank you that makes my life complicated.",
                    "label": 0
                },
                {
                    "sent": "But I think I should say easy, OK?",
                    "label": 0
                },
                {
                    "sent": "So then you can do none.",
                    "label": 0
                },
                {
                    "sent": "Actually can be seen as a collection of items with different weights on each.",
                    "label": 0
                },
                {
                    "sent": "Each of the item items.",
                    "label": 0
                },
                {
                    "sent": "Then we can, you know, draw a sample from gene as well.",
                    "label": 0
                },
                {
                    "sent": "So like say you draw aside from Gene on this cycle is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "It's first draw.",
                    "label": 0
                },
                {
                    "sent": "Any indicator variable C from multinomial distribution with parameter beta.",
                    "label": 0
                },
                {
                    "sent": "And then just simply let this biscus.",
                    "label": 0
                },
                {
                    "sent": "This slide side to be the specific item that chosen by C.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, then we can further generalize this Roman stick breaking for the HTP.",
                    "label": 1
                },
                {
                    "sent": "Now, because you know, for document level DJ, it's also a deep drawn from.",
                    "label": 0
                },
                {
                    "sent": "AP difference, like you know, the base distribution is different, so here this is the one that's just to show show you like the corpus.",
                    "label": 0
                },
                {
                    "sent": "I will DP top level DP with items V12 feet Infinity and beta is the weight.",
                    "label": 0
                },
                {
                    "sent": "And for document level DP, here is just one document where the item is drawn from gene and you see the difference like you know you could get duplicate items because say this one.",
                    "label": 0
                },
                {
                    "sent": "I artificially put like the one here and this could be 3 and it could be fee one.",
                    "label": 0
                },
                {
                    "sent": "The good thing here is like the sorry.",
                    "label": 1
                },
                {
                    "sent": "They stick breaking weights, Pi.",
                    "label": 0
                },
                {
                    "sent": "Actually it's independent of data.",
                    "label": 0
                },
                {
                    "sent": "This is different from the original HTP construction where beta is depending on beta.",
                    "label": 0
                },
                {
                    "sent": "Of course they do.",
                    "label": 0
                },
                {
                    "sent": "Pais depending on data.",
                    "label": 0
                },
                {
                    "sent": "Which actually that is that was.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A code for developing like a good like simple variously inference algorithm and hear the Arctic upload.",
                    "label": 1
                },
                {
                    "sent": "And it's actually very simple for us to develop various mediums algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now we're ready to develop a GNU Bash version for algorithm here.",
                    "label": 0
                },
                {
                    "sent": "So like this, the hidden variables like they have different colors.",
                    "label": 0
                },
                {
                    "sent": "For Gene and the corpse with DP, you know it's there's only one of them.",
                    "label": 0
                },
                {
                    "sent": "An this fee and beta.",
                    "label": 0
                },
                {
                    "sent": "You don't even use.",
                    "label": 0
                },
                {
                    "sent": "Remember this this like these notations, but you know this idea with Gina is the course level DP.",
                    "label": 0
                },
                {
                    "sent": "It's out there and then we have a.",
                    "label": 0
                },
                {
                    "sent": "We use Payton and fee to represent that and this is the course level there is no like hidden sorry the hidden variables and for each document we have also have a GJ the document with DP.",
                    "label": 1
                },
                {
                    "sent": "And they are represented by this Pi and see and they are the corresponding document level, like hidden variables for each word have a corresponding.",
                    "label": 0
                },
                {
                    "sent": "You know topic indicators Z.",
                    "label": 1
                },
                {
                    "sent": "These are the whatever.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Invariables then we use our very simple, you know, fully factorized variational distribution here to approximate the full posterior and the three blocks correspond to different like level of, like a very small distributions I like is written, But yeah, so here within we just minimize the KL divergent of the Q&A P where P is the true posterior.",
                    "label": 1
                },
                {
                    "sent": "It's out there.",
                    "label": 0
                },
                {
                    "sent": "But you can see that.",
                    "label": 0
                },
                {
                    "sent": "So this actually ends up with accordance.",
                    "label": 1
                },
                {
                    "sent": "Rhythm with where you know which optimize the lower bound of the marginal likely observed data.",
                    "label": 0
                },
                {
                    "sent": "And because remember that we have like infinite number of items in HDP but life is sore so we sort of use like truncations here.",
                    "label": 0
                },
                {
                    "sent": "We apply both truncation to the corpse that will.",
                    "label": 0
                },
                {
                    "sent": "Like a DP and document with DP, I got little bit why we use different like truncations later.",
                    "label": 0
                },
                {
                    "sent": "Accident this new version for inference algorithm actually gives us like a hour updates.",
                    "label": 0
                },
                {
                    "sent": "I encourage form and this is a very important to developing the online algorithm later on.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is like the batch of.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The inference algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is not real algorithm.",
                    "label": 1
                },
                {
                    "sent": "You know you can't implement based on this.",
                    "label": 1
                },
                {
                    "sent": "So say you have like a stopping criterion and then you re set out a sufficient statistic.",
                    "label": 0
                },
                {
                    "sent": "Then for each document you update the document level and each word level parameters.",
                    "label": 0
                },
                {
                    "sent": "Any update sufficient statistic when you go throughout documents in the corpus, then you sort of like an update.",
                    "label": 1
                },
                {
                    "sent": "The corporate level variant parameters using the sufficient statistic from our documents you have the past.",
                    "label": 0
                },
                {
                    "sent": "Clearly this, like you know Batch algorithm which require you know you need in market path through data and this is like you know, speaking asset like it's not very limited for larger applications or streaming data.",
                    "label": 0
                },
                {
                    "sent": "So I will throw something here is like using the stochastic natural gradient.",
                    "label": 1
                },
                {
                    "sent": "And this new batch inputs algorithm actually.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give us a very simple online updates as well.",
                    "label": 0
                },
                {
                    "sent": "So this is the general online various inference algorithm.",
                    "label": 1
                },
                {
                    "sent": "What we do here is like you know, we also have some stopping criterion.",
                    "label": 0
                },
                {
                    "sent": "Like say you run you want to run for like 10 days, 20 hours and what we do here we can fetch a random document.",
                    "label": 1
                },
                {
                    "sent": "And then update.",
                    "label": 0
                },
                {
                    "sent": "It's like a document level and it's worth level bearing the parameters.",
                    "label": 0
                },
                {
                    "sent": "And given this like a single document, we also updated a couple problems level parameters.",
                    "label": 0
                },
                {
                    "sent": "Using this only.",
                    "label": 1
                },
                {
                    "sent": "The only from this document.",
                    "label": 1
                },
                {
                    "sent": "So I have two points here.",
                    "label": 0
                },
                {
                    "sent": "So first like updating the document level and the word level parameters stay the same as before and we chose, we choose to update the quotes that will parameters using Stochastic natural region.",
                    "label": 0
                },
                {
                    "sent": "So here is like a very, very brief instruction to stochastic gradient.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So sarcastic gradient is 1 type of stochastic optimization method, especially efficient for logical applications.",
                    "label": 1
                },
                {
                    "sent": "You know stochastic here we can do it like we iteratively take a random subset of data to update the model parameters which back to this, only this through this subset.",
                    "label": 1
                },
                {
                    "sent": "Then the Greenway using Raphling is a noisy estimate based on only this subscriber data.",
                    "label": 0
                },
                {
                    "sent": "Then we need appropriate learning like decay, decaying learning rate to guarantee the convergence.",
                    "label": 0
                },
                {
                    "sent": "Finally, we apply this to category idea to the various not objectively HTP.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here a little math you know, 80 being positive definite matrix where you can think of like identity matrix like.",
                    "label": 0
                },
                {
                    "sent": "Roti as a learning rate thing about like 1 / T. It's, you know like that.",
                    "label": 0
                },
                {
                    "sent": "In a batch of variance inference, if we optimize the lower bound using gradient descent, we are so like ending up like this.",
                    "label": 0
                },
                {
                    "sent": "So laminates the parameter of the model.",
                    "label": 0
                },
                {
                    "sent": "Then we take the gradient of that like this.",
                    "label": 0
                },
                {
                    "sent": "Then we updated the parameters using like written assent.",
                    "label": 0
                },
                {
                    "sent": "This very very general like equation for online inference.",
                    "label": 0
                },
                {
                    "sent": "Various inference we first notice that we can write the lower bound L. As the contribution from our documents like like that like this DLJ is kind of like you treat each document as the entire corpus and then every every over our documents.",
                    "label": 1
                },
                {
                    "sent": "This forecast gradient we sort of like using a noisy estimate only from this single document.",
                    "label": 0
                },
                {
                    "sent": "And even replace this one as this.",
                    "label": 0
                },
                {
                    "sent": "Pretty similar thing, but they're different.",
                    "label": 0
                },
                {
                    "sent": "You end up with the stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "Finally, let's make it a little bit complicated.",
                    "label": 0
                },
                {
                    "sent": "If you use 80.",
                    "label": 0
                },
                {
                    "sent": "As the inverse of remoni metric, then we obtain the stochastic natural gradient in June.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this natural region is very nasty, but here it's reasonable.",
                    "label": 0
                },
                {
                    "sent": "I still that later prison, so it would say we take the topic distribution parameter Lambda for an example.",
                    "label": 1
                },
                {
                    "sent": "So let's SJB the statistic from document.",
                    "label": 1
                },
                {
                    "sent": "Then you can think about like a exciting word counts.",
                    "label": 1
                },
                {
                    "sent": "If I do like a batch inference through coding the scent.",
                    "label": 0
                },
                {
                    "sent": "Like the new batch inference algorithm we just developed, then we can sell this gradient zero and this guy was the update.",
                    "label": 0
                },
                {
                    "sent": "Like this.",
                    "label": 0
                },
                {
                    "sent": "You know, either it's like some sort like a prior parameter to smooth things and then you see that like we need sort like a need out of statistic from out of the disk from our documents with some up an updated this like a topic distribution parameter, Lambda.",
                    "label": 1
                },
                {
                    "sent": "In online inference using natural gradient which is enabled by this knew back inverse algorithm has a similar form is like that I should say like in general natural is really complicated.",
                    "label": 0
                },
                {
                    "sent": "You see that like this, the learning rate, and actually this if you look at this toolbox here, they almost very similar, like the only difference is like you know this only.",
                    "label": 0
                },
                {
                    "sent": "Depending on like.",
                    "label": 0
                },
                {
                    "sent": "On one document, but you treated at the entire corpus the.",
                    "label": 0
                },
                {
                    "sent": "But here you sort like a summer altogether.",
                    "label": 0
                },
                {
                    "sent": "In general we usually don't use like this.",
                    "label": 0
                },
                {
                    "sent": "One document is kind of like unstable, so we used like multiple samples like each time.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then here, OK here is the final online variational inference algorithm that we have in the paper.",
                    "label": 1
                },
                {
                    "sent": "So what we do here is like we have some data and have some learning rate, roti and then we have like a mini batch size S and we do.",
                    "label": 1
                },
                {
                    "sent": "Here is like we have some stopping criterion out there and then we fetch like S document maybe like say 200 and then we update the various parameters for every document in that specific set and we update next we update the corpus level parameters you gave using this location.",
                    "label": 0
                },
                {
                    "sent": "Natural gradient it says show you.",
                    "label": 0
                },
                {
                    "sent": "But this time for like using S document, this really is similar.",
                    "label": 0
                },
                {
                    "sent": "I start, it's like you know.",
                    "label": 0
                },
                {
                    "sent": "So like a reweighting this like like that.",
                    "label": 0
                },
                {
                    "sent": "Like this you have as like I stuck him in here but you sort collect the information from only this as documents.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the one thing I should say.",
                    "label": 0
                },
                {
                    "sent": "Like if you set roti to be one and as to be D, which means like you, you use constant learning rate as one an.",
                    "label": 0
                },
                {
                    "sent": "But each time you sample our documents this type is the best inference algorithm which is no.",
                    "label": 0
                },
                {
                    "sent": "Boy",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this I connected connection of that.",
                    "label": 0
                },
                {
                    "sent": "So here comes the experiment.",
                    "label": 0
                },
                {
                    "sent": "So we have two corpora.",
                    "label": 0
                },
                {
                    "sent": "One is nature with 300 and five 350,000 thousand documents, and the range from like their range from like 1860, nine 2008 and the other data set PNS is little smaller.",
                    "label": 0
                },
                {
                    "sent": "It has like 80,000 documents.",
                    "label": 0
                },
                {
                    "sent": "So we use a pretty similar predictive likelihood measure, which is similar to also similar jumps, but he exponentiated I didn't.",
                    "label": 0
                },
                {
                    "sent": "OK, so we split the test data into two half and.",
                    "label": 0
                },
                {
                    "sent": "We predict second half, you know, using using the first half and the training, the training that data we compare with the on line version LDA and the batch variance.",
                    "label": 0
                },
                {
                    "sent": "That's very very sleepy for running for six hours.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's 1 results on nature.",
                    "label": 0
                },
                {
                    "sent": "So we set the crops that will transition K to be 150 and the document level technician to be 5015.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "We we we we we did because we did this because we believe that you know Document label transition usually is much smaller than the Corporal punishment because you know a single document usually does not have that many topics in it.",
                    "label": 0
                },
                {
                    "sent": "In ink in general.",
                    "label": 0
                },
                {
                    "sent": "And in this batch, HDP is only run on like a subset of data because it's really slow and we I think I remember it's like 20 K for this batch HDP.",
                    "label": 1
                },
                {
                    "sent": "So the X axis the time this takes hours and this is the predicted log likelihood.",
                    "label": 0
                },
                {
                    "sent": "The higher the better.",
                    "label": 0
                },
                {
                    "sent": "You know this red curve is online HTP.",
                    "label": 0
                },
                {
                    "sent": "It's sort of like a does much better than all the other like methods here.",
                    "label": 0
                },
                {
                    "sent": "And it was nice thing I notice like you know it's online version.",
                    "label": 0
                },
                {
                    "sent": "HTTP use actually use less topic like fewer topics then then transition level.",
                    "label": 0
                },
                {
                    "sent": "So here we we saw like a mini member.",
                    "label": 0
                },
                {
                    "sent": "We have a two parameter to set like learning rate and the medium size is.",
                    "label": 0
                },
                {
                    "sent": "We sort of very very them and see how they perform compared with our online RDA.",
                    "label": 0
                },
                {
                    "sent": "We sort of have this similar thing like a train across different settings.",
                    "label": 0
                },
                {
                    "sent": "In most cases the energy that's better than online RDA.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is not a result on PNS like this is like a auto setting is almost in almost the same as the nature experiment.",
                    "label": 0
                },
                {
                    "sent": "The only exception is like the.",
                    "label": 0
                },
                {
                    "sent": "The Bachelors degree is trained on the whole data set.",
                    "label": 1
                },
                {
                    "sent": "It's really smaller because.",
                    "label": 0
                },
                {
                    "sent": "So here we.",
                    "label": 0
                },
                {
                    "sent": "The X axis.",
                    "label": 1
                },
                {
                    "sent": "Let's take the time and the access to the predict log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So we see that like this online, HDP is little worse than the batch HDP, but it's running much faster than that.",
                    "label": 0
                },
                {
                    "sent": "So here's a stimulus at a similar set of results.",
                    "label": 0
                },
                {
                    "sent": "Like we change different like learning rate and mini batch sizes, and we sort of find similar things.",
                    "label": 0
                },
                {
                    "sent": "So finally that we have here is like we simulate the streaming data on the net.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Corpus imagine like you know nature was like born like a you know around like 1869 and you know.",
                    "label": 0
                },
                {
                    "sent": "So we have a HTTP model like in 1869.",
                    "label": 0
                },
                {
                    "sent": "Some articles were published in Nature, then we just fed also those article to the HD model and then later on on and then we can run this edgy in order for 200 years.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in order to avoid doing doing only this thing for my life, I run it for a couple hours.",
                    "label": 0
                },
                {
                    "sent": "So sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are two topics that I have here.",
                    "label": 0
                },
                {
                    "sent": "This topic is about like if you can read this.",
                    "label": 0
                },
                {
                    "sent": "I'm happy if you count like can tell you the first topics like astronomy, astronomy research on stars.",
                    "label": 1
                },
                {
                    "sent": "They are like they were happening pretty early in Natural History so could see a big jump here.",
                    "label": 0
                },
                {
                    "sent": "And this topic was actually making sense.",
                    "label": 0
                },
                {
                    "sent": "You can see them.",
                    "label": 0
                },
                {
                    "sent": "So this topic is about the neuroscience research on rats.",
                    "label": 1
                },
                {
                    "sent": "So apparently this topic.",
                    "label": 0
                },
                {
                    "sent": "Didn't make sense until like around somewhere here and you can see.",
                    "label": 0
                },
                {
                    "sent": "Also see that from these words.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's.",
                    "label": 0
                },
                {
                    "sent": "End with.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Meant.",
                    "label": 0
                },
                {
                    "sent": "In summary, we have developed a new batch of various inference algorithm for HTP and also given this we present and you online inference algorithm for HP as well.",
                    "label": 1
                },
                {
                    "sent": "Empirical demonstrate.",
                    "label": 1
                },
                {
                    "sent": "This performance on logical applications.",
                    "label": 1
                },
                {
                    "sent": "So for the future work we want to generalize this idea to other Benson nonparametric parametric models and also in order to be a true Blue Basin parametric, we want to.",
                    "label": 1
                },
                {
                    "sent": "We don't want to like set functions.",
                    "label": 0
                },
                {
                    "sent": "Instead we can try to grow the translations on the fly.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's it, thanks.",
                    "label": 0
                }
            ]
        }
    }
}