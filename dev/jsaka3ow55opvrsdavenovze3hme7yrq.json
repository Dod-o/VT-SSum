{
    "id": "jsaka3ow55opvrsdavenovze3hme7yrq",
    "title": "Robust PAC-Bayes Bounds",
    "info": {
        "author": [
            "Olivier Catoni, Department of Mathematics and their applications, \u00c9cole normale sup\u00e9rieure Paris"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_catoni_rpbb/",
    "segmentation": [
        [
            "OK, so at least square regression which is somehow most classical topic you can think about.",
            "And when I'm going to try to explain it, but."
        ],
        [
            "And the pack buys approach is a way to.",
            "To control and to analyze the empirical processes so.",
            "The general setting is met.",
            "You have some expected risk depending on some parameter filter, which is the expectation of some.",
            "Loss function.",
            "Define which which takes as arguments of some random variable Z.",
            "So I will concentrate on the example of.",
            "Square regression with random design where it's just.",
            "The usual square loss where?",
            "Why is real valued and where X and feature lives in Rd?",
            "OK, so the problem is too.",
            "Feature will be some some bounded region in our D and we will assume it to be convex in a moment.",
            "So the problem is to minimize the unknown risk.",
            "Open feature.",
            "So I'm going to try to address this problem in a pack bias way.",
            "By going through a certain number of steps.",
            "So the first step is to be able to estimate this.",
            "This expectation for a fixed value of theater.",
            "So the first problem we have is to estimate the mean.",
            "Women of some random variable that I will call.",
            "Why I?",
            "So I will.",
            "Sorry I should have said, but here I'm going to try to solve this problem.",
            "From an observation which here is going to be idea, so the observation is an IID sample that I.",
            "And I will, I will add.",
            "Foundation purpose AZ which is going to be independent from everything else.",
            "And we have the same distributions.",
            "So.",
            "So the first thing we need is, as you already seen, in many in many talks, is is a deviation inequality or an exponential moment bound for a fixed value of feeder and.",
            "I will discuss this for random variable Yi which is going to be afterwards this one.",
            "So.",
            "So the pack buys route is through using exponential moments, so I think we can consider is the expectation of some parameter times the sum of the Y -- M, where M here is going to be the expectation of Y.",
            "And why I D. OK, basically if I, if I'm willing to use the Markov inequality from this expectation control, I will of course need the fact that.",
            "Why has exponential moments?",
            "That is a. Namely I need this to be finite.",
            "So there is a way to get rid of this while still using exponential functions too.",
            "Exploit the fact that the data are independent and it is to introduce so the first idea is to introduce some kind of influence function.",
            "Something.",
            "Which resembles robust statistics.",
            "Anne.",
            "So I'm going to introduce face.",
            "I'm going to define RI feature which is going to be one over Alpha times.",
            "I my inference, one function will be \u03c0 times for.",
            "Why I minus feature?",
            "When a feature is a is a primary parameter here.",
            "OK, so first thing one can do is choose.",
            "I of X equal laguan plus some.",
            "So if you if you choose to sign in this way.",
            "You obtain something interesting.",
            "You obtain that now.",
            "If I put our feet are equal 1 of N Alpha time, sorry.",
            "No.",
            "Now at face of where the definition of.",
            "I take the empirical mean of my eye, federal.",
            "I've got a nice.",
            "Control of the exponential moment.",
            "Of course an.",
            "This will be.",
            "Equal, so last man certainly.",
            "OK.",
            "So here I have.",
            "Assume that I that the variance is finite and nothing else.",
            "Exponential and Alpha.",
            "'cause if I take.",
            "I multiply this by an an A2 to get the product.",
            "It in fact it's an equality here.",
            "So now we can improve Saia little bit by.",
            "Using a symmetric inference function and an increasing one, because this one is not not increasing this function here is something like it's.",
            "It's the identity near 0.",
            "But then does something like that.",
            "OK, so to improve side I can use a so of course she is obtained by some hard truncating the exponential function.",
            "Should I write the identity at the logarithm of the exponential of X and I truncate the Taylor expansion of the exponential function at order two to obtain this?",
            "But now we've got something interesting.",
            "We can.",
            "Make use of this inequality.",
            "X it's even.",
            "It's strict here.",
            "So this is easy to see.",
            "It's just because we have a.",
            "This can be rewritten as.",
            "Certainly the inequality holds so we can squeeze in between these two functions that were interested in so some some some influence function size so we can find sigh.",
            "Satisfying this.",
            "And being symmetric.",
            "So of course the other one is is going like that.",
            "And the the.",
            "Yeah.",
            "And so we can.",
            "If we want to take the.",
            "The strongest possible shrinkage.",
            "Here we can define sign like this.",
            "So the definition is.",
            "Science.",
            "Sorry.",
            "Yeah, it's going to be.",
            "It's easier to use monotonic influence function.",
            "We'll see afterwards, why?",
            "That's like too far.",
            "An excellent one, Ann.",
            "K. And this this is, this is still true, because for this improved side this inequalities is still true.",
            "Without equality, now we don't.",
            "Hallway.",
            "Now you have also allowed.",
            "The larger one is is.",
            "May be interesting in some cases, but I will show in the following that using a bounded function has some advantages in some in some situations.",
            "This is the one I will lose.",
            "I will use when I will be talking about this square mcquestion.",
            "So of course in some other cases you may want to take the largest the widest possible side, which would be this one, which is also possible choice.",
            "So so increasing, but it's it's.",
            "It's closer to the original random variable you want to grasp, but but it's not bounded and in some situations it's better to threshold as much as as possible while keeping the properties we want to keep.",
            "And of course, the interesting thing about choosing.",
            "No site to be monotonic is is that now we've got interesting mounts.",
            "At least in this case.",
            "So now we know that with probability.",
            "Minus.",
            "Does this mean?",
            "This will hold with probability 1 minus epsilon.",
            "For any fixed values of feature one and feature 2.",
            "I've got bounce involving the variance.",
            "And the level of confidence.",
            "I7 strict if I want.",
            "OK.",
            "So now I can define.",
            "An estimator.",
            "Optimine by.",
            "Just putting.",
            "Choosing feature had such bad.",
            "I think the head is equal to 0.",
            "And then if I choose feature one and feature two such that this bound B1.",
            "Is less.",
            "Lesser or equal to 0, and if I choose feeder to such that B2, here is large event 0 band.",
            "Then I will know that with probability 1 -- 2 epsilons.",
            "It's also if if I define.",
            "51 and 52 by B 151 = B two 52 = 0.",
            "Then I will know that.",
            "And we have.",
            "Bounce 4 feet ahead with probability.",
            "1 -- 2 absence.",
            "So you can solve this by choosing.",
            "The appropriate value of Alpha an.",
            "So in the case when you know the variance, you end up with the following.",
            "Not a simple rebound.",
            "So you've got to consider.",
            "Muse.",
            "Sorry, I mean.",
            "Still, is just the solution of our feet ahead, yeah?",
            "But the thing that holds with probability 1 -- 2 epsilon is that the real meaning is in the center.",
            "No, it's fitted hat and then we.",
            "By solving this equation you get that feature one is equal to M. Minus something.",
            "The solution has this form."
        ],
        [
            "With with with equal to what I'm going to put here.",
            "Alright, because here you have only deterministic quantities in B1 and B2.",
            "Right?",
            "So when I will solve B1 of 51 = 0, I will obtain 51 as a function of MNV.",
            "Why choosing?",
            "And optimize value of Alpha.",
            "So here we get.",
            "2V epsilon minus one.",
            "Over N. Times 1 -- 2.",
            "Over N. Something obvious, sorry.",
            "If it's been one of 51 equals in minus 8A and B 250 = N + A to know it's feature one, sorry.",
            "It's we solve for this equal to 0 an.",
            "In the middle term?",
            "Or do you mean the entire question?",
            "This entire left right hand side.",
            "Sorry, maybe I'm not that big.",
            "I called this base left hand side B1 of feta one and I called this.",
            "This right hand side.",
            "Be 2 feet 2.",
            "Then I solved this to be equal to 0, because if because I know that small R is increases, decreasing and feta.",
            "Yeah, so far has been.",
            "Yeah here you've got to choose this value.",
            "This is of course not true for any value of Alpha.",
            "This is true for this value.",
            "So if here I assume, but I know the variance or at least a bound on the variance setting, which I called V. If I I know VI can choose this value of ether here, which depends only on the epsilon and N and this value of Alpha.",
            "And if I choose these values then then the solution of these two equations are M -- 8 M plus eater.",
            "Mansan as the influence function has been chosen to be.",
            "A size increasing the influence function.",
            "There are sorry, the empirical risk here is decreasing.",
            "So I so so if I have this.",
            "So I did use this from the fact that the.",
            "Our official minus with you again tomorrow.",
            "Yeah, maybe they did the thing.",
            "Yeah.",
            "No sorry, it's maybe I.",
            "Let me see.",
            "R is negative.",
            "Yeah, no one is negative, so I'm making a mistake here.",
            "I guess it's this.",
            "An endless.",
            "Answer So it's possible to sort what I wanted to say is that you using this kind of influence function, it's possible to get bounds that are under weak assumptions, because here we only assume that the variance is finite an here we assume that it is known and we get a bound.",
            "But up to this correction term here, which is not the big is, the band would have obtained for a Gaussian.",
            "If we had assumed why I to be Gaussian an if we have had used an exponential the next day, exponential moments of Goshen to obtain a deviation inequality.",
            "And then if the variance is not known one can I use lipsky's method to adapt to the variance or make or we can also make up hypothesis?",
            "Under kurtosis of the distribution, to be able to estimate the variance.",
            "But I will not develop is the only thing I wanted to say here is that it's possible to modify exponential inequalities to get results for random variables that have no exponential moment under week, week hypothesis and I can show you the curve if you want.",
            "Is it possible to generate?",
            "It doesn't change the fact that the it's possible to generalize to independent non.",
            "Identically distributed.",
            "Variables in a straightforward way.",
            "Other details.",
            "Around him, feeder hat is an estimator of M feeder hands.",
            "It is computed from the observations.",
            "By solving smoke, small outfitter hat equals 0.",
            "Other small deviation and now.",
            "Is it understand what this tells me about the original problem?",
            "The original the question I'm addressing right now is how to estimate the mean of a real valued random variable under the assumption that the variance is known and finite.",
            "And what I want to point out is that if you use the empirical mean, the usual estimate.",
            "You will have a deviations much larger than the Gaussian deviation.",
            "In the worst case, whereas if you use a threshold estimator in the in the in the way it is done in rubber statistics but with a different special choice of inference function and a special choice of Alpha people from rubber statistic would not consider such a small Alpha.",
            "Here Alpha is decreasing as one of square root of N, which means that we are going to threshold only very large.",
            "Outliers.",
            "Of why?",
            "The.",
            "The influence function which is here.",
            "Is thresholding outliers from the mean or from the estimate feet ahead of them in an here as the scale parameter Alpha is decreasing at like 1 / sqrt N we will only threshold really large.",
            "Outliers.",
            "So now yeah.",
            "That we may have these very large outliers, but we can't have too many of them because the variance is small.",
            "And so yeah.",
            "That is an unbiased estimator of them, but rather that it it has a small deviation.",
            "Yeah, I'm looking at the confidence interval.",
            "I'm trying to build an estimate of aminat at confidence level epsilon 1 -- 2 epsilon.",
            "Write an.",
            "And what I obtain is, is it so it's a true Nana, synthetic confidence interval?",
            "And if you look here here, I've plotted the confidence interval in the case when Y is Gaussian.",
            "So it's just the inverse of the distribution of the inverse of the repetition and the distribution function of the Gaussian distribution.",
            "And here this is the bound.",
            "This is the address I obtained by by this.",
            "Robust estimate of the mean.",
            "Yeah, the same.",
            "The same variance here.",
            "Yeah, So what you can see is that for for account fitness level of 98% here.",
            "We get a confidence interval which is of the same.",
            "Order of magnitude, as in the Gaussian case, but in a much larger model.",
            "Because we don't, we don't make any assumption and on the shape of the distribution, we just assume that the variance is known an equal to 1 here and then if you plot.",
            "Upper and lower bounds for the empirical mean estimator.",
            "The usual estimator.",
            "When you take the arithmetic means of the Yi.",
            "You see that at this confidence level, the lower bound.",
            "This is a lower bound the load is already blowing up.",
            "So even for a four, for a sample of size 100.",
            "If you want to get a confidence interval at confidence level 98%.",
            "You have a benefit in thresholding.",
            "The usual empirical mean, at least in the worst case.",
            "That is why when when the the tail is is as heavy as possible when the variance is equal to 1.",
            "So an.",
            "One other interesting thing is about the bound here.",
            "Behaves as in the Gaussian case.",
            "It is we've it's in square root of log of epsilon minus one, even for very large confidence levels we can go up to.",
            "1 -- 210 to the power minus 8 and we still follow the order of magnitude that we would have obtained for the Goshen, although we include in the model distributions with much heavier tails.",
            "It seems that to compute that fee to Cesar Pizza hat equals zero, yeah?",
            "No and no.",
            "You don't need to know him because M is not in.",
            "Is the classical trick of Robert Statistiques.",
            "She is.",
            "On the variance, you essentially obtain the downsize, the load, the lower and upper bounds for the mean for empirical mean, but I plotted.",
            "The curve I.",
            "It's essentially the championship bound is essentially reached.",
            "Yes, so this doesn't.",
            "This doesn't depend on on on.",
            "An app.",
            "So now let me talk about.",
            "The.",
            "Regression problem.",
            "Can I come back to?",
            "The quadratic risk.",
            "An feature zero will be.",
            "The best possible.",
            "Choice of parameter in.",
            "In the parameter space filter, which is going to be a convex bounded convex set in Rd.",
            "High recall that I'm losing, sorry.",
            "So now.",
            "I want to solve for this, but I don't know and the first thing and OK.",
            "So one thing we know is that the behavior of the empirical.",
            "Process depends on the covariance structure, so relevant trying to solve this problem I will.",
            "I will write this.",
            "As a minimax problem.",
            "Sure, if I got here.",
            "So it's the couple, so solving this is just solving twice this thing, right?",
            "So now I can think about.",
            "Solving the same min Max problem with but with with an empirical version of this thing.",
            "But I don't know.",
            "And this empirical in this empirical.",
            "Counterpart of the risk of a difference of risks, I will introduce a coupling through the use of an influence function.",
            "WY is.",
            "Chest.",
            "The difference of Alas is.",
            "At Point XIY I4 parameters feature one and feature 2.",
            "OK, so of course.",
            "If I'm choosing.",
            "This empirical quantity is because of the exponential bound it can get.",
            "Sorry.",
            "So when I write W, it means that it's W4 for an independent couple X&Y from from from the sample.",
            "And so I will be able at least for fixed values of 51 and 52, to estimate the difference of a risk function in 51 and 52 an.",
            "I will have a variance only variance term here.",
            "A term of including moment of order two of.",
            "I after the last time I'm considering here so I will not need exponential moment assumptions on the noise and I don't make any hypothesis under nice structure here except for the fact that I assume the.",
            "I am why are independent and the simplest case IID?",
            "I don't assume with my eyes F of XI plus plus noise.",
            "Only, but we have a giant distribution.",
            "Neubauer, an I will have some some some polynomial moment assumption only.",
            "So now the estimator is going to be.",
            "Computed by.",
            "Minimizing the supremum in fear 2.",
            "I've.",
            "Of this OK, so I'm doing the same thing as here, but here it's a small complicated because we've got a coupling due to the fact that we threshold launch outliers using this influence function PSI in order to get rid of exponential moment assumptions.",
            "So now what is going to be the?",
            "Add."
        ],
        [
            "The general idea of the.",
            "The proof for the event of a generalization bound to obtain a generalization bound for feed ahead.",
            "We're going to write this.",
            "We've got a supremum.",
            "Have our primer fitted hat.",
            "Theater 2, which certainly.",
            "Is less.",
            "Man, the supremum in theater, two of our prime of feature Zero Fear 2.",
            "And by definition of a fit, had MRSA.",
            "Morvin which is itself equal.",
            "It is symmetric symmetric, so our prime is symmetrical, so.",
            "So now if I succeed in proving that this supremum and feature 2.",
            "Of our prime of features, your filter is a deterministic parameters, it's it's it's.",
            "It's the optimal value of.",
            "Outfitter.",
            "It's a minimizers of minimizer true risk, so if I can prove that this is less than.",
            "The supremum of.",
            "Are official.",
            "Note minus R 52.",
            "Plus some small.",
            "Reminder.",
            "And here if I can prove.",
            "But this is not driven.",
            "I will have what I want I want.",
            "OK, because.",
            "I've got a chain of inequalities.",
            "This thing is going to cancel with.",
            "Sorry.",
            "This one is by construction of Fingerhut.",
            "Because I take the the minimum here, so it's less than what I got for the optimal value of feature 0.",
            "So if I can have a uniform control of our primary feature, 0 feet are too uniform, inferior to an I. I'm able to prove that this is less than what I get for the excess.",
            "Nasty accessories plus small remainder.",
            "Then I'm going to conclude in two steps, the first step.",
            "The first step is obtained by saying that this is a. OK, this is certainly this is Mrs as features heroes that the minimum on feature of this is positive also.",
            "So now if I consider the dark.",
            "So as it is positive, I will I will have.",
            "Anne.",
            "If I consider here as fear to the argmax of this, I will have a control of.",
            "I will be able to bound.",
            "This right by by putting this in the.",
            "The other way around.",
            "Well, what the?",
            "An after.",
            "Sorry the the shortest way to explain this is that, as with this quantity is less than zero.",
            "I've got only the small remainder here.",
            "And then so if I have this chain of inequalities with this proof that are fitted, hat is less than our future 0 plus the sum of the two small remainders, right?",
            "So what I need to do to get a generalization bound for fitted hat is uniform control in theater two of our primer feeder zero 52 because here it's the same sort of quantity event that appears.",
            "So it's this uniform control is going to be obtained by using pack by pack buys.",
            "Technology.",
            "Care about.",
            "Suppose that the end is attained.",
            "An argument with the thing because we are.",
            "Yeah yeah, but I don't need to to solve exactly.",
            "I can solve up to some precision and precision is going to be put here.",
            "Thanks, I wrote it like this because simply it's easier but but if we solve the min Max problem with some accuracy gamma we can just add the accuracy.",
            "I could I could, yeah.",
            "It's it's it's.",
            "It's easy to see here.",
            "You have to add that it's plus.",
            "Officially yeah OK.",
            "So as already explained, and So what we we know is how to bound.",
            "Small perturbations.",
            "So if I integrate with respect to a posterior role filter 2.",
            "I will be able to use Backpage and bounce to bound.",
            "This quantity.",
            "And a.",
            "No, I need to do something to be able to.",
            "To work with.",
            "To have a bound by this uniform in Figure 2.",
            "So what I'm going to do here is to use the following simple.",
            "Inequality.",
            "Says my inference function and I'm going to compare.",
            "So in fact unfortunately, it's not.",
            "It's not convex.",
            "But what I can do is.",
            "Use the following bound, which is obtained by adding to size something to turn it into a convex function.",
            "I what I want to do is to pull the expectation with respect to the posterior outside of the influence function.",
            "It would be easier if sign was convex, but it's not convex, but still it's possible to have an approximate inequality.",
            "If I compare these two things, we can't be more than the total variations of PSI.",
            "For sure.",
            "OK, which is lock to lock to lock for.",
            "Anne.",
            "Looking at the second derivative of Cyan, adding tips, I a quadratic function at Mega.",
            "That makes the sum convex.",
            "We can prove it here.",
            "We have to consider the variance.",
            "Of each.",
            "So then.",
            "When we can say that this is going to be less than the log of 1 + A.",
            "Times the variance term, let's call it P. No, sorry it's great.",
            "This is going to be less than bad for a.",
            "Equal to print off a log.",
            "For much less than two point.",
            "17 OK.",
            "So we can write this late.",
            "OK, so.",
            "Here.",
            "We've done some stuff.",
            "We have some improvement, mainly up to a remainder, but it's going to be small.",
            "This thing is not.",
            "It is now independent from feeder two in good situations it may be a case that the variance good choices are perturbations can make the variance independent of feature 2, but if the variance still depends in theater 2, it's possible to use some tricks to get rid of it.",
            "Along the following.",
            "Idea?",
            "So if here we have a growth say up order a times the.",
            "The norm of a feature 2 to the power P we can just.",
            "We can just live with P. It's possible to use some trick by, like for instance the following one.",
            "What can write we can go back here?",
            "And right this term here lies.",
            "OK, I can certainly write.",
            "I can bound bound my variance term here if it's if it's bounded like that, I can write it like right to bound like that and then I can say that this is less.",
            "When?",
            "Add 2 to the power P -- 1 times.",
            "A.",
            "Theater 2 minus feature P + 2 P minus one feature P. And then.",
            "When I when I use the fact that the minimum of something an A+B is less than the minimum of something and a plus the minimum of something plus B when A&B are positive.",
            "And I end up.",
            "By being less fan.",
            "So this is the nice thing for us.",
            "And here I put the expectation inside.",
            "And then I can reasonably think about the partitions for which sorry it's two here.",
            "For which to this this moment, this centered moments is independent of fear 2.",
            "It's OK.",
            "This is what you want.",
            "Because you are interested in supremo, so why do you take the averaging?",
            "On the posterior because they want to apply a pack base, I want to play a pack bias bound in which I will have.",
            "To control sorry, I will have the code Black Library divergents of role theory two with respect to some reference function pie and if I in this continuous setting where the parameters are continuous, if I choose to direct mass inferior two, this term is going to to blue.",
            "Blew up.",
            "That's why so.",
            "The idea is that the following you want to approximate.",
            "What is going on in Theater 2?",
            "You're going to add a small perturbations to feature 2.",
            "Yeah.",
            "That that's that's the idea.",
            "So using using this ideas, I end up with the following kind of inequality.",
            "So I'm going to use as already advocated in in previous talks a Gaussian.",
            "Yes, a Gaussian for traditions here and I'm going to apply this to H equal to WY.",
            "Www.i and if I use a Gaussian perturbation of a feature two, I'm going to have an explicit computation of the perturbation here.",
            "I'm going to write.",
            "I'm going to choose World Theatre 2.",
            "Proportional to the Gaussian distribution center in feature 2.",
            "Which parameter beta Peter time identity?",
            "An when I choose this, I will choose the reference measure.",
            "To be the Gaussian centered.",
            "At the optimal parameter, I want to estimate.",
            "Mike and when I do this.",
            "I end up with.",
            "So the diversions here.",
            "Is going to be equal to?",
            "Then the quadratic norm.",
            "And I'm going to be able to write.",
            "So the quantity I'm interested in is why I features Hero feature two and this can be written as.",
            "Anne.",
            "Right?",
            "So this is independent from filter, so I can I can integrate like this.",
            "So I write this like this.",
            "I'm using this identity here.",
            "And so this will give me.",
            "An exponential.",
            "A control on the exponential moment of our prime of feature zero 52.",
            "Right?",
            "So I'm getting the.",
            "For like inbound.",
            "So with probability 1 minus epsilon.",
            "So the main terms here are.",
            "These ones.",
            "Plus this is the.",
            "This is the entropy term and we have to.",
            "Confidence term here plus a remainder which is going to be.",
            "Office kind.",
            "So it's the expectation.",
            "And we have some some quantities here, but.",
            "Everything is explicit.",
            "The constant are numerical constants.",
            "I'm sorry, disease.",
            "First setra.",
            "That's all.",
            "This is what we get.",
            "Set up what we can do here so.",
            "Now we can exploit effect that we don't use.",
            "The perturbation in the in the computation of the estimator in the computation of the estimator, we only need to know the scalar product between feature one feature and X and XI.",
            "And if we assume that the gram matrix is the identity, then we end up with the fact that.",
            "Anne.",
            "Met feature 0 minus feta is less.",
            "When our future minus our filter.",
            "0.",
            "OK. Because of convexity.",
            "It's equal if it.",
            "If it features roses within, sorry.",
            "Let me explain this.",
            "And there is a square here.",
            "Amigos.",
            "Now if I define.",
            "Feather star as the argument over ID of all.",
            "Alright then.",
            "I can write a feature as our feeling.",
            "Stop sorry, orifices star plus.",
            "When the gram matrix is the identity, it's it's.",
            "If it's not died until you have to, because this is the.",
            "This is a quadratic quadratic form, so you have only the quadratic term remaining when you start from the optimum here.",
            "And then when you consider the.",
            "The optimum in the convex it's it's the projection of a future star.",
            "Here an and so.",
            "Now if you look at the difference of risk between feature and feature zero, it will be you will have this inequality by using the fact that feature is is convex.",
            "Cancel.",
            "I don't suppose it I if I want to write it properly, I put, I put.",
            "I consider this change of notations, right?",
            "An I don't know.",
            "I don't need to observe any part of the perturbations.",
            "All the things I'm using here, I don't need to observe them because we are used only in the computations.",
            "We are not used in the definition of the estimator, which is defined by solving a minimax problem for our prime.",
            "Look at your data.",
            "It's only in the computations, but I can.",
            "Make this change of notation.",
            "I make this change of notation in the computations only and I choose the perturbation.",
            "With respect to the service means that in in the original setting this would be the the Gaussian variable with with with with covariance matrix equal to the gram matrix.",
            "OK. Maybe I should not have said that, but I can assume by the gram matrix is the identity.",
            "I just do this change of notation I make all the computations.",
            "In this with this notations and this definition for the changed after the change of basis right?",
            "Apply also.",
            "2.",
            "My display to display.",
            "Yeah, yeah, yes, it's it's.",
            "It's the it's the yeah I should have write in like this, yeah.",
            "Is it because it's it's just to try to simplify the.",
            "What I can say is, but without loss of generality I can assume by the gram matrix is the identity.",
            "But if I if you want to write things in details you you may consider the change annotations and and and play with two different norms.",
            "The norm I've got to consider here is the normal associated with the crime metametrics.",
            "In order to have a comparison between the Norman and the excess risk.",
            "OK.",
            "So now if I consider feature to be.",
            "Oh now if you were to, is that if the argmax here is so, this is, this is not.",
            "This is this is this is positive, and so I've got.",
            "So this here the first term is is this one.",
            "And this means that I have a control in for well chosen values of Alpha, so I can express all these things in terms of parameters and of the distance between feature zero and feet or two, and I will get a bound for.",
            "For distance between feature 0 and 52, which is going to give me also bound for the excess risk of fear two with respect to feature zero and then going back?",
            "And doing doing it once again as I showed I doing the same applying the same thing to fit in hand.",
            "I will end with generalization bound for future hat of the same order an.",
            "What is interesting here is mad.",
            "Is going to be expressed.",
            "Using only a limited number of quantities, the bandwidth will make use of the only for the following.",
            "Quantities.",
            "So I will not detail about, I will just say that we get a pounding.",
            "The overhead depending on the following constants.",
            "Which is it cold today here?",
            "So we have only to consider some kurtosis like.",
            "Parameters here.",
            "So basically this is the variance.",
            "For optimal parameter an K. Written like this, it can be written in any basis.",
            "If you write this.",
            "And in a base it is also orthogonal for metric defined wunderground metric this is equal to 1 and this is equal to D. Did I mention?",
            "So, so this means that we can have the over and bounds.",
            "Without directly having assumptions on the gram metrics.",
            "And.",
            "For instance, if the design is Gaussian, then the Scala review and X is Gaussian.",
            "So certainly the kurtosis is is bounded by by free.",
            "And you have no problems.",
            "And when you consider linear regressions in basis of functions, you can bound this kurtosis coefficient here by considering.",
            "Almost orthogonality assumptions on the basis also, but I think it's interesting to see that.",
            "The hypothesis you have two 2.",
            "To consider on the basis of function just.",
            "Used to bound this with this kind of quantity.",
            "And for instance, when when when the Gaussian, it's trivial to see, even though the culture this is bounded here.",
            "And so it's a way to we can considerably hypothesis needed to obtain a DD of N bound for the quadratic regression case.",
            "And as it is a mainstream problem, I think.",
            "And also I think that this program can be applied in different situations for other loss functions and so the idea is that you will if you want to study an M estimator, you are going to consider a small perturbation of this M estimator.",
            "You are going to replace feature hat by some perturbation here.",
            "A feature had some rule hat just perturbation figurehead.",
            "And you are going to choose the right tradeoff between the size of the perturbation.",
            "And the size of the entropy term.",
            "An if you make the right tradeoff, you will obtain bounce and assumptions that are interesting because this technology of course is replacing the use of a concentration inequality for the supremum of the empirical process, and it gives explicit bounds OK if I have only one minute, I will.",
            "And we've just a remark.",
            "It is that if you if you consider some estimate, some randomized estimator rohat, you can ask the question of.",
            "Choosing the best possible reference prior Pi and in expectation it's trivial to see that.",
            "I'm sure many of you are aware of that, but.",
            "The optimal so if we if we reverse the usual point of view, that is, I choose a prior and when I try to optimize the posterior, knowing the bound, if I start with the posterior and I'm asking what is the best prior in expectation, there is a clear cut answer which is.",
            "The expectation of the posterior, which is a prior, of course, because I right.",
            "From usual inequalities about the callback table, and this is precisely the mutual information.",
            "Between the feet ahead and the sample.",
            "So the idea is that following the pack buys technology.",
            "Allows you to have generalization bounds where the complexity of the randomized estimator defined by a posterior.",
            "Is bounded by the mutual information between the the randomized between fetal heart, your estimator, and a sample.",
            "So the randomization is here to make this mutual information lower, because for usual M estimate, M estimators and the continuous parameter set, if you don't do anything that mutual information is infinite.",
            "But if you use a small perturbation Bender, the mutual information is becomes finite.",
            "And this is some kind of done a synthetic replacement for the Fisher information if you want to send something like that, and it's I think it's a nice interpretation of what we are controlling here.",
            "So it can be a way to distinguish pack bias approach from the bias approach, because when the sample size is going to be increased then.",
            "The good posteriors are going to be more concentrated.",
            "We are going to change the concentration of proof roof.",
            "It is going to evolve with the sample size and show the optimal prior is going in.",
            "This setting is going to depend on the sample size also and this is some difference with the usual.",
            "The usual Bayesian approach.",
            "You don't assume that the prior is depending on the sample size.",
            "So so here we see that the.",
            "Role played by the prior and the posterior in the pack by his approaches is not the same as in the beige, and the usual version setting.",
            "I kissed him, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so at least square regression which is somehow most classical topic you can think about.",
                    "label": 0
                },
                {
                    "sent": "And when I'm going to try to explain it, but.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the pack buys approach is a way to.",
                    "label": 0
                },
                {
                    "sent": "To control and to analyze the empirical processes so.",
                    "label": 0
                },
                {
                    "sent": "The general setting is met.",
                    "label": 0
                },
                {
                    "sent": "You have some expected risk depending on some parameter filter, which is the expectation of some.",
                    "label": 0
                },
                {
                    "sent": "Loss function.",
                    "label": 0
                },
                {
                    "sent": "Define which which takes as arguments of some random variable Z.",
                    "label": 0
                },
                {
                    "sent": "So I will concentrate on the example of.",
                    "label": 0
                },
                {
                    "sent": "Square regression with random design where it's just.",
                    "label": 0
                },
                {
                    "sent": "The usual square loss where?",
                    "label": 0
                },
                {
                    "sent": "Why is real valued and where X and feature lives in Rd?",
                    "label": 0
                },
                {
                    "sent": "OK, so the problem is too.",
                    "label": 0
                },
                {
                    "sent": "Feature will be some some bounded region in our D and we will assume it to be convex in a moment.",
                    "label": 0
                },
                {
                    "sent": "So the problem is to minimize the unknown risk.",
                    "label": 0
                },
                {
                    "sent": "Open feature.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to try to address this problem in a pack bias way.",
                    "label": 0
                },
                {
                    "sent": "By going through a certain number of steps.",
                    "label": 0
                },
                {
                    "sent": "So the first step is to be able to estimate this.",
                    "label": 0
                },
                {
                    "sent": "This expectation for a fixed value of theater.",
                    "label": 0
                },
                {
                    "sent": "So the first problem we have is to estimate the mean.",
                    "label": 0
                },
                {
                    "sent": "Women of some random variable that I will call.",
                    "label": 0
                },
                {
                    "sent": "Why I?",
                    "label": 0
                },
                {
                    "sent": "So I will.",
                    "label": 0
                },
                {
                    "sent": "Sorry I should have said, but here I'm going to try to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "From an observation which here is going to be idea, so the observation is an IID sample that I.",
                    "label": 0
                },
                {
                    "sent": "And I will, I will add.",
                    "label": 0
                },
                {
                    "sent": "Foundation purpose AZ which is going to be independent from everything else.",
                    "label": 0
                },
                {
                    "sent": "And we have the same distributions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we need is, as you already seen, in many in many talks, is is a deviation inequality or an exponential moment bound for a fixed value of feeder and.",
                    "label": 0
                },
                {
                    "sent": "I will discuss this for random variable Yi which is going to be afterwards this one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the pack buys route is through using exponential moments, so I think we can consider is the expectation of some parameter times the sum of the Y -- M, where M here is going to be the expectation of Y.",
                    "label": 0
                },
                {
                    "sent": "And why I D. OK, basically if I, if I'm willing to use the Markov inequality from this expectation control, I will of course need the fact that.",
                    "label": 0
                },
                {
                    "sent": "Why has exponential moments?",
                    "label": 0
                },
                {
                    "sent": "That is a. Namely I need this to be finite.",
                    "label": 0
                },
                {
                    "sent": "So there is a way to get rid of this while still using exponential functions too.",
                    "label": 0
                },
                {
                    "sent": "Exploit the fact that the data are independent and it is to introduce so the first idea is to introduce some kind of influence function.",
                    "label": 0
                },
                {
                    "sent": "Something.",
                    "label": 0
                },
                {
                    "sent": "Which resembles robust statistics.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to introduce face.",
                    "label": 0
                },
                {
                    "sent": "I'm going to define RI feature which is going to be one over Alpha times.",
                    "label": 0
                },
                {
                    "sent": "I my inference, one function will be \u03c0 times for.",
                    "label": 0
                },
                {
                    "sent": "Why I minus feature?",
                    "label": 0
                },
                {
                    "sent": "When a feature is a is a primary parameter here.",
                    "label": 0
                },
                {
                    "sent": "OK, so first thing one can do is choose.",
                    "label": 0
                },
                {
                    "sent": "I of X equal laguan plus some.",
                    "label": 0
                },
                {
                    "sent": "So if you if you choose to sign in this way.",
                    "label": 0
                },
                {
                    "sent": "You obtain something interesting.",
                    "label": 0
                },
                {
                    "sent": "You obtain that now.",
                    "label": 0
                },
                {
                    "sent": "If I put our feet are equal 1 of N Alpha time, sorry.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Now at face of where the definition of.",
                    "label": 0
                },
                {
                    "sent": "I take the empirical mean of my eye, federal.",
                    "label": 0
                },
                {
                    "sent": "I've got a nice.",
                    "label": 0
                },
                {
                    "sent": "Control of the exponential moment.",
                    "label": 0
                },
                {
                    "sent": "Of course an.",
                    "label": 0
                },
                {
                    "sent": "This will be.",
                    "label": 0
                },
                {
                    "sent": "Equal, so last man certainly.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here I have.",
                    "label": 0
                },
                {
                    "sent": "Assume that I that the variance is finite and nothing else.",
                    "label": 0
                },
                {
                    "sent": "Exponential and Alpha.",
                    "label": 0
                },
                {
                    "sent": "'cause if I take.",
                    "label": 0
                },
                {
                    "sent": "I multiply this by an an A2 to get the product.",
                    "label": 0
                },
                {
                    "sent": "It in fact it's an equality here.",
                    "label": 0
                },
                {
                    "sent": "So now we can improve Saia little bit by.",
                    "label": 0
                },
                {
                    "sent": "Using a symmetric inference function and an increasing one, because this one is not not increasing this function here is something like it's.",
                    "label": 0
                },
                {
                    "sent": "It's the identity near 0.",
                    "label": 0
                },
                {
                    "sent": "But then does something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so to improve side I can use a so of course she is obtained by some hard truncating the exponential function.",
                    "label": 0
                },
                {
                    "sent": "Should I write the identity at the logarithm of the exponential of X and I truncate the Taylor expansion of the exponential function at order two to obtain this?",
                    "label": 0
                },
                {
                    "sent": "But now we've got something interesting.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Make use of this inequality.",
                    "label": 0
                },
                {
                    "sent": "X it's even.",
                    "label": 0
                },
                {
                    "sent": "It's strict here.",
                    "label": 0
                },
                {
                    "sent": "So this is easy to see.",
                    "label": 0
                },
                {
                    "sent": "It's just because we have a.",
                    "label": 0
                },
                {
                    "sent": "This can be rewritten as.",
                    "label": 0
                },
                {
                    "sent": "Certainly the inequality holds so we can squeeze in between these two functions that were interested in so some some some influence function size so we can find sigh.",
                    "label": 0
                },
                {
                    "sent": "Satisfying this.",
                    "label": 0
                },
                {
                    "sent": "And being symmetric.",
                    "label": 0
                },
                {
                    "sent": "So of course the other one is is going like that.",
                    "label": 0
                },
                {
                    "sent": "And the the.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And so we can.",
                    "label": 0
                },
                {
                    "sent": "If we want to take the.",
                    "label": 0
                },
                {
                    "sent": "The strongest possible shrinkage.",
                    "label": 0
                },
                {
                    "sent": "Here we can define sign like this.",
                    "label": 0
                },
                {
                    "sent": "So the definition is.",
                    "label": 0
                },
                {
                    "sent": "Science.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's going to be.",
                    "label": 0
                },
                {
                    "sent": "It's easier to use monotonic influence function.",
                    "label": 0
                },
                {
                    "sent": "We'll see afterwards, why?",
                    "label": 0
                },
                {
                    "sent": "That's like too far.",
                    "label": 0
                },
                {
                    "sent": "An excellent one, Ann.",
                    "label": 0
                },
                {
                    "sent": "K. And this this is, this is still true, because for this improved side this inequalities is still true.",
                    "label": 0
                },
                {
                    "sent": "Without equality, now we don't.",
                    "label": 0
                },
                {
                    "sent": "Hallway.",
                    "label": 0
                },
                {
                    "sent": "Now you have also allowed.",
                    "label": 0
                },
                {
                    "sent": "The larger one is is.",
                    "label": 0
                },
                {
                    "sent": "May be interesting in some cases, but I will show in the following that using a bounded function has some advantages in some in some situations.",
                    "label": 0
                },
                {
                    "sent": "This is the one I will lose.",
                    "label": 0
                },
                {
                    "sent": "I will use when I will be talking about this square mcquestion.",
                    "label": 0
                },
                {
                    "sent": "So of course in some other cases you may want to take the largest the widest possible side, which would be this one, which is also possible choice.",
                    "label": 0
                },
                {
                    "sent": "So so increasing, but it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's closer to the original random variable you want to grasp, but but it's not bounded and in some situations it's better to threshold as much as as possible while keeping the properties we want to keep.",
                    "label": 0
                },
                {
                    "sent": "And of course, the interesting thing about choosing.",
                    "label": 0
                },
                {
                    "sent": "No site to be monotonic is is that now we've got interesting mounts.",
                    "label": 0
                },
                {
                    "sent": "At least in this case.",
                    "label": 0
                },
                {
                    "sent": "So now we know that with probability.",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "Does this mean?",
                    "label": 0
                },
                {
                    "sent": "This will hold with probability 1 minus epsilon.",
                    "label": 0
                },
                {
                    "sent": "For any fixed values of feature one and feature 2.",
                    "label": 0
                },
                {
                    "sent": "I've got bounce involving the variance.",
                    "label": 0
                },
                {
                    "sent": "And the level of confidence.",
                    "label": 0
                },
                {
                    "sent": "I7 strict if I want.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now I can define.",
                    "label": 0
                },
                {
                    "sent": "An estimator.",
                    "label": 0
                },
                {
                    "sent": "Optimine by.",
                    "label": 0
                },
                {
                    "sent": "Just putting.",
                    "label": 0
                },
                {
                    "sent": "Choosing feature had such bad.",
                    "label": 0
                },
                {
                    "sent": "I think the head is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "And then if I choose feature one and feature two such that this bound B1.",
                    "label": 0
                },
                {
                    "sent": "Is less.",
                    "label": 0
                },
                {
                    "sent": "Lesser or equal to 0, and if I choose feeder to such that B2, here is large event 0 band.",
                    "label": 0
                },
                {
                    "sent": "Then I will know that with probability 1 -- 2 epsilons.",
                    "label": 0
                },
                {
                    "sent": "It's also if if I define.",
                    "label": 0
                },
                {
                    "sent": "51 and 52 by B 151 = B two 52 = 0.",
                    "label": 0
                },
                {
                    "sent": "Then I will know that.",
                    "label": 0
                },
                {
                    "sent": "And we have.",
                    "label": 0
                },
                {
                    "sent": "Bounce 4 feet ahead with probability.",
                    "label": 0
                },
                {
                    "sent": "1 -- 2 absence.",
                    "label": 0
                },
                {
                    "sent": "So you can solve this by choosing.",
                    "label": 0
                },
                {
                    "sent": "The appropriate value of Alpha an.",
                    "label": 0
                },
                {
                    "sent": "So in the case when you know the variance, you end up with the following.",
                    "label": 0
                },
                {
                    "sent": "Not a simple rebound.",
                    "label": 0
                },
                {
                    "sent": "So you've got to consider.",
                    "label": 0
                },
                {
                    "sent": "Muse.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I mean.",
                    "label": 0
                },
                {
                    "sent": "Still, is just the solution of our feet ahead, yeah?",
                    "label": 0
                },
                {
                    "sent": "But the thing that holds with probability 1 -- 2 epsilon is that the real meaning is in the center.",
                    "label": 0
                },
                {
                    "sent": "No, it's fitted hat and then we.",
                    "label": 0
                },
                {
                    "sent": "By solving this equation you get that feature one is equal to M. Minus something.",
                    "label": 0
                },
                {
                    "sent": "The solution has this form.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With with with equal to what I'm going to put here.",
                    "label": 0
                },
                {
                    "sent": "Alright, because here you have only deterministic quantities in B1 and B2.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So when I will solve B1 of 51 = 0, I will obtain 51 as a function of MNV.",
                    "label": 0
                },
                {
                    "sent": "Why choosing?",
                    "label": 0
                },
                {
                    "sent": "And optimize value of Alpha.",
                    "label": 0
                },
                {
                    "sent": "So here we get.",
                    "label": 0
                },
                {
                    "sent": "2V epsilon minus one.",
                    "label": 0
                },
                {
                    "sent": "Over N. Times 1 -- 2.",
                    "label": 0
                },
                {
                    "sent": "Over N. Something obvious, sorry.",
                    "label": 0
                },
                {
                    "sent": "If it's been one of 51 equals in minus 8A and B 250 = N + A to know it's feature one, sorry.",
                    "label": 0
                },
                {
                    "sent": "It's we solve for this equal to 0 an.",
                    "label": 0
                },
                {
                    "sent": "In the middle term?",
                    "label": 0
                },
                {
                    "sent": "Or do you mean the entire question?",
                    "label": 0
                },
                {
                    "sent": "This entire left right hand side.",
                    "label": 0
                },
                {
                    "sent": "Sorry, maybe I'm not that big.",
                    "label": 0
                },
                {
                    "sent": "I called this base left hand side B1 of feta one and I called this.",
                    "label": 0
                },
                {
                    "sent": "This right hand side.",
                    "label": 0
                },
                {
                    "sent": "Be 2 feet 2.",
                    "label": 0
                },
                {
                    "sent": "Then I solved this to be equal to 0, because if because I know that small R is increases, decreasing and feta.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so far has been.",
                    "label": 0
                },
                {
                    "sent": "Yeah here you've got to choose this value.",
                    "label": 0
                },
                {
                    "sent": "This is of course not true for any value of Alpha.",
                    "label": 0
                },
                {
                    "sent": "This is true for this value.",
                    "label": 0
                },
                {
                    "sent": "So if here I assume, but I know the variance or at least a bound on the variance setting, which I called V. If I I know VI can choose this value of ether here, which depends only on the epsilon and N and this value of Alpha.",
                    "label": 0
                },
                {
                    "sent": "And if I choose these values then then the solution of these two equations are M -- 8 M plus eater.",
                    "label": 0
                },
                {
                    "sent": "Mansan as the influence function has been chosen to be.",
                    "label": 0
                },
                {
                    "sent": "A size increasing the influence function.",
                    "label": 0
                },
                {
                    "sent": "There are sorry, the empirical risk here is decreasing.",
                    "label": 0
                },
                {
                    "sent": "So I so so if I have this.",
                    "label": 0
                },
                {
                    "sent": "So I did use this from the fact that the.",
                    "label": 0
                },
                {
                    "sent": "Our official minus with you again tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe they did the thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No sorry, it's maybe I.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                },
                {
                    "sent": "R is negative.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no one is negative, so I'm making a mistake here.",
                    "label": 0
                },
                {
                    "sent": "I guess it's this.",
                    "label": 0
                },
                {
                    "sent": "An endless.",
                    "label": 0
                },
                {
                    "sent": "Answer So it's possible to sort what I wanted to say is that you using this kind of influence function, it's possible to get bounds that are under weak assumptions, because here we only assume that the variance is finite an here we assume that it is known and we get a bound.",
                    "label": 0
                },
                {
                    "sent": "But up to this correction term here, which is not the big is, the band would have obtained for a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If we had assumed why I to be Gaussian an if we have had used an exponential the next day, exponential moments of Goshen to obtain a deviation inequality.",
                    "label": 0
                },
                {
                    "sent": "And then if the variance is not known one can I use lipsky's method to adapt to the variance or make or we can also make up hypothesis?",
                    "label": 0
                },
                {
                    "sent": "Under kurtosis of the distribution, to be able to estimate the variance.",
                    "label": 0
                },
                {
                    "sent": "But I will not develop is the only thing I wanted to say here is that it's possible to modify exponential inequalities to get results for random variables that have no exponential moment under week, week hypothesis and I can show you the curve if you want.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to generate?",
                    "label": 0
                },
                {
                    "sent": "It doesn't change the fact that the it's possible to generalize to independent non.",
                    "label": 0
                },
                {
                    "sent": "Identically distributed.",
                    "label": 0
                },
                {
                    "sent": "Variables in a straightforward way.",
                    "label": 0
                },
                {
                    "sent": "Other details.",
                    "label": 0
                },
                {
                    "sent": "Around him, feeder hat is an estimator of M feeder hands.",
                    "label": 0
                },
                {
                    "sent": "It is computed from the observations.",
                    "label": 0
                },
                {
                    "sent": "By solving smoke, small outfitter hat equals 0.",
                    "label": 0
                },
                {
                    "sent": "Other small deviation and now.",
                    "label": 0
                },
                {
                    "sent": "Is it understand what this tells me about the original problem?",
                    "label": 0
                },
                {
                    "sent": "The original the question I'm addressing right now is how to estimate the mean of a real valued random variable under the assumption that the variance is known and finite.",
                    "label": 0
                },
                {
                    "sent": "And what I want to point out is that if you use the empirical mean, the usual estimate.",
                    "label": 0
                },
                {
                    "sent": "You will have a deviations much larger than the Gaussian deviation.",
                    "label": 0
                },
                {
                    "sent": "In the worst case, whereas if you use a threshold estimator in the in the in the way it is done in rubber statistics but with a different special choice of inference function and a special choice of Alpha people from rubber statistic would not consider such a small Alpha.",
                    "label": 0
                },
                {
                    "sent": "Here Alpha is decreasing as one of square root of N, which means that we are going to threshold only very large.",
                    "label": 0
                },
                {
                    "sent": "Outliers.",
                    "label": 0
                },
                {
                    "sent": "Of why?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The influence function which is here.",
                    "label": 0
                },
                {
                    "sent": "Is thresholding outliers from the mean or from the estimate feet ahead of them in an here as the scale parameter Alpha is decreasing at like 1 / sqrt N we will only threshold really large.",
                    "label": 0
                },
                {
                    "sent": "Outliers.",
                    "label": 0
                },
                {
                    "sent": "So now yeah.",
                    "label": 0
                },
                {
                    "sent": "That we may have these very large outliers, but we can't have too many of them because the variance is small.",
                    "label": 0
                },
                {
                    "sent": "And so yeah.",
                    "label": 0
                },
                {
                    "sent": "That is an unbiased estimator of them, but rather that it it has a small deviation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm looking at the confidence interval.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to build an estimate of aminat at confidence level epsilon 1 -- 2 epsilon.",
                    "label": 0
                },
                {
                    "sent": "Write an.",
                    "label": 0
                },
                {
                    "sent": "And what I obtain is, is it so it's a true Nana, synthetic confidence interval?",
                    "label": 0
                },
                {
                    "sent": "And if you look here here, I've plotted the confidence interval in the case when Y is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So it's just the inverse of the distribution of the inverse of the repetition and the distribution function of the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "And here this is the bound.",
                    "label": 0
                },
                {
                    "sent": "This is the address I obtained by by this.",
                    "label": 0
                },
                {
                    "sent": "Robust estimate of the mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the same.",
                    "label": 0
                },
                {
                    "sent": "The same variance here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what you can see is that for for account fitness level of 98% here.",
                    "label": 0
                },
                {
                    "sent": "We get a confidence interval which is of the same.",
                    "label": 0
                },
                {
                    "sent": "Order of magnitude, as in the Gaussian case, but in a much larger model.",
                    "label": 0
                },
                {
                    "sent": "Because we don't, we don't make any assumption and on the shape of the distribution, we just assume that the variance is known an equal to 1 here and then if you plot.",
                    "label": 0
                },
                {
                    "sent": "Upper and lower bounds for the empirical mean estimator.",
                    "label": 0
                },
                {
                    "sent": "The usual estimator.",
                    "label": 0
                },
                {
                    "sent": "When you take the arithmetic means of the Yi.",
                    "label": 0
                },
                {
                    "sent": "You see that at this confidence level, the lower bound.",
                    "label": 0
                },
                {
                    "sent": "This is a lower bound the load is already blowing up.",
                    "label": 0
                },
                {
                    "sent": "So even for a four, for a sample of size 100.",
                    "label": 0
                },
                {
                    "sent": "If you want to get a confidence interval at confidence level 98%.",
                    "label": 0
                },
                {
                    "sent": "You have a benefit in thresholding.",
                    "label": 0
                },
                {
                    "sent": "The usual empirical mean, at least in the worst case.",
                    "label": 1
                },
                {
                    "sent": "That is why when when the the tail is is as heavy as possible when the variance is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So an.",
                    "label": 0
                },
                {
                    "sent": "One other interesting thing is about the bound here.",
                    "label": 1
                },
                {
                    "sent": "Behaves as in the Gaussian case.",
                    "label": 0
                },
                {
                    "sent": "It is we've it's in square root of log of epsilon minus one, even for very large confidence levels we can go up to.",
                    "label": 0
                },
                {
                    "sent": "1 -- 210 to the power minus 8 and we still follow the order of magnitude that we would have obtained for the Goshen, although we include in the model distributions with much heavier tails.",
                    "label": 0
                },
                {
                    "sent": "It seems that to compute that fee to Cesar Pizza hat equals zero, yeah?",
                    "label": 0
                },
                {
                    "sent": "No and no.",
                    "label": 0
                },
                {
                    "sent": "You don't need to know him because M is not in.",
                    "label": 0
                },
                {
                    "sent": "Is the classical trick of Robert Statistiques.",
                    "label": 0
                },
                {
                    "sent": "She is.",
                    "label": 0
                },
                {
                    "sent": "On the variance, you essentially obtain the downsize, the load, the lower and upper bounds for the mean for empirical mean, but I plotted.",
                    "label": 0
                },
                {
                    "sent": "The curve I.",
                    "label": 0
                },
                {
                    "sent": "It's essentially the championship bound is essentially reached.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this doesn't.",
                    "label": 0
                },
                {
                    "sent": "This doesn't depend on on on.",
                    "label": 0
                },
                {
                    "sent": "An app.",
                    "label": 0
                },
                {
                    "sent": "So now let me talk about.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Regression problem.",
                    "label": 0
                },
                {
                    "sent": "Can I come back to?",
                    "label": 0
                },
                {
                    "sent": "The quadratic risk.",
                    "label": 0
                },
                {
                    "sent": "An feature zero will be.",
                    "label": 0
                },
                {
                    "sent": "The best possible.",
                    "label": 0
                },
                {
                    "sent": "Choice of parameter in.",
                    "label": 0
                },
                {
                    "sent": "In the parameter space filter, which is going to be a convex bounded convex set in Rd.",
                    "label": 0
                },
                {
                    "sent": "High recall that I'm losing, sorry.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "I want to solve for this, but I don't know and the first thing and OK.",
                    "label": 0
                },
                {
                    "sent": "So one thing we know is that the behavior of the empirical.",
                    "label": 0
                },
                {
                    "sent": "Process depends on the covariance structure, so relevant trying to solve this problem I will.",
                    "label": 0
                },
                {
                    "sent": "I will write this.",
                    "label": 0
                },
                {
                    "sent": "As a minimax problem.",
                    "label": 0
                },
                {
                    "sent": "Sure, if I got here.",
                    "label": 0
                },
                {
                    "sent": "So it's the couple, so solving this is just solving twice this thing, right?",
                    "label": 0
                },
                {
                    "sent": "So now I can think about.",
                    "label": 0
                },
                {
                    "sent": "Solving the same min Max problem with but with with an empirical version of this thing.",
                    "label": 0
                },
                {
                    "sent": "But I don't know.",
                    "label": 0
                },
                {
                    "sent": "And this empirical in this empirical.",
                    "label": 0
                },
                {
                    "sent": "Counterpart of the risk of a difference of risks, I will introduce a coupling through the use of an influence function.",
                    "label": 0
                },
                {
                    "sent": "WY is.",
                    "label": 0
                },
                {
                    "sent": "Chest.",
                    "label": 0
                },
                {
                    "sent": "The difference of Alas is.",
                    "label": 0
                },
                {
                    "sent": "At Point XIY I4 parameters feature one and feature 2.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course.",
                    "label": 0
                },
                {
                    "sent": "If I'm choosing.",
                    "label": 0
                },
                {
                    "sent": "This empirical quantity is because of the exponential bound it can get.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So when I write W, it means that it's W4 for an independent couple X&Y from from from the sample.",
                    "label": 0
                },
                {
                    "sent": "And so I will be able at least for fixed values of 51 and 52, to estimate the difference of a risk function in 51 and 52 an.",
                    "label": 0
                },
                {
                    "sent": "I will have a variance only variance term here.",
                    "label": 0
                },
                {
                    "sent": "A term of including moment of order two of.",
                    "label": 0
                },
                {
                    "sent": "I after the last time I'm considering here so I will not need exponential moment assumptions on the noise and I don't make any hypothesis under nice structure here except for the fact that I assume the.",
                    "label": 0
                },
                {
                    "sent": "I am why are independent and the simplest case IID?",
                    "label": 0
                },
                {
                    "sent": "I don't assume with my eyes F of XI plus plus noise.",
                    "label": 0
                },
                {
                    "sent": "Only, but we have a giant distribution.",
                    "label": 0
                },
                {
                    "sent": "Neubauer, an I will have some some some polynomial moment assumption only.",
                    "label": 0
                },
                {
                    "sent": "So now the estimator is going to be.",
                    "label": 0
                },
                {
                    "sent": "Computed by.",
                    "label": 0
                },
                {
                    "sent": "Minimizing the supremum in fear 2.",
                    "label": 0
                },
                {
                    "sent": "I've.",
                    "label": 0
                },
                {
                    "sent": "Of this OK, so I'm doing the same thing as here, but here it's a small complicated because we've got a coupling due to the fact that we threshold launch outliers using this influence function PSI in order to get rid of exponential moment assumptions.",
                    "label": 0
                },
                {
                    "sent": "So now what is going to be the?",
                    "label": 0
                },
                {
                    "sent": "Add.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The general idea of the.",
                    "label": 0
                },
                {
                    "sent": "The proof for the event of a generalization bound to obtain a generalization bound for feed ahead.",
                    "label": 0
                },
                {
                    "sent": "We're going to write this.",
                    "label": 0
                },
                {
                    "sent": "We've got a supremum.",
                    "label": 0
                },
                {
                    "sent": "Have our primer fitted hat.",
                    "label": 0
                },
                {
                    "sent": "Theater 2, which certainly.",
                    "label": 0
                },
                {
                    "sent": "Is less.",
                    "label": 0
                },
                {
                    "sent": "Man, the supremum in theater, two of our prime of feature Zero Fear 2.",
                    "label": 0
                },
                {
                    "sent": "And by definition of a fit, had MRSA.",
                    "label": 0
                },
                {
                    "sent": "Morvin which is itself equal.",
                    "label": 0
                },
                {
                    "sent": "It is symmetric symmetric, so our prime is symmetrical, so.",
                    "label": 0
                },
                {
                    "sent": "So now if I succeed in proving that this supremum and feature 2.",
                    "label": 0
                },
                {
                    "sent": "Of our prime of features, your filter is a deterministic parameters, it's it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's the optimal value of.",
                    "label": 0
                },
                {
                    "sent": "Outfitter.",
                    "label": 0
                },
                {
                    "sent": "It's a minimizers of minimizer true risk, so if I can prove that this is less than.",
                    "label": 0
                },
                {
                    "sent": "The supremum of.",
                    "label": 0
                },
                {
                    "sent": "Are official.",
                    "label": 0
                },
                {
                    "sent": "Note minus R 52.",
                    "label": 0
                },
                {
                    "sent": "Plus some small.",
                    "label": 0
                },
                {
                    "sent": "Reminder.",
                    "label": 0
                },
                {
                    "sent": "And here if I can prove.",
                    "label": 0
                },
                {
                    "sent": "But this is not driven.",
                    "label": 0
                },
                {
                    "sent": "I will have what I want I want.",
                    "label": 0
                },
                {
                    "sent": "OK, because.",
                    "label": 0
                },
                {
                    "sent": "I've got a chain of inequalities.",
                    "label": 0
                },
                {
                    "sent": "This thing is going to cancel with.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "This one is by construction of Fingerhut.",
                    "label": 0
                },
                {
                    "sent": "Because I take the the minimum here, so it's less than what I got for the optimal value of feature 0.",
                    "label": 0
                },
                {
                    "sent": "So if I can have a uniform control of our primary feature, 0 feet are too uniform, inferior to an I. I'm able to prove that this is less than what I get for the excess.",
                    "label": 0
                },
                {
                    "sent": "Nasty accessories plus small remainder.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to conclude in two steps, the first step.",
                    "label": 0
                },
                {
                    "sent": "The first step is obtained by saying that this is a. OK, this is certainly this is Mrs as features heroes that the minimum on feature of this is positive also.",
                    "label": 0
                },
                {
                    "sent": "So now if I consider the dark.",
                    "label": 0
                },
                {
                    "sent": "So as it is positive, I will I will have.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If I consider here as fear to the argmax of this, I will have a control of.",
                    "label": 0
                },
                {
                    "sent": "I will be able to bound.",
                    "label": 0
                },
                {
                    "sent": "This right by by putting this in the.",
                    "label": 0
                },
                {
                    "sent": "The other way around.",
                    "label": 0
                },
                {
                    "sent": "Well, what the?",
                    "label": 0
                },
                {
                    "sent": "An after.",
                    "label": 0
                },
                {
                    "sent": "Sorry the the shortest way to explain this is that, as with this quantity is less than zero.",
                    "label": 0
                },
                {
                    "sent": "I've got only the small remainder here.",
                    "label": 0
                },
                {
                    "sent": "And then so if I have this chain of inequalities with this proof that are fitted, hat is less than our future 0 plus the sum of the two small remainders, right?",
                    "label": 0
                },
                {
                    "sent": "So what I need to do to get a generalization bound for fitted hat is uniform control in theater two of our primer feeder zero 52 because here it's the same sort of quantity event that appears.",
                    "label": 0
                },
                {
                    "sent": "So it's this uniform control is going to be obtained by using pack by pack buys.",
                    "label": 0
                },
                {
                    "sent": "Technology.",
                    "label": 0
                },
                {
                    "sent": "Care about.",
                    "label": 0
                },
                {
                    "sent": "Suppose that the end is attained.",
                    "label": 0
                },
                {
                    "sent": "An argument with the thing because we are.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, but I don't need to to solve exactly.",
                    "label": 0
                },
                {
                    "sent": "I can solve up to some precision and precision is going to be put here.",
                    "label": 0
                },
                {
                    "sent": "Thanks, I wrote it like this because simply it's easier but but if we solve the min Max problem with some accuracy gamma we can just add the accuracy.",
                    "label": 0
                },
                {
                    "sent": "I could I could, yeah.",
                    "label": 0
                },
                {
                    "sent": "It's it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's easy to see here.",
                    "label": 0
                },
                {
                    "sent": "You have to add that it's plus.",
                    "label": 0
                },
                {
                    "sent": "Officially yeah OK.",
                    "label": 0
                },
                {
                    "sent": "So as already explained, and So what we we know is how to bound.",
                    "label": 0
                },
                {
                    "sent": "Small perturbations.",
                    "label": 0
                },
                {
                    "sent": "So if I integrate with respect to a posterior role filter 2.",
                    "label": 0
                },
                {
                    "sent": "I will be able to use Backpage and bounce to bound.",
                    "label": 0
                },
                {
                    "sent": "This quantity.",
                    "label": 0
                },
                {
                    "sent": "And a.",
                    "label": 0
                },
                {
                    "sent": "No, I need to do something to be able to.",
                    "label": 0
                },
                {
                    "sent": "To work with.",
                    "label": 0
                },
                {
                    "sent": "To have a bound by this uniform in Figure 2.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do here is to use the following simple.",
                    "label": 0
                },
                {
                    "sent": "Inequality.",
                    "label": 0
                },
                {
                    "sent": "Says my inference function and I'm going to compare.",
                    "label": 0
                },
                {
                    "sent": "So in fact unfortunately, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not convex.",
                    "label": 0
                },
                {
                    "sent": "But what I can do is.",
                    "label": 0
                },
                {
                    "sent": "Use the following bound, which is obtained by adding to size something to turn it into a convex function.",
                    "label": 0
                },
                {
                    "sent": "I what I want to do is to pull the expectation with respect to the posterior outside of the influence function.",
                    "label": 0
                },
                {
                    "sent": "It would be easier if sign was convex, but it's not convex, but still it's possible to have an approximate inequality.",
                    "label": 0
                },
                {
                    "sent": "If I compare these two things, we can't be more than the total variations of PSI.",
                    "label": 0
                },
                {
                    "sent": "For sure.",
                    "label": 0
                },
                {
                    "sent": "OK, which is lock to lock to lock for.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Looking at the second derivative of Cyan, adding tips, I a quadratic function at Mega.",
                    "label": 0
                },
                {
                    "sent": "That makes the sum convex.",
                    "label": 0
                },
                {
                    "sent": "We can prove it here.",
                    "label": 0
                },
                {
                    "sent": "We have to consider the variance.",
                    "label": 0
                },
                {
                    "sent": "Of each.",
                    "label": 0
                },
                {
                    "sent": "So then.",
                    "label": 0
                },
                {
                    "sent": "When we can say that this is going to be less than the log of 1 + A.",
                    "label": 0
                },
                {
                    "sent": "Times the variance term, let's call it P. No, sorry it's great.",
                    "label": 0
                },
                {
                    "sent": "This is going to be less than bad for a.",
                    "label": 0
                },
                {
                    "sent": "Equal to print off a log.",
                    "label": 0
                },
                {
                    "sent": "For much less than two point.",
                    "label": 0
                },
                {
                    "sent": "17 OK.",
                    "label": 0
                },
                {
                    "sent": "So we can write this late.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "We've done some stuff.",
                    "label": 0
                },
                {
                    "sent": "We have some improvement, mainly up to a remainder, but it's going to be small.",
                    "label": 0
                },
                {
                    "sent": "This thing is not.",
                    "label": 0
                },
                {
                    "sent": "It is now independent from feeder two in good situations it may be a case that the variance good choices are perturbations can make the variance independent of feature 2, but if the variance still depends in theater 2, it's possible to use some tricks to get rid of it.",
                    "label": 0
                },
                {
                    "sent": "Along the following.",
                    "label": 0
                },
                {
                    "sent": "Idea?",
                    "label": 0
                },
                {
                    "sent": "So if here we have a growth say up order a times the.",
                    "label": 0
                },
                {
                    "sent": "The norm of a feature 2 to the power P we can just.",
                    "label": 0
                },
                {
                    "sent": "We can just live with P. It's possible to use some trick by, like for instance the following one.",
                    "label": 0
                },
                {
                    "sent": "What can write we can go back here?",
                    "label": 0
                },
                {
                    "sent": "And right this term here lies.",
                    "label": 0
                },
                {
                    "sent": "OK, I can certainly write.",
                    "label": 0
                },
                {
                    "sent": "I can bound bound my variance term here if it's if it's bounded like that, I can write it like right to bound like that and then I can say that this is less.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                },
                {
                    "sent": "Add 2 to the power P -- 1 times.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "Theater 2 minus feature P + 2 P minus one feature P. And then.",
                    "label": 0
                },
                {
                    "sent": "When I when I use the fact that the minimum of something an A+B is less than the minimum of something and a plus the minimum of something plus B when A&B are positive.",
                    "label": 0
                },
                {
                    "sent": "And I end up.",
                    "label": 0
                },
                {
                    "sent": "By being less fan.",
                    "label": 0
                },
                {
                    "sent": "So this is the nice thing for us.",
                    "label": 0
                },
                {
                    "sent": "And here I put the expectation inside.",
                    "label": 0
                },
                {
                    "sent": "And then I can reasonably think about the partitions for which sorry it's two here.",
                    "label": 0
                },
                {
                    "sent": "For which to this this moment, this centered moments is independent of fear 2.",
                    "label": 0
                },
                {
                    "sent": "It's OK.",
                    "label": 0
                },
                {
                    "sent": "This is what you want.",
                    "label": 0
                },
                {
                    "sent": "Because you are interested in supremo, so why do you take the averaging?",
                    "label": 0
                },
                {
                    "sent": "On the posterior because they want to apply a pack base, I want to play a pack bias bound in which I will have.",
                    "label": 0
                },
                {
                    "sent": "To control sorry, I will have the code Black Library divergents of role theory two with respect to some reference function pie and if I in this continuous setting where the parameters are continuous, if I choose to direct mass inferior two, this term is going to to blue.",
                    "label": 0
                },
                {
                    "sent": "Blew up.",
                    "label": 0
                },
                {
                    "sent": "That's why so.",
                    "label": 0
                },
                {
                    "sent": "The idea is that the following you want to approximate.",
                    "label": 0
                },
                {
                    "sent": "What is going on in Theater 2?",
                    "label": 0
                },
                {
                    "sent": "You're going to add a small perturbations to feature 2.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That that's that's the idea.",
                    "label": 0
                },
                {
                    "sent": "So using using this ideas, I end up with the following kind of inequality.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to use as already advocated in in previous talks a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Yes, a Gaussian for traditions here and I'm going to apply this to H equal to WY.",
                    "label": 0
                },
                {
                    "sent": "Www.i and if I use a Gaussian perturbation of a feature two, I'm going to have an explicit computation of the perturbation here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to write.",
                    "label": 0
                },
                {
                    "sent": "I'm going to choose World Theatre 2.",
                    "label": 0
                },
                {
                    "sent": "Proportional to the Gaussian distribution center in feature 2.",
                    "label": 0
                },
                {
                    "sent": "Which parameter beta Peter time identity?",
                    "label": 0
                },
                {
                    "sent": "An when I choose this, I will choose the reference measure.",
                    "label": 0
                },
                {
                    "sent": "To be the Gaussian centered.",
                    "label": 1
                },
                {
                    "sent": "At the optimal parameter, I want to estimate.",
                    "label": 0
                },
                {
                    "sent": "Mike and when I do this.",
                    "label": 0
                },
                {
                    "sent": "I end up with.",
                    "label": 0
                },
                {
                    "sent": "So the diversions here.",
                    "label": 0
                },
                {
                    "sent": "Is going to be equal to?",
                    "label": 0
                },
                {
                    "sent": "Then the quadratic norm.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to be able to write.",
                    "label": 0
                },
                {
                    "sent": "So the quantity I'm interested in is why I features Hero feature two and this can be written as.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So this is independent from filter, so I can I can integrate like this.",
                    "label": 0
                },
                {
                    "sent": "So I write this like this.",
                    "label": 0
                },
                {
                    "sent": "I'm using this identity here.",
                    "label": 0
                },
                {
                    "sent": "And so this will give me.",
                    "label": 0
                },
                {
                    "sent": "An exponential.",
                    "label": 0
                },
                {
                    "sent": "A control on the exponential moment of our prime of feature zero 52.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So I'm getting the.",
                    "label": 0
                },
                {
                    "sent": "For like inbound.",
                    "label": 0
                },
                {
                    "sent": "So with probability 1 minus epsilon.",
                    "label": 0
                },
                {
                    "sent": "So the main terms here are.",
                    "label": 0
                },
                {
                    "sent": "These ones.",
                    "label": 0
                },
                {
                    "sent": "Plus this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the entropy term and we have to.",
                    "label": 0
                },
                {
                    "sent": "Confidence term here plus a remainder which is going to be.",
                    "label": 0
                },
                {
                    "sent": "Office kind.",
                    "label": 0
                },
                {
                    "sent": "So it's the expectation.",
                    "label": 0
                },
                {
                    "sent": "And we have some some quantities here, but.",
                    "label": 0
                },
                {
                    "sent": "Everything is explicit.",
                    "label": 0
                },
                {
                    "sent": "The constant are numerical constants.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, disease.",
                    "label": 0
                },
                {
                    "sent": "First setra.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                },
                {
                    "sent": "This is what we get.",
                    "label": 0
                },
                {
                    "sent": "Set up what we can do here so.",
                    "label": 0
                },
                {
                    "sent": "Now we can exploit effect that we don't use.",
                    "label": 0
                },
                {
                    "sent": "The perturbation in the in the computation of the estimator in the computation of the estimator, we only need to know the scalar product between feature one feature and X and XI.",
                    "label": 1
                },
                {
                    "sent": "And if we assume that the gram matrix is the identity, then we end up with the fact that.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Met feature 0 minus feta is less.",
                    "label": 0
                },
                {
                    "sent": "When our future minus our filter.",
                    "label": 0
                },
                {
                    "sent": "0.",
                    "label": 0
                },
                {
                    "sent": "OK. Because of convexity.",
                    "label": 0
                },
                {
                    "sent": "It's equal if it.",
                    "label": 0
                },
                {
                    "sent": "If it features roses within, sorry.",
                    "label": 0
                },
                {
                    "sent": "Let me explain this.",
                    "label": 0
                },
                {
                    "sent": "And there is a square here.",
                    "label": 0
                },
                {
                    "sent": "Amigos.",
                    "label": 0
                },
                {
                    "sent": "Now if I define.",
                    "label": 0
                },
                {
                    "sent": "Feather star as the argument over ID of all.",
                    "label": 0
                },
                {
                    "sent": "Alright then.",
                    "label": 0
                },
                {
                    "sent": "I can write a feature as our feeling.",
                    "label": 0
                },
                {
                    "sent": "Stop sorry, orifices star plus.",
                    "label": 0
                },
                {
                    "sent": "When the gram matrix is the identity, it's it's.",
                    "label": 0
                },
                {
                    "sent": "If it's not died until you have to, because this is the.",
                    "label": 0
                },
                {
                    "sent": "This is a quadratic quadratic form, so you have only the quadratic term remaining when you start from the optimum here.",
                    "label": 0
                },
                {
                    "sent": "And then when you consider the.",
                    "label": 0
                },
                {
                    "sent": "The optimum in the convex it's it's the projection of a future star.",
                    "label": 0
                },
                {
                    "sent": "Here an and so.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at the difference of risk between feature and feature zero, it will be you will have this inequality by using the fact that feature is is convex.",
                    "label": 0
                },
                {
                    "sent": "Cancel.",
                    "label": 0
                },
                {
                    "sent": "I don't suppose it I if I want to write it properly, I put, I put.",
                    "label": 0
                },
                {
                    "sent": "I consider this change of notations, right?",
                    "label": 0
                },
                {
                    "sent": "An I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't need to observe any part of the perturbations.",
                    "label": 0
                },
                {
                    "sent": "All the things I'm using here, I don't need to observe them because we are used only in the computations.",
                    "label": 0
                },
                {
                    "sent": "We are not used in the definition of the estimator, which is defined by solving a minimax problem for our prime.",
                    "label": 0
                },
                {
                    "sent": "Look at your data.",
                    "label": 0
                },
                {
                    "sent": "It's only in the computations, but I can.",
                    "label": 0
                },
                {
                    "sent": "Make this change of notation.",
                    "label": 0
                },
                {
                    "sent": "I make this change of notation in the computations only and I choose the perturbation.",
                    "label": 0
                },
                {
                    "sent": "With respect to the service means that in in the original setting this would be the the Gaussian variable with with with with covariance matrix equal to the gram matrix.",
                    "label": 0
                },
                {
                    "sent": "OK. Maybe I should not have said that, but I can assume by the gram matrix is the identity.",
                    "label": 0
                },
                {
                    "sent": "I just do this change of notation I make all the computations.",
                    "label": 0
                },
                {
                    "sent": "In this with this notations and this definition for the changed after the change of basis right?",
                    "label": 0
                },
                {
                    "sent": "Apply also.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "My display to display.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yes, it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's the it's the yeah I should have write in like this, yeah.",
                    "label": 0
                },
                {
                    "sent": "Is it because it's it's just to try to simplify the.",
                    "label": 0
                },
                {
                    "sent": "What I can say is, but without loss of generality I can assume by the gram matrix is the identity.",
                    "label": 0
                },
                {
                    "sent": "But if I if you want to write things in details you you may consider the change annotations and and and play with two different norms.",
                    "label": 0
                },
                {
                    "sent": "The norm I've got to consider here is the normal associated with the crime metametrics.",
                    "label": 0
                },
                {
                    "sent": "In order to have a comparison between the Norman and the excess risk.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now if I consider feature to be.",
                    "label": 0
                },
                {
                    "sent": "Oh now if you were to, is that if the argmax here is so, this is, this is not.",
                    "label": 0
                },
                {
                    "sent": "This is this is this is positive, and so I've got.",
                    "label": 0
                },
                {
                    "sent": "So this here the first term is is this one.",
                    "label": 0
                },
                {
                    "sent": "And this means that I have a control in for well chosen values of Alpha, so I can express all these things in terms of parameters and of the distance between feature zero and feet or two, and I will get a bound for.",
                    "label": 0
                },
                {
                    "sent": "For distance between feature 0 and 52, which is going to give me also bound for the excess risk of fear two with respect to feature zero and then going back?",
                    "label": 0
                },
                {
                    "sent": "And doing doing it once again as I showed I doing the same applying the same thing to fit in hand.",
                    "label": 0
                },
                {
                    "sent": "I will end with generalization bound for future hat of the same order an.",
                    "label": 0
                },
                {
                    "sent": "What is interesting here is mad.",
                    "label": 0
                },
                {
                    "sent": "Is going to be expressed.",
                    "label": 0
                },
                {
                    "sent": "Using only a limited number of quantities, the bandwidth will make use of the only for the following.",
                    "label": 0
                },
                {
                    "sent": "Quantities.",
                    "label": 0
                },
                {
                    "sent": "So I will not detail about, I will just say that we get a pounding.",
                    "label": 0
                },
                {
                    "sent": "The overhead depending on the following constants.",
                    "label": 0
                },
                {
                    "sent": "Which is it cold today here?",
                    "label": 0
                },
                {
                    "sent": "So we have only to consider some kurtosis like.",
                    "label": 0
                },
                {
                    "sent": "Parameters here.",
                    "label": 0
                },
                {
                    "sent": "So basically this is the variance.",
                    "label": 0
                },
                {
                    "sent": "For optimal parameter an K. Written like this, it can be written in any basis.",
                    "label": 0
                },
                {
                    "sent": "If you write this.",
                    "label": 0
                },
                {
                    "sent": "And in a base it is also orthogonal for metric defined wunderground metric this is equal to 1 and this is equal to D. Did I mention?",
                    "label": 0
                },
                {
                    "sent": "So, so this means that we can have the over and bounds.",
                    "label": 0
                },
                {
                    "sent": "Without directly having assumptions on the gram metrics.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "For instance, if the design is Gaussian, then the Scala review and X is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So certainly the kurtosis is is bounded by by free.",
                    "label": 0
                },
                {
                    "sent": "And you have no problems.",
                    "label": 0
                },
                {
                    "sent": "And when you consider linear regressions in basis of functions, you can bound this kurtosis coefficient here by considering.",
                    "label": 0
                },
                {
                    "sent": "Almost orthogonality assumptions on the basis also, but I think it's interesting to see that.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis you have two 2.",
                    "label": 0
                },
                {
                    "sent": "To consider on the basis of function just.",
                    "label": 0
                },
                {
                    "sent": "Used to bound this with this kind of quantity.",
                    "label": 0
                },
                {
                    "sent": "And for instance, when when when the Gaussian, it's trivial to see, even though the culture this is bounded here.",
                    "label": 0
                },
                {
                    "sent": "And so it's a way to we can considerably hypothesis needed to obtain a DD of N bound for the quadratic regression case.",
                    "label": 0
                },
                {
                    "sent": "And as it is a mainstream problem, I think.",
                    "label": 0
                },
                {
                    "sent": "And also I think that this program can be applied in different situations for other loss functions and so the idea is that you will if you want to study an M estimator, you are going to consider a small perturbation of this M estimator.",
                    "label": 0
                },
                {
                    "sent": "You are going to replace feature hat by some perturbation here.",
                    "label": 0
                },
                {
                    "sent": "A feature had some rule hat just perturbation figurehead.",
                    "label": 0
                },
                {
                    "sent": "And you are going to choose the right tradeoff between the size of the perturbation.",
                    "label": 0
                },
                {
                    "sent": "And the size of the entropy term.",
                    "label": 0
                },
                {
                    "sent": "An if you make the right tradeoff, you will obtain bounce and assumptions that are interesting because this technology of course is replacing the use of a concentration inequality for the supremum of the empirical process, and it gives explicit bounds OK if I have only one minute, I will.",
                    "label": 0
                },
                {
                    "sent": "And we've just a remark.",
                    "label": 0
                },
                {
                    "sent": "It is that if you if you consider some estimate, some randomized estimator rohat, you can ask the question of.",
                    "label": 0
                },
                {
                    "sent": "Choosing the best possible reference prior Pi and in expectation it's trivial to see that.",
                    "label": 0
                },
                {
                    "sent": "I'm sure many of you are aware of that, but.",
                    "label": 0
                },
                {
                    "sent": "The optimal so if we if we reverse the usual point of view, that is, I choose a prior and when I try to optimize the posterior, knowing the bound, if I start with the posterior and I'm asking what is the best prior in expectation, there is a clear cut answer which is.",
                    "label": 0
                },
                {
                    "sent": "The expectation of the posterior, which is a prior, of course, because I right.",
                    "label": 0
                },
                {
                    "sent": "From usual inequalities about the callback table, and this is precisely the mutual information.",
                    "label": 0
                },
                {
                    "sent": "Between the feet ahead and the sample.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that following the pack buys technology.",
                    "label": 0
                },
                {
                    "sent": "Allows you to have generalization bounds where the complexity of the randomized estimator defined by a posterior.",
                    "label": 0
                },
                {
                    "sent": "Is bounded by the mutual information between the the randomized between fetal heart, your estimator, and a sample.",
                    "label": 0
                },
                {
                    "sent": "So the randomization is here to make this mutual information lower, because for usual M estimate, M estimators and the continuous parameter set, if you don't do anything that mutual information is infinite.",
                    "label": 0
                },
                {
                    "sent": "But if you use a small perturbation Bender, the mutual information is becomes finite.",
                    "label": 0
                },
                {
                    "sent": "And this is some kind of done a synthetic replacement for the Fisher information if you want to send something like that, and it's I think it's a nice interpretation of what we are controlling here.",
                    "label": 0
                },
                {
                    "sent": "So it can be a way to distinguish pack bias approach from the bias approach, because when the sample size is going to be increased then.",
                    "label": 0
                },
                {
                    "sent": "The good posteriors are going to be more concentrated.",
                    "label": 0
                },
                {
                    "sent": "We are going to change the concentration of proof roof.",
                    "label": 0
                },
                {
                    "sent": "It is going to evolve with the sample size and show the optimal prior is going in.",
                    "label": 0
                },
                {
                    "sent": "This setting is going to depend on the sample size also and this is some difference with the usual.",
                    "label": 0
                },
                {
                    "sent": "The usual Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "You don't assume that the prior is depending on the sample size.",
                    "label": 0
                },
                {
                    "sent": "So so here we see that the.",
                    "label": 0
                },
                {
                    "sent": "Role played by the prior and the posterior in the pack by his approaches is not the same as in the beige, and the usual version setting.",
                    "label": 0
                },
                {
                    "sent": "I kissed him, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}