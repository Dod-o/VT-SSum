{
    "id": "bdxf5cc7hyjjitrgw6vx54aovxmvn6bs",
    "title": "Hierarchical POMDP Controller Optimization by Likelihood Maximization",
    "info": {
        "author": [
            "Marc Toussaint, Machine Learning and Intelligent Data Analysis Group, TU Berlin"
        ],
        "published": "July 30, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/uai08_toussaint_hpco/",
    "segmentation": [
        [
            "So first thank you for coming to the talk, and before I start, I definitely want to point out that this is a joint work with law and Pascal.",
            "So a lot of credit is actually to both guys who push this forward and push the idea forward and to start with as he could.",
            "As you can tell by the title of this talk.",
            "What this approach is trying to suggest is that it can use standard machine learning techniques like expectation maximization or likelihood maximization, and apply them in reinforcement learning.",
            "So my hope with this talk actually is that there's a couple of people here in the audience, maybe who are experts in inference techniques, maybe belief propagation or whatever, and well, here it can now learn how you can apply these techniques to solve harmony, peace, or derive optimal controllers in Markov decision process.",
            "So before I actually start talking about the.",
            "Like with maximization."
        ],
        [
            "The first part will just be a quick introduction of what the problem is that I'm facing, so the framework I'm working in is partially observable Markov decision process.",
            "So in comparison to the two talks that we had before.",
            "We we have an additional sign in on top of having a state space and action space and we have an agent interacting with the world by exchanging actions and on top of that we have an observation space and that means that the agent does not have a full information about the state it is in.",
            "Instead of that all it has is there exists observation probabilities that is depending on the state of the agent and the action that takes it receives a certain observation.",
            "Now that observation may be used in order to estimate the state.",
            "But it's not full information about what the status the rest of the palm DP is.",
            "As in market physician processes, we have a state transition probability.",
            "So when you are in the state and you take a certain action, then there is a probability of ending up in new state will start distribution for the agent might start and we have a reward function."
        ],
        [
            "So what the goal is, as in standard modification process or reinforce learning to maximize the expected discounted future return where these little ours here are kind of samples which have the expectation of the reward function.",
            "Um?"
        ],
        [
            "So there's many different approaches of how you could go about trying to solve problem piece, and one approach is based on actually.",
            "Keeping exact beliefs about the state you're in, but there is also like another approach which tries to find state machines which are called finite state controllers as an implementation of the policy in the palm DP.",
            "So let me let me explain these final state controllers are So what you see here?",
            "This diagram is actually a diagram of a state machine, not confused with a graphical model, right?",
            "So these things here are states of a state machine.",
            "And so final state controller.",
            "You initialize the finite state control in certain states.",
            "Let's say this note here.",
            "So these states of the controller, also called nodes.",
            "And whenever the final state controller is in a certain node and for this node we have storage probability vector, which says which kinds of actions are being emitted by this node.",
            "So here this offer says which kinds of actions are being triggered at this node when the control is selected, some action it.",
            "Well, the world state changes and we received some observation from the world, and this observation triggers probabilistic transition into a new node of the final state controller.",
            "For instance, over here, when we are in this new node, we again select some action from this distribution and so on.",
            "Now hierarchical state controllers have one additional interesting property and that is that these nodes here some of these nodes do not directly emit an action, but instead they sort of call a subroutine.",
            "So in this note, here is supposed to be an abstract node.",
            "It's not directly sampling in action and executing it, but instead of that it's sort of entering a different kind of finite state controller, which executes some stuff between.",
            "Until this finite state controller reaches some end node, and when this finite state control is done, reach the end node is sort of transition back and then go there.",
            "So this is a hierarchical final state controller and with was introduced in this paper by Hanson."
        ],
        [
            "The idea obviously is that these kinds of approaches solving palm DP's allow us to decompose the problem in sub task so it's the same as with Markov decision process options and things like that and one of the core problems however is discovering these hierarchical decomposition in the palm DP."
        ],
        [
            "I want to mention so did the starting point actually of this work?",
            "Was the paper by Law and Pascal and their colleagues in Waterloo where they for the first time actually proposed a method to discover these these hierarchies, so not only to presume a certain hierarchy and optimize the parameters of it, but discovering these hierarchies and their approach was actually quite complex in general optimization, frame it as a pure optimization problem and.",
            "Use standard optimizes for this."
        ],
        [
            "OK, so.",
            "We reframe this problem in a different view.",
            "You can express these kind of hierarchical processes in terms of dynamic vision networks and a very classical example for this hierarchical hidden Markov models.",
            "So in hierarchical hidden Markov models you have two sort of layers of Markovian chains of random variables, so this is a graphical model, right?",
            "It's a dynamic vision network and on one of these Markov chains, well, you have the standard transitions, just more coughing transitions and depending on the state of that you just image certain kinds of observations.",
            "Anne.",
            "If you however want is to sort of model with Markov models, processes like this here for instance, so did the process here.",
            "If you read it as ABC, DF, DF ABCD EF abcabc, you might think that modeling such kind of process would be much easier if you would to find out about the submodules in this process is.",
            "So what Murphy Murphy and Pascal showed is that you could reformulate hierarchical hidden Markov models with a dynamic Bayesian network where you have 2 Chainz.",
            "Well, in this case two chains of random variables and the important thing is that the lower level here is kind of the subroutine which might express these modules and their existing exit states.",
            "So whenever this subroutine is done or emits an exit state, then the higher level state might make in transition.",
            "If the exit state is 0, the higher level state is persistent.",
            "So the core idea is actually introducing these exit states, which constrain the high level process to be persistent while the lower process is still."
        ],
        [
            "Running and it's doing his job.",
            "We can exactly use the same kind of approach, the same kind of idea for representing the hierarchical finite state control for Palm DP's.",
            "So this is a two slice model of the palm DP that we're of the dynamic vision network that you're actually going to consider.",
            "It includes the state of the world down here and the transition probabilities given an action in a new state includes the probability of sampling and observation.",
            "And on top of here, this is the representation of the hierarchical policy that we're actually investigating, so we have.",
            "Different nodes, different latent variables of the controller on different levels.",
            "So here this might be nodes of the lowest level of the final state controller, and they might in here some excess states.",
            "So whenever this lower level process is triggers an exit state, then you might have in transition in the high level process of the controller whenever this high level process triggers exit state and you might have a transition in the highest level process of this controller.",
            "So since we assume that we are given in palm DP, then we know these kind of transition probabilities.",
            "What we need to learn is all these conditional probability tables which define the hierarchy and exit States and the all the parameters of.",
            "Actually this control up here."
        ],
        [
            "The you know, controllers that were previously that I that I showed as hierarchal fanatsy controllers can be represented in this."
        ],
        [
            "The framework."
        ],
        [
            "OK, so now let me get to this core part.",
            "How can you apply like inference techniques in expectation maximization for optimizing this structure?",
            "So here I represent again the full process that is going on.",
            "In this case, only with a flat finite state controller where we only have one level of nodes in the control, which is for simplicity.",
            "Now represent this whole process in one dynamic vision network.",
            "The point being that we can observe rewards in every time slice.",
            "And the goal is, as I said, to maximize the expectation of the discounted sum of rewards over the whole process."
        ],
        [
            "Now that the rough idea that I'm following is that in order to compute parameters in this dynamic range networks and parameters of the controller in particular.",
            "It seems like a quite possible idea to say, well, we want to compute or maximize the parameters of the dynamic Bayesian networks to maximize the likelihood of observing reward.",
            "That seems plausible, right?",
            "But we have to be careful because actually we want to be one work in reinforcement learning.",
            "So we have to sort of quick and tweak this rough idea to become really equivalent to maximizing the discounted the sum of rewards."
        ],
        [
            "So here's the trick.",
            "You can sort of reformulated the process, which emits rewards in every time slice into a mixture of processes and each of which only emits are rewarded.",
            "The final time slice.",
            "So this process here has only length 0, so.",
            "And only you know, it's the reward immediately after you choose to 1st action.",
            "This process here is sort of represents the probability that the agent left for one time step and then they receive their reward.",
            "And then we might have lots of these processes and each one with different lengths.",
            "We may introduce a.",
            "A mixture variable, so the big T here is now introduced as a mixture variable and we assign a probability to this process.",
            "This process in this process, according to this mixture distribution.",
            "So that kind of means that the agent sort of lifts in a mixture of worlds.",
            "It doesn't really know in which one it lives, and depending on which one it lifts, then you know the final time is different.",
            "It turns out if you just really check that if you compute now the expectation over this thing, this reward variable which appears in each of these mixture components.",
            "And because you chose the prior to be proportional to the discounting that the expectation of this reward variable is exactly what we want to maximize.",
            "Right, if there is one detail which I didn't mention.",
            "If we choose this final variable to be such that it is related to the reward function."
        ],
        [
            "OK, that means that maximizing the likelihood of observing.",
            "If this is a binary variable of observing reward in this mixture of finite time, dynamic vision networks is equivalent to maximizing and expected return.",
            "And so this paper here introduced this earlier at ICML two years ago, for the case of MPs, and here we sort of transported this to the."
        ],
        [
            "Case of palm DPS.",
            "So the message basically is policy optimization can be framed as maximizing likelihood, and if nothing else then this is the core message that I hope you care along with with."
        ],
        [
            "Talk, so in our case.",
            "That means we can approach the problem of optimizing this structured controller that we are considering using an expectation maximization algorithm, and in the step we need to do is we need to compute expectations over the latent variables of our controller and particularly, for instance, expectations of.",
            "It's actually a very intuitive expression here, so it's an expectation of conditioning on that the controller has observed rewards.",
            "So conditional on that, the controller was successful.",
            "What's the?",
            "What's the expectations or the statistics?",
            "If you like of how often did it choose an action a when it was in a latent node state an?",
            "So these these expectations actually are in two different events that you sort of condition on having success and one the expectations of what the controller did conditioner, then that it was successful and the M step well is just standard assigns the relative frequencies to the parameters."
        ],
        [
            "The controller.",
            "Let me let me have two slides to address some peculiarities about inference in this process.",
            "The first one is now that I'm saying you in the step you need to do inference in a mixture of finite time processes, and you might think that's really expensive becausw.",
            "How do you do inference in all of these models, right?",
            "You might think that in each of these models you would have to do inference separately, But it turns out if we just look at this, it's immediately clear that you don't have to do separate inference like sweeps for each of these models, because if you would just cut cut these models, cut the tail of these models away, then you would realize that the head of all of these models is exactly the same.",
            "And for that reason also the forward messages that you compute for all of these models are the same.",
            "Or if you cut the head of these processes away, you would find that details of all of these processes are exactly the same, and for that reason the backward messages that you're computing also exactly the same.",
            "Which means that you only need to compute.",
            "Basically once we will forward and backward messages, and you can sort of.",
            "Mix them together to get all."
        ],
        [
            "Seriously need.",
            "Finally, really quick, so this was the original structure that we actually wanted to influence over or do expectation maximization for.",
            "And this explicitly includes exit states.",
            "We can marginalized them out and we get simply slightly simplified structure.",
            "And if you now wonder how can you do like efficient inference in a dynamic Bayesian network like this, well, this is."
        ],
        [
            "Best illustrated by by illustrating the separators in a junction tree approach for insurance.",
            "In this and so, the separators study need when you compile this into junction tree.",
            "Actually simple junction tree."
        ],
        [
            "Chain or decep?"
        ],
        [
            "It is here so you first start off with having some."
        ],
        [
            "Some potential."
        ],
        [
            "Over this and over."
        ],
        [
            "Over this over this over this and these are kind of the clicks that you're working with, and obviously if.",
            "If you have lots of layers here, the complexity that is the maximal the largest clique size is much less than having trusted."
        ],
        [
            "Product of all of these node levels.",
            "OK, let's get to."
        ],
        [
            "The results.",
            "So we, um, we tested this maximum likelihood approach on a number of benchmarks, and in particular we tested the two level, so we assume that we have two levels in our dynamic Bayesian network and we tested quite a lot of different.",
            "We have to prefix the number of nodes on each of these levels, which is one of the maybe bottlenecks of the approach so far, and we tested it for quite a lot of different number of nodes in each of these levels and ten runs each.",
            "So.",
            "Let me first compare to this approach here, so this one here was the SSD first approach by Lorraine Pascale how to automatically find these hierarchies in the problems which they presented at NIPS 2007 and.",
            "So let's say for the photo simple small problems like pain shuttle in maize.",
            "This approach was quite successful and basically if you compare this to our approach, it's pretty much the same.",
            "So here are the times which Arkham Parable in the maze already because they said 16 states.",
            "While we are quite a lot faster than this approach for the slightly larger problems I'm going to talk about chain of change.",
            "In the second hand washing is a problem that Pascal introduced.",
            "This approach was not able to really find good solutions.",
            "1st All approach was able to find solutions in all of these cases and rather fast actually.",
            "Now if you compare this to one of the.",
            "Like solutions, palm DP solvers, which is not based on really finding hierarchy, so it's actually not addressing the problem finding hierarchies.",
            "But this approach actually had problems with this hand washing problem, which had 84 states which didn't really converge and so our solution was so far the only one we got really results with that."
        ],
        [
            "To compare what the effect is of selecting different number of nodes in a different levels, we tested, we check first the runtimes of these algorithms.",
            "So let me first concentrate on the right hand side and so our complexity analysis based on the clique size to maximum clique size for inference clearly shows that it should be very beneficial to have factored or hierarchical approach.",
            "And this is exactly what you see here.",
            "So if if you plot the running time of the approach.",
            "Of a flat controller which only has one level of nodes with a structured controller being either strictly hierarchical with exit states or being just factored, you see that the flat controller just takes much longer as the factor approach which corresponds to the complexity analysis and the click size in the junction tree algorithm.",
            "This was for hand washing in the same result we also get full for cheese taxi.",
            "If we now look at the kind of values and so the quality of the solutions that we get, one thing we can see here is that the having having this strictly hierarchical structure, which with exit states actually produce quite a lot more variants in the solutions then having the simply only the factored approach which we interpret that the having explicit exit states actually constrains the controller and makes it less general then.",
            "Then fully factored approach and it turns out that the variance that is, you know the local minimal get stuck was more problematic if we actually have a strictly hierarchical controller then we have just reflected factor controller."
        ],
        [
            "Anyway, so I'm I'm.",
            "I'm going to come to 1 one last discussion, so we were actually hoping that when we apply this approach and apply it on hand washing or cheese taxi then very nicely we would find exactly the kind of hierarchies that one would like to see and you can plot these controllers that we learned.",
            "But it turns out that are really complex, so it's very it's almost impossible actually to understand these when you just plot them so we hand coded and you toy problem and this toy problem was actually a very very simple and.",
            "Explicitly meant to see if we can uncover a simple hierarchy in the toy problem.",
            "So this type problem rewards the following procedure.",
            "What you have to do is you have to execute N times exactly the same chain of actions followed by submit action.",
            "So for the case N is 3, this is an example of what would be an optimal policy.",
            "You have to execute ABC, ABC, ABC and then D as the submit action ABC, ABC, ABC, D. So to do this you need kind of the counter.",
            "So you need a subprocess which can produce ABC and Encounter, which says I've done it once.",
            "Twice a price and then sort of called the submit action.",
            "So here's the controller that we found and well, so this controller now really is exactly what you would want to see.",
            "So we have the subprocess which this ABC, and whenever it's in C it's sort of exits and goes on the high level process to count one up, and anyway, so the higher level processes counting and it's triggering the submit action."
        ],
        [
            "Exactly at the right time.",
            "To conclude.",
            "The main message actually is that I hope, and I think that this new approach allows to apply inference methods and in particular inference inference methods which exploit the structure of the problem or the controller for it."
        ],
        [
            "Case of Palm DP's and the core ideas that we utilized in this specific approach was that we maximize the likelihood of observing rewards.",
            "So we introduced this reward variable, but in a mixture of finite time dehns so we were confronted with the problem of inference in a mixture of variable length deviance which is an interesting inference problem in itself.",
            "So and the inference methods are."
        ],
        [
            "To exploit the structure also this approach.",
            "Well, it was much more efficient in to really discover hierarchies than the previous approach.",
            "At Nips, one of the things that I haven't mentioned here is that we're doing M and yes, the M is prone to local minima, so we had problems."
        ],
        [
            "With local minimum, the code I already put the code on my web page, but it's not yet really documented or cleaned up.",
            "I'll do this soon, so thanks for your attention.",
            "Yeah.",
            "Potential yes.",
            "Yes.",
            "Yes, excellent so.",
            "Yes, the because I trust awards to be sort of proportional to the likelihood.",
            "And when as we scale the rewards, the likelihood is also rescaled and that changes the convergence of the M drastically.",
            "For two standard M right?",
            "And that is actually a problem that we've seen because we have to re scale rewards quite often depending on your rewards.",
            "So what we do is we use a different kind of EM step which is kind of invariant against these kinds of rescalings, so it's a variant of greedy version of an M step.",
            "Which basically looks at what are the?",
            "What are the Max?",
            "So it's almost the Max assignment in the M step.",
            "But there is a couple of more heuristics in there, so it turns out that we use heuristics in the M step to exactly circumvent this problem of the convergence rate is depending on the rescaling of the reward.",
            "Yeah, that's a bit messy.",
            "You would have to look in the paper for details.",
            "Thanks.",
            "Other questions.",
            "Well, I'll ask the I'm really stupid question I could you say again how did you move from the general reward framework where rewards could be anything to this are had equal 1 objective yes?"
        ],
        [
            "OK.",
            "So so one thing is that I assume this R is a binary random variable, right?",
            "I still I can define this binary random variable or the expectation of this binary random variable to be proportional to the reward function.",
            "This is what what I've written here.",
            "How can I do this?",
            "While the expectation of a binary random variables always between zero and one, I have to re scale the reward function that is fine DDR Max and Armin instead of subtracting and just re scale it.",
            "OK so this way I define the binary reward function.",
            "Now what still is a crucial thing that in the normal process rewards are being emitted at every time slice?",
            "And here on the final time slices.",
            "But it turns out that eventually what you're interested in is to maximize the expectation of a discounted sum of future rewards.",
            "But this expectation of some you can sort of reproduce.",
            "You can mimic it also by introducing a mixture and what is the mixture doing?",
            "It's also introducing in some and just as I had here some over different times where now we have this prior over the time in here, we're not just choose this prior to be proportional to a discounting.",
            "That is, the probability that you live long is exponentially less probable than and that you live shorter.",
            "Then you just reproduce exactly the same reward function.",
            "Thank you for that help.",
            "Maybe Stuart.",
            "General question is not really directed at you.",
            "You look at what's happening on the graphical model inference side.",
            "There's a lot of LP.",
            "Integrity.",
            "And yet you're saying, well, let's take an optimization problematic to a graphical model so we can use those good OL graphical model inference techniques, which will turn out the techniques that you could have applied to the original problem without seeing it.",
            "Yes, yes.",
            "Yes, you're right.",
            "I don't know my my my.",
            "My comment though, would be that it's the thing about exploring structure.",
            "So I think that at least in graphical models people have.",
            "I wouldn't say maybe more so, but at least they have different ideas and techniques to deal with structure between belief propagation or or these junction tree techniques which have not yet been transferred into the realm of reinforcement learning.",
            "So definitely I think that this is an important thing to consider.",
            "Original.",
            "Problem is actually nothing else.",
            "It's a nonlinear nonconvex, actually market for complete optimization problem and then so here.",
            "I guess that we're going to graphical models and therefore appointed folder with all the other parts that we would end up with.",
            "Come back to the original optimization problem.",
            "So it would actually be.",
            "Jarvis.",
            "Right, yeah, right.",
            "And also because we've got structure.",
            "So the heart be also adding more complexity.",
            "OK, thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first thank you for coming to the talk, and before I start, I definitely want to point out that this is a joint work with law and Pascal.",
                    "label": 1
                },
                {
                    "sent": "So a lot of credit is actually to both guys who push this forward and push the idea forward and to start with as he could.",
                    "label": 0
                },
                {
                    "sent": "As you can tell by the title of this talk.",
                    "label": 0
                },
                {
                    "sent": "What this approach is trying to suggest is that it can use standard machine learning techniques like expectation maximization or likelihood maximization, and apply them in reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "So my hope with this talk actually is that there's a couple of people here in the audience, maybe who are experts in inference techniques, maybe belief propagation or whatever, and well, here it can now learn how you can apply these techniques to solve harmony, peace, or derive optimal controllers in Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "So before I actually start talking about the.",
                    "label": 0
                },
                {
                    "sent": "Like with maximization.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first part will just be a quick introduction of what the problem is that I'm facing, so the framework I'm working in is partially observable Markov decision process.",
                    "label": 1
                },
                {
                    "sent": "So in comparison to the two talks that we had before.",
                    "label": 0
                },
                {
                    "sent": "We we have an additional sign in on top of having a state space and action space and we have an agent interacting with the world by exchanging actions and on top of that we have an observation space and that means that the agent does not have a full information about the state it is in.",
                    "label": 0
                },
                {
                    "sent": "Instead of that all it has is there exists observation probabilities that is depending on the state of the agent and the action that takes it receives a certain observation.",
                    "label": 0
                },
                {
                    "sent": "Now that observation may be used in order to estimate the state.",
                    "label": 0
                },
                {
                    "sent": "But it's not full information about what the status the rest of the palm DP is.",
                    "label": 0
                },
                {
                    "sent": "As in market physician processes, we have a state transition probability.",
                    "label": 0
                },
                {
                    "sent": "So when you are in the state and you take a certain action, then there is a probability of ending up in new state will start distribution for the agent might start and we have a reward function.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what the goal is, as in standard modification process or reinforce learning to maximize the expected discounted future return where these little ours here are kind of samples which have the expectation of the reward function.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's many different approaches of how you could go about trying to solve problem piece, and one approach is based on actually.",
                    "label": 0
                },
                {
                    "sent": "Keeping exact beliefs about the state you're in, but there is also like another approach which tries to find state machines which are called finite state controllers as an implementation of the policy in the palm DP.",
                    "label": 0
                },
                {
                    "sent": "So let me let me explain these final state controllers are So what you see here?",
                    "label": 0
                },
                {
                    "sent": "This diagram is actually a diagram of a state machine, not confused with a graphical model, right?",
                    "label": 0
                },
                {
                    "sent": "So these things here are states of a state machine.",
                    "label": 0
                },
                {
                    "sent": "And so final state controller.",
                    "label": 1
                },
                {
                    "sent": "You initialize the finite state control in certain states.",
                    "label": 0
                },
                {
                    "sent": "Let's say this note here.",
                    "label": 0
                },
                {
                    "sent": "So these states of the controller, also called nodes.",
                    "label": 0
                },
                {
                    "sent": "And whenever the final state controller is in a certain node and for this node we have storage probability vector, which says which kinds of actions are being emitted by this node.",
                    "label": 0
                },
                {
                    "sent": "So here this offer says which kinds of actions are being triggered at this node when the control is selected, some action it.",
                    "label": 0
                },
                {
                    "sent": "Well, the world state changes and we received some observation from the world, and this observation triggers probabilistic transition into a new node of the final state controller.",
                    "label": 0
                },
                {
                    "sent": "For instance, over here, when we are in this new node, we again select some action from this distribution and so on.",
                    "label": 0
                },
                {
                    "sent": "Now hierarchical state controllers have one additional interesting property and that is that these nodes here some of these nodes do not directly emit an action, but instead they sort of call a subroutine.",
                    "label": 0
                },
                {
                    "sent": "So in this note, here is supposed to be an abstract node.",
                    "label": 0
                },
                {
                    "sent": "It's not directly sampling in action and executing it, but instead of that it's sort of entering a different kind of finite state controller, which executes some stuff between.",
                    "label": 0
                },
                {
                    "sent": "Until this finite state controller reaches some end node, and when this finite state control is done, reach the end node is sort of transition back and then go there.",
                    "label": 0
                },
                {
                    "sent": "So this is a hierarchical final state controller and with was introduced in this paper by Hanson.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea obviously is that these kinds of approaches solving palm DP's allow us to decompose the problem in sub task so it's the same as with Markov decision process options and things like that and one of the core problems however is discovering these hierarchical decomposition in the palm DP.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to mention so did the starting point actually of this work?",
                    "label": 0
                },
                {
                    "sent": "Was the paper by Law and Pascal and their colleagues in Waterloo where they for the first time actually proposed a method to discover these these hierarchies, so not only to presume a certain hierarchy and optimize the parameters of it, but discovering these hierarchies and their approach was actually quite complex in general optimization, frame it as a pure optimization problem and.",
                    "label": 0
                },
                {
                    "sent": "Use standard optimizes for this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We reframe this problem in a different view.",
                    "label": 0
                },
                {
                    "sent": "You can express these kind of hierarchical processes in terms of dynamic vision networks and a very classical example for this hierarchical hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "So in hierarchical hidden Markov models you have two sort of layers of Markovian chains of random variables, so this is a graphical model, right?",
                    "label": 0
                },
                {
                    "sent": "It's a dynamic vision network and on one of these Markov chains, well, you have the standard transitions, just more coughing transitions and depending on the state of that you just image certain kinds of observations.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If you however want is to sort of model with Markov models, processes like this here for instance, so did the process here.",
                    "label": 0
                },
                {
                    "sent": "If you read it as ABC, DF, DF ABCD EF abcabc, you might think that modeling such kind of process would be much easier if you would to find out about the submodules in this process is.",
                    "label": 0
                },
                {
                    "sent": "So what Murphy Murphy and Pascal showed is that you could reformulate hierarchical hidden Markov models with a dynamic Bayesian network where you have 2 Chainz.",
                    "label": 0
                },
                {
                    "sent": "Well, in this case two chains of random variables and the important thing is that the lower level here is kind of the subroutine which might express these modules and their existing exit states.",
                    "label": 0
                },
                {
                    "sent": "So whenever this subroutine is done or emits an exit state, then the higher level state might make in transition.",
                    "label": 0
                },
                {
                    "sent": "If the exit state is 0, the higher level state is persistent.",
                    "label": 0
                },
                {
                    "sent": "So the core idea is actually introducing these exit states, which constrain the high level process to be persistent while the lower process is still.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Running and it's doing his job.",
                    "label": 0
                },
                {
                    "sent": "We can exactly use the same kind of approach, the same kind of idea for representing the hierarchical finite state control for Palm DP's.",
                    "label": 1
                },
                {
                    "sent": "So this is a two slice model of the palm DP that we're of the dynamic vision network that you're actually going to consider.",
                    "label": 0
                },
                {
                    "sent": "It includes the state of the world down here and the transition probabilities given an action in a new state includes the probability of sampling and observation.",
                    "label": 0
                },
                {
                    "sent": "And on top of here, this is the representation of the hierarchical policy that we're actually investigating, so we have.",
                    "label": 0
                },
                {
                    "sent": "Different nodes, different latent variables of the controller on different levels.",
                    "label": 1
                },
                {
                    "sent": "So here this might be nodes of the lowest level of the final state controller, and they might in here some excess states.",
                    "label": 0
                },
                {
                    "sent": "So whenever this lower level process is triggers an exit state, then you might have in transition in the high level process of the controller whenever this high level process triggers exit state and you might have a transition in the highest level process of this controller.",
                    "label": 0
                },
                {
                    "sent": "So since we assume that we are given in palm DP, then we know these kind of transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "What we need to learn is all these conditional probability tables which define the hierarchy and exit States and the all the parameters of.",
                    "label": 0
                },
                {
                    "sent": "Actually this control up here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The you know, controllers that were previously that I that I showed as hierarchal fanatsy controllers can be represented in this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The framework.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let me get to this core part.",
                    "label": 0
                },
                {
                    "sent": "How can you apply like inference techniques in expectation maximization for optimizing this structure?",
                    "label": 1
                },
                {
                    "sent": "So here I represent again the full process that is going on.",
                    "label": 0
                },
                {
                    "sent": "In this case, only with a flat finite state controller where we only have one level of nodes in the control, which is for simplicity.",
                    "label": 0
                },
                {
                    "sent": "Now represent this whole process in one dynamic vision network.",
                    "label": 0
                },
                {
                    "sent": "The point being that we can observe rewards in every time slice.",
                    "label": 1
                },
                {
                    "sent": "And the goal is, as I said, to maximize the expectation of the discounted sum of rewards over the whole process.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now that the rough idea that I'm following is that in order to compute parameters in this dynamic range networks and parameters of the controller in particular.",
                    "label": 1
                },
                {
                    "sent": "It seems like a quite possible idea to say, well, we want to compute or maximize the parameters of the dynamic Bayesian networks to maximize the likelihood of observing reward.",
                    "label": 1
                },
                {
                    "sent": "That seems plausible, right?",
                    "label": 0
                },
                {
                    "sent": "But we have to be careful because actually we want to be one work in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So we have to sort of quick and tweak this rough idea to become really equivalent to maximizing the discounted the sum of rewards.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the trick.",
                    "label": 0
                },
                {
                    "sent": "You can sort of reformulated the process, which emits rewards in every time slice into a mixture of processes and each of which only emits are rewarded.",
                    "label": 0
                },
                {
                    "sent": "The final time slice.",
                    "label": 0
                },
                {
                    "sent": "So this process here has only length 0, so.",
                    "label": 0
                },
                {
                    "sent": "And only you know, it's the reward immediately after you choose to 1st action.",
                    "label": 0
                },
                {
                    "sent": "This process here is sort of represents the probability that the agent left for one time step and then they receive their reward.",
                    "label": 0
                },
                {
                    "sent": "And then we might have lots of these processes and each one with different lengths.",
                    "label": 0
                },
                {
                    "sent": "We may introduce a.",
                    "label": 0
                },
                {
                    "sent": "A mixture variable, so the big T here is now introduced as a mixture variable and we assign a probability to this process.",
                    "label": 0
                },
                {
                    "sent": "This process in this process, according to this mixture distribution.",
                    "label": 0
                },
                {
                    "sent": "So that kind of means that the agent sort of lifts in a mixture of worlds.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really know in which one it lives, and depending on which one it lifts, then you know the final time is different.",
                    "label": 0
                },
                {
                    "sent": "It turns out if you just really check that if you compute now the expectation over this thing, this reward variable which appears in each of these mixture components.",
                    "label": 0
                },
                {
                    "sent": "And because you chose the prior to be proportional to the discounting that the expectation of this reward variable is exactly what we want to maximize.",
                    "label": 0
                },
                {
                    "sent": "Right, if there is one detail which I didn't mention.",
                    "label": 0
                },
                {
                    "sent": "If we choose this final variable to be such that it is related to the reward function.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that means that maximizing the likelihood of observing.",
                    "label": 0
                },
                {
                    "sent": "If this is a binary variable of observing reward in this mixture of finite time, dynamic vision networks is equivalent to maximizing and expected return.",
                    "label": 1
                },
                {
                    "sent": "And so this paper here introduced this earlier at ICML two years ago, for the case of MPs, and here we sort of transported this to the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case of palm DPS.",
                    "label": 0
                },
                {
                    "sent": "So the message basically is policy optimization can be framed as maximizing likelihood, and if nothing else then this is the core message that I hope you care along with with.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk, so in our case.",
                    "label": 0
                },
                {
                    "sent": "That means we can approach the problem of optimizing this structured controller that we are considering using an expectation maximization algorithm, and in the step we need to do is we need to compute expectations over the latent variables of our controller and particularly, for instance, expectations of.",
                    "label": 0
                },
                {
                    "sent": "It's actually a very intuitive expression here, so it's an expectation of conditioning on that the controller has observed rewards.",
                    "label": 0
                },
                {
                    "sent": "So conditional on that, the controller was successful.",
                    "label": 0
                },
                {
                    "sent": "What's the?",
                    "label": 0
                },
                {
                    "sent": "What's the expectations or the statistics?",
                    "label": 0
                },
                {
                    "sent": "If you like of how often did it choose an action a when it was in a latent node state an?",
                    "label": 0
                },
                {
                    "sent": "So these these expectations actually are in two different events that you sort of condition on having success and one the expectations of what the controller did conditioner, then that it was successful and the M step well is just standard assigns the relative frequencies to the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The controller.",
                    "label": 0
                },
                {
                    "sent": "Let me let me have two slides to address some peculiarities about inference in this process.",
                    "label": 0
                },
                {
                    "sent": "The first one is now that I'm saying you in the step you need to do inference in a mixture of finite time processes, and you might think that's really expensive becausw.",
                    "label": 1
                },
                {
                    "sent": "How do you do inference in all of these models, right?",
                    "label": 0
                },
                {
                    "sent": "You might think that in each of these models you would have to do inference separately, But it turns out if we just look at this, it's immediately clear that you don't have to do separate inference like sweeps for each of these models, because if you would just cut cut these models, cut the tail of these models away, then you would realize that the head of all of these models is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "And for that reason also the forward messages that you compute for all of these models are the same.",
                    "label": 0
                },
                {
                    "sent": "Or if you cut the head of these processes away, you would find that details of all of these processes are exactly the same, and for that reason the backward messages that you're computing also exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Which means that you only need to compute.",
                    "label": 0
                },
                {
                    "sent": "Basically once we will forward and backward messages, and you can sort of.",
                    "label": 0
                },
                {
                    "sent": "Mix them together to get all.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seriously need.",
                    "label": 0
                },
                {
                    "sent": "Finally, really quick, so this was the original structure that we actually wanted to influence over or do expectation maximization for.",
                    "label": 0
                },
                {
                    "sent": "And this explicitly includes exit states.",
                    "label": 0
                },
                {
                    "sent": "We can marginalized them out and we get simply slightly simplified structure.",
                    "label": 0
                },
                {
                    "sent": "And if you now wonder how can you do like efficient inference in a dynamic Bayesian network like this, well, this is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best illustrated by by illustrating the separators in a junction tree approach for insurance.",
                    "label": 0
                },
                {
                    "sent": "In this and so, the separators study need when you compile this into junction tree.",
                    "label": 0
                },
                {
                    "sent": "Actually simple junction tree.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chain or decep?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is here so you first start off with having some.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some potential.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over this and over.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over this over this over this and these are kind of the clicks that you're working with, and obviously if.",
                    "label": 0
                },
                {
                    "sent": "If you have lots of layers here, the complexity that is the maximal the largest clique size is much less than having trusted.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Product of all of these node levels.",
                    "label": 0
                },
                {
                    "sent": "OK, let's get to.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results.",
                    "label": 0
                },
                {
                    "sent": "So we, um, we tested this maximum likelihood approach on a number of benchmarks, and in particular we tested the two level, so we assume that we have two levels in our dynamic Bayesian network and we tested quite a lot of different.",
                    "label": 0
                },
                {
                    "sent": "We have to prefix the number of nodes on each of these levels, which is one of the maybe bottlenecks of the approach so far, and we tested it for quite a lot of different number of nodes in each of these levels and ten runs each.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me first compare to this approach here, so this one here was the SSD first approach by Lorraine Pascale how to automatically find these hierarchies in the problems which they presented at NIPS 2007 and.",
                    "label": 0
                },
                {
                    "sent": "So let's say for the photo simple small problems like pain shuttle in maize.",
                    "label": 0
                },
                {
                    "sent": "This approach was quite successful and basically if you compare this to our approach, it's pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "So here are the times which Arkham Parable in the maze already because they said 16 states.",
                    "label": 0
                },
                {
                    "sent": "While we are quite a lot faster than this approach for the slightly larger problems I'm going to talk about chain of change.",
                    "label": 0
                },
                {
                    "sent": "In the second hand washing is a problem that Pascal introduced.",
                    "label": 0
                },
                {
                    "sent": "This approach was not able to really find good solutions.",
                    "label": 0
                },
                {
                    "sent": "1st All approach was able to find solutions in all of these cases and rather fast actually.",
                    "label": 0
                },
                {
                    "sent": "Now if you compare this to one of the.",
                    "label": 0
                },
                {
                    "sent": "Like solutions, palm DP solvers, which is not based on really finding hierarchy, so it's actually not addressing the problem finding hierarchies.",
                    "label": 0
                },
                {
                    "sent": "But this approach actually had problems with this hand washing problem, which had 84 states which didn't really converge and so our solution was so far the only one we got really results with that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To compare what the effect is of selecting different number of nodes in a different levels, we tested, we check first the runtimes of these algorithms.",
                    "label": 0
                },
                {
                    "sent": "So let me first concentrate on the right hand side and so our complexity analysis based on the clique size to maximum clique size for inference clearly shows that it should be very beneficial to have factored or hierarchical approach.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly what you see here.",
                    "label": 0
                },
                {
                    "sent": "So if if you plot the running time of the approach.",
                    "label": 0
                },
                {
                    "sent": "Of a flat controller which only has one level of nodes with a structured controller being either strictly hierarchical with exit states or being just factored, you see that the flat controller just takes much longer as the factor approach which corresponds to the complexity analysis and the click size in the junction tree algorithm.",
                    "label": 0
                },
                {
                    "sent": "This was for hand washing in the same result we also get full for cheese taxi.",
                    "label": 0
                },
                {
                    "sent": "If we now look at the kind of values and so the quality of the solutions that we get, one thing we can see here is that the having having this strictly hierarchical structure, which with exit states actually produce quite a lot more variants in the solutions then having the simply only the factored approach which we interpret that the having explicit exit states actually constrains the controller and makes it less general then.",
                    "label": 0
                },
                {
                    "sent": "Then fully factored approach and it turns out that the variance that is, you know the local minimal get stuck was more problematic if we actually have a strictly hierarchical controller then we have just reflected factor controller.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, so I'm I'm.",
                    "label": 0
                },
                {
                    "sent": "I'm going to come to 1 one last discussion, so we were actually hoping that when we apply this approach and apply it on hand washing or cheese taxi then very nicely we would find exactly the kind of hierarchies that one would like to see and you can plot these controllers that we learned.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that are really complex, so it's very it's almost impossible actually to understand these when you just plot them so we hand coded and you toy problem and this toy problem was actually a very very simple and.",
                    "label": 0
                },
                {
                    "sent": "Explicitly meant to see if we can uncover a simple hierarchy in the toy problem.",
                    "label": 0
                },
                {
                    "sent": "So this type problem rewards the following procedure.",
                    "label": 0
                },
                {
                    "sent": "What you have to do is you have to execute N times exactly the same chain of actions followed by submit action.",
                    "label": 0
                },
                {
                    "sent": "So for the case N is 3, this is an example of what would be an optimal policy.",
                    "label": 0
                },
                {
                    "sent": "You have to execute ABC, ABC, ABC and then D as the submit action ABC, ABC, ABC, D. So to do this you need kind of the counter.",
                    "label": 0
                },
                {
                    "sent": "So you need a subprocess which can produce ABC and Encounter, which says I've done it once.",
                    "label": 0
                },
                {
                    "sent": "Twice a price and then sort of called the submit action.",
                    "label": 0
                },
                {
                    "sent": "So here's the controller that we found and well, so this controller now really is exactly what you would want to see.",
                    "label": 0
                },
                {
                    "sent": "So we have the subprocess which this ABC, and whenever it's in C it's sort of exits and goes on the high level process to count one up, and anyway, so the higher level processes counting and it's triggering the submit action.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly at the right time.",
                    "label": 0
                },
                {
                    "sent": "To conclude.",
                    "label": 0
                },
                {
                    "sent": "The main message actually is that I hope, and I think that this new approach allows to apply inference methods and in particular inference inference methods which exploit the structure of the problem or the controller for it.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Case of Palm DP's and the core ideas that we utilized in this specific approach was that we maximize the likelihood of observing rewards.",
                    "label": 0
                },
                {
                    "sent": "So we introduced this reward variable, but in a mixture of finite time dehns so we were confronted with the problem of inference in a mixture of variable length deviance which is an interesting inference problem in itself.",
                    "label": 0
                },
                {
                    "sent": "So and the inference methods are.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To exploit the structure also this approach.",
                    "label": 0
                },
                {
                    "sent": "Well, it was much more efficient in to really discover hierarchies than the previous approach.",
                    "label": 0
                },
                {
                    "sent": "At Nips, one of the things that I haven't mentioned here is that we're doing M and yes, the M is prone to local minima, so we had problems.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With local minimum, the code I already put the code on my web page, but it's not yet really documented or cleaned up.",
                    "label": 0
                },
                {
                    "sent": "I'll do this soon, so thanks for your attention.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Potential yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, excellent so.",
                    "label": 0
                },
                {
                    "sent": "Yes, the because I trust awards to be sort of proportional to the likelihood.",
                    "label": 0
                },
                {
                    "sent": "And when as we scale the rewards, the likelihood is also rescaled and that changes the convergence of the M drastically.",
                    "label": 0
                },
                {
                    "sent": "For two standard M right?",
                    "label": 0
                },
                {
                    "sent": "And that is actually a problem that we've seen because we have to re scale rewards quite often depending on your rewards.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we use a different kind of EM step which is kind of invariant against these kinds of rescalings, so it's a variant of greedy version of an M step.",
                    "label": 0
                },
                {
                    "sent": "Which basically looks at what are the?",
                    "label": 0
                },
                {
                    "sent": "What are the Max?",
                    "label": 0
                },
                {
                    "sent": "So it's almost the Max assignment in the M step.",
                    "label": 0
                },
                {
                    "sent": "But there is a couple of more heuristics in there, so it turns out that we use heuristics in the M step to exactly circumvent this problem of the convergence rate is depending on the rescaling of the reward.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a bit messy.",
                    "label": 0
                },
                {
                    "sent": "You would have to look in the paper for details.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Well, I'll ask the I'm really stupid question I could you say again how did you move from the general reward framework where rewards could be anything to this are had equal 1 objective yes?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So so one thing is that I assume this R is a binary random variable, right?",
                    "label": 0
                },
                {
                    "sent": "I still I can define this binary random variable or the expectation of this binary random variable to be proportional to the reward function.",
                    "label": 0
                },
                {
                    "sent": "This is what what I've written here.",
                    "label": 0
                },
                {
                    "sent": "How can I do this?",
                    "label": 0
                },
                {
                    "sent": "While the expectation of a binary random variables always between zero and one, I have to re scale the reward function that is fine DDR Max and Armin instead of subtracting and just re scale it.",
                    "label": 0
                },
                {
                    "sent": "OK so this way I define the binary reward function.",
                    "label": 0
                },
                {
                    "sent": "Now what still is a crucial thing that in the normal process rewards are being emitted at every time slice?",
                    "label": 0
                },
                {
                    "sent": "And here on the final time slices.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that eventually what you're interested in is to maximize the expectation of a discounted sum of future rewards.",
                    "label": 0
                },
                {
                    "sent": "But this expectation of some you can sort of reproduce.",
                    "label": 0
                },
                {
                    "sent": "You can mimic it also by introducing a mixture and what is the mixture doing?",
                    "label": 0
                },
                {
                    "sent": "It's also introducing in some and just as I had here some over different times where now we have this prior over the time in here, we're not just choose this prior to be proportional to a discounting.",
                    "label": 0
                },
                {
                    "sent": "That is, the probability that you live long is exponentially less probable than and that you live shorter.",
                    "label": 0
                },
                {
                    "sent": "Then you just reproduce exactly the same reward function.",
                    "label": 0
                },
                {
                    "sent": "Thank you for that help.",
                    "label": 0
                },
                {
                    "sent": "Maybe Stuart.",
                    "label": 0
                },
                {
                    "sent": "General question is not really directed at you.",
                    "label": 0
                },
                {
                    "sent": "You look at what's happening on the graphical model inference side.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of LP.",
                    "label": 0
                },
                {
                    "sent": "Integrity.",
                    "label": 0
                },
                {
                    "sent": "And yet you're saying, well, let's take an optimization problematic to a graphical model so we can use those good OL graphical model inference techniques, which will turn out the techniques that you could have applied to the original problem without seeing it.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, you're right.",
                    "label": 0
                },
                {
                    "sent": "I don't know my my my.",
                    "label": 0
                },
                {
                    "sent": "My comment though, would be that it's the thing about exploring structure.",
                    "label": 0
                },
                {
                    "sent": "So I think that at least in graphical models people have.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't say maybe more so, but at least they have different ideas and techniques to deal with structure between belief propagation or or these junction tree techniques which have not yet been transferred into the realm of reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So definitely I think that this is an important thing to consider.",
                    "label": 0
                },
                {
                    "sent": "Original.",
                    "label": 0
                },
                {
                    "sent": "Problem is actually nothing else.",
                    "label": 0
                },
                {
                    "sent": "It's a nonlinear nonconvex, actually market for complete optimization problem and then so here.",
                    "label": 0
                },
                {
                    "sent": "I guess that we're going to graphical models and therefore appointed folder with all the other parts that we would end up with.",
                    "label": 0
                },
                {
                    "sent": "Come back to the original optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So it would actually be.",
                    "label": 0
                },
                {
                    "sent": "Jarvis.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, right.",
                    "label": 0
                },
                {
                    "sent": "And also because we've got structure.",
                    "label": 0
                },
                {
                    "sent": "So the heart be also adding more complexity.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you again.",
                    "label": 0
                }
            ]
        }
    }
}