{
    "id": "u6kqz2la2id6bhua3weqjffohuuzic7p",
    "title": "Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo",
    "info": {
        "author": [
            "Ruslan Salakhutdinov, Machine Learning Department, Carnegie Mellon University"
        ],
        "published": "Aug. 7, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_salakhutdinov_bpm/",
    "segmentation": [
        [
            "Hello so I'm wrestling so good enough and this is joint work with Anthony who is sitting over there I guess.",
            "So what the title of the talk is Beijing probabilistic matrix factorization using Markov chain Monte Carlo.",
            "So."
        ],
        [
            "Right, so let me just put my problem in the context of what we're actually trying to solve will probably be easy to follow.",
            "What we actually do, what we actually trying to solve is we're trying to solve collaborative filtering problem so we have users an you can think of having a lot of users right, and we have movies and we have some ratings provided by users and hear the ratings for different movies.",
            "And then the goal is try to predict the ratings for.",
            "Other movies, right?",
            "So you're trying to predict the rating for user one and Movie 6, and this is typical collaborative filtering task.",
            "So in terms of notation, we have M movies.",
            "We have end users and we have an integer rating values.",
            "That range from one to K right and the goal of probabilistic matrix factorization.",
            "Basically what we're trying to pursue is we're trying to decompose this matrix into the product of 2 low rank matrices, so again you can think of those as parameters in our model that we're trying to learn.",
            "And what I'm going to do note, as I'm going to art supply, J is the rating of user iPhone movie J and those using these are going to be latent using movie feature matrices and I have to specify the dimensionality which is going to be one of the parameters that I'm going to be specifying OK?"
        ],
        [
            "Now.",
            "The probabilistic matrix factorization.",
            "Initially we looked at simple models.",
            "This is a very simple probabilistic model with the Gaussian observation noise and the graphical model is represented here.",
            "So you have movie features you have user features and combining they give you the rating and so given the user movie features, which is going to say that the distribution over the rating is just is just going to be given by Gaussian distribution with mean that takes the dot product of the movie and user feature vectors.",
            "Plus some noise, right?",
            "It's a very standard probabilistic set up now.",
            "We're also going to put some price on top of our user movie feature vectors in particular, which is going to put Gaussian priors.",
            "With zero means and some parameters here."
        ],
        [
            "So this is.",
            "A very simple model and one efficient way of learning practices in this model is just doing a simple maximum posterior learning, which is we have we're going to be maximized maximizing the work by stereo.",
            "The movie and user feature vectors with fixed value of the hyperparameters, and you can show which is a standard result is that this is just equivalent of minimizing the sum of squared error terms with some quadratic regularization terms.",
            "So what's happening in this equation is that we're trying to find the values of these matrices of these low rank matrices.",
            "Such that they give good predictions for the ratings for which we actually have the training data.",
            "And then there is a regularization term that tries to pull those movie and user feature matrices 2 towards 0.",
            "Great.",
            "So it's a simple formulation and now."
        ],
        [
            "The question is.",
            "One of the things one of the nice formulation about this this particular doing map estimation.",
            "It's very fast.",
            "You can do stochastic gradient descent, you can have millions of users and many, many thousands of movies.",
            "You can just take gradient steps in optimizing over these matrices.",
            "Unv or doing some kind of alternating optimization.",
            "But the main drawback of these models is that how do you do complexity control?",
            "In particular, if you think of an application where we have million users and we have 30 dimensional feature vectors, then we're going to have you know half a million.",
            "Sorry, we're going to have.",
            "I guess about 15 million parameters.",
            "So the question, is these guys regularization coefficients become very, very important?",
            "So how do you complexity control is?",
            "It's a little bit hard to do, and in general we're usually interested in predicting the ratings for new user movie pairs.",
            "But you know, we're not that much interested in actually getting the values of the parameters themselves.",
            "And that sort of motivated us to look at.",
            "Asian version of this."
        ],
        [
            "Which is.",
            "Has this form right?",
            "We just have a likelihood function again in the same way we put the priors.",
            "The Gaussian priors on top of our latent movie and user feature vectors or user movie feature matrices here and then.",
            "We also put the gas in which price over the user movie hyperparameters.",
            "So we just sort of represented.",
            "In this form here.",
            "So now.",
            "Now model this is observed.",
            "All of these values are now random variables, right?",
            "And again, you can see that the way we're choosing our price is is.",
            "Their conjugates so you can do sort of efficient.",
            "You can see that doing something like dips would work.",
            "Now the question is whether it's good to put conjugate priors or not is a different."
        ],
        [
            "And So what we're going to do is to get the predictive distribution of the GNU rating.",
            "So often you rating for the user movie pair given the observed rating R, and in general Bayesian framework that requires us to marginalized or integrate over the parameters and hyperparameters.",
            "That's ultimately what we want to do.",
            "So and once we do that, well, we can do a simple Monte Carlo approximation right where the samples are generated by Markov chain with the stationary distribution, which is the posterior distribution of our model parameters.",
            "It's just a general set up for doing inference in directed graphical models."
        ],
        [
            "Now.",
            "In our implementation, we use Gibbs sampling.",
            "We generate the samples, but of course you can use any other Markov chain Monte Carlo procedure.",
            "And of course you can see that due to the conjugate priors, the conditional distributions are easy to sample from, so that was our initial motivation and one other thing to notice is that the posterior distribution of a movie user, latent feature matrices dicapo, so they become as a product of individual user features.",
            "So you can paralyze it.",
            "In other words, you can make it efficient.",
            "In that for that particular graphical model that we are considering."
        ],
        [
            "And you can see from.",
            "This graph here is that conditional on these values and these values the posterior distribution over the latent.",
            "User features decouples the same As for the movie features."
        ],
        [
            "Now, um.",
            "So we do a standard Gibbs sampler, which is to say that conditional on.",
            "Movie features we can update.",
            "We can get a sample from.",
            "We can get a sample from the posterior over these latent variables and same As for those guys.",
            "And then for each feature vector for each we get a sample for each movie feature vector by conditioning on the user feature vectors, right?",
            "So that's just a standard Gibbs sampler.",
            "So in other words, conditional on this random variable and these are in the variables.",
            "We just get samples from those guys and we alternate.",
            "So now this is all very."
        ],
        [
            "100 and I haven't told you anything.",
            "You but.",
            "One of the things that we were looking at initially we were sort of playing with the Netflix data set and it's a huge data set and the main question was that how do we deal with this regularization things and what we typically do is we run cross validation.",
            "We're trying to figure out what the appropriate hyperparameters should be in just to give you the flavor.",
            "The Netflix datasets is fairly large, it contains 100 million ratings, contains half a million users, and about 18,000 movies.",
            "There is a validation set.",
            "And there is a test set for which you must be predicting.",
            "Use a movie ratings.",
            "In the data set itself is very imbalanced, so you know there are users who give only one rating and their users could give 1500 ratings.",
            "And the performance is also assessed by, you know you make predictions, you submit it to the Netflix and they give you the results.",
            "So it's one of those things that it's hard to cheat on who accidentally cheat on, right?",
            "Now one."
        ],
        [
            "The things that.",
            "You know the question is the nice thing about probabilistic matrix factorization.",
            "Doing map version of probabilistic modifications but very efficient.",
            "And you can do it fast.",
            "An initially we were thinking well, can you actually take a fully Bayesian approach?",
            "You have 100 million ratings, it's big data set, so typically if you look at attitudes of all the people, is that MCMC methods are really really for small problems and would it really work in this particular context?",
            "And we actually were surprised that."
        ],
        [
            "Indeed, it does work.",
            "So here the results.",
            "They're showing you.",
            "This is the SVD model, which is basically probabilistic matrix factorization model, but without any regularization.",
            "This is probabilistic matrix factorization model.",
            "This is slightly different version of probabilistic matrix factorization model.",
            "And here's what basean PMF gives you so.",
            "It gives you much better root mean squared errors or it gives you much better predictions compared to the map estimates, right?",
            "And to us that was a little bit surprising and the other surprising thing is that you know you can actually run these models.",
            "With 30 dimensional feature vectors or 60 dimensional feature vectors which are quite big models in you know.",
            "These are the times about 11 hours or maybe 47 hours a couple of days.",
            "And you can see that the gap between those models is actually quite large.",
            "So."
        ],
        [
            "What's going on is?",
            "Alright, so here are the results by showing that.",
            "As you increase the feature dimensionality, which is to say that as you make your model bigger.",
            "What happens with the basean PMF?",
            "The error the performance improves, which is was, you know, sort of a good result for us because it basically says that you know the Bayesian approach doesn't really require you limiting the complexity of the model based on the training data.",
            "Wait, which is not the case for the map based models where we actually trying to learn the parameters of the model and you know, for the 300 dimensional feature vectors and half a million users think you have about 75 million parameters in your model.",
            "So if you have so many parameters in your model and you don't have nearly enough data, then fitting those models becomes really challenging or regularising those models becomes a very challenging task.",
            "So again, the I guess the.",
            "The main idea here is that as you make your model bigger for this real task, the basean methods actually give you better performance or the performance improves.",
            "Um?"
        ],
        [
            "Now.",
            "One of the other good things about you know finding or getting samples from the posterior distribution is that you can take uncertainty into account.",
            "Ann, it's you can think it's better than just doing map estimates, which as I guess the reason why you want to get the posterior distribution.",
            "So what you what I'm showing you here is basically take basically taking for users right?",
            "And the first user has only four ratings in the training set.",
            "Now with the full ratings, obviously we can't infer the preferences of that particular user, so this plot basically shows what kind of predictions we get on the test set as we're running our Gibbs sampler, so you can see that we are very, very uncertain about predictions that this user is making right.",
            "On the other hand, this is the user that has 660 movies, so this is the user who has rated 660 movies and you can see that on the test set for a particular movie.",
            "We're fairly confident of what the rating for that movie should be.",
            "Right so.",
            "You can you can take this information to account and what you what we're showing here is if you have.",
            "Users grouped by number of ratings in the training set, so these are the users that provided.",
            "Less than five ratings.",
            "These are the users that provided from 6 to 10 ratings in the training set and what happens here is that the map, I guess is doing much worse than the Asian version.",
            "And of course, as you get users who provided more than I guess 600 ratings, then those two errors match.",
            "Which is to say that you know your posterior in those cases is fairly picked around particular parameter configuration.",
            "So most of the wind with the beige and model models we're getting here, so these models are very nice models because they integrate out uncertainty, particularly for users that have very few ratings.",
            "Which is the case for the Netflix data set.",
            "Of course, one way you could deal with the problem is to throw out all the users that have less than.",
            "I guess 20 ratings and then use map PMF, right?",
            "But the whole point here is that we do want to take care of take into account users that have very few right?"
        ],
        [
            "Thanks.",
            "And just to come to the conclusions.",
            "I guess the main message here is that we can successfully apply Bayesian methods to very large datasets.",
            "And we also found that these methods significantly have high significantly accuracy compared to standard map type of methods, although it is the case that with map methods the optimization is much faster.",
            "So there is also a balancing going on.",
            "And also the Bayesian methods provide the predictive distribution, so you can take confidence into account.",
            "And of course, one drawback of the Bayesian methods is that you know when you're running your Markov chain.",
            "It's very hard to determine when you actually converge to the digital distribution, but of course in our cases we just running these Markov chains and we're looking at.",
            "We're looking at the values of our hyperparameters and see whether they stabilize and will look at the auto correlations and things like that.",
            "And I think."
        ],
        [
            "I think that's the end, yes?",
            "Actually we have 2019 sample question.",
            "Yes.",
            "So about that last point.",
            "So who cares about converge?"
        ],
        [
            "I mean, as a different procedure you can just run the MCMC method and you're going to get predictions and your predictions are going to.",
            "Change as you collect more samples than exist.",
            "Do we really care about convergence?",
            "I guess in that case you're right, we don't really.",
            "I mean, we don't really care much about the convergence, but at the same time.",
            "It would also be nice to know at least that you are sort of close to the getting the appropriate samples, because what happens initially is that you start your chain randomly and then of course your model is going to be making crazy predictions.",
            "But as you keep going forward.",
            "So there is a little bit, but I think for this particular application in particular for the applications, when you have huge datasets and you want to run MCMC methods, then you mostly restricted by computational limits.",
            "So we run this procedure for two days and then we get our predictions and we just use those predictions and submitting it to the Netflix, let's say.",
            "So that's.",
            "Yes.",
            "You need to win $1,000,000.",
            "Maybe infinite.",
            "But it's.",
            "You know, it's hard to say how many features you need, but it was.",
            "Alright, right?",
            "Well if you extrapolate, I think the number was.",
            "20,000,000 or something like that, but.",
            "That would take a very long time.",
            "I mean it certainly."
        ],
        [
            "In the case that one of the nicest I think one of the nicest results here is that you know you really want to have big models when you're doing basean.",
            "Model averaging, right?",
            "You really want to have big complex models.",
            "I mean, and you shouldn't constrain yourself to having only 30 dimensions, which is what we do when we train these simple models.",
            "Because, you know, with these big models you have so many parameters that regularising this thing is very, very difficult, especially manually trying to pick on the validation set what you should be doing.",
            "But of course there is a computational.",
            "Challenge there, right?",
            "Because now you have to invert 300 by 300 matrices and there are a lot of the inversions and.",
            "Yes.",
            "Can you have like a different result for the computer dimensions?",
            "Is that Constance number examples or consent to be fine?",
            "Like what is the tradeoff between?",
            "I have fewer dimensions, but then I can compute more samples.",
            "Rights.",
            "So are you saying that for a given computational limit?",
            "What's the best thing to do?",
            "Ah, very good point.",
            "What happens with those methods is that.",
            "Um?",
            "We start our chains, not randomly, but we start our chains by learning map type of parameters and then starting our chains from those particular pronunciations.",
            "It certainly is the case that if you need a fast answer right, you probably want to map, or you probably want to do MCMC with like 60 dimensions if you really constrained by getting an answer in two hours, let's say.",
            "But for example, I can give you the number saying that 300 dimensional feature vectors, right?",
            "There's a big fairly big models, takes us about four days, four or five days to get the number over there, whereas 460 dimensional it may take us a few hours to get to the number that's there, I mean.",
            "Right?",
            "So as you get bigger models is computationally much much more challenging.",
            "And of course the improvement you get not.",
            "What if you had spent four five days training the first model?",
            "Alright, so I don't think that number would go significantly lower than this, right?",
            "So if you're interested in.",
            "In winning $1,000,000.",
            "If there is no constraint on your time, then you should be running it for a very, very long time, and you shouldn't probably be sticking to 300 dimensional, but maybe 1000 dimensional model, yeah?",
            "So it's also related question to looking at your computer keyboard at your very first block where."
        ],
        [
            "Yeah, so how we see here that the logistics team continues to go down the left block.",
            "The total runtime spent under 60 other efforts of the logistic event is opposed to the run time spent in information approach OK.",
            "Same time on the purple item there just a few minutes.",
            "Will it actually get so right?",
            "So to answer your first question.",
            "Logistic PMF on PMF.",
            "Roughly, you can train these models on Netflix in like half an hour.",
            "Or an hour, something like that.",
            "Something very fast because you're doing stochastic gradient descent and it's fantastic.",
            "Right now basean PMF if you start your Markov chains from some random configuration, you would run it to get to this number.",
            "It would probably take you for the dimensionality.",
            "This is a 30 dimensional model.",
            "This is also 30 dimensional model.",
            "This will probably take you a day, so of course it's much, much computationally more intensive.",
            "So.",
            "No, I think if you actually continue, it levels off at this number here, so it's certainly I'm very, very much convinced that the basean versions bitmap versions by significant margin.",
            "On the other hand, computationally these models are the Bayesian models are much, much more expensive.",
            "So what we actually do in practice is that we run this model.",
            "We take the parameters and we say start your Markov chain from those parameters.",
            "So to sort of have a reasonably initial conditions.",
            "One place review.",
            "I mean, we yeah, we typically running one chain, but you can also run multiple chains and start from different initialization points, but.",
            "That's a different issue, but.",
            "Because you can run multiple chains right starting from different local optimums in such.",
            "Yeah.",
            "Alright, so variational base there's been some work done on variational basin variationally M, but it was very surprising to us that other people have done it.",
            "We haven't done it.",
            "Other people have done it and the performance of variational methods is sits roughly around here.",
            "So people, people were trying to use because variationally M, for example, is the next logical step to sort of variational approximations.",
            "Next logical step to do, but those methods so far as I know, were not successful.",
            "Compared to the MCMC methods.",
            "Replacing a normal distribution with something else like this creature remaining or something like that.",
            "We do the same trick, right?",
            "You can do the same trick, except for now you will not have closed form conditionals, right?",
            "Odd I see if you have a close form conditional stand then I guess you could do that.",
            "Or you could also argue that you can put some other priors that you truly believe in and run hybrid Monte Carlo on this model.",
            "That would be an alternative thing to run, but for some reason we just found that Gibbs is working fairly fast and it's mixing fairly fast and you get results.",
            "Fairly fast as well, reasonable results.",
            "The last one last question, so this is really squirrel integer ordinal ratings from 1 to 7, one to five three.",
            "You're coming roughly within one Level 6 Seven yeah, considered by Netflix, accurate or good enough, that difference I would say is significant if you're trying to say the same, predict within plus or minus right.",
            "For them, I think it is significant.",
            "Yeah, because what happens is that.",
            "I think it is significant."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello so I'm wrestling so good enough and this is joint work with Anthony who is sitting over there I guess.",
                    "label": 0
                },
                {
                    "sent": "So what the title of the talk is Beijing probabilistic matrix factorization using Markov chain Monte Carlo.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so let me just put my problem in the context of what we're actually trying to solve will probably be easy to follow.",
                    "label": 0
                },
                {
                    "sent": "What we actually do, what we actually trying to solve is we're trying to solve collaborative filtering problem so we have users an you can think of having a lot of users right, and we have movies and we have some ratings provided by users and hear the ratings for different movies.",
                    "label": 0
                },
                {
                    "sent": "And then the goal is try to predict the ratings for.",
                    "label": 0
                },
                {
                    "sent": "Other movies, right?",
                    "label": 0
                },
                {
                    "sent": "So you're trying to predict the rating for user one and Movie 6, and this is typical collaborative filtering task.",
                    "label": 0
                },
                {
                    "sent": "So in terms of notation, we have M movies.",
                    "label": 1
                },
                {
                    "sent": "We have end users and we have an integer rating values.",
                    "label": 1
                },
                {
                    "sent": "That range from one to K right and the goal of probabilistic matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "Basically what we're trying to pursue is we're trying to decompose this matrix into the product of 2 low rank matrices, so again you can think of those as parameters in our model that we're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "And what I'm going to do note, as I'm going to art supply, J is the rating of user iPhone movie J and those using these are going to be latent using movie feature matrices and I have to specify the dimensionality which is going to be one of the parameters that I'm going to be specifying OK?",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The probabilistic matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "Initially we looked at simple models.",
                    "label": 0
                },
                {
                    "sent": "This is a very simple probabilistic model with the Gaussian observation noise and the graphical model is represented here.",
                    "label": 1
                },
                {
                    "sent": "So you have movie features you have user features and combining they give you the rating and so given the user movie features, which is going to say that the distribution over the rating is just is just going to be given by Gaussian distribution with mean that takes the dot product of the movie and user feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Plus some noise, right?",
                    "label": 1
                },
                {
                    "sent": "It's a very standard probabilistic set up now.",
                    "label": 0
                },
                {
                    "sent": "We're also going to put some price on top of our user movie feature vectors in particular, which is going to put Gaussian priors.",
                    "label": 0
                },
                {
                    "sent": "With zero means and some parameters here.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "A very simple model and one efficient way of learning practices in this model is just doing a simple maximum posterior learning, which is we have we're going to be maximized maximizing the work by stereo.",
                    "label": 0
                },
                {
                    "sent": "The movie and user feature vectors with fixed value of the hyperparameters, and you can show which is a standard result is that this is just equivalent of minimizing the sum of squared error terms with some quadratic regularization terms.",
                    "label": 1
                },
                {
                    "sent": "So what's happening in this equation is that we're trying to find the values of these matrices of these low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "Such that they give good predictions for the ratings for which we actually have the training data.",
                    "label": 0
                },
                {
                    "sent": "And then there is a regularization term that tries to pull those movie and user feature matrices 2 towards 0.",
                    "label": 0
                },
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "So it's a simple formulation and now.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The question is.",
                    "label": 0
                },
                {
                    "sent": "One of the things one of the nice formulation about this this particular doing map estimation.",
                    "label": 0
                },
                {
                    "sent": "It's very fast.",
                    "label": 0
                },
                {
                    "sent": "You can do stochastic gradient descent, you can have millions of users and many, many thousands of movies.",
                    "label": 0
                },
                {
                    "sent": "You can just take gradient steps in optimizing over these matrices.",
                    "label": 0
                },
                {
                    "sent": "Unv or doing some kind of alternating optimization.",
                    "label": 0
                },
                {
                    "sent": "But the main drawback of these models is that how do you do complexity control?",
                    "label": 1
                },
                {
                    "sent": "In particular, if you think of an application where we have million users and we have 30 dimensional feature vectors, then we're going to have you know half a million.",
                    "label": 0
                },
                {
                    "sent": "Sorry, we're going to have.",
                    "label": 0
                },
                {
                    "sent": "I guess about 15 million parameters.",
                    "label": 0
                },
                {
                    "sent": "So the question, is these guys regularization coefficients become very, very important?",
                    "label": 0
                },
                {
                    "sent": "So how do you complexity control is?",
                    "label": 0
                },
                {
                    "sent": "It's a little bit hard to do, and in general we're usually interested in predicting the ratings for new user movie pairs.",
                    "label": 1
                },
                {
                    "sent": "But you know, we're not that much interested in actually getting the values of the parameters themselves.",
                    "label": 0
                },
                {
                    "sent": "And that sort of motivated us to look at.",
                    "label": 0
                },
                {
                    "sent": "Asian version of this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "Has this form right?",
                    "label": 0
                },
                {
                    "sent": "We just have a likelihood function again in the same way we put the priors.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian priors on top of our latent movie and user feature vectors or user movie feature matrices here and then.",
                    "label": 1
                },
                {
                    "sent": "We also put the gas in which price over the user movie hyperparameters.",
                    "label": 1
                },
                {
                    "sent": "So we just sort of represented.",
                    "label": 0
                },
                {
                    "sent": "In this form here.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Now model this is observed.",
                    "label": 0
                },
                {
                    "sent": "All of these values are now random variables, right?",
                    "label": 0
                },
                {
                    "sent": "And again, you can see that the way we're choosing our price is is.",
                    "label": 0
                },
                {
                    "sent": "Their conjugates so you can do sort of efficient.",
                    "label": 0
                },
                {
                    "sent": "You can see that doing something like dips would work.",
                    "label": 0
                },
                {
                    "sent": "Now the question is whether it's good to put conjugate priors or not is a different.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what we're going to do is to get the predictive distribution of the GNU rating.",
                    "label": 0
                },
                {
                    "sent": "So often you rating for the user movie pair given the observed rating R, and in general Bayesian framework that requires us to marginalized or integrate over the parameters and hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "That's ultimately what we want to do.",
                    "label": 0
                },
                {
                    "sent": "So and once we do that, well, we can do a simple Monte Carlo approximation right where the samples are generated by Markov chain with the stationary distribution, which is the posterior distribution of our model parameters.",
                    "label": 1
                },
                {
                    "sent": "It's just a general set up for doing inference in directed graphical models.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "In our implementation, we use Gibbs sampling.",
                    "label": 1
                },
                {
                    "sent": "We generate the samples, but of course you can use any other Markov chain Monte Carlo procedure.",
                    "label": 0
                },
                {
                    "sent": "And of course you can see that due to the conjugate priors, the conditional distributions are easy to sample from, so that was our initial motivation and one other thing to notice is that the posterior distribution of a movie user, latent feature matrices dicapo, so they become as a product of individual user features.",
                    "label": 1
                },
                {
                    "sent": "So you can paralyze it.",
                    "label": 0
                },
                {
                    "sent": "In other words, you can make it efficient.",
                    "label": 0
                },
                {
                    "sent": "In that for that particular graphical model that we are considering.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can see from.",
                    "label": 0
                },
                {
                    "sent": "This graph here is that conditional on these values and these values the posterior distribution over the latent.",
                    "label": 0
                },
                {
                    "sent": "User features decouples the same As for the movie features.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, um.",
                    "label": 0
                },
                {
                    "sent": "So we do a standard Gibbs sampler, which is to say that conditional on.",
                    "label": 0
                },
                {
                    "sent": "Movie features we can update.",
                    "label": 0
                },
                {
                    "sent": "We can get a sample from.",
                    "label": 0
                },
                {
                    "sent": "We can get a sample from the posterior over these latent variables and same As for those guys.",
                    "label": 0
                },
                {
                    "sent": "And then for each feature vector for each we get a sample for each movie feature vector by conditioning on the user feature vectors, right?",
                    "label": 0
                },
                {
                    "sent": "So that's just a standard Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "So in other words, conditional on this random variable and these are in the variables.",
                    "label": 0
                },
                {
                    "sent": "We just get samples from those guys and we alternate.",
                    "label": 0
                },
                {
                    "sent": "So now this is all very.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "100 and I haven't told you anything.",
                    "label": 0
                },
                {
                    "sent": "You but.",
                    "label": 0
                },
                {
                    "sent": "One of the things that we were looking at initially we were sort of playing with the Netflix data set and it's a huge data set and the main question was that how do we deal with this regularization things and what we typically do is we run cross validation.",
                    "label": 0
                },
                {
                    "sent": "We're trying to figure out what the appropriate hyperparameters should be in just to give you the flavor.",
                    "label": 0
                },
                {
                    "sent": "The Netflix datasets is fairly large, it contains 100 million ratings, contains half a million users, and about 18,000 movies.",
                    "label": 1
                },
                {
                    "sent": "There is a validation set.",
                    "label": 0
                },
                {
                    "sent": "And there is a test set for which you must be predicting.",
                    "label": 0
                },
                {
                    "sent": "Use a movie ratings.",
                    "label": 0
                },
                {
                    "sent": "In the data set itself is very imbalanced, so you know there are users who give only one rating and their users could give 1500 ratings.",
                    "label": 1
                },
                {
                    "sent": "And the performance is also assessed by, you know you make predictions, you submit it to the Netflix and they give you the results.",
                    "label": 1
                },
                {
                    "sent": "So it's one of those things that it's hard to cheat on who accidentally cheat on, right?",
                    "label": 0
                },
                {
                    "sent": "Now one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The things that.",
                    "label": 0
                },
                {
                    "sent": "You know the question is the nice thing about probabilistic matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "Doing map version of probabilistic modifications but very efficient.",
                    "label": 0
                },
                {
                    "sent": "And you can do it fast.",
                    "label": 0
                },
                {
                    "sent": "An initially we were thinking well, can you actually take a fully Bayesian approach?",
                    "label": 1
                },
                {
                    "sent": "You have 100 million ratings, it's big data set, so typically if you look at attitudes of all the people, is that MCMC methods are really really for small problems and would it really work in this particular context?",
                    "label": 0
                },
                {
                    "sent": "And we actually were surprised that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Indeed, it does work.",
                    "label": 0
                },
                {
                    "sent": "So here the results.",
                    "label": 0
                },
                {
                    "sent": "They're showing you.",
                    "label": 0
                },
                {
                    "sent": "This is the SVD model, which is basically probabilistic matrix factorization model, but without any regularization.",
                    "label": 0
                },
                {
                    "sent": "This is probabilistic matrix factorization model.",
                    "label": 0
                },
                {
                    "sent": "This is slightly different version of probabilistic matrix factorization model.",
                    "label": 0
                },
                {
                    "sent": "And here's what basean PMF gives you so.",
                    "label": 0
                },
                {
                    "sent": "It gives you much better root mean squared errors or it gives you much better predictions compared to the map estimates, right?",
                    "label": 0
                },
                {
                    "sent": "And to us that was a little bit surprising and the other surprising thing is that you know you can actually run these models.",
                    "label": 0
                },
                {
                    "sent": "With 30 dimensional feature vectors or 60 dimensional feature vectors which are quite big models in you know.",
                    "label": 0
                },
                {
                    "sent": "These are the times about 11 hours or maybe 47 hours a couple of days.",
                    "label": 0
                },
                {
                    "sent": "And you can see that the gap between those models is actually quite large.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's going on is?",
                    "label": 0
                },
                {
                    "sent": "Alright, so here are the results by showing that.",
                    "label": 0
                },
                {
                    "sent": "As you increase the feature dimensionality, which is to say that as you make your model bigger.",
                    "label": 0
                },
                {
                    "sent": "What happens with the basean PMF?",
                    "label": 0
                },
                {
                    "sent": "The error the performance improves, which is was, you know, sort of a good result for us because it basically says that you know the Bayesian approach doesn't really require you limiting the complexity of the model based on the training data.",
                    "label": 1
                },
                {
                    "sent": "Wait, which is not the case for the map based models where we actually trying to learn the parameters of the model and you know, for the 300 dimensional feature vectors and half a million users think you have about 75 million parameters in your model.",
                    "label": 0
                },
                {
                    "sent": "So if you have so many parameters in your model and you don't have nearly enough data, then fitting those models becomes really challenging or regularising those models becomes a very challenging task.",
                    "label": 0
                },
                {
                    "sent": "So again, the I guess the.",
                    "label": 0
                },
                {
                    "sent": "The main idea here is that as you make your model bigger for this real task, the basean methods actually give you better performance or the performance improves.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "One of the other good things about you know finding or getting samples from the posterior distribution is that you can take uncertainty into account.",
                    "label": 0
                },
                {
                    "sent": "Ann, it's you can think it's better than just doing map estimates, which as I guess the reason why you want to get the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "So what you what I'm showing you here is basically take basically taking for users right?",
                    "label": 0
                },
                {
                    "sent": "And the first user has only four ratings in the training set.",
                    "label": 0
                },
                {
                    "sent": "Now with the full ratings, obviously we can't infer the preferences of that particular user, so this plot basically shows what kind of predictions we get on the test set as we're running our Gibbs sampler, so you can see that we are very, very uncertain about predictions that this user is making right.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, this is the user that has 660 movies, so this is the user who has rated 660 movies and you can see that on the test set for a particular movie.",
                    "label": 0
                },
                {
                    "sent": "We're fairly confident of what the rating for that movie should be.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "You can you can take this information to account and what you what we're showing here is if you have.",
                    "label": 0
                },
                {
                    "sent": "Users grouped by number of ratings in the training set, so these are the users that provided.",
                    "label": 0
                },
                {
                    "sent": "Less than five ratings.",
                    "label": 0
                },
                {
                    "sent": "These are the users that provided from 6 to 10 ratings in the training set and what happens here is that the map, I guess is doing much worse than the Asian version.",
                    "label": 0
                },
                {
                    "sent": "And of course, as you get users who provided more than I guess 600 ratings, then those two errors match.",
                    "label": 0
                },
                {
                    "sent": "Which is to say that you know your posterior in those cases is fairly picked around particular parameter configuration.",
                    "label": 0
                },
                {
                    "sent": "So most of the wind with the beige and model models we're getting here, so these models are very nice models because they integrate out uncertainty, particularly for users that have very few ratings.",
                    "label": 0
                },
                {
                    "sent": "Which is the case for the Netflix data set.",
                    "label": 0
                },
                {
                    "sent": "Of course, one way you could deal with the problem is to throw out all the users that have less than.",
                    "label": 0
                },
                {
                    "sent": "I guess 20 ratings and then use map PMF, right?",
                    "label": 0
                },
                {
                    "sent": "But the whole point here is that we do want to take care of take into account users that have very few right?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "And just to come to the conclusions.",
                    "label": 0
                },
                {
                    "sent": "I guess the main message here is that we can successfully apply Bayesian methods to very large datasets.",
                    "label": 0
                },
                {
                    "sent": "And we also found that these methods significantly have high significantly accuracy compared to standard map type of methods, although it is the case that with map methods the optimization is much faster.",
                    "label": 0
                },
                {
                    "sent": "So there is also a balancing going on.",
                    "label": 0
                },
                {
                    "sent": "And also the Bayesian methods provide the predictive distribution, so you can take confidence into account.",
                    "label": 1
                },
                {
                    "sent": "And of course, one drawback of the Bayesian methods is that you know when you're running your Markov chain.",
                    "label": 1
                },
                {
                    "sent": "It's very hard to determine when you actually converge to the digital distribution, but of course in our cases we just running these Markov chains and we're looking at.",
                    "label": 0
                },
                {
                    "sent": "We're looking at the values of our hyperparameters and see whether they stabilize and will look at the auto correlations and things like that.",
                    "label": 0
                },
                {
                    "sent": "And I think.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that's the end, yes?",
                    "label": 0
                },
                {
                    "sent": "Actually we have 2019 sample question.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So about that last point.",
                    "label": 0
                },
                {
                    "sent": "So who cares about converge?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, as a different procedure you can just run the MCMC method and you're going to get predictions and your predictions are going to.",
                    "label": 0
                },
                {
                    "sent": "Change as you collect more samples than exist.",
                    "label": 0
                },
                {
                    "sent": "Do we really care about convergence?",
                    "label": 0
                },
                {
                    "sent": "I guess in that case you're right, we don't really.",
                    "label": 0
                },
                {
                    "sent": "I mean, we don't really care much about the convergence, but at the same time.",
                    "label": 0
                },
                {
                    "sent": "It would also be nice to know at least that you are sort of close to the getting the appropriate samples, because what happens initially is that you start your chain randomly and then of course your model is going to be making crazy predictions.",
                    "label": 0
                },
                {
                    "sent": "But as you keep going forward.",
                    "label": 0
                },
                {
                    "sent": "So there is a little bit, but I think for this particular application in particular for the applications, when you have huge datasets and you want to run MCMC methods, then you mostly restricted by computational limits.",
                    "label": 0
                },
                {
                    "sent": "So we run this procedure for two days and then we get our predictions and we just use those predictions and submitting it to the Netflix, let's say.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "You need to win $1,000,000.",
                    "label": 0
                },
                {
                    "sent": "Maybe infinite.",
                    "label": 0
                },
                {
                    "sent": "But it's.",
                    "label": 0
                },
                {
                    "sent": "You know, it's hard to say how many features you need, but it was.",
                    "label": 0
                },
                {
                    "sent": "Alright, right?",
                    "label": 0
                },
                {
                    "sent": "Well if you extrapolate, I think the number was.",
                    "label": 0
                },
                {
                    "sent": "20,000,000 or something like that, but.",
                    "label": 0
                },
                {
                    "sent": "That would take a very long time.",
                    "label": 0
                },
                {
                    "sent": "I mean it certainly.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the case that one of the nicest I think one of the nicest results here is that you know you really want to have big models when you're doing basean.",
                    "label": 0
                },
                {
                    "sent": "Model averaging, right?",
                    "label": 0
                },
                {
                    "sent": "You really want to have big complex models.",
                    "label": 0
                },
                {
                    "sent": "I mean, and you shouldn't constrain yourself to having only 30 dimensions, which is what we do when we train these simple models.",
                    "label": 0
                },
                {
                    "sent": "Because, you know, with these big models you have so many parameters that regularising this thing is very, very difficult, especially manually trying to pick on the validation set what you should be doing.",
                    "label": 0
                },
                {
                    "sent": "But of course there is a computational.",
                    "label": 0
                },
                {
                    "sent": "Challenge there, right?",
                    "label": 0
                },
                {
                    "sent": "Because now you have to invert 300 by 300 matrices and there are a lot of the inversions and.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Can you have like a different result for the computer dimensions?",
                    "label": 0
                },
                {
                    "sent": "Is that Constance number examples or consent to be fine?",
                    "label": 0
                },
                {
                    "sent": "Like what is the tradeoff between?",
                    "label": 0
                },
                {
                    "sent": "I have fewer dimensions, but then I can compute more samples.",
                    "label": 0
                },
                {
                    "sent": "Rights.",
                    "label": 0
                },
                {
                    "sent": "So are you saying that for a given computational limit?",
                    "label": 0
                },
                {
                    "sent": "What's the best thing to do?",
                    "label": 0
                },
                {
                    "sent": "Ah, very good point.",
                    "label": 0
                },
                {
                    "sent": "What happens with those methods is that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We start our chains, not randomly, but we start our chains by learning map type of parameters and then starting our chains from those particular pronunciations.",
                    "label": 0
                },
                {
                    "sent": "It certainly is the case that if you need a fast answer right, you probably want to map, or you probably want to do MCMC with like 60 dimensions if you really constrained by getting an answer in two hours, let's say.",
                    "label": 0
                },
                {
                    "sent": "But for example, I can give you the number saying that 300 dimensional feature vectors, right?",
                    "label": 0
                },
                {
                    "sent": "There's a big fairly big models, takes us about four days, four or five days to get the number over there, whereas 460 dimensional it may take us a few hours to get to the number that's there, I mean.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So as you get bigger models is computationally much much more challenging.",
                    "label": 0
                },
                {
                    "sent": "And of course the improvement you get not.",
                    "label": 0
                },
                {
                    "sent": "What if you had spent four five days training the first model?",
                    "label": 0
                },
                {
                    "sent": "Alright, so I don't think that number would go significantly lower than this, right?",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in.",
                    "label": 0
                },
                {
                    "sent": "In winning $1,000,000.",
                    "label": 0
                },
                {
                    "sent": "If there is no constraint on your time, then you should be running it for a very, very long time, and you shouldn't probably be sticking to 300 dimensional, but maybe 1000 dimensional model, yeah?",
                    "label": 0
                },
                {
                    "sent": "So it's also related question to looking at your computer keyboard at your very first block where.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so how we see here that the logistics team continues to go down the left block.",
                    "label": 0
                },
                {
                    "sent": "The total runtime spent under 60 other efforts of the logistic event is opposed to the run time spent in information approach OK.",
                    "label": 0
                },
                {
                    "sent": "Same time on the purple item there just a few minutes.",
                    "label": 0
                },
                {
                    "sent": "Will it actually get so right?",
                    "label": 0
                },
                {
                    "sent": "So to answer your first question.",
                    "label": 0
                },
                {
                    "sent": "Logistic PMF on PMF.",
                    "label": 0
                },
                {
                    "sent": "Roughly, you can train these models on Netflix in like half an hour.",
                    "label": 0
                },
                {
                    "sent": "Or an hour, something like that.",
                    "label": 0
                },
                {
                    "sent": "Something very fast because you're doing stochastic gradient descent and it's fantastic.",
                    "label": 0
                },
                {
                    "sent": "Right now basean PMF if you start your Markov chains from some random configuration, you would run it to get to this number.",
                    "label": 0
                },
                {
                    "sent": "It would probably take you for the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "This is a 30 dimensional model.",
                    "label": 0
                },
                {
                    "sent": "This is also 30 dimensional model.",
                    "label": 0
                },
                {
                    "sent": "This will probably take you a day, so of course it's much, much computationally more intensive.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "No, I think if you actually continue, it levels off at this number here, so it's certainly I'm very, very much convinced that the basean versions bitmap versions by significant margin.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, computationally these models are the Bayesian models are much, much more expensive.",
                    "label": 0
                },
                {
                    "sent": "So what we actually do in practice is that we run this model.",
                    "label": 0
                },
                {
                    "sent": "We take the parameters and we say start your Markov chain from those parameters.",
                    "label": 0
                },
                {
                    "sent": "So to sort of have a reasonably initial conditions.",
                    "label": 0
                },
                {
                    "sent": "One place review.",
                    "label": 0
                },
                {
                    "sent": "I mean, we yeah, we typically running one chain, but you can also run multiple chains and start from different initialization points, but.",
                    "label": 0
                },
                {
                    "sent": "That's a different issue, but.",
                    "label": 0
                },
                {
                    "sent": "Because you can run multiple chains right starting from different local optimums in such.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Alright, so variational base there's been some work done on variational basin variationally M, but it was very surprising to us that other people have done it.",
                    "label": 0
                },
                {
                    "sent": "We haven't done it.",
                    "label": 0
                },
                {
                    "sent": "Other people have done it and the performance of variational methods is sits roughly around here.",
                    "label": 0
                },
                {
                    "sent": "So people, people were trying to use because variationally M, for example, is the next logical step to sort of variational approximations.",
                    "label": 0
                },
                {
                    "sent": "Next logical step to do, but those methods so far as I know, were not successful.",
                    "label": 0
                },
                {
                    "sent": "Compared to the MCMC methods.",
                    "label": 0
                },
                {
                    "sent": "Replacing a normal distribution with something else like this creature remaining or something like that.",
                    "label": 0
                },
                {
                    "sent": "We do the same trick, right?",
                    "label": 0
                },
                {
                    "sent": "You can do the same trick, except for now you will not have closed form conditionals, right?",
                    "label": 0
                },
                {
                    "sent": "Odd I see if you have a close form conditional stand then I guess you could do that.",
                    "label": 0
                },
                {
                    "sent": "Or you could also argue that you can put some other priors that you truly believe in and run hybrid Monte Carlo on this model.",
                    "label": 0
                },
                {
                    "sent": "That would be an alternative thing to run, but for some reason we just found that Gibbs is working fairly fast and it's mixing fairly fast and you get results.",
                    "label": 0
                },
                {
                    "sent": "Fairly fast as well, reasonable results.",
                    "label": 0
                },
                {
                    "sent": "The last one last question, so this is really squirrel integer ordinal ratings from 1 to 7, one to five three.",
                    "label": 0
                },
                {
                    "sent": "You're coming roughly within one Level 6 Seven yeah, considered by Netflix, accurate or good enough, that difference I would say is significant if you're trying to say the same, predict within plus or minus right.",
                    "label": 0
                },
                {
                    "sent": "For them, I think it is significant.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because what happens is that.",
                    "label": 0
                },
                {
                    "sent": "I think it is significant.",
                    "label": 0
                }
            ]
        }
    }
}