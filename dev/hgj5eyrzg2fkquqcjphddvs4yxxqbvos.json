{
    "id": "hgj5eyrzg2fkquqcjphddvs4yxxqbvos",
    "title": "Online Learning with Composite Loss Functions",
    "info": {
        "author": [
            "Ofer Dekel, Microsoft Research"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/colt2014_dekel_functions/",
    "segmentation": [
        [
            "OK hi."
        ],
        [
            "So I'd like to start by reviewing the classic setting just so we have the notation.",
            "So this is the oblivious multi armed bandit.",
            "I suppose many of you already are familiar with this, so we have a player and he plays a repeated game against an oblivious adversary.",
            "So what's an oblivious adversary?",
            "So this is a guy who is not allowed to react to the players actions, so he's oblivious to the past, and the player plays arms.",
            "So this is just the set one through K. That's the action space.",
            "And here's the game.",
            "So the adversary begins by defining all the loss functions.",
            "So we're going to play capital T rounds and defines these loss functions in advance, because he's oblivious.",
            "So we assign the loss to each arm at each iteration of the game, and then he goes to sleep.",
            "That's his role in the game, and then the player wakes up and starts iterating.",
            "So on each round the player chooses inaction, choosing an arm.",
            "Does this work chooses an arm he incurs some loss and then he views he receives bandit feedback so bad feedback is just the value of his loss, so incur some losses.",
            "He just gets to see that one number, so that's the setting.",
            "This is the classic oblivious multi arm bandit set."
        ],
        [
            "And as we've seen in the previous talks, we evaluate this by measuring the players regret.",
            "So this is just the difference between his expected cumulative loss and the loss of the best fixed action.",
            "In hindsight, this is his regret and that measures the performance of a particular player against the specific sequence of loss functions.",
            "But if we want to measure the objective difficulty of the game itself, then we define this minimax regret, which is just the regret when we assume that both the player and the adversary play optimally.",
            "So this is minimax regret.",
            "We've seen all along.",
            "And now in the classic oblivious setting, the X3 algorithm do 2 hours of Yankee friendship theory shows that the minimax regret this game is Theta square root T. Recall game like this when we see regrets, square root T will call this an easy game, so the oblivious cases easy.",
            "So this is the classic stuff.",
            "And now let's make it a little bit more interesting.",
            "So let's."
        ],
        [
            "Give some power, little bit more power to the adversary and allow him to adapt to the player's past actions, but just a little bit.",
            "So we define this one memory adversary and this guy is allowed to look one action into the past and penalize a player for one action in the past.",
            "So again, the set of actions is just the key arms, and again it's very similar, so the adversary defines all the loss functions in advance.",
            "But now these loss functions, if you'll notice, depend on two actions, right?",
            "So each FT defines F1 through F, capital T, and each one of these guys takes 2 actions and Maps them to a loss.",
            "And then he goes to sleep and then we start iterating so the player again chooses one arm on each round.",
            "But the loss that he suffers on that round is a function of his previous arm.",
            "His sorry, his current action in the previous section.",
            "OK, so it's penalized proportionate to what he did now, and also for what he chose yesterday.",
            "So that's the only difference.",
            "And then again, bandit feedback.",
            "He gets to see that one number, the last he suffered.",
            "So that's the only thing we changed."
        ],
        [
            "Again, we measure regret in the very same way, so it's the regret is the difference between the expected cognitive loss in the player and the loss of the strategy that would just stick to that one fixed best arm throughout the entire game, and minimax regret is the difficulty of the game.",
            "It's the regret when both the player and the adversary play optimally.",
            "And then recently my coauthors and I proved that in fact, this makes the game much, much harder.",
            "So before we had square root T regret, and now we have regret which is Theta of T to the 2/3.",
            "So just allowing the adversary to look one step back and that's it already makes this jump from square root T to the 2/3.",
            "OK, we call games like this hard games so square these can be easy to 2/3 is going to be hard and this was kind of exciting because in this setting of online learning and the bandit setting and so on.",
            "You know previously the sport was to say, here's an algorithm and has Red Square deal.",
            "Here's this modification and it's also screw in screw in.",
            "Every time you know we would see these square with regret rates in the papers and this in this in this specific setting when we have K actions and this kind of band interaction was the first time that we see a regret which is 2/3.",
            "So this was kind of exciting for us."
        ],
        [
            "And how we prove this?",
            "So let me give you a very, very brief summary of how we did this.",
            "So we had to invent something.",
            "You and this is something that we called the multi skill random walk.",
            "So let me explain what that is.",
            "OK, so now we're going to construct the last sequence and see how we get that to the 2/3 regret from the player.",
            "So assume that there are just two arms and let's define the loss of the first arm.",
            "OK, so we draw these ID random Gaussians and then we sum them in a very particular way.",
            "So the loss again, we're setting the loss of the first time, so an iteration one, the loss is going to be just side one.",
            "The loss on the second iteration around 2:00 it's going to be just excited to on round three, it's going to be side 2 plus side three, so you add along these paths at round four, the losses going before it around 5:00 is gonna be excited for plus size 5, and so on, and you can see how this is kind of like a Gaussian random walk.",
            "We're adding these Gaussians, but in this kind of multi multi scale way.",
            "So that's the loss of the first arm OK stochastic construction for the first loss.",
            "And in the second arm, the loss of that guy is just going to be the terministic function of the loss of the first time.",
            "So it's either going to be epsilon more or epsilon less with equal probability.",
            "OK, So what we get?"
        ],
        [
            "Our two loss sequences that are just epsilon part just this constant epsilon and the player doesn't know which one is better.",
            "And Moreover, so each round he's playing these rounds and have to choose red or blue on each, running only gets to see that one number, so he never gets to see the pair.",
            "If you could see that there even for one round, he would know which arm is better and he just stick to that.",
            "But he can see he just sees one of these values in the really epsilon close.",
            "And we were able to prove two properties for this sequence.",
            "So first of all, it's the easy one.",
            "It's as long as he doesn't recognize which ARM is the better arm.",
            "Then with probability close to half, he's going to play.",
            "You know, maybe the sub optimal arm and accumulate a regret of epsilon on each one of these iterations that he plays a suboptimal arm.",
            "So that's the problem and then kind of the magical property of this of this process is that we were able to relate his ability to identify the better armed with the number of switches that he does, right?",
            "So he has to switch back and forth between the arms to get some information about who's better.",
            "And specifically has to switch, so if he hasn't switched one over epsilon squared times, he has no way to know which ARM is better.",
            "So this is the important part.",
            "And when you have these two properties then you know almost immediate corollary of these two things, which is what we showed in the previous paper.",
            "Is that if we penalize him explicitly for switching costs.",
            "So if we define this game called the Banner with switching costs every time he switches from aren't arm has to pay, then his minimax regret is going to jump to 2/3 because you have to switch a lot in order to take the better arm.",
            "And if he doesn't take the better arm he's just going to accumulate all disregard all along the cover on the game."
        ],
        [
            "OK, now to this paper.",
            "So we were able to show that, but that was kind of very very tailored to the switching cost setting and we asked ourselves, you know, is that problem unique?",
            "And we find other online learning problems that have this property of teeth to the 2/3 regret and Moreover we are asked you know how, how much can we weaken this one memory adaptive adversary and make him almost almost oblivious and still be able to define this hard problem.",
            "So this is what we came up with.",
            "So this is called the composite loss function setting.",
            "So in this setting there's going to be a special case of the one memory adaptive adversary.",
            "So he chooses oblivious loss functions.",
            "He assigns a predetermined loss to each arm on each one of the rounds, so he's oblivious, and that's it.",
            "That's all he does, but then the loss itself is defined by this thing called the last combining function.",
            "So there's a function G which takes 2 loss values and outputs them, combine them into one, and the loss of the player plays is just this combination of today's loss in yesterday's loss.",
            "OK, so for example, take them in function of the Max function.",
            "OK, so what's the setting?",
            "Again, we have K arms.",
            "We have T rounds.",
            "Adversary defines a law secretly.",
            "Underneath each one of the arms and I don't know what that is in the player starts to play, starts pulling arms.",
            "I'm exposing these numbers and maybe even gets to see the numbers, so these are oblivious losses and he sees them in the bandit way.",
            "The only thing is that the loss that he incurs is maybe the minimum of today's number and yesterday's number.",
            "OK, the number I saw now and the number of side yesterday so it's almost oblivious.",
            "I mean you know the adaptation and their dependence on history is only through this deterministic known constant simple function G, which is maybe mid or Max."
        ],
        [
            "And the result is that you know, surprisingly, that's enough to inflict this teacher.",
            "The 2/3 regret.",
            "OK, so he's almost oblivious, and that's what he did, and the technique.",
            "I just want to say one word about that and you have to come to the poster to hear the details.",
            "So I mean this is different than switching costs were not with them in function were not explicitly charging you for switching between arms.",
            "Yet in the proof, we relied on the previous switching class result.",
            "We were able to basically modify the."
        ],
        [
            "Sequence by spiking it's we were able to add these spikes that have this property that they kind of implicitly penalize switches so they're hidden.",
            "He doesn't even know when you're the player you're switching.",
            "You don't even know that you're suffering this loss there.",
            "Hidden under the noise level, yet they're big enough.",
            "So you know, impose something which is effectively almost like a constant switching cost per each one of the switches, and that's how we reduced to the switching color setting.",
            "That's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK hi.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'd like to start by reviewing the classic setting just so we have the notation.",
                    "label": 0
                },
                {
                    "sent": "So this is the oblivious multi armed bandit.",
                    "label": 1
                },
                {
                    "sent": "I suppose many of you already are familiar with this, so we have a player and he plays a repeated game against an oblivious adversary.",
                    "label": 1
                },
                {
                    "sent": "So what's an oblivious adversary?",
                    "label": 0
                },
                {
                    "sent": "So this is a guy who is not allowed to react to the players actions, so he's oblivious to the past, and the player plays arms.",
                    "label": 0
                },
                {
                    "sent": "So this is just the set one through K. That's the action space.",
                    "label": 1
                },
                {
                    "sent": "And here's the game.",
                    "label": 0
                },
                {
                    "sent": "So the adversary begins by defining all the loss functions.",
                    "label": 0
                },
                {
                    "sent": "So we're going to play capital T rounds and defines these loss functions in advance, because he's oblivious.",
                    "label": 0
                },
                {
                    "sent": "So we assign the loss to each arm at each iteration of the game, and then he goes to sleep.",
                    "label": 1
                },
                {
                    "sent": "That's his role in the game, and then the player wakes up and starts iterating.",
                    "label": 0
                },
                {
                    "sent": "So on each round the player chooses inaction, choosing an arm.",
                    "label": 0
                },
                {
                    "sent": "Does this work chooses an arm he incurs some loss and then he views he receives bandit feedback so bad feedback is just the value of his loss, so incur some losses.",
                    "label": 0
                },
                {
                    "sent": "He just gets to see that one number, so that's the setting.",
                    "label": 0
                },
                {
                    "sent": "This is the classic oblivious multi arm bandit set.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as we've seen in the previous talks, we evaluate this by measuring the players regret.",
                    "label": 1
                },
                {
                    "sent": "So this is just the difference between his expected cumulative loss and the loss of the best fixed action.",
                    "label": 0
                },
                {
                    "sent": "In hindsight, this is his regret and that measures the performance of a particular player against the specific sequence of loss functions.",
                    "label": 0
                },
                {
                    "sent": "But if we want to measure the objective difficulty of the game itself, then we define this minimax regret, which is just the regret when we assume that both the player and the adversary play optimally.",
                    "label": 1
                },
                {
                    "sent": "So this is minimax regret.",
                    "label": 0
                },
                {
                    "sent": "We've seen all along.",
                    "label": 0
                },
                {
                    "sent": "And now in the classic oblivious setting, the X3 algorithm do 2 hours of Yankee friendship theory shows that the minimax regret this game is Theta square root T. Recall game like this when we see regrets, square root T will call this an easy game, so the oblivious cases easy.",
                    "label": 0
                },
                {
                    "sent": "So this is the classic stuff.",
                    "label": 0
                },
                {
                    "sent": "And now let's make it a little bit more interesting.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give some power, little bit more power to the adversary and allow him to adapt to the player's past actions, but just a little bit.",
                    "label": 0
                },
                {
                    "sent": "So we define this one memory adversary and this guy is allowed to look one action into the past and penalize a player for one action in the past.",
                    "label": 0
                },
                {
                    "sent": "So again, the set of actions is just the key arms, and again it's very similar, so the adversary defines all the loss functions in advance.",
                    "label": 1
                },
                {
                    "sent": "But now these loss functions, if you'll notice, depend on two actions, right?",
                    "label": 0
                },
                {
                    "sent": "So each FT defines F1 through F, capital T, and each one of these guys takes 2 actions and Maps them to a loss.",
                    "label": 0
                },
                {
                    "sent": "And then he goes to sleep and then we start iterating so the player again chooses one arm on each round.",
                    "label": 0
                },
                {
                    "sent": "But the loss that he suffers on that round is a function of his previous arm.",
                    "label": 0
                },
                {
                    "sent": "His sorry, his current action in the previous section.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's penalized proportionate to what he did now, and also for what he chose yesterday.",
                    "label": 0
                },
                {
                    "sent": "So that's the only difference.",
                    "label": 1
                },
                {
                    "sent": "And then again, bandit feedback.",
                    "label": 0
                },
                {
                    "sent": "He gets to see that one number, the last he suffered.",
                    "label": 0
                },
                {
                    "sent": "So that's the only thing we changed.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, we measure regret in the very same way, so it's the regret is the difference between the expected cognitive loss in the player and the loss of the strategy that would just stick to that one fixed best arm throughout the entire game, and minimax regret is the difficulty of the game.",
                    "label": 1
                },
                {
                    "sent": "It's the regret when both the player and the adversary play optimally.",
                    "label": 0
                },
                {
                    "sent": "And then recently my coauthors and I proved that in fact, this makes the game much, much harder.",
                    "label": 0
                },
                {
                    "sent": "So before we had square root T regret, and now we have regret which is Theta of T to the 2/3.",
                    "label": 0
                },
                {
                    "sent": "So just allowing the adversary to look one step back and that's it already makes this jump from square root T to the 2/3.",
                    "label": 0
                },
                {
                    "sent": "OK, we call games like this hard games so square these can be easy to 2/3 is going to be hard and this was kind of exciting because in this setting of online learning and the bandit setting and so on.",
                    "label": 0
                },
                {
                    "sent": "You know previously the sport was to say, here's an algorithm and has Red Square deal.",
                    "label": 0
                },
                {
                    "sent": "Here's this modification and it's also screw in screw in.",
                    "label": 0
                },
                {
                    "sent": "Every time you know we would see these square with regret rates in the papers and this in this in this specific setting when we have K actions and this kind of band interaction was the first time that we see a regret which is 2/3.",
                    "label": 0
                },
                {
                    "sent": "So this was kind of exciting for us.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And how we prove this?",
                    "label": 0
                },
                {
                    "sent": "So let me give you a very, very brief summary of how we did this.",
                    "label": 0
                },
                {
                    "sent": "So we had to invent something.",
                    "label": 0
                },
                {
                    "sent": "You and this is something that we called the multi skill random walk.",
                    "label": 1
                },
                {
                    "sent": "So let me explain what that is.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we're going to construct the last sequence and see how we get that to the 2/3 regret from the player.",
                    "label": 0
                },
                {
                    "sent": "So assume that there are just two arms and let's define the loss of the first arm.",
                    "label": 0
                },
                {
                    "sent": "OK, so we draw these ID random Gaussians and then we sum them in a very particular way.",
                    "label": 0
                },
                {
                    "sent": "So the loss again, we're setting the loss of the first time, so an iteration one, the loss is going to be just side one.",
                    "label": 0
                },
                {
                    "sent": "The loss on the second iteration around 2:00 it's going to be just excited to on round three, it's going to be side 2 plus side three, so you add along these paths at round four, the losses going before it around 5:00 is gonna be excited for plus size 5, and so on, and you can see how this is kind of like a Gaussian random walk.",
                    "label": 0
                },
                {
                    "sent": "We're adding these Gaussians, but in this kind of multi multi scale way.",
                    "label": 0
                },
                {
                    "sent": "So that's the loss of the first arm OK stochastic construction for the first loss.",
                    "label": 0
                },
                {
                    "sent": "And in the second arm, the loss of that guy is just going to be the terministic function of the loss of the first time.",
                    "label": 1
                },
                {
                    "sent": "So it's either going to be epsilon more or epsilon less with equal probability.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we get?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our two loss sequences that are just epsilon part just this constant epsilon and the player doesn't know which one is better.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, so each round he's playing these rounds and have to choose red or blue on each, running only gets to see that one number, so he never gets to see the pair.",
                    "label": 0
                },
                {
                    "sent": "If you could see that there even for one round, he would know which arm is better and he just stick to that.",
                    "label": 0
                },
                {
                    "sent": "But he can see he just sees one of these values in the really epsilon close.",
                    "label": 0
                },
                {
                    "sent": "And we were able to prove two properties for this sequence.",
                    "label": 0
                },
                {
                    "sent": "So first of all, it's the easy one.",
                    "label": 0
                },
                {
                    "sent": "It's as long as he doesn't recognize which ARM is the better arm.",
                    "label": 0
                },
                {
                    "sent": "Then with probability close to half, he's going to play.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe the sub optimal arm and accumulate a regret of epsilon on each one of these iterations that he plays a suboptimal arm.",
                    "label": 0
                },
                {
                    "sent": "So that's the problem and then kind of the magical property of this of this process is that we were able to relate his ability to identify the better armed with the number of switches that he does, right?",
                    "label": 0
                },
                {
                    "sent": "So he has to switch back and forth between the arms to get some information about who's better.",
                    "label": 0
                },
                {
                    "sent": "And specifically has to switch, so if he hasn't switched one over epsilon squared times, he has no way to know which ARM is better.",
                    "label": 0
                },
                {
                    "sent": "So this is the important part.",
                    "label": 0
                },
                {
                    "sent": "And when you have these two properties then you know almost immediate corollary of these two things, which is what we showed in the previous paper.",
                    "label": 0
                },
                {
                    "sent": "Is that if we penalize him explicitly for switching costs.",
                    "label": 0
                },
                {
                    "sent": "So if we define this game called the Banner with switching costs every time he switches from aren't arm has to pay, then his minimax regret is going to jump to 2/3 because you have to switch a lot in order to take the better arm.",
                    "label": 1
                },
                {
                    "sent": "And if he doesn't take the better arm he's just going to accumulate all disregard all along the cover on the game.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now to this paper.",
                    "label": 0
                },
                {
                    "sent": "So we were able to show that, but that was kind of very very tailored to the switching cost setting and we asked ourselves, you know, is that problem unique?",
                    "label": 0
                },
                {
                    "sent": "And we find other online learning problems that have this property of teeth to the 2/3 regret and Moreover we are asked you know how, how much can we weaken this one memory adaptive adversary and make him almost almost oblivious and still be able to define this hard problem.",
                    "label": 0
                },
                {
                    "sent": "So this is what we came up with.",
                    "label": 0
                },
                {
                    "sent": "So this is called the composite loss function setting.",
                    "label": 0
                },
                {
                    "sent": "So in this setting there's going to be a special case of the one memory adaptive adversary.",
                    "label": 0
                },
                {
                    "sent": "So he chooses oblivious loss functions.",
                    "label": 1
                },
                {
                    "sent": "He assigns a predetermined loss to each arm on each one of the rounds, so he's oblivious, and that's it.",
                    "label": 0
                },
                {
                    "sent": "That's all he does, but then the loss itself is defined by this thing called the last combining function.",
                    "label": 0
                },
                {
                    "sent": "So there's a function G which takes 2 loss values and outputs them, combine them into one, and the loss of the player plays is just this combination of today's loss in yesterday's loss.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, take them in function of the Max function.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the setting?",
                    "label": 0
                },
                {
                    "sent": "Again, we have K arms.",
                    "label": 0
                },
                {
                    "sent": "We have T rounds.",
                    "label": 1
                },
                {
                    "sent": "Adversary defines a law secretly.",
                    "label": 0
                },
                {
                    "sent": "Underneath each one of the arms and I don't know what that is in the player starts to play, starts pulling arms.",
                    "label": 0
                },
                {
                    "sent": "I'm exposing these numbers and maybe even gets to see the numbers, so these are oblivious losses and he sees them in the bandit way.",
                    "label": 0
                },
                {
                    "sent": "The only thing is that the loss that he incurs is maybe the minimum of today's number and yesterday's number.",
                    "label": 0
                },
                {
                    "sent": "OK, the number I saw now and the number of side yesterday so it's almost oblivious.",
                    "label": 0
                },
                {
                    "sent": "I mean you know the adaptation and their dependence on history is only through this deterministic known constant simple function G, which is maybe mid or Max.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the result is that you know, surprisingly, that's enough to inflict this teacher.",
                    "label": 0
                },
                {
                    "sent": "The 2/3 regret.",
                    "label": 0
                },
                {
                    "sent": "OK, so he's almost oblivious, and that's what he did, and the technique.",
                    "label": 0
                },
                {
                    "sent": "I just want to say one word about that and you have to come to the poster to hear the details.",
                    "label": 0
                },
                {
                    "sent": "So I mean this is different than switching costs were not with them in function were not explicitly charging you for switching between arms.",
                    "label": 0
                },
                {
                    "sent": "Yet in the proof, we relied on the previous switching class result.",
                    "label": 0
                },
                {
                    "sent": "We were able to basically modify the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sequence by spiking it's we were able to add these spikes that have this property that they kind of implicitly penalize switches so they're hidden.",
                    "label": 0
                },
                {
                    "sent": "He doesn't even know when you're the player you're switching.",
                    "label": 1
                },
                {
                    "sent": "You don't even know that you're suffering this loss there.",
                    "label": 0
                },
                {
                    "sent": "Hidden under the noise level, yet they're big enough.",
                    "label": 0
                },
                {
                    "sent": "So you know, impose something which is effectively almost like a constant switching cost per each one of the switches, and that's how we reduced to the switching color setting.",
                    "label": 1
                },
                {
                    "sent": "That's it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}