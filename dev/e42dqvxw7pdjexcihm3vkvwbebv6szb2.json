{
    "id": "e42dqvxw7pdjexcihm3vkvwbebv6szb2",
    "title": "Online Learning with Feedback Graphs: Beyond Bandits",
    "info": {
        "author": [
            "Tomer Koren, Technion - Israel Institute of Technology"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_koren_beyond_bandits/",
    "segmentation": [
        [
            "OK, so this talk is about online learning with feedback graphs and it is a joint work with an ogre alone, Nicolasa Bianchi and offered Eckel.",
            "OK."
        ],
        [
            "It changes every day.",
            "The direction of the OK.",
            "So learning with great feedback.",
            "So I will not describe the precise online learning protocol and problem cause many of you are familiar with.",
            "I will just say the unique things about the particular setting, so it's a usual online game over a finite number of actions, K and with adversarial losses this this model was introduced quite recently by a Mono and Shamir.",
            "And the unique thing about this model is that the feedback in this game is determined by a directed graph G. So for example, in this directed graph, if this is the, this is the graph that describes the feedback model.",
            "So when the."
        ],
        [
            "They are choosers, action number one.",
            "He gets to observe full feedback, the losses of all other actions cause this action has outgoing edges to all other actions OK and when he chooses."
        ],
        [
            "Section #2 he gets to observe only his loss.",
            "Only the loss of action #2 be 'cause this is the only outgoing edge.",
            "This action has OK.",
            "So this setting includes a special case.",
            "The full feedback game, which corresponds to the full graph that with all possible edges it also includes the bandit feedback game, where each action has only self loop.",
            "The flare only observed his own loss.",
            "The loss of the action he chose."
        ],
        [
            "But also interpolated feedback models that interpolate between full feedback and bandit feedback.",
            "For example, this graph that I showed you before."
        ],
        [
            "OK, what is the previous work on this problem?",
            "So the previous work?",
            "These three papers that I mentioned here only deals treats graphs that contains all possible self loops, so it treats only graphs that includes bandit feedback but also side observations, possibly losses of other, some other actions."
        ],
        [
            "OK, so a natural question to ask is this is the question that we ask in this paper.",
            "Is what happens when some of the self loops are missing from the feedback graph this is was not addressed by any previous work and this also this actually allows us to extrapolate beyond multi arm bandits beyond bandits.",
            "Becausw we can omit self loops which for example when the player chooses this action it does not observe his own loss.",
            "So this is beyond.",
            "They're beyond bandits that appears in the title."
        ],
        [
            "OK, so our main result is a complete characterization of the minimax regret in this game, in terms of the horizon T of course, but also is in terms of the combinatorial properties of the federal graph.",
            "So in particular, these properties are the independence number of the graph, namely the size of the maximal independent set in this graph, and also a weak domination of the graph, which is which is the size of the minimal domination dominating set of some certain subset of the graph, which I will not describe formally here.",
            "And we have a very nice theorem in the paper which I will not show here.",
            "Instead, I will.",
            "I will not bore you with the technical details and definitions.",
            "I will show you several examples that illustrate this theorem.",
            "So for exam."
        ],
        [
            "Apple.",
            "It rains, yeah?",
            "Yeah.",
            "OK, so for the first example is the graph I showed you before, so this graph includes all self loops and it belongs to a first class of graph."
        ],
        [
            "Who's minimax regret scales with the rooty and also the root of the independence number of the graph, in which in this case is the number of actions K, so the minimum minimax regret in this case is root Katie."
        ],
        [
            "This way.",
            "Yep."
        ],
        [
            "Sway.",
            "So what happens when we remove one of the self loops in this graph?",
            "So in this case the minimax regret phase transition occurs and suddenly the minimax regret scales with T to the 2/3 and not rooty, but perhaps more surprisingly, it also squared scales with a different graph theoretic property of the graph, namely a certain domination number of the graph.",
            "Thanks.",
            "So both the rating T and the rate with respect to the graph changes.",
            "A phase transition in the phase transition.",
            "OK. Poop."
        ],
        [
            "And."
        ],
        [
            "This example is when the first action does not does not have a self loop no.",
            "So now this action has a cell phone, but this one doesn't.",
            "Does not have and this third class of graphs is actually quite simple cause this first this first action does not have any incoming edges and therefore it is not observable.",
            "Learner can never observe it's a loss and therefore this problem is not learnable and they regret is scales linearly with the OK.",
            "So any possible graph belongs to one of the of these.",
            "Classes that I showed you and but also we have this model captures some interesting problems that classic where more feedback models such as the Apple."
        ],
        [
            "Facing problem, we also capture this problem which surprisingly has rooty regret.",
            "This is known, but it is captured by our algorithm."
        ],
        [
            "I will end with one final example, which is kind of surprising to me.",
            "Is that the full feedback graph without without self loops, namely, it corresponds to a problem where the player receives.",
            "Pics, one of the actions say action number one.",
            "He observes the losses of all other actions, but not the actions that he actually chose.",
            "OK, so he does not observe full feedback.",
            "An surprise.",
            "Quite surprisingly, the minimax regret achievable in this case coincides with the rate achievable in the full feedback case.",
            "So even though that we have partial feedback model, we can still obtain the same regret as in the full feedback model.",
            "Up to constants so we know we don't lose anything OK?"
        ],
        [
            "So I don't have time to describe everything, so please come by our poster to hear all the other details and thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this talk is about online learning with feedback graphs and it is a joint work with an ogre alone, Nicolasa Bianchi and offered Eckel.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It changes every day.",
                    "label": 0
                },
                {
                    "sent": "The direction of the OK.",
                    "label": 0
                },
                {
                    "sent": "So learning with great feedback.",
                    "label": 0
                },
                {
                    "sent": "So I will not describe the precise online learning protocol and problem cause many of you are familiar with.",
                    "label": 0
                },
                {
                    "sent": "I will just say the unique things about the particular setting, so it's a usual online game over a finite number of actions, K and with adversarial losses this this model was introduced quite recently by a Mono and Shamir.",
                    "label": 0
                },
                {
                    "sent": "And the unique thing about this model is that the feedback in this game is determined by a directed graph G. So for example, in this directed graph, if this is the, this is the graph that describes the feedback model.",
                    "label": 1
                },
                {
                    "sent": "So when the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They are choosers, action number one.",
                    "label": 0
                },
                {
                    "sent": "He gets to observe full feedback, the losses of all other actions cause this action has outgoing edges to all other actions OK and when he chooses.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Section #2 he gets to observe only his loss.",
                    "label": 0
                },
                {
                    "sent": "Only the loss of action #2 be 'cause this is the only outgoing edge.",
                    "label": 0
                },
                {
                    "sent": "This action has OK.",
                    "label": 0
                },
                {
                    "sent": "So this setting includes a special case.",
                    "label": 0
                },
                {
                    "sent": "The full feedback game, which corresponds to the full graph that with all possible edges it also includes the bandit feedback game, where each action has only self loop.",
                    "label": 1
                },
                {
                    "sent": "The flare only observed his own loss.",
                    "label": 0
                },
                {
                    "sent": "The loss of the action he chose.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But also interpolated feedback models that interpolate between full feedback and bandit feedback.",
                    "label": 0
                },
                {
                    "sent": "For example, this graph that I showed you before.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what is the previous work on this problem?",
                    "label": 1
                },
                {
                    "sent": "So the previous work?",
                    "label": 0
                },
                {
                    "sent": "These three papers that I mentioned here only deals treats graphs that contains all possible self loops, so it treats only graphs that includes bandit feedback but also side observations, possibly losses of other, some other actions.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so a natural question to ask is this is the question that we ask in this paper.",
                    "label": 0
                },
                {
                    "sent": "Is what happens when some of the self loops are missing from the feedback graph this is was not addressed by any previous work and this also this actually allows us to extrapolate beyond multi arm bandits beyond bandits.",
                    "label": 1
                },
                {
                    "sent": "Becausw we can omit self loops which for example when the player chooses this action it does not observe his own loss.",
                    "label": 0
                },
                {
                    "sent": "So this is beyond.",
                    "label": 0
                },
                {
                    "sent": "They're beyond bandits that appears in the title.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so our main result is a complete characterization of the minimax regret in this game, in terms of the horizon T of course, but also is in terms of the combinatorial properties of the federal graph.",
                    "label": 1
                },
                {
                    "sent": "So in particular, these properties are the independence number of the graph, namely the size of the maximal independent set in this graph, and also a weak domination of the graph, which is which is the size of the minimal domination dominating set of some certain subset of the graph, which I will not describe formally here.",
                    "label": 0
                },
                {
                    "sent": "And we have a very nice theorem in the paper which I will not show here.",
                    "label": 0
                },
                {
                    "sent": "Instead, I will.",
                    "label": 0
                },
                {
                    "sent": "I will not bore you with the technical details and definitions.",
                    "label": 0
                },
                {
                    "sent": "I will show you several examples that illustrate this theorem.",
                    "label": 0
                },
                {
                    "sent": "So for exam.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apple.",
                    "label": 0
                },
                {
                    "sent": "It rains, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so for the first example is the graph I showed you before, so this graph includes all self loops and it belongs to a first class of graph.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who's minimax regret scales with the rooty and also the root of the independence number of the graph, in which in this case is the number of actions K, so the minimum minimax regret in this case is root Katie.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This way.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sway.",
                    "label": 0
                },
                {
                    "sent": "So what happens when we remove one of the self loops in this graph?",
                    "label": 0
                },
                {
                    "sent": "So in this case the minimax regret phase transition occurs and suddenly the minimax regret scales with T to the 2/3 and not rooty, but perhaps more surprisingly, it also squared scales with a different graph theoretic property of the graph, namely a certain domination number of the graph.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So both the rating T and the rate with respect to the graph changes.",
                    "label": 0
                },
                {
                    "sent": "A phase transition in the phase transition.",
                    "label": 0
                },
                {
                    "sent": "OK. Poop.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This example is when the first action does not does not have a self loop no.",
                    "label": 0
                },
                {
                    "sent": "So now this action has a cell phone, but this one doesn't.",
                    "label": 0
                },
                {
                    "sent": "Does not have and this third class of graphs is actually quite simple cause this first this first action does not have any incoming edges and therefore it is not observable.",
                    "label": 0
                },
                {
                    "sent": "Learner can never observe it's a loss and therefore this problem is not learnable and they regret is scales linearly with the OK.",
                    "label": 0
                },
                {
                    "sent": "So any possible graph belongs to one of the of these.",
                    "label": 0
                },
                {
                    "sent": "Classes that I showed you and but also we have this model captures some interesting problems that classic where more feedback models such as the Apple.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Facing problem, we also capture this problem which surprisingly has rooty regret.",
                    "label": 0
                },
                {
                    "sent": "This is known, but it is captured by our algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will end with one final example, which is kind of surprising to me.",
                    "label": 0
                },
                {
                    "sent": "Is that the full feedback graph without without self loops, namely, it corresponds to a problem where the player receives.",
                    "label": 0
                },
                {
                    "sent": "Pics, one of the actions say action number one.",
                    "label": 0
                },
                {
                    "sent": "He observes the losses of all other actions, but not the actions that he actually chose.",
                    "label": 0
                },
                {
                    "sent": "OK, so he does not observe full feedback.",
                    "label": 0
                },
                {
                    "sent": "An surprise.",
                    "label": 0
                },
                {
                    "sent": "Quite surprisingly, the minimax regret achievable in this case coincides with the rate achievable in the full feedback case.",
                    "label": 0
                },
                {
                    "sent": "So even though that we have partial feedback model, we can still obtain the same regret as in the full feedback model.",
                    "label": 0
                },
                {
                    "sent": "Up to constants so we know we don't lose anything OK?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I don't have time to describe everything, so please come by our poster to hear all the other details and thank you.",
                    "label": 0
                }
            ]
        }
    }
}