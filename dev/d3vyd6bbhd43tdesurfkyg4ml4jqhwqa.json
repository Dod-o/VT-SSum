{
    "id": "d3vyd6bbhd43tdesurfkyg4ml4jqhwqa",
    "title": "Fixed-Size Pegasos for Large Scale Pinball Loss SVM",
    "info": {
        "author": [
            "Vilen Jumutc, Department of Electrical Engineering, KU Leuven"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_jumutc_pegasos/",
    "segmentation": [
        [
            "My name is Vivian Youmans and I want to present one of our latest.",
            "Works fix size big asses for large scale pinball losses via OK we start."
        ],
        [
            "OK, with a small outline, but I also want to give a brief introduction about our work.",
            "So basically in this research we try to combine.",
            "Big asses where were acknowledged and known algorithm.",
            "For linear SVM's with some nonlinear techniques like fixed size approach and also with another loss functions other than hinge loss used in original Pegasus by Shadow Schwartz, and to show that both these approaches from both these approaches, we can benefit.",
            "So basically we start with a brief introduction to what is static.",
            "Programming or so basically.",
            "Basically."
        ],
        [
            "We can see here that stochastic programming has the following form, which is basically unconstrained.",
            "And we work with the unbounded closed convex sets and.",
            "As some random vector, those probabilities should be supported on some other sets so.",
            "This is the basic setting for stochastic programming.",
            "Next want to talk a little bit on Pegasus itself, so Pegasus is also."
        ],
        [
            "Widely known and has the.",
            "Also, benefits from strongly convex optimization objective and it doesn't have any linear constraints, so it's some constraint by nature."
        ],
        [
            "So which and it also can be put?",
            "So basically what Schwartz proposes that we can put it to stochastic setting, and by this we can reduce the computational costs, but as it was already mentioned before in previous talks.",
            "So basically every step is cheaper, but we might take much more steps initially, so we work with this instantaneous optimization objective here, here, and where eight is our current data set.",
            "Evolution step T. So basically it can be only a single data point, or some subsample from initials finite sample, which we're working on.",
            "And also in the original paper they work with this type of loss which is kinda slow."
        ],
        [
            "Everybody knows so.",
            "And within this setting we also work with the.",
            "This basically subgradient step.",
            "And it's it's sort of the simple and also we every step we are projecting back our solution or somebody calls convex set B with this.",
            "When the in theory expectation we're taking spectation over our iterates.",
            "So and little bit afterwards, we will present also some bounds related to this expectation.",
            "So.",
            "We start with our proposed basically our proposed approach with pinball loss.",
            "And the pinball losses are kind of a modification.",
            "Note the modification, let's say hinge loss is a modification of pinball loss and it has been applied successfully in variety of settings and for instance for quantile regression.",
            "And yeah, he just lost a special case of pinball loss with now equals equal to 0 and this is the expression for pinball loss.",
            "I will show some author figures."
        ],
        [
            "Letter to pinball loss, for instance.",
            "This ones.",
            "We can see different loss functions here with hinge loss and I'll 2 Norman sigmoid en El truncated.",
            "And here on the figures of the subfigure.",
            "See we can see.",
            "Cable loss with different Tower Tao parameter.",
            "For instance one and 0.5.",
            "So.",
            "On the next slide, we can see also."
        ],
        [
            "The figure which gives a kind of.",
            "Presentation of what happens, for instance, to hinge loss and to pinball loss.",
            "If in this figure we have the similar similar kind of a similar pictures for pinball lesson and hinge loss, but after small perturbation on the next slide we can see that."
        ],
        [
            "Our solid lines, which represent our hinge loss, are are changed but with respect to a little bit perturbated data, but which comes from the same distribution but pinball loss stays the same price, the same decision boundaries.",
            "So.",
            "Next"
        ],
        [
            "We present our algorithm, it's an outline.",
            "It's a wall algorithm, so we can see that I will explain it for awhile.",
            "We can see that what we introduce Additionally to big asses algorithm is.",
            "Additional additional, for instance set where which is specific to.",
            "Two to pinball loss and afterwards also the gradient steps are in.",
            "This step is modified a little bit with respect to this loss.",
            "An additional parameter Tau.",
            "And what I have to emphasize on this in this algorithm is that basically we as we can see, we output the vector W here, and we computed bias.",
            "But we do not optimize over this bias.",
            "So basically we work and work and find the solution over for these W vector and where would the final one.",
            "Here."
        ],
        [
            "So.",
            "Next next we present some convergence bonds related to our algorithm, so we can see that.",
            "If we assume that are we we have this solution to our initial problem problem.",
            "Which is the note here as W star and we let.",
            "Additional parameter C to be this one at this parameter also relates to this term Tau, an parameter Tau of pinball loss.",
            "Then we have this bound for our solution basically.",
            "We can see that.",
            "Our optimal solution is within.",
            "Basically this storm of the.",
            "Of the final final output, let's say WT.",
            "And there and there.",
            "Let's say the proof of this bound follows the also lemma and theorem one in our latest paper and lemma one in the paper of shallow Schwartz.",
            "Now I would like."
        ],
        [
            "To talk about a little bit about fixed size approach and why we introduced it in our.",
            "In our research in our paper, the fixed size approach helps us to go beyond the linear nature of gases and.",
            "And the the algorithm itself.",
            "So we use basically.",
            "First we select by some entropy criteria, some M prototype vectors or M data points, and then we construct M by M BASIC RBF kernel matrix K. And then we use also learn knowledge than Eastern approximation that was originally mentioned in the paper of Williams and Seeger.",
            "To approximate our feature map nonlinear feature map by by this expression here, where let's say Lambda here and here is their value and the eigenvector of our metrics K. So.",
            "Here we present."
        ],
        [
            "Our complete procedure.",
            "How we?",
            "How we be former?",
            "Finally, our classification task, for instance, first we we have this as training data and also labeling Y and our parameters and we output our model and also mapping file here.",
            "So first we find the active set using a Rooney entropy.",
            "Yeah, I will mention that I'd like to mention that it's rainy entropy and then we compute Mr Proximation to find our feature map and then we proceed to our initial Pegasus algorithm that was mentioned here.",
            "Yeah, here I'll give him one.",
            "So this is the basic and complete procedure that we do follow.",
            "But he."
        ],
        [
            "We would like to present some some evaluation of this approach with respect to 1st our toy data set with some amount of distortion introduced or switch between labels.",
            "So we see that hinge loss performs quite poor with respect to pinball loss, and we can see that for some amount of distortion, Towell parameter 0.1 is the best Stanford.",
            "Another Dow parameter one is also quite better than than for hinge loss here.",
            "And then we evaluate also some."
        ],
        [
            "See I data says yeah, we the size is quite growing here till the till quarter of million data points and we can see that everywhere.",
            "Also pinball loss and also with respect to.",
            "In the in the contest of fixed size approach and here hinge loss also was was evaluated in this manner.",
            "So basically with the with the feature map, press summated by fixed size approach.",
            "So everywhere pinball loss gives better better results than hinge loss."
        ],
        [
            "And here is their small figure about the convergent.",
            "We can see that for sure.",
            "I mean we can't compare.",
            "I mean fairly the objective values here, but we can compare how basically algorithm converges.",
            "We can see that it's quite in the Bing unstable.",
            "Yeah, but because we are projecting back and also.",
            "But we can also notice that somehow big asses with pinball, loss convergence, little bit earlier than then because with hinge loss.",
            "Um?",
            "So."
        ],
        [
            "Some conclusions about our work, so I guess algorithm is quite suitable for.",
            "Linear and fixed sizes from learning.",
            "And also from my my perspective is that big us also can be used quite easily in the online setting and by incorporating other loss functions so we can benefit from better generalization, errors, generalization and also convergence.",
            "So.",
            "Yeah there are some."
        ],
        [
            "References to other papers and publications."
        ],
        [
            "That's all, thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Vivian Youmans and I want to present one of our latest.",
                    "label": 0
                },
                {
                    "sent": "Works fix size big asses for large scale pinball losses via OK we start.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, with a small outline, but I also want to give a brief introduction about our work.",
                    "label": 0
                },
                {
                    "sent": "So basically in this research we try to combine.",
                    "label": 0
                },
                {
                    "sent": "Big asses where were acknowledged and known algorithm.",
                    "label": 0
                },
                {
                    "sent": "For linear SVM's with some nonlinear techniques like fixed size approach and also with another loss functions other than hinge loss used in original Pegasus by Shadow Schwartz, and to show that both these approaches from both these approaches, we can benefit.",
                    "label": 0
                },
                {
                    "sent": "So basically we start with a brief introduction to what is static.",
                    "label": 0
                },
                {
                    "sent": "Programming or so basically.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can see here that stochastic programming has the following form, which is basically unconstrained.",
                    "label": 1
                },
                {
                    "sent": "And we work with the unbounded closed convex sets and.",
                    "label": 1
                },
                {
                    "sent": "As some random vector, those probabilities should be supported on some other sets so.",
                    "label": 1
                },
                {
                    "sent": "This is the basic setting for stochastic programming.",
                    "label": 0
                },
                {
                    "sent": "Next want to talk a little bit on Pegasus itself, so Pegasus is also.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Widely known and has the.",
                    "label": 0
                },
                {
                    "sent": "Also, benefits from strongly convex optimization objective and it doesn't have any linear constraints, so it's some constraint by nature.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So which and it also can be put?",
                    "label": 0
                },
                {
                    "sent": "So basically what Schwartz proposes that we can put it to stochastic setting, and by this we can reduce the computational costs, but as it was already mentioned before in previous talks.",
                    "label": 0
                },
                {
                    "sent": "So basically every step is cheaper, but we might take much more steps initially, so we work with this instantaneous optimization objective here, here, and where eight is our current data set.",
                    "label": 1
                },
                {
                    "sent": "Evolution step T. So basically it can be only a single data point, or some subsample from initials finite sample, which we're working on.",
                    "label": 0
                },
                {
                    "sent": "And also in the original paper they work with this type of loss which is kinda slow.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Everybody knows so.",
                    "label": 0
                },
                {
                    "sent": "And within this setting we also work with the.",
                    "label": 0
                },
                {
                    "sent": "This basically subgradient step.",
                    "label": 0
                },
                {
                    "sent": "And it's it's sort of the simple and also we every step we are projecting back our solution or somebody calls convex set B with this.",
                    "label": 0
                },
                {
                    "sent": "When the in theory expectation we're taking spectation over our iterates.",
                    "label": 0
                },
                {
                    "sent": "So and little bit afterwards, we will present also some bounds related to this expectation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We start with our proposed basically our proposed approach with pinball loss.",
                    "label": 1
                },
                {
                    "sent": "And the pinball losses are kind of a modification.",
                    "label": 0
                },
                {
                    "sent": "Note the modification, let's say hinge loss is a modification of pinball loss and it has been applied successfully in variety of settings and for instance for quantile regression.",
                    "label": 1
                },
                {
                    "sent": "And yeah, he just lost a special case of pinball loss with now equals equal to 0 and this is the expression for pinball loss.",
                    "label": 0
                },
                {
                    "sent": "I will show some author figures.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Letter to pinball loss, for instance.",
                    "label": 1
                },
                {
                    "sent": "This ones.",
                    "label": 0
                },
                {
                    "sent": "We can see different loss functions here with hinge loss and I'll 2 Norman sigmoid en El truncated.",
                    "label": 1
                },
                {
                    "sent": "And here on the figures of the subfigure.",
                    "label": 1
                },
                {
                    "sent": "See we can see.",
                    "label": 0
                },
                {
                    "sent": "Cable loss with different Tower Tao parameter.",
                    "label": 0
                },
                {
                    "sent": "For instance one and 0.5.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "On the next slide, we can see also.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The figure which gives a kind of.",
                    "label": 0
                },
                {
                    "sent": "Presentation of what happens, for instance, to hinge loss and to pinball loss.",
                    "label": 0
                },
                {
                    "sent": "If in this figure we have the similar similar kind of a similar pictures for pinball lesson and hinge loss, but after small perturbation on the next slide we can see that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our solid lines, which represent our hinge loss, are are changed but with respect to a little bit perturbated data, but which comes from the same distribution but pinball loss stays the same price, the same decision boundaries.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Next",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We present our algorithm, it's an outline.",
                    "label": 0
                },
                {
                    "sent": "It's a wall algorithm, so we can see that I will explain it for awhile.",
                    "label": 0
                },
                {
                    "sent": "We can see that what we introduce Additionally to big asses algorithm is.",
                    "label": 0
                },
                {
                    "sent": "Additional additional, for instance set where which is specific to.",
                    "label": 0
                },
                {
                    "sent": "Two to pinball loss and afterwards also the gradient steps are in.",
                    "label": 0
                },
                {
                    "sent": "This step is modified a little bit with respect to this loss.",
                    "label": 0
                },
                {
                    "sent": "An additional parameter Tau.",
                    "label": 0
                },
                {
                    "sent": "And what I have to emphasize on this in this algorithm is that basically we as we can see, we output the vector W here, and we computed bias.",
                    "label": 0
                },
                {
                    "sent": "But we do not optimize over this bias.",
                    "label": 0
                },
                {
                    "sent": "So basically we work and work and find the solution over for these W vector and where would the final one.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Next next we present some convergence bonds related to our algorithm, so we can see that.",
                    "label": 0
                },
                {
                    "sent": "If we assume that are we we have this solution to our initial problem problem.",
                    "label": 0
                },
                {
                    "sent": "Which is the note here as W star and we let.",
                    "label": 0
                },
                {
                    "sent": "Additional parameter C to be this one at this parameter also relates to this term Tau, an parameter Tau of pinball loss.",
                    "label": 0
                },
                {
                    "sent": "Then we have this bound for our solution basically.",
                    "label": 0
                },
                {
                    "sent": "We can see that.",
                    "label": 0
                },
                {
                    "sent": "Our optimal solution is within.",
                    "label": 0
                },
                {
                    "sent": "Basically this storm of the.",
                    "label": 0
                },
                {
                    "sent": "Of the final final output, let's say WT.",
                    "label": 0
                },
                {
                    "sent": "And there and there.",
                    "label": 0
                },
                {
                    "sent": "Let's say the proof of this bound follows the also lemma and theorem one in our latest paper and lemma one in the paper of shallow Schwartz.",
                    "label": 0
                },
                {
                    "sent": "Now I would like.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To talk about a little bit about fixed size approach and why we introduced it in our.",
                    "label": 0
                },
                {
                    "sent": "In our research in our paper, the fixed size approach helps us to go beyond the linear nature of gases and.",
                    "label": 0
                },
                {
                    "sent": "And the the algorithm itself.",
                    "label": 0
                },
                {
                    "sent": "So we use basically.",
                    "label": 0
                },
                {
                    "sent": "First we select by some entropy criteria, some M prototype vectors or M data points, and then we construct M by M BASIC RBF kernel matrix K. And then we use also learn knowledge than Eastern approximation that was originally mentioned in the paper of Williams and Seeger.",
                    "label": 1
                },
                {
                    "sent": "To approximate our feature map nonlinear feature map by by this expression here, where let's say Lambda here and here is their value and the eigenvector of our metrics K. So.",
                    "label": 0
                },
                {
                    "sent": "Here we present.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our complete procedure.",
                    "label": 0
                },
                {
                    "sent": "How we?",
                    "label": 0
                },
                {
                    "sent": "How we be former?",
                    "label": 0
                },
                {
                    "sent": "Finally, our classification task, for instance, first we we have this as training data and also labeling Y and our parameters and we output our model and also mapping file here.",
                    "label": 0
                },
                {
                    "sent": "So first we find the active set using a Rooney entropy.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I will mention that I'd like to mention that it's rainy entropy and then we compute Mr Proximation to find our feature map and then we proceed to our initial Pegasus algorithm that was mentioned here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, here I'll give him one.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic and complete procedure that we do follow.",
                    "label": 0
                },
                {
                    "sent": "But he.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We would like to present some some evaluation of this approach with respect to 1st our toy data set with some amount of distortion introduced or switch between labels.",
                    "label": 0
                },
                {
                    "sent": "So we see that hinge loss performs quite poor with respect to pinball loss, and we can see that for some amount of distortion, Towell parameter 0.1 is the best Stanford.",
                    "label": 1
                },
                {
                    "sent": "Another Dow parameter one is also quite better than than for hinge loss here.",
                    "label": 0
                },
                {
                    "sent": "And then we evaluate also some.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See I data says yeah, we the size is quite growing here till the till quarter of million data points and we can see that everywhere.",
                    "label": 0
                },
                {
                    "sent": "Also pinball loss and also with respect to.",
                    "label": 0
                },
                {
                    "sent": "In the in the contest of fixed size approach and here hinge loss also was was evaluated in this manner.",
                    "label": 0
                },
                {
                    "sent": "So basically with the with the feature map, press summated by fixed size approach.",
                    "label": 0
                },
                {
                    "sent": "So everywhere pinball loss gives better better results than hinge loss.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is their small figure about the convergent.",
                    "label": 0
                },
                {
                    "sent": "We can see that for sure.",
                    "label": 0
                },
                {
                    "sent": "I mean we can't compare.",
                    "label": 0
                },
                {
                    "sent": "I mean fairly the objective values here, but we can compare how basically algorithm converges.",
                    "label": 0
                },
                {
                    "sent": "We can see that it's quite in the Bing unstable.",
                    "label": 1
                },
                {
                    "sent": "Yeah, but because we are projecting back and also.",
                    "label": 0
                },
                {
                    "sent": "But we can also notice that somehow big asses with pinball, loss convergence, little bit earlier than then because with hinge loss.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some conclusions about our work, so I guess algorithm is quite suitable for.",
                    "label": 0
                },
                {
                    "sent": "Linear and fixed sizes from learning.",
                    "label": 1
                },
                {
                    "sent": "And also from my my perspective is that big us also can be used quite easily in the online setting and by incorporating other loss functions so we can benefit from better generalization, errors, generalization and also convergence.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah there are some.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "References to other papers and publications.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's all, thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}