{
    "id": "xc7mt6aa66faiifdza36svianb6meypa",
    "title": "Why Bayesian nonparametrics?",
    "info": {
        "author": [
            "Zoubin Ghahramani, Department of Engineering, University of Cambridge"
        ],
        "published": "Jan. 24, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_ghahramani_nonparametrics/",
    "segmentation": [
        [
            "OK, so.",
            "I'm going to talk about Bayesian nonparametrics, obviously, and the title of my talk is why Bayesian Nonparametrics?",
            "Which I think is.",
            "Sort of closely related to the vision of this workshop, which is to figure out why the people who are doing it are doing it, and does it really make sense, and what are the advantages and shortcomings?",
            "So this is obviously a personal view and I'm a machine learning researcher and you know, I'm becoming more and more of a statistician, but I'm not a card carrying statistician and different people have different motivations for doing Bayesian nonparametrics.",
            "In preparing these slides, I was both trying to think at some points I was very excited about doing Bayesian nonparametrics.",
            "And so there's some very positive feelings in here about it.",
            "And then at other points I was, I was sort of trying to figure out all the downsides, and so there's ups and downs and mood in this talk.",
            "As you'll probably see, OK."
        ],
        [
            "So I'm going to break the question up into two questions.",
            "Why be Bayesian and why do nonparametrics?",
            "And I think the answer is sort of finally, well, we like to be Bayesian because it's a very simple framework, so simplicity goes in our favor as compared to other frameworks for doing learning and inference.",
            "On the other hand, we want to do nonparametrics because of complexity, because the real world is actually complicated in the data that we're trying to model.",
            "Come from complicated phenomena.",
            "So let me first focus on on the 1st part.",
            "Why be Bayesian and I apologize to people who you know, eat and sleep this stuff."
        ],
        [
            "Well, it's so damn simple.",
            "I mean, what other framework in machine learning can you write down with two rules?",
            "OK, even I can remember these rules, you know everything falls through from 2 simple rules of probability.",
            "You have the sum rule in the product rule and you know if you forget everything else, you could try to derive what you're doing from this.",
            "Some rule in the product rule.",
            "You know there's no rule that says.",
            "You have to come back, suffice something and optimize it, and then you know, add a regularizer and and dance around the Table 3 times and then try it again with a bit of cross validation you just write down a model.",
            "You apply this sum rule in the product rule as best you can.",
            "There's a footnote there.",
            "We'll talk about the footnote.",
            "This is the up part of the talk when I'm really, you know, a fundamentalist Bayesian, nonparametric and.",
            "And you know, it's just beautiful.",
            "Alright, so where can we use this summer over product rule?",
            "I wrote it as a sum rule in case there any computer scientists like myself in former computer scientists.",
            "I'll sit in the audience because computer scientists are scared of integrals but just look at the integrals in tournament to sums.",
            "There's some fancy math behind that you know all that.",
            "So we can apply this some room product rule to all sorts of interesting things, for example to doing inference about parameters Theta given some data D under some model M, Some rule product rule, we get Bayes rule coming out.",
            "We can give these things names like likelihood prior and posterior.",
            "And then we, you know somebody says, Oh well, what's the use of that posterior?",
            "I want predictions.",
            "I'm an engineer.",
            "I want predictions.",
            "OK, well predictions come from some rule product rule.",
            "There's no other way of doing it.",
            "OK, no arbitrariness so and it's very sensible.",
            "You look at the equation you say?",
            "Oh yeah, that makes sense.",
            "OK, each of your parameters make some prediction.",
            "Its prediction is a probability distribution.",
            "You don't know which is the right parameter.",
            "You're not going to do anything crazy like pick one of them.",
            "OK, so you.",
            "You average 'cause you don't know.",
            "You know that's another fundamental thing.",
            "It's not just two equations.",
            "In mathematics.",
            "There's something fundamental about Bayesians is.",
            "We don't know.",
            "We're uncertain all the time.",
            "I think that's why a few of us were wandering around Granada for half an hour trying to pick a restaurant because we just didn't know.",
            "You know what was the best thing to do.",
            "We didn't end up sampling from them, but you know, you know what this equation tells.",
            "Use you take your predictions and you average them with respect to your parameters and you average them with weights given by.",
            "You know you're familiar.",
            "Some room product rule weights from the data that you observed in the model and the assumptions you put in.",
            "OK, so that's one fundamental thing.",
            "You're always uncertain the only language for dealing with uncertainty is probability theory.",
            "As far as I'm concerned, it's a bit like you know, the language for dealing with rates of changes.",
            "Calculus.",
            "I don't think there are people now arguing I've got a better framework than calculus for rates of change or something like that.",
            "You know that that issue has been settled for a couple 100 years.",
            "Probability is the language for dealing with uncertainty.",
            "So we express all our forms of uncertainty in terms of probabilities and models are all about uncertainty.",
            "The only thing we're certain about is the data.",
            "So everything else we have to treat as uncertain quantities and we put probability distributions over them which represent our state of uncertainty before.",
            "And after observing the data.",
            "OK, and then we can do other nice things like model comparisons you all know.",
            "Again some rule product rule.",
            "So we get this expression here, which tells us that the probability of model given the data can be obtained in terms of something we call the marginal likelihood or model evidence, which again is obtained by averaging quantity.",
            "The likelihood over the parameters.",
            "OK. Everybody, anybody object to this.",
            "Leave now.",
            "No, you know you know there.",
            "There's somebody in the audience right now thinking the pry or where does it come from.",
            "OK, we can talk about that.",
            "I don't want to get hung up on that because this is not just about bayesianism.",
            "There's a lot of subtlety in here.",
            "Obviously that's where my down, the more down and depressed part of the talk, OK?",
            "Yes.",
            "You have all the models for model, is it?",
            "Yes, how do you know that you have all the models right?",
            "OK, so so this is this, by the way, is the marginal likelihood for a particular model M. Now if you want to do.",
            "Real Bayesian prediction.",
            "OK, I'm not being a real Bayesian here because I'm conditioning on a model I should really be averaging over all models that I would envision, and that's hard OK, because you know, my imagination may be limited and these averages might be expensive to do, but this is where the nonparametrics part comes in.",
            "Interesting Lee, I think rather than averaging over some simple models and then saying, oh maybe I missed out other simple models.",
            "The nonparametric framework which I'm about to talk about says.",
            "I don't know none of these simple models look good to me.",
            "Let me make a mega flexible, complicated model and then average over then.",
            "OK so."
        ],
        [
            "Now of course, not everybody does machine learning like this, yeah?",
            "And in fact, there seem to be 2 camps in machine learning there is.",
            "I would say a modeling camp versus a toolbox camp and I put in statistics in small font.",
            "I added that to this slide for the statisticians, 'cause I think what I'm going to say actually applies to statistics as well.",
            "You can interpret it as you want.",
            "I'm not claiming that you know anything about machine learning and statistics by font size here.",
            "I just wanted to add it to the slide and but not make a big deal out of it, which I have.",
            "Alright, so there are two camps.",
            "Machine Learning seeks to learn models of data.",
            "Alright, you define a space of possible models.",
            "That's a VIMS concern here.",
            "Well, let's start by defining it.",
            "Then you learn the parameters and structure of the models from the data and you make predictions and decisions.",
            "That seems fairly straightforward.",
            "A different view.",
            "Machine learning is machine learning is a toolbox of methods of processing data.",
            "You can feed the data into one of your possible methods in your toolbox.",
            "And you want to choose methods that have good theoretical or empirical performance so you know somebody proves the nice theorem which says that this method has, you know, is consistent and has even optimal convergence rates.",
            "You're like, wow, that sounds damn good.",
            "I'm going to stick my data in that method.",
            "OK, now you know or Alternatively somebody did a benchmark and compare 25 methods.",
            "You pick the best one and you try your data on that method.",
            "Then of course at the end you make predictions and decisions.",
            "So.",
            "You know?",
            "Obviously a lot of what I'm going to talk about is in this modeling framework here, but there are parts of it that make me uncomfortable which I'll get to in the more depressed part of my talk is sort of nice.",
            "This is very nice if I'm modeling a particular problem and I want to really solve it, and I try to put some understanding into that, etc.",
            "OK."
        ],
        [
            "So what about the second part of this question?",
            "Why basing on Paris?",
            "So let's just talk about parametric versus nonparametric models, so we're all on the same page.",
            "Parametric models assume some finite set of parameters stayed up.",
            "Given the parameters predictions, X are independent of the observed data in the sense you know that's captured by this equation here.",
            "Um?",
            "You know, if you're if you're frequentist, this is fine.",
            "You just don't put a conditioning bar here, but essentially the idea is that you you capture everything there is to know about the data through your parameters Theta.",
            "This is what Peter Orbans was describing in his in his part of the tutorial, where he's talking about the pattern and the noise.",
            "Theta captures the pattern.",
            "But now if you have a parametric model, the finite set of parameters, here's the man himself.",
            "Obviously, after a good breakfast, unlike mine, there was rush.",
            "So the complexity of the model is bounded even if the amount of data is unbounded, so there is a if you like information theory.",
            "And of course I do as well.",
            "The parameters are an information channel between your training data and your future predictions, and that Channel has a bottleneck.",
            "There's like traffic jams through that Channel because it's a finite width.",
            "Now.",
            "Nonparametric models assume that the data distribution cannot be defined in terms of such a finite set of parameters.",
            "But they can often be defined by assuming some infinite dimensional parameter Theta.",
            "So you add more lanes to your information highway so that you don't get traffic jams.",
            "Now we can think of our parameter Theta as a function.",
            "And the nice thing about that is the amount of information that data can capture about the data.",
            "D can grow as the amount of data grows, so again data has bits.",
            "If it's just some finite precision and you want to capture some bits to make predictions, and this makes them more flexible.",
            "So that's part of the point of doing nonparametric modeling.",
            "OK."
        ],
        [
            "OK, now why nonparametrics?",
            "I think actually this transcends the Bayesian thing.",
            "Actually, if you look around in machine learning, a lot of successful methods are essentially nonparametric.",
            "Like the whole kernel revolution with SVM's and things like that and the related Gaussian process.",
            "These are nonparametric models and I would argue that's one of the reasons why they were so successful.",
            "Of course, kernel methods replaced neural networks is a big paradigm of machine learning, and then people came back afterwards and said, oh we'll make bigger neural networks or bigger and deeper neural networks.",
            "These are not nonparametric in go formal mathematical sense, but there heckuva lot parametric.",
            "OK, you know?",
            "You know one of Geoff Hinton or young Lacunes deep networks will have a million parameters, which is like more than my nonparametric model ever expresses on the datasets that I have on my computer.",
            "So essentially nobody I haven't heard anybody say, oh, I have a deep network with six units.",
            "Let me show you what it does.",
            "Nobody does a deep network.",
            "6 units.",
            "So these are also successful because they are very flexible.",
            "Even methods we like to poo poo like K. Nearest neighbors are pretty damn good and they were pretty flexible and their sort of nonparametric.",
            "But there are very nonparametric.",
            "So I put a footnote here which is successful methods in machine learning are essentially nonparametric.",
            "And then if you can read down there.",
            "Or highly scalable?",
            "OK, so there also, you know, you know, I don't know how to measure success in machine learning.",
            "I just look at, you know things that people are excited about, and they're often excited about these very fancy.",
            "Flexible methods, or they're excited if they can run logistic regression on 100 billion data points in 20,000 million dimensions?",
            "Because you know some methods you can make very very scalable.",
            "And if you're Google or Microsoft or Yahoo.",
            "You care about that a lot.",
            "And those are successful as well.",
            "I don't want to put that.",
            "In fact, if I was working in industry, God forbid I would probably be, you know, less interested in these cool mathematical flexibel things and wanting to figure out how to do logistic regression on the cloud of 10,000 computers.",
            "OK, so nonparametrics as a successful thing isn't restricted to Bayesian methods.",
            "But I already argued Bayesian methods are very nice, right?",
            "So we'll put the thing the two together."
        ],
        [
            "So another way to approach this is to start out with like A wish list.",
            "OK, I want to do machine learning or statistics.",
            "What are the desirable properties of my methods?",
            "Let me write down a list.",
            "This was my list from late last night, so I apologize if I omitted any of your favorable favorite desirable properties.",
            "So here here I go.",
            "Good predictive performance OK?",
            "Flexible that sort of goes hand in hand with good predictive performance, robust overfitting.",
            "Hence you know the all this industry of figuring out regularizers and things like that.",
            "Model based I think is a sort of.",
            "You know, if you're doing, there are some advantages to this.",
            "It's not absolutely necessary.",
            "If you have good predictive performance and you're flexible and robust.",
            "It doesn't have to be that whatever you're doing corresponds to a particular model, but you know if you have a model based approach, you can make it.",
            "You can make it modular and fit with other things easily.",
            "Automatic.",
            "Automatic is nice because people are expensive and computers are cheap and you want your computer to do the thinking for you.",
            "That's the sort of original AI idea.",
            "The originally idea wasn't to have some schmuck sitting there tuning parameters of you know, machine learning algorithm.",
            "You want it to be automatic.",
            "Scalable.",
            "Interpretable a lot of people are interested in this.",
            "And reproducible.",
            "OK, so this is a limited list.",
            "And I would argue that.",
            "Generally, Bayesian nonparametric methods.",
            "Satisfied the the five that I have in green.",
            "OK so in general because they are quite flexible.",
            "They are going to have good predictive performance.",
            "Of course you can really mess it up by giving these things.",
            "Ridiculous priors, or using the completely inappropriate model for your data at hand?",
            "There's no guarantee it's not idiot proof, right?",
            "But you know, generally, you'll have good predictive performance if you're using them in a reasonable setting there flexibel overfitting, what's that?",
            "Your Bayesian sum and product rule have no fitting in them.",
            "OK, you're just doing averaging, so we have problems.",
            "I'm not claiming Bayesians don't have problems, but overfitting is not one of them.",
            "'cause we don't fit.",
            "If you don't fit, you don't overfit.",
            "It's like over skiing.",
            "If you don't ski, don't over ski, OK.",
            "They're clearly model based.",
            "I could actually generate data from them.",
            "Now that's interesting.",
            "I would say, well, what's a model if you know, would anybody like to define a model?",
            "I've got a definition of model is.",
            "A representation of possible data you could observe.",
            "OK, if that's not a model.",
            "Then what is?",
            "If it can't predict data, then you can't falsify it.",
            "So models are essentially once you add the probability magic dust to them, their probabilistic distributions are generative models.",
            "And the nice thing about basic nonparametric methods is that their automatic innocence.",
            "They learn the structure automatically from the data.",
            "Now, unfortunately.",
            "We work hard to make them scalable, but you know there are.",
            "You know they're not all that scalable yet, but there are lots of smart people trying to make them more scalable.",
            "We could go either way on interpretable.",
            "You know, if you have infinitely many parameters, is.",
            "You're not going to be able to explain it like a decision tree to your client, or you know like an inductive logic program or something like that.",
            "And reproducible, I put that in there because I think a lot of people are troubled by, you know, what do you mean you ran the method again and you got a different result?",
            "You know, obviously, if we're using something like MCMC in our inference.",
            "The less we save the random seed.",
            "We're not going to be reproducible when we do inference, so a lot of people are troubled by that.",
            "You know, the airplane flew fine 957 times in the 950 eighth time it went and crashed.",
            "OK, the engineers don't really like that.",
            "OK, so these are some desirable properties of machine learning methods and and you know, I think we fairly well hit these and not so well those.",
            "Any comments on this?",
            "Yeah.",
            "Concerning flexibility, there is the fact that you have few branches are hyperparameters to to be too and this.",
            "If this were you, fix it.",
            "No, it is true that most nonparametric models have only a few hyperparameters at the top, and I think that that's nice because that makes them easier to make automatic.",
            "You could learn those hyperparameters as well from the data, so you don't have to tune those by hand.",
            "By flexibility, I mean that under those hyperparameters, generally at the level below that you have infinitely many parameters, and that makes them very flexible.",
            "Their distributions on measures or functions, things like that.",
            "Those are more flexible than you know.",
            "Distributions on Gaussian measures K distributions on.",
            "Oh or not, we can have additional measures, but you know there are more flexible generally than the parametric cousins that they come from.",
            "OK, we'll see some examples of that.",
            "I see there are no other questions.",
            "Alright, yeah, so I'm not so convinced by the automatic because a common experience but Oh well.",
            "I have to clamp my hyper these values and then burn in and then release them and then add this bit of the model next and then I have to do variational On this date and there's a lot of choices.",
            "I think you've got to be pretty expert to make this work.",
            "Yeah, it's a good thing you are.",
            "By automatic, what I mean is.",
            "In a sense that you can learn the structure from data in an automatic way, rather than saying I want 7 hidden units in my first layer in 300 in my second layer and so on.",
            "That's what I mean by automatic.",
            "I don't mean that the inference is automatic.",
            "If it was totally automatic, I wouldn't need grad students.",
            "I'm talking about the fundamental properties of the modeling framework rather than the algorithm.",
            "The lot of the painful stuff happens when you take the beautiful idea that comes from some product rule and then you turn it into a working system.",
            "There are a lot of non automatic steps there, I agree.",
            "OK, yes.",
            "Scalable property.",
            "Statistics background OK. By scalable I mean some.",
            "If you go to some talks in machine learning, they're trying to run their methods on.",
            "You know, hundreds of millions of data points, like you know, every time somebody clicks on an ad on, you know Microsoft's Bing.",
            "You know it goes into a probabilistic model somewhere, then does inference and they're not using basically nonparametric methods for that, because they want something really fast that they can.",
            "Paralyzed and running an online streaming manner and it's just harder to do that with Bayesian non parametric methods.",
            "I'll talk about this in a minute actually, but remind me if I don't.",
            "OK."
        ],
        [
            "So now as just as a reminder.",
            "There are two.",
            "Nonparametric families that have occupied a lot of interest in in the machine learning and statistics community.",
            "And I'm going to just basically review a little bit of that.",
            "And then talk a little bit about some of the interesting things that one can do within the nonparametric framework and then come back to some of the ups and downs of the framework as well.",
            "Towards the end.",
            "OK, so here are some nice building blocks for a lot of Bayesian nonparametrics.",
            "One of 'em is the Gaussian process.",
            "And this defines the distribution over functions.",
            "So this is a sample from.",
            "A Gaussian process.",
            "It's a draw from a function, and Gaussian process is really most intuitively we can think of it as an infinite dimensional Gaussian distribution.",
            "So just like the multivariate Gaussian, it has a mean and covariance.",
            "But now it's a mean function and a covariance function that takes 2 arguments.",
            "So in a lot of models we have unknown functions and we can plug in Gaussian processes in those places and then we can worry about whether that makes sense or not as a distribution over functions.",
            "The other process that's been studied a lot in Beijing Nonparametrics is Additionally process.",
            "This defines a distribution on distributions or measure or measures, so think of it as a function that's non negative and integrates to one.",
            "And again, just just like the Gaussian process is the infinite dimensional analogue of a finite dimensional thing, which is the Dirichlet distribution.",
            "So we say G, which is a random probability measure, is drawn from.",
            "Additionally process with base measure or among you and me, the mean G0 and then concentration parameter Alpha which controls the.",
            "Sort of dispersion around that mean in a sense.",
            "OK, and so these are probability distributions over infinite dimensional quantities.",
            "In this case, functions in this case distributions.",
            "No."
        ],
        [
            "So I've with permission.",
            "I borrowed a few slides from Peter in UI.",
            "And this is a nice slide to motivate why people are interested in things like Gaussian processes and Ashley processes, which is this notion of coverage of priors?",
            "So imagine you have a space of possible objects that you want to put probability distributions over.",
            "So for example, this could be.",
            "The space of all measures.",
            "Well, now you know it's drawn as a triangle because you know you can think of it as measures on you know three probability solutions over.",
            "Three possible events.",
            "OK, but think of it as more an infinite dimensional triangle.",
            "If you like, OK, so measures are measures or measures on functions.",
            "This is a really big space.",
            "OK, this thing in Gray.",
            "And usually when we define a class of models in the classical parametric world, we restrict ourselves to some subspace of this.",
            "OK, so for example, this could be the triangle here could be all functions.",
            "And this could be all polynomials up to order 2.",
            "Alright.",
            "And of course.",
            "Polynomials of order two don't have good coverage in the space of functions.",
            "Alright, because.",
            "You know they're good at modeling polynomials of order two, but for any function that I pick in some suitable class of functions.",
            "Suitably large class of functions.",
            "It's not the case that there is a polynomial of order two that's arbitrarily close to it.",
            "OK, I'm not spreading probability mass over this whole space in a nice way.",
            "Of course I can't.",
            "It turns out for reasons that are a little beyond my mathematical abilities that you know I can't put a distribution on the space of all functions.",
            "It's just way too big a space.",
            "But you can put distributions on.",
            "Very, very large subspaces of those things like, you know, near all continuous functions, let's say.",
            "So what we want is we want our prior to have good coverage in the sense that means that we want our models to put to sprinkle probability mass near everywhere on this triangle.",
            "They won't cover the whole triangle, but there will be kind of probability dust all over the place.",
            "So when the truth is somewhere, there's some probability mass near that.",
            "OK, in this case, if your true generating process is this you you're very far from it, even asymptotically.",
            "OK, any questions about that?",
            "That was a bit hand WAVY, I apologize, but I think it's a good intuition and this is one of the motivations why Gaussian processes and earthly processes have been so widely studied 'cause they have good coverage properties.",
            "OK."
        ],
        [
            "Now.",
            "Another reason why we're interested in Bayesian nonparametrics.",
            "Is that it's not just like one model for one kind of problem, but it's actually an incredibly flexible framework for.",
            "For modeling all sorts of things.",
            "So again, this is a slide borrowed from Peter and UIC Tauriel.",
            "Where?",
            "They've listed.",
            "All sorts of problems or applications that people solve in machine learning and many related fields and including statistics.",
            "Where there there is at the heart of this problem, there is some fundamental object of interest.",
            "And so you try to put distributions on those objects of interest and those objects of interest are fundamentally.",
            "High dimensional or nonparametric?",
            "OK, for example in classification and regression there is an unknown function that we're trying to learn and so we can use something like Gaussian process to put a distribution on that unknown function.",
            "Of course that's not the.",
            "It's not like a.",
            "It's not like a mathematical necessity that every time we have an unknown function we place a Gaussian process.",
            "In fact, it's easy to show that this is a pretty limited.",
            "You know way of defining distributions on functions limited in a funny way in that you know yes, with the Gaussian process you could probably get very close to any function if you have enough data.",
            "But it's a.",
            "It's not.",
            "It's not the case that is the most natural thing for modeling functions which have certain properties, like for example, if you had a monotonically increasing function or a convex function, or a piecewise linear function, or something like that, then this would be not a very natural thing to do, even if it has good coverage properties, you'll get there eventually, but there might be more natural distribution that you could use here and in all of these cases there are a huge number of different.",
            "Bayesian nonparametric models that can be applied to almost any topic in machine learning.",
            "So, so that's good.",
            "It is sense it's a framework rather than a small set of models.",
            "It's not just all about the Chinese restaurant process.",
            "OK, that's why people in machine learning get first exposed to, I think.",
            "And they get obsessed with things like why does the NTH person have to sit at the table in proportion to the previous forget about that stuff?",
            "I mean, yes, that's a particular detail of something like the Chinese restaurant process, but there is a huge and very rich world of models out there that we can use in all sorts of problems.",
            "OK."
        ],
        [
            "So.",
            "So I want to talk a little bit, maybe about some of the work that that.",
            "That I've been thinking about in terms of applying Bayesian nonparametrics to other kinds of structured objects other than functions and distributions.",
            "I'll try to go through this quickly because I want to spend some time at the end on some of the discussion topics.",
            "OK, so here are some of the things that I think are interesting, so let's put distributions on other structured objects.",
            "We're going to talk about sparse matrices, deep, sparse graphical models, covariances, and some recent work on network structured regression.",
            "Just as examples of.",
            "Fun things we can do by composing different kinds of models."
        ],
        [
            "So sparse matrices.",
            "Probably a lot of you know about this, but I'll go through very quickly, so I think sparse matrices are very fundamental thing that we can use in lots of different models.",
            "I didn't realize that until we did some work with sparse matrices and then it turned out everywhere I looked like, oh, I can throw a sparse matrix into there and it makes it more natural or more interesting."
        ],
        [
            "So here is how we can generate.",
            "Infinite sparse matrices, but particular kind.",
            "This is not the only kind of sparse matrix, obviously.",
            "So here's the interpretation.",
            "We're going to use sparse matrices in a particular context, but we can use them in other other ways as well.",
            "So imagine we have objects.",
            "The rows correspond to objects.",
            "And the columns correspond to latent features that an object may possess, so we're trying to do some latent variable learning, and we're going to think of binary latent variables that an object may possess.",
            "But instead of limiting ourselves to, you know seven latent variables.",
            "We're going to do something, which is take a distribution on finite binary matrices and see what happens when we take the limit as the number of columns K goes to Infinity.",
            "So here is really the simplest way I could imagine doing this.",
            "ZN K = 1 means object An has hidden feature K like for example, you know.",
            "ITunes user 3.",
            "Likes country music or something like that.",
            "OK, but you don't directly observed that.",
            "They like country music that you just observe that they have some music files or something.",
            "Now, if XN K = 1 means that object then has feature K. The way we're going to model this is going to say like.",
            "Let's say that's a Bernoulli random variable with parameter Theta, K. The parameters they decay correspond to.",
            "The sort of frequency with which objects have particular features.",
            "There's one parameter per column, so some features might be, you know, more frequent, like liking country music or something like that.",
            "I guess some features might be less frequent, like you know liking Moldovian folk songs or something right now.",
            "The parameter Theta K in each column we can draw from a conjugate prior to this Bernoulli.",
            "In this case, we're going to make it a beta prior with parameter Alpha over Big K. The number of columns and 2nd parameter one.",
            "This is really I thought.",
            "I think maybe the simplest way of doing this, because when you take the limit as big K goes to Infinity, what happens to this matrix is that obviously gets wider and wider, but it gets sparser and sparser as well.",
            "So if Z is an N by K matrix, then the expected number of nonzero entries will be N times Alpha.",
            "On average, each row will have Alpha entries.",
            "And this limiting procedure gives you the Indian buffet process which is a distribution on infinite sparse binary matrices.",
            "OK, so."
        ],
        [
            "So it's nice because you can actually write down what this distribution is, or in particular you can write down the probability of a particular class of matrices once you reorder the columns so that you shove all the zeros to one side.",
            "You can write this down explicitly, and it's an exchangeable distribution, and it has a bunch of interesting properties and then.",
            "And then once you start looking at it closely, which a lot of people, including us, did you see that it's related to lots of other things, like just like the Chinese restaurant process?",
            "Or rather the Dirichlet process, we can write down a stick breaking representation.",
            "This is definitely mixing distribution in the beta process and we can generate more flexible two and three parameter versions.",
            "Winky"
        ],
        [
            "Now, of course, in a lot of applications we don't want just to limit ourselves to binary variables.",
            "We want non binary sparse latent features, so we can use this matrix Z to model the sparsity structure of non binary variables, for example by taking an elementwise product between Z and a matrix of any other random variable.",
            "OK, so Zed captures the sparsity structure and then in the other random variables we can have the context content of the non sparse elements."
        ],
        [
            "So like for Chinese restaurant processes and earthly process, there's been a lot of work on inference in these models, including a variety of samplers and some deterministic algorithms, so this sort of difficult computational bits sometimes take years to workout well.",
            "I'm not convinced this completely worked out well yet, but I think it's much more scalable than when when the model was.",
            "Originally proposed.",
            "So what do we do with?"
        ],
        [
            "These things um.",
            "Well.",
            "We use them as components of larger models, so this is all within this modeling framework.",
            "Our models of probability distribution over data.",
            "We have different kinds of components.",
            "One of our components might be one of these sparse binary matrices.",
            "Then we can combine that matrix zed with different kinds of likelihood functions that relate the hidden matrix to the data.",
            "And once we do that, we find that there are lots of neat places where you can put that in.",
            "For example, as models of graph structures.",
            "Or protein complexes or collaborative filtering settings, or overlapping clustering.",
            "And these are things that look a little more like standard machine learning right?",
            "Originally that thing just looked like some funny distribution on some funny object, and now we're trying to do actual machine learning with it.",
            "So here."
        ],
        [
            "Just a couple of examples of that.",
            "Here is a nonparametric binary matrix factorization.",
            "This is work with Meads and Ruyssen Neil where we have a matrix of data which could be something like jeans versus patients or users versus movies and we want to represent we want we don't want to do like their models out there that do Co clustering so they cluster jeans and a cluster movies.",
            "Well that's a weird one.",
            "Now they cluster jeans jeans and they cluster patients.",
            "OK, but clustering you know clustering isn't flexible enough.",
            "I'm not happy with clustering.",
            "Clustering says if if I'm a gene I can only be in one and only one cluster.",
            "That seems a bit limited to me or I'm a user.",
            "You have to either GroupMe in with the people who like country music or with the people who like folk dancing, music or something.",
            "And I don't really like that.",
            "I don't like either of those actually.",
            "So we want more flexible models.",
            "We want models in which objects can belong to multiple clusters at the same time.",
            "And this allows you to do that by have by having a binary vector representation of the hidden features for each object in each row in each column.",
            "OK."
        ],
        [
            "So I'll give you another example very quickly, and then I'll do a reality check.",
            "So the other example is learning the structure of deep sparse graphical models so we can use an IDP matrix to relate a bunch of observed variables to layer of hidden units.",
            "We're going to do deep learning here.",
            "OK, we're going to deep learning, but in a slightly subversive way.",
            "By using Bayesian nonparametrics.",
            "So let's build up a deep network by having a layer of observe units at the bottom and infinitely wide.",
            "It's not just deep, it's wide as well, an infinitely wide hidden unit layer here, and this binary matrix represents which hidden units are connected to which observed units.",
            "Everybody happy with that.",
            "Observed Unit 1 is connected to the first 2 hidden units.",
            "OK, etc.",
            "So we can stack this."
        ],
        [
            "Yes.",
            "Bye."
        ],
        [
            "Trying multiple cascading."
        ],
        [
            "Draws from the Indian buffet process.",
            "And so in this work with Ryan Adams and Hannah Lala.",
            "In the end, the ICP is just used as a building block to define infinitely deep, infinitely wide networks.",
            "A priore, of course, once you observe data, you sample from the distribution of structures given the data.",
            "So."
        ],
        [
            "So.",
            "You can apply this sort of thing to the kinds of image data that deep belief networks have been applied to here.",
            "For example, it's faces where you can then say OK, let's predict the bottom half of the face from the top half and you get sort of fuzzy chins and things like that.",
            "And you can look at the the hidden units and see what they end up connecting to that sparse connectivity pattern from the hidden units of the observed units is learned automatically by the model.",
            "OK, this is what I mean by automatic.",
            "The number of hidden layers.",
            "Seem to be around 3 for these data.",
            "The number of units per hidden layer seems to be around 70 and the sparse connectivity of hidden units to the observer units.",
            "All of those things are learned automatically simply by taking our prior.",
            "And then conditioning on the data.",
            "OK, so."
        ],
        [
            "Then you can look at the higher level units and see fantasies of the model which look like these.",
            "Specially faces.",
            "And see what happens when you look at activations of higher level units.",
            "You see these scary looking faces lurking deep inside this nonparametric Bayesian deep wide network.",
            "OK now.",
            "I think I'm."
        ],
        [
            "Hey.",
            "Ready for the reality check?",
            "OK, here's a reality check.",
            "I just told you a model that does users buy movies.",
            "That sounds like a fantastic collaborative filtering model.",
            "So did I make $1,000,000 out of this or that wouldn't be me anyway?",
            "Did the people that worked on it make $1,000,000 out of it?",
            "Did we win the Netflix Prize?",
            "No, we didn't win the Netflix Prize.",
            "Are high are infinitely wide, infinitely deep?",
            "Nonparametric Bayesian network.",
            "Did it beat the best emnace test results compared to other deep networks?",
            "No.",
            "OK, is that fair?",
            "Ryan, that's fair, right?",
            "Whynott right?",
            "I think as one of the workshop organizers, you better answer this before the end of the workshop.",
            "This is the reality check.",
            "This stuff is is beautiful, it's elegant.",
            "It all follows from the sum and product rule and a bit of clever MCMC, OK?",
            "But hasn't made a humongous splash.",
            "Not not yet no no OK. That's sad.",
            "But it's still beautiful, so maybe we can take comfort in it.",
            "Well, you know it could be something to do with the kinds of projects I get involved in.",
            "I have had a look around and it seems that you know none of none of my Bayesian nonparametric things have beat anything.",
            "Maybe I should worry about that.",
            "But"
        ],
        [
            "But there are success stories, apparently not mine, unfortunately.",
            "Well, :) 'cause I can talk about them and.",
            "I'll just give you a few examples, so it's not all hopeless.",
            "It's like you know you can have beautiful models that have very elegant properties that perform quite loudly, but occasionally they are beautiful models that are elegant and they perform state of the art, and that's what gives.",
            "That's what gives me hope.",
            "So here's a few."
        ],
        [
            "Examples, so again borrowing from some of the examples in the tutorial.",
            "Here is work on segmentation of motion capture data by Emily Fox and colleagues.",
            "Where you can get very fairly state of the art segmentations of pretty complicated rich data from Bayesian nonparametric models."
        ],
        [
            "There is some work on word segmentation, again using Bayesian nonparametric models and.",
            "You know the the line in bold in these papers is the Bayesian nonparametric model is compared to competitors?",
            "So that gives you some comfort."
        ],
        [
            "There is excellent work in language modeling with the hierarchical Pitman, your language model, which can give excellent results compared to other competing methods.",
            "And related work in compression with the sequence memoizer that is beating state of the art compression algorithms.",
            "So there are success stories.",
            "I guess I'm just not involved in any of that work.",
            "OK."
        ],
        [
            "So here is another one.",
            "From nation Goodman, an hold in which compares support vector machines with a sparse form of Gaussian process classifier.",
            "And the interesting thing here is that on a whole bunch of different kind of standard issue CIS datasets you get error rates for the SVM and Gaussian process and then a sparse version of the Gaussian process, which is actually an approximation to the full Gaussian process by the way.",
            "But it's a lot faster and it's a hell of a lot sparser.",
            "OK, so if you look if you compare these columns here, if you look at the number of support vectors which is support vector machines are supposed to be sparse.",
            "Here's the number of support vectors for these different problems here, as they sort of roughly comparable quantity, which is the number of.",
            "Basis points for the sparse Gaussian process.",
            "So.",
            "You know, sometimes you get.",
            "Very comparable performance with two rather than, you know, two rather than 122 or something like that.",
            "And often this one does actually better than the SVM and this is using exactly the same kernel.",
            "By the way, you could do probably better if you did a lot of kernel learning in the Gaussian process, which is certainly very, very doable, well understood.",
            "So if you want to hear me rant more about SPMS and Gaussian processes, I'm actually going to speak more about that in the preference Learning Workshop.",
            "Make some more enemies there.",
            "OK, and in fact I think this stuff is promising, but we're actually pursuing a much more systematic, large scale comparison because people are so damn interested in classification.",
            "So with Alex Davies, my student were planning to kind of really flesh out this comparison a little more.",
            "OK, so."
        ],
        [
            "I wanted to talk briefly about covariance matrices.",
            "Modeling covariance matrix is a different kind of structure that we can model here.",
            "I think we do have.",
            "Actually this is maybe there is a trend.",
            "There is a bit of recent work where I think we have competitive results with set of the art methods."
        ],
        [
            "So why, why would you be interested in modeling covariance matrices?",
            "Well, the problem of modeling covariance matrices as they change as a function of time or other input variables.",
            "Is actually interesting to a lot of people, and they tend to be people in econometrics where they're looking at relationships between a bunch of different variables that change overtime.",
            "And the models that are commonly used are things like multivariate garch and multivarious lukasik volatility models.",
            "But it turns out they're not all that flexible and actually they don't scale very well.",
            "So."
        ],
        [
            "So with Andrew Wilson, we've been doing some work on generalized.",
            "We share process for covariance modeling, where we're modeling time and spatially varying covariance matrices, and the only difficulty in doing that is that these covariance matrices have to be symmetric and positive definite.",
            "As you vary the covariate and there is a well known and very simple construction for such objects, which is if you take a bunch of say, normally distributed vectors.",
            "You, I and you take the sum of the outer products of these normal vectors.",
            "Then the matrix that you get is symmetric and positive definite.",
            "As long as you take enough of these compared to the dimensionality of the matrix.",
            "And in fact it has a wishard distribution.",
            "And there's also.",
            "This is also closely related to copula processes.",
            "I'm actually pretty excited about copulas.",
            "We had a whole workshop yesterday which was lots of fun.",
            "So if you want to follow up on that, ask me.",
            "Anyway, so now we have a basic nonparametric model for covariance matrices as a function of time or other deep."
        ],
        [
            "The variables and we can use them for.",
            "Modeling multivariate, changing things like some of these econometric data.",
            "And compare them to methods that other people consider to be state of the art and get good results in terms of predictive likelihood.",
            "So here the Bayesian nonparametric thing seems to be the one in bold."
        ],
        [
            "And I'll skip over some of these."
        ],
        [
            "So let me talk about one more idea and then leave some time for discussion.",
            "So here's the another idea that kind of combines Gaussian processes with neural network like structure.",
            "They don't seem.",
            "We don't seem to be able to completely shed neural networks in our community.",
            "They keep coming back coming back to haunt us and so here they are again.",
            "So here is this idea.",
            "It's basically imagine you're trying to do multivariate prediction.",
            "Let's take regression.",
            "You start with a few Gaussian processes, functions of X, and then we're going to combine them.",
            "Through weights to get our multivariate output.",
            "So it's a lot like kind of factor analysis decomposition.",
            "Or, you know, Gaussian process latent variable models.",
            "Except these things are observed.",
            "The axes are observed and we're trying to predict why, and then the interesting thing about this is that you get a lot of flexibility by making the weights themselves be functions of the input.",
            "So that's why we have these colorful things here.",
            "Imagine each of these objects included the inputs here, and the weights are functions of X are covariate vector.",
            "So the form of the model is written down here at the bottom.",
            "And it ends up being.",
            "Hello."
        ],
        [
            "OK, it ends up having a lot of nice properties like input dependent, correlation structure modeling, nonstationarity, have heavy tails, scaling well, etc."
        ],
        [
            "So skip over the results.",
            "We can use it to model correlated outputs nicely."
        ],
        [
            "And again, that we see, even though this is now in this supervised setting, we seem to be able to get good results compared to other methods."
        ],
        [
            "OK, so I'll summarize and then I'll have one slide of discussion points.",
            "So the summary is probabilistic, modeling Bayesian inference are two sides of the same coin.",
            "I actually have started liking to use the term inverse probability instead of Bayes.",
            "Bayesian sounds a bit like I'm following some some leader whose portrait is on my wall or something.",
            "Where is inverse probability?",
            "It's what it used to be called in the 19th century I think.",
            "And before and it's sort of just the notes.",
            "The idea that.",
            "You write down a probabilistic model and then you apply the rules of probability to infer things in the inverse direction about your model given the data.",
            "Now, Bayesian machine learning treats learning is a probabilistic inference problem.",
            "These things seem to work well when the models are flexible enough to capture relevant properties of data.",
            "I tried to motivate nonparametric Bayesian methods and I talked about the sort of three examples here."
        ],
        [
            "OK, so.",
            "Here is a discussion and critique side which we can dwell on if we want.",
            "If we have time.",
            "OK, now all this stuff seems very beautiful, but the Bayesian coherence arguments which you're probably familiar with are hard to defend if we have to do approximate inference right?",
            "I said oh, if we just stick to the sum rule in the product rule, then we're fine.",
            "But the problem is when we do approximate inference, we're not just sticking to the sum rule in the product rule, whether we're doing MCMC or variational or EP or whatever you want to call it.",
            "So there is an element of hope.",
            "We still hope to have good properties when when we do this.",
            "So what?",
            "How do we deal with this?",
            "That's one question I have some thoughts.",
            "You know, if we might be able to, for example, stand on the shoulders of people who have developed frequentist guarantees for Bayesian procedures and say I want it to be hardcore fundamentalist Bayesian.",
            "My computer was not big enough, so I had to do approximate inference, but yet I'm comfortable communicating this to the rest of the world because I can show that my procedure has certain nice frequentist properties.",
            "Even though I did approximate inference, that's the key thing.",
            "If I can show that, then that's nice.",
            "So some methods like PAC Bayes are actually nice ways of dealing with that.",
            "Now the Bayesian framework.",
            "The more you think about it, the more it forces you to do modeling, which can be very cumbersome and often there are some really simple non Bayesian algorithms that do very well like witness something like spectral clustering or locally linear embedding.",
            "Or for example Classical 2 sample tests.",
            "These are really easy things to do and it would be really hard to come up with a Bayesian analogue of that.",
            "Is Chris Holmes here?",
            "So he's come up with a Bayesian analogue of two sample tests.",
            "I've done it as well, but you know they're way more complicated than the frequentist thing.",
            "Very interesting thing to try to do, But anyway.",
            "Now structure discovery this whole thing that Oh my Bayesian method is going to discover the right number of hidden units or the right number of clusters.",
            "For example, the right number of clusters with additional process mixture.",
            "Well, that seems fraught with dangerous assumptions.",
            "In particular, I think discovering the number of clusters with the mixture model is is probably complete rubbish because you know what we intuitively think of as clusters or structure in the data correspond to.",
            "Don't correspond to nice parametric families of distributions, so if I fit a mixture of Gaussians to two bumps of data, if those bumps have some wiggles and funny tales, I might get my dear sleep process mixture to end up with 300 components because it needs to model all that stuff eventually.",
            "OK, so I can't interpret that as the number of clusters.",
            "Mixture models are not clustering methods guys.",
            "OK, I even called them clustering methods.",
            "But you know that's to undergraduates 'cause it's nice and intuitive.",
            "But we are adults here.",
            "Mixture models are not clustering methods.",
            "OK.",
            "So, nonparametric methods are memory based because they have this wide information channel from past data to future predictions.",
            "They are often defined in terms of remembering previous data, and that's often very computationally expensive.",
            "So that's another downer.",
            "Well, it's hard to be a subjective Bayesian.",
            "And then develop general purpose Bayesian models that somebody can download from your website and run them on their problems.",
            "OK, that's another downer.",
            "I think for us.",
            "Because OK, let me just try to explain why.",
            "Let's say I'm a subjective Bayesian.",
            "I want to build a really good prior into my model, but.",
            "You know, I have to know what I'm trying to model to do that.",
            "I can't write general purpose code with some sort of like the prior that will work for everybody.",
            "I mean, I could try to do that.",
            "You know there are people who try to do universal type things, but I think it's very hard to do that and still kind of.",
            "Say honestly that you're doing good subjective bayesianism.",
            "Here is another one that a lot of people get hung up on exchange ability assumptions well, they constrain the form of models that we are allowed to have.",
            "So for example, as was mentioned in the tutorial, most clustering models that have preferential attachment are not actually exchangeable.",
            "Only if you do preferential attachment in a particular way, we get an exchangeable distribution.",
            "And so you know, people are uncomfortable with that, understandably.",
            "Then finally, large parametric models often perform very well in practice.",
            "So is nonparametrics just a question of elegance?",
            "OK, I'll stop there."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about Bayesian nonparametrics, obviously, and the title of my talk is why Bayesian Nonparametrics?",
                    "label": 1
                },
                {
                    "sent": "Which I think is.",
                    "label": 0
                },
                {
                    "sent": "Sort of closely related to the vision of this workshop, which is to figure out why the people who are doing it are doing it, and does it really make sense, and what are the advantages and shortcomings?",
                    "label": 0
                },
                {
                    "sent": "So this is obviously a personal view and I'm a machine learning researcher and you know, I'm becoming more and more of a statistician, but I'm not a card carrying statistician and different people have different motivations for doing Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "In preparing these slides, I was both trying to think at some points I was very excited about doing Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "And so there's some very positive feelings in here about it.",
                    "label": 0
                },
                {
                    "sent": "And then at other points I was, I was sort of trying to figure out all the downsides, and so there's ups and downs and mood in this talk.",
                    "label": 0
                },
                {
                    "sent": "As you'll probably see, OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to break the question up into two questions.",
                    "label": 0
                },
                {
                    "sent": "Why be Bayesian and why do nonparametrics?",
                    "label": 0
                },
                {
                    "sent": "And I think the answer is sort of finally, well, we like to be Bayesian because it's a very simple framework, so simplicity goes in our favor as compared to other frameworks for doing learning and inference.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we want to do nonparametrics because of complexity, because the real world is actually complicated in the data that we're trying to model.",
                    "label": 1
                },
                {
                    "sent": "Come from complicated phenomena.",
                    "label": 0
                },
                {
                    "sent": "So let me first focus on on the 1st part.",
                    "label": 0
                },
                {
                    "sent": "Why be Bayesian and I apologize to people who you know, eat and sleep this stuff.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, it's so damn simple.",
                    "label": 0
                },
                {
                    "sent": "I mean, what other framework in machine learning can you write down with two rules?",
                    "label": 0
                },
                {
                    "sent": "OK, even I can remember these rules, you know everything falls through from 2 simple rules of probability.",
                    "label": 0
                },
                {
                    "sent": "You have the sum rule in the product rule and you know if you forget everything else, you could try to derive what you're doing from this.",
                    "label": 0
                },
                {
                    "sent": "Some rule in the product rule.",
                    "label": 0
                },
                {
                    "sent": "You know there's no rule that says.",
                    "label": 0
                },
                {
                    "sent": "You have to come back, suffice something and optimize it, and then you know, add a regularizer and and dance around the Table 3 times and then try it again with a bit of cross validation you just write down a model.",
                    "label": 0
                },
                {
                    "sent": "You apply this sum rule in the product rule as best you can.",
                    "label": 0
                },
                {
                    "sent": "There's a footnote there.",
                    "label": 0
                },
                {
                    "sent": "We'll talk about the footnote.",
                    "label": 0
                },
                {
                    "sent": "This is the up part of the talk when I'm really, you know, a fundamentalist Bayesian, nonparametric and.",
                    "label": 0
                },
                {
                    "sent": "And you know, it's just beautiful.",
                    "label": 0
                },
                {
                    "sent": "Alright, so where can we use this summer over product rule?",
                    "label": 0
                },
                {
                    "sent": "I wrote it as a sum rule in case there any computer scientists like myself in former computer scientists.",
                    "label": 0
                },
                {
                    "sent": "I'll sit in the audience because computer scientists are scared of integrals but just look at the integrals in tournament to sums.",
                    "label": 0
                },
                {
                    "sent": "There's some fancy math behind that you know all that.",
                    "label": 0
                },
                {
                    "sent": "So we can apply this some room product rule to all sorts of interesting things, for example to doing inference about parameters Theta given some data D under some model M, Some rule product rule, we get Bayes rule coming out.",
                    "label": 1
                },
                {
                    "sent": "We can give these things names like likelihood prior and posterior.",
                    "label": 0
                },
                {
                    "sent": "And then we, you know somebody says, Oh well, what's the use of that posterior?",
                    "label": 0
                },
                {
                    "sent": "I want predictions.",
                    "label": 0
                },
                {
                    "sent": "I'm an engineer.",
                    "label": 0
                },
                {
                    "sent": "I want predictions.",
                    "label": 0
                },
                {
                    "sent": "OK, well predictions come from some rule product rule.",
                    "label": 0
                },
                {
                    "sent": "There's no other way of doing it.",
                    "label": 0
                },
                {
                    "sent": "OK, no arbitrariness so and it's very sensible.",
                    "label": 0
                },
                {
                    "sent": "You look at the equation you say?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, that makes sense.",
                    "label": 0
                },
                {
                    "sent": "OK, each of your parameters make some prediction.",
                    "label": 0
                },
                {
                    "sent": "Its prediction is a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "You don't know which is the right parameter.",
                    "label": 0
                },
                {
                    "sent": "You're not going to do anything crazy like pick one of them.",
                    "label": 0
                },
                {
                    "sent": "OK, so you.",
                    "label": 0
                },
                {
                    "sent": "You average 'cause you don't know.",
                    "label": 0
                },
                {
                    "sent": "You know that's another fundamental thing.",
                    "label": 0
                },
                {
                    "sent": "It's not just two equations.",
                    "label": 0
                },
                {
                    "sent": "In mathematics.",
                    "label": 0
                },
                {
                    "sent": "There's something fundamental about Bayesians is.",
                    "label": 0
                },
                {
                    "sent": "We don't know.",
                    "label": 0
                },
                {
                    "sent": "We're uncertain all the time.",
                    "label": 0
                },
                {
                    "sent": "I think that's why a few of us were wandering around Granada for half an hour trying to pick a restaurant because we just didn't know.",
                    "label": 0
                },
                {
                    "sent": "You know what was the best thing to do.",
                    "label": 0
                },
                {
                    "sent": "We didn't end up sampling from them, but you know, you know what this equation tells.",
                    "label": 0
                },
                {
                    "sent": "Use you take your predictions and you average them with respect to your parameters and you average them with weights given by.",
                    "label": 0
                },
                {
                    "sent": "You know you're familiar.",
                    "label": 0
                },
                {
                    "sent": "Some room product rule weights from the data that you observed in the model and the assumptions you put in.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's one fundamental thing.",
                    "label": 0
                },
                {
                    "sent": "You're always uncertain the only language for dealing with uncertainty is probability theory.",
                    "label": 0
                },
                {
                    "sent": "As far as I'm concerned, it's a bit like you know, the language for dealing with rates of changes.",
                    "label": 0
                },
                {
                    "sent": "Calculus.",
                    "label": 0
                },
                {
                    "sent": "I don't think there are people now arguing I've got a better framework than calculus for rates of change or something like that.",
                    "label": 0
                },
                {
                    "sent": "You know that that issue has been settled for a couple 100 years.",
                    "label": 0
                },
                {
                    "sent": "Probability is the language for dealing with uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So we express all our forms of uncertainty in terms of probabilities and models are all about uncertainty.",
                    "label": 0
                },
                {
                    "sent": "The only thing we're certain about is the data.",
                    "label": 0
                },
                {
                    "sent": "So everything else we have to treat as uncertain quantities and we put probability distributions over them which represent our state of uncertainty before.",
                    "label": 0
                },
                {
                    "sent": "And after observing the data.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we can do other nice things like model comparisons you all know.",
                    "label": 0
                },
                {
                    "sent": "Again some rule product rule.",
                    "label": 0
                },
                {
                    "sent": "So we get this expression here, which tells us that the probability of model given the data can be obtained in terms of something we call the marginal likelihood or model evidence, which again is obtained by averaging quantity.",
                    "label": 0
                },
                {
                    "sent": "The likelihood over the parameters.",
                    "label": 0
                },
                {
                    "sent": "OK. Everybody, anybody object to this.",
                    "label": 0
                },
                {
                    "sent": "Leave now.",
                    "label": 0
                },
                {
                    "sent": "No, you know you know there.",
                    "label": 0
                },
                {
                    "sent": "There's somebody in the audience right now thinking the pry or where does it come from.",
                    "label": 0
                },
                {
                    "sent": "OK, we can talk about that.",
                    "label": 0
                },
                {
                    "sent": "I don't want to get hung up on that because this is not just about bayesianism.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of subtlety in here.",
                    "label": 0
                },
                {
                    "sent": "Obviously that's where my down, the more down and depressed part of the talk, OK?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "You have all the models for model, is it?",
                    "label": 0
                },
                {
                    "sent": "Yes, how do you know that you have all the models right?",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is this, by the way, is the marginal likelihood for a particular model M. Now if you want to do.",
                    "label": 0
                },
                {
                    "sent": "Real Bayesian prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm not being a real Bayesian here because I'm conditioning on a model I should really be averaging over all models that I would envision, and that's hard OK, because you know, my imagination may be limited and these averages might be expensive to do, but this is where the nonparametrics part comes in.",
                    "label": 0
                },
                {
                    "sent": "Interesting Lee, I think rather than averaging over some simple models and then saying, oh maybe I missed out other simple models.",
                    "label": 0
                },
                {
                    "sent": "The nonparametric framework which I'm about to talk about says.",
                    "label": 0
                },
                {
                    "sent": "I don't know none of these simple models look good to me.",
                    "label": 0
                },
                {
                    "sent": "Let me make a mega flexible, complicated model and then average over then.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now of course, not everybody does machine learning like this, yeah?",
                    "label": 0
                },
                {
                    "sent": "And in fact, there seem to be 2 camps in machine learning there is.",
                    "label": 0
                },
                {
                    "sent": "I would say a modeling camp versus a toolbox camp and I put in statistics in small font.",
                    "label": 0
                },
                {
                    "sent": "I added that to this slide for the statisticians, 'cause I think what I'm going to say actually applies to statistics as well.",
                    "label": 0
                },
                {
                    "sent": "You can interpret it as you want.",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming that you know anything about machine learning and statistics by font size here.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to add it to the slide and but not make a big deal out of it, which I have.",
                    "label": 0
                },
                {
                    "sent": "Alright, so there are two camps.",
                    "label": 0
                },
                {
                    "sent": "Machine Learning seeks to learn models of data.",
                    "label": 1
                },
                {
                    "sent": "Alright, you define a space of possible models.",
                    "label": 0
                },
                {
                    "sent": "That's a VIMS concern here.",
                    "label": 0
                },
                {
                    "sent": "Well, let's start by defining it.",
                    "label": 0
                },
                {
                    "sent": "Then you learn the parameters and structure of the models from the data and you make predictions and decisions.",
                    "label": 1
                },
                {
                    "sent": "That seems fairly straightforward.",
                    "label": 1
                },
                {
                    "sent": "A different view.",
                    "label": 0
                },
                {
                    "sent": "Machine learning is machine learning is a toolbox of methods of processing data.",
                    "label": 0
                },
                {
                    "sent": "You can feed the data into one of your possible methods in your toolbox.",
                    "label": 0
                },
                {
                    "sent": "And you want to choose methods that have good theoretical or empirical performance so you know somebody proves the nice theorem which says that this method has, you know, is consistent and has even optimal convergence rates.",
                    "label": 1
                },
                {
                    "sent": "You're like, wow, that sounds damn good.",
                    "label": 0
                },
                {
                    "sent": "I'm going to stick my data in that method.",
                    "label": 0
                },
                {
                    "sent": "OK, now you know or Alternatively somebody did a benchmark and compare 25 methods.",
                    "label": 0
                },
                {
                    "sent": "You pick the best one and you try your data on that method.",
                    "label": 0
                },
                {
                    "sent": "Then of course at the end you make predictions and decisions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Obviously a lot of what I'm going to talk about is in this modeling framework here, but there are parts of it that make me uncomfortable which I'll get to in the more depressed part of my talk is sort of nice.",
                    "label": 0
                },
                {
                    "sent": "This is very nice if I'm modeling a particular problem and I want to really solve it, and I try to put some understanding into that, etc.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what about the second part of this question?",
                    "label": 0
                },
                {
                    "sent": "Why basing on Paris?",
                    "label": 0
                },
                {
                    "sent": "So let's just talk about parametric versus nonparametric models, so we're all on the same page.",
                    "label": 0
                },
                {
                    "sent": "Parametric models assume some finite set of parameters stayed up.",
                    "label": 1
                },
                {
                    "sent": "Given the parameters predictions, X are independent of the observed data in the sense you know that's captured by this equation here.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You know, if you're if you're frequentist, this is fine.",
                    "label": 0
                },
                {
                    "sent": "You just don't put a conditioning bar here, but essentially the idea is that you you capture everything there is to know about the data through your parameters Theta.",
                    "label": 0
                },
                {
                    "sent": "This is what Peter Orbans was describing in his in his part of the tutorial, where he's talking about the pattern and the noise.",
                    "label": 0
                },
                {
                    "sent": "Theta captures the pattern.",
                    "label": 0
                },
                {
                    "sent": "But now if you have a parametric model, the finite set of parameters, here's the man himself.",
                    "label": 0
                },
                {
                    "sent": "Obviously, after a good breakfast, unlike mine, there was rush.",
                    "label": 0
                },
                {
                    "sent": "So the complexity of the model is bounded even if the amount of data is unbounded, so there is a if you like information theory.",
                    "label": 1
                },
                {
                    "sent": "And of course I do as well.",
                    "label": 0
                },
                {
                    "sent": "The parameters are an information channel between your training data and your future predictions, and that Channel has a bottleneck.",
                    "label": 0
                },
                {
                    "sent": "There's like traffic jams through that Channel because it's a finite width.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 1
                },
                {
                    "sent": "Nonparametric models assume that the data distribution cannot be defined in terms of such a finite set of parameters.",
                    "label": 1
                },
                {
                    "sent": "But they can often be defined by assuming some infinite dimensional parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "So you add more lanes to your information highway so that you don't get traffic jams.",
                    "label": 0
                },
                {
                    "sent": "Now we can think of our parameter Theta as a function.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about that is the amount of information that data can capture about the data.",
                    "label": 0
                },
                {
                    "sent": "D can grow as the amount of data grows, so again data has bits.",
                    "label": 0
                },
                {
                    "sent": "If it's just some finite precision and you want to capture some bits to make predictions, and this makes them more flexible.",
                    "label": 0
                },
                {
                    "sent": "So that's part of the point of doing nonparametric modeling.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now why nonparametrics?",
                    "label": 0
                },
                {
                    "sent": "I think actually this transcends the Bayesian thing.",
                    "label": 0
                },
                {
                    "sent": "Actually, if you look around in machine learning, a lot of successful methods are essentially nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Like the whole kernel revolution with SVM's and things like that and the related Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "These are nonparametric models and I would argue that's one of the reasons why they were so successful.",
                    "label": 0
                },
                {
                    "sent": "Of course, kernel methods replaced neural networks is a big paradigm of machine learning, and then people came back afterwards and said, oh we'll make bigger neural networks or bigger and deeper neural networks.",
                    "label": 0
                },
                {
                    "sent": "These are not nonparametric in go formal mathematical sense, but there heckuva lot parametric.",
                    "label": 0
                },
                {
                    "sent": "OK, you know?",
                    "label": 0
                },
                {
                    "sent": "You know one of Geoff Hinton or young Lacunes deep networks will have a million parameters, which is like more than my nonparametric model ever expresses on the datasets that I have on my computer.",
                    "label": 0
                },
                {
                    "sent": "So essentially nobody I haven't heard anybody say, oh, I have a deep network with six units.",
                    "label": 0
                },
                {
                    "sent": "Let me show you what it does.",
                    "label": 0
                },
                {
                    "sent": "Nobody does a deep network.",
                    "label": 0
                },
                {
                    "sent": "6 units.",
                    "label": 0
                },
                {
                    "sent": "So these are also successful because they are very flexible.",
                    "label": 0
                },
                {
                    "sent": "Even methods we like to poo poo like K. Nearest neighbors are pretty damn good and they were pretty flexible and their sort of nonparametric.",
                    "label": 0
                },
                {
                    "sent": "But there are very nonparametric.",
                    "label": 0
                },
                {
                    "sent": "So I put a footnote here which is successful methods in machine learning are essentially nonparametric.",
                    "label": 1
                },
                {
                    "sent": "And then if you can read down there.",
                    "label": 0
                },
                {
                    "sent": "Or highly scalable?",
                    "label": 0
                },
                {
                    "sent": "OK, so there also, you know, you know, I don't know how to measure success in machine learning.",
                    "label": 0
                },
                {
                    "sent": "I just look at, you know things that people are excited about, and they're often excited about these very fancy.",
                    "label": 0
                },
                {
                    "sent": "Flexible methods, or they're excited if they can run logistic regression on 100 billion data points in 20,000 million dimensions?",
                    "label": 0
                },
                {
                    "sent": "Because you know some methods you can make very very scalable.",
                    "label": 0
                },
                {
                    "sent": "And if you're Google or Microsoft or Yahoo.",
                    "label": 0
                },
                {
                    "sent": "You care about that a lot.",
                    "label": 0
                },
                {
                    "sent": "And those are successful as well.",
                    "label": 0
                },
                {
                    "sent": "I don't want to put that.",
                    "label": 0
                },
                {
                    "sent": "In fact, if I was working in industry, God forbid I would probably be, you know, less interested in these cool mathematical flexibel things and wanting to figure out how to do logistic regression on the cloud of 10,000 computers.",
                    "label": 0
                },
                {
                    "sent": "OK, so nonparametrics as a successful thing isn't restricted to Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "But I already argued Bayesian methods are very nice, right?",
                    "label": 0
                },
                {
                    "sent": "So we'll put the thing the two together.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another way to approach this is to start out with like A wish list.",
                    "label": 0
                },
                {
                    "sent": "OK, I want to do machine learning or statistics.",
                    "label": 0
                },
                {
                    "sent": "What are the desirable properties of my methods?",
                    "label": 1
                },
                {
                    "sent": "Let me write down a list.",
                    "label": 0
                },
                {
                    "sent": "This was my list from late last night, so I apologize if I omitted any of your favorable favorite desirable properties.",
                    "label": 0
                },
                {
                    "sent": "So here here I go.",
                    "label": 0
                },
                {
                    "sent": "Good predictive performance OK?",
                    "label": 0
                },
                {
                    "sent": "Flexible that sort of goes hand in hand with good predictive performance, robust overfitting.",
                    "label": 0
                },
                {
                    "sent": "Hence you know the all this industry of figuring out regularizers and things like that.",
                    "label": 0
                },
                {
                    "sent": "Model based I think is a sort of.",
                    "label": 0
                },
                {
                    "sent": "You know, if you're doing, there are some advantages to this.",
                    "label": 0
                },
                {
                    "sent": "It's not absolutely necessary.",
                    "label": 0
                },
                {
                    "sent": "If you have good predictive performance and you're flexible and robust.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have to be that whatever you're doing corresponds to a particular model, but you know if you have a model based approach, you can make it.",
                    "label": 0
                },
                {
                    "sent": "You can make it modular and fit with other things easily.",
                    "label": 0
                },
                {
                    "sent": "Automatic.",
                    "label": 0
                },
                {
                    "sent": "Automatic is nice because people are expensive and computers are cheap and you want your computer to do the thinking for you.",
                    "label": 0
                },
                {
                    "sent": "That's the sort of original AI idea.",
                    "label": 0
                },
                {
                    "sent": "The originally idea wasn't to have some schmuck sitting there tuning parameters of you know, machine learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "You want it to be automatic.",
                    "label": 0
                },
                {
                    "sent": "Scalable.",
                    "label": 0
                },
                {
                    "sent": "Interpretable a lot of people are interested in this.",
                    "label": 0
                },
                {
                    "sent": "And reproducible.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a limited list.",
                    "label": 0
                },
                {
                    "sent": "And I would argue that.",
                    "label": 0
                },
                {
                    "sent": "Generally, Bayesian nonparametric methods.",
                    "label": 0
                },
                {
                    "sent": "Satisfied the the five that I have in green.",
                    "label": 0
                },
                {
                    "sent": "OK so in general because they are quite flexible.",
                    "label": 1
                },
                {
                    "sent": "They are going to have good predictive performance.",
                    "label": 0
                },
                {
                    "sent": "Of course you can really mess it up by giving these things.",
                    "label": 0
                },
                {
                    "sent": "Ridiculous priors, or using the completely inappropriate model for your data at hand?",
                    "label": 0
                },
                {
                    "sent": "There's no guarantee it's not idiot proof, right?",
                    "label": 0
                },
                {
                    "sent": "But you know, generally, you'll have good predictive performance if you're using them in a reasonable setting there flexibel overfitting, what's that?",
                    "label": 0
                },
                {
                    "sent": "Your Bayesian sum and product rule have no fitting in them.",
                    "label": 0
                },
                {
                    "sent": "OK, you're just doing averaging, so we have problems.",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming Bayesians don't have problems, but overfitting is not one of them.",
                    "label": 0
                },
                {
                    "sent": "'cause we don't fit.",
                    "label": 0
                },
                {
                    "sent": "If you don't fit, you don't overfit.",
                    "label": 0
                },
                {
                    "sent": "It's like over skiing.",
                    "label": 0
                },
                {
                    "sent": "If you don't ski, don't over ski, OK.",
                    "label": 0
                },
                {
                    "sent": "They're clearly model based.",
                    "label": 0
                },
                {
                    "sent": "I could actually generate data from them.",
                    "label": 0
                },
                {
                    "sent": "Now that's interesting.",
                    "label": 0
                },
                {
                    "sent": "I would say, well, what's a model if you know, would anybody like to define a model?",
                    "label": 0
                },
                {
                    "sent": "I've got a definition of model is.",
                    "label": 0
                },
                {
                    "sent": "A representation of possible data you could observe.",
                    "label": 0
                },
                {
                    "sent": "OK, if that's not a model.",
                    "label": 0
                },
                {
                    "sent": "Then what is?",
                    "label": 0
                },
                {
                    "sent": "If it can't predict data, then you can't falsify it.",
                    "label": 0
                },
                {
                    "sent": "So models are essentially once you add the probability magic dust to them, their probabilistic distributions are generative models.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about basic nonparametric methods is that their automatic innocence.",
                    "label": 0
                },
                {
                    "sent": "They learn the structure automatically from the data.",
                    "label": 0
                },
                {
                    "sent": "Now, unfortunately.",
                    "label": 0
                },
                {
                    "sent": "We work hard to make them scalable, but you know there are.",
                    "label": 0
                },
                {
                    "sent": "You know they're not all that scalable yet, but there are lots of smart people trying to make them more scalable.",
                    "label": 0
                },
                {
                    "sent": "We could go either way on interpretable.",
                    "label": 0
                },
                {
                    "sent": "You know, if you have infinitely many parameters, is.",
                    "label": 0
                },
                {
                    "sent": "You're not going to be able to explain it like a decision tree to your client, or you know like an inductive logic program or something like that.",
                    "label": 0
                },
                {
                    "sent": "And reproducible, I put that in there because I think a lot of people are troubled by, you know, what do you mean you ran the method again and you got a different result?",
                    "label": 0
                },
                {
                    "sent": "You know, obviously, if we're using something like MCMC in our inference.",
                    "label": 0
                },
                {
                    "sent": "The less we save the random seed.",
                    "label": 0
                },
                {
                    "sent": "We're not going to be reproducible when we do inference, so a lot of people are troubled by that.",
                    "label": 0
                },
                {
                    "sent": "You know, the airplane flew fine 957 times in the 950 eighth time it went and crashed.",
                    "label": 0
                },
                {
                    "sent": "OK, the engineers don't really like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are some desirable properties of machine learning methods and and you know, I think we fairly well hit these and not so well those.",
                    "label": 0
                },
                {
                    "sent": "Any comments on this?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Concerning flexibility, there is the fact that you have few branches are hyperparameters to to be too and this.",
                    "label": 0
                },
                {
                    "sent": "If this were you, fix it.",
                    "label": 0
                },
                {
                    "sent": "No, it is true that most nonparametric models have only a few hyperparameters at the top, and I think that that's nice because that makes them easier to make automatic.",
                    "label": 0
                },
                {
                    "sent": "You could learn those hyperparameters as well from the data, so you don't have to tune those by hand.",
                    "label": 0
                },
                {
                    "sent": "By flexibility, I mean that under those hyperparameters, generally at the level below that you have infinitely many parameters, and that makes them very flexible.",
                    "label": 0
                },
                {
                    "sent": "Their distributions on measures or functions, things like that.",
                    "label": 0
                },
                {
                    "sent": "Those are more flexible than you know.",
                    "label": 0
                },
                {
                    "sent": "Distributions on Gaussian measures K distributions on.",
                    "label": 0
                },
                {
                    "sent": "Oh or not, we can have additional measures, but you know there are more flexible generally than the parametric cousins that they come from.",
                    "label": 0
                },
                {
                    "sent": "OK, we'll see some examples of that.",
                    "label": 0
                },
                {
                    "sent": "I see there are no other questions.",
                    "label": 0
                },
                {
                    "sent": "Alright, yeah, so I'm not so convinced by the automatic because a common experience but Oh well.",
                    "label": 0
                },
                {
                    "sent": "I have to clamp my hyper these values and then burn in and then release them and then add this bit of the model next and then I have to do variational On this date and there's a lot of choices.",
                    "label": 0
                },
                {
                    "sent": "I think you've got to be pretty expert to make this work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a good thing you are.",
                    "label": 0
                },
                {
                    "sent": "By automatic, what I mean is.",
                    "label": 0
                },
                {
                    "sent": "In a sense that you can learn the structure from data in an automatic way, rather than saying I want 7 hidden units in my first layer in 300 in my second layer and so on.",
                    "label": 0
                },
                {
                    "sent": "That's what I mean by automatic.",
                    "label": 0
                },
                {
                    "sent": "I don't mean that the inference is automatic.",
                    "label": 0
                },
                {
                    "sent": "If it was totally automatic, I wouldn't need grad students.",
                    "label": 0
                },
                {
                    "sent": "I'm talking about the fundamental properties of the modeling framework rather than the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The lot of the painful stuff happens when you take the beautiful idea that comes from some product rule and then you turn it into a working system.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of non automatic steps there, I agree.",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "Scalable property.",
                    "label": 0
                },
                {
                    "sent": "Statistics background OK. By scalable I mean some.",
                    "label": 0
                },
                {
                    "sent": "If you go to some talks in machine learning, they're trying to run their methods on.",
                    "label": 0
                },
                {
                    "sent": "You know, hundreds of millions of data points, like you know, every time somebody clicks on an ad on, you know Microsoft's Bing.",
                    "label": 0
                },
                {
                    "sent": "You know it goes into a probabilistic model somewhere, then does inference and they're not using basically nonparametric methods for that, because they want something really fast that they can.",
                    "label": 0
                },
                {
                    "sent": "Paralyzed and running an online streaming manner and it's just harder to do that with Bayesian non parametric methods.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about this in a minute actually, but remind me if I don't.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now as just as a reminder.",
                    "label": 0
                },
                {
                    "sent": "There are two.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric families that have occupied a lot of interest in in the machine learning and statistics community.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to just basically review a little bit of that.",
                    "label": 0
                },
                {
                    "sent": "And then talk a little bit about some of the interesting things that one can do within the nonparametric framework and then come back to some of the ups and downs of the framework as well.",
                    "label": 0
                },
                {
                    "sent": "Towards the end.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are some nice building blocks for a lot of Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "One of 'em is the Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "And this defines the distribution over functions.",
                    "label": 0
                },
                {
                    "sent": "So this is a sample from.",
                    "label": 1
                },
                {
                    "sent": "A Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "It's a draw from a function, and Gaussian process is really most intuitively we can think of it as an infinite dimensional Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "So just like the multivariate Gaussian, it has a mean and covariance.",
                    "label": 1
                },
                {
                    "sent": "But now it's a mean function and a covariance function that takes 2 arguments.",
                    "label": 0
                },
                {
                    "sent": "So in a lot of models we have unknown functions and we can plug in Gaussian processes in those places and then we can worry about whether that makes sense or not as a distribution over functions.",
                    "label": 0
                },
                {
                    "sent": "The other process that's been studied a lot in Beijing Nonparametrics is Additionally process.",
                    "label": 1
                },
                {
                    "sent": "This defines a distribution on distributions or measure or measures, so think of it as a function that's non negative and integrates to one.",
                    "label": 0
                },
                {
                    "sent": "And again, just just like the Gaussian process is the infinite dimensional analogue of a finite dimensional thing, which is the Dirichlet distribution.",
                    "label": 1
                },
                {
                    "sent": "So we say G, which is a random probability measure, is drawn from.",
                    "label": 0
                },
                {
                    "sent": "Additionally process with base measure or among you and me, the mean G0 and then concentration parameter Alpha which controls the.",
                    "label": 0
                },
                {
                    "sent": "Sort of dispersion around that mean in a sense.",
                    "label": 0
                },
                {
                    "sent": "OK, and so these are probability distributions over infinite dimensional quantities.",
                    "label": 0
                },
                {
                    "sent": "In this case, functions in this case distributions.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I've with permission.",
                    "label": 0
                },
                {
                    "sent": "I borrowed a few slides from Peter in UI.",
                    "label": 0
                },
                {
                    "sent": "And this is a nice slide to motivate why people are interested in things like Gaussian processes and Ashley processes, which is this notion of coverage of priors?",
                    "label": 0
                },
                {
                    "sent": "So imagine you have a space of possible objects that you want to put probability distributions over.",
                    "label": 0
                },
                {
                    "sent": "So for example, this could be.",
                    "label": 0
                },
                {
                    "sent": "The space of all measures.",
                    "label": 0
                },
                {
                    "sent": "Well, now you know it's drawn as a triangle because you know you can think of it as measures on you know three probability solutions over.",
                    "label": 0
                },
                {
                    "sent": "Three possible events.",
                    "label": 0
                },
                {
                    "sent": "OK, but think of it as more an infinite dimensional triangle.",
                    "label": 0
                },
                {
                    "sent": "If you like, OK, so measures are measures or measures on functions.",
                    "label": 0
                },
                {
                    "sent": "This is a really big space.",
                    "label": 0
                },
                {
                    "sent": "OK, this thing in Gray.",
                    "label": 0
                },
                {
                    "sent": "And usually when we define a class of models in the classical parametric world, we restrict ourselves to some subspace of this.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, this could be the triangle here could be all functions.",
                    "label": 0
                },
                {
                    "sent": "And this could be all polynomials up to order 2.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "And of course.",
                    "label": 0
                },
                {
                    "sent": "Polynomials of order two don't have good coverage in the space of functions.",
                    "label": 0
                },
                {
                    "sent": "Alright, because.",
                    "label": 0
                },
                {
                    "sent": "You know they're good at modeling polynomials of order two, but for any function that I pick in some suitable class of functions.",
                    "label": 0
                },
                {
                    "sent": "Suitably large class of functions.",
                    "label": 0
                },
                {
                    "sent": "It's not the case that there is a polynomial of order two that's arbitrarily close to it.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm not spreading probability mass over this whole space in a nice way.",
                    "label": 0
                },
                {
                    "sent": "Of course I can't.",
                    "label": 0
                },
                {
                    "sent": "It turns out for reasons that are a little beyond my mathematical abilities that you know I can't put a distribution on the space of all functions.",
                    "label": 0
                },
                {
                    "sent": "It's just way too big a space.",
                    "label": 0
                },
                {
                    "sent": "But you can put distributions on.",
                    "label": 0
                },
                {
                    "sent": "Very, very large subspaces of those things like, you know, near all continuous functions, let's say.",
                    "label": 0
                },
                {
                    "sent": "So what we want is we want our prior to have good coverage in the sense that means that we want our models to put to sprinkle probability mass near everywhere on this triangle.",
                    "label": 0
                },
                {
                    "sent": "They won't cover the whole triangle, but there will be kind of probability dust all over the place.",
                    "label": 0
                },
                {
                    "sent": "So when the truth is somewhere, there's some probability mass near that.",
                    "label": 0
                },
                {
                    "sent": "OK, in this case, if your true generating process is this you you're very far from it, even asymptotically.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions about that?",
                    "label": 0
                },
                {
                    "sent": "That was a bit hand WAVY, I apologize, but I think it's a good intuition and this is one of the motivations why Gaussian processes and earthly processes have been so widely studied 'cause they have good coverage properties.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Another reason why we're interested in Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "Is that it's not just like one model for one kind of problem, but it's actually an incredibly flexible framework for.",
                    "label": 0
                },
                {
                    "sent": "For modeling all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "So again, this is a slide borrowed from Peter and UIC Tauriel.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "They've listed.",
                    "label": 0
                },
                {
                    "sent": "All sorts of problems or applications that people solve in machine learning and many related fields and including statistics.",
                    "label": 0
                },
                {
                    "sent": "Where there there is at the heart of this problem, there is some fundamental object of interest.",
                    "label": 1
                },
                {
                    "sent": "And so you try to put distributions on those objects of interest and those objects of interest are fundamentally.",
                    "label": 0
                },
                {
                    "sent": "High dimensional or nonparametric?",
                    "label": 0
                },
                {
                    "sent": "OK, for example in classification and regression there is an unknown function that we're trying to learn and so we can use something like Gaussian process to put a distribution on that unknown function.",
                    "label": 0
                },
                {
                    "sent": "Of course that's not the.",
                    "label": 0
                },
                {
                    "sent": "It's not like a.",
                    "label": 0
                },
                {
                    "sent": "It's not like a mathematical necessity that every time we have an unknown function we place a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's easy to show that this is a pretty limited.",
                    "label": 0
                },
                {
                    "sent": "You know way of defining distributions on functions limited in a funny way in that you know yes, with the Gaussian process you could probably get very close to any function if you have enough data.",
                    "label": 0
                },
                {
                    "sent": "But it's a.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not the case that is the most natural thing for modeling functions which have certain properties, like for example, if you had a monotonically increasing function or a convex function, or a piecewise linear function, or something like that, then this would be not a very natural thing to do, even if it has good coverage properties, you'll get there eventually, but there might be more natural distribution that you could use here and in all of these cases there are a huge number of different.",
                    "label": 1
                },
                {
                    "sent": "Bayesian nonparametric models that can be applied to almost any topic in machine learning.",
                    "label": 0
                },
                {
                    "sent": "So, so that's good.",
                    "label": 0
                },
                {
                    "sent": "It is sense it's a framework rather than a small set of models.",
                    "label": 0
                },
                {
                    "sent": "It's not just all about the Chinese restaurant process.",
                    "label": 1
                },
                {
                    "sent": "OK, that's why people in machine learning get first exposed to, I think.",
                    "label": 0
                },
                {
                    "sent": "And they get obsessed with things like why does the NTH person have to sit at the table in proportion to the previous forget about that stuff?",
                    "label": 0
                },
                {
                    "sent": "I mean, yes, that's a particular detail of something like the Chinese restaurant process, but there is a huge and very rich world of models out there that we can use in all sorts of problems.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So I want to talk a little bit, maybe about some of the work that that.",
                    "label": 0
                },
                {
                    "sent": "That I've been thinking about in terms of applying Bayesian nonparametrics to other kinds of structured objects other than functions and distributions.",
                    "label": 0
                },
                {
                    "sent": "I'll try to go through this quickly because I want to spend some time at the end on some of the discussion topics.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are some of the things that I think are interesting, so let's put distributions on other structured objects.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about sparse matrices, deep, sparse graphical models, covariances, and some recent work on network structured regression.",
                    "label": 1
                },
                {
                    "sent": "Just as examples of.",
                    "label": 0
                },
                {
                    "sent": "Fun things we can do by composing different kinds of models.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So sparse matrices.",
                    "label": 0
                },
                {
                    "sent": "Probably a lot of you know about this, but I'll go through very quickly, so I think sparse matrices are very fundamental thing that we can use in lots of different models.",
                    "label": 0
                },
                {
                    "sent": "I didn't realize that until we did some work with sparse matrices and then it turned out everywhere I looked like, oh, I can throw a sparse matrix into there and it makes it more natural or more interesting.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is how we can generate.",
                    "label": 0
                },
                {
                    "sent": "Infinite sparse matrices, but particular kind.",
                    "label": 0
                },
                {
                    "sent": "This is not the only kind of sparse matrix, obviously.",
                    "label": 0
                },
                {
                    "sent": "So here's the interpretation.",
                    "label": 0
                },
                {
                    "sent": "We're going to use sparse matrices in a particular context, but we can use them in other other ways as well.",
                    "label": 0
                },
                {
                    "sent": "So imagine we have objects.",
                    "label": 0
                },
                {
                    "sent": "The rows correspond to objects.",
                    "label": 0
                },
                {
                    "sent": "And the columns correspond to latent features that an object may possess, so we're trying to do some latent variable learning, and we're going to think of binary latent variables that an object may possess.",
                    "label": 0
                },
                {
                    "sent": "But instead of limiting ourselves to, you know seven latent variables.",
                    "label": 0
                },
                {
                    "sent": "We're going to do something, which is take a distribution on finite binary matrices and see what happens when we take the limit as the number of columns K goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So here is really the simplest way I could imagine doing this.",
                    "label": 0
                },
                {
                    "sent": "ZN K = 1 means object An has hidden feature K like for example, you know.",
                    "label": 1
                },
                {
                    "sent": "ITunes user 3.",
                    "label": 0
                },
                {
                    "sent": "Likes country music or something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, but you don't directly observed that.",
                    "label": 0
                },
                {
                    "sent": "They like country music that you just observe that they have some music files or something.",
                    "label": 0
                },
                {
                    "sent": "Now, if XN K = 1 means that object then has feature K. The way we're going to model this is going to say like.",
                    "label": 0
                },
                {
                    "sent": "Let's say that's a Bernoulli random variable with parameter Theta, K. The parameters they decay correspond to.",
                    "label": 0
                },
                {
                    "sent": "The sort of frequency with which objects have particular features.",
                    "label": 0
                },
                {
                    "sent": "There's one parameter per column, so some features might be, you know, more frequent, like liking country music or something like that.",
                    "label": 0
                },
                {
                    "sent": "I guess some features might be less frequent, like you know liking Moldovian folk songs or something right now.",
                    "label": 0
                },
                {
                    "sent": "The parameter Theta K in each column we can draw from a conjugate prior to this Bernoulli.",
                    "label": 0
                },
                {
                    "sent": "In this case, we're going to make it a beta prior with parameter Alpha over Big K. The number of columns and 2nd parameter one.",
                    "label": 0
                },
                {
                    "sent": "This is really I thought.",
                    "label": 0
                },
                {
                    "sent": "I think maybe the simplest way of doing this, because when you take the limit as big K goes to Infinity, what happens to this matrix is that obviously gets wider and wider, but it gets sparser and sparser as well.",
                    "label": 0
                },
                {
                    "sent": "So if Z is an N by K matrix, then the expected number of nonzero entries will be N times Alpha.",
                    "label": 1
                },
                {
                    "sent": "On average, each row will have Alpha entries.",
                    "label": 1
                },
                {
                    "sent": "And this limiting procedure gives you the Indian buffet process which is a distribution on infinite sparse binary matrices.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's nice because you can actually write down what this distribution is, or in particular you can write down the probability of a particular class of matrices once you reorder the columns so that you shove all the zeros to one side.",
                    "label": 0
                },
                {
                    "sent": "You can write this down explicitly, and it's an exchangeable distribution, and it has a bunch of interesting properties and then.",
                    "label": 0
                },
                {
                    "sent": "And then once you start looking at it closely, which a lot of people, including us, did you see that it's related to lots of other things, like just like the Chinese restaurant process?",
                    "label": 0
                },
                {
                    "sent": "Or rather the Dirichlet process, we can write down a stick breaking representation.",
                    "label": 0
                },
                {
                    "sent": "This is definitely mixing distribution in the beta process and we can generate more flexible two and three parameter versions.",
                    "label": 1
                },
                {
                    "sent": "Winky",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, of course, in a lot of applications we don't want just to limit ourselves to binary variables.",
                    "label": 0
                },
                {
                    "sent": "We want non binary sparse latent features, so we can use this matrix Z to model the sparsity structure of non binary variables, for example by taking an elementwise product between Z and a matrix of any other random variable.",
                    "label": 1
                },
                {
                    "sent": "OK, so Zed captures the sparsity structure and then in the other random variables we can have the context content of the non sparse elements.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So like for Chinese restaurant processes and earthly process, there's been a lot of work on inference in these models, including a variety of samplers and some deterministic algorithms, so this sort of difficult computational bits sometimes take years to workout well.",
                    "label": 0
                },
                {
                    "sent": "I'm not convinced this completely worked out well yet, but I think it's much more scalable than when when the model was.",
                    "label": 0
                },
                {
                    "sent": "Originally proposed.",
                    "label": 0
                },
                {
                    "sent": "So what do we do with?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These things um.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "We use them as components of larger models, so this is all within this modeling framework.",
                    "label": 0
                },
                {
                    "sent": "Our models of probability distribution over data.",
                    "label": 0
                },
                {
                    "sent": "We have different kinds of components.",
                    "label": 0
                },
                {
                    "sent": "One of our components might be one of these sparse binary matrices.",
                    "label": 0
                },
                {
                    "sent": "Then we can combine that matrix zed with different kinds of likelihood functions that relate the hidden matrix to the data.",
                    "label": 1
                },
                {
                    "sent": "And once we do that, we find that there are lots of neat places where you can put that in.",
                    "label": 1
                },
                {
                    "sent": "For example, as models of graph structures.",
                    "label": 1
                },
                {
                    "sent": "Or protein complexes or collaborative filtering settings, or overlapping clustering.",
                    "label": 0
                },
                {
                    "sent": "And these are things that look a little more like standard machine learning right?",
                    "label": 0
                },
                {
                    "sent": "Originally that thing just looked like some funny distribution on some funny object, and now we're trying to do actual machine learning with it.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just a couple of examples of that.",
                    "label": 0
                },
                {
                    "sent": "Here is a nonparametric binary matrix factorization.",
                    "label": 1
                },
                {
                    "sent": "This is work with Meads and Ruyssen Neil where we have a matrix of data which could be something like jeans versus patients or users versus movies and we want to represent we want we don't want to do like their models out there that do Co clustering so they cluster jeans and a cluster movies.",
                    "label": 0
                },
                {
                    "sent": "Well that's a weird one.",
                    "label": 0
                },
                {
                    "sent": "Now they cluster jeans jeans and they cluster patients.",
                    "label": 0
                },
                {
                    "sent": "OK, but clustering you know clustering isn't flexible enough.",
                    "label": 0
                },
                {
                    "sent": "I'm not happy with clustering.",
                    "label": 0
                },
                {
                    "sent": "Clustering says if if I'm a gene I can only be in one and only one cluster.",
                    "label": 0
                },
                {
                    "sent": "That seems a bit limited to me or I'm a user.",
                    "label": 0
                },
                {
                    "sent": "You have to either GroupMe in with the people who like country music or with the people who like folk dancing, music or something.",
                    "label": 0
                },
                {
                    "sent": "And I don't really like that.",
                    "label": 0
                },
                {
                    "sent": "I don't like either of those actually.",
                    "label": 0
                },
                {
                    "sent": "So we want more flexible models.",
                    "label": 0
                },
                {
                    "sent": "We want models in which objects can belong to multiple clusters at the same time.",
                    "label": 0
                },
                {
                    "sent": "And this allows you to do that by have by having a binary vector representation of the hidden features for each object in each row in each column.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll give you another example very quickly, and then I'll do a reality check.",
                    "label": 0
                },
                {
                    "sent": "So the other example is learning the structure of deep sparse graphical models so we can use an IDP matrix to relate a bunch of observed variables to layer of hidden units.",
                    "label": 1
                },
                {
                    "sent": "We're going to do deep learning here.",
                    "label": 0
                },
                {
                    "sent": "OK, we're going to deep learning, but in a slightly subversive way.",
                    "label": 0
                },
                {
                    "sent": "By using Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "So let's build up a deep network by having a layer of observe units at the bottom and infinitely wide.",
                    "label": 0
                },
                {
                    "sent": "It's not just deep, it's wide as well, an infinitely wide hidden unit layer here, and this binary matrix represents which hidden units are connected to which observed units.",
                    "label": 0
                },
                {
                    "sent": "Everybody happy with that.",
                    "label": 0
                },
                {
                    "sent": "Observed Unit 1 is connected to the first 2 hidden units.",
                    "label": 0
                },
                {
                    "sent": "OK, etc.",
                    "label": 0
                },
                {
                    "sent": "So we can stack this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trying multiple cascading.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Draws from the Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "And so in this work with Ryan Adams and Hannah Lala.",
                    "label": 0
                },
                {
                    "sent": "In the end, the ICP is just used as a building block to define infinitely deep, infinitely wide networks.",
                    "label": 0
                },
                {
                    "sent": "A priore, of course, once you observe data, you sample from the distribution of structures given the data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You can apply this sort of thing to the kinds of image data that deep belief networks have been applied to here.",
                    "label": 0
                },
                {
                    "sent": "For example, it's faces where you can then say OK, let's predict the bottom half of the face from the top half and you get sort of fuzzy chins and things like that.",
                    "label": 0
                },
                {
                    "sent": "And you can look at the the hidden units and see what they end up connecting to that sparse connectivity pattern from the hidden units of the observed units is learned automatically by the model.",
                    "label": 0
                },
                {
                    "sent": "OK, this is what I mean by automatic.",
                    "label": 0
                },
                {
                    "sent": "The number of hidden layers.",
                    "label": 1
                },
                {
                    "sent": "Seem to be around 3 for these data.",
                    "label": 1
                },
                {
                    "sent": "The number of units per hidden layer seems to be around 70 and the sparse connectivity of hidden units to the observer units.",
                    "label": 0
                },
                {
                    "sent": "All of those things are learned automatically simply by taking our prior.",
                    "label": 0
                },
                {
                    "sent": "And then conditioning on the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can look at the higher level units and see fantasies of the model which look like these.",
                    "label": 0
                },
                {
                    "sent": "Specially faces.",
                    "label": 0
                },
                {
                    "sent": "And see what happens when you look at activations of higher level units.",
                    "label": 0
                },
                {
                    "sent": "You see these scary looking faces lurking deep inside this nonparametric Bayesian deep wide network.",
                    "label": 0
                },
                {
                    "sent": "OK now.",
                    "label": 0
                },
                {
                    "sent": "I think I'm.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "Ready for the reality check?",
                    "label": 0
                },
                {
                    "sent": "OK, here's a reality check.",
                    "label": 0
                },
                {
                    "sent": "I just told you a model that does users buy movies.",
                    "label": 0
                },
                {
                    "sent": "That sounds like a fantastic collaborative filtering model.",
                    "label": 0
                },
                {
                    "sent": "So did I make $1,000,000 out of this or that wouldn't be me anyway?",
                    "label": 0
                },
                {
                    "sent": "Did the people that worked on it make $1,000,000 out of it?",
                    "label": 0
                },
                {
                    "sent": "Did we win the Netflix Prize?",
                    "label": 1
                },
                {
                    "sent": "No, we didn't win the Netflix Prize.",
                    "label": 0
                },
                {
                    "sent": "Are high are infinitely wide, infinitely deep?",
                    "label": 0
                },
                {
                    "sent": "Nonparametric Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "Did it beat the best emnace test results compared to other deep networks?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK, is that fair?",
                    "label": 0
                },
                {
                    "sent": "Ryan, that's fair, right?",
                    "label": 0
                },
                {
                    "sent": "Whynott right?",
                    "label": 0
                },
                {
                    "sent": "I think as one of the workshop organizers, you better answer this before the end of the workshop.",
                    "label": 0
                },
                {
                    "sent": "This is the reality check.",
                    "label": 0
                },
                {
                    "sent": "This stuff is is beautiful, it's elegant.",
                    "label": 0
                },
                {
                    "sent": "It all follows from the sum and product rule and a bit of clever MCMC, OK?",
                    "label": 0
                },
                {
                    "sent": "But hasn't made a humongous splash.",
                    "label": 0
                },
                {
                    "sent": "Not not yet no no OK. That's sad.",
                    "label": 0
                },
                {
                    "sent": "But it's still beautiful, so maybe we can take comfort in it.",
                    "label": 0
                },
                {
                    "sent": "Well, you know it could be something to do with the kinds of projects I get involved in.",
                    "label": 0
                },
                {
                    "sent": "I have had a look around and it seems that you know none of none of my Bayesian nonparametric things have beat anything.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should worry about that.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there are success stories, apparently not mine, unfortunately.",
                    "label": 0
                },
                {
                    "sent": "Well, :) 'cause I can talk about them and.",
                    "label": 0
                },
                {
                    "sent": "I'll just give you a few examples, so it's not all hopeless.",
                    "label": 0
                },
                {
                    "sent": "It's like you know you can have beautiful models that have very elegant properties that perform quite loudly, but occasionally they are beautiful models that are elegant and they perform state of the art, and that's what gives.",
                    "label": 0
                },
                {
                    "sent": "That's what gives me hope.",
                    "label": 0
                },
                {
                    "sent": "So here's a few.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Examples, so again borrowing from some of the examples in the tutorial.",
                    "label": 0
                },
                {
                    "sent": "Here is work on segmentation of motion capture data by Emily Fox and colleagues.",
                    "label": 1
                },
                {
                    "sent": "Where you can get very fairly state of the art segmentations of pretty complicated rich data from Bayesian nonparametric models.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is some work on word segmentation, again using Bayesian nonparametric models and.",
                    "label": 0
                },
                {
                    "sent": "You know the the line in bold in these papers is the Bayesian nonparametric model is compared to competitors?",
                    "label": 0
                },
                {
                    "sent": "So that gives you some comfort.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is excellent work in language modeling with the hierarchical Pitman, your language model, which can give excellent results compared to other competing methods.",
                    "label": 0
                },
                {
                    "sent": "And related work in compression with the sequence memoizer that is beating state of the art compression algorithms.",
                    "label": 0
                },
                {
                    "sent": "So there are success stories.",
                    "label": 0
                },
                {
                    "sent": "I guess I'm just not involved in any of that work.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is another one.",
                    "label": 0
                },
                {
                    "sent": "From nation Goodman, an hold in which compares support vector machines with a sparse form of Gaussian process classifier.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing here is that on a whole bunch of different kind of standard issue CIS datasets you get error rates for the SVM and Gaussian process and then a sparse version of the Gaussian process, which is actually an approximation to the full Gaussian process by the way.",
                    "label": 0
                },
                {
                    "sent": "But it's a lot faster and it's a hell of a lot sparser.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you look if you compare these columns here, if you look at the number of support vectors which is support vector machines are supposed to be sparse.",
                    "label": 0
                },
                {
                    "sent": "Here's the number of support vectors for these different problems here, as they sort of roughly comparable quantity, which is the number of.",
                    "label": 0
                },
                {
                    "sent": "Basis points for the sparse Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know, sometimes you get.",
                    "label": 0
                },
                {
                    "sent": "Very comparable performance with two rather than, you know, two rather than 122 or something like that.",
                    "label": 0
                },
                {
                    "sent": "And often this one does actually better than the SVM and this is using exactly the same kernel.",
                    "label": 0
                },
                {
                    "sent": "By the way, you could do probably better if you did a lot of kernel learning in the Gaussian process, which is certainly very, very doable, well understood.",
                    "label": 0
                },
                {
                    "sent": "So if you want to hear me rant more about SPMS and Gaussian processes, I'm actually going to speak more about that in the preference Learning Workshop.",
                    "label": 0
                },
                {
                    "sent": "Make some more enemies there.",
                    "label": 0
                },
                {
                    "sent": "OK, and in fact I think this stuff is promising, but we're actually pursuing a much more systematic, large scale comparison because people are so damn interested in classification.",
                    "label": 0
                },
                {
                    "sent": "So with Alex Davies, my student were planning to kind of really flesh out this comparison a little more.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I wanted to talk briefly about covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "Modeling covariance matrix is a different kind of structure that we can model here.",
                    "label": 0
                },
                {
                    "sent": "I think we do have.",
                    "label": 0
                },
                {
                    "sent": "Actually this is maybe there is a trend.",
                    "label": 0
                },
                {
                    "sent": "There is a bit of recent work where I think we have competitive results with set of the art methods.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why, why would you be interested in modeling covariance matrices?",
                    "label": 0
                },
                {
                    "sent": "Well, the problem of modeling covariance matrices as they change as a function of time or other input variables.",
                    "label": 1
                },
                {
                    "sent": "Is actually interesting to a lot of people, and they tend to be people in econometrics where they're looking at relationships between a bunch of different variables that change overtime.",
                    "label": 1
                },
                {
                    "sent": "And the models that are commonly used are things like multivariate garch and multivarious lukasik volatility models.",
                    "label": 0
                },
                {
                    "sent": "But it turns out they're not all that flexible and actually they don't scale very well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with Andrew Wilson, we've been doing some work on generalized.",
                    "label": 0
                },
                {
                    "sent": "We share process for covariance modeling, where we're modeling time and spatially varying covariance matrices, and the only difficulty in doing that is that these covariance matrices have to be symmetric and positive definite.",
                    "label": 1
                },
                {
                    "sent": "As you vary the covariate and there is a well known and very simple construction for such objects, which is if you take a bunch of say, normally distributed vectors.",
                    "label": 0
                },
                {
                    "sent": "You, I and you take the sum of the outer products of these normal vectors.",
                    "label": 0
                },
                {
                    "sent": "Then the matrix that you get is symmetric and positive definite.",
                    "label": 0
                },
                {
                    "sent": "As long as you take enough of these compared to the dimensionality of the matrix.",
                    "label": 0
                },
                {
                    "sent": "And in fact it has a wishard distribution.",
                    "label": 0
                },
                {
                    "sent": "And there's also.",
                    "label": 1
                },
                {
                    "sent": "This is also closely related to copula processes.",
                    "label": 0
                },
                {
                    "sent": "I'm actually pretty excited about copulas.",
                    "label": 0
                },
                {
                    "sent": "We had a whole workshop yesterday which was lots of fun.",
                    "label": 0
                },
                {
                    "sent": "So if you want to follow up on that, ask me.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so now we have a basic nonparametric model for covariance matrices as a function of time or other deep.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The variables and we can use them for.",
                    "label": 0
                },
                {
                    "sent": "Modeling multivariate, changing things like some of these econometric data.",
                    "label": 0
                },
                {
                    "sent": "And compare them to methods that other people consider to be state of the art and get good results in terms of predictive likelihood.",
                    "label": 0
                },
                {
                    "sent": "So here the Bayesian nonparametric thing seems to be the one in bold.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'll skip over some of these.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me talk about one more idea and then leave some time for discussion.",
                    "label": 0
                },
                {
                    "sent": "So here's the another idea that kind of combines Gaussian processes with neural network like structure.",
                    "label": 0
                },
                {
                    "sent": "They don't seem.",
                    "label": 0
                },
                {
                    "sent": "We don't seem to be able to completely shed neural networks in our community.",
                    "label": 0
                },
                {
                    "sent": "They keep coming back coming back to haunt us and so here they are again.",
                    "label": 0
                },
                {
                    "sent": "So here is this idea.",
                    "label": 0
                },
                {
                    "sent": "It's basically imagine you're trying to do multivariate prediction.",
                    "label": 0
                },
                {
                    "sent": "Let's take regression.",
                    "label": 0
                },
                {
                    "sent": "You start with a few Gaussian processes, functions of X, and then we're going to combine them.",
                    "label": 0
                },
                {
                    "sent": "Through weights to get our multivariate output.",
                    "label": 0
                },
                {
                    "sent": "So it's a lot like kind of factor analysis decomposition.",
                    "label": 0
                },
                {
                    "sent": "Or, you know, Gaussian process latent variable models.",
                    "label": 0
                },
                {
                    "sent": "Except these things are observed.",
                    "label": 0
                },
                {
                    "sent": "The axes are observed and we're trying to predict why, and then the interesting thing about this is that you get a lot of flexibility by making the weights themselves be functions of the input.",
                    "label": 0
                },
                {
                    "sent": "So that's why we have these colorful things here.",
                    "label": 0
                },
                {
                    "sent": "Imagine each of these objects included the inputs here, and the weights are functions of X are covariate vector.",
                    "label": 0
                },
                {
                    "sent": "So the form of the model is written down here at the bottom.",
                    "label": 0
                },
                {
                    "sent": "And it ends up being.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, it ends up having a lot of nice properties like input dependent, correlation structure modeling, nonstationarity, have heavy tails, scaling well, etc.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So skip over the results.",
                    "label": 0
                },
                {
                    "sent": "We can use it to model correlated outputs nicely.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, that we see, even though this is now in this supervised setting, we seem to be able to get good results compared to other methods.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'll summarize and then I'll have one slide of discussion points.",
                    "label": 0
                },
                {
                    "sent": "So the summary is probabilistic, modeling Bayesian inference are two sides of the same coin.",
                    "label": 1
                },
                {
                    "sent": "I actually have started liking to use the term inverse probability instead of Bayes.",
                    "label": 0
                },
                {
                    "sent": "Bayesian sounds a bit like I'm following some some leader whose portrait is on my wall or something.",
                    "label": 0
                },
                {
                    "sent": "Where is inverse probability?",
                    "label": 0
                },
                {
                    "sent": "It's what it used to be called in the 19th century I think.",
                    "label": 0
                },
                {
                    "sent": "And before and it's sort of just the notes.",
                    "label": 0
                },
                {
                    "sent": "The idea that.",
                    "label": 0
                },
                {
                    "sent": "You write down a probabilistic model and then you apply the rules of probability to infer things in the inverse direction about your model given the data.",
                    "label": 1
                },
                {
                    "sent": "Now, Bayesian machine learning treats learning is a probabilistic inference problem.",
                    "label": 0
                },
                {
                    "sent": "These things seem to work well when the models are flexible enough to capture relevant properties of data.",
                    "label": 1
                },
                {
                    "sent": "I tried to motivate nonparametric Bayesian methods and I talked about the sort of three examples here.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here is a discussion and critique side which we can dwell on if we want.",
                    "label": 0
                },
                {
                    "sent": "If we have time.",
                    "label": 0
                },
                {
                    "sent": "OK, now all this stuff seems very beautiful, but the Bayesian coherence arguments which you're probably familiar with are hard to defend if we have to do approximate inference right?",
                    "label": 1
                },
                {
                    "sent": "I said oh, if we just stick to the sum rule in the product rule, then we're fine.",
                    "label": 0
                },
                {
                    "sent": "But the problem is when we do approximate inference, we're not just sticking to the sum rule in the product rule, whether we're doing MCMC or variational or EP or whatever you want to call it.",
                    "label": 0
                },
                {
                    "sent": "So there is an element of hope.",
                    "label": 0
                },
                {
                    "sent": "We still hope to have good properties when when we do this.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "How do we deal with this?",
                    "label": 0
                },
                {
                    "sent": "That's one question I have some thoughts.",
                    "label": 0
                },
                {
                    "sent": "You know, if we might be able to, for example, stand on the shoulders of people who have developed frequentist guarantees for Bayesian procedures and say I want it to be hardcore fundamentalist Bayesian.",
                    "label": 0
                },
                {
                    "sent": "My computer was not big enough, so I had to do approximate inference, but yet I'm comfortable communicating this to the rest of the world because I can show that my procedure has certain nice frequentist properties.",
                    "label": 0
                },
                {
                    "sent": "Even though I did approximate inference, that's the key thing.",
                    "label": 0
                },
                {
                    "sent": "If I can show that, then that's nice.",
                    "label": 0
                },
                {
                    "sent": "So some methods like PAC Bayes are actually nice ways of dealing with that.",
                    "label": 0
                },
                {
                    "sent": "Now the Bayesian framework.",
                    "label": 0
                },
                {
                    "sent": "The more you think about it, the more it forces you to do modeling, which can be very cumbersome and often there are some really simple non Bayesian algorithms that do very well like witness something like spectral clustering or locally linear embedding.",
                    "label": 0
                },
                {
                    "sent": "Or for example Classical 2 sample tests.",
                    "label": 0
                },
                {
                    "sent": "These are really easy things to do and it would be really hard to come up with a Bayesian analogue of that.",
                    "label": 0
                },
                {
                    "sent": "Is Chris Holmes here?",
                    "label": 0
                },
                {
                    "sent": "So he's come up with a Bayesian analogue of two sample tests.",
                    "label": 0
                },
                {
                    "sent": "I've done it as well, but you know they're way more complicated than the frequentist thing.",
                    "label": 0
                },
                {
                    "sent": "Very interesting thing to try to do, But anyway.",
                    "label": 0
                },
                {
                    "sent": "Now structure discovery this whole thing that Oh my Bayesian method is going to discover the right number of hidden units or the right number of clusters.",
                    "label": 1
                },
                {
                    "sent": "For example, the right number of clusters with additional process mixture.",
                    "label": 1
                },
                {
                    "sent": "Well, that seems fraught with dangerous assumptions.",
                    "label": 0
                },
                {
                    "sent": "In particular, I think discovering the number of clusters with the mixture model is is probably complete rubbish because you know what we intuitively think of as clusters or structure in the data correspond to.",
                    "label": 0
                },
                {
                    "sent": "Don't correspond to nice parametric families of distributions, so if I fit a mixture of Gaussians to two bumps of data, if those bumps have some wiggles and funny tales, I might get my dear sleep process mixture to end up with 300 components because it needs to model all that stuff eventually.",
                    "label": 0
                },
                {
                    "sent": "OK, so I can't interpret that as the number of clusters.",
                    "label": 0
                },
                {
                    "sent": "Mixture models are not clustering methods guys.",
                    "label": 0
                },
                {
                    "sent": "OK, I even called them clustering methods.",
                    "label": 0
                },
                {
                    "sent": "But you know that's to undergraduates 'cause it's nice and intuitive.",
                    "label": 0
                },
                {
                    "sent": "But we are adults here.",
                    "label": 0
                },
                {
                    "sent": "Mixture models are not clustering methods.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, nonparametric methods are memory based because they have this wide information channel from past data to future predictions.",
                    "label": 0
                },
                {
                    "sent": "They are often defined in terms of remembering previous data, and that's often very computationally expensive.",
                    "label": 1
                },
                {
                    "sent": "So that's another downer.",
                    "label": 0
                },
                {
                    "sent": "Well, it's hard to be a subjective Bayesian.",
                    "label": 0
                },
                {
                    "sent": "And then develop general purpose Bayesian models that somebody can download from your website and run them on their problems.",
                    "label": 0
                },
                {
                    "sent": "OK, that's another downer.",
                    "label": 0
                },
                {
                    "sent": "I think for us.",
                    "label": 0
                },
                {
                    "sent": "Because OK, let me just try to explain why.",
                    "label": 0
                },
                {
                    "sent": "Let's say I'm a subjective Bayesian.",
                    "label": 0
                },
                {
                    "sent": "I want to build a really good prior into my model, but.",
                    "label": 0
                },
                {
                    "sent": "You know, I have to know what I'm trying to model to do that.",
                    "label": 0
                },
                {
                    "sent": "I can't write general purpose code with some sort of like the prior that will work for everybody.",
                    "label": 0
                },
                {
                    "sent": "I mean, I could try to do that.",
                    "label": 0
                },
                {
                    "sent": "You know there are people who try to do universal type things, but I think it's very hard to do that and still kind of.",
                    "label": 0
                },
                {
                    "sent": "Say honestly that you're doing good subjective bayesianism.",
                    "label": 1
                },
                {
                    "sent": "Here is another one that a lot of people get hung up on exchange ability assumptions well, they constrain the form of models that we are allowed to have.",
                    "label": 0
                },
                {
                    "sent": "So for example, as was mentioned in the tutorial, most clustering models that have preferential attachment are not actually exchangeable.",
                    "label": 0
                },
                {
                    "sent": "Only if you do preferential attachment in a particular way, we get an exchangeable distribution.",
                    "label": 0
                },
                {
                    "sent": "And so you know, people are uncomfortable with that, understandably.",
                    "label": 1
                },
                {
                    "sent": "Then finally, large parametric models often perform very well in practice.",
                    "label": 0
                },
                {
                    "sent": "So is nonparametrics just a question of elegance?",
                    "label": 0
                },
                {
                    "sent": "OK, I'll stop there.",
                    "label": 0
                }
            ]
        }
    }
}