{
    "id": "5tphyyegycvcpkcvsskuqtx36izp5bf6",
    "title": "Robust Matching and Recognition using Context-Dependent Kernels",
    "info": {
        "author": [
            "Hichem Sahbi, CNRS - LTCI UMR 5141 Telecom ParisTech"
        ],
        "published": "Aug. 6, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Computer Vision->Object Recognition"
        ]
    },
    "url": "http://videolectures.net/icml08_sahbi_rmr/",
    "segmentation": [
        [
            "OK, good morning everybody.",
            "My name is Shamsa, behemoths, energy searcher from telecom Paris tech.",
            "Today I'm going to talk about robust matching and recognition using what we call by context dependent similarity kernel.",
            "This work is done jointly with Jean Yves Audible and one OK even who are former colleagues from the connection on deposit should see and genre is well is our PhD student from the same institution."
        ],
        [
            "So I will start my presentation with some issues about kernels and the main contributions.",
            "Afterward, I will give a quick reminder on subset kernels.",
            "Then I will follow my presentation with the main contribution, which is the design of context dependent similarity kernels and I will collaborate.",
            "All the theoretical statements by experiments, mainly on object recognition, image retrieval in the machine translation I will provide with the take home message and also."
        ],
        [
            "So the possible extensions."
        ],
        [
            "Alright, I'm not going to tell you what is a kernel, but I would like just to mention two big categories of kernels which are holistic San subset kernels.",
            "So initially kernels were designed in order to handle fixed length and order data, and these kinds of kernel are called holistic.",
            "Which means that, for instance given to images, what we do is we try to extract set of characteristics on a regular grid and we will map each characteristic into into a feature vector.",
            "So two images will be mapped into a fixed length feature vector and ordered because the order is always consistent.",
            "Then we will define similarity between 2 images as any decreasing function of the distance.",
            "What's happen if in practice we don't be such techniques that do not work very well?",
            "Because for instance, if we try to capture the local information present into images, then this characteristics are present in some locations like the interest points and corners and so on.",
            "So what we need is to extract this characteristic using some filters like the Harris filter or the SIFT or the gist and so on.",
            "The problem with this is given 2 images will not end up with the same number of characteristics, so it will change from one image to another and the order will not be the same.",
            "It's not consistent.",
            "Information can affect the order in which we can parse all the interest points and the same thing happened if we try to compare two sequence of reduce.",
            "If even if the order is always the same, but the problem is we might not have the same number of frames that we can compare it.",
            "That's why it's important to consider and fix it.",
            "Lengthen under an order data like graft, resent response by using the subset kernels and these subset kernels rely on what we call by minor or label or local kernels."
        ],
        [
            "Alright, so the technique which handle subset kernels or subset kernels can be categorized into two categories or two groups of kernels.",
            "We have the alignment kernel, so these require a preliminary step of aligning the characteristics using some matching techniques and those which are order and lengthen sensitive, which means that whatever the order and the length of the data, we can design kernel which are which can be expressed without any preliminary step of alignments.",
            "And usually they rely on some.",
            "Statistical information like the crib, labor, labor diversions, and so on, so I'm not going to detail all these techniques, but I would like just to mention that the proposed techniques can be fit or fit into this class."
        ],
        [
            "Alright, so these are the main contribution, so I will introduce a way to design a little Colonel or major Colonel using variational framework and this will allow us to define what we call via context dependent similarity kernel and this kernel is a fixed point of an energy.",
            "This one proposed here and I will show some properties, mainly the positive definiteness of the kernel and also some some properties about the convergence as an application or will show mainly object matching and recognition.",
            "I will show you an extension.",
            "In machine translation and also a possible extension to video retrieval."
        ],
        [
            "OK, let's talk."
        ],
        [
            "With the subject kernel.",
            "So as I said, so given a database of images, given 2 images characterized by set of interest points.",
            "So how can we define the similarity function which is a decreasing?",
            "Which is decreasing in terms of the distance.",
            "So you consider calligraphic X as the union of all possible images of the world.",
            "Each image is a set of interest point and given two subsets from the input space.",
            "So we define a similarity between the two subsets as the double sum over all the pairs taken from the two subset SP&SQVA little kernel K. So one property is one day, little kernel K is positive definite.",
            "Then we end up with a positive definite kernel K. Which means that there exists a mapping from the input space into ALKHA space such that the kernel K can be written as a dot pro."
        ],
        [
            "So what happen?"
        ],
        [
            "With the minor kernel is some."
        ],
        [
            "Times we can have some problem.",
            "We have a deficiency which means for instance if we have two subsets to compare like this string.",
            "Hi Sir and another one, Sir.",
            "So one straightforward way to compare the two strings via little kernel is to define as little kernel indication function which is equal to 1 if the two characters are similar and 0 otherwise.",
            "So in this case between this I and this one we have one and this one and this one we have one but.",
            "What we want is to have a higher similarity between this I and this one because they have the same context.",
            "While this is narrative between this one and this one should be small because they share a different context.",
            "So the similar we want to define similarity not only in terms of the intrinsic properties of the elements or the features, but also in terms of their context.",
            "The same thing happened here.",
            "If we take two grids from 2 images, one of them is a noise version of other other one.",
            "So globally these two are the same, but there is some noise which is added in the color.",
            "And if we try to define or display the match between the two grids, so by maximizing the value of the kernel.",
            "So we have a problem here.",
            "We have some outlier in terms of the match.",
            "When we use only the intrinsic properties of the features, only the color.",
            "What we want is something different like that which match two elements if they share the same intrinsic color but or the same intrinsic property, but also they share the same context because this will give a more robust way to define the similarity."
        ],
        [
            "OK."
        ],
        [
            "So let's see how it works.",
            "So the design consists into defining this little kernel K between any pair XI XJ taken from 2 subset SP&SQ as a probability of match between the two points as XI and XG.",
            "So we have redefined version of framework which is the in which the.",
            "Unknown is the kernel K. So here we have K. So it contains a Fidelity term and a context or neighborhood term, and the regularization term.",
            "So the first one tells us that the kernel takes high value between a pair XIXG from 2 subsets if the distance or the intrinsic distance between the two points is small.",
            "So here for instance in case of shift in image in vision we consider 128 coefficients of the shift.",
            "And we start with the third term.",
            "I'll continue with the term which tell us that the similarity between a pair XIXG is high if all the neighboring points have a value of the kernel high 2.",
            "So what is the distribution of the random variables on the path Capital X?",
            "Sorry.",
            "What is the distribution uncrackable actor on the radio that started this?",
            "This is just this one variable is taken from the input space.",
            "Just take all the possible like if you process contours, take all the contours of the world and put them into an input space and this is random variable is taken from this input space.",
            "It stands for all possible.",
            "Points that you might take from the input space, and this will characterize the 2D location of the point in the contour and also the feature vector that you can see there, like sift or like the shape, context feature and so on.",
            "OK, so the third term tells us that the kernel takes high value if the neighboring points.",
            "The Paris Imdg have high value of their kernel.",
            "And this is weighted by a parameter Alpha and we see that this is important factor in reducing the outlier in the match.",
            "And the second term here is a a pre term which tell us that without any information about the 1st and the 3rd terms we need to consider this probability as a flat distribution, which means that we need to maximize the entropy and to minimize so to minimize the negative one Sharpie and write this by a parameter beta.",
            "So we minimize this problem and under the constraint that K is a probability distribution which means that it belongs to zero and one and it doubles sum to one.",
            "So this Colonel here is a P kernel on SP.",
            "I'm ask you if you want to make it appear kernel on X time X, so we just add a double sum here over all possible pair of subsets here and here 2."
        ],
        [
            "OK, so solving this problem is not really complicated, so we showed in the paper that the solution can be written as this function that we call a context dependent similarity kernel which contain this term, which is just the just the Gaussian kernel time.",
            "Another term, which is what we call by context dependent term.",
            "So inside this term we have this double sum which tells us that.",
            "We have a high similarity between 2.6 IXG if they share the same intrinsic properties and also all the points which are close to despair should have high similarity between their underlying intrinsic and also context information and this will correspond to a Gibbs distribution."
        ],
        [
            "So here, as I showed in the immunization problem, we have this Alpha."
        ],
        [
            "Parameter which appears here."
        ],
        [
            "And we have the beta, which appears close to the regularization terms."
        ],
        [
            "Plans to scale of the Goshen kernel."
        ],
        [
            "This is the effect of increasing the parameter Alpha in the kernel in the CDK Colonel.",
            "So here we have the results of matching between two sets of points from 2 curves when we use a context free Colonel like anyone, the polynomial, Goshen or linear, we have the matching result by maximizing the value of the kernel between one point and all possible points in the second curve, we see that we have a lot of outliers.",
            "As we increase the parameter Alpha, we reduce more.",
            "We reduce the outliers and at the end we for a given value of Alpha.",
            "We have only good matches.",
            "This figure in the left hand side show the distribution of the value of the kernel from one point here, which is fixed to all possible other points in the second image.",
            "And when we take the highest valued correspond to the right match, which is wrong here when you use a context free kernel, if we use a context dependent kernel then we see that the highest value changes location and it corresponds to the right match."
        ],
        [
            "OK, before I stating the result about the Mercer condition, I would just like to remind one property which tells us that the sum of the product of 2 Mercer kernels are canal and the exponential of any Mercer kernel is also Mercer kernel.",
            "So here is our results which tell us that if the function which appears into the expanding second exponential in the CDK kernel, this one if it can be written as a product of 2G functions or the first one measure.",
            "Take a high value 1.6 K is closer point XI, which means that K is in the context of XI and the same thing here.",
            "For the second curve, which tell us that it takes it a high value if the point XL is in the context of HG.",
            "And if you consider K0 the initial value of the kernel as a positive definite kernel, then any kernel Katie estimated at any iteration is positive definite too."
        ],
        [
            "I'm not going to give a lot of detail of the of the proof.",
            "You can find it in the paper.",
            "I would like just to give a sketch, so initially the kernel is part definitions positive definite.",
            "By induction we assume that the kernel at iteration T -- 1 is Mercer kernel, which means that we can write it as a dot product in some Hilbert space.",
            "So now the sufficient condition will be to show that the kernel inside the second exponential is also massive Colonel, so we can find the detail about how to show this in the paper.",
            "So by the closure of the exponential and the product we can show.",
            "That Katie is also measure control.",
            "So if."
        ],
        [
            "This is a more circle then exponential of Mercer.",
            "Kernel will remain in motion and the product since this one is a Goshen.",
            "So it's also my circle and the product will remain."
        ],
        [
            "Amber circle."
        ],
        [
            "What about the convergence?",
            "So this Colonel is estimated iteratively to recursive Colonel, so we can we need to show that it converges to some something which is fixed.",
            "So this G function which appears into the form of the kernel here is considered to be a function which is decreasing in terms of the kleidion distance.",
            "For instance, we take 2D curves, so we need to consider that is G function is decreasing when one the KSK.",
            "Is far from XI, so we can consider this function between zero and one and.",
            "We try to bound first the second term of this exponential by A and if we take the upper bound of the first term in the exponential to be less than B, then."
        ],
        [
            "For a given L which is equal to this expression.",
            "So to be Alpha over beta exponential this term then we can show that when Alpha is less than beta over to a function or the function of the kernel is construction so it is convergent.",
            "So it goes to 01.",
            "T goes to Infinity.",
            "So if you want the."
        ],
        [
            "If you can find it in the paper, I'm not."
        ],
        [
            "To give more details about."
        ],
        [
            "OK, so let's see how it works in practice.",
            "So the experiments were conducted on the database.",
            "We took ten class attend classes and we have handled Image Park class for training and hundred image for testing.",
            "And the Swedish is a database of leaf contours which contain points to the points of leaves and we took so we have 15 classes and 25 for training and 50 for testing.",
            "So the comparison here involves the comparison of the subset kernel when using a context free kernel.",
            "As KO here and context dependent, Colonel Katie here, 40 bigger than one of the bigger one.",
            "OK, so we have here two kinds, two initializations for KO we have linear and polynomial and we see here the results for the amnesty using the N fold cross validation and is equal to 5.",
            "4K zero.",
            "We have the results which correspond to the context free kernel, which is the linear order polynomial.",
            "As we increase the iteration number.",
            "So we estimate a context dependent kernel and we see how the results decreases.",
            "So the error decrease and it's constant at some point and this collaborate.",
            "The fact that the kernel is convergent.",
            "So we see how the results decreases when we use the context dependent kernel and we have the same behavior for the Swedish database and we.",
            "With three iteration here we see here the results when using the context free kernel with linear and polynomial and context dependent kernel.",
            "After two iterations."
        ],
        [
            "OK, I'm going to give in conclusion then I will explain the extension on machine translation.",
            "How we can handle machine translation.",
            "So the main take home message here is that similarity.",
            "Kernel design is not only interested, you do not depends only on the information inside every point that we consider into a subset.",
            "But it depends also in the context surrounding the point.",
            "So the similarity would not depends on one point, but it depends on the cloud of points.",
            "So context dependent kernels show better matching results with respect to context.",
            "Kernels as shown in experiments, even though we're trying also some experiments with other kernels which are currently currently under investigation.",
            "So this approach is model free approach.",
            "We don't.",
            "We do not need to estimate any matrix similarity metrics or rotation or any kinds of parameters for doing matching and estimating the similarity, and I think I didn't explain in the presentation this kernel is rotation, translation and scale invariant.",
            "And also tolerant to some deformations in the context of matching shapes as an extension, I will show quickly how we can extend this kernel for machine translation and also in video matching and chivo."
        ],
        [
            "So let's consider in machine translation the translation of an English sentence.",
            "He may go into to possibilities in French.",
            "One of them is wrong and the other one is correct.",
            "So one possibility to estimate to take the right translation is to estimate the value of the kernel between S1 and S2 and S1 and S3 and take take the sentence which maximizes similarity.",
            "So the subset kernel here is taken as the sum using the little kernel K between all possible.",
            "Pairs of phrase pairs of words.",
            "And here I show only the the pair which are either simple or either make some ambiguities.",
            "So here we have an ambiguity between May, which might be in French the month or might be the ability.",
            "So the way to estimate the two possibilities here is to consider a dictionary which will give us a similarity as the frequency or some joint statistics in a by tech or something like that.",
            "So if we have the similarity between May and the month bigger than May and the ability, then we will end up with the wrong French translation, which is this one.",
            "So why not to consider similarity using context?",
            "Or phrase based translation.",
            "So we consider that the similarity between words as similarity between the words and also the similarity between the context surrounding the words.",
            "So here we use the context dependent kernel which estimate the similarity between may, which is maybe and we have two possibilities here.",
            "May as a month or the ability.",
            "And here we have the expression of the context dependent kernel which has the first term which depends on the dictionary which has no contact information on it and the second one is dependent on the context and we see here.",
            "How we can make the similarity between May and the month less than the similarity between May and the ability in this context and we see here that we have this G function that I showed previously, which in fact estimate the joint or the Co occurrence between the pronounce and the and the month and the Co occurrence between the pronounce and the ability and we see here if the we know that in French or.",
            "Even in English that the Co occurrence between ill and the month is less than the coherence between the month and the ability, and sorry between the ill and the ability.",
            "And this will makes.",
            "This will make this term vanish ING or going quickly to zero in the case we consider a similarity between ill and the month.",
            "So if the value of the kernel which is estimated at the convergence stage between May and the month is less than the similarity between May and the ability, then we will end up with the correct French translation."
        ],
        [
            "Alright, so let's see also how it works for.",
            "Do you think that this is just going to wrap up in two or three words?",
            "I'm finishing OK, just one word about video retrieval.",
            "If we try to compare two sequence of videos, if they don't have the same length and we use a subset kernel and we will estimate the similarity between any pair of frames, we can do it either using context free kernel by estimating the kernel as a decreasing function of the intrinsic properties of the two frames, or we consider all the neighboring frames and this via at temporal coherence between.",
            "The two videos.",
            "So we the similarity will be dependent on the two frames and on the neighboring frames, so this will end up with the with a better video retrieval performance as currently done in our project.",
            "OK, thank you.",
            "Who is it?",
            "Quick question.",
            "Sorry.",
            "Statement.",
            "That's where the appearance will be discouraged, right?",
            "Because yeah, yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "My name is Shamsa, behemoths, energy searcher from telecom Paris tech.",
                    "label": 0
                },
                {
                    "sent": "Today I'm going to talk about robust matching and recognition using what we call by context dependent similarity kernel.",
                    "label": 1
                },
                {
                    "sent": "This work is done jointly with Jean Yves Audible and one OK even who are former colleagues from the connection on deposit should see and genre is well is our PhD student from the same institution.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will start my presentation with some issues about kernels and the main contributions.",
                    "label": 0
                },
                {
                    "sent": "Afterward, I will give a quick reminder on subset kernels.",
                    "label": 1
                },
                {
                    "sent": "Then I will follow my presentation with the main contribution, which is the design of context dependent similarity kernels and I will collaborate.",
                    "label": 0
                },
                {
                    "sent": "All the theoretical statements by experiments, mainly on object recognition, image retrieval in the machine translation I will provide with the take home message and also.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the possible extensions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, I'm not going to tell you what is a kernel, but I would like just to mention two big categories of kernels which are holistic San subset kernels.",
                    "label": 0
                },
                {
                    "sent": "So initially kernels were designed in order to handle fixed length and order data, and these kinds of kernel are called holistic.",
                    "label": 1
                },
                {
                    "sent": "Which means that, for instance given to images, what we do is we try to extract set of characteristics on a regular grid and we will map each characteristic into into a feature vector.",
                    "label": 0
                },
                {
                    "sent": "So two images will be mapped into a fixed length feature vector and ordered because the order is always consistent.",
                    "label": 0
                },
                {
                    "sent": "Then we will define similarity between 2 images as any decreasing function of the distance.",
                    "label": 0
                },
                {
                    "sent": "What's happen if in practice we don't be such techniques that do not work very well?",
                    "label": 0
                },
                {
                    "sent": "Because for instance, if we try to capture the local information present into images, then this characteristics are present in some locations like the interest points and corners and so on.",
                    "label": 0
                },
                {
                    "sent": "So what we need is to extract this characteristic using some filters like the Harris filter or the SIFT or the gist and so on.",
                    "label": 0
                },
                {
                    "sent": "The problem with this is given 2 images will not end up with the same number of characteristics, so it will change from one image to another and the order will not be the same.",
                    "label": 0
                },
                {
                    "sent": "It's not consistent.",
                    "label": 0
                },
                {
                    "sent": "Information can affect the order in which we can parse all the interest points and the same thing happened if we try to compare two sequence of reduce.",
                    "label": 0
                },
                {
                    "sent": "If even if the order is always the same, but the problem is we might not have the same number of frames that we can compare it.",
                    "label": 0
                },
                {
                    "sent": "That's why it's important to consider and fix it.",
                    "label": 0
                },
                {
                    "sent": "Lengthen under an order data like graft, resent response by using the subset kernels and these subset kernels rely on what we call by minor or label or local kernels.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the technique which handle subset kernels or subset kernels can be categorized into two categories or two groups of kernels.",
                    "label": 0
                },
                {
                    "sent": "We have the alignment kernel, so these require a preliminary step of aligning the characteristics using some matching techniques and those which are order and lengthen sensitive, which means that whatever the order and the length of the data, we can design kernel which are which can be expressed without any preliminary step of alignments.",
                    "label": 0
                },
                {
                    "sent": "And usually they rely on some.",
                    "label": 0
                },
                {
                    "sent": "Statistical information like the crib, labor, labor diversions, and so on, so I'm not going to detail all these techniques, but I would like just to mention that the proposed techniques can be fit or fit into this class.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so these are the main contribution, so I will introduce a way to design a little Colonel or major Colonel using variational framework and this will allow us to define what we call via context dependent similarity kernel and this kernel is a fixed point of an energy.",
                    "label": 1
                },
                {
                    "sent": "This one proposed here and I will show some properties, mainly the positive definiteness of the kernel and also some some properties about the convergence as an application or will show mainly object matching and recognition.",
                    "label": 0
                },
                {
                    "sent": "I will show you an extension.",
                    "label": 1
                },
                {
                    "sent": "In machine translation and also a possible extension to video retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's talk.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With the subject kernel.",
                    "label": 0
                },
                {
                    "sent": "So as I said, so given a database of images, given 2 images characterized by set of interest points.",
                    "label": 1
                },
                {
                    "sent": "So how can we define the similarity function which is a decreasing?",
                    "label": 1
                },
                {
                    "sent": "Which is decreasing in terms of the distance.",
                    "label": 0
                },
                {
                    "sent": "So you consider calligraphic X as the union of all possible images of the world.",
                    "label": 0
                },
                {
                    "sent": "Each image is a set of interest point and given two subsets from the input space.",
                    "label": 0
                },
                {
                    "sent": "So we define a similarity between the two subsets as the double sum over all the pairs taken from the two subset SP&SQVA little kernel K. So one property is one day, little kernel K is positive definite.",
                    "label": 0
                },
                {
                    "sent": "Then we end up with a positive definite kernel K. Which means that there exists a mapping from the input space into ALKHA space such that the kernel K can be written as a dot pro.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what happen?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the minor kernel is some.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Times we can have some problem.",
                    "label": 0
                },
                {
                    "sent": "We have a deficiency which means for instance if we have two subsets to compare like this string.",
                    "label": 0
                },
                {
                    "sent": "Hi Sir and another one, Sir.",
                    "label": 0
                },
                {
                    "sent": "So one straightforward way to compare the two strings via little kernel is to define as little kernel indication function which is equal to 1 if the two characters are similar and 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "So in this case between this I and this one we have one and this one and this one we have one but.",
                    "label": 0
                },
                {
                    "sent": "What we want is to have a higher similarity between this I and this one because they have the same context.",
                    "label": 0
                },
                {
                    "sent": "While this is narrative between this one and this one should be small because they share a different context.",
                    "label": 0
                },
                {
                    "sent": "So the similar we want to define similarity not only in terms of the intrinsic properties of the elements or the features, but also in terms of their context.",
                    "label": 0
                },
                {
                    "sent": "The same thing happened here.",
                    "label": 0
                },
                {
                    "sent": "If we take two grids from 2 images, one of them is a noise version of other other one.",
                    "label": 0
                },
                {
                    "sent": "So globally these two are the same, but there is some noise which is added in the color.",
                    "label": 0
                },
                {
                    "sent": "And if we try to define or display the match between the two grids, so by maximizing the value of the kernel.",
                    "label": 0
                },
                {
                    "sent": "So we have a problem here.",
                    "label": 0
                },
                {
                    "sent": "We have some outlier in terms of the match.",
                    "label": 0
                },
                {
                    "sent": "When we use only the intrinsic properties of the features, only the color.",
                    "label": 0
                },
                {
                    "sent": "What we want is something different like that which match two elements if they share the same intrinsic color but or the same intrinsic property, but also they share the same context because this will give a more robust way to define the similarity.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see how it works.",
                    "label": 0
                },
                {
                    "sent": "So the design consists into defining this little kernel K between any pair XI XJ taken from 2 subset SP&SQ as a probability of match between the two points as XI and XG.",
                    "label": 0
                },
                {
                    "sent": "So we have redefined version of framework which is the in which the.",
                    "label": 0
                },
                {
                    "sent": "Unknown is the kernel K. So here we have K. So it contains a Fidelity term and a context or neighborhood term, and the regularization term.",
                    "label": 0
                },
                {
                    "sent": "So the first one tells us that the kernel takes high value between a pair XIXG from 2 subsets if the distance or the intrinsic distance between the two points is small.",
                    "label": 0
                },
                {
                    "sent": "So here for instance in case of shift in image in vision we consider 128 coefficients of the shift.",
                    "label": 0
                },
                {
                    "sent": "And we start with the third term.",
                    "label": 0
                },
                {
                    "sent": "I'll continue with the term which tell us that the similarity between a pair XIXG is high if all the neighboring points have a value of the kernel high 2.",
                    "label": 0
                },
                {
                    "sent": "So what is the distribution of the random variables on the path Capital X?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "What is the distribution uncrackable actor on the radio that started this?",
                    "label": 0
                },
                {
                    "sent": "This is just this one variable is taken from the input space.",
                    "label": 0
                },
                {
                    "sent": "Just take all the possible like if you process contours, take all the contours of the world and put them into an input space and this is random variable is taken from this input space.",
                    "label": 0
                },
                {
                    "sent": "It stands for all possible.",
                    "label": 0
                },
                {
                    "sent": "Points that you might take from the input space, and this will characterize the 2D location of the point in the contour and also the feature vector that you can see there, like sift or like the shape, context feature and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so the third term tells us that the kernel takes high value if the neighboring points.",
                    "label": 0
                },
                {
                    "sent": "The Paris Imdg have high value of their kernel.",
                    "label": 0
                },
                {
                    "sent": "And this is weighted by a parameter Alpha and we see that this is important factor in reducing the outlier in the match.",
                    "label": 0
                },
                {
                    "sent": "And the second term here is a a pre term which tell us that without any information about the 1st and the 3rd terms we need to consider this probability as a flat distribution, which means that we need to maximize the entropy and to minimize so to minimize the negative one Sharpie and write this by a parameter beta.",
                    "label": 0
                },
                {
                    "sent": "So we minimize this problem and under the constraint that K is a probability distribution which means that it belongs to zero and one and it doubles sum to one.",
                    "label": 0
                },
                {
                    "sent": "So this Colonel here is a P kernel on SP.",
                    "label": 0
                },
                {
                    "sent": "I'm ask you if you want to make it appear kernel on X time X, so we just add a double sum here over all possible pair of subsets here and here 2.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so solving this problem is not really complicated, so we showed in the paper that the solution can be written as this function that we call a context dependent similarity kernel which contain this term, which is just the just the Gaussian kernel time.",
                    "label": 0
                },
                {
                    "sent": "Another term, which is what we call by context dependent term.",
                    "label": 0
                },
                {
                    "sent": "So inside this term we have this double sum which tells us that.",
                    "label": 0
                },
                {
                    "sent": "We have a high similarity between 2.6 IXG if they share the same intrinsic properties and also all the points which are close to despair should have high similarity between their underlying intrinsic and also context information and this will correspond to a Gibbs distribution.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here, as I showed in the immunization problem, we have this Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameter which appears here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have the beta, which appears close to the regularization terms.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plans to scale of the Goshen kernel.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the effect of increasing the parameter Alpha in the kernel in the CDK Colonel.",
                    "label": 0
                },
                {
                    "sent": "So here we have the results of matching between two sets of points from 2 curves when we use a context free Colonel like anyone, the polynomial, Goshen or linear, we have the matching result by maximizing the value of the kernel between one point and all possible points in the second curve, we see that we have a lot of outliers.",
                    "label": 0
                },
                {
                    "sent": "As we increase the parameter Alpha, we reduce more.",
                    "label": 0
                },
                {
                    "sent": "We reduce the outliers and at the end we for a given value of Alpha.",
                    "label": 0
                },
                {
                    "sent": "We have only good matches.",
                    "label": 0
                },
                {
                    "sent": "This figure in the left hand side show the distribution of the value of the kernel from one point here, which is fixed to all possible other points in the second image.",
                    "label": 0
                },
                {
                    "sent": "And when we take the highest valued correspond to the right match, which is wrong here when you use a context free kernel, if we use a context dependent kernel then we see that the highest value changes location and it corresponds to the right match.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, before I stating the result about the Mercer condition, I would just like to remind one property which tells us that the sum of the product of 2 Mercer kernels are canal and the exponential of any Mercer kernel is also Mercer kernel.",
                    "label": 1
                },
                {
                    "sent": "So here is our results which tell us that if the function which appears into the expanding second exponential in the CDK kernel, this one if it can be written as a product of 2G functions or the first one measure.",
                    "label": 0
                },
                {
                    "sent": "Take a high value 1.6 K is closer point XI, which means that K is in the context of XI and the same thing here.",
                    "label": 0
                },
                {
                    "sent": "For the second curve, which tell us that it takes it a high value if the point XL is in the context of HG.",
                    "label": 0
                },
                {
                    "sent": "And if you consider K0 the initial value of the kernel as a positive definite kernel, then any kernel Katie estimated at any iteration is positive definite too.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm not going to give a lot of detail of the of the proof.",
                    "label": 1
                },
                {
                    "sent": "You can find it in the paper.",
                    "label": 0
                },
                {
                    "sent": "I would like just to give a sketch, so initially the kernel is part definitions positive definite.",
                    "label": 0
                },
                {
                    "sent": "By induction we assume that the kernel at iteration T -- 1 is Mercer kernel, which means that we can write it as a dot product in some Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So now the sufficient condition will be to show that the kernel inside the second exponential is also massive Colonel, so we can find the detail about how to show this in the paper.",
                    "label": 1
                },
                {
                    "sent": "So by the closure of the exponential and the product we can show.",
                    "label": 1
                },
                {
                    "sent": "That Katie is also measure control.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a more circle then exponential of Mercer.",
                    "label": 0
                },
                {
                    "sent": "Kernel will remain in motion and the product since this one is a Goshen.",
                    "label": 0
                },
                {
                    "sent": "So it's also my circle and the product will remain.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amber circle.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What about the convergence?",
                    "label": 0
                },
                {
                    "sent": "So this Colonel is estimated iteratively to recursive Colonel, so we can we need to show that it converges to some something which is fixed.",
                    "label": 0
                },
                {
                    "sent": "So this G function which appears into the form of the kernel here is considered to be a function which is decreasing in terms of the kleidion distance.",
                    "label": 0
                },
                {
                    "sent": "For instance, we take 2D curves, so we need to consider that is G function is decreasing when one the KSK.",
                    "label": 0
                },
                {
                    "sent": "Is far from XI, so we can consider this function between zero and one and.",
                    "label": 0
                },
                {
                    "sent": "We try to bound first the second term of this exponential by A and if we take the upper bound of the first term in the exponential to be less than B, then.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For a given L which is equal to this expression.",
                    "label": 0
                },
                {
                    "sent": "So to be Alpha over beta exponential this term then we can show that when Alpha is less than beta over to a function or the function of the kernel is construction so it is convergent.",
                    "label": 0
                },
                {
                    "sent": "So it goes to 01.",
                    "label": 0
                },
                {
                    "sent": "T goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So if you want the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you can find it in the paper, I'm not.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To give more details about.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's see how it works in practice.",
                    "label": 0
                },
                {
                    "sent": "So the experiments were conducted on the database.",
                    "label": 0
                },
                {
                    "sent": "We took ten class attend classes and we have handled Image Park class for training and hundred image for testing.",
                    "label": 0
                },
                {
                    "sent": "And the Swedish is a database of leaf contours which contain points to the points of leaves and we took so we have 15 classes and 25 for training and 50 for testing.",
                    "label": 0
                },
                {
                    "sent": "So the comparison here involves the comparison of the subset kernel when using a context free kernel.",
                    "label": 0
                },
                {
                    "sent": "As KO here and context dependent, Colonel Katie here, 40 bigger than one of the bigger one.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have here two kinds, two initializations for KO we have linear and polynomial and we see here the results for the amnesty using the N fold cross validation and is equal to 5.",
                    "label": 0
                },
                {
                    "sent": "4K zero.",
                    "label": 0
                },
                {
                    "sent": "We have the results which correspond to the context free kernel, which is the linear order polynomial.",
                    "label": 0
                },
                {
                    "sent": "As we increase the iteration number.",
                    "label": 0
                },
                {
                    "sent": "So we estimate a context dependent kernel and we see how the results decreases.",
                    "label": 1
                },
                {
                    "sent": "So the error decrease and it's constant at some point and this collaborate.",
                    "label": 0
                },
                {
                    "sent": "The fact that the kernel is convergent.",
                    "label": 0
                },
                {
                    "sent": "So we see how the results decreases when we use the context dependent kernel and we have the same behavior for the Swedish database and we.",
                    "label": 0
                },
                {
                    "sent": "With three iteration here we see here the results when using the context free kernel with linear and polynomial and context dependent kernel.",
                    "label": 0
                },
                {
                    "sent": "After two iterations.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to give in conclusion then I will explain the extension on machine translation.",
                    "label": 0
                },
                {
                    "sent": "How we can handle machine translation.",
                    "label": 0
                },
                {
                    "sent": "So the main take home message here is that similarity.",
                    "label": 0
                },
                {
                    "sent": "Kernel design is not only interested, you do not depends only on the information inside every point that we consider into a subset.",
                    "label": 0
                },
                {
                    "sent": "But it depends also in the context surrounding the point.",
                    "label": 0
                },
                {
                    "sent": "So the similarity would not depends on one point, but it depends on the cloud of points.",
                    "label": 0
                },
                {
                    "sent": "So context dependent kernels show better matching results with respect to context.",
                    "label": 0
                },
                {
                    "sent": "Kernels as shown in experiments, even though we're trying also some experiments with other kernels which are currently currently under investigation.",
                    "label": 0
                },
                {
                    "sent": "So this approach is model free approach.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "We do not need to estimate any matrix similarity metrics or rotation or any kinds of parameters for doing matching and estimating the similarity, and I think I didn't explain in the presentation this kernel is rotation, translation and scale invariant.",
                    "label": 0
                },
                {
                    "sent": "And also tolerant to some deformations in the context of matching shapes as an extension, I will show quickly how we can extend this kernel for machine translation and also in video matching and chivo.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's consider in machine translation the translation of an English sentence.",
                    "label": 1
                },
                {
                    "sent": "He may go into to possibilities in French.",
                    "label": 0
                },
                {
                    "sent": "One of them is wrong and the other one is correct.",
                    "label": 0
                },
                {
                    "sent": "So one possibility to estimate to take the right translation is to estimate the value of the kernel between S1 and S2 and S1 and S3 and take take the sentence which maximizes similarity.",
                    "label": 0
                },
                {
                    "sent": "So the subset kernel here is taken as the sum using the little kernel K between all possible.",
                    "label": 0
                },
                {
                    "sent": "Pairs of phrase pairs of words.",
                    "label": 0
                },
                {
                    "sent": "And here I show only the the pair which are either simple or either make some ambiguities.",
                    "label": 0
                },
                {
                    "sent": "So here we have an ambiguity between May, which might be in French the month or might be the ability.",
                    "label": 0
                },
                {
                    "sent": "So the way to estimate the two possibilities here is to consider a dictionary which will give us a similarity as the frequency or some joint statistics in a by tech or something like that.",
                    "label": 0
                },
                {
                    "sent": "So if we have the similarity between May and the month bigger than May and the ability, then we will end up with the wrong French translation, which is this one.",
                    "label": 0
                },
                {
                    "sent": "So why not to consider similarity using context?",
                    "label": 0
                },
                {
                    "sent": "Or phrase based translation.",
                    "label": 0
                },
                {
                    "sent": "So we consider that the similarity between words as similarity between the words and also the similarity between the context surrounding the words.",
                    "label": 0
                },
                {
                    "sent": "So here we use the context dependent kernel which estimate the similarity between may, which is maybe and we have two possibilities here.",
                    "label": 0
                },
                {
                    "sent": "May as a month or the ability.",
                    "label": 0
                },
                {
                    "sent": "And here we have the expression of the context dependent kernel which has the first term which depends on the dictionary which has no contact information on it and the second one is dependent on the context and we see here.",
                    "label": 1
                },
                {
                    "sent": "How we can make the similarity between May and the month less than the similarity between May and the ability in this context and we see here that we have this G function that I showed previously, which in fact estimate the joint or the Co occurrence between the pronounce and the and the month and the Co occurrence between the pronounce and the ability and we see here if the we know that in French or.",
                    "label": 0
                },
                {
                    "sent": "Even in English that the Co occurrence between ill and the month is less than the coherence between the month and the ability, and sorry between the ill and the ability.",
                    "label": 0
                },
                {
                    "sent": "And this will makes.",
                    "label": 0
                },
                {
                    "sent": "This will make this term vanish ING or going quickly to zero in the case we consider a similarity between ill and the month.",
                    "label": 0
                },
                {
                    "sent": "So if the value of the kernel which is estimated at the convergence stage between May and the month is less than the similarity between May and the ability, then we will end up with the correct French translation.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so let's see also how it works for.",
                    "label": 0
                },
                {
                    "sent": "Do you think that this is just going to wrap up in two or three words?",
                    "label": 0
                },
                {
                    "sent": "I'm finishing OK, just one word about video retrieval.",
                    "label": 0
                },
                {
                    "sent": "If we try to compare two sequence of videos, if they don't have the same length and we use a subset kernel and we will estimate the similarity between any pair of frames, we can do it either using context free kernel by estimating the kernel as a decreasing function of the intrinsic properties of the two frames, or we consider all the neighboring frames and this via at temporal coherence between.",
                    "label": 0
                },
                {
                    "sent": "The two videos.",
                    "label": 0
                },
                {
                    "sent": "So we the similarity will be dependent on the two frames and on the neighboring frames, so this will end up with the with a better video retrieval performance as currently done in our project.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Who is it?",
                    "label": 0
                },
                {
                    "sent": "Quick question.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Statement.",
                    "label": 0
                },
                {
                    "sent": "That's where the appearance will be discouraged, right?",
                    "label": 0
                },
                {
                    "sent": "Because yeah, yeah.",
                    "label": 0
                }
            ]
        }
    }
}