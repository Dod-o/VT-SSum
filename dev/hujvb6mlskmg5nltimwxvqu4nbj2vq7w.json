{
    "id": "hujvb6mlskmg5nltimwxvqu4nbj2vq7w",
    "title": "Three Dimensional Binary Edge Feature Representation for Pain Expression Analysis",
    "info": {
        "author": [
            "Lijun Yin, Thomas J. Watson School of Engineering and Applied Sciences, Binghamton University, State University of New York"
        ],
        "published": "July 2, 2015",
        "recorded": "May 2015",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Computer Vision->Face & Gesture Analysis"
        ]
    },
    "url": "http://videolectures.net/fgconference2015_yin_pain_expression/",
    "segmentation": [
        [
            "Thank you everyone.",
            "My name is Legion from the Binghamton University of the New York State and this is joint work and actually the most part of the improvement by my PhD students.",
            "Shing, who cannot make today and he is he's busy on the job hunting right now.",
            "So and also right now, and we collaborate with Jeff and the University of Pittsburgh and this is joint project would render for several years already.",
            "And as we know, the motivation."
        ],
        [
            "This work is based on our current public data data set, which is the BP 40 and the Binghamton Pittsburgh 4 dimensional Spontaneous Expression database, and the database is about the over 2 terabytes over 2 terrible, about 2.6 and including the eight tasks and all the sequence elicited by the spontaneous.",
            "The interviewed activities and so every self sequence have a different rental range.",
            "Also have the error you annotated the data as well, so motivated by this data we are curious to see how we validate this data and what type of the case study can help us to further to investigate what type of features on the 3D geometry, what applications, whether they can apply this in a UA emotion expressions, those type of validations.",
            "So based on this motivation we are thinking about the see.",
            "Let's have a one case study and pick up a dull pain and this is a one of the one of the interesting domain areas.",
            "People are working on for many years so."
        ],
        [
            "The paint issue is relatively very subjective in terms of pain ratings from one to five, one to 10 from varied from person to person, so so many work."
        ],
        [
            "Have done in the objective way to use the computer based approaches automatically detect those pain levels and action units related to pain issues.",
            "So there are many work.",
            "I just only listed a couple right now here."
        ],
        [
            "So the three dimensional domain have their own individual special features which can describe on the street geometry surface, not like a 2 dimensional domain, but of course 3 dimensional domain will introduce new problem there.",
            "But in terms of the post issues elimination issues, intuitively they could elevate those issues from suffered from the two dimension domain.",
            "So we're interested to see whether we can explore the Morn Yuan.",
            "Also simple and easy way to represent this geometry.",
            "OK, and later we will compare with other geometry as well.",
            "So we could also potentially to evaluate this temporal information as well.",
            "So that's the motivation to let us leave."
        ],
        [
            "As a designer, some approaches actually to represent the features and so this work.",
            "I'm going to report our progress of this work in the three parts.",
            "Basically, this paper talk about the three issues one representation issues which are from the 3D domain to represent the geometry features.",
            "Purely job, your surface space in the another, simple and easy describe the way to represent these features.",
            "Second, if you related to the pain, how many a use would be related to pain?",
            "There's no standard answer right now.",
            "There's some previous work already, but based on our current funding from the database in the previous previous one, we find some subset of common subset as our working AU.",
            "Based on this one.",
            "Then we take the latent latent and dynamic conditional random field for the classification.",
            "Whether we can use this temporary information as well to classify the identified another, only the binary AU but also the intensity of the year, which means the intensity of the.",
            "Ping Ping expression could be evaluated."
        ],
        [
            "So let's talk about the first part.",
            "My student at the rendered this is a purely 3D.",
            "Actually this is a sketch using the 3D model.",
            "We have about a 50,000 models.",
            "Those models you simply without any texture use it just simply using the sketch domain to just the artist draw the sketch, but we automatically render the sketch and on the 3D models so this one it does not include any texture information.",
            "The our finding is based on observation.",
            "See, there's most actually the ceiling areas is around those age instead of shadows.",
            "OK, so this motivate us.",
            "See a way we can.",
            "We can still use the similar way as a 2 dimensional domain to like the feature detection edge detection so we could have a job service major detection because the age also exists."
        ],
        [
            "On the surface as well, just a concave convex in the mountain in the two Hill they were indicated one top areas on the undermountain, so mathematically we described this way in the two dot product of the two normal on the two surface.",
            "If they're big enough, which means they should have some agent in between.",
            "OK so but in order to include improve this.",
            "Liability, so we are using the not only the one point we using the neighbor four point at the two dot product.",
            "Between this center with the neighbor and Adam together, the higher the value, which means the high likelihood of this agent exists in on this on this points.",
            "OK, so given this one."
        ],
        [
            "So our idea is to use those you got a normal map to converter converter, three component XYZ of the normal to the RGB, which means represented the 3D geometry normal information on the 2 dimensional and map.",
            "So this map give us the very easy way to handle the things just like the two dimensional domain ahead processing.",
            "So in this case we can also smooth out lots of noises from the from 3 domain can also get those edges.",
            "From this review, domains as well.",
            "So the things is how?"
        ],
        [
            "Look where you find it a good way to find out a good threshold.",
            "Just same thing as a 2 dimensional the edge detectors.",
            "So 3 dimensional agentx same thing has suffered over though.",
            "What's the optimal threshold so you can see the different level of the thresher.",
            "Show the different level detail and so we use our data about about a 50,000.",
            "The models we get those rendered and get those the ages.",
            "Then we find the difference between the two threshold the difference image.",
            "OK, they will change the the rates of the change with different around with the special changes, so this way here and the speed of change between the difference of the two pictures, the job rate is dramatically in the beginning they were they were going to slow down slow down so we need to find out the one reflection points that points you give us the relatively general to all these all these models so we pick up the 80.",
            "Actually this is scale in the 1000 so actually this Val."
        ],
        [
            "Is 0.08.",
            "So in order to get the one descriptor from these pictures, we need to also do the traditional way.",
            "Post alignments.",
            "OK, Croppings, and this is happening down the quite mature in the before and so, but for the one we get this normal map, so we divide it into 1414, one 44 and 144 and for each cell.",
            "Each cell you can count the number of the age of points, those data points versus the entire number of data points, or for the interface region.",
            "Then we take the ratio.",
            "That ratio will be derived for each individual cell.",
            "Then put all universal.",
            "Then we put as a one hour descriptor.",
            "So I think it's very simple descriptor in this way, but at the beginning we're not sure whether this will work, so we did a very."
        ],
        [
            "It's simple.",
            "Evaluation very beginning before we move on to the real detection, so we want to capture several different other features.",
            "For example shipping index in the three domain and every features with considered a temper domain and also Topa graphic surface features.",
            "Just just like the different type of graphic primitive features.",
            "We also take their traditional 2 dimensional textures as a descriptor is very commonly used as a baseline to see whether our work can be promising.",
            "So we compared to other data.",
            "And using the current database, we pick up the 12 Au's.",
            "We do the classifications then end.",
            "The One Show the 3D binary agent once shows in general average actually is better than those comparisons.",
            "Better than those those features then this give us the really encouraging results and we say, OK, let's move on to see whether we can make this one is even better to see any other AU detection radiating pain."
        ],
        [
            "So now once we face the pain expressions then we have to think about how many AU be good enough or how many user related one.",
            "OK, there's no standard theory right now.",
            "Can have one unique set to address those AU number.",
            "So for example, the paper by the parking assortments as adults about the 12:30 different values and also user combinations.",
            "It can also infer called the intensity scale intended scale, but the phrase on our current database, I'm not sure some people maybe have gathered data already.",
            "You will see our number of AU in total number, about a 34.",
            "Actually this is thanks to Jeff and Jeff Gerard at work in a very hard work to make this happen, and then we take the.",
            "Two set in the intersection part.",
            "Then we arrive.",
            "We eventually we decided to see.",
            "OK, we take the intersection.",
            "This two based on previous theory.",
            "Based on our current funding we take the intersection.",
            "We take the 4679 ten twelve 2027 as candidates and the candidate this."
        ],
        [
            "Our statistics here.",
            "For the pain parts we have about that over 20,000 bottle frames about related to pay.",
            "So for each individual use we have the status code for the number of the AU happened for for those frames and also for each individual individual frame, but also their combination of all these years.",
            "So they have about over 1000 combinations.",
            "We take about 30% of the AU combinations distribution around 41 subjects, so eventually we shrink them down.",
            "2 original One this number 2123456.",
            "Six day use.",
            "OK so after this."
        ],
        [
            "Western not very sure whether this can be verified.",
            "OK, now we conduct another part of subset of the peace core of the paired T tests.",
            "OK, between the pain expression and unpaid expression, nothing expression, including this a task startled embarrassment for example.",
            "Fear upsetting, discussed OK. Then we eventually find OK.",
            "This is the five.",
            "The subset as we derive before we find this peace core value in the lowest.",
            "Lawyer, which means they have a strong separation between the pain expression limping expression.",
            "That's why you see or other combination.",
            "They always have some values over 0.05.",
            "We're looking for something small enough lower than 0.05, so that's why.",
            "OK said, now we find that this is still encouraging.",
            "Most likely we still promising to go move on."
        ],
        [
            "Then we are going to decide in 2K.",
            "Using those they use.",
            "To based on the current data sets, we pick up the several traditional and commonly and also state of art.",
            "So those classifiers, particularly for the LDC Fr, see SRF.",
            "OK, then we conduct experiments.",
            "Experiments the test.",
            "The test is related to the pain expression data, the including the."
        ],
        [
            "All this they say here at boosting the red one, the PC, SVN PC early CIF the PC is too used to reduce the dimensionality from original over 1000 dimensionality to reduce to 40 dimensionality.",
            "So give me.",
            "This way you can see the green one general shows the better performance, but not a very significant but still the best in terms of 6 or 7.",
            "11:50 and also do the similar.",
            "Similar results in a U2 and I, for example similar ones.",
            "They combine all this one in general, the green ones still showed good results.",
            "OK, the reason we pick up the LDCRF engineers.",
            "This part because we're thinking about the weather."
        ],
        [
            "We can even use this one to infer the intensity.",
            "OK, no that right now.",
            "So far we haven't used any intensity code yet.",
            "We only use the binary code based on the Jeff Code 010101, but we only based on binary code to infer the continuous domain.",
            "The intensive one OK?",
            "So for example here.",
            "The likelihood of the output of classified early CRF could be the indication of the intensity on the code of the six, 1011 or 12 and 9.",
            "Whatever we mentioned before, so.",
            "Let's say if we have the scale on the output from the scale from zero to one.",
            "OK, so we can easily to split into the half of the five parts, the five parts, each part represented the one intensity because there's a file in the real doctor will ask the patient to give me that.",
            "You're paying 125.",
            "You see file, you see four.",
            "OK, very subjective, but we just uniformly divided into 5 pieces, each piece represented by intensity.",
            "But actually this is not still good because the our ping development is not linear.",
            "OK, the onset offsets must be random linear parts, so that's why we pick up the nonlinear 6 power polynomial curve to fit on this.",
            "The range.",
            "OK, then we divide it into five pieces, so that's why you see here.",
            "If we evaluate the AU from this three and the difference from the estimated intensity with the ground truth intensity, which included better Jeff Groups, and this one showed a smaller difference, this one showed the big difference continue, so that's why we're pick up the using the nonlinear.",
            "Mapping linear mapping.",
            "OK."
        ],
        [
            "So take a look this one.",
            "This one shows the pain intensity directions and is at one example.",
            "And this data shows from beginning and we started very pain.",
            "Very strong pain and the subject emerged hand into the ice, ice, water and last no more than one minutes.",
            "We tell them you can feel free to withdraw any other times, but some subjects is very endurance.",
            "They want to see how tough they are.",
            "OK, some subject are the five.",
            "Secondly, take it out.",
            "OK so but this guy shows the first one then.",
            "Then slow down.",
            "Then he feel the pain again because the people feel pain us sometimes immediately have the delay.",
            "OK sometimes they feel the numbness as well.",
            "So after first first part then we will feel much better because they feel numb after a couple of seconds, minutes or maybe less than minutes.",
            "They will sometimes will come back come back again.",
            "So this figure showed a 3D geometry.",
            "This is really the kind of 3D binary age information here which is using normal map but the red one.",
            "The warm color to show the strong high intensity of the pain and for the code color to show the low intensity.",
            "So this wants to see here AU those 4 three groups three groups.",
            "They show the change of the intensity from the graduate to slow down and move.",
            "No the Dash 1 to show the binary code which is the ground truth with ground truth.",
            "And so for this work I haven't used their intensity code at the comparison yet, so we're still working on the comparison for the.",
            "Intensity training parts.",
            "So here this is going down.",
            "This is going down followed basically follow this one approximately on this change from up to down.",
            "So this is the pictures.",
            "Let's take a look here.",
            "So this one showed a 3D ages to map on the normal map immediately.",
            "This students 3D domain, but represent looks like in the 2D and then start of pain.",
            "Then going down, slow down, slow down, and then start paying again.",
            "Looks actually the expression looks at.",
            "Sometimes it is funny and give some smiling phase.",
            "So the follow this change.",
            "This went to indicate the guy still pain but not showing too much.",
            "This is the this lady, so this indicates that low intensity because the intensity or maybe slow down and then certain threshold.",
            "So this is general work we find still."
        ],
        [
            "Promising on the on this data on this data, so we are.",
            "Well, basically to look at those simple and easy way to describe this job for features and the same time to pick up the subset of the AU for release of the pain and the same time to evaluate those intensity through this 01 code 01 code.",
            "So I would like to make a particular notes, those type of description of this feature also applied to any other a use an expression as well and basically right now we're testing the summer discussed the.",
            "They use and also the embarrassment and also the status.",
            "So this is also very promising to identify those values and four infer those expression as well.",
            "And so I think this will be."
        ],
        [
            "Sure enough to the future of virtual further developments.",
            "So we're also looking at some ground truth with the self reports 'cause we have subject herself reports.",
            "They also tell us what they feel.",
            "We're also working on something like the using the person that put a hand on the football once they feel strong pain in the hand will be have.",
            "The strong force will be signal be transmitted to another machine to get the 4th signals.",
            "This full signal is their own feeling.",
            "This data can be also related to the intensity of the pain, so that's another physical part of the signals.",
            "With our soft power signals also.",
            "So that's why I think this is still the work is move on.",
            "I hope for it.",
            "I hope this data can be big.",
            "The community of this research community to behold, interesting this one to make it more contribution, added the more more different their findings and gather.",
            "Give us the feedback we can move on to the next level.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is Legion from the Binghamton University of the New York State and this is joint work and actually the most part of the improvement by my PhD students.",
                    "label": 0
                },
                {
                    "sent": "Shing, who cannot make today and he is he's busy on the job hunting right now.",
                    "label": 0
                },
                {
                    "sent": "So and also right now, and we collaborate with Jeff and the University of Pittsburgh and this is joint project would render for several years already.",
                    "label": 1
                },
                {
                    "sent": "And as we know, the motivation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This work is based on our current public data data set, which is the BP 40 and the Binghamton Pittsburgh 4 dimensional Spontaneous Expression database, and the database is about the over 2 terabytes over 2 terrible, about 2.6 and including the eight tasks and all the sequence elicited by the spontaneous.",
                    "label": 0
                },
                {
                    "sent": "The interviewed activities and so every self sequence have a different rental range.",
                    "label": 0
                },
                {
                    "sent": "Also have the error you annotated the data as well, so motivated by this data we are curious to see how we validate this data and what type of the case study can help us to further to investigate what type of features on the 3D geometry, what applications, whether they can apply this in a UA emotion expressions, those type of validations.",
                    "label": 0
                },
                {
                    "sent": "So based on this motivation we are thinking about the see.",
                    "label": 0
                },
                {
                    "sent": "Let's have a one case study and pick up a dull pain and this is a one of the one of the interesting domain areas.",
                    "label": 0
                },
                {
                    "sent": "People are working on for many years so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The paint issue is relatively very subjective in terms of pain ratings from one to five, one to 10 from varied from person to person, so so many work.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have done in the objective way to use the computer based approaches automatically detect those pain levels and action units related to pain issues.",
                    "label": 0
                },
                {
                    "sent": "So there are many work.",
                    "label": 0
                },
                {
                    "sent": "I just only listed a couple right now here.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the three dimensional domain have their own individual special features which can describe on the street geometry surface, not like a 2 dimensional domain, but of course 3 dimensional domain will introduce new problem there.",
                    "label": 0
                },
                {
                    "sent": "But in terms of the post issues elimination issues, intuitively they could elevate those issues from suffered from the two dimension domain.",
                    "label": 0
                },
                {
                    "sent": "So we're interested to see whether we can explore the Morn Yuan.",
                    "label": 0
                },
                {
                    "sent": "Also simple and easy way to represent this geometry.",
                    "label": 0
                },
                {
                    "sent": "OK, and later we will compare with other geometry as well.",
                    "label": 0
                },
                {
                    "sent": "So we could also potentially to evaluate this temporal information as well.",
                    "label": 0
                },
                {
                    "sent": "So that's the motivation to let us leave.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a designer, some approaches actually to represent the features and so this work.",
                    "label": 0
                },
                {
                    "sent": "I'm going to report our progress of this work in the three parts.",
                    "label": 0
                },
                {
                    "sent": "Basically, this paper talk about the three issues one representation issues which are from the 3D domain to represent the geometry features.",
                    "label": 0
                },
                {
                    "sent": "Purely job, your surface space in the another, simple and easy describe the way to represent these features.",
                    "label": 0
                },
                {
                    "sent": "Second, if you related to the pain, how many a use would be related to pain?",
                    "label": 0
                },
                {
                    "sent": "There's no standard answer right now.",
                    "label": 0
                },
                {
                    "sent": "There's some previous work already, but based on our current funding from the database in the previous previous one, we find some subset of common subset as our working AU.",
                    "label": 0
                },
                {
                    "sent": "Based on this one.",
                    "label": 0
                },
                {
                    "sent": "Then we take the latent latent and dynamic conditional random field for the classification.",
                    "label": 0
                },
                {
                    "sent": "Whether we can use this temporary information as well to classify the identified another, only the binary AU but also the intensity of the year, which means the intensity of the.",
                    "label": 0
                },
                {
                    "sent": "Ping Ping expression could be evaluated.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's talk about the first part.",
                    "label": 0
                },
                {
                    "sent": "My student at the rendered this is a purely 3D.",
                    "label": 0
                },
                {
                    "sent": "Actually this is a sketch using the 3D model.",
                    "label": 0
                },
                {
                    "sent": "We have about a 50,000 models.",
                    "label": 0
                },
                {
                    "sent": "Those models you simply without any texture use it just simply using the sketch domain to just the artist draw the sketch, but we automatically render the sketch and on the 3D models so this one it does not include any texture information.",
                    "label": 0
                },
                {
                    "sent": "The our finding is based on observation.",
                    "label": 0
                },
                {
                    "sent": "See, there's most actually the ceiling areas is around those age instead of shadows.",
                    "label": 0
                },
                {
                    "sent": "OK, so this motivate us.",
                    "label": 0
                },
                {
                    "sent": "See a way we can.",
                    "label": 0
                },
                {
                    "sent": "We can still use the similar way as a 2 dimensional domain to like the feature detection edge detection so we could have a job service major detection because the age also exists.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the surface as well, just a concave convex in the mountain in the two Hill they were indicated one top areas on the undermountain, so mathematically we described this way in the two dot product of the two normal on the two surface.",
                    "label": 0
                },
                {
                    "sent": "If they're big enough, which means they should have some agent in between.",
                    "label": 0
                },
                {
                    "sent": "OK so but in order to include improve this.",
                    "label": 0
                },
                {
                    "sent": "Liability, so we are using the not only the one point we using the neighbor four point at the two dot product.",
                    "label": 0
                },
                {
                    "sent": "Between this center with the neighbor and Adam together, the higher the value, which means the high likelihood of this agent exists in on this on this points.",
                    "label": 0
                },
                {
                    "sent": "OK, so given this one.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our idea is to use those you got a normal map to converter converter, three component XYZ of the normal to the RGB, which means represented the 3D geometry normal information on the 2 dimensional and map.",
                    "label": 0
                },
                {
                    "sent": "So this map give us the very easy way to handle the things just like the two dimensional domain ahead processing.",
                    "label": 0
                },
                {
                    "sent": "So in this case we can also smooth out lots of noises from the from 3 domain can also get those edges.",
                    "label": 0
                },
                {
                    "sent": "From this review, domains as well.",
                    "label": 0
                },
                {
                    "sent": "So the things is how?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look where you find it a good way to find out a good threshold.",
                    "label": 0
                },
                {
                    "sent": "Just same thing as a 2 dimensional the edge detectors.",
                    "label": 1
                },
                {
                    "sent": "So 3 dimensional agentx same thing has suffered over though.",
                    "label": 1
                },
                {
                    "sent": "What's the optimal threshold so you can see the different level of the thresher.",
                    "label": 0
                },
                {
                    "sent": "Show the different level detail and so we use our data about about a 50,000.",
                    "label": 0
                },
                {
                    "sent": "The models we get those rendered and get those the ages.",
                    "label": 0
                },
                {
                    "sent": "Then we find the difference between the two threshold the difference image.",
                    "label": 0
                },
                {
                    "sent": "OK, they will change the the rates of the change with different around with the special changes, so this way here and the speed of change between the difference of the two pictures, the job rate is dramatically in the beginning they were they were going to slow down slow down so we need to find out the one reflection points that points you give us the relatively general to all these all these models so we pick up the 80.",
                    "label": 0
                },
                {
                    "sent": "Actually this is scale in the 1000 so actually this Val.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is 0.08.",
                    "label": 0
                },
                {
                    "sent": "So in order to get the one descriptor from these pictures, we need to also do the traditional way.",
                    "label": 0
                },
                {
                    "sent": "Post alignments.",
                    "label": 0
                },
                {
                    "sent": "OK, Croppings, and this is happening down the quite mature in the before and so, but for the one we get this normal map, so we divide it into 1414, one 44 and 144 and for each cell.",
                    "label": 0
                },
                {
                    "sent": "Each cell you can count the number of the age of points, those data points versus the entire number of data points, or for the interface region.",
                    "label": 0
                },
                {
                    "sent": "Then we take the ratio.",
                    "label": 0
                },
                {
                    "sent": "That ratio will be derived for each individual cell.",
                    "label": 0
                },
                {
                    "sent": "Then put all universal.",
                    "label": 0
                },
                {
                    "sent": "Then we put as a one hour descriptor.",
                    "label": 0
                },
                {
                    "sent": "So I think it's very simple descriptor in this way, but at the beginning we're not sure whether this will work, so we did a very.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's simple.",
                    "label": 0
                },
                {
                    "sent": "Evaluation very beginning before we move on to the real detection, so we want to capture several different other features.",
                    "label": 0
                },
                {
                    "sent": "For example shipping index in the three domain and every features with considered a temper domain and also Topa graphic surface features.",
                    "label": 0
                },
                {
                    "sent": "Just just like the different type of graphic primitive features.",
                    "label": 0
                },
                {
                    "sent": "We also take their traditional 2 dimensional textures as a descriptor is very commonly used as a baseline to see whether our work can be promising.",
                    "label": 0
                },
                {
                    "sent": "So we compared to other data.",
                    "label": 0
                },
                {
                    "sent": "And using the current database, we pick up the 12 Au's.",
                    "label": 0
                },
                {
                    "sent": "We do the classifications then end.",
                    "label": 0
                },
                {
                    "sent": "The One Show the 3D binary agent once shows in general average actually is better than those comparisons.",
                    "label": 0
                },
                {
                    "sent": "Better than those those features then this give us the really encouraging results and we say, OK, let's move on to see whether we can make this one is even better to see any other AU detection radiating pain.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now once we face the pain expressions then we have to think about how many AU be good enough or how many user related one.",
                    "label": 0
                },
                {
                    "sent": "OK, there's no standard theory right now.",
                    "label": 0
                },
                {
                    "sent": "Can have one unique set to address those AU number.",
                    "label": 0
                },
                {
                    "sent": "So for example, the paper by the parking assortments as adults about the 12:30 different values and also user combinations.",
                    "label": 0
                },
                {
                    "sent": "It can also infer called the intensity scale intended scale, but the phrase on our current database, I'm not sure some people maybe have gathered data already.",
                    "label": 0
                },
                {
                    "sent": "You will see our number of AU in total number, about a 34.",
                    "label": 0
                },
                {
                    "sent": "Actually this is thanks to Jeff and Jeff Gerard at work in a very hard work to make this happen, and then we take the.",
                    "label": 0
                },
                {
                    "sent": "Two set in the intersection part.",
                    "label": 0
                },
                {
                    "sent": "Then we arrive.",
                    "label": 0
                },
                {
                    "sent": "We eventually we decided to see.",
                    "label": 0
                },
                {
                    "sent": "OK, we take the intersection.",
                    "label": 0
                },
                {
                    "sent": "This two based on previous theory.",
                    "label": 0
                },
                {
                    "sent": "Based on our current funding we take the intersection.",
                    "label": 0
                },
                {
                    "sent": "We take the 4679 ten twelve 2027 as candidates and the candidate this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our statistics here.",
                    "label": 0
                },
                {
                    "sent": "For the pain parts we have about that over 20,000 bottle frames about related to pay.",
                    "label": 0
                },
                {
                    "sent": "So for each individual use we have the status code for the number of the AU happened for for those frames and also for each individual individual frame, but also their combination of all these years.",
                    "label": 0
                },
                {
                    "sent": "So they have about over 1000 combinations.",
                    "label": 0
                },
                {
                    "sent": "We take about 30% of the AU combinations distribution around 41 subjects, so eventually we shrink them down.",
                    "label": 0
                },
                {
                    "sent": "2 original One this number 2123456.",
                    "label": 0
                },
                {
                    "sent": "Six day use.",
                    "label": 0
                },
                {
                    "sent": "OK so after this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Western not very sure whether this can be verified.",
                    "label": 0
                },
                {
                    "sent": "OK, now we conduct another part of subset of the peace core of the paired T tests.",
                    "label": 0
                },
                {
                    "sent": "OK, between the pain expression and unpaid expression, nothing expression, including this a task startled embarrassment for example.",
                    "label": 0
                },
                {
                    "sent": "Fear upsetting, discussed OK. Then we eventually find OK.",
                    "label": 0
                },
                {
                    "sent": "This is the five.",
                    "label": 0
                },
                {
                    "sent": "The subset as we derive before we find this peace core value in the lowest.",
                    "label": 0
                },
                {
                    "sent": "Lawyer, which means they have a strong separation between the pain expression limping expression.",
                    "label": 0
                },
                {
                    "sent": "That's why you see or other combination.",
                    "label": 0
                },
                {
                    "sent": "They always have some values over 0.05.",
                    "label": 0
                },
                {
                    "sent": "We're looking for something small enough lower than 0.05, so that's why.",
                    "label": 0
                },
                {
                    "sent": "OK said, now we find that this is still encouraging.",
                    "label": 0
                },
                {
                    "sent": "Most likely we still promising to go move on.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we are going to decide in 2K.",
                    "label": 0
                },
                {
                    "sent": "Using those they use.",
                    "label": 0
                },
                {
                    "sent": "To based on the current data sets, we pick up the several traditional and commonly and also state of art.",
                    "label": 0
                },
                {
                    "sent": "So those classifiers, particularly for the LDC Fr, see SRF.",
                    "label": 0
                },
                {
                    "sent": "OK, then we conduct experiments.",
                    "label": 0
                },
                {
                    "sent": "Experiments the test.",
                    "label": 0
                },
                {
                    "sent": "The test is related to the pain expression data, the including the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All this they say here at boosting the red one, the PC, SVN PC early CIF the PC is too used to reduce the dimensionality from original over 1000 dimensionality to reduce to 40 dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So give me.",
                    "label": 0
                },
                {
                    "sent": "This way you can see the green one general shows the better performance, but not a very significant but still the best in terms of 6 or 7.",
                    "label": 0
                },
                {
                    "sent": "11:50 and also do the similar.",
                    "label": 0
                },
                {
                    "sent": "Similar results in a U2 and I, for example similar ones.",
                    "label": 0
                },
                {
                    "sent": "They combine all this one in general, the green ones still showed good results.",
                    "label": 0
                },
                {
                    "sent": "OK, the reason we pick up the LDCRF engineers.",
                    "label": 0
                },
                {
                    "sent": "This part because we're thinking about the weather.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can even use this one to infer the intensity.",
                    "label": 1
                },
                {
                    "sent": "OK, no that right now.",
                    "label": 0
                },
                {
                    "sent": "So far we haven't used any intensity code yet.",
                    "label": 0
                },
                {
                    "sent": "We only use the binary code based on the Jeff Code 010101, but we only based on binary code to infer the continuous domain.",
                    "label": 0
                },
                {
                    "sent": "The intensive one OK?",
                    "label": 0
                },
                {
                    "sent": "So for example here.",
                    "label": 0
                },
                {
                    "sent": "The likelihood of the output of classified early CRF could be the indication of the intensity on the code of the six, 1011 or 12 and 9.",
                    "label": 0
                },
                {
                    "sent": "Whatever we mentioned before, so.",
                    "label": 0
                },
                {
                    "sent": "Let's say if we have the scale on the output from the scale from zero to one.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can easily to split into the half of the five parts, the five parts, each part represented the one intensity because there's a file in the real doctor will ask the patient to give me that.",
                    "label": 0
                },
                {
                    "sent": "You're paying 125.",
                    "label": 0
                },
                {
                    "sent": "You see file, you see four.",
                    "label": 0
                },
                {
                    "sent": "OK, very subjective, but we just uniformly divided into 5 pieces, each piece represented by intensity.",
                    "label": 1
                },
                {
                    "sent": "But actually this is not still good because the our ping development is not linear.",
                    "label": 0
                },
                {
                    "sent": "OK, the onset offsets must be random linear parts, so that's why we pick up the nonlinear 6 power polynomial curve to fit on this.",
                    "label": 0
                },
                {
                    "sent": "The range.",
                    "label": 0
                },
                {
                    "sent": "OK, then we divide it into five pieces, so that's why you see here.",
                    "label": 0
                },
                {
                    "sent": "If we evaluate the AU from this three and the difference from the estimated intensity with the ground truth intensity, which included better Jeff Groups, and this one showed a smaller difference, this one showed the big difference continue, so that's why we're pick up the using the nonlinear.",
                    "label": 1
                },
                {
                    "sent": "Mapping linear mapping.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So take a look this one.",
                    "label": 0
                },
                {
                    "sent": "This one shows the pain intensity directions and is at one example.",
                    "label": 1
                },
                {
                    "sent": "And this data shows from beginning and we started very pain.",
                    "label": 0
                },
                {
                    "sent": "Very strong pain and the subject emerged hand into the ice, ice, water and last no more than one minutes.",
                    "label": 0
                },
                {
                    "sent": "We tell them you can feel free to withdraw any other times, but some subjects is very endurance.",
                    "label": 0
                },
                {
                    "sent": "They want to see how tough they are.",
                    "label": 0
                },
                {
                    "sent": "OK, some subject are the five.",
                    "label": 0
                },
                {
                    "sent": "Secondly, take it out.",
                    "label": 0
                },
                {
                    "sent": "OK so but this guy shows the first one then.",
                    "label": 0
                },
                {
                    "sent": "Then slow down.",
                    "label": 0
                },
                {
                    "sent": "Then he feel the pain again because the people feel pain us sometimes immediately have the delay.",
                    "label": 0
                },
                {
                    "sent": "OK sometimes they feel the numbness as well.",
                    "label": 0
                },
                {
                    "sent": "So after first first part then we will feel much better because they feel numb after a couple of seconds, minutes or maybe less than minutes.",
                    "label": 0
                },
                {
                    "sent": "They will sometimes will come back come back again.",
                    "label": 0
                },
                {
                    "sent": "So this figure showed a 3D geometry.",
                    "label": 0
                },
                {
                    "sent": "This is really the kind of 3D binary age information here which is using normal map but the red one.",
                    "label": 0
                },
                {
                    "sent": "The warm color to show the strong high intensity of the pain and for the code color to show the low intensity.",
                    "label": 0
                },
                {
                    "sent": "So this wants to see here AU those 4 three groups three groups.",
                    "label": 0
                },
                {
                    "sent": "They show the change of the intensity from the graduate to slow down and move.",
                    "label": 0
                },
                {
                    "sent": "No the Dash 1 to show the binary code which is the ground truth with ground truth.",
                    "label": 0
                },
                {
                    "sent": "And so for this work I haven't used their intensity code at the comparison yet, so we're still working on the comparison for the.",
                    "label": 0
                },
                {
                    "sent": "Intensity training parts.",
                    "label": 0
                },
                {
                    "sent": "So here this is going down.",
                    "label": 0
                },
                {
                    "sent": "This is going down followed basically follow this one approximately on this change from up to down.",
                    "label": 0
                },
                {
                    "sent": "So this is the pictures.",
                    "label": 0
                },
                {
                    "sent": "Let's take a look here.",
                    "label": 0
                },
                {
                    "sent": "So this one showed a 3D ages to map on the normal map immediately.",
                    "label": 0
                },
                {
                    "sent": "This students 3D domain, but represent looks like in the 2D and then start of pain.",
                    "label": 0
                },
                {
                    "sent": "Then going down, slow down, slow down, and then start paying again.",
                    "label": 0
                },
                {
                    "sent": "Looks actually the expression looks at.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it is funny and give some smiling phase.",
                    "label": 0
                },
                {
                    "sent": "So the follow this change.",
                    "label": 0
                },
                {
                    "sent": "This went to indicate the guy still pain but not showing too much.",
                    "label": 0
                },
                {
                    "sent": "This is the this lady, so this indicates that low intensity because the intensity or maybe slow down and then certain threshold.",
                    "label": 0
                },
                {
                    "sent": "So this is general work we find still.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Promising on the on this data on this data, so we are.",
                    "label": 0
                },
                {
                    "sent": "Well, basically to look at those simple and easy way to describe this job for features and the same time to pick up the subset of the AU for release of the pain and the same time to evaluate those intensity through this 01 code 01 code.",
                    "label": 0
                },
                {
                    "sent": "So I would like to make a particular notes, those type of description of this feature also applied to any other a use an expression as well and basically right now we're testing the summer discussed the.",
                    "label": 0
                },
                {
                    "sent": "They use and also the embarrassment and also the status.",
                    "label": 0
                },
                {
                    "sent": "So this is also very promising to identify those values and four infer those expression as well.",
                    "label": 0
                },
                {
                    "sent": "And so I think this will be.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure enough to the future of virtual further developments.",
                    "label": 1
                },
                {
                    "sent": "So we're also looking at some ground truth with the self reports 'cause we have subject herself reports.",
                    "label": 0
                },
                {
                    "sent": "They also tell us what they feel.",
                    "label": 0
                },
                {
                    "sent": "We're also working on something like the using the person that put a hand on the football once they feel strong pain in the hand will be have.",
                    "label": 0
                },
                {
                    "sent": "The strong force will be signal be transmitted to another machine to get the 4th signals.",
                    "label": 0
                },
                {
                    "sent": "This full signal is their own feeling.",
                    "label": 0
                },
                {
                    "sent": "This data can be also related to the intensity of the pain, so that's another physical part of the signals.",
                    "label": 0
                },
                {
                    "sent": "With our soft power signals also.",
                    "label": 0
                },
                {
                    "sent": "So that's why I think this is still the work is move on.",
                    "label": 0
                },
                {
                    "sent": "I hope for it.",
                    "label": 0
                },
                {
                    "sent": "I hope this data can be big.",
                    "label": 0
                },
                {
                    "sent": "The community of this research community to behold, interesting this one to make it more contribution, added the more more different their findings and gather.",
                    "label": 0
                },
                {
                    "sent": "Give us the feedback we can move on to the next level.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}