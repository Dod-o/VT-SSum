{
    "id": "4ymtwluh22qtzsxrmodo7hrdamgnl4q7",
    "title": "Correlation Clustering in MapReduce",
    "info": {
        "author": [
            "Flavio Chierichetti, Dipartimento di Informatica, Sapienza University of Rome"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_chierichetti_correlation_clustering/",
    "segmentation": [
        [
            "So this is joint work with Nilesh Dalvi that truly with Ravi Kumar at Google Mountain View.",
            "So.",
            "The title we will be talking mostly about two things.",
            "One is correlation clustering and the other one is map reduce.",
            "In fact, not just about MapReduce, but most parallel computation frameworks."
        ],
        [
            "So what's correlation clustering?",
            "It's clustering tool or a clustering problem that has been defined by Bansal, Blum, and Shola and it has been used since then in many different data mining tasks.",
            "That's a list of papers which is not comprehensive.",
            "That as be using correlation clustering to find some cluster in some type of data.",
            "What's this type of data?",
            "Well, as you see in the picture at the bottom of the slide.",
            "You have a bunch of items, the black dots an for each couple of items.",
            "You're told that these items are similar or dissimilar.",
            "OK, you have Vera plus one or minus one."
        ],
        [
            "You can think of the items as a web pages, users or ads, or actually anything that they come up to your mind."
        ],
        [
            "Say in the case of maybe stories, web pages, maybe you can have on your time storycnn.com story.",
            "So blog post story that you could find on WordPress, Blogger or on Google Plus and then you have some machine learning algorithm that tells you whether two stories are similar or dissimilar.",
            "OK?"
        ],
        [
            "Your goal is to find the clustering.",
            "So partition of the items into subsets that keeps together similar items and places the similar items in distinct clusters."
        ],
        [
            "So in this case it's it's a very easy problem because you could just take all the sort of right elements, put them together, all the left elements, put them together, and then you have that all the similar item, all the similar couples are together and all the dissimilar carports are not together in these different clusters."
        ],
        [
            "So that's.",
            "That's the first thing I want to say about correlation clustering.",
            "I will be telling you more about that, but let's talk about the second part of the title.",
            "So map reduce or more generally large scale computation.",
            "So what we want to do is to find these fine very good correlation clusterings for datasets that are very large.",
            "You can think of web, social or mobile data sets which are so large that you cannot hope to hold them in, say, a single computer memory and so you have to distribute them across multiple machines.",
            "So say computer cluster.",
            "You do that, then data processing becomes much more complicated than, say, in a single process or setting."
        ],
        [
            "So you want data processing to be fast to be parallel, you have many machines and also it has to have tiny memory footprint because there's no single machine that can hold all the data on its own."
        ],
        [
            "There are many frameworks that allow you to do that.",
            "Some famous ones are MapReduce, Pregel or you know more simple setting streaming, setting the algorithm that I will be telling you about will sort of works in all of them, so it's you know framework you care about.",
            "Hopefully you can make the algorithm work on it."
        ],
        [
            "So.",
            "Dietert OK, so OK so now I gave you a bird eye view of bother things in the title.",
            "Let me go a little bit deeper on what is correlation clustering and what kind of function we really trying to optimize here."
        ],
        [
            "So suppose I give you these four items.",
            "You see that there are some similarities between them, the left and the left items are circles, the right items are squares, so."
        ],
        [
            "I expect say the circles to be similar to each others.",
            "The squares to be similar to each other."
        ],
        [
            "Then maybe you know every circle to be this similar to every square, OK?"
        ],
        [
            "So now what's the best clustering in this case?",
            "Well, it's very easy if you take the left elements or by themselves and the right elements all by themselves, you get a perfect clustering, right?",
            "You have zero mistakes.",
            "All the similar items are together.",
            "All the similar items are together like in the first slides, and in this case we actually make zero mistakes, and that's a very simple data set to cluster."
        ],
        [
            "We could make some mistakes though.",
            "Say we could put two similar items into different clusters or or say we could put every item on its own cluster, in which case we would make very many mistakes.",
            "The interesting thing here is that the number of clusters is not fixed in correlation clustering OK, But in fact correlation clustering sort of lets the right number of clusters for a given data set to come out of the data set itself.",
            "So it's not fixed to anything."
        ],
        [
            "And again, the goal of correlation clustering is to partition the element in this joint clusters.",
            "So to minimize the number of mistakes.",
            "So so to minimize the number of similar items that end up in different clusters and the number of these similar items that end up together."
        ],
        [
            "The number of clusters does not have to be selected by end.",
            "OK."
        ],
        [
            "So again, this is a very simple case where you can find a perfect clustering with your mistakes, but it's not what happens in general."
        ],
        [
            "Let's take this slightly more complicated example."
        ],
        [
            "Well, what would you do?",
            "You could plaster this way so you could take the two vertical edges together and the bottom element by itself, in which case you will make two mistakes.",
            "You see the two similar.",
            "The two green edges, which means similar elements, are not in the same cluster."
        ],
        [
            "Or you could do the following.",
            "You could pick all the left elements plus the central one and the right elements on the other side.",
            "That would give you a red edge.",
            "Couple of these similar element that that's inside the cluster.",
            "So in fact, in this case one can show that no matter how you cluster, no matter how you select the cluster, you will make at least two different mistakes, OK?"
        ],
        [
            "So.",
            "So you see the question here is not can we find the optimal cluster?",
            "A cluster that puts together all the similar elements an puts in different clusters.",
            "All the dissimilar elements.",
            "But we want to find a cluster which is as good as possible, which makes as little mistakes as possible."
        ],
        [
            "So how can you do that?",
            "And by how can you do that?",
            "I mean how to minimize the number of mistakes?",
            "How can I produce a cluster that minimizes the number of mistakes that minimum number doesn't have to be 0?",
            "It can be anything.",
            "It depends on the day."
        ],
        [
            "Recent.",
            "So there is this algorithm by Island Cherry Kara Neumann, which is very simple and very elegant, which gives you not the best possible clustering, but gives you something which is close to that.",
            "Now let me explain this algorithm because it's sort of the.",
            "The the basis of what we will do."
        ],
        [
            "So say we start from this instance.",
            "The first step is pick an element P from."
        ],
        [
            "Instance uniformly at random.",
            "So we choose that element there."
        ],
        [
            "And then we create that cluster containing the this element P, which I will be calling the people element and all the elements similar to it."
        ],
        [
            "OK, so in this case it has two similar elements and I will put all of them in in a cluster."
        ],
        [
            "Then I will remove that cluster from the data set and I will just repeat so now."
        ],
        [
            "So I will pick another piece."
        ],
        [
            "At random and I will create its cluster OK and I will go until I will have elements to pick this way."
        ],
        [
            "So, so this produces a clustering and."
        ],
        [
            "So in this case this clustering makes mistakes, right?",
            "Like what we saw in the previous slide?"
        ],
        [
            "What what I launch Areekara Newman showed is that this algorithm, the people algorithm, gives unexpected three approximation to the problem of minimizing the number of mistakes.",
            "So if you run this algorithm on expectation, the number of mistakes it will make is at most three times times the minimum number you can make one on that given instance."
        ],
        [
            "Why can we just use this algorithm?",
            "The reason is that the people algorithm is inherently sequential, and so you cannot really hope to run it on huge datasets.",
            "Why's it indirectly sequential?"
        ],
        [
            "Suppose I'm a very bad guy and I give you this instance so all the elements are dissimilar from each other."
        ],
        [
            "Now let's run this algorithm.",
            "You will pick one people.",
            "It will create a cluster containing that people and all the similar over all the elements similar to."
        ],
        [
            "There are no such elements, so we just remove one node and it will."
        ],
        [
            "Don't like this?"
        ],
        [
            "Right, so we have to make 10 different iterations, one for each single element, and in some of the datasets you want to cluster."
        ],
        [
            "It can be huge, so hundreds of millions or 10s of millions."
        ],
        [
            "So that's not good.",
            "There's no hope you can run such an algorithm on such large data sets, and so the question we asked is how to speed up the computation.",
            "Is there any hope of sort of obtaining the same approximation but in a much faster way?"
        ],
        [
            "And I guess the answer is yes, and we propose a parallel version of the people algorithm of the algorithm that I just showed you.",
            "This version only requires log square and iterations, so much less than an iterations and still guarantees are three plus epsilon approximation."
        ],
        [
            "And I guess the key thing here is that log square any is is much smaller than N, so that in many cases it can be.",
            "It can actually be run."
        ],
        [
            "Moreover, the new algorithm is still easy, like the people algorithm an.",
            "In fact, it's much easy to implement it in MapReduce, Pregel or in a streaming setting."
        ],
        [
            "We also run this algorithm on a number of datasets, the largest which is this Twitter data set at 41 million elements, an 2.5 billion positive edges.",
            "So 2.5 billion couples of elements that were similar to each other.",
            "So now let me let me tell you about this this algorithm and then we can discuss it."
        ],
        [
            "So the algorithm works as the people are going away, so it goes on up until the instance is not empty up until you still have some elements to cluster an at the very beginning of an iteration.",
            "What you do is you compute the maximum positive degree of an element in the remaining list."
        ],
        [
            "So if this is the instance that we're running our app."
        ],
        [
            "Rhythm on we can see that the maximum positive degree is to write.",
            "If you take in all the elements, the one that has the largest number of positive edges, it's too right.",
            "It's anyone but the rightmost ones.",
            "So you compute this Delta plus."
        ],
        [
            "And now you activate each element independently with probability, which is roughly one over Delta plus a little bit smaller, say epsilon over Delta plus, and that's what you do.",
            "You do all this in parallel, so you flip a coin for each of the elements an if it says with probability epsilon over Delta plus you activate the Tele."
        ],
        [
            "So say we activate these four elements.",
            "OK, all the elements, but the central ones."
        ],
        [
            "Now we have to make sort of step that keeps everything consistent.",
            "So we have to deactivate the active elements that are connected through positive edge to other active elements."
        ],
        [
            "This case, it means that the leftmost elements have to be deactivated because you see the way we want the active elements to be the people, and we don't want people to be close to each other right in the original algorithm, each you pick a single people at a time and then people created a cluster so we can.",
            "If you have two people that are both connected to each other, then how can you create the cluster which people goes into cluster which other people?",
            "So if this happens you deactivate both of them and you just hope that it doesn't happen to often."
        ],
        [
            "So we deactivate these two elements and the remaining active nodes become people.",
            "OK, once you have them."
        ],
        [
            "So you create one cluster for each people."
        ],
        [
            "So, so you create these two clusters like in the previous algorithm."
        ],
        [
            "And then you repeat now."
        ],
        [
            "Classes one.",
            "So."
        ],
        [
            "You're lucky you activate that element."
        ],
        [
            "You create its cluster and that's it.",
            "So that's the algorithm.",
            "Or actually it's most of the algorithm."
        ],
        [
            "What I didn't tell you about is what happens when you end up selecting two clusters which are connected to the same.",
            "Sorry you are.",
            "You select two people which are connected to the same non people element.",
            "You see that central element does it have to be less, you have to end up in the left people cluster or in the right people."
        ],
        [
            "So what actually happens if you want to?",
            "If you want me to tell you that you break ties at random, and in fact you do the following at the very beginning before starting the algorithm, you assign a uniform at random."
        ],
        [
            "To each element, OK, an whenever you're in a situation like this.",
            "Whenever you have an element which is connected to two or more people."
        ],
        [
            "You say you you let that element end up in the cluster or the people say with smallest idea.",
            "OK, so that's what you do."
        ],
        [
            "So this is what would happen in this case."
        ],
        [
            "So now for the analysis.",
            "Also, now I told you what the algorithm is now.",
            "Why do I claim that it's fast and that it gives a good approximation?",
            "So The thing is that we can show that this algorithm is parallel people algorithm Alves the maximum degree Delta plus after roughly log in many iterations.",
            "OK, so if you go on for log N times, log Delta plus iterations, the maximum degree will have shrunk to 0, meaning that there are no more positive edges and therefore every remaining node is a cluster by itself.",
            "That's why it's best OK."
        ],
        [
            "The actual time it takes to end to end is one over epsilon times log N times log Delta plus since Delta plus is at most N. Are bound on the running time.",
            "Use log squared over epsilon OK."
        ],
        [
            "And also so this is what guarantees that the algorithm is quick.",
            "The other thing is that this parallel people approach induces a close enough to IAD marginal distribution of that."
        ],
        [
            "And we can use this this property to still guarantee that the quality of the solution is larger than three plus epsilon times the cost of the DLP solu."
        ],
        [
            "And this gives you that the solution that you get is an expectation of three plus epsilon approximation."
        ],
        [
            "OK, now.",
            "Now you know when.",
            "Natural question is why do we sample elements with that probability?",
            "There are many possibilities.",
            "Why did we actually ended up choosing that?",
            "And it was something that we tried.",
            "A lot of them, and that was the only one that worked, and so I want to tell you quickly why the other approaches would."
        ],
        [
            "Work.",
            "So, so how can you sample these these active elements that will end up being people so well, you could say you could choose an element to become active with some probability, which is much smaller than epsilon over Delta plus.",
            "But if you do that, you end up getting too few people, and if you have too few few active nodes, then you have to few people and therefore you remove to few elements from in a single iteration and therefore you'll need to many iterations to end OK."
        ],
        [
            "If instead you make nodes become active with probability much larger than epsilon Delta plus what you end up."
        ],
        [
            "Thing is, many of the active elements will be connected to each other and therefore they will not become people.",
            "And this means that you will still end up removing too few elements per iteration.",
            "So that's the reason why we actually epsilon over Delta plus.",
            "And."
        ],
        [
            "And there are also other sampling approach which are more which are non uniform which sort of select each node with a probability that depends on that node alone and they don't work.",
            "I mean so they guarantee that the process will end up quickly, but they don't give you an approximation.",
            "So there are instances in which the solution that you end up with is very very bad.",
            "So that's why we know the epsilon over Delta plus.",
            "It doesn't have these bad properties and effect works.",
            "So."
        ],
        [
            "This is parallel people.",
            "It's a very simple algorithm.",
            "It's quite easy to implement it in Praga Laura do por or one of these frameworks."
        ],
        [
            "Also the algorithm.",
            "So I told you that the algorithm in the worst case requires log, square and iterations, But in fact, as I as I sort of highlighted during some of the previous slides, the algorithm only requires log in many iterations.",
            "If Delta Plus is a constant.",
            "So if you start with an instance where every node is similar to few nodes, then you don't need log squared, but Logan is enough."
        ],
        [
            "So now what I would like to do is to show you what actually happened when you run this algorithm on one data set.",
            "the Twitter data set which was made public by quack and others."
        ],
        [
            "It has 41 million elements which are in fact Twitter users, an 2.5 billion positive edges are positive edges, nothing else than a follow edge.",
            "So in the Twitter, as you know.",
            "Peter is a directed graph.",
            "We made it undirected and the maximum degree that the maximum positive degree that we ended up having was 2.9 millions.",
            "So Delta plus was 2.9 millions and every non follow nofollow relation was treated as a negative edge.",
            "So minus one at this similarity."
        ],
        [
            "OK, sure, So what happens is you see you have iterations on the X axis, an remaining elements, so many elements remain in the instance after those many iterations, and you see that essentially after 120 iterations the graph is, so the instances finished, you've removed every element you have clustered every element."
        ],
        [
            "And and in fact.",
            "This this other plot which you see is again, you know linear scale in the X axis.",
            "In the iteration Axis, whereas it is a log scale in the Y axis.",
            "The maximum positive degree you see at the maximum positive degree goes down logarithmically.",
            "OK. Or if you want exponentially depending on if you look at it.",
            "But so it goes down by a sort of a constant fraction at every single iteration, which is better than what our theorem proves, but our theorem is tight in the sense that there exists.",
            "After which you cannot hope to do better than what we do.",
            "OK, but in actually in real world graphs, it seems that you actually really need just log in iterations.",
            "And let's see it, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is joint work with Nilesh Dalvi that truly with Ravi Kumar at Google Mountain View.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The title we will be talking mostly about two things.",
                    "label": 0
                },
                {
                    "sent": "One is correlation clustering and the other one is map reduce.",
                    "label": 0
                },
                {
                    "sent": "In fact, not just about MapReduce, but most parallel computation frameworks.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's correlation clustering?",
                    "label": 0
                },
                {
                    "sent": "It's clustering tool or a clustering problem that has been defined by Bansal, Blum, and Shola and it has been used since then in many different data mining tasks.",
                    "label": 0
                },
                {
                    "sent": "That's a list of papers which is not comprehensive.",
                    "label": 0
                },
                {
                    "sent": "That as be using correlation clustering to find some cluster in some type of data.",
                    "label": 1
                },
                {
                    "sent": "What's this type of data?",
                    "label": 0
                },
                {
                    "sent": "Well, as you see in the picture at the bottom of the slide.",
                    "label": 0
                },
                {
                    "sent": "You have a bunch of items, the black dots an for each couple of items.",
                    "label": 0
                },
                {
                    "sent": "You're told that these items are similar or dissimilar.",
                    "label": 0
                },
                {
                    "sent": "OK, you have Vera plus one or minus one.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can think of the items as a web pages, users or ads, or actually anything that they come up to your mind.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say in the case of maybe stories, web pages, maybe you can have on your time storycnn.com story.",
                    "label": 0
                },
                {
                    "sent": "So blog post story that you could find on WordPress, Blogger or on Google Plus and then you have some machine learning algorithm that tells you whether two stories are similar or dissimilar.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your goal is to find the clustering.",
                    "label": 0
                },
                {
                    "sent": "So partition of the items into subsets that keeps together similar items and places the similar items in distinct clusters.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case it's it's a very easy problem because you could just take all the sort of right elements, put them together, all the left elements, put them together, and then you have that all the similar item, all the similar couples are together and all the dissimilar carports are not together in these different clusters.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "That's the first thing I want to say about correlation clustering.",
                    "label": 0
                },
                {
                    "sent": "I will be telling you more about that, but let's talk about the second part of the title.",
                    "label": 0
                },
                {
                    "sent": "So map reduce or more generally large scale computation.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is to find these fine very good correlation clusterings for datasets that are very large.",
                    "label": 0
                },
                {
                    "sent": "You can think of web, social or mobile data sets which are so large that you cannot hope to hold them in, say, a single computer memory and so you have to distribute them across multiple machines.",
                    "label": 1
                },
                {
                    "sent": "So say computer cluster.",
                    "label": 0
                },
                {
                    "sent": "You do that, then data processing becomes much more complicated than, say, in a single process or setting.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you want data processing to be fast to be parallel, you have many machines and also it has to have tiny memory footprint because there's no single machine that can hold all the data on its own.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are many frameworks that allow you to do that.",
                    "label": 0
                },
                {
                    "sent": "Some famous ones are MapReduce, Pregel or you know more simple setting streaming, setting the algorithm that I will be telling you about will sort of works in all of them, so it's you know framework you care about.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you can make the algorithm work on it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Dietert OK, so OK so now I gave you a bird eye view of bother things in the title.",
                    "label": 0
                },
                {
                    "sent": "Let me go a little bit deeper on what is correlation clustering and what kind of function we really trying to optimize here.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So suppose I give you these four items.",
                    "label": 0
                },
                {
                    "sent": "You see that there are some similarities between them, the left and the left items are circles, the right items are squares, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I expect say the circles to be similar to each others.",
                    "label": 0
                },
                {
                    "sent": "The squares to be similar to each other.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then maybe you know every circle to be this similar to every square, OK?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now what's the best clustering in this case?",
                    "label": 0
                },
                {
                    "sent": "Well, it's very easy if you take the left elements or by themselves and the right elements all by themselves, you get a perfect clustering, right?",
                    "label": 0
                },
                {
                    "sent": "You have zero mistakes.",
                    "label": 0
                },
                {
                    "sent": "All the similar items are together.",
                    "label": 0
                },
                {
                    "sent": "All the similar items are together like in the first slides, and in this case we actually make zero mistakes, and that's a very simple data set to cluster.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We could make some mistakes though.",
                    "label": 0
                },
                {
                    "sent": "Say we could put two similar items into different clusters or or say we could put every item on its own cluster, in which case we would make very many mistakes.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing here is that the number of clusters is not fixed in correlation clustering OK, But in fact correlation clustering sort of lets the right number of clusters for a given data set to come out of the data set itself.",
                    "label": 0
                },
                {
                    "sent": "So it's not fixed to anything.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And again, the goal of correlation clustering is to partition the element in this joint clusters.",
                    "label": 1
                },
                {
                    "sent": "So to minimize the number of mistakes.",
                    "label": 1
                },
                {
                    "sent": "So so to minimize the number of similar items that end up in different clusters and the number of these similar items that end up together.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The number of clusters does not have to be selected by end.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, this is a very simple case where you can find a perfect clustering with your mistakes, but it's not what happens in general.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's take this slightly more complicated example.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, what would you do?",
                    "label": 0
                },
                {
                    "sent": "You could plaster this way so you could take the two vertical edges together and the bottom element by itself, in which case you will make two mistakes.",
                    "label": 0
                },
                {
                    "sent": "You see the two similar.",
                    "label": 0
                },
                {
                    "sent": "The two green edges, which means similar elements, are not in the same cluster.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or you could do the following.",
                    "label": 0
                },
                {
                    "sent": "You could pick all the left elements plus the central one and the right elements on the other side.",
                    "label": 0
                },
                {
                    "sent": "That would give you a red edge.",
                    "label": 0
                },
                {
                    "sent": "Couple of these similar element that that's inside the cluster.",
                    "label": 0
                },
                {
                    "sent": "So in fact, in this case one can show that no matter how you cluster, no matter how you select the cluster, you will make at least two different mistakes, OK?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So you see the question here is not can we find the optimal cluster?",
                    "label": 1
                },
                {
                    "sent": "A cluster that puts together all the similar elements an puts in different clusters.",
                    "label": 0
                },
                {
                    "sent": "All the dissimilar elements.",
                    "label": 0
                },
                {
                    "sent": "But we want to find a cluster which is as good as possible, which makes as little mistakes as possible.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how can you do that?",
                    "label": 0
                },
                {
                    "sent": "And by how can you do that?",
                    "label": 0
                },
                {
                    "sent": "I mean how to minimize the number of mistakes?",
                    "label": 1
                },
                {
                    "sent": "How can I produce a cluster that minimizes the number of mistakes that minimum number doesn't have to be 0?",
                    "label": 0
                },
                {
                    "sent": "It can be anything.",
                    "label": 0
                },
                {
                    "sent": "It depends on the day.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recent.",
                    "label": 0
                },
                {
                    "sent": "So there is this algorithm by Island Cherry Kara Neumann, which is very simple and very elegant, which gives you not the best possible clustering, but gives you something which is close to that.",
                    "label": 0
                },
                {
                    "sent": "Now let me explain this algorithm because it's sort of the.",
                    "label": 0
                },
                {
                    "sent": "The the basis of what we will do.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So say we start from this instance.",
                    "label": 0
                },
                {
                    "sent": "The first step is pick an element P from.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instance uniformly at random.",
                    "label": 0
                },
                {
                    "sent": "So we choose that element there.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we create that cluster containing the this element P, which I will be calling the people element and all the elements similar to it.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in this case it has two similar elements and I will put all of them in in a cluster.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I will remove that cluster from the data set and I will just repeat so now.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will pick another piece.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At random and I will create its cluster OK and I will go until I will have elements to pick this way.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so this produces a clustering and.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case this clustering makes mistakes, right?",
                    "label": 0
                },
                {
                    "sent": "Like what we saw in the previous slide?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What what I launch Areekara Newman showed is that this algorithm, the people algorithm, gives unexpected three approximation to the problem of minimizing the number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "So if you run this algorithm on expectation, the number of mistakes it will make is at most three times times the minimum number you can make one on that given instance.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why can we just use this algorithm?",
                    "label": 0
                },
                {
                    "sent": "The reason is that the people algorithm is inherently sequential, and so you cannot really hope to run it on huge datasets.",
                    "label": 1
                },
                {
                    "sent": "Why's it indirectly sequential?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose I'm a very bad guy and I give you this instance so all the elements are dissimilar from each other.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's run this algorithm.",
                    "label": 0
                },
                {
                    "sent": "You will pick one people.",
                    "label": 0
                },
                {
                    "sent": "It will create a cluster containing that people and all the similar over all the elements similar to.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are no such elements, so we just remove one node and it will.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't like this?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so we have to make 10 different iterations, one for each single element, and in some of the datasets you want to cluster.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It can be huge, so hundreds of millions or 10s of millions.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's not good.",
                    "label": 0
                },
                {
                    "sent": "There's no hope you can run such an algorithm on such large data sets, and so the question we asked is how to speed up the computation.",
                    "label": 1
                },
                {
                    "sent": "Is there any hope of sort of obtaining the same approximation but in a much faster way?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I guess the answer is yes, and we propose a parallel version of the people algorithm of the algorithm that I just showed you.",
                    "label": 0
                },
                {
                    "sent": "This version only requires log square and iterations, so much less than an iterations and still guarantees are three plus epsilon approximation.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I guess the key thing here is that log square any is is much smaller than N, so that in many cases it can be.",
                    "label": 0
                },
                {
                    "sent": "It can actually be run.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moreover, the new algorithm is still easy, like the people algorithm an.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's much easy to implement it in MapReduce, Pregel or in a streaming setting.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also run this algorithm on a number of datasets, the largest which is this Twitter data set at 41 million elements, an 2.5 billion positive edges.",
                    "label": 1
                },
                {
                    "sent": "So 2.5 billion couples of elements that were similar to each other.",
                    "label": 0
                },
                {
                    "sent": "So now let me let me tell you about this this algorithm and then we can discuss it.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the algorithm works as the people are going away, so it goes on up until the instance is not empty up until you still have some elements to cluster an at the very beginning of an iteration.",
                    "label": 0
                },
                {
                    "sent": "What you do is you compute the maximum positive degree of an element in the remaining list.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if this is the instance that we're running our app.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rhythm on we can see that the maximum positive degree is to write.",
                    "label": 1
                },
                {
                    "sent": "If you take in all the elements, the one that has the largest number of positive edges, it's too right.",
                    "label": 0
                },
                {
                    "sent": "It's anyone but the rightmost ones.",
                    "label": 0
                },
                {
                    "sent": "So you compute this Delta plus.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now you activate each element independently with probability, which is roughly one over Delta plus a little bit smaller, say epsilon over Delta plus, and that's what you do.",
                    "label": 0
                },
                {
                    "sent": "You do all this in parallel, so you flip a coin for each of the elements an if it says with probability epsilon over Delta plus you activate the Tele.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So say we activate these four elements.",
                    "label": 0
                },
                {
                    "sent": "OK, all the elements, but the central ones.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we have to make sort of step that keeps everything consistent.",
                    "label": 0
                },
                {
                    "sent": "So we have to deactivate the active elements that are connected through positive edge to other active elements.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This case, it means that the leftmost elements have to be deactivated because you see the way we want the active elements to be the people, and we don't want people to be close to each other right in the original algorithm, each you pick a single people at a time and then people created a cluster so we can.",
                    "label": 0
                },
                {
                    "sent": "If you have two people that are both connected to each other, then how can you create the cluster which people goes into cluster which other people?",
                    "label": 0
                },
                {
                    "sent": "So if this happens you deactivate both of them and you just hope that it doesn't happen to often.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we deactivate these two elements and the remaining active nodes become people.",
                    "label": 0
                },
                {
                    "sent": "OK, once you have them.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you create one cluster for each people.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so you create these two clusters like in the previous algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you repeat now.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classes one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're lucky you activate that element.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You create its cluster and that's it.",
                    "label": 0
                },
                {
                    "sent": "So that's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Or actually it's most of the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I didn't tell you about is what happens when you end up selecting two clusters which are connected to the same.",
                    "label": 0
                },
                {
                    "sent": "Sorry you are.",
                    "label": 0
                },
                {
                    "sent": "You select two people which are connected to the same non people element.",
                    "label": 0
                },
                {
                    "sent": "You see that central element does it have to be less, you have to end up in the left people cluster or in the right people.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what actually happens if you want to?",
                    "label": 0
                },
                {
                    "sent": "If you want me to tell you that you break ties at random, and in fact you do the following at the very beginning before starting the algorithm, you assign a uniform at random.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To each element, OK, an whenever you're in a situation like this.",
                    "label": 0
                },
                {
                    "sent": "Whenever you have an element which is connected to two or more people.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You say you you let that element end up in the cluster or the people say with smallest idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what you do.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what would happen in this case.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now for the analysis.",
                    "label": 0
                },
                {
                    "sent": "Also, now I told you what the algorithm is now.",
                    "label": 0
                },
                {
                    "sent": "Why do I claim that it's fast and that it gives a good approximation?",
                    "label": 0
                },
                {
                    "sent": "So The thing is that we can show that this algorithm is parallel people algorithm Alves the maximum degree Delta plus after roughly log in many iterations.",
                    "label": 1
                },
                {
                    "sent": "OK, so if you go on for log N times, log Delta plus iterations, the maximum degree will have shrunk to 0, meaning that there are no more positive edges and therefore every remaining node is a cluster by itself.",
                    "label": 0
                },
                {
                    "sent": "That's why it's best OK.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The actual time it takes to end to end is one over epsilon times log N times log Delta plus since Delta plus is at most N. Are bound on the running time.",
                    "label": 0
                },
                {
                    "sent": "Use log squared over epsilon OK.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also so this is what guarantees that the algorithm is quick.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that this parallel people approach induces a close enough to IAD marginal distribution of that.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can use this this property to still guarantee that the quality of the solution is larger than three plus epsilon times the cost of the DLP solu.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this gives you that the solution that you get is an expectation of three plus epsilon approximation.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "Now you know when.",
                    "label": 0
                },
                {
                    "sent": "Natural question is why do we sample elements with that probability?",
                    "label": 1
                },
                {
                    "sent": "There are many possibilities.",
                    "label": 0
                },
                {
                    "sent": "Why did we actually ended up choosing that?",
                    "label": 0
                },
                {
                    "sent": "And it was something that we tried.",
                    "label": 0
                },
                {
                    "sent": "A lot of them, and that was the only one that worked, and so I want to tell you quickly why the other approaches would.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "So, so how can you sample these these active elements that will end up being people so well, you could say you could choose an element to become active with some probability, which is much smaller than epsilon over Delta plus.",
                    "label": 0
                },
                {
                    "sent": "But if you do that, you end up getting too few people, and if you have too few few active nodes, then you have to few people and therefore you remove to few elements from in a single iteration and therefore you'll need to many iterations to end OK.",
                    "label": 1
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If instead you make nodes become active with probability much larger than epsilon Delta plus what you end up.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing is, many of the active elements will be connected to each other and therefore they will not become people.",
                    "label": 0
                },
                {
                    "sent": "And this means that you will still end up removing too few elements per iteration.",
                    "label": 1
                },
                {
                    "sent": "So that's the reason why we actually epsilon over Delta plus.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are also other sampling approach which are more which are non uniform which sort of select each node with a probability that depends on that node alone and they don't work.",
                    "label": 0
                },
                {
                    "sent": "I mean so they guarantee that the process will end up quickly, but they don't give you an approximation.",
                    "label": 0
                },
                {
                    "sent": "So there are instances in which the solution that you end up with is very very bad.",
                    "label": 0
                },
                {
                    "sent": "So that's why we know the epsilon over Delta plus.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have these bad properties and effect works.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is parallel people.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's quite easy to implement it in Praga Laura do por or one of these frameworks.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So I told you that the algorithm in the worst case requires log, square and iterations, But in fact, as I as I sort of highlighted during some of the previous slides, the algorithm only requires log in many iterations.",
                    "label": 0
                },
                {
                    "sent": "If Delta Plus is a constant.",
                    "label": 0
                },
                {
                    "sent": "So if you start with an instance where every node is similar to few nodes, then you don't need log squared, but Logan is enough.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now what I would like to do is to show you what actually happened when you run this algorithm on one data set.",
                    "label": 0
                },
                {
                    "sent": "the Twitter data set which was made public by quack and others.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It has 41 million elements which are in fact Twitter users, an 2.5 billion positive edges are positive edges, nothing else than a follow edge.",
                    "label": 0
                },
                {
                    "sent": "So in the Twitter, as you know.",
                    "label": 0
                },
                {
                    "sent": "Peter is a directed graph.",
                    "label": 0
                },
                {
                    "sent": "We made it undirected and the maximum degree that the maximum positive degree that we ended up having was 2.9 millions.",
                    "label": 1
                },
                {
                    "sent": "So Delta plus was 2.9 millions and every non follow nofollow relation was treated as a negative edge.",
                    "label": 0
                },
                {
                    "sent": "So minus one at this similarity.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, sure, So what happens is you see you have iterations on the X axis, an remaining elements, so many elements remain in the instance after those many iterations, and you see that essentially after 120 iterations the graph is, so the instances finished, you've removed every element you have clustered every element.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And and in fact.",
                    "label": 0
                },
                {
                    "sent": "This this other plot which you see is again, you know linear scale in the X axis.",
                    "label": 0
                },
                {
                    "sent": "In the iteration Axis, whereas it is a log scale in the Y axis.",
                    "label": 0
                },
                {
                    "sent": "The maximum positive degree you see at the maximum positive degree goes down logarithmically.",
                    "label": 1
                },
                {
                    "sent": "OK. Or if you want exponentially depending on if you look at it.",
                    "label": 0
                },
                {
                    "sent": "But so it goes down by a sort of a constant fraction at every single iteration, which is better than what our theorem proves, but our theorem is tight in the sense that there exists.",
                    "label": 0
                },
                {
                    "sent": "After which you cannot hope to do better than what we do.",
                    "label": 0
                },
                {
                    "sent": "OK, but in actually in real world graphs, it seems that you actually really need just log in iterations.",
                    "label": 0
                },
                {
                    "sent": "And let's see it, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}