{
    "id": "weac34vtdnrtoimjrlsziujv5ogek5je",
    "title": "Foundations: Models and Methods of IR",
    "info": {
        "author": [
            "Hinrich Sch\u00fctze, Institute for Natural Language Processing, University of Stuttgart"
        ],
        "published": "Sept. 20, 2011",
        "recorded": "August 2011",
        "category": [
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/essir2011_schutze_foundations/",
    "segmentation": [
        [
            "Thank you very much for this very kind introduction, so I think my job here is very tough because I'm supposed to lecture on models and methods, so this is supposed to be the foundation.",
            "The foundational lecture for information retrieval, and I think it is necessary so that we all have a common basis here for the summer school, but it's difficult because you are a very heterogeneous audience, some already have, or we know a lot about information retrieval.",
            "And for some of you this is your first exposure to it so.",
            "It's difficult to give a lecture to such a heterogeneous audience, but I think."
        ],
        [
            "So what I decided to do was to talk about six of the most important models in information retrieval, the Boolean model and its limitations, the vector space model, probabilistic models language model based retrieval, Layton, semantic indexing and learning to rank, and all of these models are important either because they are kind of the workhorses of doing information retrieval and a lot of research is conducted in them, or because they are important reference points that you will.",
            "Need to know about when you do your own research.",
            "So let's start with."
        ],
        [
            "The Boolean and so the plan is to cover each model in 30 minutes.",
            "I will see what I can do that, if not then we'll just drop some stuff at the end.",
            "Yes.",
            "Oh, that's a very good idea.",
            "Thank you.",
            "Can you hear me better now, no.",
            "Hello better OK great.",
            "Let's see.",
            "This.",
            "OK, Boolean retrieval.",
            "And."
        ],
        [
            "Its limitations.",
            "So in this picture I want to cover the Boolean model and inverted index very briefly.",
            "Missing Boolean queries and then why is Boolean retrieval not enough, or why do we need ranked retrieval?"
        ],
        [
            "Quick definition of information retrieval.",
            "Information retrieval is finding material, usually documents of an unstructured nature, usually text, that satisfies an information need from within large collections, usually stored on computers.",
            "So it's about finding.",
            "Usually documents unstructured.",
            "And information is so we have a user in information retrieval who we want to satisfy and large collections because it's only interesting if you have a large collection.",
            "By the way, unstructured for me means it would include the semantic web as soon as something like language or video is concerned.",
            "I would call that unstructured.",
            "This kind of traditional definition of information retrieval.",
            "And as the week progresses people will go beyond this traditional.",
            "Definition and talk about multimedia and about the social, web, and so on.",
            "And the."
        ],
        [
            "Additional this traditional definition.",
            "That means that we are addressing the ad hoc retrieval problem.",
            "That's what it's called.",
            "Given the user information need and a collection of documents, the IR system determines how well the documents satisfy the query and returns a subset of relevant documents to the user.",
            "So that is the ad hoc retrieval problem, and that's most of what I'm going to cover in my 2 lecture."
        ],
        [
            "William retrieval the Boolean models, arguably the simplest model to base an information retrieval system on.",
            "Queries are simply boolean expressions like Caesar and Brutus.",
            "And the search engine returns all documents that satisfy the Boolean expr."
        ],
        [
            "Let's start with the.",
            "Let's use a model collection here.",
            "So we have some examples.",
            "Shakespeare.",
            "Each of Shakespeare's tragedies and comedies and so on, is a document in this collection."
        ],
        [
            "The basic representation of the collection we use in the Boolean model is this term incidence matrix, so it has these terms as rows of the matrix and the documents as columns.",
            "As I said, every tragedy and comedy is a document here and one in the matrix means that the corresponding term occurs in the document."
        ],
        [
            "And a zero means that the corresponding term does not occur in the document, so that is the basic representation we are using in the Boolean MoD."
        ],
        [
            "And we will return to this matrix several times today as we go to different models that use different represe."
        ],
        [
            "Tations now we can build this incidence matrix for large collections because the size of it is number of documents, times, number of terms and that's too large for large collections.",
            "So what we'll use instead is we're only going to use the ones in the matrix because there are few ones and most of the other entries are zeros.",
            "That is, it is a sparse matrix and the inverted index is basically the data structure that only recalls the."
        ],
        [
            "This is the inverted index for each term TV show.",
            "A list of all documents that contain T. And for each, that's equivalent to saying we store the ones and its role in the incidence matrix.",
            "So the inverted index consists of the terms.",
            "That's the dictionary, the dictionary data structure, and for each term we have a list of postings, the postings list and that we call all documents that the tournament person.",
            "So Brutus occurs in one documents 124, eleven, and so on.",
            "So in fact, in the next consists of a dictionary and the postings."
        ],
        [
            "How do we process Boolean queries in this infrastrux?"
        ],
        [
            "After in this data structure, let's take an example.",
            "A simple conjunctive conjunctive query like routers and Calpurnia.",
            "To find all matching documents using the inverted index, we first locate Brutus and the dictionary.",
            "Then we retrieve its postings list from the postings file.",
            "Then we do the same for Calpurnia.",
            "And then we intersect the two postings lists and return the intersection to the use."
        ],
        [
            "This shows you."
        ],
        [
            "How that?",
            "Works out in for these too."
        ],
        [
            "Postings lists, so we find 2 is a common."
        ],
        [
            "Document in the true post."
        ],
        [
            "Lists."
        ],
        [
            "Then 30."
        ],
        [
            "1."
        ],
        [
            "And that's the result."
        ],
        [
            "This is linear in the length of the postings lists, so this is a very efficient way of processing Boolean queries that works for most building queries."
        ],
        [
            "The example was a simple conjunctive query.",
            "The Boolean retrieval model can answer any query that is a Boolean expression, not just conjunctive queries where building queries are queries that use an or not to join query terms.",
            "We view each document here as a set of terms.",
            "That's what it means to use a binary incidence matrix here.",
            "And the Boolean model is very precise.",
            "A document either matches or it does not.",
            "The booty model was the primary commercial retrieval tool for three decades.",
            "In the early days of information retrieval.",
            "And many professional searches still like it in patents, for example, or in the legal profession, because you know exactly what you are getting.",
            "And many such systems you are using also a Boolean.",
            "For example, the search system on your laptop or in your email reader or on many Internet search engines.",
            "These are Boolean queries.",
            "So the question, I'll be done for today or for the week 'cause we have a very simple retrieval system here that seems to do something interesting.",
            "An.",
            "I mean, there's a little bit more to it in terms of efficiency and so on, but I think what I've told you in the last five or 10 minutes is basically half of what you need to do.",
            "Do about a simple Boolean retrieval model."
        ],
        [
            "Now, the reason it's not enough is that William Retrieval, in its simplest form, is not ranked, and we need ranked retrieval."
        ],
        [
            "So.",
            "Here are the pros and cons of Boolean of the Boolean model.",
            "The key property is.",
            "Documents either match or they don't match.",
            "This is good for expert users with a precise understanding of their needs and of the collection.",
            "And it's also good for applications because applications can easily process thousands of results.",
            "But it's not good for the majority of users.",
            "Most users are either not capable of writing Boolean queries.",
            "If you think about your mother or grandmother or Gran father, or they are, but they think it's too much work.",
            "If you have a choice between writing a complicated Boolean query or just typing in some key words into a search engine, I think a lot of you would choose just typing in the keyboards.",
            "And most users don't want to wait through thousands of results, and this is particularly true of web."
        ],
        [
            "One way of characterizing this problem with Boolean searches to call it the feast or famine problem.",
            "Boolean queries often result in either too few or too many results.",
            "So here's a query that I ran a long time ago on a Boolean web search engine.",
            "I had a network card that wasn't working and I got an error message for it.",
            "The error message started with the words standard user dealing 650, so I wanted to know why is my network card not working and I typed it into the search engine and I got back 200,000 hits.",
            "So that is fixed.",
            "So I made the query more specific and added three words.",
            "No card found that was part of the error message and as a result I got zero hits, that's famine.",
            "In Boolean retrieval, it takes a lot of skill to come up with a query that produces a manageable number of hits, something that lies between 200,000 and zero.",
            "Ideally, you want to look at 5:10 or 20 hits, but not at 20,000, and you want to have at least one hit, and where the Boolean model.",
            "That's very difficult to do."
        ],
        [
            "But useful family is no problem in ranked retrieval becausw with ranking large result sets are not an issue.",
            "You just show the top 10 results and the user won't be over well.",
            "The premise, of course, is that the ranking algorithm works more.",
            "Relevant results are ranked higher than less relevant results, and that's why a lot of information retrieval research is about ranking and about which different factors, including page rank and social data and so on.",
            "You can use for good ranking."
        ],
        [
            "OK, now, so it's supposed to be about models and methods.",
            "So now I want to say a little bit about methods.",
            "How can we study?",
            "Information retrieval in the wild, and I want to show you some data on empirical investigation of the effect of ranking.",
            "So how can we measure how important ranking is?",
            "We can observe what searchers do when they are searching in a controlled setting.",
            "Videotape them, ask them to think aloud.",
            "Interview them.",
            "I tracked them, time them recording counter clicks and this is something that people do in user experience studies and I want to show you one example of that by Dan Russell from his 2007 J CDL talk and then Russell at the time was the Uber technique for search quality and user happen is that group?"
        ],
        [
            "So what you do in this type of study is I don't.",
            "I guess you know, maybe you can see it.",
            "So this year is a searcher who was invited to come into the Google Labs and this is a researcher user experience Searcher who tries to improve the user experience of web search and so the search is running your web search here and is looking at the results.",
            "And then here's an interview where the user experience researcher tries to find out how well the search is working.",
            "So the researchers are asking, did you notice the FTD official site and the search for the users answers?",
            "To be honest, I didn't even look at that.",
            "At first I saw from $20 and $20 was looking for to be honest.",
            "One internal Flowers is what I'm familiar with and by event there next, even though I kind of assumed they wouldn't have $20 Flowers.",
            "So it gives you a very detailed.",
            "Impression of it's a very detailed investigation of how a user interacted with the search result."
        ],
        [
            "And one aspect of that is too I track where the user looks at.",
            "So here the query is children's unicycle and these were the results and the user first looked at.",
            "This resolved and this then this then this and went back up and back down.",
            "They went back up here because they two and four kind of similar.",
            "So they once they hit four they went back to compare it to two and then they went further down the list.",
            "So you can use this type of study to get a very detailed record of what people do when they interact with research."
        ],
        [
            "Adults.",
            "And his data that are very relevant to ranking by ranking is important.",
            "This shows you how many users look at how many abstracts and abstract is simply the snippets you get with the search result.",
            "So if you get a list of 10 blue links on your web search engine, then it's not just the link but also a summary of the link.",
            "And so the question is how many of those summaries do people look at?",
            "And the statistics show you that.",
            "In this case, it was counted per session or per search result.",
            "In 100 cases, people looked at only one summary.",
            "In 120 cases, almost two summaries and so on.",
            "So you can see in most people.",
            "In most cases people only look at one, 2, three or four different summaries, yes.",
            "Does that also include zero?",
            "You know, look up.",
            "Uh, he I don't know.",
            "There are cases where people don't look at any, so I guess it's surprising that 0 doesn't appear here, but I'm not sure whether maybe it was so infrequent that they didn't put it there.",
            "So this is the first indication of the importance of ranking people only look at very few summary, so it's important to be very selective about what you show at the prominent places on the page."
        ],
        [
            "And this also takes into account clicking, not just looking so the.",
            "The purple data is what we just saw that people look at one, 2, three or four abstracts, but not really more than that.",
            "Black is how often they click on a link and you can see that it's even more skewed for that.",
            "In this case almost three times as many people click on the result ranked number one, then on the result rank #2.",
            "So ranking is very important in the sense that people click on the top ranked result."
        ],
        [
            "And you can actually fool people here.",
            "We have a result for a regular search ranking where the 1st result is relevant and the 2nd is not relevant.",
            "And in that case it's not surprising that most people click on the 1st result.",
            "But then if you switch around the true so now the first one is not relevant but the second the second is the relevant one.",
            "People still take on the first one even though it is not relevant.",
            "So you can see that some people automatically click on the first one because they have developed some.",
            "Trust in the search engines and the search engines will actually present the most relevant result first."
        ],
        [
            "So summary importance of ranking viewing abstracts users are lot more likely to read the abstracts of the Top Rank pages than the abstracts of the lower rank pages.",
            "Clicking the distribution is even more skewed for clicking.",
            "There's a very strong bias to click on the Top Rank page.",
            "Even if the Top Rank page is not relevant, 30% of users will click on it.",
            "So getting the ranking is very important and getting the top ranked page right is most important, and so that's supposed to serve as motivation for for a ranking in information retrieval is an important problem."
        ],
        [
            "OK, so that was the model inverted index processing, building queries and why do we need ranked retrieval?",
            "Maybe I can ask her?"
        ],
        [
            "So I put some resources on our books siteinformationretrieval.org so you for this lecture, here you find there a list of useful information resources Shakespeare search engine that is Boolean and Daniel Russell's home page where you can look up the original slides that I of the user study that I showed here.",
            "Maybe I can ask a quick question."
        ],
        [
            "Many of you, I think, use Max and so I assumed you spotlight, and I'm not sure whether on PC's as a similar search system is Spotlight, a Boolean search system.",
            "No no why not?",
            "I believe it will count the number of opening or something this phone number of user Mail instead."
        ],
        [
            "Right, so it's important to distinguish between a pure Boolean system where things are where everything is presented that that satisfies the Boolean query.",
            "But then some ordering is imposed on that.",
            "I mean, whether it, yeah, whether some other audio and ordering is imposed on this set or not, so spotlight imposes an ordering like you suggested that I think it's actually the most recent opening or a combination of the most recent opening at the number of openings and.",
            "And many other search engines use use use time.",
            "For example, if you search for citations, often you get the most recent citations on Medline, for example, the most recent citations first, but they are Boolean engine's in the sense that a document that does not contain all the keywords is not included in the result list, and Google also was initially a search engine like that it was Boolean in the sense that only documents that contained all the search terms were included in the result list, but then.",
            "And ordering was imposed on that.",
            "Any questions about this point or anything?"
        ],
        [
            "In this part of the lecture.",
            "OK then, let's continue with the vector space model.",
            "Truly."
        ],
        [
            "OK, a quick review of TF IDF rating.",
            "Then the vector space model which represents queries and documents in high dimensional space and then pivot normalization.",
            "That's an alternative to cosine normalization that removes a bias inherent in standard length normalization.",
            "The reason I included this is that again, it's a very nice methodological point if your system is not doing what you wanted to do.",
            "How can you analyze it and how?",
            "How can you improve it?",
            "This is a nice example of that."
        ],
        [
            "So we started with this binary instructs.",
            "These are the terms or words.",
            "By the way.",
            "I'm using term and word interchangeably here, and these are the documents.",
            "And now we're moving from a binary to."
        ],
        [
            "Incidence matrix."
        ],
        [
            "To a count."
        ],
        [
            "Matrix, where each document is represented as a vector of counts.",
            "So now we want to use the account information.",
            "The fact that sometimes occurs several times in the document because that is important."
        ],
        [
            "Nation for ranking.",
            "We use the term frequency for that and that is defined as the number of times that T occurs.",
            "Indeed, that is the term frequency TF TD.",
            "We want to rank documents according to query document matching scores and use this TF as a component in these matching scores.",
            "But how?",
            "We could use raw term frequency, simply the count, but that's not what we want, cause a document with 10 occurrences of the term is more relevant than a document with one occurrence.",
            "We could surmise, but not type 10 times more relevant.",
            "Relevance does not increase proportionately with strong frequency, so we're trying to find some criteria for ranking, but we don't want to do it proportionally becausw.",
            "We need to have some.",
            "We need to be very careful about how we include this information in the ranking."
        ],
        [
            "So instead of raw frequency you're going to use log frequency rating.",
            "That's defined as follows.",
            "If the term frequency rate is greater than zero, then the log frequency rate is 1 plus log TFD and 0 otherwise.",
            "And so that has the nice property that it transforms a raw frequency of 0 into 01 into 121 point 310 into two 1000 into 4.",
            "So you can see that the log is obviously very strong dampening function here.",
            "The matching score for a document query pair is then simply the sum of all all terms that occur in both query and document of this term frequency rate.",
            "So now we have our first ranking function.",
            "This is already a ranking function that does better than just an unordered set of documents."
        ],
        [
            "In addition to term frequency, the frequency of determine the document, we also want to use the frequency of the term in the collection for waiting and ranking."
        ],
        [
            "Um?",
            "So the F is defined as the document frequency as the number of documents that he Akerson.",
            "OK. That's an inverse measure of the informativeness of a Trump because very frequent term terms cannot distinguish between different documents because they occur very frequently.",
            "That's why we use inverse document frequency idea, which is a direct measure of the informativeness of the of the term, and that's defined as follows log of N divided by DF.",
            "Where N is the number of documents in the collection, so this is the inverse inverse document frequency rate.",
            "Again, we use the logarithm to dampen the effect of ideas.",
            "Again, we don't want a proportional influence of this measure, because that would be too strong."
        ],
        [
            "Here's some examples.",
            "So if a word occurs in every single document of the collection, then IDF is 0.",
            "So that means that term will be ignored because it cannot serve to distinguish between one document and another, and the highest idea we get for a term that only occurs in one document.",
            "That would be log off the number of documents."
        ],
        [
            "What is the effect of idea on ranking?",
            "IDF gives high weights, too rare terms like correctness centric.",
            "That's my favorite restaurant, dragon centric.",
            "And it gets low weights too frequent words like good increase in line and hopefully everybody gets the intuition that a term like Iraq concentric is more informative than a term then terms like good increase online.",
            "Although these terms are still important and we want to use them for ranking.",
            "IDF affects the ranking of documents for queries with at least two terms.",
            "For example, in the query iraqna centric line, IDF greatly increases the relative rate of reaction centric and decreases the relative grade of line.",
            "IDF has little effect on ranking for one term queries.",
            "So it only has an effect if we have more."
        ],
        [
            "One turn.",
            "Summary in IDF and TF IDF weighting you sign a TF IDF grade for each term T in each document D. This is the log frequency.",
            "The frequency compute component.",
            "This is the idea of component.",
            "The TF IDF weight increases with the number of occurrences within the document and increases with the rarity of the term in the collection."
        ],
        [
            "OK, the vector space model.",
            "So we went from."
        ],
        [
            "I'm expect us to come."
        ],
        [
            "Vectors and now we are going to real valued vectors."
        ],
        [
            "Where this vector is a vector of TF IDF weights.",
            "And now we're going to look at the vector as a whole and look at his its properties."
        ],
        [
            "So each document is now represented as a real valued vector of TF IDF weights.",
            "In this Space, V is the vocabulary, so we have a very high dimensional space.",
            "AV dimensional real valued vector space?",
            "Terms are X axis of the space.",
            "So each word is an axis of the space and documents are points or vectors in this space.",
            "It's a very high dimensional space, can be up to 10s of millions of dimensions when you apply this to a web search engine.",
            "But each individual vector for a document is very sparse.",
            "Most of the entries in the vector of zeros."
        ],
        [
            "Now the key idea one for the vector space model is that we do the same for queries.",
            "We represent them as vectors in the high dimensional space, just as we represent documents.",
            "And the key idea 2 is that we rank documents according to their proximity to the query.",
            "Where proximity is roughly similarity or negative distance in the sense that proximity and distance are opposites of each other.",
            "Recall we're doing this because we want to get away from the you're either in or out feast or famine Boolean models.",
            "So now we have something that supports graded relevance.",
            "Instead, we rank relevant documents higher than not relevant documents using this proximi."
        ],
        [
            "Now, how do we formalize vector space similarity this proximity?",
            "The first cut would be negative distance between two points.",
            "The distance between the end points of the two vectors.",
            "Should we use Euclidean distance?",
            "No, that would be a bad idea because Euclidean distance is large for vectors of different lengths."
        ],
        [
            "That's why we can't use it.",
            "And you can see this in this graph here so.",
            "This here is a very simple vector space with just two words.",
            "The words are poor and rich, and I've put three documents in this space.",
            "Do you want you 2D3D1 ranks of solving codes?",
            "Well, that's a document where the word poor dominates and rich doesn't occur.",
            "Then we have the document record baseball salaries in 2010.",
            "In this document, the word rich dominates and put us in occur and then we have a document D2 rich poor gap grows.",
            "We're both rich and poor occur our query, which is as I said, also represented in this vector space is rich poor.",
            "Now if we use the Euclidean distance to rank documents here, then the one MP3 would actually be closer closer to the query then D2.",
            "Because if you look at look at distance, then D 190, three are closer to the query than D2.",
            "But that's not the result we want, because the information that we get out of this vector space is the direction of the vector, the direction of the vector tells us about the mixture of terms that occur in the document and their relative importance, and that's the information we want to use, not the length of the vector.",
            "The length of the vector doesn't really give us any useful information for ranking.",
            "So that's why we can't use Euclidean distance."
        ],
        [
            "Instead, we use the angle for ranking so we rank documents according to the angle with the query.",
            "The following two notions are equivalent rank documents according to the angle between query and documented decreasing order or rank documents according to cosine theory document, in increasing order that simply follows out of the monotonicity of the coastline with respect to the angle.",
            "So we will do the ranking according to the cosine and then we will actually use direction and things that are close in direction to the query will be ranked higher than things that are that are pointing in a different direction."
        ],
        [
            "This year is the formula for the cosine, so you can either call a cosine or similarity percent similarity, and if you remember that fun linear from linear algebra, it's simply the dot product of the normalized query vector and the normalized document vector.",
            "And this is the definition of the dot product.",
            "For those of you who don't remember that Qi here is the TF IDF rate of termite in the query.",
            "DI is the TF IDF rate of term I in the document.",
            "Q&D are the lengths of community and this is the cosine similarity, or equivalently the cosine of the angle between Q&T.",
            "Um?",
            "Yeah, this you have to.",
            "Remember from linear algebra, so this is what we're going to use for ranking in the vector space model, the cosine similarity."
        ],
        [
            "And so another way of looking at it is that what we're basically doing is we're putting we're making all vectors equally long and putting them all on the hyper sphere.",
            "That's what the underlying math is doing."
        ],
        [
            "OK, so I've told you about 1 rating scheme, cave, IDF rating, logarithmic term, frequency rating.",
            "Idea of document waiting and the cosine normalization.",
            "There are many others.",
            "I'm not going to go through them here, but yeah, there's there's many other ways of doing it, but this is probably the most best known one, and the others are variants that are interesting, one that's more interesting than the others I think, is pivoted length normalization so that that's what we'll do next."
        ],
        [
            "I think I'm going to skip this example for those of you have never seen if IDF weighting in action can go through this slide and see how you compute a final similarity score between a query in the document."
        ],
        [
            "It's all on the slides.",
            "OK pivot, pivot length normalization.",
            "Here's a problem for the cosine normalization that I just presented.",
            "Let's let's look at this query here.",
            "Antidoping rules Beijing 2008 Olympics and let's say we have three documents.",
            "Ivan is a short document on anti doping rules at the 2008 Olympics.",
            "D2 is a long document that consists of a copy of the one in five other news stories on topics different from Olympics and Olympics and anti doping.",
            "And then we have T3.",
            "That's a short document on anti doping rules at the 2004 F and sudden lympics.",
            "This is not working.",
            "Ranking to be expected in the vector space model.",
            "Here, if we use cosine normalization, there's somebody that.",
            "I think there are.",
            "How many there are six factor factorial possible ranking so which one could we get?",
            "Any takers?",
            "123 any other opinions?",
            "Tooth last one.",
            "That's the correct answer.",
            "Yeah, two is the last one wise to the last one.",
            "Because it's long, yes.",
            "Right, because the normalization divides by all the stuff that's in the document and most of the stuff in the document is not relevant to the query.",
            "So we have a long document here that contains a little bit about about enter doping rules speech in 2008 Olympics.",
            "But relative to the length of the document it's very little and that's why the two role will be ranked last, whereas the three, even though it's not.",
            "It only contains partially the right answer here, or I mean it's actually not relevant relative to the length of the three.",
            "It has a lot more relevant information, at least that's what the vector space model thinks."
        ],
        [
            "So this is an example for a problem.",
            "Of course I normalization 'cause the normalization produces rates that are too large for short documents and too small for long documents on average.",
            "So we need to adjust cosine normalization and one way to do that is a linear adjustment, and I'm going to present one particular way of doing that now.",
            "Which is used called pivot normalization.",
            "The effect of this will be that similarities of short documents with the query will decrease and similarities of long documents with the query will increase.",
            "And this removes the unfair advantage that short documents have.",
            "As I said, part of the reason I'm presenting this is because it's interesting methodologically.",
            "If your system is not working, what can you do to?"
        ],
        [
            "And this is basically the graph that I mean by that.",
            "So this is a graph that shows you this is document length and this is the probability of relevance of documents of a particular document length.",
            "The red curve here is true relevance.",
            "Unfortunately, the scales are not clear, but let's say these are documents of length 30 or something and this might be 2%.",
            "So of all documents of length, 32% are truly relevant.",
            "And this curve rises because the longer document is, the more chances it has of being relevant to more queries.",
            "So overall the curve rises for that reason.",
            "Now, this is the prediction of cosine and you can see that cosine overestimates the relevance of short documents and underestimates the relevance of long documents, and it's precisely for the reason that we saw earlier that.",
            "In this example.",
            "So we have to correct that, and the way we correct that.",
            "A simple way of correcting that is to just turn this curve around to turn it around this pivot.",
            "The pivot is the point where the two relevance and the estimated relevance intersect, so that's what we're going to."
        ],
        [
            "Next so this is here cosine normalization, standard cosine normalization.",
            "And instead of using this cosine normalization factor, we're going to use a pivoted normalization factor.",
            "This one.",
            "So we're going to increase the normalization factor for short documents and decrease it for long documents, because increasing it for subdocuments means since the normalization factor is in the nominator, the number, the similarity number will decrease.",
            "And if you do."
        ],
        [
            "That then.",
            "You these are some results that show you that you get a clear improvement in this experiment of 10% in relevance if you remove this bias from cosine norm."
        ],
        [
            "Station.",
            "OK summary.",
            "Ranked retrieval in the vector space model we represent each document as rated TF IDF vector.",
            "We represent the query also as a beta TF IDF vector.",
            "We compute the cosine similarity between the query vector and each document vector, or Alternatively we use pivot normalization.",
            "Then we rank the documents with respect to the query and return the top K to the user.",
            "OK."
        ],
        [
            "So that was two ideas, waiting vector space model pivot normalization.",
            "Any questions?",
            "Yes.",
            "Normalized black what is your feeling?",
            "This is not good anymore.",
            "Good question, once they are."
        ],
        [
            "Normalized, you can use Euclidean distance, yes, but.",
            "I mean.",
            "You're basically doing the same thing.",
            "Either you can directly compute the cosine, which which implicitly includes normalization, length normalization, or you can do length normalization and then rank according to Euclidean distance so.",
            "On the hyper sphere, Euclidean distance and cosine normalization ranking our cosine ranking are equivalent.",
            "That is true.",
            "Does that answer your question?",
            "Yes.",
            "Presentation wow.",
            "You know?",
            "The problem is enough problem.",
            "Either.",
            "Not the query is founding document within an collection of documents or it does not appear.",
            "I think also in back with this model, it's the same.",
            "Actually the difference is that here we have rank.",
            "I mean for example, if yes, I agree, but it's more.",
            "I mean this related to feast or famine problem.",
            "I mean in Boolean model you don't really have a choice of including too many query Trump's, because if you have a conjunctive interpretation of your query, once you include too many query words then your result set is going to be 0.",
            "So all of this is under the assumption that you're willing to write longer queries and that the longer queries are going to be interpreted.",
            "Well.",
            "Other questions.",
            "The previous slide instead.",
            "That idea thanks only ranking of the Curry which has at least in terms yes.",
            "Does it mean that I can remove?",
            "Computation of IDF value that you can do what with the computation of there.",
            "For one time queries, yes.",
            "Basically IDF is not doing any work, I mean it's doing work actually be cause in the normalization of the of the documents it is actually having a certain effect, so you will get different results even for one term queries.",
            "If IDF ranking is used for the documents.",
            "But a big effect, you will only get if you have queries of at least length 2.",
            "Yes.",
            "Normalizations you showed up, so there is a magic angle like people are supposed to find.",
            "Yeah, understand this has been done only based on user testing.",
            "That's correct, yes, and that's a typical way people do that information retrieval.",
            "They come up with a nice method, but then there's a parameter that has to be tuned.",
            "In this case, the parameter that has to be tuned is the pivot angle, and that is determined empirically.",
            "Yes, the way you suggest.",
            "The user base for this crafting folder.",
            "Partner.",
            "For this graph, but this was done on one of the standard evaluation collections in information retrieval, which is called track, and so on.",
            "It was estimated on held out data and then applied to unseen data, and then they showed that it improves the results.",
            "Yes.",
            "The documents referenced images.",
            "It is a general rule that selection dependent that's football collection using dialogue with the document frequency is a good idea because we tried this by example with the collision image 5th 2008 and we find that when we use the look the the result is very.",
            "It's very bad.",
            "It's very good.",
            "That surprises me.",
            "I would works.",
            "It's a good idea in general if you have long documents.",
            "But maybe we can talk about that offline.",
            "I mean, it is possible that.",
            "Yeah.",
            "I think it's possible to find collection fair doesn't work, but I would think it works pretty poorly for most collections."
        ],
        [
            "OK, how much more time do I have?",
            "Another half hour or one hour.",
            "OK, quick.",
            "Then we're doing Valentine.",
            "So.",
            "The.",
            "3rd part for this morning is probabilistic information retrieval."
        ],
        [
            "Probabilistic models"
        ],
        [
            "So what I would want to cover in this sector is probabilistic approach to our introduction and then two models.",
            "The binary independence model that is the first influential probabilistic model that is kind of a reference point for probabilistic."
        ],
        [
            "Models and information retrieval.",
            "An Okapi Game 25 or more modern, better performing."
        ],
        [
            "Holistic model"
        ],
        [
            "This year was the ad hoc retrieval problem.",
            "The traditional information retrieval problem.",
            "Given the user information need and a collection of documents the our system must determine how well the documents satisfy the query.",
            "The our system has an uncertain understanding of the user query.",
            "This is the distinction between information need and user query.",
            "The user types in some keywords that is not a complete description of what the user wants, right?",
            "So that means the other system can only have an uncertain understanding of what the user really wants.",
            "NPR system makes an uncertain guess of whether a document satisfied satisfies the query because it's difficult to.",
            "To make an assessment, even for for human, and it's even more difficult for our system.",
            "Probability theory provides a principled foundation for such reasoning under uncertainty.",
            "Probabilistic IR models exploit this foundation to estimate how likely it is that a document is relevant to a query.",
            "So what this slide is trying to say is.",
            "IR is feeling a problem of uncertainty, so why aren't we using the mathematical theory of uncertainty?"
        ],
        [
            "So that's an attempt here to do that.",
            "Difference between probabilistic and vector space models.",
            "The vector space model ranks documents according to similarity to the query.",
            "The notion of similarity does not translate directly to an assessment of is the document a good document to give to the user or not?",
            "The most similar document can be highly relevant or completely non relevant.",
            "I mean in the extreme case, if you have no answer to your query in the collection then there will still be a most similar document.",
            "Probability theory is arguably a cleaner formalization of what we really want an IR system to do.",
            "Give relevant documents to the user because we are.",
            "Ideally we will have a direct assessment of relevance, that's the."
        ],
        [
            "The goal.",
            "The classical probabilistic retrieval models are binary independence model and Okapi BM 25.",
            "I'm going to cover cover them both now.",
            "Then there's also Bayesian networks for text retrieval.",
            "I don't have time for this, and then there are language model approaches to IR and that's going to be the first lecture after lunch."
        ],
        [
            "The rank rule set up is the user is a query and a ranked list of documents is returned.",
            "How can we rank probabilistically?",
            "First we introduce a random dichotomous variable RDQ.",
            "And that's one if the document is relevant to with respect to the query and 0 otherwise.",
            "This is a binary notion of relevance.",
            "Probabilistic ranking orders documents decreasingly by their estimated probability of elements with respect to this probability.",
            "The probability of relevance given a document and a query.",
            "How can we justify this way of proceeding?",
            "It seems kind of intuitive, but let's dig into this a little."
        ],
        [
            "But further.",
            "And the justification that is usually given in information retrieval is the probabilistic.",
            "The probability ranking principle.",
            "If the retrieved documents are ranked increasingly on their probability of relevance, then the effectiveness of the system will be the best that is obtainable.",
            "That's not really a theorem that's derived from anything, it just follows from the definition of probability of relevance.",
            "Because if you rank documents according to probability of relevance, and then you have to make a decision which document are going, am I going to put next in this rank list then?",
            "It's obvious that you have to choose the document that has the highest relevance, highest probability of relevance, 'cause that's most likely to satisfy the user.",
            "Right, so the probability ranking principle is simply a way of stating this.",
            "This fact that follows directly out of the definition of probability of relevance.",
            "The fundamental assumption here is that the relevance of each document is independent of the relevance of other documents.",
            "That's actually not true.",
            "Why is that not true?",
            "In this context, in the context of a ranked list.",
            "So already seeing pieces of information, maybe a reiteration of the same information, because we had to get it right.",
            "I mean, extreme example is if you have duplicates, so you put a duplicate and #3 and now you have a duplicate that your system estimates to be as relevant as the other duplicate.",
            "And of course you don't want to put the duplicate there, 'cause it gives no additional information to the user.",
            "But that's very difficult to handle in ranked retrieval model.",
            "So usually we ignore it.",
            "And we do some post processing to remove duplicates on YouTube."
        ],
        [
            "So the first probabilistic model that is based on this is the binary."
        ],
        [
            "This model.",
            "It's binary documents and queries are represented as binary term incidence vectors, and independence means that times are independent of each other.",
            "That's the assumption which is the naive base assumption.",
            "That we are making."
        ],
        [
            "Binary representation simply means binary incidence matrix, so as in the Boolean model we represent the collection in this form and only consider binary information."
        ],
        [
            "We start by applying Bayes rule.",
            "So here we have the probability of relevance given X and Q&A threatening here for the two cases just to be very clear and this is not simply based rule.",
            "Recall here that documented query are modeled as term incidence vectors and we write them as X&Q here.",
            "So this is the term incidence vector of the document and this is the term incidence vector of the query.",
            "What do these expressions on the right side mean?",
            "The probability of X given equals one Q.",
            "This is the probability that for a given query, if I know I'm relevant, what is the probability I'm going to get a particular document incidence vector?",
            "At the same for the case I = 0.",
            "And now we will use statistics about the document collection to estimate this probability here."
        ],
        [
            "This number here so that was this expression.",
            "Now what is this here P of R = 1 given Q, that is simply the prior probability of retrieving a relevant or non relevant document for query Q.",
            "We estimate that from the percentage of relevant documents in the collection.",
            "So the question is simply for this query, what relative, what proportion of documents are relevant for it in non relevant for it?"
        ],
        [
            "I just said that we're going to rank documents according to this probability of relevance.",
            "Actually, it's easier to rank according to the odds of relevance.",
            "So instead of ranking according to this, we're going to rank according to this 'cause if you whether you rank according to P, the probability or P / 1 -- P, The odds of the probability is the same.",
            "So we're going to use this for ranking because it makes things simpler and then applying Bayes rule.",
            "We get this expression.",
            "Now this here is a constant becausw the probability of the odds of relevance given the query are not different within one ranking because one ranking is always for one query, right?",
            "So this is going to be a constant for that query.",
            "So all we need to do for ranking is consider this factor."
        ],
        [
            "So here's our factor.",
            "This is going to be the fact that we're going to use for ranking, and now we're making the naive Bayes conditional independence assumption.",
            "But that is saying is that the probability of getting this term incidence vector given relevance and a query is the same as the probability of all the individual indicators given relevance in the query.",
            "So our assumption is that.",
            "The terms are all independent of each other in the document given the query.",
            "And given relevance, given a particular relevance.",
            "So now we rank according to this year.",
            "Of course this naive Bayes independence assumption is not true.",
            "I mean, what would be a simple argument or a simple example to show that it's not true?",
            "Yep.",
            "United States, right?",
            "So if I mean my favorite example is Hong Kong, because United States also occurs in many other, you know, those two words also occur in many other contexts.",
            "But Hong Kong in English?",
            "Well, Kong I guess can also occur in King Kong, but mostly they occur in that combination Hong Kong.",
            "So if you know Hong occurs in the document, it's pretty certain that Kong also occurs, and vice versa.",
            "So terms are not independent of each other.",
            "Um but.",
            "This assumption works really well and at the end I have a link that to a paper that explains why it's working so well even though it is not very intuitive.",
            "It's very nice work."
        ],
        [
            "Pedro domingos so.",
            "Right, so we want to rank according to this.",
            "Now we're going to do a few more arithmetic manipulations.",
            "So this is the binary indicator that tells me that T the trumpty occurs in the document or not zero or one.",
            "And we can separate this into.",
            "The product into the cases where the term occurs in the document and the term doesn't occur in the document, so this is simply."
        ],
        [
            "Dividing up the factors and now we introduce some notation.",
            "We call the probability that the term occurs given relevance PT.",
            "And the probability that the term occurs given non relevance UT.",
            "So this is the probability that the tongue occurs if the documents relevant and this is the probability that term occurs if the document is non relevant.",
            "And this is comes from this contingency table here.",
            "Relevance non relevance to impresence term absence.",
            "This is PTUT 1 -- P T 1 -- U T. Now we can write the ranking function, eat more compactly as this."
        ],
        [
            "And.",
            "One final simplifying assumption.",
            "If Q T = 0, then PT equals UT.",
            "In other words, in other words, a term not occurring in the query is equally likely to occur in relevant and non relevant documents.",
            "So if a term doesn't occur in the query, our assumption is it's equally likely that it occurs, involvement or non relevant documents.",
            "Now we need only to consider terms in the products that appear in the query.",
            "So now this was the expression before and now I've added the constraint that QT equals one.",
            "That is, the term occurs in the query in both products.",
            "And I think I need two or three slides more, but then there's going to be a really nice interpretation of this.",
            "So this is right.",
            "So the last thing I think this is certainly the last arithmetic manipulation we have to do is that we let this one overall.",
            "It's the overall query terms and not just the ones that don't occur in the document.",
            "That means we have to add some 1 minus P 1 -- P / 1 -- U factors here and then also multiply the reciprocals."
        ],
        [
            "That looks like this and the right product is not overall query terms, hence constant for a particular query and can be ignored.",
            "Again, we're ranking for a particular query this term here will be the same for each document, so we can omit it.",
            "And this is then the only quantity we need for ranking.",
            "Since logs and some so easier to deal with than multiplication, we actually use the log of this for ranking."
        ],
        [
            "And finally we write it a little bit differently, like this, and this is the thing that has a very nice interpretation.",
            "So if you look at this and this and you see that this is a simple arithmetic manipulation, so this has the following very nice interpretation.",
            "This is an odds ratio that is a ratio of two odds.",
            "This here is the odds of the term appearing in the document.",
            "If the document is relevant and this year is the odds of the term appearing if the document is non relevant.",
            "OK, so.",
            "And see T is the difference between the two.",
            "Now, if this city is 0, then the odds of the term appearing in relevant documents are the same as the odds of the term appearing in non relevant documents and that means this term does not give us any information about the document, whether it's relevant or not relevant, and in that case it is zero and it does not add anything to the retrieval status value.",
            "If City is positive, then that means that you have higher odds of the term appearing in relevant documents then of the term appearing on relevant documents and that means this term is evidenced for relevance.",
            "Positive evidence for relevance and then it adds something positive to the retrieval status value.",
            "And finally, if we have higher odds of the term appearing non relevant documents in relevant documents, then it's negative evidence and it will subtract something from the retrieval status value.",
            "So that's really.",
            "And so the final formula for ranking is that we add up all these setae that occur in the document.",
            "Right, and so this slide really is, I think the main reason that that you might prefer a probabilistic model to the vector space model becausw you actually know to some extent what these term rates mean.",
            "In the vector space model, I've been through a lot of motivation for why we did this or that with TF IDF, but in the end it's not really clear what these values mean with probabilistic model.",
            "At least, you have a pretty nice interpretation of of what the term rate for a term means and where it comes from."
        ],
        [
            "So the retrieval service value in this probabilistic pin model is simply the sum over all terms occurring in the document of the term rates.",
            "So been vector space model is similar on an operational level.",
            "In particular, we can use the same data structures like the inverted index for the two models.",
            "The main difference is really in how the term rates are computed."
        ],
        [
            "And this is how you would actually compute the trumpets.",
            "In practice, you compute a contingency table for every term tone presence from absence relevant non relevant service, for example, would be the number of documents that contain the term that are relevant, and so on.",
            "And from these numbers you can then compute utpd CTCTS that run rate and use this formula, there's one."
        ],
        [
            "Complication, but just we want to avoid zeros if any of the counts is a zero, then the term rate is not well defined.",
            "And in that case, the reason is that maximum likelihood estimates.",
            "That's what we're using here effectively do not work for events to avoid zeros, we add 0.5 to each count.",
            "That's called expected likelihood estimation, or we can use any other type of smoothing we want to use, avoid zeros."
        ],
        [
            "OK um.",
            "Right, so we can actually.",
            "I just talked about the vector space model.",
            "Now I want to make an even more direct comparison between the bin and the vector space model.",
            "Because if we make one more simplifying assumption, we'll see how the two are actually very similar.",
            "If you make the assumption that relevant documents are very small, percentage of the collection.",
            "So for any given query, most documents are not relevant, right, so?",
            "The proportion of relevant documents for any given query is pretty small.",
            "If you make this assumption, then we can actually approximate this term of the term weight by log N divided by DF.",
            "So the definition of this you can look this up on the slides in minus DF divided by DF.",
            "And if the number of documents any term across in is pretty small, then we get.",
            "This log N divided by ideas and this should look look familiar because this is simply the definition of the IDF rate.",
            "So this means that one part of this of the term rate city is actually a systematic or a. Derivation from first principles of the IDF rate log N divided by DF."
        ],
        [
            "OK, so how do we actually apply the bin model for for our different tasks?",
            "If we have relevance feedback where the user has given us some relevance judgments, so user the user has told us for some documents this document is relevant for the query.",
            "This document is not.",
            "Then we can directly compute term rates based on the can."
        ],
        [
            "Agency table so we use this table that are just introduced and we're done."
        ],
        [
            "For ad hoc retrieval it's more complicated because the assumption and not awkward travel is that we have a query, but we don't have a single relevant structure meant for any document.",
            "In that case, we have to make yet another assumption.",
            "And the assumption is that PT is 0.5 for all terms XD in the queries.",
            "So each query term is equally likely to occur in a relevant document, and so the PT involvement is factors cancel out in the expression for RSV.",
            "That's a weak estimate, but doesn't disagree violently with expectation that query terms appear in many, but not all relevant documents.",
            "So if we make this assumption, then the CT rate is actually just IDF.",
            "Because in that case P 0.5, this becomes zero and this we had just approximated by by the idea of weight.",
            "So the simplest probabilistic model is actually just IDF weighting and the difference to the vector space model is just that we now have a principle derivation of this idea freight and for short documents.",
            "This simple version of been actually works pretty well, and that's where it was developed, because in the very very beginning of information retrieval in the 50s and 60s there were no full length documents online, so the only things that were available electronically were titles.",
            "And very short abstracts.",
            "And in those titles and short abstracts.",
            "If a time occurs, it only across once in most cases, and therefore term frequency is not important because you literally have only a binary distinction between a term either occurring or not occurring.",
            "Tone frequency does not contribute any information in that case is in those cases just using IDF is sufficient."
        ],
        [
            "OK."
        ],
        [
            "Then I'll conclude with the Okapi BM 25 model Okapi BM 25 is a probabilistic model that incorporates term frequency and length normalization.",
            "As I just said, Beam was originally designed for short catalog records so fairly consistent link and it works reasonably in these contexts.",
            "But for modern fulltext search collections, the model should pay attention to term Frequency, document link and that's what BM 25 does."
        ],
        [
            "So the starting point is this is the simplest form of been the simplest form of BIM was IDF weighting.",
            "So we simply sum the idea of rates for the terms that are current query and document."
        ],
        [
            "What to copy adds to that is this expression here.",
            "And I'm not going to go through the details of the derivation, but I'm just going to give you some intuitions what these terms mean.",
            "So first the terminology is TF is as before term frequency.",
            "LD is the length of document, D&LF is the average document length in the whole collection.",
            "K1 is a tuning parameter controlling the scaling of term frequency.",
            "And B is a tuning parameter controlling the scaling by document length.",
            "Now you can see how this works by just playing around with some values for these numbers.",
            "Here.",
            "Let's say we have B = 0.",
            "That means we don't take length into account, and in that case the formula simply becomes Cape K 1 + 1 / K One plus TF.",
            "Now if you put.",
            "K10 here, then.",
            "This cancels out.",
            "In that case the model defaults back to the simple BIM model.",
            "So K10 means simple BIM model.",
            "If you let gave one K1 go to Infinity then you can see that what we get will be TF the raw frequency, because then if one goes to Infinity then K1 and K1 will cancel each other out in just TF.",
            "Remains.",
            "So for zero we get the simple BIM model for Infinity.",
            "We get raw frequency rating where TF enters into the equation proportionally and any values between zero and Infinity will will have effects, intermediate effects.",
            "So K1 is a way of tuning how much.",
            "You want to give a term frequency.",
            "And be it works very similarly for length in the extreme case, if we set P20, then length is ignored.",
            "If we set beta one, then we have a factor here that will penalize long documents.",
            "So a long document will will be penalized and short documents will be preferred."
        ],
        [
            "OK. That's basically is it so motivation of the probabilistic approach, binary dependence model, or coffee bean?"
        ],
        [
            "In five and on the website I've put the original paper that introduced the binary dependence model by Robertson and Spark Jones.",
            "More details on or copy M 25.",
            "And the paper I mentioned earlier that explains why the naive Bayes independence assumption works, even though it is so naive."
        ],
        [
            "I guess we already answered that question so.",
            "Are there any other questions?",
            "You can also.",
            "Ask questions about the other two lectures.",
            "I guess otherwise we would just.",
            "Go to lunch.",
            "For."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much for this very kind introduction, so I think my job here is very tough because I'm supposed to lecture on models and methods, so this is supposed to be the foundation.",
                    "label": 0
                },
                {
                    "sent": "The foundational lecture for information retrieval, and I think it is necessary so that we all have a common basis here for the summer school, but it's difficult because you are a very heterogeneous audience, some already have, or we know a lot about information retrieval.",
                    "label": 0
                },
                {
                    "sent": "And for some of you this is your first exposure to it so.",
                    "label": 0
                },
                {
                    "sent": "It's difficult to give a lecture to such a heterogeneous audience, but I think.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I decided to do was to talk about six of the most important models in information retrieval, the Boolean model and its limitations, the vector space model, probabilistic models language model based retrieval, Layton, semantic indexing and learning to rank, and all of these models are important either because they are kind of the workhorses of doing information retrieval and a lot of research is conducted in them, or because they are important reference points that you will.",
                    "label": 1
                },
                {
                    "sent": "Need to know about when you do your own research.",
                    "label": 0
                },
                {
                    "sent": "So let's start with.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Boolean and so the plan is to cover each model in 30 minutes.",
                    "label": 0
                },
                {
                    "sent": "I will see what I can do that, if not then we'll just drop some stuff at the end.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Oh, that's a very good idea.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Can you hear me better now, no.",
                    "label": 0
                },
                {
                    "sent": "Hello better OK great.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "OK, Boolean retrieval.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Its limitations.",
                    "label": 0
                },
                {
                    "sent": "So in this picture I want to cover the Boolean model and inverted index very briefly.",
                    "label": 0
                },
                {
                    "sent": "Missing Boolean queries and then why is Boolean retrieval not enough, or why do we need ranked retrieval?",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quick definition of information retrieval.",
                    "label": 1
                },
                {
                    "sent": "Information retrieval is finding material, usually documents of an unstructured nature, usually text, that satisfies an information need from within large collections, usually stored on computers.",
                    "label": 1
                },
                {
                    "sent": "So it's about finding.",
                    "label": 0
                },
                {
                    "sent": "Usually documents unstructured.",
                    "label": 0
                },
                {
                    "sent": "And information is so we have a user in information retrieval who we want to satisfy and large collections because it's only interesting if you have a large collection.",
                    "label": 0
                },
                {
                    "sent": "By the way, unstructured for me means it would include the semantic web as soon as something like language or video is concerned.",
                    "label": 0
                },
                {
                    "sent": "I would call that unstructured.",
                    "label": 0
                },
                {
                    "sent": "This kind of traditional definition of information retrieval.",
                    "label": 0
                },
                {
                    "sent": "And as the week progresses people will go beyond this traditional.",
                    "label": 0
                },
                {
                    "sent": "Definition and talk about multimedia and about the social, web, and so on.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Additional this traditional definition.",
                    "label": 0
                },
                {
                    "sent": "That means that we are addressing the ad hoc retrieval problem.",
                    "label": 0
                },
                {
                    "sent": "That's what it's called.",
                    "label": 0
                },
                {
                    "sent": "Given the user information need and a collection of documents, the IR system determines how well the documents satisfy the query and returns a subset of relevant documents to the user.",
                    "label": 1
                },
                {
                    "sent": "So that is the ad hoc retrieval problem, and that's most of what I'm going to cover in my 2 lecture.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "William retrieval the Boolean models, arguably the simplest model to base an information retrieval system on.",
                    "label": 1
                },
                {
                    "sent": "Queries are simply boolean expressions like Caesar and Brutus.",
                    "label": 1
                },
                {
                    "sent": "And the search engine returns all documents that satisfy the Boolean expr.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's start with the.",
                    "label": 0
                },
                {
                    "sent": "Let's use a model collection here.",
                    "label": 0
                },
                {
                    "sent": "So we have some examples.",
                    "label": 0
                },
                {
                    "sent": "Shakespeare.",
                    "label": 0
                },
                {
                    "sent": "Each of Shakespeare's tragedies and comedies and so on, is a document in this collection.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The basic representation of the collection we use in the Boolean model is this term incidence matrix, so it has these terms as rows of the matrix and the documents as columns.",
                    "label": 0
                },
                {
                    "sent": "As I said, every tragedy and comedy is a document here and one in the matrix means that the corresponding term occurs in the document.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a zero means that the corresponding term does not occur in the document, so that is the basic representation we are using in the Boolean MoD.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we will return to this matrix several times today as we go to different models that use different represe.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tations now we can build this incidence matrix for large collections because the size of it is number of documents, times, number of terms and that's too large for large collections.",
                    "label": 1
                },
                {
                    "sent": "So what we'll use instead is we're only going to use the ones in the matrix because there are few ones and most of the other entries are zeros.",
                    "label": 0
                },
                {
                    "sent": "That is, it is a sparse matrix and the inverted index is basically the data structure that only recalls the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the inverted index for each term TV show.",
                    "label": 1
                },
                {
                    "sent": "A list of all documents that contain T. And for each, that's equivalent to saying we store the ones and its role in the incidence matrix.",
                    "label": 1
                },
                {
                    "sent": "So the inverted index consists of the terms.",
                    "label": 0
                },
                {
                    "sent": "That's the dictionary, the dictionary data structure, and for each term we have a list of postings, the postings list and that we call all documents that the tournament person.",
                    "label": 0
                },
                {
                    "sent": "So Brutus occurs in one documents 124, eleven, and so on.",
                    "label": 0
                },
                {
                    "sent": "So in fact, in the next consists of a dictionary and the postings.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we process Boolean queries in this infrastrux?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After in this data structure, let's take an example.",
                    "label": 0
                },
                {
                    "sent": "A simple conjunctive conjunctive query like routers and Calpurnia.",
                    "label": 1
                },
                {
                    "sent": "To find all matching documents using the inverted index, we first locate Brutus and the dictionary.",
                    "label": 1
                },
                {
                    "sent": "Then we retrieve its postings list from the postings file.",
                    "label": 1
                },
                {
                    "sent": "Then we do the same for Calpurnia.",
                    "label": 0
                },
                {
                    "sent": "And then we intersect the two postings lists and return the intersection to the use.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This shows you.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How that?",
                    "label": 0
                },
                {
                    "sent": "Works out in for these too.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Postings lists, so we find 2 is a common.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Document in the true post.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lists.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then 30.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's the result.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is linear in the length of the postings lists, so this is a very efficient way of processing Boolean queries that works for most building queries.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The example was a simple conjunctive query.",
                    "label": 0
                },
                {
                    "sent": "The Boolean retrieval model can answer any query that is a Boolean expression, not just conjunctive queries where building queries are queries that use an or not to join query terms.",
                    "label": 0
                },
                {
                    "sent": "We view each document here as a set of terms.",
                    "label": 0
                },
                {
                    "sent": "That's what it means to use a binary incidence matrix here.",
                    "label": 0
                },
                {
                    "sent": "And the Boolean model is very precise.",
                    "label": 0
                },
                {
                    "sent": "A document either matches or it does not.",
                    "label": 0
                },
                {
                    "sent": "The booty model was the primary commercial retrieval tool for three decades.",
                    "label": 0
                },
                {
                    "sent": "In the early days of information retrieval.",
                    "label": 0
                },
                {
                    "sent": "And many professional searches still like it in patents, for example, or in the legal profession, because you know exactly what you are getting.",
                    "label": 0
                },
                {
                    "sent": "And many such systems you are using also a Boolean.",
                    "label": 0
                },
                {
                    "sent": "For example, the search system on your laptop or in your email reader or on many Internet search engines.",
                    "label": 0
                },
                {
                    "sent": "These are Boolean queries.",
                    "label": 0
                },
                {
                    "sent": "So the question, I'll be done for today or for the week 'cause we have a very simple retrieval system here that seems to do something interesting.",
                    "label": 0
                },
                {
                    "sent": "An.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a little bit more to it in terms of efficiency and so on, but I think what I've told you in the last five or 10 minutes is basically half of what you need to do.",
                    "label": 0
                },
                {
                    "sent": "Do about a simple Boolean retrieval model.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, the reason it's not enough is that William Retrieval, in its simplest form, is not ranked, and we need ranked retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here are the pros and cons of Boolean of the Boolean model.",
                    "label": 0
                },
                {
                    "sent": "The key property is.",
                    "label": 0
                },
                {
                    "sent": "Documents either match or they don't match.",
                    "label": 0
                },
                {
                    "sent": "This is good for expert users with a precise understanding of their needs and of the collection.",
                    "label": 0
                },
                {
                    "sent": "And it's also good for applications because applications can easily process thousands of results.",
                    "label": 0
                },
                {
                    "sent": "But it's not good for the majority of users.",
                    "label": 0
                },
                {
                    "sent": "Most users are either not capable of writing Boolean queries.",
                    "label": 0
                },
                {
                    "sent": "If you think about your mother or grandmother or Gran father, or they are, but they think it's too much work.",
                    "label": 0
                },
                {
                    "sent": "If you have a choice between writing a complicated Boolean query or just typing in some key words into a search engine, I think a lot of you would choose just typing in the keyboards.",
                    "label": 0
                },
                {
                    "sent": "And most users don't want to wait through thousands of results, and this is particularly true of web.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way of characterizing this problem with Boolean searches to call it the feast or famine problem.",
                    "label": 0
                },
                {
                    "sent": "Boolean queries often result in either too few or too many results.",
                    "label": 0
                },
                {
                    "sent": "So here's a query that I ran a long time ago on a Boolean web search engine.",
                    "label": 0
                },
                {
                    "sent": "I had a network card that wasn't working and I got an error message for it.",
                    "label": 0
                },
                {
                    "sent": "The error message started with the words standard user dealing 650, so I wanted to know why is my network card not working and I typed it into the search engine and I got back 200,000 hits.",
                    "label": 0
                },
                {
                    "sent": "So that is fixed.",
                    "label": 0
                },
                {
                    "sent": "So I made the query more specific and added three words.",
                    "label": 0
                },
                {
                    "sent": "No card found that was part of the error message and as a result I got zero hits, that's famine.",
                    "label": 0
                },
                {
                    "sent": "In Boolean retrieval, it takes a lot of skill to come up with a query that produces a manageable number of hits, something that lies between 200,000 and zero.",
                    "label": 0
                },
                {
                    "sent": "Ideally, you want to look at 5:10 or 20 hits, but not at 20,000, and you want to have at least one hit, and where the Boolean model.",
                    "label": 0
                },
                {
                    "sent": "That's very difficult to do.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But useful family is no problem in ranked retrieval becausw with ranking large result sets are not an issue.",
                    "label": 0
                },
                {
                    "sent": "You just show the top 10 results and the user won't be over well.",
                    "label": 0
                },
                {
                    "sent": "The premise, of course, is that the ranking algorithm works more.",
                    "label": 0
                },
                {
                    "sent": "Relevant results are ranked higher than less relevant results, and that's why a lot of information retrieval research is about ranking and about which different factors, including page rank and social data and so on.",
                    "label": 0
                },
                {
                    "sent": "You can use for good ranking.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now, so it's supposed to be about models and methods.",
                    "label": 0
                },
                {
                    "sent": "So now I want to say a little bit about methods.",
                    "label": 0
                },
                {
                    "sent": "How can we study?",
                    "label": 0
                },
                {
                    "sent": "Information retrieval in the wild, and I want to show you some data on empirical investigation of the effect of ranking.",
                    "label": 0
                },
                {
                    "sent": "So how can we measure how important ranking is?",
                    "label": 0
                },
                {
                    "sent": "We can observe what searchers do when they are searching in a controlled setting.",
                    "label": 0
                },
                {
                    "sent": "Videotape them, ask them to think aloud.",
                    "label": 0
                },
                {
                    "sent": "Interview them.",
                    "label": 0
                },
                {
                    "sent": "I tracked them, time them recording counter clicks and this is something that people do in user experience studies and I want to show you one example of that by Dan Russell from his 2007 J CDL talk and then Russell at the time was the Uber technique for search quality and user happen is that group?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what you do in this type of study is I don't.",
                    "label": 0
                },
                {
                    "sent": "I guess you know, maybe you can see it.",
                    "label": 0
                },
                {
                    "sent": "So this year is a searcher who was invited to come into the Google Labs and this is a researcher user experience Searcher who tries to improve the user experience of web search and so the search is running your web search here and is looking at the results.",
                    "label": 0
                },
                {
                    "sent": "And then here's an interview where the user experience researcher tries to find out how well the search is working.",
                    "label": 0
                },
                {
                    "sent": "So the researchers are asking, did you notice the FTD official site and the search for the users answers?",
                    "label": 0
                },
                {
                    "sent": "To be honest, I didn't even look at that.",
                    "label": 0
                },
                {
                    "sent": "At first I saw from $20 and $20 was looking for to be honest.",
                    "label": 0
                },
                {
                    "sent": "One internal Flowers is what I'm familiar with and by event there next, even though I kind of assumed they wouldn't have $20 Flowers.",
                    "label": 0
                },
                {
                    "sent": "So it gives you a very detailed.",
                    "label": 0
                },
                {
                    "sent": "Impression of it's a very detailed investigation of how a user interacted with the search result.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one aspect of that is too I track where the user looks at.",
                    "label": 0
                },
                {
                    "sent": "So here the query is children's unicycle and these were the results and the user first looked at.",
                    "label": 1
                },
                {
                    "sent": "This resolved and this then this then this and went back up and back down.",
                    "label": 0
                },
                {
                    "sent": "They went back up here because they two and four kind of similar.",
                    "label": 0
                },
                {
                    "sent": "So they once they hit four they went back to compare it to two and then they went further down the list.",
                    "label": 0
                },
                {
                    "sent": "So you can use this type of study to get a very detailed record of what people do when they interact with research.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Adults.",
                    "label": 0
                },
                {
                    "sent": "And his data that are very relevant to ranking by ranking is important.",
                    "label": 1
                },
                {
                    "sent": "This shows you how many users look at how many abstracts and abstract is simply the snippets you get with the search result.",
                    "label": 0
                },
                {
                    "sent": "So if you get a list of 10 blue links on your web search engine, then it's not just the link but also a summary of the link.",
                    "label": 0
                },
                {
                    "sent": "And so the question is how many of those summaries do people look at?",
                    "label": 0
                },
                {
                    "sent": "And the statistics show you that.",
                    "label": 0
                },
                {
                    "sent": "In this case, it was counted per session or per search result.",
                    "label": 0
                },
                {
                    "sent": "In 100 cases, people looked at only one summary.",
                    "label": 0
                },
                {
                    "sent": "In 120 cases, almost two summaries and so on.",
                    "label": 0
                },
                {
                    "sent": "So you can see in most people.",
                    "label": 0
                },
                {
                    "sent": "In most cases people only look at one, 2, three or four different summaries, yes.",
                    "label": 0
                },
                {
                    "sent": "Does that also include zero?",
                    "label": 0
                },
                {
                    "sent": "You know, look up.",
                    "label": 0
                },
                {
                    "sent": "Uh, he I don't know.",
                    "label": 0
                },
                {
                    "sent": "There are cases where people don't look at any, so I guess it's surprising that 0 doesn't appear here, but I'm not sure whether maybe it was so infrequent that they didn't put it there.",
                    "label": 0
                },
                {
                    "sent": "So this is the first indication of the importance of ranking people only look at very few summary, so it's important to be very selective about what you show at the prominent places on the page.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this also takes into account clicking, not just looking so the.",
                    "label": 0
                },
                {
                    "sent": "The purple data is what we just saw that people look at one, 2, three or four abstracts, but not really more than that.",
                    "label": 0
                },
                {
                    "sent": "Black is how often they click on a link and you can see that it's even more skewed for that.",
                    "label": 0
                },
                {
                    "sent": "In this case almost three times as many people click on the result ranked number one, then on the result rank #2.",
                    "label": 0
                },
                {
                    "sent": "So ranking is very important in the sense that people click on the top ranked result.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can actually fool people here.",
                    "label": 0
                },
                {
                    "sent": "We have a result for a regular search ranking where the 1st result is relevant and the 2nd is not relevant.",
                    "label": 0
                },
                {
                    "sent": "And in that case it's not surprising that most people click on the 1st result.",
                    "label": 0
                },
                {
                    "sent": "But then if you switch around the true so now the first one is not relevant but the second the second is the relevant one.",
                    "label": 0
                },
                {
                    "sent": "People still take on the first one even though it is not relevant.",
                    "label": 0
                },
                {
                    "sent": "So you can see that some people automatically click on the first one because they have developed some.",
                    "label": 0
                },
                {
                    "sent": "Trust in the search engines and the search engines will actually present the most relevant result first.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So summary importance of ranking viewing abstracts users are lot more likely to read the abstracts of the Top Rank pages than the abstracts of the lower rank pages.",
                    "label": 0
                },
                {
                    "sent": "Clicking the distribution is even more skewed for clicking.",
                    "label": 0
                },
                {
                    "sent": "There's a very strong bias to click on the Top Rank page.",
                    "label": 0
                },
                {
                    "sent": "Even if the Top Rank page is not relevant, 30% of users will click on it.",
                    "label": 0
                },
                {
                    "sent": "So getting the ranking is very important and getting the top ranked page right is most important, and so that's supposed to serve as motivation for for a ranking in information retrieval is an important problem.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that was the model inverted index processing, building queries and why do we need ranked retrieval?",
                    "label": 0
                },
                {
                    "sent": "Maybe I can ask her?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I put some resources on our books siteinformationretrieval.org so you for this lecture, here you find there a list of useful information resources Shakespeare search engine that is Boolean and Daniel Russell's home page where you can look up the original slides that I of the user study that I showed here.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can ask a quick question.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Many of you, I think, use Max and so I assumed you spotlight, and I'm not sure whether on PC's as a similar search system is Spotlight, a Boolean search system.",
                    "label": 0
                },
                {
                    "sent": "No no why not?",
                    "label": 0
                },
                {
                    "sent": "I believe it will count the number of opening or something this phone number of user Mail instead.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so it's important to distinguish between a pure Boolean system where things are where everything is presented that that satisfies the Boolean query.",
                    "label": 0
                },
                {
                    "sent": "But then some ordering is imposed on that.",
                    "label": 0
                },
                {
                    "sent": "I mean, whether it, yeah, whether some other audio and ordering is imposed on this set or not, so spotlight imposes an ordering like you suggested that I think it's actually the most recent opening or a combination of the most recent opening at the number of openings and.",
                    "label": 0
                },
                {
                    "sent": "And many other search engines use use use time.",
                    "label": 0
                },
                {
                    "sent": "For example, if you search for citations, often you get the most recent citations on Medline, for example, the most recent citations first, but they are Boolean engine's in the sense that a document that does not contain all the keywords is not included in the result list, and Google also was initially a search engine like that it was Boolean in the sense that only documents that contained all the search terms were included in the result list, but then.",
                    "label": 0
                },
                {
                    "sent": "And ordering was imposed on that.",
                    "label": 0
                },
                {
                    "sent": "Any questions about this point or anything?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this part of the lecture.",
                    "label": 0
                },
                {
                    "sent": "OK then, let's continue with the vector space model.",
                    "label": 0
                },
                {
                    "sent": "Truly.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, a quick review of TF IDF rating.",
                    "label": 0
                },
                {
                    "sent": "Then the vector space model which represents queries and documents in high dimensional space and then pivot normalization.",
                    "label": 0
                },
                {
                    "sent": "That's an alternative to cosine normalization that removes a bias inherent in standard length normalization.",
                    "label": 0
                },
                {
                    "sent": "The reason I included this is that again, it's a very nice methodological point if your system is not doing what you wanted to do.",
                    "label": 0
                },
                {
                    "sent": "How can you analyze it and how?",
                    "label": 0
                },
                {
                    "sent": "How can you improve it?",
                    "label": 0
                },
                {
                    "sent": "This is a nice example of that.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we started with this binary instructs.",
                    "label": 0
                },
                {
                    "sent": "These are the terms or words.",
                    "label": 0
                },
                {
                    "sent": "By the way.",
                    "label": 0
                },
                {
                    "sent": "I'm using term and word interchangeably here, and these are the documents.",
                    "label": 0
                },
                {
                    "sent": "And now we're moving from a binary to.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Incidence matrix.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To a count.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matrix, where each document is represented as a vector of counts.",
                    "label": 1
                },
                {
                    "sent": "So now we want to use the account information.",
                    "label": 0
                },
                {
                    "sent": "The fact that sometimes occurs several times in the document because that is important.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation for ranking.",
                    "label": 0
                },
                {
                    "sent": "We use the term frequency for that and that is defined as the number of times that T occurs.",
                    "label": 0
                },
                {
                    "sent": "Indeed, that is the term frequency TF TD.",
                    "label": 0
                },
                {
                    "sent": "We want to rank documents according to query document matching scores and use this TF as a component in these matching scores.",
                    "label": 0
                },
                {
                    "sent": "But how?",
                    "label": 0
                },
                {
                    "sent": "We could use raw term frequency, simply the count, but that's not what we want, cause a document with 10 occurrences of the term is more relevant than a document with one occurrence.",
                    "label": 0
                },
                {
                    "sent": "We could surmise, but not type 10 times more relevant.",
                    "label": 0
                },
                {
                    "sent": "Relevance does not increase proportionately with strong frequency, so we're trying to find some criteria for ranking, but we don't want to do it proportionally becausw.",
                    "label": 0
                },
                {
                    "sent": "We need to have some.",
                    "label": 0
                },
                {
                    "sent": "We need to be very careful about how we include this information in the ranking.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So instead of raw frequency you're going to use log frequency rating.",
                    "label": 0
                },
                {
                    "sent": "That's defined as follows.",
                    "label": 0
                },
                {
                    "sent": "If the term frequency rate is greater than zero, then the log frequency rate is 1 plus log TFD and 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "And so that has the nice property that it transforms a raw frequency of 0 into 01 into 121 point 310 into two 1000 into 4.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the log is obviously very strong dampening function here.",
                    "label": 0
                },
                {
                    "sent": "The matching score for a document query pair is then simply the sum of all all terms that occur in both query and document of this term frequency rate.",
                    "label": 0
                },
                {
                    "sent": "So now we have our first ranking function.",
                    "label": 0
                },
                {
                    "sent": "This is already a ranking function that does better than just an unordered set of documents.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In addition to term frequency, the frequency of determine the document, we also want to use the frequency of the term in the collection for waiting and ranking.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the F is defined as the document frequency as the number of documents that he Akerson.",
                    "label": 1
                },
                {
                    "sent": "OK. That's an inverse measure of the informativeness of a Trump because very frequent term terms cannot distinguish between different documents because they occur very frequently.",
                    "label": 1
                },
                {
                    "sent": "That's why we use inverse document frequency idea, which is a direct measure of the informativeness of the of the term, and that's defined as follows log of N divided by DF.",
                    "label": 0
                },
                {
                    "sent": "Where N is the number of documents in the collection, so this is the inverse inverse document frequency rate.",
                    "label": 0
                },
                {
                    "sent": "Again, we use the logarithm to dampen the effect of ideas.",
                    "label": 0
                },
                {
                    "sent": "Again, we don't want a proportional influence of this measure, because that would be too strong.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's some examples.",
                    "label": 0
                },
                {
                    "sent": "So if a word occurs in every single document of the collection, then IDF is 0.",
                    "label": 0
                },
                {
                    "sent": "So that means that term will be ignored because it cannot serve to distinguish between one document and another, and the highest idea we get for a term that only occurs in one document.",
                    "label": 0
                },
                {
                    "sent": "That would be log off the number of documents.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is the effect of idea on ranking?",
                    "label": 0
                },
                {
                    "sent": "IDF gives high weights, too rare terms like correctness centric.",
                    "label": 0
                },
                {
                    "sent": "That's my favorite restaurant, dragon centric.",
                    "label": 0
                },
                {
                    "sent": "And it gets low weights too frequent words like good increase in line and hopefully everybody gets the intuition that a term like Iraq concentric is more informative than a term then terms like good increase online.",
                    "label": 0
                },
                {
                    "sent": "Although these terms are still important and we want to use them for ranking.",
                    "label": 1
                },
                {
                    "sent": "IDF affects the ranking of documents for queries with at least two terms.",
                    "label": 1
                },
                {
                    "sent": "For example, in the query iraqna centric line, IDF greatly increases the relative rate of reaction centric and decreases the relative grade of line.",
                    "label": 0
                },
                {
                    "sent": "IDF has little effect on ranking for one term queries.",
                    "label": 0
                },
                {
                    "sent": "So it only has an effect if we have more.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One turn.",
                    "label": 0
                },
                {
                    "sent": "Summary in IDF and TF IDF weighting you sign a TF IDF grade for each term T in each document D. This is the log frequency.",
                    "label": 0
                },
                {
                    "sent": "The frequency compute component.",
                    "label": 0
                },
                {
                    "sent": "This is the idea of component.",
                    "label": 0
                },
                {
                    "sent": "The TF IDF weight increases with the number of occurrences within the document and increases with the rarity of the term in the collection.",
                    "label": 1
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the vector space model.",
                    "label": 0
                },
                {
                    "sent": "So we went from.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm expect us to come.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vectors and now we are going to real valued vectors.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where this vector is a vector of TF IDF weights.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to look at the vector as a whole and look at his its properties.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So each document is now represented as a real valued vector of TF IDF weights.",
                    "label": 1
                },
                {
                    "sent": "In this Space, V is the vocabulary, so we have a very high dimensional space.",
                    "label": 1
                },
                {
                    "sent": "AV dimensional real valued vector space?",
                    "label": 0
                },
                {
                    "sent": "Terms are X axis of the space.",
                    "label": 0
                },
                {
                    "sent": "So each word is an axis of the space and documents are points or vectors in this space.",
                    "label": 0
                },
                {
                    "sent": "It's a very high dimensional space, can be up to 10s of millions of dimensions when you apply this to a web search engine.",
                    "label": 0
                },
                {
                    "sent": "But each individual vector for a document is very sparse.",
                    "label": 0
                },
                {
                    "sent": "Most of the entries in the vector of zeros.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the key idea one for the vector space model is that we do the same for queries.",
                    "label": 0
                },
                {
                    "sent": "We represent them as vectors in the high dimensional space, just as we represent documents.",
                    "label": 0
                },
                {
                    "sent": "And the key idea 2 is that we rank documents according to their proximity to the query.",
                    "label": 0
                },
                {
                    "sent": "Where proximity is roughly similarity or negative distance in the sense that proximity and distance are opposites of each other.",
                    "label": 0
                },
                {
                    "sent": "Recall we're doing this because we want to get away from the you're either in or out feast or famine Boolean models.",
                    "label": 0
                },
                {
                    "sent": "So now we have something that supports graded relevance.",
                    "label": 0
                },
                {
                    "sent": "Instead, we rank relevant documents higher than not relevant documents using this proximi.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, how do we formalize vector space similarity this proximity?",
                    "label": 0
                },
                {
                    "sent": "The first cut would be negative distance between two points.",
                    "label": 0
                },
                {
                    "sent": "The distance between the end points of the two vectors.",
                    "label": 0
                },
                {
                    "sent": "Should we use Euclidean distance?",
                    "label": 0
                },
                {
                    "sent": "No, that would be a bad idea because Euclidean distance is large for vectors of different lengths.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's why we can't use it.",
                    "label": 0
                },
                {
                    "sent": "And you can see this in this graph here so.",
                    "label": 0
                },
                {
                    "sent": "This here is a very simple vector space with just two words.",
                    "label": 1
                },
                {
                    "sent": "The words are poor and rich, and I've put three documents in this space.",
                    "label": 1
                },
                {
                    "sent": "Do you want you 2D3D1 ranks of solving codes?",
                    "label": 0
                },
                {
                    "sent": "Well, that's a document where the word poor dominates and rich doesn't occur.",
                    "label": 1
                },
                {
                    "sent": "Then we have the document record baseball salaries in 2010.",
                    "label": 0
                },
                {
                    "sent": "In this document, the word rich dominates and put us in occur and then we have a document D2 rich poor gap grows.",
                    "label": 1
                },
                {
                    "sent": "We're both rich and poor occur our query, which is as I said, also represented in this vector space is rich poor.",
                    "label": 0
                },
                {
                    "sent": "Now if we use the Euclidean distance to rank documents here, then the one MP3 would actually be closer closer to the query then D2.",
                    "label": 1
                },
                {
                    "sent": "Because if you look at look at distance, then D 190, three are closer to the query than D2.",
                    "label": 0
                },
                {
                    "sent": "But that's not the result we want, because the information that we get out of this vector space is the direction of the vector, the direction of the vector tells us about the mixture of terms that occur in the document and their relative importance, and that's the information we want to use, not the length of the vector.",
                    "label": 0
                },
                {
                    "sent": "The length of the vector doesn't really give us any useful information for ranking.",
                    "label": 0
                },
                {
                    "sent": "So that's why we can't use Euclidean distance.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instead, we use the angle for ranking so we rank documents according to the angle with the query.",
                    "label": 1
                },
                {
                    "sent": "The following two notions are equivalent rank documents according to the angle between query and documented decreasing order or rank documents according to cosine theory document, in increasing order that simply follows out of the monotonicity of the coastline with respect to the angle.",
                    "label": 0
                },
                {
                    "sent": "So we will do the ranking according to the cosine and then we will actually use direction and things that are close in direction to the query will be ranked higher than things that are that are pointing in a different direction.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This year is the formula for the cosine, so you can either call a cosine or similarity percent similarity, and if you remember that fun linear from linear algebra, it's simply the dot product of the normalized query vector and the normalized document vector.",
                    "label": 0
                },
                {
                    "sent": "And this is the definition of the dot product.",
                    "label": 1
                },
                {
                    "sent": "For those of you who don't remember that Qi here is the TF IDF rate of termite in the query.",
                    "label": 0
                },
                {
                    "sent": "DI is the TF IDF rate of term I in the document.",
                    "label": 0
                },
                {
                    "sent": "Q&D are the lengths of community and this is the cosine similarity, or equivalently the cosine of the angle between Q&T.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, this you have to.",
                    "label": 0
                },
                {
                    "sent": "Remember from linear algebra, so this is what we're going to use for ranking in the vector space model, the cosine similarity.",
                    "label": 1
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so another way of looking at it is that what we're basically doing is we're putting we're making all vectors equally long and putting them all on the hyper sphere.",
                    "label": 0
                },
                {
                    "sent": "That's what the underlying math is doing.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I've told you about 1 rating scheme, cave, IDF rating, logarithmic term, frequency rating.",
                    "label": 0
                },
                {
                    "sent": "Idea of document waiting and the cosine normalization.",
                    "label": 0
                },
                {
                    "sent": "There are many others.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through them here, but yeah, there's there's many other ways of doing it, but this is probably the most best known one, and the others are variants that are interesting, one that's more interesting than the others I think, is pivoted length normalization so that that's what we'll do next.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I'm going to skip this example for those of you have never seen if IDF weighting in action can go through this slide and see how you compute a final similarity score between a query in the document.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's all on the slides.",
                    "label": 0
                },
                {
                    "sent": "OK pivot, pivot length normalization.",
                    "label": 1
                },
                {
                    "sent": "Here's a problem for the cosine normalization that I just presented.",
                    "label": 0
                },
                {
                    "sent": "Let's let's look at this query here.",
                    "label": 0
                },
                {
                    "sent": "Antidoping rules Beijing 2008 Olympics and let's say we have three documents.",
                    "label": 0
                },
                {
                    "sent": "Ivan is a short document on anti doping rules at the 2008 Olympics.",
                    "label": 0
                },
                {
                    "sent": "D2 is a long document that consists of a copy of the one in five other news stories on topics different from Olympics and Olympics and anti doping.",
                    "label": 0
                },
                {
                    "sent": "And then we have T3.",
                    "label": 0
                },
                {
                    "sent": "That's a short document on anti doping rules at the 2004 F and sudden lympics.",
                    "label": 0
                },
                {
                    "sent": "This is not working.",
                    "label": 1
                },
                {
                    "sent": "Ranking to be expected in the vector space model.",
                    "label": 0
                },
                {
                    "sent": "Here, if we use cosine normalization, there's somebody that.",
                    "label": 0
                },
                {
                    "sent": "I think there are.",
                    "label": 0
                },
                {
                    "sent": "How many there are six factor factorial possible ranking so which one could we get?",
                    "label": 0
                },
                {
                    "sent": "Any takers?",
                    "label": 0
                },
                {
                    "sent": "123 any other opinions?",
                    "label": 0
                },
                {
                    "sent": "Tooth last one.",
                    "label": 0
                },
                {
                    "sent": "That's the correct answer.",
                    "label": 0
                },
                {
                    "sent": "Yeah, two is the last one wise to the last one.",
                    "label": 0
                },
                {
                    "sent": "Because it's long, yes.",
                    "label": 0
                },
                {
                    "sent": "Right, because the normalization divides by all the stuff that's in the document and most of the stuff in the document is not relevant to the query.",
                    "label": 0
                },
                {
                    "sent": "So we have a long document here that contains a little bit about about enter doping rules speech in 2008 Olympics.",
                    "label": 0
                },
                {
                    "sent": "But relative to the length of the document it's very little and that's why the two role will be ranked last, whereas the three, even though it's not.",
                    "label": 1
                },
                {
                    "sent": "It only contains partially the right answer here, or I mean it's actually not relevant relative to the length of the three.",
                    "label": 0
                },
                {
                    "sent": "It has a lot more relevant information, at least that's what the vector space model thinks.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is an example for a problem.",
                    "label": 0
                },
                {
                    "sent": "Of course I normalization 'cause the normalization produces rates that are too large for short documents and too small for long documents on average.",
                    "label": 0
                },
                {
                    "sent": "So we need to adjust cosine normalization and one way to do that is a linear adjustment, and I'm going to present one particular way of doing that now.",
                    "label": 0
                },
                {
                    "sent": "Which is used called pivot normalization.",
                    "label": 0
                },
                {
                    "sent": "The effect of this will be that similarities of short documents with the query will decrease and similarities of long documents with the query will increase.",
                    "label": 0
                },
                {
                    "sent": "And this removes the unfair advantage that short documents have.",
                    "label": 0
                },
                {
                    "sent": "As I said, part of the reason I'm presenting this is because it's interesting methodologically.",
                    "label": 0
                },
                {
                    "sent": "If your system is not working, what can you do to?",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is basically the graph that I mean by that.",
                    "label": 0
                },
                {
                    "sent": "So this is a graph that shows you this is document length and this is the probability of relevance of documents of a particular document length.",
                    "label": 0
                },
                {
                    "sent": "The red curve here is true relevance.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, the scales are not clear, but let's say these are documents of length 30 or something and this might be 2%.",
                    "label": 0
                },
                {
                    "sent": "So of all documents of length, 32% are truly relevant.",
                    "label": 0
                },
                {
                    "sent": "And this curve rises because the longer document is, the more chances it has of being relevant to more queries.",
                    "label": 0
                },
                {
                    "sent": "So overall the curve rises for that reason.",
                    "label": 0
                },
                {
                    "sent": "Now, this is the prediction of cosine and you can see that cosine overestimates the relevance of short documents and underestimates the relevance of long documents, and it's precisely for the reason that we saw earlier that.",
                    "label": 0
                },
                {
                    "sent": "In this example.",
                    "label": 0
                },
                {
                    "sent": "So we have to correct that, and the way we correct that.",
                    "label": 0
                },
                {
                    "sent": "A simple way of correcting that is to just turn this curve around to turn it around this pivot.",
                    "label": 0
                },
                {
                    "sent": "The pivot is the point where the two relevance and the estimated relevance intersect, so that's what we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next so this is here cosine normalization, standard cosine normalization.",
                    "label": 0
                },
                {
                    "sent": "And instead of using this cosine normalization factor, we're going to use a pivoted normalization factor.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "So we're going to increase the normalization factor for short documents and decrease it for long documents, because increasing it for subdocuments means since the normalization factor is in the nominator, the number, the similarity number will decrease.",
                    "label": 0
                },
                {
                    "sent": "And if you do.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That then.",
                    "label": 0
                },
                {
                    "sent": "You these are some results that show you that you get a clear improvement in this experiment of 10% in relevance if you remove this bias from cosine norm.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Station.",
                    "label": 0
                },
                {
                    "sent": "OK summary.",
                    "label": 0
                },
                {
                    "sent": "Ranked retrieval in the vector space model we represent each document as rated TF IDF vector.",
                    "label": 1
                },
                {
                    "sent": "We represent the query also as a beta TF IDF vector.",
                    "label": 0
                },
                {
                    "sent": "We compute the cosine similarity between the query vector and each document vector, or Alternatively we use pivot normalization.",
                    "label": 0
                },
                {
                    "sent": "Then we rank the documents with respect to the query and return the top K to the user.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was two ideas, waiting vector space model pivot normalization.",
                    "label": 1
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Normalized black what is your feeling?",
                    "label": 0
                },
                {
                    "sent": "This is not good anymore.",
                    "label": 0
                },
                {
                    "sent": "Good question, once they are.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Normalized, you can use Euclidean distance, yes, but.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "You're basically doing the same thing.",
                    "label": 0
                },
                {
                    "sent": "Either you can directly compute the cosine, which which implicitly includes normalization, length normalization, or you can do length normalization and then rank according to Euclidean distance so.",
                    "label": 0
                },
                {
                    "sent": "On the hyper sphere, Euclidean distance and cosine normalization ranking our cosine ranking are equivalent.",
                    "label": 0
                },
                {
                    "sent": "That is true.",
                    "label": 0
                },
                {
                    "sent": "Does that answer your question?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Presentation wow.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "The problem is enough problem.",
                    "label": 0
                },
                {
                    "sent": "Either.",
                    "label": 0
                },
                {
                    "sent": "Not the query is founding document within an collection of documents or it does not appear.",
                    "label": 0
                },
                {
                    "sent": "I think also in back with this model, it's the same.",
                    "label": 0
                },
                {
                    "sent": "Actually the difference is that here we have rank.",
                    "label": 0
                },
                {
                    "sent": "I mean for example, if yes, I agree, but it's more.",
                    "label": 0
                },
                {
                    "sent": "I mean this related to feast or famine problem.",
                    "label": 0
                },
                {
                    "sent": "I mean in Boolean model you don't really have a choice of including too many query Trump's, because if you have a conjunctive interpretation of your query, once you include too many query words then your result set is going to be 0.",
                    "label": 0
                },
                {
                    "sent": "So all of this is under the assumption that you're willing to write longer queries and that the longer queries are going to be interpreted.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "The previous slide instead.",
                    "label": 0
                },
                {
                    "sent": "That idea thanks only ranking of the Curry which has at least in terms yes.",
                    "label": 0
                },
                {
                    "sent": "Does it mean that I can remove?",
                    "label": 0
                },
                {
                    "sent": "Computation of IDF value that you can do what with the computation of there.",
                    "label": 0
                },
                {
                    "sent": "For one time queries, yes.",
                    "label": 0
                },
                {
                    "sent": "Basically IDF is not doing any work, I mean it's doing work actually be cause in the normalization of the of the documents it is actually having a certain effect, so you will get different results even for one term queries.",
                    "label": 0
                },
                {
                    "sent": "If IDF ranking is used for the documents.",
                    "label": 0
                },
                {
                    "sent": "But a big effect, you will only get if you have queries of at least length 2.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Normalizations you showed up, so there is a magic angle like people are supposed to find.",
                    "label": 0
                },
                {
                    "sent": "Yeah, understand this has been done only based on user testing.",
                    "label": 0
                },
                {
                    "sent": "That's correct, yes, and that's a typical way people do that information retrieval.",
                    "label": 0
                },
                {
                    "sent": "They come up with a nice method, but then there's a parameter that has to be tuned.",
                    "label": 0
                },
                {
                    "sent": "In this case, the parameter that has to be tuned is the pivot angle, and that is determined empirically.",
                    "label": 0
                },
                {
                    "sent": "Yes, the way you suggest.",
                    "label": 0
                },
                {
                    "sent": "The user base for this crafting folder.",
                    "label": 0
                },
                {
                    "sent": "Partner.",
                    "label": 0
                },
                {
                    "sent": "For this graph, but this was done on one of the standard evaluation collections in information retrieval, which is called track, and so on.",
                    "label": 0
                },
                {
                    "sent": "It was estimated on held out data and then applied to unseen data, and then they showed that it improves the results.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The documents referenced images.",
                    "label": 0
                },
                {
                    "sent": "It is a general rule that selection dependent that's football collection using dialogue with the document frequency is a good idea because we tried this by example with the collision image 5th 2008 and we find that when we use the look the the result is very.",
                    "label": 0
                },
                {
                    "sent": "It's very bad.",
                    "label": 0
                },
                {
                    "sent": "It's very good.",
                    "label": 0
                },
                {
                    "sent": "That surprises me.",
                    "label": 0
                },
                {
                    "sent": "I would works.",
                    "label": 0
                },
                {
                    "sent": "It's a good idea in general if you have long documents.",
                    "label": 0
                },
                {
                    "sent": "But maybe we can talk about that offline.",
                    "label": 0
                },
                {
                    "sent": "I mean, it is possible that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I think it's possible to find collection fair doesn't work, but I would think it works pretty poorly for most collections.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, how much more time do I have?",
                    "label": 0
                },
                {
                    "sent": "Another half hour or one hour.",
                    "label": 0
                },
                {
                    "sent": "OK, quick.",
                    "label": 0
                },
                {
                    "sent": "Then we're doing Valentine.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "3rd part for this morning is probabilistic information retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probabilistic models",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I would want to cover in this sector is probabilistic approach to our introduction and then two models.",
                    "label": 0
                },
                {
                    "sent": "The binary independence model that is the first influential probabilistic model that is kind of a reference point for probabilistic.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Models and information retrieval.",
                    "label": 0
                },
                {
                    "sent": "An Okapi Game 25 or more modern, better performing.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Holistic model",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This year was the ad hoc retrieval problem.",
                    "label": 0
                },
                {
                    "sent": "The traditional information retrieval problem.",
                    "label": 1
                },
                {
                    "sent": "Given the user information need and a collection of documents the our system must determine how well the documents satisfy the query.",
                    "label": 0
                },
                {
                    "sent": "The our system has an uncertain understanding of the user query.",
                    "label": 0
                },
                {
                    "sent": "This is the distinction between information need and user query.",
                    "label": 0
                },
                {
                    "sent": "The user types in some keywords that is not a complete description of what the user wants, right?",
                    "label": 0
                },
                {
                    "sent": "So that means the other system can only have an uncertain understanding of what the user really wants.",
                    "label": 0
                },
                {
                    "sent": "NPR system makes an uncertain guess of whether a document satisfied satisfies the query because it's difficult to.",
                    "label": 0
                },
                {
                    "sent": "To make an assessment, even for for human, and it's even more difficult for our system.",
                    "label": 0
                },
                {
                    "sent": "Probability theory provides a principled foundation for such reasoning under uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic IR models exploit this foundation to estimate how likely it is that a document is relevant to a query.",
                    "label": 0
                },
                {
                    "sent": "So what this slide is trying to say is.",
                    "label": 0
                },
                {
                    "sent": "IR is feeling a problem of uncertainty, so why aren't we using the mathematical theory of uncertainty?",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's an attempt here to do that.",
                    "label": 0
                },
                {
                    "sent": "Difference between probabilistic and vector space models.",
                    "label": 0
                },
                {
                    "sent": "The vector space model ranks documents according to similarity to the query.",
                    "label": 0
                },
                {
                    "sent": "The notion of similarity does not translate directly to an assessment of is the document a good document to give to the user or not?",
                    "label": 0
                },
                {
                    "sent": "The most similar document can be highly relevant or completely non relevant.",
                    "label": 0
                },
                {
                    "sent": "I mean in the extreme case, if you have no answer to your query in the collection then there will still be a most similar document.",
                    "label": 0
                },
                {
                    "sent": "Probability theory is arguably a cleaner formalization of what we really want an IR system to do.",
                    "label": 0
                },
                {
                    "sent": "Give relevant documents to the user because we are.",
                    "label": 0
                },
                {
                    "sent": "Ideally we will have a direct assessment of relevance, that's the.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The goal.",
                    "label": 0
                },
                {
                    "sent": "The classical probabilistic retrieval models are binary independence model and Okapi BM 25.",
                    "label": 0
                },
                {
                    "sent": "I'm going to cover cover them both now.",
                    "label": 0
                },
                {
                    "sent": "Then there's also Bayesian networks for text retrieval.",
                    "label": 0
                },
                {
                    "sent": "I don't have time for this, and then there are language model approaches to IR and that's going to be the first lecture after lunch.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The rank rule set up is the user is a query and a ranked list of documents is returned.",
                    "label": 1
                },
                {
                    "sent": "How can we rank probabilistically?",
                    "label": 0
                },
                {
                    "sent": "First we introduce a random dichotomous variable RDQ.",
                    "label": 0
                },
                {
                    "sent": "And that's one if the document is relevant to with respect to the query and 0 otherwise.",
                    "label": 1
                },
                {
                    "sent": "This is a binary notion of relevance.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic ranking orders documents decreasingly by their estimated probability of elements with respect to this probability.",
                    "label": 1
                },
                {
                    "sent": "The probability of relevance given a document and a query.",
                    "label": 0
                },
                {
                    "sent": "How can we justify this way of proceeding?",
                    "label": 0
                },
                {
                    "sent": "It seems kind of intuitive, but let's dig into this a little.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But further.",
                    "label": 0
                },
                {
                    "sent": "And the justification that is usually given in information retrieval is the probabilistic.",
                    "label": 1
                },
                {
                    "sent": "The probability ranking principle.",
                    "label": 0
                },
                {
                    "sent": "If the retrieved documents are ranked increasingly on their probability of relevance, then the effectiveness of the system will be the best that is obtainable.",
                    "label": 0
                },
                {
                    "sent": "That's not really a theorem that's derived from anything, it just follows from the definition of probability of relevance.",
                    "label": 1
                },
                {
                    "sent": "Because if you rank documents according to probability of relevance, and then you have to make a decision which document are going, am I going to put next in this rank list then?",
                    "label": 1
                },
                {
                    "sent": "It's obvious that you have to choose the document that has the highest relevance, highest probability of relevance, 'cause that's most likely to satisfy the user.",
                    "label": 0
                },
                {
                    "sent": "Right, so the probability ranking principle is simply a way of stating this.",
                    "label": 0
                },
                {
                    "sent": "This fact that follows directly out of the definition of probability of relevance.",
                    "label": 0
                },
                {
                    "sent": "The fundamental assumption here is that the relevance of each document is independent of the relevance of other documents.",
                    "label": 0
                },
                {
                    "sent": "That's actually not true.",
                    "label": 0
                },
                {
                    "sent": "Why is that not true?",
                    "label": 0
                },
                {
                    "sent": "In this context, in the context of a ranked list.",
                    "label": 0
                },
                {
                    "sent": "So already seeing pieces of information, maybe a reiteration of the same information, because we had to get it right.",
                    "label": 1
                },
                {
                    "sent": "I mean, extreme example is if you have duplicates, so you put a duplicate and #3 and now you have a duplicate that your system estimates to be as relevant as the other duplicate.",
                    "label": 0
                },
                {
                    "sent": "And of course you don't want to put the duplicate there, 'cause it gives no additional information to the user.",
                    "label": 0
                },
                {
                    "sent": "But that's very difficult to handle in ranked retrieval model.",
                    "label": 0
                },
                {
                    "sent": "So usually we ignore it.",
                    "label": 0
                },
                {
                    "sent": "And we do some post processing to remove duplicates on YouTube.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first probabilistic model that is based on this is the binary.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This model.",
                    "label": 0
                },
                {
                    "sent": "It's binary documents and queries are represented as binary term incidence vectors, and independence means that times are independent of each other.",
                    "label": 0
                },
                {
                    "sent": "That's the assumption which is the naive base assumption.",
                    "label": 0
                },
                {
                    "sent": "That we are making.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Binary representation simply means binary incidence matrix, so as in the Boolean model we represent the collection in this form and only consider binary information.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We start by applying Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "So here we have the probability of relevance given X and Q&A threatening here for the two cases just to be very clear and this is not simply based rule.",
                    "label": 1
                },
                {
                    "sent": "Recall here that documented query are modeled as term incidence vectors and we write them as X&Q here.",
                    "label": 0
                },
                {
                    "sent": "So this is the term incidence vector of the document and this is the term incidence vector of the query.",
                    "label": 1
                },
                {
                    "sent": "What do these expressions on the right side mean?",
                    "label": 0
                },
                {
                    "sent": "The probability of X given equals one Q.",
                    "label": 0
                },
                {
                    "sent": "This is the probability that for a given query, if I know I'm relevant, what is the probability I'm going to get a particular document incidence vector?",
                    "label": 1
                },
                {
                    "sent": "At the same for the case I = 0.",
                    "label": 0
                },
                {
                    "sent": "And now we will use statistics about the document collection to estimate this probability here.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This number here so that was this expression.",
                    "label": 0
                },
                {
                    "sent": "Now what is this here P of R = 1 given Q, that is simply the prior probability of retrieving a relevant or non relevant document for query Q.",
                    "label": 0
                },
                {
                    "sent": "We estimate that from the percentage of relevant documents in the collection.",
                    "label": 0
                },
                {
                    "sent": "So the question is simply for this query, what relative, what proportion of documents are relevant for it in non relevant for it?",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just said that we're going to rank documents according to this probability of relevance.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's easier to rank according to the odds of relevance.",
                    "label": 0
                },
                {
                    "sent": "So instead of ranking according to this, we're going to rank according to this 'cause if you whether you rank according to P, the probability or P / 1 -- P, The odds of the probability is the same.",
                    "label": 0
                },
                {
                    "sent": "So we're going to use this for ranking because it makes things simpler and then applying Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "We get this expression.",
                    "label": 0
                },
                {
                    "sent": "Now this here is a constant becausw the probability of the odds of relevance given the query are not different within one ranking because one ranking is always for one query, right?",
                    "label": 0
                },
                {
                    "sent": "So this is going to be a constant for that query.",
                    "label": 0
                },
                {
                    "sent": "So all we need to do for ranking is consider this factor.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's our factor.",
                    "label": 0
                },
                {
                    "sent": "This is going to be the fact that we're going to use for ranking, and now we're making the naive Bayes conditional independence assumption.",
                    "label": 0
                },
                {
                    "sent": "But that is saying is that the probability of getting this term incidence vector given relevance and a query is the same as the probability of all the individual indicators given relevance in the query.",
                    "label": 0
                },
                {
                    "sent": "So our assumption is that.",
                    "label": 0
                },
                {
                    "sent": "The terms are all independent of each other in the document given the query.",
                    "label": 0
                },
                {
                    "sent": "And given relevance, given a particular relevance.",
                    "label": 0
                },
                {
                    "sent": "So now we rank according to this year.",
                    "label": 0
                },
                {
                    "sent": "Of course this naive Bayes independence assumption is not true.",
                    "label": 0
                },
                {
                    "sent": "I mean, what would be a simple argument or a simple example to show that it's not true?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "United States, right?",
                    "label": 0
                },
                {
                    "sent": "So if I mean my favorite example is Hong Kong, because United States also occurs in many other, you know, those two words also occur in many other contexts.",
                    "label": 0
                },
                {
                    "sent": "But Hong Kong in English?",
                    "label": 0
                },
                {
                    "sent": "Well, Kong I guess can also occur in King Kong, but mostly they occur in that combination Hong Kong.",
                    "label": 0
                },
                {
                    "sent": "So if you know Hong occurs in the document, it's pretty certain that Kong also occurs, and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So terms are not independent of each other.",
                    "label": 0
                },
                {
                    "sent": "Um but.",
                    "label": 0
                },
                {
                    "sent": "This assumption works really well and at the end I have a link that to a paper that explains why it's working so well even though it is not very intuitive.",
                    "label": 0
                },
                {
                    "sent": "It's very nice work.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pedro domingos so.",
                    "label": 0
                },
                {
                    "sent": "Right, so we want to rank according to this.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to do a few more arithmetic manipulations.",
                    "label": 0
                },
                {
                    "sent": "So this is the binary indicator that tells me that T the trumpty occurs in the document or not zero or one.",
                    "label": 0
                },
                {
                    "sent": "And we can separate this into.",
                    "label": 0
                },
                {
                    "sent": "The product into the cases where the term occurs in the document and the term doesn't occur in the document, so this is simply.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dividing up the factors and now we introduce some notation.",
                    "label": 0
                },
                {
                    "sent": "We call the probability that the term occurs given relevance PT.",
                    "label": 0
                },
                {
                    "sent": "And the probability that the term occurs given non relevance UT.",
                    "label": 0
                },
                {
                    "sent": "So this is the probability that the tongue occurs if the documents relevant and this is the probability that term occurs if the document is non relevant.",
                    "label": 0
                },
                {
                    "sent": "And this is comes from this contingency table here.",
                    "label": 0
                },
                {
                    "sent": "Relevance non relevance to impresence term absence.",
                    "label": 0
                },
                {
                    "sent": "This is PTUT 1 -- P T 1 -- U T. Now we can write the ranking function, eat more compactly as this.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "One final simplifying assumption.",
                    "label": 0
                },
                {
                    "sent": "If Q T = 0, then PT equals UT.",
                    "label": 0
                },
                {
                    "sent": "In other words, in other words, a term not occurring in the query is equally likely to occur in relevant and non relevant documents.",
                    "label": 0
                },
                {
                    "sent": "So if a term doesn't occur in the query, our assumption is it's equally likely that it occurs, involvement or non relevant documents.",
                    "label": 0
                },
                {
                    "sent": "Now we need only to consider terms in the products that appear in the query.",
                    "label": 0
                },
                {
                    "sent": "So now this was the expression before and now I've added the constraint that QT equals one.",
                    "label": 0
                },
                {
                    "sent": "That is, the term occurs in the query in both products.",
                    "label": 0
                },
                {
                    "sent": "And I think I need two or three slides more, but then there's going to be a really nice interpretation of this.",
                    "label": 0
                },
                {
                    "sent": "So this is right.",
                    "label": 0
                },
                {
                    "sent": "So the last thing I think this is certainly the last arithmetic manipulation we have to do is that we let this one overall.",
                    "label": 0
                },
                {
                    "sent": "It's the overall query terms and not just the ones that don't occur in the document.",
                    "label": 0
                },
                {
                    "sent": "That means we have to add some 1 minus P 1 -- P / 1 -- U factors here and then also multiply the reciprocals.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That looks like this and the right product is not overall query terms, hence constant for a particular query and can be ignored.",
                    "label": 0
                },
                {
                    "sent": "Again, we're ranking for a particular query this term here will be the same for each document, so we can omit it.",
                    "label": 0
                },
                {
                    "sent": "And this is then the only quantity we need for ranking.",
                    "label": 0
                },
                {
                    "sent": "Since logs and some so easier to deal with than multiplication, we actually use the log of this for ranking.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally we write it a little bit differently, like this, and this is the thing that has a very nice interpretation.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this and this and you see that this is a simple arithmetic manipulation, so this has the following very nice interpretation.",
                    "label": 0
                },
                {
                    "sent": "This is an odds ratio that is a ratio of two odds.",
                    "label": 0
                },
                {
                    "sent": "This here is the odds of the term appearing in the document.",
                    "label": 1
                },
                {
                    "sent": "If the document is relevant and this year is the odds of the term appearing if the document is non relevant.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And see T is the difference between the two.",
                    "label": 0
                },
                {
                    "sent": "Now, if this city is 0, then the odds of the term appearing in relevant documents are the same as the odds of the term appearing in non relevant documents and that means this term does not give us any information about the document, whether it's relevant or not relevant, and in that case it is zero and it does not add anything to the retrieval status value.",
                    "label": 0
                },
                {
                    "sent": "If City is positive, then that means that you have higher odds of the term appearing in relevant documents then of the term appearing on relevant documents and that means this term is evidenced for relevance.",
                    "label": 1
                },
                {
                    "sent": "Positive evidence for relevance and then it adds something positive to the retrieval status value.",
                    "label": 0
                },
                {
                    "sent": "And finally, if we have higher odds of the term appearing non relevant documents in relevant documents, then it's negative evidence and it will subtract something from the retrieval status value.",
                    "label": 0
                },
                {
                    "sent": "So that's really.",
                    "label": 0
                },
                {
                    "sent": "And so the final formula for ranking is that we add up all these setae that occur in the document.",
                    "label": 0
                },
                {
                    "sent": "Right, and so this slide really is, I think the main reason that that you might prefer a probabilistic model to the vector space model becausw you actually know to some extent what these term rates mean.",
                    "label": 1
                },
                {
                    "sent": "In the vector space model, I've been through a lot of motivation for why we did this or that with TF IDF, but in the end it's not really clear what these values mean with probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "At least, you have a pretty nice interpretation of of what the term rate for a term means and where it comes from.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the retrieval service value in this probabilistic pin model is simply the sum over all terms occurring in the document of the term rates.",
                    "label": 0
                },
                {
                    "sent": "So been vector space model is similar on an operational level.",
                    "label": 0
                },
                {
                    "sent": "In particular, we can use the same data structures like the inverted index for the two models.",
                    "label": 0
                },
                {
                    "sent": "The main difference is really in how the term rates are computed.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is how you would actually compute the trumpets.",
                    "label": 0
                },
                {
                    "sent": "In practice, you compute a contingency table for every term tone presence from absence relevant non relevant service, for example, would be the number of documents that contain the term that are relevant, and so on.",
                    "label": 0
                },
                {
                    "sent": "And from these numbers you can then compute utpd CTCTS that run rate and use this formula, there's one.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complication, but just we want to avoid zeros if any of the counts is a zero, then the term rate is not well defined.",
                    "label": 0
                },
                {
                    "sent": "And in that case, the reason is that maximum likelihood estimates.",
                    "label": 0
                },
                {
                    "sent": "That's what we're using here effectively do not work for events to avoid zeros, we add 0.5 to each count.",
                    "label": 0
                },
                {
                    "sent": "That's called expected likelihood estimation, or we can use any other type of smoothing we want to use, avoid zeros.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "Right, so we can actually.",
                    "label": 0
                },
                {
                    "sent": "I just talked about the vector space model.",
                    "label": 1
                },
                {
                    "sent": "Now I want to make an even more direct comparison between the bin and the vector space model.",
                    "label": 1
                },
                {
                    "sent": "Because if we make one more simplifying assumption, we'll see how the two are actually very similar.",
                    "label": 0
                },
                {
                    "sent": "If you make the assumption that relevant documents are very small, percentage of the collection.",
                    "label": 0
                },
                {
                    "sent": "So for any given query, most documents are not relevant, right, so?",
                    "label": 0
                },
                {
                    "sent": "The proportion of relevant documents for any given query is pretty small.",
                    "label": 1
                },
                {
                    "sent": "If you make this assumption, then we can actually approximate this term of the term weight by log N divided by DF.",
                    "label": 0
                },
                {
                    "sent": "So the definition of this you can look this up on the slides in minus DF divided by DF.",
                    "label": 0
                },
                {
                    "sent": "And if the number of documents any term across in is pretty small, then we get.",
                    "label": 0
                },
                {
                    "sent": "This log N divided by ideas and this should look look familiar because this is simply the definition of the IDF rate.",
                    "label": 0
                },
                {
                    "sent": "So this means that one part of this of the term rate city is actually a systematic or a. Derivation from first principles of the IDF rate log N divided by DF.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do we actually apply the bin model for for our different tasks?",
                    "label": 0
                },
                {
                    "sent": "If we have relevance feedback where the user has given us some relevance judgments, so user the user has told us for some documents this document is relevant for the query.",
                    "label": 0
                },
                {
                    "sent": "This document is not.",
                    "label": 0
                },
                {
                    "sent": "Then we can directly compute term rates based on the can.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Agency table so we use this table that are just introduced and we're done.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For ad hoc retrieval it's more complicated because the assumption and not awkward travel is that we have a query, but we don't have a single relevant structure meant for any document.",
                    "label": 0
                },
                {
                    "sent": "In that case, we have to make yet another assumption.",
                    "label": 0
                },
                {
                    "sent": "And the assumption is that PT is 0.5 for all terms XD in the queries.",
                    "label": 0
                },
                {
                    "sent": "So each query term is equally likely to occur in a relevant document, and so the PT involvement is factors cancel out in the expression for RSV.",
                    "label": 0
                },
                {
                    "sent": "That's a weak estimate, but doesn't disagree violently with expectation that query terms appear in many, but not all relevant documents.",
                    "label": 0
                },
                {
                    "sent": "So if we make this assumption, then the CT rate is actually just IDF.",
                    "label": 0
                },
                {
                    "sent": "Because in that case P 0.5, this becomes zero and this we had just approximated by by the idea of weight.",
                    "label": 0
                },
                {
                    "sent": "So the simplest probabilistic model is actually just IDF weighting and the difference to the vector space model is just that we now have a principle derivation of this idea freight and for short documents.",
                    "label": 0
                },
                {
                    "sent": "This simple version of been actually works pretty well, and that's where it was developed, because in the very very beginning of information retrieval in the 50s and 60s there were no full length documents online, so the only things that were available electronically were titles.",
                    "label": 0
                },
                {
                    "sent": "And very short abstracts.",
                    "label": 0
                },
                {
                    "sent": "And in those titles and short abstracts.",
                    "label": 0
                },
                {
                    "sent": "If a time occurs, it only across once in most cases, and therefore term frequency is not important because you literally have only a binary distinction between a term either occurring or not occurring.",
                    "label": 0
                },
                {
                    "sent": "Tone frequency does not contribute any information in that case is in those cases just using IDF is sufficient.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I'll conclude with the Okapi BM 25 model Okapi BM 25 is a probabilistic model that incorporates term frequency and length normalization.",
                    "label": 0
                },
                {
                    "sent": "As I just said, Beam was originally designed for short catalog records so fairly consistent link and it works reasonably in these contexts.",
                    "label": 0
                },
                {
                    "sent": "But for modern fulltext search collections, the model should pay attention to term Frequency, document link and that's what BM 25 does.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the starting point is this is the simplest form of been the simplest form of BIM was IDF weighting.",
                    "label": 0
                },
                {
                    "sent": "So we simply sum the idea of rates for the terms that are current query and document.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What to copy adds to that is this expression here.",
                    "label": 0
                },
                {
                    "sent": "And I'm not going to go through the details of the derivation, but I'm just going to give you some intuitions what these terms mean.",
                    "label": 0
                },
                {
                    "sent": "So first the terminology is TF is as before term frequency.",
                    "label": 0
                },
                {
                    "sent": "LD is the length of document, D&LF is the average document length in the whole collection.",
                    "label": 0
                },
                {
                    "sent": "K1 is a tuning parameter controlling the scaling of term frequency.",
                    "label": 0
                },
                {
                    "sent": "And B is a tuning parameter controlling the scaling by document length.",
                    "label": 0
                },
                {
                    "sent": "Now you can see how this works by just playing around with some values for these numbers.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have B = 0.",
                    "label": 0
                },
                {
                    "sent": "That means we don't take length into account, and in that case the formula simply becomes Cape K 1 + 1 / K One plus TF.",
                    "label": 0
                },
                {
                    "sent": "Now if you put.",
                    "label": 0
                },
                {
                    "sent": "K10 here, then.",
                    "label": 0
                },
                {
                    "sent": "This cancels out.",
                    "label": 0
                },
                {
                    "sent": "In that case the model defaults back to the simple BIM model.",
                    "label": 0
                },
                {
                    "sent": "So K10 means simple BIM model.",
                    "label": 0
                },
                {
                    "sent": "If you let gave one K1 go to Infinity then you can see that what we get will be TF the raw frequency, because then if one goes to Infinity then K1 and K1 will cancel each other out in just TF.",
                    "label": 0
                },
                {
                    "sent": "Remains.",
                    "label": 0
                },
                {
                    "sent": "So for zero we get the simple BIM model for Infinity.",
                    "label": 0
                },
                {
                    "sent": "We get raw frequency rating where TF enters into the equation proportionally and any values between zero and Infinity will will have effects, intermediate effects.",
                    "label": 0
                },
                {
                    "sent": "So K1 is a way of tuning how much.",
                    "label": 0
                },
                {
                    "sent": "You want to give a term frequency.",
                    "label": 0
                },
                {
                    "sent": "And be it works very similarly for length in the extreme case, if we set P20, then length is ignored.",
                    "label": 0
                },
                {
                    "sent": "If we set beta one, then we have a factor here that will penalize long documents.",
                    "label": 0
                },
                {
                    "sent": "So a long document will will be penalized and short documents will be preferred.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. That's basically is it so motivation of the probabilistic approach, binary dependence model, or coffee bean?",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In five and on the website I've put the original paper that introduced the binary dependence model by Robertson and Spark Jones.",
                    "label": 0
                },
                {
                    "sent": "More details on or copy M 25.",
                    "label": 0
                },
                {
                    "sent": "And the paper I mentioned earlier that explains why the naive Bayes independence assumption works, even though it is so naive.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess we already answered that question so.",
                    "label": 0
                },
                {
                    "sent": "Are there any other questions?",
                    "label": 0
                },
                {
                    "sent": "You can also.",
                    "label": 0
                },
                {
                    "sent": "Ask questions about the other two lectures.",
                    "label": 0
                },
                {
                    "sent": "I guess otherwise we would just.",
                    "label": 0
                },
                {
                    "sent": "Go to lunch.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                }
            ]
        }
    }
}