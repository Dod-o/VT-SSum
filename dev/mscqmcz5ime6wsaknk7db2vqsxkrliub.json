{
    "id": "mscqmcz5ime6wsaknk7db2vqsxkrliub",
    "title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",
    "info": {
        "author": [
            "Song Han, Stanford University"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_han_deep_compression/",
    "segmentation": [
        [
            "And it's a it's a best paper award.",
            "Our second base pay power so it to pleasure to give that based power to someone.",
            "Thank you.",
            "Hi everyone, good morning.",
            "My name is song today.",
            "My presentation is about deep compression compressing the neural network to make the model smaller and have the same accuracy as at the same moment."
        ],
        [
            "So I briefly introduction of ourselves and the 4th year PhD student in Stanford with Professor PEO Tally Ann.",
            "My research interest is in model compression and hardware acceleration and who is he?",
            "Is the undergraduate chinqua.",
            "He's going to join us soon.",
            "My advisor, bill that is professor at Stanford and also the chief scientist at Media."
        ],
        [
            "So deep learning is the next wave of AI is really wide range of applications, ranging from each image recognition to speech recognition, natural language process."
        ],
        [
            "So natural question to ask is since the embedded systems has a limited amount of computation power, where should we put those competition?",
            "So if we put."
        ],
        [
            "Yeah, on the cloud the problems are we suffer from network delay.",
            "One second for self driving.",
            "Car to car might be several meters away.",
            "Already suffer from the power budget budget as well for the total cost of ownership of a large data center and also it has compromised user privacy since we have to upload the images and pictures to the cloud.",
            "So."
        ],
        [
            "Doing deep learning on the cloud is intelligent but inefficient.",
            "So what if we write the deep learning locally on the mobile?",
            "So that we are first the problem of the large model size.",
            "For example, the Apple app developers suffer from the model size because I post or has this limitation that if an item is over over 100 megabyte you cannot download it unless you connect to the Wi-Fi.",
            "So after we do our work of decompression, I'm uploading the paper to archive.",
            "Like a few days ago, a few days later and ruin sent us an email thanking us that we can make the model smaller so that the Baidu apps can squeeze a lot of squeeze neural networks into Baidu apps without changing much of the size of their app.",
            "So."
        ],
        [
            "Another motivation is that for hardware engineers, they really suffer from the model science because embedded system have limited resources, specially limited power resource and limited battery.",
            "So if we see where are the energy really consumed, that's the bottom line here, which moves of the energy is consumed by accessing DRAM, which is two or three orders of magnitude more than multiplication add.",
            "And accessing the cache so if you do one DRAM access that's equal to 100 or even more multiplication and add operation."
        ],
        [
            "So so memory access is really expensive and we want to cut the cut the size of the model to make it really efficient analyze memory access."
        ],
        [
            "So I'm proposing deep compression which gave the deep neural network smaller model size ranging from 10 times to 50 times smaller with the same accuracy.",
            "I verified on image Net and with speed up, especially with specialized hardware.",
            "So."
        ],
        [
            "Overview of the results I can compress Alex Net by 35 times to only around 7 megabytes.",
            "VGG net by 50 or 49 times to 11 megabytes.",
            "Original idea.",
            "Did Google net that comprise it turn times to only around 2.8 megabytes and squeeze net by 10 times to only around 0.47 megabytes is roughly a license 5 megabytes.",
            "Too much fit in the 15 to cash.",
            "And are all these cases.",
            "It has no loss of accuracy on image net compared with the original network.",
            "So now we can fulfill fully fit on SRAM cache."
        ],
        [
            "So what is the pipeline like?",
            "It has three stages, the first one is network pruning, second one with sharing and 3rd one Huffman coding.",
            "Let me start with the first one."
        ],
        [
            "So we want to have less number of weights."
        ],
        [
            "So the idea is using pruning.",
            "Pruning a network is pretty similar to if you prune a tree, you get rid of those redundant connections, so this work has been pioneered by young account back in the 1990s were called optimal brain damage, and later optimal optical brain optimal brain surgery that proposed various high risztics of how do we get rid of those redundant weights and showed pretty good results?",
            "And 25 years later, we pretty much removed faster from the table and revisited this idea and show that actually worked really well on modern networks.",
            "So."
        ],
        [
            "Motivation is if we see how human brain works.",
            "So at birth we have 50 trading connections in the brain.",
            "I one year old that grows to 1000.",
            "Trading really surged at 10 years old and mature and adult.",
            "We have 500 trading connections.",
            "So I get pruned.",
            "So does the similar mechanism happen?",
            "Can happen in artificial neural network and the answer is yes."
        ],
        [
            "So this is the redundancy on Alex net on the top and VGG net on the bottom.",
            "The green is the redundancy that can be safely pruned.",
            "The blue is what is left.",
            "We can see that for combinators we can prune away around three times.",
            "So 66% roughly, but the first layer is pretty sensitive to pruning.",
            "Sensitive directly interacts with the input image, which has only three channels and FC layers are pretty much resilient pruning.",
            "We can prune away 90%.",
            "With ten times less parameters, so overall it's roughly 10% seven percent parameters left on Alexander VG net."
        ],
        [
            "So the method we do it is by training the connected first, just training on network normally and prune the connectionstring the weights again and do this iteratively so we can see that for different regularization with our two regularization without one recognization without retraining, we can have worse accuracy.",
            "We can pruning around 80% but with proper training, especially L2 regularization, sometimes we get even higher accuracy and get we can prove more.",
            "So by iterative pruning we can get rid of 90% of the parameters in Alex net without hurting the accuracy at all."
        ],
        [
            "So this is the result across our net Alex Net VGG net and this is the model parameters we can prune away without hurting the accuracy.",
            "It ranges from 9 times to 13 times."
        ],
        [
            "And so yeah, other thing is about seeing how about R and R&R STM.",
            "Yes, the pruning works as well, so this we experiment on neural talk which is done by car party at all.",
            "So we after pruning win 90% of the parameters.",
            "Here we can see that the blue score doesn't hurt but 95% it begins to get hurt the accuracy."
        ],
        [
            "So for example.",
            "At the original network sensor, Brown dogs running through a grassy field printing 90% of Brown dogs running through a grassy area.",
            "Pretty much the same.",
            "But if not, prune away 95%.",
            "It says a man in a red shirt and black and white black shirt is running through a field.",
            "It's getting drunk, but not completely drunk.",
            "But if a human it's only have 5% of parameters left.",
            "If any one of us here only have 5% of the connections in our brain, probably we are not going to be very well now, so it's interesting result."
        ],
        [
            "The weight distribution I plotted here.",
            "This is the original weight distribution of a network and this is after pruning.",
            "We can see that everything that's close to 0 has been removed and after retraining it gets this roughly positive part.",
            "Here a negative part here is roughly separates the weights into positive and negative two clusters.",
            "Solve."
        ],
        [
            "This naturally motivates the next step, which is quantization we sharing.",
            "So can we."
        ],
        [
            "Make the weights further discreet and the answer is yes we can do with sharing to reduce the storage for each remaining weight by giving them less number of bits to represent.",
            "So instead of doing the linear quantization, which people usually do with sharing is a nonlinear quantization, the space is different between different places and can have more compression rate than linear compression.",
            "So here is the schedule.",
            "We cluster the weight first look by claiming class ring and then we generate a code book and then we quantized weights with the codebook.",
            "Finally we retrieve the code book and we do it iteratively."
        ],
        [
            "So this is overview of the pipeline.",
            "So say this is the weight matrix of a 4 by 4 weight matrix.",
            "We color them such that similar weights have the same color, for example 2.09 and 2.1 two are both blue.",
            "And then we."
        ],
        [
            "Do a K mean clustering on the weights too?"
        ],
        [
            "Now we need only to store the index.",
            "Only need to store the index, which is the last.",
            "For example, in this case it's only two bits since there are four entries in the codebook instead of 32 bit floating point numbers.",
            "And the centroid can be represented with either floating point or 16 bit integer.",
            "So this is feedforward?",
            "How about training?",
            "How do we train such network?",
            "Is it?",
            "And we find this pretty much deliverable so."
        ],
        [
            "Say we get the gradient for other weights so they are of the same size.",
            "And then we do a group by."
        ],
        [
            "So all the colors are the weights with the same color are grouped together and we do."
        ],
        [
            "A reduction multiplied by the learning rate subtracted from the original centroid."
        ],
        [
            "So that's our iteration of stochastic gradient descent, so we do this iteratively to fine tune the centroids.",
            "Know that we also try to fine tune the weight assignment the cluster, but it turned out get in few result inferior result.",
            "Then if we fix the assignment and just fine tune the centroids."
        ],
        [
            "So this shows the weight distribution after pruning and the red dots are after quantization with only four bits, we have 16 such weights, so all the weight values pick from such 1616 weights.",
            "Note that there is a zero here, and the more dense here, the more weight we have.",
            "This is different from dinner conversation.",
            "So this is also original network after pruning and after quantization.",
            "This is using 6 bits which apply to Google Net and Squeeze in at 6 bits are enough.",
            "All the way to values are now discrete.",
            "So how many bits do we really need for a network so we have see later?",
            "We found not until 2 bits.",
            "Does accuracy start to drop drastically?",
            "And for conveyors from 8 bits is starting to decrease but not until 4 bits.",
            "Did we really reach the Cliff?"
        ],
        [
            "And also we notice that quantization pruning, which is can work really well together from the Dash line and the solid line is pruning only, and its quantization only.",
            "And pruning and quantization, they don't differ at all.",
            "Sometimes even better if they get combined together."
        ],
        [
            "So this shows how many bits do we really need so."
        ],
        [
            "Or Alex net with eight bit and the five bits we absolutely have no loss of accuracy.",
            "This is a little better accuracy, probably called because of noise and with four bits and two bits we got only like 2% loss of accuracy for 2.",
            "Four base for conveyor and two bits for FC layer."
        ],
        [
            "So this talks about the whole total amount of compression rate we can get.",
            "We can push the limit to roughly only three 3% of original size without hurting the accuracy.",
            "By pruning and quantization together compared with the working individually, it works better than working individually and even better than the cheap SVD method.",
            "So why does prune and quantization work well together?",
            "Because as we see this picture."
        ],
        [
            "Pruning already roughly separates the ways into two groups, positive and negative, and we have less to do for the coming clustering.",
            "That's why we need to do pruning 1st and then quantization 2nd.",
            "OK.",
            "So we come to."
        ],
        [
            "Last step, Huffman coding.",
            "We really want to really reach the entropy of the total remaining weights.",
            "So that's."
        ],
        [
            "After quantization we can have last number of bits to represent those more frequently appearing with and more number."
        ],
        [
            "Wait and more number of bits to represent those less current ways.",
            "For example here.",
            "So by pruning quantization with sharing altogether and Huffman coding altogether."
        ],
        [
            "The compression rate we can get for Leonard, Alex.",
            "Net VGG net squeeze net and Google.",
            "Net the compression rates ranges from 10 times for those inception model which is really efficient 249 times for the other networks.",
            "So these two results are new is not in the paper yet, so this is just by pruning quantization, since Huffman coding have some overhead and people sometimes complain to me, so these results are no Huffman coding in code in here Inception model is really efficient and still, but still we got an order of magnitude of compression rate, so now everything fits in SRAM cache.",
            "What's the consequence of fitting SRAM cache?",
            "That's the speedup."
        ],
        [
            "So this is the speed up and energy efficiency on CPU and GPU with and without the.",
            "Sparsification, so this is the FC layer only, since we can directly call cool sparse to do that."
        ],
        [
            "Slater has roughly a three times speedup for the absolute value of, ranging from Alex Net, VGG.",
            "Net and neuro talk.",
            "And if you're not not satisfied with the result here, 3X3X speedup 3X energy efficiency we."
        ],
        [
            "Do the hardware accelerator which is called efficient inferior engine an ASIC chip that further pushed the result to 180 * 89 times faster than CPU 13 times faster than GPU 24,000 times more energy efficient than CPU and 3000 times more energy efficient than GPU?",
            "Mostly because everything fits in SRAM, so that's 22 orders of magnitude less energy and you don't have to go off chip.",
            "Everything is pretty much.",
            "On a single chip and you have less less work to do because of compression and you have less weight to fetch because your model is compressed.",
            "So this is beyond the scope of this paper, but you very interested.",
            "Feel free to check out our paper East Coast 16."
        ],
        [
            "OK, so as a conclusion, what does the 10 times to 50 times compression mean so we can compress the end so that deep neural network and deep learning can be put into mobile applications that are less than a 10 megabytes total, so 500 network if you have FC layers OK, we can become 10 megabytes.",
            "If you don't have FC layers, that's fine.",
            "We can still help you to combine compress it to roughly 1 megabytes from 10 megabytes to 1 megabytes, 10 times compression.",
            "So Baidu is already using this technique widely in there.",
            "Title frame their framework called Paddle, which is like the Baidu brain.",
            "So we can also save the memory bandwidth by 10 times to 50 times, particularly for FC layers in real time applications with less reuse and really expensive to fetch from the DRAM.",
            "And memory working sets fully fits in on CHIP SRAM, SRAM stand for static static RAM is on chip is pretty small but really fast.",
            "So it has only 5 pico joules per word access instead of 60 hundred 640 people.",
            "Joules per word.",
            "If you go off Chip.",
            "So that saves 100 times energy if you put it on trip."
        ],
        [
            "Finally, I would thank Bill, Mark and Andrew and Faith for the helpful guidance and we really useful discussions.",
            "And thank you."
        ],
        [
            "For the attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's a it's a best paper award.",
                    "label": 0
                },
                {
                    "sent": "Our second base pay power so it to pleasure to give that based power to someone.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Hi everyone, good morning.",
                    "label": 0
                },
                {
                    "sent": "My name is song today.",
                    "label": 0
                },
                {
                    "sent": "My presentation is about deep compression compressing the neural network to make the model smaller and have the same accuracy as at the same moment.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I briefly introduction of ourselves and the 4th year PhD student in Stanford with Professor PEO Tally Ann.",
                    "label": 1
                },
                {
                    "sent": "My research interest is in model compression and hardware acceleration and who is he?",
                    "label": 0
                },
                {
                    "sent": "Is the undergraduate chinqua.",
                    "label": 0
                },
                {
                    "sent": "He's going to join us soon.",
                    "label": 0
                },
                {
                    "sent": "My advisor, bill that is professor at Stanford and also the chief scientist at Media.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So deep learning is the next wave of AI is really wide range of applications, ranging from each image recognition to speech recognition, natural language process.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So natural question to ask is since the embedded systems has a limited amount of computation power, where should we put those competition?",
                    "label": 0
                },
                {
                    "sent": "So if we put.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, on the cloud the problems are we suffer from network delay.",
                    "label": 1
                },
                {
                    "sent": "One second for self driving.",
                    "label": 0
                },
                {
                    "sent": "Car to car might be several meters away.",
                    "label": 0
                },
                {
                    "sent": "Already suffer from the power budget budget as well for the total cost of ownership of a large data center and also it has compromised user privacy since we have to upload the images and pictures to the cloud.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing deep learning on the cloud is intelligent but inefficient.",
                    "label": 0
                },
                {
                    "sent": "So what if we write the deep learning locally on the mobile?",
                    "label": 1
                },
                {
                    "sent": "So that we are first the problem of the large model size.",
                    "label": 0
                },
                {
                    "sent": "For example, the Apple app developers suffer from the model size because I post or has this limitation that if an item is over over 100 megabyte you cannot download it unless you connect to the Wi-Fi.",
                    "label": 0
                },
                {
                    "sent": "So after we do our work of decompression, I'm uploading the paper to archive.",
                    "label": 0
                },
                {
                    "sent": "Like a few days ago, a few days later and ruin sent us an email thanking us that we can make the model smaller so that the Baidu apps can squeeze a lot of squeeze neural networks into Baidu apps without changing much of the size of their app.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another motivation is that for hardware engineers, they really suffer from the model science because embedded system have limited resources, specially limited power resource and limited battery.",
                    "label": 0
                },
                {
                    "sent": "So if we see where are the energy really consumed, that's the bottom line here, which moves of the energy is consumed by accessing DRAM, which is two or three orders of magnitude more than multiplication add.",
                    "label": 0
                },
                {
                    "sent": "And accessing the cache so if you do one DRAM access that's equal to 100 or even more multiplication and add operation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so memory access is really expensive and we want to cut the cut the size of the model to make it really efficient analyze memory access.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm proposing deep compression which gave the deep neural network smaller model size ranging from 10 times to 50 times smaller with the same accuracy.",
                    "label": 1
                },
                {
                    "sent": "I verified on image Net and with speed up, especially with specialized hardware.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overview of the results I can compress Alex Net by 35 times to only around 7 megabytes.",
                    "label": 0
                },
                {
                    "sent": "VGG net by 50 or 49 times to 11 megabytes.",
                    "label": 1
                },
                {
                    "sent": "Original idea.",
                    "label": 0
                },
                {
                    "sent": "Did Google net that comprise it turn times to only around 2.8 megabytes and squeeze net by 10 times to only around 0.47 megabytes is roughly a license 5 megabytes.",
                    "label": 0
                },
                {
                    "sent": "Too much fit in the 15 to cash.",
                    "label": 0
                },
                {
                    "sent": "And are all these cases.",
                    "label": 0
                },
                {
                    "sent": "It has no loss of accuracy on image net compared with the original network.",
                    "label": 1
                },
                {
                    "sent": "So now we can fulfill fully fit on SRAM cache.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the pipeline like?",
                    "label": 0
                },
                {
                    "sent": "It has three stages, the first one is network pruning, second one with sharing and 3rd one Huffman coding.",
                    "label": 1
                },
                {
                    "sent": "Let me start with the first one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we want to have less number of weights.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea is using pruning.",
                    "label": 1
                },
                {
                    "sent": "Pruning a network is pretty similar to if you prune a tree, you get rid of those redundant connections, so this work has been pioneered by young account back in the 1990s were called optimal brain damage, and later optimal optical brain optimal brain surgery that proposed various high risztics of how do we get rid of those redundant weights and showed pretty good results?",
                    "label": 1
                },
                {
                    "sent": "And 25 years later, we pretty much removed faster from the table and revisited this idea and show that actually worked really well on modern networks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Motivation is if we see how human brain works.",
                    "label": 1
                },
                {
                    "sent": "So at birth we have 50 trading connections in the brain.",
                    "label": 1
                },
                {
                    "sent": "I one year old that grows to 1000.",
                    "label": 1
                },
                {
                    "sent": "Trading really surged at 10 years old and mature and adult.",
                    "label": 0
                },
                {
                    "sent": "We have 500 trading connections.",
                    "label": 0
                },
                {
                    "sent": "So I get pruned.",
                    "label": 0
                },
                {
                    "sent": "So does the similar mechanism happen?",
                    "label": 0
                },
                {
                    "sent": "Can happen in artificial neural network and the answer is yes.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the redundancy on Alex net on the top and VGG net on the bottom.",
                    "label": 1
                },
                {
                    "sent": "The green is the redundancy that can be safely pruned.",
                    "label": 0
                },
                {
                    "sent": "The blue is what is left.",
                    "label": 0
                },
                {
                    "sent": "We can see that for combinators we can prune away around three times.",
                    "label": 0
                },
                {
                    "sent": "So 66% roughly, but the first layer is pretty sensitive to pruning.",
                    "label": 1
                },
                {
                    "sent": "Sensitive directly interacts with the input image, which has only three channels and FC layers are pretty much resilient pruning.",
                    "label": 0
                },
                {
                    "sent": "We can prune away 90%.",
                    "label": 0
                },
                {
                    "sent": "With ten times less parameters, so overall it's roughly 10% seven percent parameters left on Alexander VG net.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the method we do it is by training the connected first, just training on network normally and prune the connectionstring the weights again and do this iteratively so we can see that for different regularization with our two regularization without one recognization without retraining, we can have worse accuracy.",
                    "label": 1
                },
                {
                    "sent": "We can pruning around 80% but with proper training, especially L2 regularization, sometimes we get even higher accuracy and get we can prove more.",
                    "label": 1
                },
                {
                    "sent": "So by iterative pruning we can get rid of 90% of the parameters in Alex net without hurting the accuracy at all.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the result across our net Alex Net VGG net and this is the model parameters we can prune away without hurting the accuracy.",
                    "label": 0
                },
                {
                    "sent": "It ranges from 9 times to 13 times.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so yeah, other thing is about seeing how about R and R&R STM.",
                    "label": 0
                },
                {
                    "sent": "Yes, the pruning works as well, so this we experiment on neural talk which is done by car party at all.",
                    "label": 1
                },
                {
                    "sent": "So we after pruning win 90% of the parameters.",
                    "label": 1
                },
                {
                    "sent": "Here we can see that the blue score doesn't hurt but 95% it begins to get hurt the accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "At the original network sensor, Brown dogs running through a grassy field printing 90% of Brown dogs running through a grassy area.",
                    "label": 1
                },
                {
                    "sent": "Pretty much the same.",
                    "label": 1
                },
                {
                    "sent": "But if not, prune away 95%.",
                    "label": 0
                },
                {
                    "sent": "It says a man in a red shirt and black and white black shirt is running through a field.",
                    "label": 1
                },
                {
                    "sent": "It's getting drunk, but not completely drunk.",
                    "label": 0
                },
                {
                    "sent": "But if a human it's only have 5% of parameters left.",
                    "label": 0
                },
                {
                    "sent": "If any one of us here only have 5% of the connections in our brain, probably we are not going to be very well now, so it's interesting result.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The weight distribution I plotted here.",
                    "label": 1
                },
                {
                    "sent": "This is the original weight distribution of a network and this is after pruning.",
                    "label": 1
                },
                {
                    "sent": "We can see that everything that's close to 0 has been removed and after retraining it gets this roughly positive part.",
                    "label": 0
                },
                {
                    "sent": "Here a negative part here is roughly separates the weights into positive and negative two clusters.",
                    "label": 0
                },
                {
                    "sent": "Solve.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This naturally motivates the next step, which is quantization we sharing.",
                    "label": 0
                },
                {
                    "sent": "So can we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make the weights further discreet and the answer is yes we can do with sharing to reduce the storage for each remaining weight by giving them less number of bits to represent.",
                    "label": 1
                },
                {
                    "sent": "So instead of doing the linear quantization, which people usually do with sharing is a nonlinear quantization, the space is different between different places and can have more compression rate than linear compression.",
                    "label": 1
                },
                {
                    "sent": "So here is the schedule.",
                    "label": 1
                },
                {
                    "sent": "We cluster the weight first look by claiming class ring and then we generate a code book and then we quantized weights with the codebook.",
                    "label": 0
                },
                {
                    "sent": "Finally we retrieve the code book and we do it iteratively.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is overview of the pipeline.",
                    "label": 0
                },
                {
                    "sent": "So say this is the weight matrix of a 4 by 4 weight matrix.",
                    "label": 0
                },
                {
                    "sent": "We color them such that similar weights have the same color, for example 2.09 and 2.1 two are both blue.",
                    "label": 0
                },
                {
                    "sent": "And then we.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do a K mean clustering on the weights too?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we need only to store the index.",
                    "label": 0
                },
                {
                    "sent": "Only need to store the index, which is the last.",
                    "label": 0
                },
                {
                    "sent": "For example, in this case it's only two bits since there are four entries in the codebook instead of 32 bit floating point numbers.",
                    "label": 0
                },
                {
                    "sent": "And the centroid can be represented with either floating point or 16 bit integer.",
                    "label": 0
                },
                {
                    "sent": "So this is feedforward?",
                    "label": 0
                },
                {
                    "sent": "How about training?",
                    "label": 0
                },
                {
                    "sent": "How do we train such network?",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "And we find this pretty much deliverable so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say we get the gradient for other weights so they are of the same size.",
                    "label": 0
                },
                {
                    "sent": "And then we do a group by.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So all the colors are the weights with the same color are grouped together and we do.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A reduction multiplied by the learning rate subtracted from the original centroid.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's our iteration of stochastic gradient descent, so we do this iteratively to fine tune the centroids.",
                    "label": 0
                },
                {
                    "sent": "Know that we also try to fine tune the weight assignment the cluster, but it turned out get in few result inferior result.",
                    "label": 0
                },
                {
                    "sent": "Then if we fix the assignment and just fine tune the centroids.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this shows the weight distribution after pruning and the red dots are after quantization with only four bits, we have 16 such weights, so all the weight values pick from such 1616 weights.",
                    "label": 0
                },
                {
                    "sent": "Note that there is a zero here, and the more dense here, the more weight we have.",
                    "label": 0
                },
                {
                    "sent": "This is different from dinner conversation.",
                    "label": 0
                },
                {
                    "sent": "So this is also original network after pruning and after quantization.",
                    "label": 1
                },
                {
                    "sent": "This is using 6 bits which apply to Google Net and Squeeze in at 6 bits are enough.",
                    "label": 0
                },
                {
                    "sent": "All the way to values are now discrete.",
                    "label": 0
                },
                {
                    "sent": "So how many bits do we really need for a network so we have see later?",
                    "label": 0
                },
                {
                    "sent": "We found not until 2 bits.",
                    "label": 0
                },
                {
                    "sent": "Does accuracy start to drop drastically?",
                    "label": 0
                },
                {
                    "sent": "And for conveyors from 8 bits is starting to decrease but not until 4 bits.",
                    "label": 0
                },
                {
                    "sent": "Did we really reach the Cliff?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also we notice that quantization pruning, which is can work really well together from the Dash line and the solid line is pruning only, and its quantization only.",
                    "label": 1
                },
                {
                    "sent": "And pruning and quantization, they don't differ at all.",
                    "label": 0
                },
                {
                    "sent": "Sometimes even better if they get combined together.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this shows how many bits do we really need so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or Alex net with eight bit and the five bits we absolutely have no loss of accuracy.",
                    "label": 0
                },
                {
                    "sent": "This is a little better accuracy, probably called because of noise and with four bits and two bits we got only like 2% loss of accuracy for 2.",
                    "label": 0
                },
                {
                    "sent": "Four base for conveyor and two bits for FC layer.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this talks about the whole total amount of compression rate we can get.",
                    "label": 0
                },
                {
                    "sent": "We can push the limit to roughly only three 3% of original size without hurting the accuracy.",
                    "label": 0
                },
                {
                    "sent": "By pruning and quantization together compared with the working individually, it works better than working individually and even better than the cheap SVD method.",
                    "label": 1
                },
                {
                    "sent": "So why does prune and quantization work well together?",
                    "label": 0
                },
                {
                    "sent": "Because as we see this picture.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pruning already roughly separates the ways into two groups, positive and negative, and we have less to do for the coming clustering.",
                    "label": 0
                },
                {
                    "sent": "That's why we need to do pruning 1st and then quantization 2nd.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we come to.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last step, Huffman coding.",
                    "label": 0
                },
                {
                    "sent": "We really want to really reach the entropy of the total remaining weights.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After quantization we can have last number of bits to represent those more frequently appearing with and more number.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wait and more number of bits to represent those less current ways.",
                    "label": 1
                },
                {
                    "sent": "For example here.",
                    "label": 0
                },
                {
                    "sent": "So by pruning quantization with sharing altogether and Huffman coding altogether.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The compression rate we can get for Leonard, Alex.",
                    "label": 0
                },
                {
                    "sent": "Net VGG net squeeze net and Google.",
                    "label": 0
                },
                {
                    "sent": "Net the compression rates ranges from 10 times for those inception model which is really efficient 249 times for the other networks.",
                    "label": 0
                },
                {
                    "sent": "So these two results are new is not in the paper yet, so this is just by pruning quantization, since Huffman coding have some overhead and people sometimes complain to me, so these results are no Huffman coding in code in here Inception model is really efficient and still, but still we got an order of magnitude of compression rate, so now everything fits in SRAM cache.",
                    "label": 1
                },
                {
                    "sent": "What's the consequence of fitting SRAM cache?",
                    "label": 0
                },
                {
                    "sent": "That's the speedup.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the speed up and energy efficiency on CPU and GPU with and without the.",
                    "label": 0
                },
                {
                    "sent": "Sparsification, so this is the FC layer only, since we can directly call cool sparse to do that.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Slater has roughly a three times speedup for the absolute value of, ranging from Alex Net, VGG.",
                    "label": 0
                },
                {
                    "sent": "Net and neuro talk.",
                    "label": 0
                },
                {
                    "sent": "And if you're not not satisfied with the result here, 3X3X speedup 3X energy efficiency we.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do the hardware accelerator which is called efficient inferior engine an ASIC chip that further pushed the result to 180 * 89 times faster than CPU 13 times faster than GPU 24,000 times more energy efficient than CPU and 3000 times more energy efficient than GPU?",
                    "label": 0
                },
                {
                    "sent": "Mostly because everything fits in SRAM, so that's 22 orders of magnitude less energy and you don't have to go off chip.",
                    "label": 0
                },
                {
                    "sent": "Everything is pretty much.",
                    "label": 0
                },
                {
                    "sent": "On a single chip and you have less less work to do because of compression and you have less weight to fetch because your model is compressed.",
                    "label": 0
                },
                {
                    "sent": "So this is beyond the scope of this paper, but you very interested.",
                    "label": 0
                },
                {
                    "sent": "Feel free to check out our paper East Coast 16.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so as a conclusion, what does the 10 times to 50 times compression mean so we can compress the end so that deep neural network and deep learning can be put into mobile applications that are less than a 10 megabytes total, so 500 network if you have FC layers OK, we can become 10 megabytes.",
                    "label": 0
                },
                {
                    "sent": "If you don't have FC layers, that's fine.",
                    "label": 0
                },
                {
                    "sent": "We can still help you to combine compress it to roughly 1 megabytes from 10 megabytes to 1 megabytes, 10 times compression.",
                    "label": 0
                },
                {
                    "sent": "So Baidu is already using this technique widely in there.",
                    "label": 0
                },
                {
                    "sent": "Title frame their framework called Paddle, which is like the Baidu brain.",
                    "label": 0
                },
                {
                    "sent": "So we can also save the memory bandwidth by 10 times to 50 times, particularly for FC layers in real time applications with less reuse and really expensive to fetch from the DRAM.",
                    "label": 0
                },
                {
                    "sent": "And memory working sets fully fits in on CHIP SRAM, SRAM stand for static static RAM is on chip is pretty small but really fast.",
                    "label": 0
                },
                {
                    "sent": "So it has only 5 pico joules per word access instead of 60 hundred 640 people.",
                    "label": 0
                },
                {
                    "sent": "Joules per word.",
                    "label": 0
                },
                {
                    "sent": "If you go off Chip.",
                    "label": 0
                },
                {
                    "sent": "So that saves 100 times energy if you put it on trip.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, I would thank Bill, Mark and Andrew and Faith for the helpful guidance and we really useful discussions.",
                    "label": 0
                },
                {
                    "sent": "And thank you.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the attention.",
                    "label": 0
                }
            ]
        }
    }
}