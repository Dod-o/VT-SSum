{
    "id": "hhypdjnbfgcmqrjj2otdp4p3jxruc655",
    "title": "PAC-Bayesian Analysis in Unsupervised Learning",
    "info": {
        "author": [
            "Yevgeny Seldin, Department of Computer Science, University of Copenhagen"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_seldin_pbau/",
    "segmentation": [
        [
            "In my talk, I will take this back based analysis to a new field of unsupervised learning and."
        ],
        [
            "The motivation is finalized clustering and more general structure learning approaches and the problem with unsupervised learning is.",
            "The unsupervised learning is often ambiguous and it is ambiguous because many structures in any non trivial data set coexist simultaneously.",
            "And it's very hard to compare solutions.",
            "My testing some properties of the clustering itself.",
            "So for example, in image segmentation we quite often have that there are multiple different segmentations of images.",
            "That look more or less reasonable and we can't say which segmentation is better.",
            "And once we don't have a comparison once, we can't say about two solutions.",
            "Which one is better?",
            "It's very problematic for that advancement so.",
            "And there the approach that we take.",
            "We said that we don't cluster the data, though some other type of structure learning."
        ],
        [
            "Just for the sake of clustering the data, but we cluster the data in order to solve some higher level task.",
            "And the quality of Cluster Inc or some other structure learning methods should be evaluated by its contribution to the solution of the tasks that were actually trying to solve.",
            "And in my opinion, the main optical in the development of unsupervised learning is the lack of.",
            "A good formulations of those tasks that are solved with unsupervised learning.",
            "So what kind of formulation it could be so, for example?"
        ],
        [
            "If we get back to this clustering, configures example.",
            "If I say you that after you cluster this figures you will have to plug them.",
            "Then clearly clustering configures by shape is better than clustering configures.",
            "Bicolor becausw packing is indifferent to color, and you can even evaluate the amount of time saved when you pray.",
            "Cluster the figures and you get a numerical evaluation of how clustering help, so probably doesn't help in this task.",
            "So this is kind of a toy example.",
            "Now to the outline of my talk, so I will analyze this clustering problem in the context of a higher level task in the problem of clustering.",
            "And they will show that there are actually.",
            "So."
        ],
        [
            "Sorry.",
            "I will show that there are actually two types of problems which are solved by koklass think it's discriminative prediction.",
            "Of the interests of the matrix and density estimation.",
            "I will show pack based on analysis of discriminative prediction with clustering and there I will introduce combinatorial proud because it's.",
            "Combinatorial optimization problem and then I will design pack base and bound for discrete density estimation and I will use it to analyze density estimation with clustering.",
            "And then I will talk about extensions of this approach to other problems."
        ],
        [
            "So first, what is the problem of discriminative prediction with Coke last encounter?",
            "Most famous example of this problem is collaborative filtering.",
            "So in collaborative filtering we have a matrix of viewers by movies and we have the ratings that the viewers give to the movies and we want to predict the missing entries in this matrix.",
            "So here the goal is actually to find the discriminative prediction rule that, given a viewer and the movie.",
            "Will give us the right the expected rating that the viewer is going to give that movie and they will."
        ],
        [
            "Creation here is clear.",
            "We want to take the expectation with respect to their true joint probability distribution of viewers, movies and ratings of the expectation with respect to our class.",
            "Discriminative classifier of some given loss function.",
            "Now the second problem that is usually solved and this comparison is actually model independent comparison.",
            "So you can."
        ],
        [
            "Solve IT vehicle class and we can solve it via London's alert and semantic analysis or some other methods, and we can just compare which method gives you better predictions.",
            "Now."
        ],
        [
            "Now the second problem is an analysis of coherence data and the most famous example here is analysis of Word document Co occurrence matrices.",
            "And the this task was usually people design algorithm for solving 1 task and applied to the second task and vice versa.",
            "But this task is actually different from the previous task.",
            "And why's it different in the previous task?",
            "The rating was some function of your and movie.",
            "No matter you can add more viewers, you can add more movies.",
            "The rating that given viewer gives to a given movie.",
            "It will not change by.",
            "I didn't more interested in the matrix here.",
            "It's a giant probably."
        ],
        [
            "The distribution if you add more documents or add more words.",
            "The joint probability of current event will change because you are changing the space.",
            "You have to re normalize it.",
            "So the way we suggested to think about this problem is to think."
        ],
        [
            "About this problem is density estimation task of actually learning the joint probability distribution of words and documents.",
            "And the way you can think about it is that you have some collection of documents.",
            "You sample a random document, you sample a random word from the document, you write it down.",
            "Your sample some document, again sample somewhere from the document you write it, the count and this matrix at some point and tell you well.",
            "I don't need anymore something I can predict the distribution of all the words over all the documents.",
            "And this means that they have learned this problem.",
            "So I'm generally trying to find some estimator for this joint probability distribution of words and documents, and then the natural way to evaluate such rule is."
        ],
        [
            "To consider the expectation of logarithmic loss of my predictor.",
            "And once again this evaluation, it's independent."
        ],
        [
            "Of the specific way you try.",
            "To solve this problem so you can once again solve it with class and we can.",
            "Use probabilistic Latin models and so on.",
            "You can compare clustering control two or class and three clusters.",
            "My weather classroom to clusters gives you better predictions than clustering into three clusters, something that you could not do with current models.",
            "So it's just simply here.",
            "The word occurs in the document.",
            "Probability of that.",
            "You actually get also the number of like you have this joint probability distribution.",
            "If I give you the total size of the corpus, we can predict how many words are there in each document.",
            "Just the normalization factor.",
            "Um?"
        ],
        [
            "Now what is our?",
            "Prediction model, so the prediction model were analyzed.",
            "Is discriminative prediction based on clustering, which means that given a viewer and the movie pair.",
            "So we take this space of viewers by movies and we cluster it into clusters of years and clusters of movies.",
            "And this assignment is actually stochastic assignment.",
            "And given some viewer movie pair, we assign the viewer to some cluster of viewers assigned the movie to some cluster of movies and we see what is the expected rating in that cluster space of view viewer clusters by movies clusters and return that correspondent rating.",
            "You can also see this as this graphical model where the viewer sum up to the clusters of years.",
            "The movie sum up to the clusters of movies and the prediction is done based on this cluster product space."
        ],
        [
            "So we want to make based on analysis of this problem and this is our prediction rule and in order to make the back base analysis we have to define our hypothesis space.",
            "We have to define some prior over reporters space.",
            "We have to define the posterior and then we have to calculate the KL diversion between the prior and the posterior.",
            "So first of all that he.",
            "Outer space is the space of all hard partitions of the space of viewers by movies.",
            "Plus, the labels that are assigned to this cluster product space.",
            "The prior I will define in the next slide that will be some combinatorial prior.",
            "Now know that this three conditional distributions in our prediction model.",
            "They actually define a distribution over this space of.",
            "I keep up with this.",
            "How they define it.",
            "We can go and for each viewer.",
            "Draw a random cluster according to the first distribution.",
            "The same to do for the movies and then to draw a label.",
            "According to this conditional distribution and then we can predict that label.",
            "So generally this prediction rule.",
            "It actually skips the step of drawing the complete hypothesis given a certain viewer and movie.",
            "But we can see it as there is some hypothesis that is drawn at the background.",
            "And one more technical thing that will come a bit later than according to this condition.",
            "So our joint distribution according to the model of, for example, viewers and clusters of viewers, goes as a uniform distribution over.",
            "The viewers.",
            "Times the conditional distribution of drawing cluster given the view just because of this process that we go and for each viewer we assign that we're to cluster.",
            "So now to the definition of our prior over this hypothesis space.",
            "And we do that just by counting the possible ways to assign viewers to clusters of years.",
            "So first of all we have."
        ],
        [
            "Have the number of viewers possibilities to choose the number of clusters.",
            "We can put all the viewers in one big cluster.",
            "We can put every viewer in a separate cluster and all the possibilities in the middle."
        ],
        [
            "Now, once we have chosen the number of clusters, there are at most the number of viewers to the power of the number of clusters minus one.",
            "Possibilities to choose the sizes of the clusters.",
            "The cause, the number of clusters minus one.",
            "Sizes are flexible and then the last the size of the last cluster is determined by the size of the remaining cluster, because they'll have to sum up to the number of years.",
            "Once we've chosen the cardinality's of the clusters."
        ],
        [
            "We have this multinomial coefficient number of possibilities to actually to assign the viewers to the cluster so that the sizes of the clusters are.",
            "As we specified them.",
            "So for example, if we want to cluster four viewers into two clusters.",
            "And let's say that we have one cluster of size one and one cluster of size 3.",
            "There are four possibilities to make this, and if we say that the clusters are balanced, then there are 4 / 2, which is 6 possibilities to assign the viewers clusters of years.",
            "And this multinomial coefficient can bounded by this card number of years times the entropy of this cardinality profile.",
            "M."
        ],
        [
            "Then we have the number of labels to the power of the number of partitions, cells, possibilities to assign labels.",
            "To this cluster cells.",
            "Thanks.",
            "Now if we just put all the combinatorial calculations together.",
            "We get that we can define a prior which is.",
            "Exponent of the minus number of years times the entropy of their."
        ],
        [
            "Masters this second term comes from here and the last term from the number of possibilities to assign.",
            "The labels to the partition cells.",
            "Came."
        ],
        [
            "Now the next step is to calculate the KL diversions between the prior and the posterior that we have defined, yeah?",
            "Number, I mean what most expected size.",
            "Just take it."
        ],
        [
            "And we use that relevant for class and then you will get a very characteristic distribution.",
            "So thick.",
            "Distribution of cluster size yeah?",
            "So it comes at so actually the main term in this prior it's this entropy, and generally what you get is that the prior is so.",
            "What you do, you define this all possible clusterings.",
            "According to um.",
            "Symmetric symmetric subgroups.",
            "What is a symmetric subgroup?",
            "That's a group which is in different under permutation of the names.",
            "Of the clusters.",
            "And then the main term is actually the number of, so this term it's logarithmic in the number of viewers and this term it's linear in the number of years, so this one is dominating the second one.",
            "And this time it shows once again, once you have chosen the cardinalities of clusters, how many ways you have to assign the viewers to the clusters?",
            "Anne.",
            "Maybe it's exactly where their self lack the slack share is where like this bound on the Hamilton normal coefficient is very tight.",
            "So in some ways.",
            "I think it will be something like.",
            "Nothing for me.",
            "But Dennis come.",
            "I'm missing this put double if you sample it, drop it by writing letters about how you do it right and say, well, you know she's a neighbor.",
            "Or you could say skate 3 or whatever, and I'm kind of missing that here.",
            "It seems like you know it's just some more biased towards cluster size distributions that computer in practice, or which are more like skate 3 type thing that you have some very big clusters.",
            "You know there's all these people analyzing this instead.",
            "So.",
            "Essentially, the entropy isn't it the content, so I think it's going to make you know, because it basically all possible partitions are sort of.",
            "All actual custom partitions are equally likely.",
            "Most partitions occur with roughly 4 components, so it's going to be average.",
            "It's going to be sort of much more likely you pick partitions which are roughly.",
            "I.",
            "No, no, no.",
            "It is so like generally.",
            "Throw it to the according to the sample to the prior you want to get simple partitions.",
            "Simple partitions are actually.",
            "Other partitions which have few few clusters.",
            "All partitions which are highly unbalanced.",
            "So you don't want to choose balanced partitions because the subspace of balanced partitions is complex.",
            "And the way you can think about it is that if you want to specify some unbalanced partition, you have to spend much more bits in order to specify it you have, you need.",
            "So if you look at."
        ],
        [
            "This picture you need more bits to specify a balanced partition because you have to tell for each of them which cluster it goes to, rather than to space.",
            "Specify an unbalanced partitioning.",
            "Unbalanced partitioner.",
            "Just tell me who is the outlier.",
            "So you get log off.",
            "Here you have a log of four.",
            "Bits to specify unbalanced partition.",
            "You have log of 6 bits.",
            "To specify a balanced partition.",
            "So.",
            "Don't wanna have all the same size.",
            "Yeah, so unbalanced unbalanced partitions are simpler than balanced partitions and despair favors choosing unbalanced partitions OK. OK.",
            "Very good question and now how I calculate this KL divergent within the posterior and prior.",
            "So this is the prior and then actually about this product I can."
        ],
        [
            "Also so.",
            "If you don't have some a priori."
        ],
        [
            "Knowledge about.",
            "Actually, which pairs of yours?",
            "For example, should go together?",
            "I can show you that you can't construct any prior that will give you significantly significantly better bound becausw this term of the entropy will will get into the bound if you are indifferent to permutations of the viewers, you will get this entropy term, which is the leading term it is bound.",
            "So generally, if you don't have some specific information about, for example pairs of years that should come together, then you can design A better prior on this space.",
            "OK.",
            "I."
        ],
        [
            "So how we calculate the care divergent between the posterior on the prior?",
            "So this is our prior and once again.",
            "I reminded according to the posterior, we go over all the viewers and withdraw a cluster for each viewer.",
            "Which means that.",
            "The probability of a class that is actually this one over the number of years.",
            "Some overall views of the probability of a sign that we are to the cluster."
        ],
        [
            "And they look at this scale.",
            "Divergent says the expectation of logarithm of the prior minus the entropy of Q.",
            "Now for the first term, if I go according to this."
        ],
        [
            "Is um?",
            "Strategy of drawing partition.",
            "The entropy of the like.",
            "But the class status is going to be concentrating around the entropy of this distribution.",
            "If I go over all viewers and assign the cluster to ensure according to this conditional distribution, then the entropy of the resulting.",
            "Distribution is going to be the entropy of this marginal.",
            "OK. And I can show him slightly skipping here, but I can show that the entropy of there.",
            "Q is actually the entropy of this conditional distribution."
        ],
        [
            "And then if I put this two together."
        ],
        [
            "I get the kill the versions is actually bounded by the mutual information.",
            "The clusters preserve on their original parameters, which are viewers or movies.",
            "And some other things that are less important here.",
            "And this mutual information is the mutual information according to this joint distribution, which is uniform in viewers and movies, and the conditional that we have chosen in our model."
        ],
        [
            "K. Which actually gives us the generalization bound which tells that the KL divergent between the empirical and.",
            "The expected loss of this prediction strategy is bounded by.",
            "Some of the cardinality's of the variables times the mutual information that the cluster variables preserved from the original variables and some other.",
            "So this is the logarithmic terms that we had.",
            "This is the number of partitions cells.",
            "And the usual part of Park Basin bonds.",
            "That we have."
        ],
        [
            "Now first, let's see that this bound follows our intuitions about this problem.",
            "So if we go take the extreme case as if we put all the data in one big cluster, then our empirical loss is going to be high.",
            "But we know that this empirical loss is close to the true loss.",
            "And the bond actually shows that this mutual information is zero in this case, and therefore the bound is closed like the empirical loss is close to the true one.",
            "But it can be large if we go to the case where we put everybody in a separate cluster, then we can achieve empirical loss which is 0.",
            "But the mutual information is large in this case.",
            "And we get that this empirical loss of zero.",
            "It's actually far away from the true loss.",
            "So this month follows our intuition.",
            "It."
        ],
        [
            "Further tells us that we should prefer unbalanced partitions over balanced partitions, and this trend is actually opposite to.",
            "Quite a lot of clustering algorithms where they introduced this artificially introduced the bias to balance tradition.",
            "So here we show that this bias is incorrect.",
            "You should prefer unbalanced partitions in clustering rather than balanced partitions.",
            "So we get this optimization tradeoff between the empirical loss and the mutual information that the clusters preserve from the original parameters.",
            "And we have applied.",
            "So here I just replaced."
        ],
        [
            "With the thread of this parameter beta.",
            "Chelsea goes to this cat on a farm of bound.",
            "That's what I mentioned and then we applied it to the movielens data set which is.",
            "100,000 ratings for both 1000 by 1500 movies.",
            "And we achieved the mean absolute error of zero point 72 in predicting the ratings which are on five star scale.",
            "So the maximum error is 4 here.",
            "Not one.",
            "And here are some."
        ],
        [
            "Graphs So what?",
            "I show you this is.",
            "The test loss as a function of betting.",
            "This tradeoff and this is actually what France are called.",
            "Cigarettes that bound I didn't like.",
            "Probably I should have used the catoni bound here.",
            "Um?",
            "And you see that.",
            "And this is the same curve just zoomed in.",
            "Here is 0.72 the optimal prediction and you see that if we use certain by 6 clusters, then actually the number of clusters provide some regularization.",
            "So when beta grows, this doesn't really.",
            "Influence the performance, but if."
        ],
        [
            "Goal if we increase the number of clusters then we see that actually this thread of it controls regularization of this problem.",
            "And the bound it still like once again maximum value is 4, so the the bound doesn't really follow the shape of the laws, but it's still meaningful and we can even go to the."
        ],
        [
            "Theme of taken so in this almost 300 by 300 clusters, the size of the cluster space is the same as the number of samples.",
            "So in the bad case we can just put every sample in separate cluster.",
            "But we see that this thread of controls regularization of this problem and in this case it's the complete response of the responsibility of this thread of control regularization.",
            "The number of clusters doesn't provide any regularization."
        ],
        [
            "Now to the Park Basin bounds of discrete.",
            "For discrete density estimation.",
            "So we have a sample SpaceX and our supportive spaces space of functions from this sample space, which can be also infinite to some finite subspace as finite space Z.",
            "And we say that.",
            "The probability for each such function, the probability of.",
            "There is the probability according to the distribution of X that verb serve.",
            "This value Z. K."
        ],
        [
            "And once again, we can define a prior over to his point in space.",
            "We can define the posterior over things reported space, and we say that.",
            "Even though exports according to the posterior with noted by PQ.",
            "Of this, it's the expectation with respect to Q of the probability of observing them."
        ],
        [
            "And then forget that with probability greater than one minus Delta for all possible distributions over age, the KL divergent between the distribution.",
            "The empirical distribution over the and the true distribution over the is bounded.",
            "Bye.",
            "This term.",
            "So if we compare it."
        ],
        [
            "We can say so this is the bound and actually the bound for classification.",
            "It's a special case of this bound for density estimation, because if we say that this Z is actually the error variable.",
            "Then classification is density estimation on this error variable.",
            "And the cardinality of these two.",
            "So here we get one and we.",
            "Forget the pack base and bound for classification.",
            "But generally it means that we can obtain some more refined information, not just a single parameter like the error.",
            "But we can.",
            "Get the modifying the information about X by this projections to multiple sets.",
            "K. Uh, the idea?"
        ],
        [
            "But the proof is really simple, so we use this change of measure and quality are.",
            "Is Matthias said the most important inequality and all this field?",
            "So for some functions, expectation with respect to Q is bounded by the KL divergent plus the log logarithm of expectation."
        ],
        [
            "Vectibix and if we choose this fee as the Guild N times the KL divergent between the empirical and through distribution.",
            "And the next slide I will show that this is bounded by N + 1 to the power of the cardinality of the minus one.",
            "And just the standard method of types argument.",
            "And then we can show that this guy is bounded by actually N + 1 to the power cardinality of the."
        ],
        [
            "Minus one which gives the bound.",
            "OK so the only thing we have to do is to bound.",
            "This time, so how we do it?"
        ],
        [
            "Um, I did not buy an want to end this.",
            "The number of times we observed each value of Z and then the expectation of this scale divergent between the empirical.",
            "And the through distribution over Z.",
            "It's some over this possible types that we can observe of the number of ways to.",
            "To get this type times the probability of the type types times.",
            "Actually, the values that we are bounding."
        ],
        [
            "And then, once again, this multinomial coefficient can be bounded by the end times the entropy.",
            "Here I just put this to the exponent, then these two.",
            "Are just minus the KL divergent's."
        ],
        [
            "And they get that this is actually sum over all possible types of 1, which is bounded by.",
            "N + 1 to the cardinality of Z -- 5.",
            "Actually, the same proof that we had for the classification case.",
            "Just we look at more values of these and how we apply."
        ],
        [
            "This result to density estimation with clustering, so we are given some empirical distribution over X1 and X2 and we want to estimate the true distribution.",
            "Actually we want to find some estimator that will minimize the expected loss logarithmic loss.",
            "For Q.",
            "And the model that we are using is again this core clustering model so."
        ],
        [
            "We use once again clustering golf.",
            "In this case words and documents, and we said that the probability of, let's say, a Word document is some over the clusters of the probability of.",
            "Observing this pair of clusters times the probability of observing some some word in its cluster and the document and that cluster of documents.",
            "And we can just rewrite it using the base rule.",
            "As.",
            "The second formula.",
            "And now the point is, is that when we cluster this space of words by documents?",
            "The empirical distribution over the cluster space is going to converge to the true distribution over the cluster space.",
            "Much faster than the distribution over the whole space converges to the true distribution over the whole space because the cluster space is much smaller.",
            "So what we can do?"
        ],
        [
            "We can take this distribution over the cluster space plus some small constant like to take it away from zero.",
            "And we can just use it.",
            "As an estimator.",
            "For this values.",
            "OK."
        ],
        [
            "Ha.",
            "So this is our estimator.",
            "And using that bound on the density estimation, we can get that.",
            "This expectation that we wanted to bound is bounded by.",
            "The mutual information between the cluster variables.",
            "Plus something that once again depends on the information.",
            "That the cluster viral variables preserve on the original parameters.",
            "OK."
        ],
        [
            "And there are some constant that I'm not showing.",
            "So related work for example wasn't the information theoretic clustering, but information.",
            "Theoretical classroom could just say to maximize this mutual information.",
            "And the park, based on the project, really provides the regularization term.",
            "To this problem.",
            "Um?",
            "And once again, it may be useful to look at the extreme cases so."
        ],
        [
            "If we put all the data in one big cluster, well, we will know actually the distribution over that that big cluster.",
            "But this big cluster.",
            "It provides us no additional information.",
            "But the bond is actually good, so zero is actually trivial solution to this problem.",
            "If we go to the other extreme and we put everybody in a separate cluster, then this mutual information is going to be as the mutual information between the variables.",
            "But.",
            "We will know that.",
            "It's actually far away from the truth.",
            "So we want to find some some clustering that will take actually the thread of will be smaller than zero, because zero is trivial solution, But the bound is actually not zero because I have some of the entropies of the variables in this K. So it's actually positive.",
            "Old time.",
            "Um, so now I will talk about some."
        ],
        [
            "I'm right around, yeah, so about that towards about extension of this approaches to graphical models and some other problems so we can see this discriminative prediction and density estimation model that we have."
        ],
        [
            "And now we can just recursively applied, bound and get graphical models in in shapes of the tree and then the Mount will tell that.",
            "The generalization performance is governed by a tradeoff between the empirical performance and the amount of mutual information that is propagated up the tree.",
            "And actually the future work will be how we optimize, like how we actually propagate this mutual information up the threshold.",
            "We put more information at the lower variable.",
            "So maybe to make.",
            "A more tight bound to the higher level and applications and more general graphical models.",
            "Another application of this framework can be to graph clustering."
        ],
        [
            "And the pairwise clustering.",
            "So in graph clustering we can say that the weights on the edges are generated according to.",
            "Some conditional distribution.",
            "And then if we cluster the nodes.",
            "Into clusters, and we said that the wait actually only depends on the clusters where the node seat.",
            "We get exactly the Co clustering model.",
            "Just these two distributions are the same.",
            "OK, and.",
            "That's actually."
        ],
        [
            "Then with my talk, so I presented you back based on analysis of unsupervised learning of Co clustering, three shape graphical models, graph clustering and pairwise clustering.",
            "Pack base and bound for discrete density estimation.",
            "The usage of combinatorial priors in this discrete optimization problems.",
            "They have shown that they bring to mutual information validation."
        ],
        [
            "Adams and some future directions that would be interesting, it's.",
            "Extension to continuous clustering models like for example K means mixture of Gaussians.",
            "Extension to general graphical models.",
            "Not only three shape graphical models.",
            "And also application reinforcement learning with that for already mentioned couple of times in this workshop.",
            "So thanks for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In my talk, I will take this back based analysis to a new field of unsupervised learning and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The motivation is finalized clustering and more general structure learning approaches and the problem with unsupervised learning is.",
                    "label": 1
                },
                {
                    "sent": "The unsupervised learning is often ambiguous and it is ambiguous because many structures in any non trivial data set coexist simultaneously.",
                    "label": 0
                },
                {
                    "sent": "And it's very hard to compare solutions.",
                    "label": 1
                },
                {
                    "sent": "My testing some properties of the clustering itself.",
                    "label": 0
                },
                {
                    "sent": "So for example, in image segmentation we quite often have that there are multiple different segmentations of images.",
                    "label": 0
                },
                {
                    "sent": "That look more or less reasonable and we can't say which segmentation is better.",
                    "label": 0
                },
                {
                    "sent": "And once we don't have a comparison once, we can't say about two solutions.",
                    "label": 0
                },
                {
                    "sent": "Which one is better?",
                    "label": 0
                },
                {
                    "sent": "It's very problematic for that advancement so.",
                    "label": 0
                },
                {
                    "sent": "And there the approach that we take.",
                    "label": 0
                },
                {
                    "sent": "We said that we don't cluster the data, though some other type of structure learning.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just for the sake of clustering the data, but we cluster the data in order to solve some higher level task.",
                    "label": 1
                },
                {
                    "sent": "And the quality of Cluster Inc or some other structure learning methods should be evaluated by its contribution to the solution of the tasks that were actually trying to solve.",
                    "label": 0
                },
                {
                    "sent": "And in my opinion, the main optical in the development of unsupervised learning is the lack of.",
                    "label": 0
                },
                {
                    "sent": "A good formulations of those tasks that are solved with unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So what kind of formulation it could be so, for example?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we get back to this clustering, configures example.",
                    "label": 0
                },
                {
                    "sent": "If I say you that after you cluster this figures you will have to plug them.",
                    "label": 0
                },
                {
                    "sent": "Then clearly clustering configures by shape is better than clustering configures.",
                    "label": 1
                },
                {
                    "sent": "Bicolor becausw packing is indifferent to color, and you can even evaluate the amount of time saved when you pray.",
                    "label": 1
                },
                {
                    "sent": "Cluster the figures and you get a numerical evaluation of how clustering help, so probably doesn't help in this task.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a toy example.",
                    "label": 0
                },
                {
                    "sent": "Now to the outline of my talk, so I will analyze this clustering problem in the context of a higher level task in the problem of clustering.",
                    "label": 0
                },
                {
                    "sent": "And they will show that there are actually.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "I will show that there are actually two types of problems which are solved by koklass think it's discriminative prediction.",
                    "label": 0
                },
                {
                    "sent": "Of the interests of the matrix and density estimation.",
                    "label": 0
                },
                {
                    "sent": "I will show pack based on analysis of discriminative prediction with clustering and there I will introduce combinatorial proud because it's.",
                    "label": 1
                },
                {
                    "sent": "Combinatorial optimization problem and then I will design pack base and bound for discrete density estimation and I will use it to analyze density estimation with clustering.",
                    "label": 1
                },
                {
                    "sent": "And then I will talk about extensions of this approach to other problems.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first, what is the problem of discriminative prediction with Coke last encounter?",
                    "label": 1
                },
                {
                    "sent": "Most famous example of this problem is collaborative filtering.",
                    "label": 0
                },
                {
                    "sent": "So in collaborative filtering we have a matrix of viewers by movies and we have the ratings that the viewers give to the movies and we want to predict the missing entries in this matrix.",
                    "label": 1
                },
                {
                    "sent": "So here the goal is actually to find the discriminative prediction rule that, given a viewer and the movie.",
                    "label": 0
                },
                {
                    "sent": "Will give us the right the expected rating that the viewer is going to give that movie and they will.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Creation here is clear.",
                    "label": 0
                },
                {
                    "sent": "We want to take the expectation with respect to their true joint probability distribution of viewers, movies and ratings of the expectation with respect to our class.",
                    "label": 0
                },
                {
                    "sent": "Discriminative classifier of some given loss function.",
                    "label": 0
                },
                {
                    "sent": "Now the second problem that is usually solved and this comparison is actually model independent comparison.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solve IT vehicle class and we can solve it via London's alert and semantic analysis or some other methods, and we can just compare which method gives you better predictions.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the second problem is an analysis of coherence data and the most famous example here is analysis of Word document Co occurrence matrices.",
                    "label": 0
                },
                {
                    "sent": "And the this task was usually people design algorithm for solving 1 task and applied to the second task and vice versa.",
                    "label": 0
                },
                {
                    "sent": "But this task is actually different from the previous task.",
                    "label": 0
                },
                {
                    "sent": "And why's it different in the previous task?",
                    "label": 0
                },
                {
                    "sent": "The rating was some function of your and movie.",
                    "label": 0
                },
                {
                    "sent": "No matter you can add more viewers, you can add more movies.",
                    "label": 0
                },
                {
                    "sent": "The rating that given viewer gives to a given movie.",
                    "label": 0
                },
                {
                    "sent": "It will not change by.",
                    "label": 0
                },
                {
                    "sent": "I didn't more interested in the matrix here.",
                    "label": 0
                },
                {
                    "sent": "It's a giant probably.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The distribution if you add more documents or add more words.",
                    "label": 0
                },
                {
                    "sent": "The joint probability of current event will change because you are changing the space.",
                    "label": 1
                },
                {
                    "sent": "You have to re normalize it.",
                    "label": 0
                },
                {
                    "sent": "So the way we suggested to think about this problem is to think.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About this problem is density estimation task of actually learning the joint probability distribution of words and documents.",
                    "label": 0
                },
                {
                    "sent": "And the way you can think about it is that you have some collection of documents.",
                    "label": 0
                },
                {
                    "sent": "You sample a random document, you sample a random word from the document, you write it down.",
                    "label": 0
                },
                {
                    "sent": "Your sample some document, again sample somewhere from the document you write it, the count and this matrix at some point and tell you well.",
                    "label": 0
                },
                {
                    "sent": "I don't need anymore something I can predict the distribution of all the words over all the documents.",
                    "label": 0
                },
                {
                    "sent": "And this means that they have learned this problem.",
                    "label": 0
                },
                {
                    "sent": "So I'm generally trying to find some estimator for this joint probability distribution of words and documents, and then the natural way to evaluate such rule is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To consider the expectation of logarithmic loss of my predictor.",
                    "label": 0
                },
                {
                    "sent": "And once again this evaluation, it's independent.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the specific way you try.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem so you can once again solve it with class and we can.",
                    "label": 0
                },
                {
                    "sent": "Use probabilistic Latin models and so on.",
                    "label": 0
                },
                {
                    "sent": "You can compare clustering control two or class and three clusters.",
                    "label": 0
                },
                {
                    "sent": "My weather classroom to clusters gives you better predictions than clustering into three clusters, something that you could not do with current models.",
                    "label": 0
                },
                {
                    "sent": "So it's just simply here.",
                    "label": 0
                },
                {
                    "sent": "The word occurs in the document.",
                    "label": 0
                },
                {
                    "sent": "Probability of that.",
                    "label": 0
                },
                {
                    "sent": "You actually get also the number of like you have this joint probability distribution.",
                    "label": 0
                },
                {
                    "sent": "If I give you the total size of the corpus, we can predict how many words are there in each document.",
                    "label": 0
                },
                {
                    "sent": "Just the normalization factor.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now what is our?",
                    "label": 0
                },
                {
                    "sent": "Prediction model, so the prediction model were analyzed.",
                    "label": 0
                },
                {
                    "sent": "Is discriminative prediction based on clustering, which means that given a viewer and the movie pair.",
                    "label": 0
                },
                {
                    "sent": "So we take this space of viewers by movies and we cluster it into clusters of years and clusters of movies.",
                    "label": 0
                },
                {
                    "sent": "And this assignment is actually stochastic assignment.",
                    "label": 0
                },
                {
                    "sent": "And given some viewer movie pair, we assign the viewer to some cluster of viewers assigned the movie to some cluster of movies and we see what is the expected rating in that cluster space of view viewer clusters by movies clusters and return that correspondent rating.",
                    "label": 0
                },
                {
                    "sent": "You can also see this as this graphical model where the viewer sum up to the clusters of years.",
                    "label": 0
                },
                {
                    "sent": "The movie sum up to the clusters of movies and the prediction is done based on this cluster product space.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we want to make based on analysis of this problem and this is our prediction rule and in order to make the back base analysis we have to define our hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "We have to define some prior over reporters space.",
                    "label": 0
                },
                {
                    "sent": "We have to define the posterior and then we have to calculate the KL diversion between the prior and the posterior.",
                    "label": 0
                },
                {
                    "sent": "So first of all that he.",
                    "label": 0
                },
                {
                    "sent": "Outer space is the space of all hard partitions of the space of viewers by movies.",
                    "label": 0
                },
                {
                    "sent": "Plus, the labels that are assigned to this cluster product space.",
                    "label": 0
                },
                {
                    "sent": "The prior I will define in the next slide that will be some combinatorial prior.",
                    "label": 0
                },
                {
                    "sent": "Now know that this three conditional distributions in our prediction model.",
                    "label": 0
                },
                {
                    "sent": "They actually define a distribution over this space of.",
                    "label": 0
                },
                {
                    "sent": "I keep up with this.",
                    "label": 0
                },
                {
                    "sent": "How they define it.",
                    "label": 0
                },
                {
                    "sent": "We can go and for each viewer.",
                    "label": 0
                },
                {
                    "sent": "Draw a random cluster according to the first distribution.",
                    "label": 0
                },
                {
                    "sent": "The same to do for the movies and then to draw a label.",
                    "label": 0
                },
                {
                    "sent": "According to this conditional distribution and then we can predict that label.",
                    "label": 0
                },
                {
                    "sent": "So generally this prediction rule.",
                    "label": 0
                },
                {
                    "sent": "It actually skips the step of drawing the complete hypothesis given a certain viewer and movie.",
                    "label": 0
                },
                {
                    "sent": "But we can see it as there is some hypothesis that is drawn at the background.",
                    "label": 0
                },
                {
                    "sent": "And one more technical thing that will come a bit later than according to this condition.",
                    "label": 0
                },
                {
                    "sent": "So our joint distribution according to the model of, for example, viewers and clusters of viewers, goes as a uniform distribution over.",
                    "label": 0
                },
                {
                    "sent": "The viewers.",
                    "label": 0
                },
                {
                    "sent": "Times the conditional distribution of drawing cluster given the view just because of this process that we go and for each viewer we assign that we're to cluster.",
                    "label": 0
                },
                {
                    "sent": "So now to the definition of our prior over this hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "And we do that just by counting the possible ways to assign viewers to clusters of years.",
                    "label": 0
                },
                {
                    "sent": "So first of all we have.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have the number of viewers possibilities to choose the number of clusters.",
                    "label": 1
                },
                {
                    "sent": "We can put all the viewers in one big cluster.",
                    "label": 0
                },
                {
                    "sent": "We can put every viewer in a separate cluster and all the possibilities in the middle.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, once we have chosen the number of clusters, there are at most the number of viewers to the power of the number of clusters minus one.",
                    "label": 0
                },
                {
                    "sent": "Possibilities to choose the sizes of the clusters.",
                    "label": 1
                },
                {
                    "sent": "The cause, the number of clusters minus one.",
                    "label": 0
                },
                {
                    "sent": "Sizes are flexible and then the last the size of the last cluster is determined by the size of the remaining cluster, because they'll have to sum up to the number of years.",
                    "label": 0
                },
                {
                    "sent": "Once we've chosen the cardinality's of the clusters.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have this multinomial coefficient number of possibilities to actually to assign the viewers to the cluster so that the sizes of the clusters are.",
                    "label": 0
                },
                {
                    "sent": "As we specified them.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we want to cluster four viewers into two clusters.",
                    "label": 0
                },
                {
                    "sent": "And let's say that we have one cluster of size one and one cluster of size 3.",
                    "label": 0
                },
                {
                    "sent": "There are four possibilities to make this, and if we say that the clusters are balanced, then there are 4 / 2, which is 6 possibilities to assign the viewers clusters of years.",
                    "label": 0
                },
                {
                    "sent": "And this multinomial coefficient can bounded by this card number of years times the entropy of this cardinality profile.",
                    "label": 0
                },
                {
                    "sent": "M.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we have the number of labels to the power of the number of partitions, cells, possibilities to assign labels.",
                    "label": 1
                },
                {
                    "sent": "To this cluster cells.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Now if we just put all the combinatorial calculations together.",
                    "label": 0
                },
                {
                    "sent": "We get that we can define a prior which is.",
                    "label": 0
                },
                {
                    "sent": "Exponent of the minus number of years times the entropy of their.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Masters this second term comes from here and the last term from the number of possibilities to assign.",
                    "label": 1
                },
                {
                    "sent": "The labels to the partition cells.",
                    "label": 1
                },
                {
                    "sent": "Came.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the next step is to calculate the KL diversions between the prior and the posterior that we have defined, yeah?",
                    "label": 0
                },
                {
                    "sent": "Number, I mean what most expected size.",
                    "label": 0
                },
                {
                    "sent": "Just take it.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we use that relevant for class and then you will get a very characteristic distribution.",
                    "label": 0
                },
                {
                    "sent": "So thick.",
                    "label": 0
                },
                {
                    "sent": "Distribution of cluster size yeah?",
                    "label": 0
                },
                {
                    "sent": "So it comes at so actually the main term in this prior it's this entropy, and generally what you get is that the prior is so.",
                    "label": 0
                },
                {
                    "sent": "What you do, you define this all possible clusterings.",
                    "label": 0
                },
                {
                    "sent": "According to um.",
                    "label": 0
                },
                {
                    "sent": "Symmetric symmetric subgroups.",
                    "label": 0
                },
                {
                    "sent": "What is a symmetric subgroup?",
                    "label": 0
                },
                {
                    "sent": "That's a group which is in different under permutation of the names.",
                    "label": 0
                },
                {
                    "sent": "Of the clusters.",
                    "label": 0
                },
                {
                    "sent": "And then the main term is actually the number of, so this term it's logarithmic in the number of viewers and this term it's linear in the number of years, so this one is dominating the second one.",
                    "label": 0
                },
                {
                    "sent": "And this time it shows once again, once you have chosen the cardinalities of clusters, how many ways you have to assign the viewers to the clusters?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's exactly where their self lack the slack share is where like this bound on the Hamilton normal coefficient is very tight.",
                    "label": 0
                },
                {
                    "sent": "So in some ways.",
                    "label": 0
                },
                {
                    "sent": "I think it will be something like.",
                    "label": 0
                },
                {
                    "sent": "Nothing for me.",
                    "label": 0
                },
                {
                    "sent": "But Dennis come.",
                    "label": 0
                },
                {
                    "sent": "I'm missing this put double if you sample it, drop it by writing letters about how you do it right and say, well, you know she's a neighbor.",
                    "label": 0
                },
                {
                    "sent": "Or you could say skate 3 or whatever, and I'm kind of missing that here.",
                    "label": 0
                },
                {
                    "sent": "It seems like you know it's just some more biased towards cluster size distributions that computer in practice, or which are more like skate 3 type thing that you have some very big clusters.",
                    "label": 0
                },
                {
                    "sent": "You know there's all these people analyzing this instead.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the entropy isn't it the content, so I think it's going to make you know, because it basically all possible partitions are sort of.",
                    "label": 0
                },
                {
                    "sent": "All actual custom partitions are equally likely.",
                    "label": 0
                },
                {
                    "sent": "Most partitions occur with roughly 4 components, so it's going to be average.",
                    "label": 0
                },
                {
                    "sent": "It's going to be sort of much more likely you pick partitions which are roughly.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "It is so like generally.",
                    "label": 0
                },
                {
                    "sent": "Throw it to the according to the sample to the prior you want to get simple partitions.",
                    "label": 0
                },
                {
                    "sent": "Simple partitions are actually.",
                    "label": 0
                },
                {
                    "sent": "Other partitions which have few few clusters.",
                    "label": 0
                },
                {
                    "sent": "All partitions which are highly unbalanced.",
                    "label": 0
                },
                {
                    "sent": "So you don't want to choose balanced partitions because the subspace of balanced partitions is complex.",
                    "label": 0
                },
                {
                    "sent": "And the way you can think about it is that if you want to specify some unbalanced partition, you have to spend much more bits in order to specify it you have, you need.",
                    "label": 0
                },
                {
                    "sent": "So if you look at.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This picture you need more bits to specify a balanced partition because you have to tell for each of them which cluster it goes to, rather than to space.",
                    "label": 0
                },
                {
                    "sent": "Specify an unbalanced partitioning.",
                    "label": 0
                },
                {
                    "sent": "Unbalanced partitioner.",
                    "label": 0
                },
                {
                    "sent": "Just tell me who is the outlier.",
                    "label": 0
                },
                {
                    "sent": "So you get log off.",
                    "label": 0
                },
                {
                    "sent": "Here you have a log of four.",
                    "label": 0
                },
                {
                    "sent": "Bits to specify unbalanced partition.",
                    "label": 0
                },
                {
                    "sent": "You have log of 6 bits.",
                    "label": 0
                },
                {
                    "sent": "To specify a balanced partition.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Don't wanna have all the same size.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so unbalanced unbalanced partitions are simpler than balanced partitions and despair favors choosing unbalanced partitions OK. OK.",
                    "label": 0
                },
                {
                    "sent": "Very good question and now how I calculate this KL divergent within the posterior and prior.",
                    "label": 0
                },
                {
                    "sent": "So this is the prior and then actually about this product I can.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also so.",
                    "label": 0
                },
                {
                    "sent": "If you don't have some a priori.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Knowledge about.",
                    "label": 0
                },
                {
                    "sent": "Actually, which pairs of yours?",
                    "label": 0
                },
                {
                    "sent": "For example, should go together?",
                    "label": 0
                },
                {
                    "sent": "I can show you that you can't construct any prior that will give you significantly significantly better bound becausw this term of the entropy will will get into the bound if you are indifferent to permutations of the viewers, you will get this entropy term, which is the leading term it is bound.",
                    "label": 0
                },
                {
                    "sent": "So generally, if you don't have some specific information about, for example pairs of years that should come together, then you can design A better prior on this space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how we calculate the care divergent between the posterior on the prior?",
                    "label": 0
                },
                {
                    "sent": "So this is our prior and once again.",
                    "label": 0
                },
                {
                    "sent": "I reminded according to the posterior, we go over all the viewers and withdraw a cluster for each viewer.",
                    "label": 0
                },
                {
                    "sent": "Which means that.",
                    "label": 0
                },
                {
                    "sent": "The probability of a class that is actually this one over the number of years.",
                    "label": 0
                },
                {
                    "sent": "Some overall views of the probability of a sign that we are to the cluster.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they look at this scale.",
                    "label": 0
                },
                {
                    "sent": "Divergent says the expectation of logarithm of the prior minus the entropy of Q.",
                    "label": 0
                },
                {
                    "sent": "Now for the first term, if I go according to this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is um?",
                    "label": 0
                },
                {
                    "sent": "Strategy of drawing partition.",
                    "label": 0
                },
                {
                    "sent": "The entropy of the like.",
                    "label": 0
                },
                {
                    "sent": "But the class status is going to be concentrating around the entropy of this distribution.",
                    "label": 0
                },
                {
                    "sent": "If I go over all viewers and assign the cluster to ensure according to this conditional distribution, then the entropy of the resulting.",
                    "label": 0
                },
                {
                    "sent": "Distribution is going to be the entropy of this marginal.",
                    "label": 0
                },
                {
                    "sent": "OK. And I can show him slightly skipping here, but I can show that the entropy of there.",
                    "label": 0
                },
                {
                    "sent": "Q is actually the entropy of this conditional distribution.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then if I put this two together.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I get the kill the versions is actually bounded by the mutual information.",
                    "label": 0
                },
                {
                    "sent": "The clusters preserve on their original parameters, which are viewers or movies.",
                    "label": 0
                },
                {
                    "sent": "And some other things that are less important here.",
                    "label": 0
                },
                {
                    "sent": "And this mutual information is the mutual information according to this joint distribution, which is uniform in viewers and movies, and the conditional that we have chosen in our model.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K. Which actually gives us the generalization bound which tells that the KL divergent between the empirical and.",
                    "label": 1
                },
                {
                    "sent": "The expected loss of this prediction strategy is bounded by.",
                    "label": 0
                },
                {
                    "sent": "Some of the cardinality's of the variables times the mutual information that the cluster variables preserved from the original variables and some other.",
                    "label": 0
                },
                {
                    "sent": "So this is the logarithmic terms that we had.",
                    "label": 0
                },
                {
                    "sent": "This is the number of partitions cells.",
                    "label": 1
                },
                {
                    "sent": "And the usual part of Park Basin bonds.",
                    "label": 0
                },
                {
                    "sent": "That we have.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now first, let's see that this bound follows our intuitions about this problem.",
                    "label": 0
                },
                {
                    "sent": "So if we go take the extreme case as if we put all the data in one big cluster, then our empirical loss is going to be high.",
                    "label": 0
                },
                {
                    "sent": "But we know that this empirical loss is close to the true loss.",
                    "label": 0
                },
                {
                    "sent": "And the bond actually shows that this mutual information is zero in this case, and therefore the bound is closed like the empirical loss is close to the true one.",
                    "label": 0
                },
                {
                    "sent": "But it can be large if we go to the case where we put everybody in a separate cluster, then we can achieve empirical loss which is 0.",
                    "label": 0
                },
                {
                    "sent": "But the mutual information is large in this case.",
                    "label": 0
                },
                {
                    "sent": "And we get that this empirical loss of zero.",
                    "label": 0
                },
                {
                    "sent": "It's actually far away from the true loss.",
                    "label": 0
                },
                {
                    "sent": "So this month follows our intuition.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Further tells us that we should prefer unbalanced partitions over balanced partitions, and this trend is actually opposite to.",
                    "label": 0
                },
                {
                    "sent": "Quite a lot of clustering algorithms where they introduced this artificially introduced the bias to balance tradition.",
                    "label": 0
                },
                {
                    "sent": "So here we show that this bias is incorrect.",
                    "label": 0
                },
                {
                    "sent": "You should prefer unbalanced partitions in clustering rather than balanced partitions.",
                    "label": 0
                },
                {
                    "sent": "So we get this optimization tradeoff between the empirical loss and the mutual information that the clusters preserve from the original parameters.",
                    "label": 1
                },
                {
                    "sent": "And we have applied.",
                    "label": 0
                },
                {
                    "sent": "So here I just replaced.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With the thread of this parameter beta.",
                    "label": 0
                },
                {
                    "sent": "Chelsea goes to this cat on a farm of bound.",
                    "label": 0
                },
                {
                    "sent": "That's what I mentioned and then we applied it to the movielens data set which is.",
                    "label": 0
                },
                {
                    "sent": "100,000 ratings for both 1000 by 1500 movies.",
                    "label": 1
                },
                {
                    "sent": "And we achieved the mean absolute error of zero point 72 in predicting the ratings which are on five star scale.",
                    "label": 1
                },
                {
                    "sent": "So the maximum error is 4 here.",
                    "label": 0
                },
                {
                    "sent": "Not one.",
                    "label": 0
                },
                {
                    "sent": "And here are some.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graphs So what?",
                    "label": 0
                },
                {
                    "sent": "I show you this is.",
                    "label": 0
                },
                {
                    "sent": "The test loss as a function of betting.",
                    "label": 0
                },
                {
                    "sent": "This tradeoff and this is actually what France are called.",
                    "label": 0
                },
                {
                    "sent": "Cigarettes that bound I didn't like.",
                    "label": 0
                },
                {
                    "sent": "Probably I should have used the catoni bound here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And you see that.",
                    "label": 0
                },
                {
                    "sent": "And this is the same curve just zoomed in.",
                    "label": 0
                },
                {
                    "sent": "Here is 0.72 the optimal prediction and you see that if we use certain by 6 clusters, then actually the number of clusters provide some regularization.",
                    "label": 0
                },
                {
                    "sent": "So when beta grows, this doesn't really.",
                    "label": 0
                },
                {
                    "sent": "Influence the performance, but if.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Goal if we increase the number of clusters then we see that actually this thread of it controls regularization of this problem.",
                    "label": 0
                },
                {
                    "sent": "And the bound it still like once again maximum value is 4, so the the bound doesn't really follow the shape of the laws, but it's still meaningful and we can even go to the.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Theme of taken so in this almost 300 by 300 clusters, the size of the cluster space is the same as the number of samples.",
                    "label": 0
                },
                {
                    "sent": "So in the bad case we can just put every sample in separate cluster.",
                    "label": 0
                },
                {
                    "sent": "But we see that this thread of controls regularization of this problem and in this case it's the complete response of the responsibility of this thread of control regularization.",
                    "label": 0
                },
                {
                    "sent": "The number of clusters doesn't provide any regularization.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to the Park Basin bounds of discrete.",
                    "label": 0
                },
                {
                    "sent": "For discrete density estimation.",
                    "label": 0
                },
                {
                    "sent": "So we have a sample SpaceX and our supportive spaces space of functions from this sample space, which can be also infinite to some finite subspace as finite space Z.",
                    "label": 0
                },
                {
                    "sent": "And we say that.",
                    "label": 0
                },
                {
                    "sent": "The probability for each such function, the probability of.",
                    "label": 0
                },
                {
                    "sent": "There is the probability according to the distribution of X that verb serve.",
                    "label": 0
                },
                {
                    "sent": "This value Z. K.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And once again, we can define a prior over to his point in space.",
                    "label": 1
                },
                {
                    "sent": "We can define the posterior over things reported space, and we say that.",
                    "label": 1
                },
                {
                    "sent": "Even though exports according to the posterior with noted by PQ.",
                    "label": 0
                },
                {
                    "sent": "Of this, it's the expectation with respect to Q of the probability of observing them.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then forget that with probability greater than one minus Delta for all possible distributions over age, the KL divergent between the distribution.",
                    "label": 0
                },
                {
                    "sent": "The empirical distribution over the and the true distribution over the is bounded.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                },
                {
                    "sent": "This term.",
                    "label": 0
                },
                {
                    "sent": "So if we compare it.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can say so this is the bound and actually the bound for classification.",
                    "label": 0
                },
                {
                    "sent": "It's a special case of this bound for density estimation, because if we say that this Z is actually the error variable.",
                    "label": 1
                },
                {
                    "sent": "Then classification is density estimation on this error variable.",
                    "label": 0
                },
                {
                    "sent": "And the cardinality of these two.",
                    "label": 0
                },
                {
                    "sent": "So here we get one and we.",
                    "label": 0
                },
                {
                    "sent": "Forget the pack base and bound for classification.",
                    "label": 0
                },
                {
                    "sent": "But generally it means that we can obtain some more refined information, not just a single parameter like the error.",
                    "label": 0
                },
                {
                    "sent": "But we can.",
                    "label": 0
                },
                {
                    "sent": "Get the modifying the information about X by this projections to multiple sets.",
                    "label": 0
                },
                {
                    "sent": "K. Uh, the idea?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the proof is really simple, so we use this change of measure and quality are.",
                    "label": 1
                },
                {
                    "sent": "Is Matthias said the most important inequality and all this field?",
                    "label": 0
                },
                {
                    "sent": "So for some functions, expectation with respect to Q is bounded by the KL divergent plus the log logarithm of expectation.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vectibix and if we choose this fee as the Guild N times the KL divergent between the empirical and through distribution.",
                    "label": 0
                },
                {
                    "sent": "And the next slide I will show that this is bounded by N + 1 to the power of the cardinality of the minus one.",
                    "label": 0
                },
                {
                    "sent": "And just the standard method of types argument.",
                    "label": 0
                },
                {
                    "sent": "And then we can show that this guy is bounded by actually N + 1 to the power cardinality of the.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Minus one which gives the bound.",
                    "label": 0
                },
                {
                    "sent": "OK so the only thing we have to do is to bound.",
                    "label": 0
                },
                {
                    "sent": "This time, so how we do it?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, I did not buy an want to end this.",
                    "label": 0
                },
                {
                    "sent": "The number of times we observed each value of Z and then the expectation of this scale divergent between the empirical.",
                    "label": 0
                },
                {
                    "sent": "And the through distribution over Z.",
                    "label": 0
                },
                {
                    "sent": "It's some over this possible types that we can observe of the number of ways to.",
                    "label": 0
                },
                {
                    "sent": "To get this type times the probability of the type types times.",
                    "label": 0
                },
                {
                    "sent": "Actually, the values that we are bounding.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then, once again, this multinomial coefficient can be bounded by the end times the entropy.",
                    "label": 0
                },
                {
                    "sent": "Here I just put this to the exponent, then these two.",
                    "label": 0
                },
                {
                    "sent": "Are just minus the KL divergent's.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they get that this is actually sum over all possible types of 1, which is bounded by.",
                    "label": 0
                },
                {
                    "sent": "N + 1 to the cardinality of Z -- 5.",
                    "label": 0
                },
                {
                    "sent": "Actually, the same proof that we had for the classification case.",
                    "label": 0
                },
                {
                    "sent": "Just we look at more values of these and how we apply.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This result to density estimation with clustering, so we are given some empirical distribution over X1 and X2 and we want to estimate the true distribution.",
                    "label": 1
                },
                {
                    "sent": "Actually we want to find some estimator that will minimize the expected loss logarithmic loss.",
                    "label": 0
                },
                {
                    "sent": "For Q.",
                    "label": 0
                },
                {
                    "sent": "And the model that we are using is again this core clustering model so.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use once again clustering golf.",
                    "label": 0
                },
                {
                    "sent": "In this case words and documents, and we said that the probability of, let's say, a Word document is some over the clusters of the probability of.",
                    "label": 0
                },
                {
                    "sent": "Observing this pair of clusters times the probability of observing some some word in its cluster and the document and that cluster of documents.",
                    "label": 0
                },
                {
                    "sent": "And we can just rewrite it using the base rule.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                },
                {
                    "sent": "The second formula.",
                    "label": 0
                },
                {
                    "sent": "And now the point is, is that when we cluster this space of words by documents?",
                    "label": 0
                },
                {
                    "sent": "The empirical distribution over the cluster space is going to converge to the true distribution over the cluster space.",
                    "label": 0
                },
                {
                    "sent": "Much faster than the distribution over the whole space converges to the true distribution over the whole space because the cluster space is much smaller.",
                    "label": 0
                },
                {
                    "sent": "So what we can do?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can take this distribution over the cluster space plus some small constant like to take it away from zero.",
                    "label": 0
                },
                {
                    "sent": "And we can just use it.",
                    "label": 0
                },
                {
                    "sent": "As an estimator.",
                    "label": 0
                },
                {
                    "sent": "For this values.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "So this is our estimator.",
                    "label": 0
                },
                {
                    "sent": "And using that bound on the density estimation, we can get that.",
                    "label": 0
                },
                {
                    "sent": "This expectation that we wanted to bound is bounded by.",
                    "label": 0
                },
                {
                    "sent": "The mutual information between the cluster variables.",
                    "label": 0
                },
                {
                    "sent": "Plus something that once again depends on the information.",
                    "label": 0
                },
                {
                    "sent": "That the cluster viral variables preserve on the original parameters.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are some constant that I'm not showing.",
                    "label": 0
                },
                {
                    "sent": "So related work for example wasn't the information theoretic clustering, but information.",
                    "label": 0
                },
                {
                    "sent": "Theoretical classroom could just say to maximize this mutual information.",
                    "label": 0
                },
                {
                    "sent": "And the park, based on the project, really provides the regularization term.",
                    "label": 0
                },
                {
                    "sent": "To this problem.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And once again, it may be useful to look at the extreme cases so.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we put all the data in one big cluster, well, we will know actually the distribution over that that big cluster.",
                    "label": 0
                },
                {
                    "sent": "But this big cluster.",
                    "label": 0
                },
                {
                    "sent": "It provides us no additional information.",
                    "label": 0
                },
                {
                    "sent": "But the bond is actually good, so zero is actually trivial solution to this problem.",
                    "label": 0
                },
                {
                    "sent": "If we go to the other extreme and we put everybody in a separate cluster, then this mutual information is going to be as the mutual information between the variables.",
                    "label": 0
                },
                {
                    "sent": "But.",
                    "label": 0
                },
                {
                    "sent": "We will know that.",
                    "label": 0
                },
                {
                    "sent": "It's actually far away from the truth.",
                    "label": 0
                },
                {
                    "sent": "So we want to find some some clustering that will take actually the thread of will be smaller than zero, because zero is trivial solution, But the bound is actually not zero because I have some of the entropies of the variables in this K. So it's actually positive.",
                    "label": 0
                },
                {
                    "sent": "Old time.",
                    "label": 0
                },
                {
                    "sent": "Um, so now I will talk about some.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm right around, yeah, so about that towards about extension of this approaches to graphical models and some other problems so we can see this discriminative prediction and density estimation model that we have.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we can just recursively applied, bound and get graphical models in in shapes of the tree and then the Mount will tell that.",
                    "label": 0
                },
                {
                    "sent": "The generalization performance is governed by a tradeoff between the empirical performance and the amount of mutual information that is propagated up the tree.",
                    "label": 1
                },
                {
                    "sent": "And actually the future work will be how we optimize, like how we actually propagate this mutual information up the threshold.",
                    "label": 0
                },
                {
                    "sent": "We put more information at the lower variable.",
                    "label": 0
                },
                {
                    "sent": "So maybe to make.",
                    "label": 1
                },
                {
                    "sent": "A more tight bound to the higher level and applications and more general graphical models.",
                    "label": 0
                },
                {
                    "sent": "Another application of this framework can be to graph clustering.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the pairwise clustering.",
                    "label": 0
                },
                {
                    "sent": "So in graph clustering we can say that the weights on the edges are generated according to.",
                    "label": 1
                },
                {
                    "sent": "Some conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "And then if we cluster the nodes.",
                    "label": 0
                },
                {
                    "sent": "Into clusters, and we said that the wait actually only depends on the clusters where the node seat.",
                    "label": 0
                },
                {
                    "sent": "We get exactly the Co clustering model.",
                    "label": 0
                },
                {
                    "sent": "Just these two distributions are the same.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "That's actually.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then with my talk, so I presented you back based on analysis of unsupervised learning of Co clustering, three shape graphical models, graph clustering and pairwise clustering.",
                    "label": 1
                },
                {
                    "sent": "Pack base and bound for discrete density estimation.",
                    "label": 0
                },
                {
                    "sent": "The usage of combinatorial priors in this discrete optimization problems.",
                    "label": 0
                },
                {
                    "sent": "They have shown that they bring to mutual information validation.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Adams and some future directions that would be interesting, it's.",
                    "label": 1
                },
                {
                    "sent": "Extension to continuous clustering models like for example K means mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Extension to general graphical models.",
                    "label": 1
                },
                {
                    "sent": "Not only three shape graphical models.",
                    "label": 0
                },
                {
                    "sent": "And also application reinforcement learning with that for already mentioned couple of times in this workshop.",
                    "label": 0
                },
                {
                    "sent": "So thanks for your attention.",
                    "label": 0
                }
            ]
        }
    }
}