{
    "id": "g6ay7p7xkvz2igvbc6divvc767hg2nvp",
    "title": "Privacy Aware Learning",
    "info": {
        "author": [
            "John Duchi, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Optimization Methods->Stochastic Optimization",
            "Top->Computer Science->Information Retrieval",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/nips2012_duchi_learning/",
    "segmentation": [
        [
            "So thanks for giving me the chance to speak here to everyone.",
            "I'm really excited about this.",
            "Let me let me start this talk."
        ],
        [
            "With a little example that will hopefully motivate the rest of rest of the talk.",
            "So let's."
        ],
        [
            "Say that we want to construct a good image classifier, say of military personnel or."
        ],
        [
            "Things like this and you could imagine that at least in the United States that the CIA or something might have a lot of utility for this.",
            "So, for example, here's a picture of David Petraeus in a market in his military gear.",
            "So you might use this as an example."
        ],
        [
            "Here's another picture of David Petraeus thinking you know, he's in military gear.",
            "You want to be able to classify that this is a someone who belongs to the military, but I think we all know where this is going."
        ],
        [
            "Maybe there are some pictures that Petraeus might not."
        ],
        [
            "Want shared?"
        ],
        [
            "For example, this one.",
            "Right, So what I'm going to do in this talk is."
        ],
        [
            "You know, talk about OK. Well, what can we do if we actually want to do learning with privacy?",
            "You know I want a good image classifier.",
            "So what can I do instead?",
            "Well, through."
        ],
        [
            "This talk is sort of.",
            "I'm going to try to start to lay some foundations, start, develop a theory of learning from private data so."
        ],
        [
            "Instead of this.",
            "We're going to be able to."
        ],
        [
            "Be trying to learn from pictures like this."
        ],
        [
            "This or."
        ],
        [
            "Test this."
        ],
        [
            "Or this?"
        ],
        [
            "Or this?"
        ],
        [
            "So I'm going to try to get sort of the fundamental issues."
        ],
        [
            "Lying problems like this."
        ],
        [
            "OK, so a quick outline.",
            "It offered with another sort of more formal statement of the problem that I'm going to consider and give maybe a slightly more serious motivating example.",
            "I'm going to talk about, sort of.",
            "We set the sort of maintaining privacy up as a game between the providers of the data and the world.",
            "Sort of at large, and there's some sort of natural competition there that we're going to have to deal with in the sort of towards the end of the talk.",
            "Then I'll actually show very precise tradeoffs between statistical estimation insisted and statistical learning and how much privacy that we actually can maintain.",
            "And then I'll kind of give some conclusions and some open questions that we still have so."
        ],
        [
            "At A at a very high level, what I'd like to answer in this talk or what are the tradeoffs between maintaining privacy and statistical estimation?"
        ],
        [
            "So I'd like to get sort of fine grained tradeoffs between privacy and utility, and we're going to be able to define these shortly."
        ],
        [
            "OK, so so a little more formally, our setting is that sort of the standard statistical learning setting.",
            "We're going to have samples.",
            "I'm going to always label them with X, and we're going to send samples X one through XN.",
            "We have some parameter that we want to infer, and the parameter is going to be data, and then we're going to measure the performance of the parameter, at least for a particular example.",
            "With the loss, little L of Theta index.",
            "OK, so this should be fairly hopefully familiar from sort of standard machine learning type applications."
        ],
        [
            "As a as a as another example, we can imagine trying to predict breast cancer from a set of features.",
            "So in this case we have data coming in XY pairs, where X is a regressor in D dimensions, and then we have a label which is plus or minus one, and as examples you know you may be measuring sort of whether there's a clump in the breast tissue, how uniform the cells are in the breast tissue, whether they adhere to one another, and different genetic markers, and then the label will be plus one if the cells are cancerous and minus one otherwise, and you can easily imagine you know you have.",
            "A lot of utility for being able to decide whether cells are cancerous, but maybe you don't want to share that with the rest of the world.",
            "So in this case."
        ],
        [
            "The natural goal is to find some parameter vector Theta so that the inner product of Theta with our regressors has the correct sign and."
        ],
        [
            "A natural loss in this case is what we use for support vector machines.",
            "It's called the hinge loss, so this is going to measure how sort of correct or incorrect we are.",
            "So I just kind of try to keep this example in mind throughout the truck."
        ],
        [
            "So this sort of model that we use for maintaining privacy, we're going to assume that there's some method which gets access to all the data and it spits out some parameter Theta hat.",
            "This is, this is how we're going to be talking about doing doing learning.",
            "And how we're going to measure?"
        ],
        [
            "Utility basically is the risk and so the goal overarching goal is to minimize a risk measure R which is going to measure sort of the average performance of a given parameter Theta, and the risk is sort of just the expectation of your loss across the whole population.",
            "And we're going to do."
        ],
        [
            "Is trying to use the samples X one through XN?"
        ],
        [
            "And so the question that we're going to answer is OK, can we make?",
            "Can we find some Theta hat, some estimate of Theta so that the gap in the gap in the risk is small without learning anything about the data X1 through XN?",
            "So really, this whole talk is just a question of prepositions were."
        ],
        [
            "To learn from the data, but we don't want to actually learn anything about it.",
            "OK."
        ],
        [
            "So there's been certainly a lot of work on formalizing what it means to be private.",
            "In the literature.",
            "There's some people here, Kamalika Chaudhuri and collaborators Cynthia Dwork, Larry Wasserman, and Jew, and in this work they sort of focus on.",
            "OK, what happens if the method is given access to all the data so the statistician or the learner sees everything?",
            "And then you try to keep private what's actually released and we're going to use a slightly different model where I'm actually going to do what's."
        ],
        [
            "Local privacy and this is sort of.",
            "This is actually one of the more classical approaches to privacy, but it's still been studied more recently.",
            "Sort of it was proposed back in essentially 1965 in the statistics."
        ],
        [
            "But here we pushed the privacy barrier in between the data and the statistician.",
            "So this is this is useful when you don't even trust sort of the people collecting the data.",
            "Maybe you don't trust, you know you like having a good search engine, but maybe you don't want Google to know everything about you.",
            "Something like that."
        ],
        [
            "So in this case, what that looks like is before the data actually goes to the learner to the method is going to go through a channel and I'm going to call that Channel Q and what that Channel is going to do is going to sort of privatize your data in some way, and you're going to send out actually some vector Z, which is going to be the private version of whatever data we had to begin with."
        ],
        [
            "So it's a natural sort of what's a plausible way to get privacy, sort of.",
            "The most natural thing we would do well one?"
        ],
        [
            "Example might be to just add independent random noise to sort of any of the data we get.",
            "So in the classification example you know we'll just sort of noise if I all of our regressors, it turns out that this is highly suboptimal.",
            "Sort of when you're trying to do learning or estimation, the dimension dependence is going to blow up, and it's going to cause you to really suffer in ways that you don't need to."
        ],
        [
            "So let me let me sort of."
        ],
        [
            "Formalize the communication model that we're going to use.",
            "That is actually going to let us kind of give a little bit sharper statements about how we get privacy.",
            "So recall that we're trying to minimize this risk, which is our expected loss."
        ],
        [
            "And I'm actually going to focus on procedures where what's actually communicated is the gradient of the loss.",
            "Now this is this is sort of natural because we're minimize."
        ],
        [
            "This function and when we minimize things, it's useful to have gradients.",
            "From a sort of statistical perspective, I'm not going to get into this too deeply, but in an asymptotic sense, the gradient of a loss function actually looks like a sufficient statistic, so it actually encodes all of the information in the problem.",
            "All of the statistical information of the problem."
        ],
        [
            "In linear models such as the SVM problem we saw earlier, the gradient really is just the data, so all we're going to be doing is somehow privatizing the data and then we run by communicating things that look like gradients will be able to sort of leverage stochastic optimization techniques to actually be able to get sort of developed.",
            "What these tradeoffs in privacy and estimation R. So what we're really going to do is we're going to be studying channels Q so that the vector Z that we actually communicate.",
            "This private data are just unbiased estimators of our gradients of our loss functions."
        ],
        [
            "So the main there's sort of two main contributions that I'm going to.",
            "We're going to make in this paper, and the first one is we're going to sort of develop the optimal types of noise to use to guarantee privacy, and I'll give a slightly more formal version of what optimal noises in a second, and then the set."
        ],
        [
            "And contribution is using these types of sort of optimal privacy, guaranteeing noise.",
            "We're going to be able to give sharp upper and lower bounds on the convergence rates of essentially any any learning procedure as a function of the amount of privacy we get."
        ],
        [
            "So let's start with kind of the optimal noise for maintaining privacy.",
            "So we're going to formalize sort of what I will call optimal local privacy, and what this means is that basically the providers of the data are going to be playing sort of a game, and what the providers of the data are going to do is they're going to try to maximize the privacy of Q.",
            "That's what they're actually using to give themselves sort of to add noise and maintain privacy.",
            "Subject to this unbiased constraint.",
            "So there's this kind of natural tension between.",
            "What they're actually willing to release in maintaining privacy, and they're going to try to be as private as possible, as much as sort of the method will allow.",
            "OK."
        ],
        [
            "So how we how we actually formalize this is we're going to use mutual information as our prime?"
        ],
        [
            "Symmetric and there are a lot of other privacy metrics out there.",
            "We also consider some others in the full version of the paper, but for now, let's just use mutual."
        ],
        [
            "Formation and what we're going to do, actually is, say your private only if you're sort of private to kind of all possible distributions on the data.",
            "So what this means is that the data is generated according to some distribution.",
            "P&Q is private if and only if it keeps the mutual information bound between what's actually released and the initial points X small for all possible prior distributions on X.",
            "So this is."
        ],
        [
            "In the worst case mutual information measure.",
            "And."
        ],
        [
            "What we do actually is we provide a general solution to this saddle point problem, which is to minimize over all possible distributions.",
            "Q This worst case mutual information measure and I won't tell you exactly the precise statement is somewhat complicated, but.",
            "Essentially subject to certain constraints, we can completely characterize what the sort of best and most private distributions are."
        ],
        [
            "And justice as an example, because it's relatively simple.",
            "Let's imagine our data are vectors in minus one, one in a D dimensional space, and we're going to allow the vector this communicated to beat Lions.",
            "Sort of a slightly larger box save radius M. They."
        ],
        [
            "What we do is what you can see in this picture and the data starts in this sort of smaller box on the inside, and then we're going to randomly send the data to different corners of the larger box, and we do this in such a way that the expectation of the of the actual vectors this communicated is correct.",
            "And so basically we just."
        ],
        [
            "The Vector Z has independent coordinates and each of them is chosen from the corners of the box so that the expectations are correct.",
            "So we just fly out to the corners of the box and this is the optimal way to get privacy, at least for data with this kind of geometry."
        ],
        [
            "And just as you can hopefully see, sort of a picture of this, let's."
        ],
        [
            "Take a look at our friend General Petraeus.",
            "So if we're willing to communicate, sort of a bit for every bit in the original information in the original image, then it's going to look like this.",
            "But as we communicate less and less according to these sort of optimal."
        ],
        [
            "Privacy distributions, so now we're communicating.",
            "Sort of a tenth of the bits in the image, and it turns out that what we'll see shortly is that this leads to about a factor of two penalty in the convergence rate of any kind of estimator for."
        ],
        [
            "These problems."
        ],
        [
            "And then we can get worse."
        ],
        [
            "Worse and worse, and at some point hopefully you say, OK, I'm willing to share this amount of information at this kind."
        ],
        [
            "Penalty in convergence."
        ],
        [
            "You know, by the time we're only sharing sort of .0001 bits, it looks fairly private.",
            "You know?",
            "There's not really a lot going on here that you could use to say oh, that's General Petraeus."
        ],
        [
            "Alright, so those sort of privacy distributions talked about.",
            "Let me spend some time talking about what the actual trade offs we get are."
        ],
        [
            "So our goal throughout this whole thing was to understand the tradeoff between our mutual information bound, which was this sort of mini Max type of mutual information.",
            "This kind of worst case mutual information.",
            "And the number of samples N that we actually have access to.",
            "And recall that we're trying to minimize this risk, which is the expected loss of our parameters.",
            "And it turns out that.",
            "For a D dimensional problem, the if you actually want to maintain privacy and they were called istar are sort of mutual information level of privacy that the effective sample size of any estimation learning statistical procedure goes from North to N times the mutual information divided by D. So this essentially we can prove a lower bound that's going to hold for all possible statistical methods and we can give also upper bounds that are achieved by stochastic approximation.",
            "So if you just, I mean if you're only going to take on one thing from this entire talk, it's that equation.",
            "Which is which basically precisely characterizes how much penalty we're going to pay by maintaining privacy, and this is sort of intuitive.",
            "Hopefully because if you know if we think about mutual information, if you have D dimensions and you're only willing to communicate sort of 1 bit, then we might expect that we lose sort of a factor of 1D in our in our kind of learning procedures."
        ],
        [
            "And put slightly differently.",
            "If we look again sort of in terms of just the convergence rate."
        ],
        [
            "For our optimization procedures, sort of the standard convergence."
        ],
        [
            "Rate for statistical."
        ],
        [
            "Visitation procedures is basically one on square root of the number of samples, But when we actually enforce privacy and the providers of the data play this privacy game where they try to maintain as much privacy as possible."
        ],
        [
            "Then this penalty goes to this.",
            "And."
        ],
        [
            "Again, this lower bound holds for all possible statistical methods.",
            "Nothing can beat it, and we show that the upper bound can be achieved by simple stochastic optimization algorithms."
        ],
        [
            "Real quick, I'd just like to give one brief experiment to show, sort of, hopefully, kind of.",
            "Sent home this idea that we can actually trade off how much privacy we get with how much utility we get.",
            "So looking at their breast cancer breast cancer example again are regressors.",
            "X are sort of the markers for breast cancer, breast cancer and the labels.",
            "Why are the presence or absence of a tumor and what this plot this plot shows on the X axis, the number of bits were actually willing to communicate, so this is sort of a measure of privacy.",
            "And as we go right, we have less and less privacy and the Y axis measures the error rate of the final predictor.",
            "We learn after doing a pass through the entire data set.",
            "What you can see and the two different lines are different types of geometry.",
            "We sort of imposed on the problem, but essentially the take home message is that as we communicate less and less information, the performance gets worse and worse.",
            "But if we're willing to sort of release a little bit more information, we can get down to sort of 5% or lower error rates after looking at all the data once.",
            "So we actually really do get this kind of trade off, and we can then go ahead and pick the point on the curve that we feel is sort of we're willing to sacrifice this much privacy to have this much sort of predictability."
        ],
        [
            "Anyway, I just like to conclude now.",
            "What we've done in this in this paper is we've actually given sharp rates of convergence when the providers of the data play this privacy game that I described earlier and in the full version of this paper we have extensions to differential privacy as well."
        ],
        [
            "In a soon to be on the archive paper, hopefully by this weekend, maybe a little bit later depends how quickly I can work.",
            "While I'm here, we actually generalize all these results.",
            "We can actually remove the privacy game and extend every everything in this paper to basically applying to essentially any statistical estimator, not just risk minimization problems, but essentially anything in the same types of results actually hold.",
            "And now there's a.",
            "There's certainly some open questions that we have left in this."
        ],
        [
            "For example, is it possible to just release a perturbed version of the data X1 through XN rather than sort of knowing what problem we're going to solve in the beginning?",
            "Can we just release a private data set that will be useful in a lot of different cases?",
            "I don't know the answer to that and."
        ],
        [
            "There may be cases where maybe all we care about is protecting some function of our data.",
            "You know, I don't care that you know that I'm male, but I do care that you know when I last had a tetanus shot, say your last one to the doctor.",
            "So maybe we only care about certain functions of the data to make private, and these are certainly interesting open questions.",
            "I think there's a lot more."
        ],
        [
            "So thanks for your attention and I'm happy to take some questions.",
            "So I have a gut level question.",
            "What happened to conduct cryptography?",
            "So isn't it possible to just crypt if I the encode cryptophyte the data center to cross fully and then somehow still be able to predict something even though it's not decodable?",
            "Maybe I'm dreaming, I don't know, so I think you're asking about homomorphic encryption.",
            "Certainly I'm not an expert on homomorphic encryption.",
            "There is some small difficulty.",
            "Now you'd still be like to release the predictor, and usually the predictor is going to be released in an unencrypted way, and that can actually release a lot of information, and so here we're guaranteeing that no matter what is released, we still have privacy.",
            "But it is a really, really interesting area, and there are many people who are more expert than I am in it.",
            "Can you can you apply a similar approach to protect the privacy aspects of Genome Wide Association study genotypes?",
            "That's a good question.",
            "So the answer is.",
            "I can't completely answer that, but I think the answer is sort of.",
            "The issue is that in these sort of genome wide Association studies, we're looking at sort of very high dimensional things and we have very small amounts of information that give us the signal right?",
            "There's only one or two predictors that we're kind of looking for, and in this case it's not clear that these techniques will necessarily work, and I think there's some open issues there.",
            "Maybe I missed part of the show.",
            "The slides do assume that this thing you try to learn if independent of privacy or not.",
            "Or today.",
            "I. I oh, so you're saying, how do we measure the risk?",
            "The risk is measured in your result to assume that the thing you try to learn finally is independent of privacy.",
            "I think if I'm understanding your question, the answer is yes, we were only going to measure risk on the unprivate data.",
            "So.",
            "So we're measuring our risk as though there were no privacy and we're actually trying to get the true predictor out.",
            "Does that answer your question?",
            "Yeah, basically.",
            "And just say in some case what I measure, but I try to measure have some coupling.",
            "With privacy mean either male or female, those are saying much contributes to what disease you have.",
            "So in those cases you have any result covering.",
            "That's the thing we try to predict is still kept private.",
            "We don't have any results in that Case No, I think it's very interesting though.",
            "Have you considered the possibility of somehow?",
            "Coordinate dairy sending and receiving machines or devices or whatever such that you use your using a sequence of pseudorandom numbers and the seeds and the number you're dividing, but by and all the parameters are the same, but you are deviating the information using this, so random number generator and you are saying of course.",
            "Noise, apparently.",
            "But at the ending point, you can put it back together as it was by having the same parameters.",
            "I don't know.",
            "I explain myself.",
            "I might have to take that question offline.",
            "I'm not sure I understand it well enough to give you a good answer here, so maybe we can talk?",
            "Yeah sure, that'd be great.",
            "OK, yeah so.",
            "OK. Like there's one more question, but we should be under.",
            "OK, yeah, last question is sometimes you want to hide some properties of the distribution and corrupting individual instances would be best strategy for that.",
            "Do you have an answer to that like when you want to not reveal something about?",
            "Some expectation of something on the on the example, for example.",
            "So we want to learn everything about the population.",
            "We just care about protecting individual people.",
            "So the answer to your question is no, because we're only we want to release as much population information as possible, but it's a good question anyway, thanks a lot.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks for giving me the chance to speak here to everyone.",
                    "label": 0
                },
                {
                    "sent": "I'm really excited about this.",
                    "label": 0
                },
                {
                    "sent": "Let me let me start this talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With a little example that will hopefully motivate the rest of rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say that we want to construct a good image classifier, say of military personnel or.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things like this and you could imagine that at least in the United States that the CIA or something might have a lot of utility for this.",
                    "label": 0
                },
                {
                    "sent": "So, for example, here's a picture of David Petraeus in a market in his military gear.",
                    "label": 1
                },
                {
                    "sent": "So you might use this as an example.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another picture of David Petraeus thinking you know, he's in military gear.",
                    "label": 0
                },
                {
                    "sent": "You want to be able to classify that this is a someone who belongs to the military, but I think we all know where this is going.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe there are some pictures that Petraeus might not.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Want shared?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, this one.",
                    "label": 0
                },
                {
                    "sent": "Right, So what I'm going to do in this talk is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, talk about OK. Well, what can we do if we actually want to do learning with privacy?",
                    "label": 0
                },
                {
                    "sent": "You know I want a good image classifier.",
                    "label": 1
                },
                {
                    "sent": "So what can I do instead?",
                    "label": 0
                },
                {
                    "sent": "Well, through.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This talk is sort of.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to start to lay some foundations, start, develop a theory of learning from private data so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of this.",
                    "label": 0
                },
                {
                    "sent": "We're going to be able to.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be trying to learn from pictures like this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This or.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Test this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or this?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or this?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to try to get sort of the fundamental issues.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lying problems like this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so a quick outline.",
                    "label": 0
                },
                {
                    "sent": "It offered with another sort of more formal statement of the problem that I'm going to consider and give maybe a slightly more serious motivating example.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about, sort of.",
                    "label": 0
                },
                {
                    "sent": "We set the sort of maintaining privacy up as a game between the providers of the data and the world.",
                    "label": 0
                },
                {
                    "sent": "Sort of at large, and there's some sort of natural competition there that we're going to have to deal with in the sort of towards the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "Then I'll actually show very precise tradeoffs between statistical estimation insisted and statistical learning and how much privacy that we actually can maintain.",
                    "label": 0
                },
                {
                    "sent": "And then I'll kind of give some conclusions and some open questions that we still have so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At A at a very high level, what I'd like to answer in this talk or what are the tradeoffs between maintaining privacy and statistical estimation?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'd like to get sort of fine grained tradeoffs between privacy and utility, and we're going to be able to define these shortly.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so a little more formally, our setting is that sort of the standard statistical learning setting.",
                    "label": 0
                },
                {
                    "sent": "We're going to have samples.",
                    "label": 0
                },
                {
                    "sent": "I'm going to always label them with X, and we're going to send samples X one through XN.",
                    "label": 0
                },
                {
                    "sent": "We have some parameter that we want to infer, and the parameter is going to be data, and then we're going to measure the performance of the parameter, at least for a particular example.",
                    "label": 0
                },
                {
                    "sent": "With the loss, little L of Theta index.",
                    "label": 0
                },
                {
                    "sent": "OK, so this should be fairly hopefully familiar from sort of standard machine learning type applications.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a as a as another example, we can imagine trying to predict breast cancer from a set of features.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have data coming in XY pairs, where X is a regressor in D dimensions, and then we have a label which is plus or minus one, and as examples you know you may be measuring sort of whether there's a clump in the breast tissue, how uniform the cells are in the breast tissue, whether they adhere to one another, and different genetic markers, and then the label will be plus one if the cells are cancerous and minus one otherwise, and you can easily imagine you know you have.",
                    "label": 0
                },
                {
                    "sent": "A lot of utility for being able to decide whether cells are cancerous, but maybe you don't want to share that with the rest of the world.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The natural goal is to find some parameter vector Theta so that the inner product of Theta with our regressors has the correct sign and.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A natural loss in this case is what we use for support vector machines.",
                    "label": 0
                },
                {
                    "sent": "It's called the hinge loss, so this is going to measure how sort of correct or incorrect we are.",
                    "label": 0
                },
                {
                    "sent": "So I just kind of try to keep this example in mind throughout the truck.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this sort of model that we use for maintaining privacy, we're going to assume that there's some method which gets access to all the data and it spits out some parameter Theta hat.",
                    "label": 0
                },
                {
                    "sent": "This is, this is how we're going to be talking about doing doing learning.",
                    "label": 0
                },
                {
                    "sent": "And how we're going to measure?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Utility basically is the risk and so the goal overarching goal is to minimize a risk measure R which is going to measure sort of the average performance of a given parameter Theta, and the risk is sort of just the expectation of your loss across the whole population.",
                    "label": 0
                },
                {
                    "sent": "And we're going to do.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is trying to use the samples X one through XN?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the question that we're going to answer is OK, can we make?",
                    "label": 0
                },
                {
                    "sent": "Can we find some Theta hat, some estimate of Theta so that the gap in the gap in the risk is small without learning anything about the data X1 through XN?",
                    "label": 0
                },
                {
                    "sent": "So really, this whole talk is just a question of prepositions were.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To learn from the data, but we don't want to actually learn anything about it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's been certainly a lot of work on formalizing what it means to be private.",
                    "label": 0
                },
                {
                    "sent": "In the literature.",
                    "label": 0
                },
                {
                    "sent": "There's some people here, Kamalika Chaudhuri and collaborators Cynthia Dwork, Larry Wasserman, and Jew, and in this work they sort of focus on.",
                    "label": 0
                },
                {
                    "sent": "OK, what happens if the method is given access to all the data so the statistician or the learner sees everything?",
                    "label": 0
                },
                {
                    "sent": "And then you try to keep private what's actually released and we're going to use a slightly different model where I'm actually going to do what's.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Local privacy and this is sort of.",
                    "label": 0
                },
                {
                    "sent": "This is actually one of the more classical approaches to privacy, but it's still been studied more recently.",
                    "label": 0
                },
                {
                    "sent": "Sort of it was proposed back in essentially 1965 in the statistics.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But here we pushed the privacy barrier in between the data and the statistician.",
                    "label": 0
                },
                {
                    "sent": "So this is this is useful when you don't even trust sort of the people collecting the data.",
                    "label": 0
                },
                {
                    "sent": "Maybe you don't trust, you know you like having a good search engine, but maybe you don't want Google to know everything about you.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case, what that looks like is before the data actually goes to the learner to the method is going to go through a channel and I'm going to call that Channel Q and what that Channel is going to do is going to sort of privatize your data in some way, and you're going to send out actually some vector Z, which is going to be the private version of whatever data we had to begin with.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's a natural sort of what's a plausible way to get privacy, sort of.",
                    "label": 0
                },
                {
                    "sent": "The most natural thing we would do well one?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example might be to just add independent random noise to sort of any of the data we get.",
                    "label": 0
                },
                {
                    "sent": "So in the classification example you know we'll just sort of noise if I all of our regressors, it turns out that this is highly suboptimal.",
                    "label": 0
                },
                {
                    "sent": "Sort of when you're trying to do learning or estimation, the dimension dependence is going to blow up, and it's going to cause you to really suffer in ways that you don't need to.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me let me sort of.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formalize the communication model that we're going to use.",
                    "label": 0
                },
                {
                    "sent": "That is actually going to let us kind of give a little bit sharper statements about how we get privacy.",
                    "label": 1
                },
                {
                    "sent": "So recall that we're trying to minimize this risk, which is our expected loss.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm actually going to focus on procedures where what's actually communicated is the gradient of the loss.",
                    "label": 0
                },
                {
                    "sent": "Now this is this is sort of natural because we're minimize.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This function and when we minimize things, it's useful to have gradients.",
                    "label": 0
                },
                {
                    "sent": "From a sort of statistical perspective, I'm not going to get into this too deeply, but in an asymptotic sense, the gradient of a loss function actually looks like a sufficient statistic, so it actually encodes all of the information in the problem.",
                    "label": 0
                },
                {
                    "sent": "All of the statistical information of the problem.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In linear models such as the SVM problem we saw earlier, the gradient really is just the data, so all we're going to be doing is somehow privatizing the data and then we run by communicating things that look like gradients will be able to sort of leverage stochastic optimization techniques to actually be able to get sort of developed.",
                    "label": 0
                },
                {
                    "sent": "What these tradeoffs in privacy and estimation R. So what we're really going to do is we're going to be studying channels Q so that the vector Z that we actually communicate.",
                    "label": 0
                },
                {
                    "sent": "This private data are just unbiased estimators of our gradients of our loss functions.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the main there's sort of two main contributions that I'm going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to make in this paper, and the first one is we're going to sort of develop the optimal types of noise to use to guarantee privacy, and I'll give a slightly more formal version of what optimal noises in a second, and then the set.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And contribution is using these types of sort of optimal privacy, guaranteeing noise.",
                    "label": 0
                },
                {
                    "sent": "We're going to be able to give sharp upper and lower bounds on the convergence rates of essentially any any learning procedure as a function of the amount of privacy we get.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with kind of the optimal noise for maintaining privacy.",
                    "label": 0
                },
                {
                    "sent": "So we're going to formalize sort of what I will call optimal local privacy, and what this means is that basically the providers of the data are going to be playing sort of a game, and what the providers of the data are going to do is they're going to try to maximize the privacy of Q.",
                    "label": 0
                },
                {
                    "sent": "That's what they're actually using to give themselves sort of to add noise and maintain privacy.",
                    "label": 0
                },
                {
                    "sent": "Subject to this unbiased constraint.",
                    "label": 0
                },
                {
                    "sent": "So there's this kind of natural tension between.",
                    "label": 0
                },
                {
                    "sent": "What they're actually willing to release in maintaining privacy, and they're going to try to be as private as possible, as much as sort of the method will allow.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how we how we actually formalize this is we're going to use mutual information as our prime?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Symmetric and there are a lot of other privacy metrics out there.",
                    "label": 0
                },
                {
                    "sent": "We also consider some others in the full version of the paper, but for now, let's just use mutual.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formation and what we're going to do, actually is, say your private only if you're sort of private to kind of all possible distributions on the data.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that the data is generated according to some distribution.",
                    "label": 0
                },
                {
                    "sent": "P&Q is private if and only if it keeps the mutual information bound between what's actually released and the initial points X small for all possible prior distributions on X.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the worst case mutual information measure.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we do actually is we provide a general solution to this saddle point problem, which is to minimize over all possible distributions.",
                    "label": 0
                },
                {
                    "sent": "Q This worst case mutual information measure and I won't tell you exactly the precise statement is somewhat complicated, but.",
                    "label": 1
                },
                {
                    "sent": "Essentially subject to certain constraints, we can completely characterize what the sort of best and most private distributions are.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And justice as an example, because it's relatively simple.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine our data are vectors in minus one, one in a D dimensional space, and we're going to allow the vector this communicated to beat Lions.",
                    "label": 0
                },
                {
                    "sent": "Sort of a slightly larger box save radius M. They.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do is what you can see in this picture and the data starts in this sort of smaller box on the inside, and then we're going to randomly send the data to different corners of the larger box, and we do this in such a way that the expectation of the of the actual vectors this communicated is correct.",
                    "label": 0
                },
                {
                    "sent": "And so basically we just.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Vector Z has independent coordinates and each of them is chosen from the corners of the box so that the expectations are correct.",
                    "label": 0
                },
                {
                    "sent": "So we just fly out to the corners of the box and this is the optimal way to get privacy, at least for data with this kind of geometry.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just as you can hopefully see, sort of a picture of this, let's.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take a look at our friend General Petraeus.",
                    "label": 0
                },
                {
                    "sent": "So if we're willing to communicate, sort of a bit for every bit in the original information in the original image, then it's going to look like this.",
                    "label": 0
                },
                {
                    "sent": "But as we communicate less and less according to these sort of optimal.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Privacy distributions, so now we're communicating.",
                    "label": 0
                },
                {
                    "sent": "Sort of a tenth of the bits in the image, and it turns out that what we'll see shortly is that this leads to about a factor of two penalty in the convergence rate of any kind of estimator for.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These problems.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can get worse.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Worse and worse, and at some point hopefully you say, OK, I'm willing to share this amount of information at this kind.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Penalty in convergence.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, by the time we're only sharing sort of .0001 bits, it looks fairly private.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "There's not really a lot going on here that you could use to say oh, that's General Petraeus.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so those sort of privacy distributions talked about.",
                    "label": 0
                },
                {
                    "sent": "Let me spend some time talking about what the actual trade offs we get are.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our goal throughout this whole thing was to understand the tradeoff between our mutual information bound, which was this sort of mini Max type of mutual information.",
                    "label": 0
                },
                {
                    "sent": "This kind of worst case mutual information.",
                    "label": 0
                },
                {
                    "sent": "And the number of samples N that we actually have access to.",
                    "label": 0
                },
                {
                    "sent": "And recall that we're trying to minimize this risk, which is the expected loss of our parameters.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that.",
                    "label": 0
                },
                {
                    "sent": "For a D dimensional problem, the if you actually want to maintain privacy and they were called istar are sort of mutual information level of privacy that the effective sample size of any estimation learning statistical procedure goes from North to N times the mutual information divided by D. So this essentially we can prove a lower bound that's going to hold for all possible statistical methods and we can give also upper bounds that are achieved by stochastic approximation.",
                    "label": 0
                },
                {
                    "sent": "So if you just, I mean if you're only going to take on one thing from this entire talk, it's that equation.",
                    "label": 0
                },
                {
                    "sent": "Which is which basically precisely characterizes how much penalty we're going to pay by maintaining privacy, and this is sort of intuitive.",
                    "label": 0
                },
                {
                    "sent": "Hopefully because if you know if we think about mutual information, if you have D dimensions and you're only willing to communicate sort of 1 bit, then we might expect that we lose sort of a factor of 1D in our in our kind of learning procedures.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And put slightly differently.",
                    "label": 0
                },
                {
                    "sent": "If we look again sort of in terms of just the convergence rate.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For our optimization procedures, sort of the standard convergence.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rate for statistical.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visitation procedures is basically one on square root of the number of samples, But when we actually enforce privacy and the providers of the data play this privacy game where they try to maintain as much privacy as possible.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then this penalty goes to this.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, this lower bound holds for all possible statistical methods.",
                    "label": 0
                },
                {
                    "sent": "Nothing can beat it, and we show that the upper bound can be achieved by simple stochastic optimization algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Real quick, I'd just like to give one brief experiment to show, sort of, hopefully, kind of.",
                    "label": 0
                },
                {
                    "sent": "Sent home this idea that we can actually trade off how much privacy we get with how much utility we get.",
                    "label": 0
                },
                {
                    "sent": "So looking at their breast cancer breast cancer example again are regressors.",
                    "label": 0
                },
                {
                    "sent": "X are sort of the markers for breast cancer, breast cancer and the labels.",
                    "label": 0
                },
                {
                    "sent": "Why are the presence or absence of a tumor and what this plot this plot shows on the X axis, the number of bits were actually willing to communicate, so this is sort of a measure of privacy.",
                    "label": 0
                },
                {
                    "sent": "And as we go right, we have less and less privacy and the Y axis measures the error rate of the final predictor.",
                    "label": 0
                },
                {
                    "sent": "We learn after doing a pass through the entire data set.",
                    "label": 0
                },
                {
                    "sent": "What you can see and the two different lines are different types of geometry.",
                    "label": 0
                },
                {
                    "sent": "We sort of imposed on the problem, but essentially the take home message is that as we communicate less and less information, the performance gets worse and worse.",
                    "label": 0
                },
                {
                    "sent": "But if we're willing to sort of release a little bit more information, we can get down to sort of 5% or lower error rates after looking at all the data once.",
                    "label": 0
                },
                {
                    "sent": "So we actually really do get this kind of trade off, and we can then go ahead and pick the point on the curve that we feel is sort of we're willing to sacrifice this much privacy to have this much sort of predictability.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, I just like to conclude now.",
                    "label": 0
                },
                {
                    "sent": "What we've done in this in this paper is we've actually given sharp rates of convergence when the providers of the data play this privacy game that I described earlier and in the full version of this paper we have extensions to differential privacy as well.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a soon to be on the archive paper, hopefully by this weekend, maybe a little bit later depends how quickly I can work.",
                    "label": 0
                },
                {
                    "sent": "While I'm here, we actually generalize all these results.",
                    "label": 0
                },
                {
                    "sent": "We can actually remove the privacy game and extend every everything in this paper to basically applying to essentially any statistical estimator, not just risk minimization problems, but essentially anything in the same types of results actually hold.",
                    "label": 0
                },
                {
                    "sent": "And now there's a.",
                    "label": 0
                },
                {
                    "sent": "There's certainly some open questions that we have left in this.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, is it possible to just release a perturbed version of the data X1 through XN rather than sort of knowing what problem we're going to solve in the beginning?",
                    "label": 0
                },
                {
                    "sent": "Can we just release a private data set that will be useful in a lot of different cases?",
                    "label": 0
                },
                {
                    "sent": "I don't know the answer to that and.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There may be cases where maybe all we care about is protecting some function of our data.",
                    "label": 0
                },
                {
                    "sent": "You know, I don't care that you know that I'm male, but I do care that you know when I last had a tetanus shot, say your last one to the doctor.",
                    "label": 0
                },
                {
                    "sent": "So maybe we only care about certain functions of the data to make private, and these are certainly interesting open questions.",
                    "label": 0
                },
                {
                    "sent": "I think there's a lot more.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thanks for your attention and I'm happy to take some questions.",
                    "label": 0
                },
                {
                    "sent": "So I have a gut level question.",
                    "label": 0
                },
                {
                    "sent": "What happened to conduct cryptography?",
                    "label": 0
                },
                {
                    "sent": "So isn't it possible to just crypt if I the encode cryptophyte the data center to cross fully and then somehow still be able to predict something even though it's not decodable?",
                    "label": 1
                },
                {
                    "sent": "Maybe I'm dreaming, I don't know, so I think you're asking about homomorphic encryption.",
                    "label": 0
                },
                {
                    "sent": "Certainly I'm not an expert on homomorphic encryption.",
                    "label": 0
                },
                {
                    "sent": "There is some small difficulty.",
                    "label": 1
                },
                {
                    "sent": "Now you'd still be like to release the predictor, and usually the predictor is going to be released in an unencrypted way, and that can actually release a lot of information, and so here we're guaranteeing that no matter what is released, we still have privacy.",
                    "label": 0
                },
                {
                    "sent": "But it is a really, really interesting area, and there are many people who are more expert than I am in it.",
                    "label": 0
                },
                {
                    "sent": "Can you can you apply a similar approach to protect the privacy aspects of Genome Wide Association study genotypes?",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "So the answer is.",
                    "label": 0
                },
                {
                    "sent": "I can't completely answer that, but I think the answer is sort of.",
                    "label": 0
                },
                {
                    "sent": "The issue is that in these sort of genome wide Association studies, we're looking at sort of very high dimensional things and we have very small amounts of information that give us the signal right?",
                    "label": 1
                },
                {
                    "sent": "There's only one or two predictors that we're kind of looking for, and in this case it's not clear that these techniques will necessarily work, and I think there's some open issues there.",
                    "label": 0
                },
                {
                    "sent": "Maybe I missed part of the show.",
                    "label": 0
                },
                {
                    "sent": "The slides do assume that this thing you try to learn if independent of privacy or not.",
                    "label": 0
                },
                {
                    "sent": "Or today.",
                    "label": 0
                },
                {
                    "sent": "I. I oh, so you're saying, how do we measure the risk?",
                    "label": 0
                },
                {
                    "sent": "The risk is measured in your result to assume that the thing you try to learn finally is independent of privacy.",
                    "label": 0
                },
                {
                    "sent": "I think if I'm understanding your question, the answer is yes, we were only going to measure risk on the unprivate data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we're measuring our risk as though there were no privacy and we're actually trying to get the true predictor out.",
                    "label": 0
                },
                {
                    "sent": "Does that answer your question?",
                    "label": 0
                },
                {
                    "sent": "Yeah, basically.",
                    "label": 0
                },
                {
                    "sent": "And just say in some case what I measure, but I try to measure have some coupling.",
                    "label": 0
                },
                {
                    "sent": "With privacy mean either male or female, those are saying much contributes to what disease you have.",
                    "label": 0
                },
                {
                    "sent": "So in those cases you have any result covering.",
                    "label": 0
                },
                {
                    "sent": "That's the thing we try to predict is still kept private.",
                    "label": 0
                },
                {
                    "sent": "We don't have any results in that Case No, I think it's very interesting though.",
                    "label": 0
                },
                {
                    "sent": "Have you considered the possibility of somehow?",
                    "label": 0
                },
                {
                    "sent": "Coordinate dairy sending and receiving machines or devices or whatever such that you use your using a sequence of pseudorandom numbers and the seeds and the number you're dividing, but by and all the parameters are the same, but you are deviating the information using this, so random number generator and you are saying of course.",
                    "label": 0
                },
                {
                    "sent": "Noise, apparently.",
                    "label": 0
                },
                {
                    "sent": "But at the ending point, you can put it back together as it was by having the same parameters.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I explain myself.",
                    "label": 0
                },
                {
                    "sent": "I might have to take that question offline.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I understand it well enough to give you a good answer here, so maybe we can talk?",
                    "label": 0
                },
                {
                    "sent": "Yeah sure, that'd be great.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah so.",
                    "label": 0
                },
                {
                    "sent": "OK. Like there's one more question, but we should be under.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, last question is sometimes you want to hide some properties of the distribution and corrupting individual instances would be best strategy for that.",
                    "label": 0
                },
                {
                    "sent": "Do you have an answer to that like when you want to not reveal something about?",
                    "label": 1
                },
                {
                    "sent": "Some expectation of something on the on the example, for example.",
                    "label": 0
                },
                {
                    "sent": "So we want to learn everything about the population.",
                    "label": 0
                },
                {
                    "sent": "We just care about protecting individual people.",
                    "label": 0
                },
                {
                    "sent": "So the answer to your question is no, because we're only we want to release as much population information as possible, but it's a good question anyway, thanks a lot.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}