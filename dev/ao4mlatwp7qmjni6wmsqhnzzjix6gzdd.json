{
    "id": "ao4mlatwp7qmjni6wmsqhnzzjix6gzdd",
    "title": "A Bayesian Approach to Occupancy Mapping with Uncertain Inputs",
    "info": {
        "author": [
            "Simon T. O'Callaghan, University of Sydney"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_ocallaghan_baom/",
    "segmentation": [
        [
            "OK, so generating spatially accurate representations overall."
        ],
        [
            "What environment is a critical task in autonomous robotics?",
            "So you would have seen there are a lot of examples.",
            "These representations can take on forms like, say, a semantic representation like you might you might have rolled here like you were saying the previous talk and then further down.",
            "You want to set of trees and some grass insight.",
            "Or you might also be interested in generating a map that represents the area in terms of areas that are occupied and unoccupied.",
            "So even ideas where you can travel for example.",
            "But in any case decent representation is essential for activities like localization.",
            "Path planning, cooperation and interactions with the environment.",
            "So obviously it's been the focus of huge amounts of research over the last 20 years in the robotics community, but it still is a largely unsolved problem.",
            "So what we're doing is we're proposing a statistical modeling technique for building occupancy Maps using contextual information from potentially noisy sources, multiple sources, and integrating them all into one common probabilistic map of the environment."
        ],
        [
            "And that will hopefully help address some of those issues.",
            "So there are dozens and dozens of different mapping techniques, and I can't talk about all them obviously, so I'll just pick on arguably the most popular one which is the occupancy grid, and what it does is it divides the world up into these discrete independent cells, and each cell you have this hypothesis of occupancy, and each time then your laser passes over that cell.",
            "You're updating your hypothesis occupancy based on Bayes rule using Facebook update, so it's making this great simplification here of the environment where you're seeing each cell is independent.",
            "And then because of this then you're getting a really fast computational time, which is probably why they're so popular.",
            "But it does have a number of drawbacks.",
            "So first of all, what you're doing is you're taking this really high dimensional problem of mapping and you're decomposing it into lots of single dimensional binary classification tasks.",
            "So you might look at a map, and because each cell is independent.",
            "It might be obvious to a person by taking the readings around it into context.",
            "What a cell should be, but.",
            "The map here you can see these areas are really high uncertainty here.",
            "Just because the laser didn't pass over those exact cells and also then you're stuck to a fixed resolution in the classical occupancy grid and you're getting discretization errors as well because you're dividing the world up into these grids."
        ],
        [
            "So there are a lot of techniques out there that that address some of these issues, so ideally what you want is you want a technique that exploits structural dependencies in the environment, so the world isn't just this random distribution of occupied and unoccupied cells, their structure in and in our environment, and a good mapping technique should exploit the structure you want be capable of handling large data sets and have an adjustable resolution so that you can kind of see the application.",
            "You might want a large, coarse resolution scan of a really large area.",
            "Or maybe smaller higher resolution over specific area of interest.",
            "And also if you're going to be making inference into areas that the laser hasn't scanned over, you want to have kind of an associated variance.",
            "Even ideas to how confident you can be with that estimate.",
            "And it was these then that kind of lead us to think that maybe we could use the Gaussian process as a tool for solving these.",
            "So some of the characteristics are that it learns dependencies between the training points.",
            "It also generates a likelihood function and an associated variance function, and this variance function increases as you travel further away from your data, and that's a desirable characteristic.",
            "And also you have a continuous underlying function, so no scale becomes arbitrary.",
            "So."
        ],
        [
            "This is just some work that we presented at icro this year, this section where we kind of decided to address the problem as a boy trying to classify the world into areas that we think are occupied in areas that we think are unoccupied.",
            "So we had this robot driving around and we're getting this raw data in the form of Rangers and bearings, and the vehicle poses then convert this into Cartesian planes and represent the data as occupied points and unoccupied line segments.",
            "And then we store list inside a KD tree.",
            "And then we use the Gaussian process to map this information into a hypothesis of occupancy using the sensor data to help come up with this probability of occupancy for each test point, we had to use a neural network covariance function.",
            "We found data is very nonstationary 'cause you're getting these sudden changes from areas that are unoccupied to these occupied points.",
            "So this neural covariance function.",
            "Here you can just compare it with the squared exponential.",
            "It's very good at handling these step changes.",
            "So we can see this is the squared exponential versus the neural network.",
            "Here is to write by Chris Williams will be talking at this later on actually, so that's just another step comparison.",
            "A noisy step we stored in a KD tree, so we have to make this kind of localization approximation because of this inversion.",
            "Here it takes too long.",
            "People try to model the entire every training point at once, and for our application it makes sense to make this kind of local approximation anyway.",
            "That's just the sensor model who won't talk too much and then we get this output.",
            "It predicted, meaning predicted variance and goes into this probabilistic least squares classifier.",
            "And then you're making this.",
            "We've trained this and then it gives you an output saying, OK, I think the probability of this area being occupied is whatever some number between one and 0."
        ],
        [
            "So just looking at some simulated results for this first section we just made this kind of street map where you have some buildings here, some side streets.",
            "If you parked cars and we drove robot just around here, you can see this is the data that we got back so we can see the robots poses are in red and the laser returns are in blue.",
            "Sophie."
        ],
        [
            "Apply this data to the occupancy grid.",
            "We can see a number of drawbacks at the occupancy grid has you can see that large portions of this map are still an altered from the prior because the laser hasn't passed over those exact things, so you're not doing any inference in occupancy map, even though might be obvious to a person you know, you can make some decent estimates as to where the road is here.",
            "The occupancy map doesn't tell you this information.",
            "A common trick is to with occupancy grids.",
            "It's going to add some surplus noise, but all your doing is smearing the data there really and there still are.",
            "Even with this, it's not true inference first of all, but also there still are a lot of areas of uncertainty here, like behind the cars and down those roadways there that you only have a few scans so."
        ],
        [
            "Taking a Bayesian approach.",
            "And using the Gaussian process, this is the result we get here.",
            "So Red is areas that are occupied and going down to blue areas you're certain or unoccupied, so it bears much closer resemblance to the ground truth.",
            "You can make out the side streets and cars is 1 much more clearly, and also you can see that as you go further away from your data, that the probability falls back down to nought .5 which is going to want.",
            "You want an I said that scale becomes arbitrary as well, 'cause it's a continuous function that you can just quickly re sample if you want a very quick QR scanner of the areas."
        ],
        [
            "And that's just the classified classifying it there on the left, and that's your variance map there.",
            "So the variance is large inside the buildings, obviously."
        ],
        [
            "That's just a rocker analysis compared."
        ],
        [
            "So what we ignored in network and up until now was an important focus of this is how do we incorporate information from multiple sources into the same map?",
            "If you're if you're different sources have different levels of noise.",
            "So in robotics here also and have some noise associated with your sensor, so you're going to range and bearing noise, and you're also going to have noise on your robots location, so you might have some kind of encoders on the wheels, and in this case if you're only using that.",
            "This uncertainty is going to grow exponentially, or else even if you have GPS or using something like SLAM there which was described in the last talk, you're still going to have always going to have uncertainty in your position.",
            "So how we're modeling.",
            "This is where using an unscented transform.",
            "So I have a slide in this and I can kind of the presentation, but if people ask me with this, I can explain it later, so we use an unscented transform to model all these noises as multivariate Gaussians, and then we incorporate that into the training inputs of the upper map.",
            "So the problem with this is."
        ],
        [
            "With the Gaussian processes that we now have noise on, the training inputs not in the outputs for on the X domain.",
            "So if we look at how a GPS deals with this, this is just some ground truth function that we've taken some observations of, and we can put a GP to this nicely with using squared exponential covariance function.",
            "But if one of these points is noisy in the X domain, so if we if this point we actually recorded as being over here Now we're asking GP2 trying to fit into this data and you can see that it has a Mass Effect on the output of the GP here.",
            "So it assumes that this data set is no very noisy, because as far as it's concerned, this point lies somewhere in this column here, which isn't the case.",
            "So what we'd like to do if you look at the comparison of the covariance matrix with the training points in the test points, you can see here that this training point here is incorrectly influencing the wrong set of training of test points, so we don't like to do with disperse its influence over the area.",
            "That's kind of proportion to how noisy that test point is."
        ],
        [
            "So if we redefine our, our training points is observations that are corrupted by some noise, we can say that this noise is Gaussian because we're using the unscented transform, we kind of follow this logic.",
            "We define the kernel function over distributions now instead of just deterministic points.",
            "So we come up with this double integral, which unfortunately is on intractable for intractable for a lot of cases.",
            "Now you can make some big assumptions and say that this noise is independent, which isn't the case for us.",
            "But if we do this for the time being.",
            "So far only concerted sensor noise.",
            "Say we can say that that's independent.",
            "However, for us all our noise is dependent because it's all linked through the robots location.",
            "So then we can.",
            "We can write it like this.",
            "We can separate over this this joint here.",
            "And if we assume as well that are covariance matrix is Gaussian, so something like a squared exponential, then we can come up with a closed form solution and it looks like this.",
            "So I can see here that the noise is reducing the Sigma F here and it's also increasing the length scale.",
            "So it's kind of having that effective dispersing the influence of that of that point, which is what we want.",
            "So if we take this new."
        ],
        [
            "Covariance function and this time account for this noise on the training input we can see we get a much better fit, so it's no longer just almost flat thing that these points are correctly influencing the the mean as we'd expect.",
            "And if we look at that same graph again, we can see that this noisy point its influence has been dispersed like what we want."
        ],
        [
            "I.",
            "So I mentioned that in our case we don't have enough to separate this joint here, and also that we have a neural network covariance function, so we can't make this assumption that it's Gaussian.",
            "So what we need to do is we use the Ghost Herman Quadrature and when it says that if we can represent her equation like this.",
            "Then we can come up with.",
            "A decent approximation of it in terms of just sampling from some function.",
            "So because this is a multivariate Gaussian, we can represent that as an exponential with just a kind of a.",
            "We can solve this and we come up with this covariance function here, and this is a place any kind of a covariance function, so we can use your neural network here.",
            "Now in this case, and just comparing the closed form with with the quadrature here, this is a level one approximation, so you can see it as well, but for increased input in the noise it kind of starts to diverge.",
            "And you can use a Level 2 Level 3 or whatever.",
            "This is the level 2.",
            "You can see that improves, but obviously you need to take more samples so your algorithm slows down a bit, so it's up to the user really, however."
        ],
        [
            "I want it to be so now if we look at some results using this we can see two robots.",
            "The 1st is a slow moving robot with very accurate sensors and the 2nd is a fast moving robot which takes a lot of extremely noisy range returns.",
            "Here you can see them in blue and it's the boat ride around the same environment here and we want to represent this all in a common probabilistic framework.",
            "So if we compare our previous tech."
        ],
        [
            "With this updated technique and seen the previous technique here, it doesn't distinguish between the noisy training inputs and the clean training inputs, and we get this kind of fragmented look, whereas here you can see that the red is saying areas that you're very competent or occupied, and this comes from areas that have been scanned with the cleaner sensor and then over here you can see this area has been scanned by the noisier sensor, but so it's making some kind of estimate as to where the wall is, but it's not too confident, which is kind of what you want.",
            "This is just classified version over here, so you can see these fragments here.",
            "Which can be very problematic if you're actually navigating this environment, whereas here you get much closer representation, and finally the variance.",
            "What's interesting here is that you can see that the variance no longer just depends on the sparsity of the data that it's also depending on how noisy your training inputs are.",
            "So over here, even though this is being scanned, it's been scanned by the noise your sensor, so it has a higher variance, so this might be an area that you might want to explore next so you can maximize your understanding of the environment.",
            "So just to finish up with on, that's just a rocker that compares the two.",
            "And yeah, what's important here is you can see that in this modified example is that.",
            "This kind of slow rise here kind of points to areas that are occupied but haven't been identified yet, so this can be very problematic.",
            "Like I said, if you try to navigate that environment."
        ],
        [
            "So just to finish up an outdoor data set.",
            "So we had this huge and we just talk a lot of sensors onto it.",
            "We had a laser range Finder.",
            "We drove it around the campus.",
            "I took some range finders as well as vehicle position so we can see the vehicle pose.",
            "Here is in blue and then this is at the vehicles their covariance in its position.",
            "It's uncertainty, you can see it grows exponentially, but if we use Lambda technique that I was talking on the previous session, there are previous talks.",
            "Sorry you can see that we can close the loop here once it returns and we shrink down that variance and the uncertainty is propagated back along the loop.",
            "So to compare."
        ],
        [
            "Just this is prior to closure and after the closure, and you can see that the uncertainty in the vehicles position is correctly and kind of input into the output variance here, so you can see the variance increases as you travel and as a result then the output of your probability of occupancy.",
            "Here it's correctly uncertain, doesn't make any definite calls as to where the road is, where the buildings are.",
            "You can see you can kind of make up the road here, so just looking at this area and then the buildings.",
            "Whereas when you close the loop you get a much cleaner and crisper representation of of what's going on.",
            "So that's it then.",
            "This is just a quick comparison with the occupancy grid, so the car drove here and you took some scans of some parked cars so we can see.",
            "It makes inference into this region, but with the occupancy grid, it's kind of hard to distinguish between the cars in the buildings here, so that's just a major benefit of this approach, so I'll just finish."
        ],
        [
            "Important conclusions, so it removes in dependencies between the data points.",
            "It enables accurate Maps we generated with relatively sparse data, and you're correctly incorporating information from multiple sources.",
            "It eliminates the restriction of building Maps in a single scale, and, importantly, you have this variance, so you have some degree of how much you can trust the inferences that you're making it to the unscanned regions.",
            "So."
        ],
        [
            "Thanks very much for listening.",
            "Take any questions.",
            "What's this?",
            "Yeah.",
            "Information for Prespective observed along the beach.",
            "So yeah, I guess I kind of skipped over that point, kind of quickly.",
            "That's right there now.",
            "What we're doing is that we're modeling that as an unoccupied line segment.",
            "So just wait.",
            "Yeah, so we model is an awkward line segment, so if you have a test point, we find the training points that are near, so we're finding the nearest occupied points and then we look pointed the nearest on occupied line segments and what we had an initial pass.",
            "What we did is we broke up this line segment into lots of unoccupied points, but the data set first of all grew massively.",
            "And also you're actually kind of enforcing a scale onto your.",
            "Onto your map.",
            "Then you know.",
            "So how far apart to space out your unoccupied points?",
            "So what we do is for each unoccupied line segment, we find the closest point on that unoccupied line segment and use that.",
            "Then as the as the input it represents the free space in that example.",
            "In space.",
            "They cannot be used in distance to occupy points.",
            "Yeah, that's one thing that we're looking at now is maybe integrating over that line segment and taking considering each as if it was broken with the infinite number of unoccupied points and looking there, but that's just really recent stuff that we're looking at now.",
            "Yeah, yeah, yeah, yeah.",
            "Still waiting for answers pondering.",
            "As I look at this, it seems to me that we weren't currently about.",
            "Including price, but the shapes of the occupied regions, like Europe, Obviously they're going to propose the rectangular segments.",
            "Yeah, yeah.",
            "How easy would be?",
            "We remove that fire, fire assumptions.",
            "Yeah, I. I guess I haven't thought about that.",
            "So at the moment we're using an isotropic.",
            "Covariance function where but maybe with your approach there.",
            "So if you're saying like on a street you have this long building and say going around to each side so there's like a preference as to which way.",
            "Correlation occurs, it is or probably you can't really do that on the level of correlation between individual very small, very small pieces with the hierarchy.",
            "OK yeah.",
            "So semantically, is that what you're saying or above that?",
            "Or yeah, sure, maybe we can talk about it, OK?",
            "The area occupied or not, arguing rectangle that yeah, yeah it would be nice if they agree it could.",
            "It could incorporate them somehow.",
            "If you have any ideas or anything I could talk about you with you afterwards about that.",
            "Yeah so.",
            "Yeah, yeah.",
            "Sorry Oh yeah yeah.",
            "So one thing that we're doing actually.",
            "So when we have a we have a test point we shift that towards so we take a test point between the training points around and we shift more back to the origin then.",
            "So it is around 0 or not in that sense.",
            "Yeah.",
            "Yeah yeah, so because using this KD tree approach we're taking each point.",
            "It's like thousands of different Gaussian process each time we're taking one point in the training points around and doing Gaussian process.",
            "And at one point, and we're shifting or the training points for that back to the origin.",
            "And in that sense, so so you're never considering points that are very far away from the origin.",
            "Another one that we want to look at is this path you wreck.",
            "He has a covariance function that models, so it's like a squared exponential, but it also has another GP underneath that models the model changing in the lens scale, which might be something we want to look at next, so you could closer towards where this transition.",
            "The length scale would shorten and you could get a sudden change, then from occupied, unoccupied in that sense.",
            "Yeah, that's.",
            "Oh, how is our one train does it or yeah man.",
            "So another parameter that we have is we allowed to train where the origin is.",
            "Then in that sense as well.",
            "So it's training high parameters and location for the origin and we just do it on one street at a time then.",
            "So we take one street and then training from there.",
            "I can talk to you about it after this.",
            "Yeah yeah, sure yeah, no worries OK?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so generating spatially accurate representations overall.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What environment is a critical task in autonomous robotics?",
                    "label": 0
                },
                {
                    "sent": "So you would have seen there are a lot of examples.",
                    "label": 0
                },
                {
                    "sent": "These representations can take on forms like, say, a semantic representation like you might you might have rolled here like you were saying the previous talk and then further down.",
                    "label": 0
                },
                {
                    "sent": "You want to set of trees and some grass insight.",
                    "label": 0
                },
                {
                    "sent": "Or you might also be interested in generating a map that represents the area in terms of areas that are occupied and unoccupied.",
                    "label": 0
                },
                {
                    "sent": "So even ideas where you can travel for example.",
                    "label": 0
                },
                {
                    "sent": "But in any case decent representation is essential for activities like localization.",
                    "label": 0
                },
                {
                    "sent": "Path planning, cooperation and interactions with the environment.",
                    "label": 0
                },
                {
                    "sent": "So obviously it's been the focus of huge amounts of research over the last 20 years in the robotics community, but it still is a largely unsolved problem.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is we're proposing a statistical modeling technique for building occupancy Maps using contextual information from potentially noisy sources, multiple sources, and integrating them all into one common probabilistic map of the environment.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that will hopefully help address some of those issues.",
                    "label": 0
                },
                {
                    "sent": "So there are dozens and dozens of different mapping techniques, and I can't talk about all them obviously, so I'll just pick on arguably the most popular one which is the occupancy grid, and what it does is it divides the world up into these discrete independent cells, and each cell you have this hypothesis of occupancy, and each time then your laser passes over that cell.",
                    "label": 1
                },
                {
                    "sent": "You're updating your hypothesis occupancy based on Bayes rule using Facebook update, so it's making this great simplification here of the environment where you're seeing each cell is independent.",
                    "label": 0
                },
                {
                    "sent": "And then because of this then you're getting a really fast computational time, which is probably why they're so popular.",
                    "label": 0
                },
                {
                    "sent": "But it does have a number of drawbacks.",
                    "label": 0
                },
                {
                    "sent": "So first of all, what you're doing is you're taking this really high dimensional problem of mapping and you're decomposing it into lots of single dimensional binary classification tasks.",
                    "label": 0
                },
                {
                    "sent": "So you might look at a map, and because each cell is independent.",
                    "label": 0
                },
                {
                    "sent": "It might be obvious to a person by taking the readings around it into context.",
                    "label": 0
                },
                {
                    "sent": "What a cell should be, but.",
                    "label": 0
                },
                {
                    "sent": "The map here you can see these areas are really high uncertainty here.",
                    "label": 0
                },
                {
                    "sent": "Just because the laser didn't pass over those exact cells and also then you're stuck to a fixed resolution in the classical occupancy grid and you're getting discretization errors as well because you're dividing the world up into these grids.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are a lot of techniques out there that that address some of these issues, so ideally what you want is you want a technique that exploits structural dependencies in the environment, so the world isn't just this random distribution of occupied and unoccupied cells, their structure in and in our environment, and a good mapping technique should exploit the structure you want be capable of handling large data sets and have an adjustable resolution so that you can kind of see the application.",
                    "label": 1
                },
                {
                    "sent": "You might want a large, coarse resolution scan of a really large area.",
                    "label": 0
                },
                {
                    "sent": "Or maybe smaller higher resolution over specific area of interest.",
                    "label": 0
                },
                {
                    "sent": "And also if you're going to be making inference into areas that the laser hasn't scanned over, you want to have kind of an associated variance.",
                    "label": 0
                },
                {
                    "sent": "Even ideas to how confident you can be with that estimate.",
                    "label": 0
                },
                {
                    "sent": "And it was these then that kind of lead us to think that maybe we could use the Gaussian process as a tool for solving these.",
                    "label": 1
                },
                {
                    "sent": "So some of the characteristics are that it learns dependencies between the training points.",
                    "label": 0
                },
                {
                    "sent": "It also generates a likelihood function and an associated variance function, and this variance function increases as you travel further away from your data, and that's a desirable characteristic.",
                    "label": 1
                },
                {
                    "sent": "And also you have a continuous underlying function, so no scale becomes arbitrary.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is just some work that we presented at icro this year, this section where we kind of decided to address the problem as a boy trying to classify the world into areas that we think are occupied in areas that we think are unoccupied.",
                    "label": 0
                },
                {
                    "sent": "So we had this robot driving around and we're getting this raw data in the form of Rangers and bearings, and the vehicle poses then convert this into Cartesian planes and represent the data as occupied points and unoccupied line segments.",
                    "label": 1
                },
                {
                    "sent": "And then we store list inside a KD tree.",
                    "label": 0
                },
                {
                    "sent": "And then we use the Gaussian process to map this information into a hypothesis of occupancy using the sensor data to help come up with this probability of occupancy for each test point, we had to use a neural network covariance function.",
                    "label": 1
                },
                {
                    "sent": "We found data is very nonstationary 'cause you're getting these sudden changes from areas that are unoccupied to these occupied points.",
                    "label": 0
                },
                {
                    "sent": "So this neural covariance function.",
                    "label": 0
                },
                {
                    "sent": "Here you can just compare it with the squared exponential.",
                    "label": 0
                },
                {
                    "sent": "It's very good at handling these step changes.",
                    "label": 0
                },
                {
                    "sent": "So we can see this is the squared exponential versus the neural network.",
                    "label": 0
                },
                {
                    "sent": "Here is to write by Chris Williams will be talking at this later on actually, so that's just another step comparison.",
                    "label": 0
                },
                {
                    "sent": "A noisy step we stored in a KD tree, so we have to make this kind of localization approximation because of this inversion.",
                    "label": 0
                },
                {
                    "sent": "Here it takes too long.",
                    "label": 0
                },
                {
                    "sent": "People try to model the entire every training point at once, and for our application it makes sense to make this kind of local approximation anyway.",
                    "label": 1
                },
                {
                    "sent": "That's just the sensor model who won't talk too much and then we get this output.",
                    "label": 0
                },
                {
                    "sent": "It predicted, meaning predicted variance and goes into this probabilistic least squares classifier.",
                    "label": 0
                },
                {
                    "sent": "And then you're making this.",
                    "label": 0
                },
                {
                    "sent": "We've trained this and then it gives you an output saying, OK, I think the probability of this area being occupied is whatever some number between one and 0.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just looking at some simulated results for this first section we just made this kind of street map where you have some buildings here, some side streets.",
                    "label": 0
                },
                {
                    "sent": "If you parked cars and we drove robot just around here, you can see this is the data that we got back so we can see the robots poses are in red and the laser returns are in blue.",
                    "label": 0
                },
                {
                    "sent": "Sophie.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apply this data to the occupancy grid.",
                    "label": 1
                },
                {
                    "sent": "We can see a number of drawbacks at the occupancy grid has you can see that large portions of this map are still an altered from the prior because the laser hasn't passed over those exact things, so you're not doing any inference in occupancy map, even though might be obvious to a person you know, you can make some decent estimates as to where the road is here.",
                    "label": 1
                },
                {
                    "sent": "The occupancy map doesn't tell you this information.",
                    "label": 0
                },
                {
                    "sent": "A common trick is to with occupancy grids.",
                    "label": 1
                },
                {
                    "sent": "It's going to add some surplus noise, but all your doing is smearing the data there really and there still are.",
                    "label": 0
                },
                {
                    "sent": "Even with this, it's not true inference first of all, but also there still are a lot of areas of uncertainty here, like behind the cars and down those roadways there that you only have a few scans so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Taking a Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "And using the Gaussian process, this is the result we get here.",
                    "label": 0
                },
                {
                    "sent": "So Red is areas that are occupied and going down to blue areas you're certain or unoccupied, so it bears much closer resemblance to the ground truth.",
                    "label": 0
                },
                {
                    "sent": "You can make out the side streets and cars is 1 much more clearly, and also you can see that as you go further away from your data, that the probability falls back down to nought .5 which is going to want.",
                    "label": 0
                },
                {
                    "sent": "You want an I said that scale becomes arbitrary as well, 'cause it's a continuous function that you can just quickly re sample if you want a very quick QR scanner of the areas.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's just the classified classifying it there on the left, and that's your variance map there.",
                    "label": 0
                },
                {
                    "sent": "So the variance is large inside the buildings, obviously.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's just a rocker analysis compared.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we ignored in network and up until now was an important focus of this is how do we incorporate information from multiple sources into the same map?",
                    "label": 0
                },
                {
                    "sent": "If you're if you're different sources have different levels of noise.",
                    "label": 0
                },
                {
                    "sent": "So in robotics here also and have some noise associated with your sensor, so you're going to range and bearing noise, and you're also going to have noise on your robots location, so you might have some kind of encoders on the wheels, and in this case if you're only using that.",
                    "label": 0
                },
                {
                    "sent": "This uncertainty is going to grow exponentially, or else even if you have GPS or using something like SLAM there which was described in the last talk, you're still going to have always going to have uncertainty in your position.",
                    "label": 0
                },
                {
                    "sent": "So how we're modeling.",
                    "label": 0
                },
                {
                    "sent": "This is where using an unscented transform.",
                    "label": 0
                },
                {
                    "sent": "So I have a slide in this and I can kind of the presentation, but if people ask me with this, I can explain it later, so we use an unscented transform to model all these noises as multivariate Gaussians, and then we incorporate that into the training inputs of the upper map.",
                    "label": 0
                },
                {
                    "sent": "So the problem with this is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the Gaussian processes that we now have noise on, the training inputs not in the outputs for on the X domain.",
                    "label": 0
                },
                {
                    "sent": "So if we look at how a GPS deals with this, this is just some ground truth function that we've taken some observations of, and we can put a GP to this nicely with using squared exponential covariance function.",
                    "label": 0
                },
                {
                    "sent": "But if one of these points is noisy in the X domain, so if we if this point we actually recorded as being over here Now we're asking GP2 trying to fit into this data and you can see that it has a Mass Effect on the output of the GP here.",
                    "label": 0
                },
                {
                    "sent": "So it assumes that this data set is no very noisy, because as far as it's concerned, this point lies somewhere in this column here, which isn't the case.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to do if you look at the comparison of the covariance matrix with the training points in the test points, you can see here that this training point here is incorrectly influencing the wrong set of training of test points, so we don't like to do with disperse its influence over the area.",
                    "label": 0
                },
                {
                    "sent": "That's kind of proportion to how noisy that test point is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we redefine our, our training points is observations that are corrupted by some noise, we can say that this noise is Gaussian because we're using the unscented transform, we kind of follow this logic.",
                    "label": 0
                },
                {
                    "sent": "We define the kernel function over distributions now instead of just deterministic points.",
                    "label": 1
                },
                {
                    "sent": "So we come up with this double integral, which unfortunately is on intractable for intractable for a lot of cases.",
                    "label": 0
                },
                {
                    "sent": "Now you can make some big assumptions and say that this noise is independent, which isn't the case for us.",
                    "label": 0
                },
                {
                    "sent": "But if we do this for the time being.",
                    "label": 0
                },
                {
                    "sent": "So far only concerted sensor noise.",
                    "label": 0
                },
                {
                    "sent": "Say we can say that that's independent.",
                    "label": 1
                },
                {
                    "sent": "However, for us all our noise is dependent because it's all linked through the robots location.",
                    "label": 0
                },
                {
                    "sent": "So then we can.",
                    "label": 0
                },
                {
                    "sent": "We can write it like this.",
                    "label": 0
                },
                {
                    "sent": "We can separate over this this joint here.",
                    "label": 0
                },
                {
                    "sent": "And if we assume as well that are covariance matrix is Gaussian, so something like a squared exponential, then we can come up with a closed form solution and it looks like this.",
                    "label": 0
                },
                {
                    "sent": "So I can see here that the noise is reducing the Sigma F here and it's also increasing the length scale.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of having that effective dispersing the influence of that of that point, which is what we want.",
                    "label": 0
                },
                {
                    "sent": "So if we take this new.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Covariance function and this time account for this noise on the training input we can see we get a much better fit, so it's no longer just almost flat thing that these points are correctly influencing the the mean as we'd expect.",
                    "label": 0
                },
                {
                    "sent": "And if we look at that same graph again, we can see that this noisy point its influence has been dispersed like what we want.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned that in our case we don't have enough to separate this joint here, and also that we have a neural network covariance function, so we can't make this assumption that it's Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So what we need to do is we use the Ghost Herman Quadrature and when it says that if we can represent her equation like this.",
                    "label": 0
                },
                {
                    "sent": "Then we can come up with.",
                    "label": 0
                },
                {
                    "sent": "A decent approximation of it in terms of just sampling from some function.",
                    "label": 0
                },
                {
                    "sent": "So because this is a multivariate Gaussian, we can represent that as an exponential with just a kind of a.",
                    "label": 0
                },
                {
                    "sent": "We can solve this and we come up with this covariance function here, and this is a place any kind of a covariance function, so we can use your neural network here.",
                    "label": 1
                },
                {
                    "sent": "Now in this case, and just comparing the closed form with with the quadrature here, this is a level one approximation, so you can see it as well, but for increased input in the noise it kind of starts to diverge.",
                    "label": 0
                },
                {
                    "sent": "And you can use a Level 2 Level 3 or whatever.",
                    "label": 0
                },
                {
                    "sent": "This is the level 2.",
                    "label": 1
                },
                {
                    "sent": "You can see that improves, but obviously you need to take more samples so your algorithm slows down a bit, so it's up to the user really, however.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want it to be so now if we look at some results using this we can see two robots.",
                    "label": 0
                },
                {
                    "sent": "The 1st is a slow moving robot with very accurate sensors and the 2nd is a fast moving robot which takes a lot of extremely noisy range returns.",
                    "label": 0
                },
                {
                    "sent": "Here you can see them in blue and it's the boat ride around the same environment here and we want to represent this all in a common probabilistic framework.",
                    "label": 0
                },
                {
                    "sent": "So if we compare our previous tech.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With this updated technique and seen the previous technique here, it doesn't distinguish between the noisy training inputs and the clean training inputs, and we get this kind of fragmented look, whereas here you can see that the red is saying areas that you're very competent or occupied, and this comes from areas that have been scanned with the cleaner sensor and then over here you can see this area has been scanned by the noisier sensor, but so it's making some kind of estimate as to where the wall is, but it's not too confident, which is kind of what you want.",
                    "label": 0
                },
                {
                    "sent": "This is just classified version over here, so you can see these fragments here.",
                    "label": 0
                },
                {
                    "sent": "Which can be very problematic if you're actually navigating this environment, whereas here you get much closer representation, and finally the variance.",
                    "label": 0
                },
                {
                    "sent": "What's interesting here is that you can see that the variance no longer just depends on the sparsity of the data that it's also depending on how noisy your training inputs are.",
                    "label": 0
                },
                {
                    "sent": "So over here, even though this is being scanned, it's been scanned by the noise your sensor, so it has a higher variance, so this might be an area that you might want to explore next so you can maximize your understanding of the environment.",
                    "label": 0
                },
                {
                    "sent": "So just to finish up with on, that's just a rocker that compares the two.",
                    "label": 0
                },
                {
                    "sent": "And yeah, what's important here is you can see that in this modified example is that.",
                    "label": 0
                },
                {
                    "sent": "This kind of slow rise here kind of points to areas that are occupied but haven't been identified yet, so this can be very problematic.",
                    "label": 0
                },
                {
                    "sent": "Like I said, if you try to navigate that environment.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to finish up an outdoor data set.",
                    "label": 0
                },
                {
                    "sent": "So we had this huge and we just talk a lot of sensors onto it.",
                    "label": 0
                },
                {
                    "sent": "We had a laser range Finder.",
                    "label": 0
                },
                {
                    "sent": "We drove it around the campus.",
                    "label": 0
                },
                {
                    "sent": "I took some range finders as well as vehicle position so we can see the vehicle pose.",
                    "label": 0
                },
                {
                    "sent": "Here is in blue and then this is at the vehicles their covariance in its position.",
                    "label": 0
                },
                {
                    "sent": "It's uncertainty, you can see it grows exponentially, but if we use Lambda technique that I was talking on the previous session, there are previous talks.",
                    "label": 0
                },
                {
                    "sent": "Sorry you can see that we can close the loop here once it returns and we shrink down that variance and the uncertainty is propagated back along the loop.",
                    "label": 0
                },
                {
                    "sent": "So to compare.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just this is prior to closure and after the closure, and you can see that the uncertainty in the vehicles position is correctly and kind of input into the output variance here, so you can see the variance increases as you travel and as a result then the output of your probability of occupancy.",
                    "label": 1
                },
                {
                    "sent": "Here it's correctly uncertain, doesn't make any definite calls as to where the road is, where the buildings are.",
                    "label": 0
                },
                {
                    "sent": "You can see you can kind of make up the road here, so just looking at this area and then the buildings.",
                    "label": 0
                },
                {
                    "sent": "Whereas when you close the loop you get a much cleaner and crisper representation of of what's going on.",
                    "label": 0
                },
                {
                    "sent": "So that's it then.",
                    "label": 0
                },
                {
                    "sent": "This is just a quick comparison with the occupancy grid, so the car drove here and you took some scans of some parked cars so we can see.",
                    "label": 0
                },
                {
                    "sent": "It makes inference into this region, but with the occupancy grid, it's kind of hard to distinguish between the cars in the buildings here, so that's just a major benefit of this approach, so I'll just finish.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Important conclusions, so it removes in dependencies between the data points.",
                    "label": 1
                },
                {
                    "sent": "It enables accurate Maps we generated with relatively sparse data, and you're correctly incorporating information from multiple sources.",
                    "label": 1
                },
                {
                    "sent": "It eliminates the restriction of building Maps in a single scale, and, importantly, you have this variance, so you have some degree of how much you can trust the inferences that you're making it to the unscanned regions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks very much for listening.",
                    "label": 0
                },
                {
                    "sent": "Take any questions.",
                    "label": 0
                },
                {
                    "sent": "What's this?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Information for Prespective observed along the beach.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I guess I kind of skipped over that point, kind of quickly.",
                    "label": 0
                },
                {
                    "sent": "That's right there now.",
                    "label": 0
                },
                {
                    "sent": "What we're doing is that we're modeling that as an unoccupied line segment.",
                    "label": 0
                },
                {
                    "sent": "So just wait.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we model is an awkward line segment, so if you have a test point, we find the training points that are near, so we're finding the nearest occupied points and then we look pointed the nearest on occupied line segments and what we had an initial pass.",
                    "label": 0
                },
                {
                    "sent": "What we did is we broke up this line segment into lots of unoccupied points, but the data set first of all grew massively.",
                    "label": 0
                },
                {
                    "sent": "And also you're actually kind of enforcing a scale onto your.",
                    "label": 0
                },
                {
                    "sent": "Onto your map.",
                    "label": 0
                },
                {
                    "sent": "Then you know.",
                    "label": 0
                },
                {
                    "sent": "So how far apart to space out your unoccupied points?",
                    "label": 0
                },
                {
                    "sent": "So what we do is for each unoccupied line segment, we find the closest point on that unoccupied line segment and use that.",
                    "label": 0
                },
                {
                    "sent": "Then as the as the input it represents the free space in that example.",
                    "label": 0
                },
                {
                    "sent": "In space.",
                    "label": 0
                },
                {
                    "sent": "They cannot be used in distance to occupy points.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's one thing that we're looking at now is maybe integrating over that line segment and taking considering each as if it was broken with the infinite number of unoccupied points and looking there, but that's just really recent stuff that we're looking at now.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Still waiting for answers pondering.",
                    "label": 0
                },
                {
                    "sent": "As I look at this, it seems to me that we weren't currently about.",
                    "label": 0
                },
                {
                    "sent": "Including price, but the shapes of the occupied regions, like Europe, Obviously they're going to propose the rectangular segments.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "How easy would be?",
                    "label": 0
                },
                {
                    "sent": "We remove that fire, fire assumptions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I. I guess I haven't thought about that.",
                    "label": 0
                },
                {
                    "sent": "So at the moment we're using an isotropic.",
                    "label": 0
                },
                {
                    "sent": "Covariance function where but maybe with your approach there.",
                    "label": 0
                },
                {
                    "sent": "So if you're saying like on a street you have this long building and say going around to each side so there's like a preference as to which way.",
                    "label": 0
                },
                {
                    "sent": "Correlation occurs, it is or probably you can't really do that on the level of correlation between individual very small, very small pieces with the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "OK yeah.",
                    "label": 0
                },
                {
                    "sent": "So semantically, is that what you're saying or above that?",
                    "label": 0
                },
                {
                    "sent": "Or yeah, sure, maybe we can talk about it, OK?",
                    "label": 0
                },
                {
                    "sent": "The area occupied or not, arguing rectangle that yeah, yeah it would be nice if they agree it could.",
                    "label": 0
                },
                {
                    "sent": "It could incorporate them somehow.",
                    "label": 0
                },
                {
                    "sent": "If you have any ideas or anything I could talk about you with you afterwards about that.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Sorry Oh yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So one thing that we're doing actually.",
                    "label": 0
                },
                {
                    "sent": "So when we have a we have a test point we shift that towards so we take a test point between the training points around and we shift more back to the origin then.",
                    "label": 0
                },
                {
                    "sent": "So it is around 0 or not in that sense.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, so because using this KD tree approach we're taking each point.",
                    "label": 0
                },
                {
                    "sent": "It's like thousands of different Gaussian process each time we're taking one point in the training points around and doing Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "And at one point, and we're shifting or the training points for that back to the origin.",
                    "label": 0
                },
                {
                    "sent": "And in that sense, so so you're never considering points that are very far away from the origin.",
                    "label": 0
                },
                {
                    "sent": "Another one that we want to look at is this path you wreck.",
                    "label": 0
                },
                {
                    "sent": "He has a covariance function that models, so it's like a squared exponential, but it also has another GP underneath that models the model changing in the lens scale, which might be something we want to look at next, so you could closer towards where this transition.",
                    "label": 0
                },
                {
                    "sent": "The length scale would shorten and you could get a sudden change, then from occupied, unoccupied in that sense.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's.",
                    "label": 0
                },
                {
                    "sent": "Oh, how is our one train does it or yeah man.",
                    "label": 0
                },
                {
                    "sent": "So another parameter that we have is we allowed to train where the origin is.",
                    "label": 0
                },
                {
                    "sent": "Then in that sense as well.",
                    "label": 0
                },
                {
                    "sent": "So it's training high parameters and location for the origin and we just do it on one street at a time then.",
                    "label": 0
                },
                {
                    "sent": "So we take one street and then training from there.",
                    "label": 0
                },
                {
                    "sent": "I can talk to you about it after this.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, sure yeah, no worries OK?",
                    "label": 0
                }
            ]
        }
    }
}