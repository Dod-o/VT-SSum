{
    "id": "3ghpu3u34sen74v2jnogbotlr56zdbwu",
    "title": "Inference for piecewise-constant gaussian graphical models",
    "info": {
        "author": [
            "Alex Gibberd, Department of Statistical Science, University College London"
        ],
        "published": "March 7, 2016",
        "recorded": "December 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Physics->Statistical Physics"
        ]
    },
    "url": "http://videolectures.net/netadis2015_gibberd_gaussian_graphical_models/",
    "segmentation": [
        [
            "My name is Alex Gilbert.",
            "I'm going to be talking about some work we've been doing for the past year or two.",
            "My supervisor, James Nelson, over at UCL in London.",
            "So you point out now I'm I used to be a physicist, so my undergraduate was in physics, but now I'm kind of on the machine learning side of stuff so.",
            "It would be interesting to see you guys thoughts on some of this stuff.",
            "So basically what I'm going to be talking about is how do we infer these kind of Gaussian graphical models when we allow them to change overtime and why we might want to do this."
        ],
        [
            "So.",
            "First of all, talk about some different types of network graph models.",
            "And then I'll talk about the Gaussian graphical models, which you guys probably kind of know about already before introducing the kind of dynamics that we're considering.",
            "Then I'll give an example application.",
            "And then.",
            "What we're going to look at."
        ],
        [
            "Major so.",
            "There's two kind of broad models of.",
            "Types of network models, so does that kind of relational network models where it's like you might see an edge between two nodes and you use that measurement off the edge to say there's an edge there.",
            "For example, Facebook friends.",
            "You see these friends are connected and you just draw the graph.",
            "So that's this kind of relational network.",
            "I'm not going to be interested in that today, so.",
            "What I'm talking about is attributional graphical models.",
            "So this is basically where you treat the edges.",
            "This kind of latent variables.",
            "So you observe some nodes, for example like neurons or e.g sensors.",
            "And then you want to try and infer some kind of network structure which.",
            "Describes the behavior of the processors on these nodes, so we're trying to infer these kind of edges here.",
            "I think this is.",
            "Some kind of roundabout in Singapore.",
            "Very awful junction."
        ],
        [
            "And OK, so the basic setup.",
            "There's a kind of Gaussian graphical models going to Canonical.",
            "Conditional independence type graph.",
            "So the idea is here that you decompose your joint distribution over Gaussian across different edges, and as you guys probably all know, you can kind of identify the edges by the non zeros in the precision matrix.",
            "So the position matrix, just the inverse of the covariance.",
            "So this is kind of a really ancient model.",
            "It's the most basic one you really think of using.",
            "Anne.",
            "And if we think about using this as studying time series data, we we kind of assume that the covariance structure here is kind of the same free time.",
            "So.",
            "What we think what many people think is that if you use such a model, then you're kind of fitting this stationary model to something which might evolve through time.",
            "So the idea is how do we like generalize this model so we can get some of that behavior through time.",
            "And then that will hopefully tell us something interesting about our system.",
            "So."
        ],
        [
            "That picture is not really work, but so the simplest extension to this is just simply say you have discrete time process and you just say the covariance matrix can vary at every point in time.",
            "Anne.",
            "Obviously there's massive issues here, so we only get one measurement of data point at each point in time.",
            "There's nowhere near identifiable, so the model complexity can scale with TP squared roughly.",
            "So P is the number of variables you're measuring, the number of data streams, and tease the number of time points you're measuring on.",
            "Anne.",
            "So what you have to do is kind of restrict your model quite a lot to be able to actually do estimation in this kind of framework.",
            "So not too many people have kind of looked at this and kind of statistical estimation.",
            "Methods, but like.",
            "You're going to have to restrict the variation of your model overtime, so instead of just assuming the sparsity, which is the traditional thing with things like the graphical lasso.",
            "We're going to start making some assumptions about how this varies overtime."
        ],
        [
            "So what we do is we just kind of use this kind of penalized maximum likelihood approach, so we assume that the drawings are going to be independent.",
            "But now where letting the kind of distribution varies, so they're not identical, right?",
            "And so this is the kind of.",
            "Log likelihood you get for sampling T points from a Gaussian process.",
            "With precision matrix feature right?",
            "So the S here is just the kind of empirical covariance matrix an estimated individual time point.",
            "And as we suggested before, this is kind of not identifiable if you don't put additional constraints on how this feature can vary.",
            "So what we've basically done is the Super simple thing where you just kind of explicitly constrain your estimates at points in time.",
            "So you put a kind of smoothness constraint on.",
            "Both the sparsity of your graph.",
            "And the smoothness of your graph.",
            "And what we're particularly looking at is different types of smoothness assumptions you can put in there.",
            "So I think Zhao and wiser man did something in 2008, Wedi, assuming continuously varying graphical models, so they use the kernel estimator.",
            "So the currently kernel estimate of DS.",
            "We wanted to try and generalize to kind of piecewise constant process is 'cause.",
            "Often such big jumps in systems actually going to tell you something about kind of large scale structure of the system.",
            "So this penalty here is one thing which is being investigated, so this is kind of saying the changes in the graph structure going to be sparse overtime and another one we've recently looked at is this kind of fused graphical or so.",
            "So this is where we're saying changes are sparse with respect to time, so there's only a few changes, but the effects, like lots of the dynamics within the system.",
            "So lots of the edges can change out those time points.",
            "So they suddenly diff."
        ],
        [
            "I'm going to talk about a little bit of an application now.",
            "I'm not going to have time to go through it properly and so.",
            "Come get that poster afterwards.",
            "The basic idea here is.",
            "We want to try and do inference on a system where we have some kind of ground truth, I mean.",
            "In reality, I think it was said before.",
            "There's not really much ground truth in these things.",
            "And so we were looking for a system where.",
            "We could get some kind of plausible changepoints and kind of verify offering empirically.",
            "So what we're doing here is looking at genetic activity of the Drosophila fruit fly, so this is as it grows up from, like a lava to a kind of adult fly.",
            "So there's clear kind of physiological changes in how to fly.",
            "Looks like can we recover these just by looking at the genetic structure?",
            "Using our methods, so we're using a super simple kind of.",
            "Shark model system.",
            "Look at order aggressive model, super simple, but our noise term here is going to be our kind of regularize Gaussian process."
        ],
        [
            "So here is the kind of results.",
            "So this is a nightmare slide.",
            "You'll probably have to talk to me more very later, but.",
            "Basically this is comparing the two types of smoothness assumption that we get when we do the inference in the model.",
            "So we have the graphs which kind of recovered at different points in time.",
            "But then we have a different types of behavior for the kind of how these graphs evolve depending on the kind of structure we imposed through our regularization terms.",
            "So in this one we're using this kind of group regularizer, so we're grouping changes across the whole set of nodes.",
            "So here this is a subset of 100 jeans.",
            "So you can kind of see like when there is a change, it's affecting a large subset of the genes in there.",
            "So it's kind of interesting because you get these kind of.",
            "It's not shown on here, but there's kind of a.",
            "Where it kind of grows up, you can recover that kind of structure without having any kind of assumptions on that a priori, so it's kind of suggesting that maybe this is a useful tool."
        ],
        [
            "So.",
            "That's kind of where we're at the moment we're trying to assess this kind of more theoretically, so, like looking at sample complexity of these models, it's quite hard in this kind of change point set up to do that.",
            "Generalizing these to non Gaussian cases, so looking at popular versions of this an for cases where you have kind of mean structure.",
            "So here I've used this kind of a 0 mean model so we have some papers on kind of wavelet version of this which is much more flexible still.",
            "I.",
            "And then can we layer techniques for analyzing the kind of structures we recover from these graphs to kind of bring further insight to these kind of systems?",
            "This is pretty much it.",
            "Any questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Alex Gilbert.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be talking about some work we've been doing for the past year or two.",
                    "label": 0
                },
                {
                    "sent": "My supervisor, James Nelson, over at UCL in London.",
                    "label": 0
                },
                {
                    "sent": "So you point out now I'm I used to be a physicist, so my undergraduate was in physics, but now I'm kind of on the machine learning side of stuff so.",
                    "label": 0
                },
                {
                    "sent": "It would be interesting to see you guys thoughts on some of this stuff.",
                    "label": 0
                },
                {
                    "sent": "So basically what I'm going to be talking about is how do we infer these kind of Gaussian graphical models when we allow them to change overtime and why we might want to do this.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "First of all, talk about some different types of network graph models.",
                    "label": 1
                },
                {
                    "sent": "And then I'll talk about the Gaussian graphical models, which you guys probably kind of know about already before introducing the kind of dynamics that we're considering.",
                    "label": 1
                },
                {
                    "sent": "Then I'll give an example application.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "What we're going to look at.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Major so.",
                    "label": 0
                },
                {
                    "sent": "There's two kind of broad models of.",
                    "label": 0
                },
                {
                    "sent": "Types of network models, so does that kind of relational network models where it's like you might see an edge between two nodes and you use that measurement off the edge to say there's an edge there.",
                    "label": 0
                },
                {
                    "sent": "For example, Facebook friends.",
                    "label": 0
                },
                {
                    "sent": "You see these friends are connected and you just draw the graph.",
                    "label": 0
                },
                {
                    "sent": "So that's this kind of relational network.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to be interested in that today, so.",
                    "label": 0
                },
                {
                    "sent": "What I'm talking about is attributional graphical models.",
                    "label": 0
                },
                {
                    "sent": "So this is basically where you treat the edges.",
                    "label": 0
                },
                {
                    "sent": "This kind of latent variables.",
                    "label": 0
                },
                {
                    "sent": "So you observe some nodes, for example like neurons or e.g sensors.",
                    "label": 0
                },
                {
                    "sent": "And then you want to try and infer some kind of network structure which.",
                    "label": 0
                },
                {
                    "sent": "Describes the behavior of the processors on these nodes, so we're trying to infer these kind of edges here.",
                    "label": 0
                },
                {
                    "sent": "I think this is.",
                    "label": 0
                },
                {
                    "sent": "Some kind of roundabout in Singapore.",
                    "label": 0
                },
                {
                    "sent": "Very awful junction.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And OK, so the basic setup.",
                    "label": 0
                },
                {
                    "sent": "There's a kind of Gaussian graphical models going to Canonical.",
                    "label": 1
                },
                {
                    "sent": "Conditional independence type graph.",
                    "label": 0
                },
                {
                    "sent": "So the idea is here that you decompose your joint distribution over Gaussian across different edges, and as you guys probably all know, you can kind of identify the edges by the non zeros in the precision matrix.",
                    "label": 1
                },
                {
                    "sent": "So the position matrix, just the inverse of the covariance.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a really ancient model.",
                    "label": 0
                },
                {
                    "sent": "It's the most basic one you really think of using.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And if we think about using this as studying time series data, we we kind of assume that the covariance structure here is kind of the same free time.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we think what many people think is that if you use such a model, then you're kind of fitting this stationary model to something which might evolve through time.",
                    "label": 0
                },
                {
                    "sent": "So the idea is how do we like generalize this model so we can get some of that behavior through time.",
                    "label": 0
                },
                {
                    "sent": "And then that will hopefully tell us something interesting about our system.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That picture is not really work, but so the simplest extension to this is just simply say you have discrete time process and you just say the covariance matrix can vary at every point in time.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Obviously there's massive issues here, so we only get one measurement of data point at each point in time.",
                    "label": 1
                },
                {
                    "sent": "There's nowhere near identifiable, so the model complexity can scale with TP squared roughly.",
                    "label": 1
                },
                {
                    "sent": "So P is the number of variables you're measuring, the number of data streams, and tease the number of time points you're measuring on.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So what you have to do is kind of restrict your model quite a lot to be able to actually do estimation in this kind of framework.",
                    "label": 0
                },
                {
                    "sent": "So not too many people have kind of looked at this and kind of statistical estimation.",
                    "label": 0
                },
                {
                    "sent": "Methods, but like.",
                    "label": 0
                },
                {
                    "sent": "You're going to have to restrict the variation of your model overtime, so instead of just assuming the sparsity, which is the traditional thing with things like the graphical lasso.",
                    "label": 0
                },
                {
                    "sent": "We're going to start making some assumptions about how this varies overtime.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do is we just kind of use this kind of penalized maximum likelihood approach, so we assume that the drawings are going to be independent.",
                    "label": 0
                },
                {
                    "sent": "But now where letting the kind of distribution varies, so they're not identical, right?",
                    "label": 0
                },
                {
                    "sent": "And so this is the kind of.",
                    "label": 0
                },
                {
                    "sent": "Log likelihood you get for sampling T points from a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "With precision matrix feature right?",
                    "label": 0
                },
                {
                    "sent": "So the S here is just the kind of empirical covariance matrix an estimated individual time point.",
                    "label": 0
                },
                {
                    "sent": "And as we suggested before, this is kind of not identifiable if you don't put additional constraints on how this feature can vary.",
                    "label": 0
                },
                {
                    "sent": "So what we've basically done is the Super simple thing where you just kind of explicitly constrain your estimates at points in time.",
                    "label": 0
                },
                {
                    "sent": "So you put a kind of smoothness constraint on.",
                    "label": 0
                },
                {
                    "sent": "Both the sparsity of your graph.",
                    "label": 0
                },
                {
                    "sent": "And the smoothness of your graph.",
                    "label": 0
                },
                {
                    "sent": "And what we're particularly looking at is different types of smoothness assumptions you can put in there.",
                    "label": 0
                },
                {
                    "sent": "So I think Zhao and wiser man did something in 2008, Wedi, assuming continuously varying graphical models, so they use the kernel estimator.",
                    "label": 0
                },
                {
                    "sent": "So the currently kernel estimate of DS.",
                    "label": 0
                },
                {
                    "sent": "We wanted to try and generalize to kind of piecewise constant process is 'cause.",
                    "label": 0
                },
                {
                    "sent": "Often such big jumps in systems actually going to tell you something about kind of large scale structure of the system.",
                    "label": 0
                },
                {
                    "sent": "So this penalty here is one thing which is being investigated, so this is kind of saying the changes in the graph structure going to be sparse overtime and another one we've recently looked at is this kind of fused graphical or so.",
                    "label": 0
                },
                {
                    "sent": "So this is where we're saying changes are sparse with respect to time, so there's only a few changes, but the effects, like lots of the dynamics within the system.",
                    "label": 0
                },
                {
                    "sent": "So lots of the edges can change out those time points.",
                    "label": 0
                },
                {
                    "sent": "So they suddenly diff.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about a little bit of an application now.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to have time to go through it properly and so.",
                    "label": 0
                },
                {
                    "sent": "Come get that poster afterwards.",
                    "label": 0
                },
                {
                    "sent": "The basic idea here is.",
                    "label": 0
                },
                {
                    "sent": "We want to try and do inference on a system where we have some kind of ground truth, I mean.",
                    "label": 0
                },
                {
                    "sent": "In reality, I think it was said before.",
                    "label": 0
                },
                {
                    "sent": "There's not really much ground truth in these things.",
                    "label": 0
                },
                {
                    "sent": "And so we were looking for a system where.",
                    "label": 0
                },
                {
                    "sent": "We could get some kind of plausible changepoints and kind of verify offering empirically.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing here is looking at genetic activity of the Drosophila fruit fly, so this is as it grows up from, like a lava to a kind of adult fly.",
                    "label": 0
                },
                {
                    "sent": "So there's clear kind of physiological changes in how to fly.",
                    "label": 0
                },
                {
                    "sent": "Looks like can we recover these just by looking at the genetic structure?",
                    "label": 0
                },
                {
                    "sent": "Using our methods, so we're using a super simple kind of.",
                    "label": 0
                },
                {
                    "sent": "Shark model system.",
                    "label": 0
                },
                {
                    "sent": "Look at order aggressive model, super simple, but our noise term here is going to be our kind of regularize Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the kind of results.",
                    "label": 0
                },
                {
                    "sent": "So this is a nightmare slide.",
                    "label": 0
                },
                {
                    "sent": "You'll probably have to talk to me more very later, but.",
                    "label": 0
                },
                {
                    "sent": "Basically this is comparing the two types of smoothness assumption that we get when we do the inference in the model.",
                    "label": 0
                },
                {
                    "sent": "So we have the graphs which kind of recovered at different points in time.",
                    "label": 0
                },
                {
                    "sent": "But then we have a different types of behavior for the kind of how these graphs evolve depending on the kind of structure we imposed through our regularization terms.",
                    "label": 0
                },
                {
                    "sent": "So in this one we're using this kind of group regularizer, so we're grouping changes across the whole set of nodes.",
                    "label": 0
                },
                {
                    "sent": "So here this is a subset of 100 jeans.",
                    "label": 0
                },
                {
                    "sent": "So you can kind of see like when there is a change, it's affecting a large subset of the genes in there.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of interesting because you get these kind of.",
                    "label": 0
                },
                {
                    "sent": "It's not shown on here, but there's kind of a.",
                    "label": 0
                },
                {
                    "sent": "Where it kind of grows up, you can recover that kind of structure without having any kind of assumptions on that a priori, so it's kind of suggesting that maybe this is a useful tool.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's kind of where we're at the moment we're trying to assess this kind of more theoretically, so, like looking at sample complexity of these models, it's quite hard in this kind of change point set up to do that.",
                    "label": 0
                },
                {
                    "sent": "Generalizing these to non Gaussian cases, so looking at popular versions of this an for cases where you have kind of mean structure.",
                    "label": 0
                },
                {
                    "sent": "So here I've used this kind of a 0 mean model so we have some papers on kind of wavelet version of this which is much more flexible still.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "And then can we layer techniques for analyzing the kind of structures we recover from these graphs to kind of bring further insight to these kind of systems?",
                    "label": 1
                },
                {
                    "sent": "This is pretty much it.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                }
            ]
        }
    }
}